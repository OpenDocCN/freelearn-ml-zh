["```py\nimport math\nimport os\nimport mldatasets\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.sequence import \\ TimeseriesGenerator\nfrom keras.utils import get_file\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import TwoSlopeNorm\nimport seaborn as sns\nfrom alibi.explainers import IntegratedGradients\nfrom distython import HEOM\nimport shap\nfrom SALib.sample import morris as ms\nfrom SALib.analyze import morris as ma\nfrom SALib.plotting import morris as mp\nfrom SALib.sample.saltelli import sample as ss\nfrom SALib.analyze.sobol import analyze as sa\nfrom SALib.plotting.bar import plot as barplot \n```", "```py\nare loading the data into a DataFrame called traffic_df. Please note that the prepare=True parameter is important because it performs necessary tasks such as subsetting the DataFrame to the required timeframe, since October 2015, some interpolation, correcting holidays, and performing one-hot encoding:\n```", "```py\ntraffic_df = mldatasets.load(\"traffic-volume-v2\", prepare=True) \n```", "```py\nlb = 168\nfig, (ax0,ax1,ax2,ax3) = plt.subplots(4,1, figsize=(15,8))\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4)\ntraffic_df[(lb*160):(lb*161)].traffic_volume.plot(ax=ax0)\ntraffic_df[(lb*173):(lb*174)].traffic_volume.plot(ax=ax1)\ntraffic_df[(lb*186):(lb*187)].traffic_volume.plot(ax=ax2)\ntraffic_df[(lb*199):(lb*200)].traffic_volume.plot(ax=ax3) \n```", "```py\nweekend_df = traffic_df[\n    ['hr', 'dow', 'is_holiday', 'traffic_volume']].copy()\nweekend_df['type_of_day'] = np.where(\n    weekend_df.is_holiday == 1,\n    'Holiday',\n    np.where(weekend_df.dow >= 5, 'Weekend', 'Weekday')\n)\nweekend_df = weekend_df.groupby(\n['type_of_day','hr']) ['traffic_volume']\n    .agg(['mean','std'])\n    .reset_index()\n    .pivot(index='hr', columns='type_of_day', values=['mean', 'std']\n)\nweekend_df.columns = [\n    ''.join(col).strip().replace('mean','')\\\n    for col in weekend_df.columns.values\n]\nfig, ax = plt.subplots(figsize=(15,8))\nweekend_df[['Holiday','Weekday','Weekend']].plot(ax=ax)\nplt.fill_between(\n    weekend_df.index,\n    np.maximum(weekend_df.Weekday - 2 * weekend_df.std_Weekday, 0),\n    weekend_df.Weekday + 2 * weekend_df.std_Weekday,\n    color='darkorange',\n    alpha=0.2\n)\nplt.fill_between(\n    weekend_df.index,\\\n    np.maximum(weekend_df.Weekend - 2 * weekend_df.std_Weekend, 0),\n    weekend_df.Weekend + 2 * weekend_df.std_Weekend,\n    color='green',\n    alpha=0.1\n)\nplt.fill_between(\n    weekend_df.index,\\\n    np.maximum(weekend_df.Holiday - 2 * weekend_df.std_Holiday, 0),\n    weekend_df.Holiday + 2 * weekend_df.std_Holiday,\n    color='cornflowerblue',\n    alpha=0.1\n) \n```", "```py\ntrain = traffic_df[:-4368]\nvalid = traffic_df[-4368:-2184]\ntest = traffic_df[-2184:] \n```", "```py\nplt.plot(train.index.values, train.traffic_volume.values,\n          label='train')\nplt.plot(valid.index.values, valid.traffic_volume.values,\n           label='validation')\nplt.plot(test.index.values, test.traffic_volume.values,\n          label='test')\nplt.ylabel('Traffic Volume')\nplt.legend() \n```", "```py\ny_scaler = MinMaxScaler()\ny_scaler.fit(traffic_df[['traffic_volume']])\nX_scaler = MinMaxScaler()\nX_scaler.fit(traffic_df.drop(['traffic_volume'], axis=1)) \n```", "```py\ny_train = y_scaler.transform(train[['traffic_volume']])\nX_train = X_scaler.transform(train.drop(['traffic_volume'], axis=1))\ny_test = y_scaler.transform(test[['traffic_volume']])\nX_test = X_scaler.transform(test.drop(['traffic_volume'], axis=1)) \n```", "```py\ngen_train = TimeseriesGenerator(\n    X_train,\n    y_train,\n    length=lb,\n    batch_size=24\n)\ngen_test = TimeseriesGenerator(\n    X_test,\n    y_test,\n    length=lb,\n    batch_size=24\n)\nprint(\n    \"gen_train:%s×%s→%s\" % (len(gen_train),\n    gen_train[0][0].shape, gen_train[0][1].shape)\n)\nprint(\n    \"gen_test:%s×%s→%s\" % (len(gen_test),\n    gen_test[0][0].shape, gen_test[0][1].shape)\n) \ngen_train) and the testing generator (gen_test), which use a length of 168 hours and a batch size of 24:\n```", "```py\ngen_train:  1454 ×   (24, 168, 15)   →   (24, 1)\ngen_test:   357  ×   (24, 168, 15)   →   (24, 1) \n```", "```py\nrand = 9\nos.environ['PYTHONHASHSEED']=str(rand)\ntf.random.set_seed(rand)\nnp.random.seed(rand) \n```", "```py\nmodel_name = 'LSTM_traffic_168_compact1.hdf5'\nmodel_path = get_file(\n    model_name,\n    'https://github.com/PacktPublishing/Interpretable-\\ \n    Machine-Learning-with-Python-2E/blob/main/models/{}?raw=true'\n    .format(model_name)\n)\nlstm_traffic_mdl = keras.models.load_model(model_path)\nlstm_traffic_mdl.summary() \nbidirectional LSTM layer with an output of (24, 168). 24 corresponds to the batch size, while 168 means that there’s not one but two 84-unit LSTMs going in opposite directions and meeting in the middle. It has a dropout of 10%, and then a dense layer with a single ReLu-activated unit. The ReLu ensures that all the predictions are over zero since negative traffic volume makes no sense:\n```", "```py\nModel: \"LSTM_traffic_168_compact1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nBidir_LSTM (Bidirectional)   (24, 168)                 67200     \n_________________________________________________________________\nDropout (Dropout)            (24, 168)                 0         \n_________________________________________________________________\nDense (Dense)                (24, 1)                   169       \n=================================================================\nTotal params: 67,369\nTrainable params: 67,369\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```py\ny_train_pred, y_test_pred, y_train, y_test =\\\n    mldatasets.evaluate_reg_mdl(lstm_traffic_mdl,\n        gen_train,\n        gen_test,\n        y_train,\n        y_test,\n        scaler=y_scaler,\n        y_truncate=True,\n        predopts={\"verbose\":1}\n) \nFigure 9.5. The *regression plot* is, essentially, a scatter plot of the observed versus predicted traffic volumes, fitted to a linear regression model to show how well they match. These plots show that the model tends to predict zero traffic when it’s substantially higher. Besides that, there are a number of extreme outliers, but it fits relatively well with a test RMSE of 430 and only a slightly better train RMSE:\n```", "```py\nevaluate_df = test.iloc[-y_test_pred.shape[0]:,[0,1,6,7]]\n    .rename(columns={'traffic_volume':'actual_traffic'}\n)\nevaluate_df['predicted_traffic'] = y_test_pred\nevaluate_df['type_of_day'] = np.where(\n    evaluate_df.is_holiday == 1,\n    'Holiday',\n    np.where(evaluate_df.dow >= 5,\n    'Weekend', 'Weekday')\n)\nevaluate_df.drop(['dow','is_holiday'], axis=1, inplace=True) \n```", "```py\ndef rmse(g):\n    rmse = np.sqrt(\n    metrics.mean_squared_error(g['actual_traffic'],\n                               g['predicted_traffic'])\n    )\n    return pd.Series({'rmse': rmse})\nevaluate_by_hr_df = evaluate_df.groupby(['type_of_day', 'hr'])\n    .apply(rmse).reset_index()\n    .pivot(index='hr', columns='type_of_day', values='rmse')\nmean_by_daytype_s = evaluate_by_hr_df.mean(axis=0) \n```", "```py\nevaluate_by_hr_df.plot()\nax = plt.gca()\nax.set_title('Hourly RMSE distribution', fontsize=16)\nax.set_ylim([0,2500])\nax.axhline(\n    y=mean_by_daytype_s.Holiday,\n    linewidth=2,\n    color='cornflowerblue',\n    dashes=(2,2)\n)\nax.axhline(\n    y=mean_by_daytype_s.Weekday,\n    linewidth=2,\n    color='darkorange',\n    dashes=(2,2)\n)\nax.axhline(\n    y=mean_by_daytype_s.Weekend,\n    linewidth=2,\n    color='green',\n    dashes=(2,2)\n) \n```", "```py\nactual_over_half_cap = np.where(evaluate_df['actual_traffic'] >\\\n                                2650, 1, 0)\npred_over_half_cap = np.where(evaluate_df['predicted_traffic'] >\\\n                              2650, 1, 0)\nactual_over_nc_thresh = np.where(evaluate_df['actual_traffic'] >\\\n                                 1500, 1, 0)\npred_over_nc_thresh = np.where(evaluate_df['predicted_traffic'] >\\\n                               1500, 1, 0)\nmldatasets.compare_confusion_matrices(\n    actual_over_half_cap,\n    pred_over_half_cap,\n    actual_over_nc_thresh,\n    pred_over_nc_thresh,\n    'Over Half-Capacity',\n    'Over No-Construction Threshold'\n) \nFigure 9.7.\n```", "```py\ny_all = y_scaler.transform(traffic_df[['traffic_volume']])\nX_all = X_scaler.transform(\n    traffic_df.drop(['traffic_volume'], axis=1)\n)\ngen_all = TimeseriesGenerator(\n    X_all, y_all, length=lb, batch_size=24\n) \n```", "```py\nX_df = traffic_df.drop(['traffic_volume'], axis=1 \\\n    ).reset_index(drop=True)\nholiday_afternoon_s = X_df[\n    (X_df.index >= 43800) & (X_df.dow==0) &\\\n    (X_df.hr==16) &(X_df.is_holiday==1)\n].tail(1)\npeak_morning_s = X_df[\n    (X_df.index >= 43800) & (X_df.dow==2) &\\\n    (X_df.hr==8) & (X_df.weather_Clouds==1) & (X_df.temp<20)\n].tail(1)\nhot_Saturday_s = X_df[\n    (X_df.index >= 43800) & (X_df.dow==5) &\\\n    (X_df.hr==12) & (X_df.temp>29) & (X_df.weather_Clear==1)\n].tail(1) \n```", "```py\nig = IntegratedGradients(\n    lstm_traffic_mdl, n_steps=25, internal_batch_size=24\n) \n```", "```py\nnidx = holiday_afternoon_s.index.tolist()[0] – lb\nbatch_X = gen_all[nidx//24][0]\nprint(batch_X.shape) \n```", "```py\nsamples = [holiday_afternoon_s, peak_morning_s, hot_saturday_s]\nsample_names = ['Holiday Afternoon', 'Peak Morning' , 'Hot Saturday']\nfor s in range(len(samples)):\n    nidx = samples[s].index.tolist()[0] - lb\n    batch_X = gen_all[nidx//24][0]\n    explanation = ig.explain(batch_X, target=None)\n    attributions = explanation.attributions[0]\n    attribution_img = np.transpose(attributions[nidx%24,:,:])\n    end_date = traffic_df.iloc[samples[s].index\n        ].index.to_pydatetime()[0]\n    date_range = pd.date_range(\n        end=end_date, periods=8, freq='1D').to_pydatetime().tolist()\n    columns = samples[s].columns.tolist()  \n    plt.title(\n        'Integrated Gradient Attribution Map for \"{}\"'.\\\n        format(sample_names[s], lb), fontsize=16\n    )\n    divnorm = TwoSlopeNorm(\n        vmin=attribution_img.min(),\n        vcenter=0,\n        vmax=attribution_img.max()\n    )\n    plt.imshow(\n        attribution_img,\n        interpolation='nearest' ,\n        aspect='auto',\n        cmap='coolwarm_r',\n        norm=divnorm\n)\n    plt.xticks(np.linspace(0,lb,8).astype(int), labels=date_range)\n    plt.yticks([*range(15)], labels=columns)\n    plt.colorbar(pad=0.01,fraction=0.02,anchor=(1.0,0.0))\n    plt.show() \n```", "```py\ndef filt_fn(X_df, x, lookback):\n    x_ = x.copy()\n    x_[0] = round(x_[0]) #round dow\n    x_[1] = round(x_[1]) #round hr\n    x_[6] = round(x_[6]) #round is_holiday\n    if x_[1] < 0:#if hr < 0\n        x_[1] = 24 + x_[1]\n        x_[0] = x_[0] – 1  #make it previous day\n    if x_[0] < 0:#if dow < 0\n        x_[0] = 7 + x_[0] #make it previous week\n        X_filt_df = X_df[\n            (X_df.index >= lookback) & (X_df.dow==x_[0]) &\\\n            (X_df.hr==x_[1]) & (X_df.is_holiday==x_[6]) &\\\n            (X_df.temp-5<=x_[2]) & (X_df.temp+5>=x_[2])\n        ]\n    return X_filt_df, x_ \n```", "```py\ncat_idxs = np.where(traffic_df.drop(['traffic_volume'],\\\n                                    axis=1).dtypes != np.float64)[0]\nheom_dist = HEOM(X_df.values, cat_idxs)\nprint(cat_idxs) \n```", "```py\npredict_fn = lambda X: mldatasets.approx_predict_ts(\n    X, X_df,\n    gen_all,\n    lstm_traffic_mdl,\n    dist_metric=heom_dist.heom,\n    lookback=lookback,\n    filt_fn=filt_fn,\n    X_scaler=X_scaler,\n    y_scaler=y_scaler\n) \n```", "```py\nworking_season_df =\\\n    traffic_df[lookback:].drop(['traffic_volume'], axis=1).copy()\nworking_season_df =\\\n    working_season_df[(working_season_df.index.month >= 3) &\\\n                      (working_season_df.index.month <= 11)]\nexplainer = shap.KernelExplainer(\n    predict_fn, shap.kmeans(working_season_df.values, 24)\n) \n```", "```py\nX_samp_df = working_season_df.sample(80, random_state=rand)\nshap_values = explainer.shap_values(X_samp_df, nsamples=10)\nshap.summary_plot(shap_values, X_samp_df) \n```", "```py\nfor s in range(len(samples)):\n    print('Local Force Plot for \"{}\"'.format(sample_names[s]))\n    shap_values_single = explainer.shap_values(\n        datapoints[i], nsamples=80)\n    shap.force_plot(\n    explainer.expected_value,\n    shap_values_single[0],\n    samples[s],\n    matplotlib=True\n)\n    plt.show() \n```", "```py\nworking_hrs_df = working_season_df[\n    (working_season_df.dow < 5)\n    & ((working_season_df.hr < 5) | (working_season_df.hr > 22))\n]\nworking_hrs_df.describe(percentiles=[.01,.5,.99]).transpose() \n```", "```py\nmorris_problem = {\n    # There are nine variables\n    'num_vars': 10,\n    # These are their names\n    'names': ['dow', 'hr', 'temp', 'rain_1h', 'snow_1h',\\\n              'cloud_coverage', 'is_holiday', 'weather_Clear',\\\n              'weather_Clouds', 'weather_Rain'],\n    # Plausible ranges over which we'll move the variables\n    'bounds': [\n        [0, 4], # dow Monday - Firday\n        [-1, 4], # hr\n        [-12, 25.], # temp (C)\n        [0., 3.1], # rain_1h\n        [0., .3], # snow_1h\n        [0., 100.], # cloud_coverage\n        [0, 1], # is_holiday\n        [0, 1], # weather_Clear\n        [0, 1], # weather_Clouds\n        [0, 1] # weather_Rain\n    ],\n    # Only weather is grouped together\n    'groups': ['dow', 'hr', 'temp', 'rain_1h', 'snow_1h',\\\n                'cloud_coverage', 'is_holiday', 'weather', 'weather',\\\n                'weather']\n} \n```", "```py\nmorris_sample = ms.sample(morris_problem, 256,\\\n                          num_levels=4, seed=rand)\nprint(morris_sample.shape) \n```", "```py\nmorris_sample_mod = np.hstack(\n    (\n        morris_sample[:,0:9],\n        np.zeros((morris_sample.shape[0],3)),\n        morris_sample[:,9:10],\n        np.zeros((morris_sample.shape[0],2))\n    )\n)\nprint(morris_sample_mod.shape) \n```", "```py\nmorris_preds = mldatasets.approx_predict_ts(\n    morris_sample_mod,\n    X_df,\n    gen_all,\n    lstm_traffic_mdl,\n    filt_fn=filt_fn,\n    dist_metric=heom_dist.heom,\n    lookback=lookback,\n    X_scaler=X_scaler,\n    y_scaler=y_scaler,\n    progress_bar=True\n) \n```", "```py\nmorris_sensitivities = ma.analyze(\n    morris_problem, morris_sample, morris_preds,\\\n    print_to_console=False\n) \n```", "```py\nmorris_df = pd.DataFrame(\n    {\n        'features':morris_sensitivities['names'],\n        'μ':morris_sensitivities['mu'],\n        'μ*':morris_sensitivities['mu_star'],\n        'σ':morris_sensitivities['sigma']\n    }\n)\nmorris_df.sort_values('μ*', ascending=False).style\\\n    .background_gradient(cmap='plasma', subset=['μ*']) \n```", "```py\nfig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,8))\nmp.horizontal_bar_plot(ax0, morris_sensitivities, {})\nmp.covariance_plot(ax1, morris_sensitivities, {})\nax1.text(\n    ax1.get_xlim()[1] * 0.45, ax1.get_ylim()[1] * 0.75,\\\n    'Non-linear and/or-monotonic', color='gray',\\\n    horizontalalignment='center'\n)\nax1.text(ax1.get_xlim()[1] * 0.75, ax1.get_ylim()[1] * 0.5,\\\n    'Almost Monotonic', color='gray', horizontalalignment='center')\nax1.text(ax1.get_xlim()[1] * 0.83, ax1.get_ylim()[1] * 0.2,\\\n    'Monotonic', color='gray', horizontalalignment='center')\nax1.text(ax1.get_xlim()[1] * 0.9, ax1.get_ylim()[1] * 0.025,\n    'Linear', color='gray', horizontalalignment='center') \n```", "```py\nsobol_problem = {\n    'num_vars': 8,\n    'names': ['dow', 'hr', 'temp', 'rain_1h', 'snow_1h',\n              'cloud_coverage', 'is_holiday', 'weather_Clear'],\n    'bounds': [\n        [0, 4], # dow Monday through Friday\n        [-1, 4], # hr\n        [-3., 31.], # temp (C)\n        [0., 21.], # rain_1h\n        [0., 1.6], # snow_1h\n        [0., 100.], # cloud_coverage\n        [0, 1], # is_holiday\n        [0, 1] # weather_Clear\n      ],\n    'groups': None\n} \n```", "```py\nsaltelli_sample = ss.sample(\n    sobol_problem, 256, calc_second_order=True, seed=rand\n)\nsaltelli_sample_mod = np.hstack(\n    (saltelli_sample, np.zeros((saltelli_sample.shape[0],7)))\n)\nprint(saltelli_sample_mod.shape) \n```", "```py\nsaltelli_preds = mldatasets.pprox._predict_ts(\n    saltelli_sample_mod,\n    X_df,\n    gen_all,\n    lstm_traffic_mdl,\n    filt_fn=filt_fn,\n    dist_metric=heom_dist.heom,\n    lookback=lookback,\n    X_scaler=X_scaler,\n    y_scaler=y_scaler,\n    progress_bar=True\n) \n```", "```py\ncosts = np.where(saltelli_preds > 1500, 1,0)[:,0]\nfactor_fixing_sa = sa.analyze(\n    sobol_problem,\n    costs,\n    calc_second_order=True,\n    print_to_console=False\n) \n```", "```py\nsobol_df = pd.DataFrame(\n    {\n        'features':sobol_problem['names'],\n        '1st':factor_fixing_sa['S1'],\n        'Total':factor_fixing_sa['ST'],\n        'Total Conf':factor_fixing_sa['ST_conf'],\n        'Mean of Input':saltelli_sample.mean(axis=0)[:8]\n    }\n)\nsobol_df.sort_values('Total', ascending=False).style\n    .background_gradient(cmap='plasma', subset=['Total']) \n```", "```py\nS2 = factor_fixing_sa['S2']\ndivnorm = TwoSlopeNorm(vmin=S2.min(), vcenter=0, vmax=S2.max())\nsns.heatmap(S2, center=0.00, norm=divnorm, cmap='coolwarm_r',\\\n            annot=True, fmt ='.2f',\\\n            xticklabels=sobol_problem['names'],\\\n            yticklabels=sobol_problem['names']) \n```", "```py\n#Join input and outputs into a sample+prediction array\nsaltelli_sample_preds = np.hstack((saltelli_sample, saltelli_preds)) \n```", "```py\n#Define cost function\ndef cost_fn(x):\n    cost = 0\n    if x[8] > 1500:\n        cost = (x[8] - 1500) * 15\n    if round(x[1]) == 4:\n        cost = cost + 1500\n        if round(x[0]) == 4:\n            cost = cost + 4500\n    return cost\n#Use list comprehension to compute costs for sample+prediction array\ncosts2 = np.array([cost_fn(xi) for xi in saltelli_sample_preds])\n#Print total fines for entire sample predictions\nprint('Total Fines: $%s' % '{:,.2f}'.format(sum(costs2))) \n```", "```py\nfactor_fixing2_sa = sa.analyze(\n    sobol_problem, costs2, calc_second_order=True,\n    print_to_console=False\n) \n```", "```py\nfactor_fixing2_df = factor_fixing2_sa.to_df()\nfig, (ax) = plt.subplots(1,1, figsize=(15, 7))\nsp.plot(factor_fixing2_df[0], ax=ax) \n```"]