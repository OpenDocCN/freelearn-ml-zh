- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Using Synthetic Data in Data-Centric Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在以数据为中心的机器学习中使用合成数据
- en: In previous chapters, we discussed various approaches to improving data quality
    for machine learning purposes through better collection and labeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了通过更好的收集和标注来提高机器学习目的数据质量的各种方法。
- en: Although human labelers, data ownership, and technical data quality improvement
    practices are critical to data centricity, there are limits to the kind of labeling
    and data creation that can be performed by individuals or through empirical observation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人类标注者、数据所有权和技术数据质量改进实践对于数据中心化至关重要，但个人或通过经验观察所能执行的数据标注和数据创建类型是有局限性的。
- en: Synthetic data has the potential to fill in these gaps and produce comprehensive
    training data at a fraction of the cost and time of other approaches.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据有潜力填补这些空白，并以其他方法成本和时间的一小部分产生全面训练数据。
- en: 'This chapter provides an introduction to synthetic data generation. We will
    cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了合成数据生成。我们将涵盖以下主要主题：
- en: What synthetic data is and why it matters for data centricity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合成数据是什么以及为什么它对数据中心化很重要
- en: How synthetic data is being used to generate better models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合成数据如何被用来生成更好的模型
- en: Common techniques used to generate synthetic data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常用的生成合成数据的技术
- en: The risks and challenges with synthetic data use
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用合成数据的风险和挑战
- en: Let’s start by defining what synthetic data is.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义一下什么是合成数据。
- en: Understanding synthetic data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解合成数据
- en: Synthetic data is artificially created data that, if done right, contains all
    the characteristics of production data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据是人工创建的数据，如果做得正确，它包含了生产数据的所有特征。
- en: The reason it’s called synthetic data is that it doesn’t have a physical existence
    – that is, it doesn’t come from real-life observations or experiments that we
    create to gather data that we subsequently use to run analysis or build machine
    learning models on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为合成数据的原因是它没有物理存在——也就是说，它不是来自我们为了收集数据而创建的真实生活观察或实验。
- en: A foundational principle of machine learning is that you need a lot of data,
    ranging from thousands to billions of observations. The amount you need depends
    on your model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个基本原理是你需要大量的数据，从几千到几十亿个观测值不等。所需的数据量取决于你的模型。
- en: As we have outlined many times already, when the required volume of data is
    difficult to come by, one approach is to improve the signal in your data to make
    it possible to produce accurate and relevant outputs, even on smaller datasets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经多次概述的那样，当所需的数据量难以获得时，一种方法是提高你数据中的信号，使其能够在较小的数据集上产生准确和相关的输出。
- en: Another option is to create synthetic data to cover the gaps. A major benefit
    of synthetic data is its scalability. Real training data is collected linearly,
    one example at a time, which can be both time-consuming and expensive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是创建合成数据来填补空白。合成数据的一个主要优点是其可扩展性。真实训练数据的收集是线性的，一次收集一个示例，这既耗时又昂贵。
- en: In contrast, synthetic data can be generated in very large quantities in a relatively
    short time and typically at a lower cost. As an example, a training image that
    may cost $5 if it’s obtained from a labeling service might cost $0.05 if it’s
    produced artificially1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，合成数据可以在相对较短的时间内以非常大的数量生成，并且通常成本较低。例如，如果从标注服务中获得，一个训练图像可能需要花费5美元，但如果人工生成，可能只需0.05美元。
- en: Synthetic data is touted as the answer to many challenges in the development
    of more powerful machine learning and AI solutions. From solving privacy issues
    to inexpensively generating rare but important observations for your modeling
    and training data, synthetic data can fill the gaps where real-world data falls
    short.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据被誉为解决更强大的机器学习和AI解决方案开发中许多挑战的答案。从解决隐私问题到以低成本生成建模和训练数据中罕见但重要的观察结果，合成数据可以填补现实世界数据不足的空白。
- en: According to Gartner predictions2, 60% of the data used in AI and analytics
    projects will be synthetically generated rather than gathered through real-world
    observations by 2024.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Gartner的预测2，到2024年，用于AI和数据分析项目中60%的数据将是合成的，而不是通过现实世界的观察收集的。
- en: Traditionally, the use of data for analytical purposes has been driven by the
    data we have available and its limitations. We might imagine the perfect data
    solution, but often, the depth, breadth, reliability, and privacy constraints
    of a dataset limit what we can do in reality. To a large extent, this is what
    synthetic data aims to fix.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，用于分析目的的数据使用是由我们拥有的数据和其局限性所驱动的。我们可能会想象一个完美的数据解决方案，但通常，数据集的深度、广度、可靠性和隐私限制会限制我们在现实中能做的事情。在很大程度上，这就是合成数据旨在解决的问题。
- en: There are different ways to create synthetic data, and to some extent, the technical
    creation of the data is the least complex part. Validating whether a synthetic
    dataset is a relevant reflection of potential real-world scenarios and defending
    against unwanted bias can be time-consuming and challenging.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 创建合成数据有不同的方法，在某种程度上，数据的创建技术是最不复杂的部分。验证一个合成数据集是否是潜在真实世界场景的相关反映，以及防御不希望的偏差可能既耗时又具有挑战性。
- en: If you choose to use synthetic data for your next project, the first important
    question is always, “*What are you going to use the data for?*” The answer to
    this question determines your data needs, which, in turn, will highlight your
    data gaps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择为你的下一个项目使用合成数据，最重要的第一个问题总是，“*你打算用这些数据做什么？*”这个问题的答案决定了你的数据需求，反过来，这将突出你的数据空白。
- en: Let’s take a closer look at the typical reasons for using synthetic data and
    explore some common use cases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解使用合成数据的典型原因，并探讨一些常见的使用场景。
- en: The use case for synthetic data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成数据的使用场景
- en: 'The reasons for using synthetic data generally fall into the following four
    categories:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成数据的原因通常可以分为以下四个类别：
- en: '**Availability**: Synthetic data creation is used to compensate for the lack
    of data in a domain. It may be that we have imbalanced classes in a dataset compared
    to the real-life distribution, so to make those classes balanced, we create synthetic
    data to compensate.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：合成数据创建用于弥补某个领域数据不足的问题。可能的情况是，与现实生活中的分布相比，数据集中存在不平衡的类别，因此为了使这些类别平衡，我们创建合成数据来补偿。'
- en: '**Cost**: It can be very costly and time-consuming to collect certain types
    of data, in which case it can be useful to generate synthetic data to reduce the
    time and cost spent on a project.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：收集某些类型的数据可能非常昂贵且耗时，在这种情况下，生成合成数据以减少项目在时间和成本上的投入可能是有用的。'
- en: '**Risk management**: In some cases, synthetic data can also be used to lower
    the risk of human or financial damage. An example of this is flight simulators,
    which are used to train new and experienced pilots in all sorts of situations.
    Training pilots in a simulated environment allows us to safely and knowingly introduce
    rare events that would be hard to create in a natural environment without unacceptable
    risk.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险管理**：在某些情况下，合成数据也可以用来降低人类或财务损失的风险。一个例子是飞行模拟器，它被用来在各种情况下训练新飞行员和经验丰富的飞行员。在模拟环境中训练飞行员使我们能够安全且有意识地引入在自然环境中难以创建且风险不可接受的事件。'
- en: '**Security and legal compliance**: The data you need may already exist but
    it is unsafe or illegal to use it for machine learning purposes. For example,
    some regulations, such as Europe’s **General Data Protection Regulation** (**GDPR**),
    forbid the use of certain kinds of data without clear consent from the underlying
    individual. Alternatively, it might just be too slow and cumbersome to get signoff
    in your organization.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和法律合规性**：你可能已经拥有了所需的数据，但出于机器学习目的使用它可能是不安全或不合法的。例如，某些法规，如欧洲的**通用数据保护条例**（**GDPR**），禁止在没有底层个人明确同意的情况下使用某些类型的数据。或者，在你的组织中获取批准可能过于缓慢和繁琐。'
- en: 'Here are some examples of common and potential use cases for synthetic data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些合成数据常见和潜在使用场景的例子：
- en: Computer vision and image and video processing
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉和图像及视频处理
- en: Natural language processing
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Privacy preservation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护
- en: Correcting bias (discussed in [*Chapter 8*](B19297_08.xhtml#_idTextAnchor125)*,
    Techniques for Identifying and* *Removing Bias*)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**纠正偏差**（在第8章中讨论，*识别和移除偏差的技术*）'
- en: Improving data quality or gaps (more cheaply)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高数据质量或填补数据空白（更便宜）
- en: Increasing modeling data volumes for rare events (discussed in [*Chapter 9*](B19297_09.xhtml#_idTextAnchor141)*,
    Dealing with Edge Cases and Rare Events in* *Machine Learning*)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加罕见事件建模数据量（在第9章中讨论，*处理机器学习中的边缘情况和罕见事件*）
- en: Simulation
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟
- en: We will explore some of these topics in this chapter to illustrate how synthetic
    data can be used as part of your model development strategy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中探讨一些这些主题，以说明合成数据如何作为您模型开发策略的一部分被使用。
- en: To set the scene, let’s look at an example of just how powerful synthetic data
    can be in the right setting, courtesy of the world’s leading computer games software
    development company, Unity Technologies. By way of background, the Unity platform
    was used to create 72% of the top 1,000 mobile phone games and 50% of all computer
    games across mobile, PC, and consoles in 20213.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设定场景，让我们看看合成数据在正确设置下可以有多强大，这得益于世界上领先的计算机游戏软件开发公司Unity Technologies。作为背景，Unity平台被用于创建2021年排名前1000的移动游戏的72%以及移动、PC和游戏机上的所有电脑游戏的50%。
- en: The users of Unity’s technology have improved object recognition rates from
    70% to almost 100% simply by augmenting real-world data with synthetic data. Synthetic
    data adds a lot more variety and many more scenarios to the training data, which
    enables objects to be recognized from many angles4.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Unity技术的用户通过仅通过将合成数据添加到现实世界数据中，就将物体识别率从70%提高到几乎100%。合成数据为训练数据增加了更多的多样性和更多的场景，使得物体可以从多个角度被识别。
- en: 'Unity’s Vice President of AI and machine learning, David Lange, says the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Unity人工智能和机器学习副总裁David Lange表示如下：
- en: “*We’re using the Unity engine to recreate three-dimensional worlds with objects
    in there. Then, we can generate synthetic images that look very much like what
    they would look like in the real world,* *perfectly labeled.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “*我们使用Unity引擎来重新创建包含物体的三维世界。然后，我们可以生成看起来非常像在现实世界中的合成图像，**标签完美。*”
- en: “*Real-world data is really just a snapshot of the situation. What you can do
    with the synthetic data is augment that real world with special use cases, special
    situations, special events. You can improve the diversity of your data by adding
    synthetic data to* *your dataset.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: “*现实世界的数据实际上只是对情况的快照。你可以通过添加合成数据来增强现实世界，包括特殊用例、特殊情况和特殊事件。你可以在你的数据集中添加合成数据来提高你数据的多样性。*”
- en: “*We can create improbable situations because it’s not going to cost us anything
    in milliseconds, rather than trying to stage them in reality. The ease with which
    you can create all these scenarios is driving the use of* *synthetic data.*”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: “*我们可以创建不可能的情况，因为这不会在毫秒内给我们带来任何成本，而不是试图在现实中安排它们。你可以轻松创建所有这些场景的便利性正在推动合成数据的使用。*”
- en: 'David Lange shares the view of Gartner in that synthetic data is going to be
    the predominant raw material for machine learning in the future:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: David Lange与Gartner的观点一致，认为合成数据将成为未来机器学习的主要原材料：
- en: “*I believe that the vast majority of training data will be synthetic. You have
    to have the real world as a baseline, but synthetic data eliminates privacy concerns
    because there are no real people involved. You can eliminate bias. You can do
    your data analytics and ensure that your data represents the real world in a very
    even way, better than the real* *world does.*”
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: “*我相信大部分的训练数据将是合成数据。你必须有一个现实世界作为基准，但合成数据消除了隐私问题，因为没有涉及真实的人。你可以消除偏见。你可以进行数据分析，并确保你的数据以非常均匀的方式代表现实世界，比现实世界做得更好。*”
- en: 'Take note of the benefits of synthetic data that David Lange mentions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意David Lange提到的合成数据的益处：
- en: Objects can be perfectly labeled, thereby avoiding the labeling ambiguities
    discussed in previous chapters
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体可以被完美地标记，从而避免在前面章节中讨论的标签模糊性
- en: The diversity of the dataset can be increased substantially to cover slight
    variations in probable scenarios, as well as rare events and edge cases
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的多样性可以大幅增加，以覆盖可能的场景的微小变化，以及罕见事件和边缘情况
- en: Datasets can be scaled quickly and cheaply
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集可以快速且便宜地扩展
- en: Bias and privacy concerns can be reduced because data is cleaner and depersonalized
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据更干净且去个性化，可以减少偏见和隐私问题
- en: Let’s dig deeper into the various uses of synthetic data to understand the possibilities,
    benefits, risks, and constraints associated with it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨合成数据的各种用途，以了解与之相关的可能性、好处、风险和限制。
- en: Synthetic data for computer vision and image and video processing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于计算机视觉、图像和视频处理的合成数据
- en: At the time of writing, the most prevalent use of synthetic data is in computer
    vision problems. This is because we can often create this type of data with limited
    risk, while outliers (often rare but impactful events) can be particularly hard
    to get hold of in image data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，合成数据在计算机视觉问题中最普遍的使用。这是因为我们通常可以以有限的风险创建这类数据，而异常值（通常是罕见但影响重大的事件）在图像数据中尤其难以获取。
- en: A common challenge in computer vision (and most other machine learning problems
    for that matter) is that real-world data typically contains a large proportion
    of observations describing the most probable scenarios, and very few or no examples
    of rare events.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（以及大多数其他机器学习问题）的一个共同挑战是，现实世界的数据通常包含大量描述最可能场景的观察，而罕见事件几乎没有或没有例子。
- en: At the same time, real-world data can be difficult, expensive, or outright dangerous
    to collect. As an example, autonomous vehicle models can’t be trained to avoid
    car crashes by putting real cars into dangerous situations. Instead, these crashes
    and other rare but significant events must be simulated.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，收集现实世界数据可能很困难、昂贵，甚至直接危险。例如，自动驾驶汽车模型不能通过将真实汽车置于危险情况来训练以避免车祸。相反，这些车祸和其他罕见但重要的事件必须进行模拟。
- en: A common problem for image classification algorithms is recognizing familiar
    objects in slightly unfamiliar positions or environments. Because machine learning
    algorithms don’t reason by logic or abstraction, even models that perform very
    well on both training and test datasets will often fail to generalize to out-of-distribution
    observations. This is true whether these observations are introduced as an adversarial
    test of model performance or occur naturally.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类算法的一个常见问题是识别在略微不熟悉的位置或环境中的熟悉物体。由于机器学习算法不是通过逻辑或抽象进行推理，即使模型在训练和测试数据集上都表现良好，也往往无法推广到分布外的观察。无论这些观察是作为对模型性能的对抗性测试引入，还是自然发生，都是如此。
- en: '*Figure 7**.1* provides a simplified example of this phenomenon. A square that
    is rotated 45 degrees may be interpreted by some – humans and algorithms alike
    – as a diamond, and not simply a tilted square with the same dimensions as a square
    positioned on its side:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1*提供了一个这种现象的简化示例。一个旋转45度的正方形可能被一些人——包括人类和算法——解释为钻石，而不仅仅是与侧置的正方形具有相同尺寸的倾斜正方形：'
- en: '![Figure 7.1 – These two squares are either identical or different, depending
    on the rules we use to interpret them](img/B19297_07_1.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 这两个正方形要么相同，要么不同，这取决于我们用来解释它们的规则](img/B19297_07_1.jpg)'
- en: Figure 7.1 – These two squares are either identical or different, depending
    on the rules we use to interpret them
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 这两个正方形要么相同，要么不同，这取决于我们用来解释它们的规则
- en: The implications of this bias are often substantial. In an analysis of deep
    neural networks’ performance on images from the ImageNet dataset, Alcorn et al.
    (2019)5 describe how the common image classifiers *Google Inception-v3*, *AlexNet*,
    and *ResNet-50* can easily be fooled by slight changes to the positioning of an
    object within an image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差的影响往往是重大的。在分析ImageNet数据集上的图像的深度神经网络性能时，Alcorn等人（2019）5描述了常见的图像分类器*Google
    Inception-v3*、*AlexNet*和*ResNet-50*如何能轻易被图像中物体位置的一点点变化所欺骗。
- en: 'In *Figure 7**.2*, the images in column (d) are real photographs collected
    from the internet, whereas columns (a) to (c) are out-of-distribution images of
    the same objects in unusual positions. The main object within the image is flipped
    and rotated, but the background is kept constant:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.2*中，列（d）中的图像是从互联网上收集的真实照片，而列（a）到（c）是同一物体在不寻常位置中的分布外图像。图像中的主要物体被翻转和旋转，但背景保持不变：
- en: '![Figure 7.2 – Deep neural networks can easily be fooled when familiar objects
    are in uncommon positions. Column (d) represents real-life images, while columns
    (a) to (c) are synthetic](img/B19297_07_2.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 当熟悉物体处于不常见位置时，深度神经网络很容易被欺骗。列（d）代表真实图像，而列（a）到（c）是合成的](img/B19297_07_2.jpg)'
- en: Figure 7.2 – Deep neural networks can easily be fooled when familiar objects
    are in uncommon positions. Column (d) represents real-life images, while columns
    (a) to (c) are synthetic
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 当熟悉物体处于不常见位置时，深度神经网络很容易被欺骗。列（d）代表真实图像，而列（a）到（c）是合成的
- en: The authors then used *Inception-v3* to classify these images, with the resulting
    label and confidence score depicted under each image. In these examples, the algorithm
    was able to classify with a high degree of accuracy and confidence when objects
    were in a commonly observed position in real-life scenarios. However, the algorithm
    misclassified images with a high degree of confidence when objects were being
    flipped, rotated, or moved very close.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作者随后使用*Inception-v3*对这些图像进行分类，每个图像下的标签和置信度得分被描绘出来。在这些例子中，当物体在现实场景中的常见位置时，算法能够以高精度和置信度进行分类。然而，当物体被翻转、旋转或非常接近移动时，算法会以高置信度错误地分类图像。
- en: Not only did the algorithm misclassify objects, but it also confidently mislabeled
    them as objects they are not. A rolling bus and a punching bag are like chalk
    and cheese, and an overturned scooter is nowhere close to looking like a parachute.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法不仅错误地分类了物体，而且还自信地将它们错误地标记为它们不是的物体。一辆滚动的公交车和一个沙袋就像白垩和奶酪一样，而一辆翻倒的滑板车与降落伞相差甚远。
- en: Being able to recognize familiar objects in unfamiliar positions is especially
    critical when it comes to observing and classifying moving objects. In the example
    of self-driving cars, algorithmic misinterpretations introduce novel and unexpected
    events into the traffic environment, even though these vehicles are statistically
    safer than human drivers6.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在陌生位置识别熟悉物体对于观察和分类移动物体尤为重要。在自动驾驶汽车的例子中，算法的错误解读会将新颖且意外的事件引入交通环境，尽管这些车辆在统计数据上比人类驾驶员更安全6。
- en: 'The following image, which was captured from a traffic camera in Taiwan, shows
    an example of this issue. A truck has overturned on a busy highway, and an autonomous
    Tesla sedan doesn’t recognize the truck as an obstacle in the way and crashes
    into the truck at high speed:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片是从台湾的交通摄像头中捕捉到的，展示了这个问题的一个例子。一辆卡车在繁忙的高速公路上翻车，一辆自动驾驶的特斯拉轿车没有将卡车识别为路上的障碍物，并以高速撞向了卡车：
- en: '![Figure 7.3 – An autonomous vehicle crashes into an overturned truck at high
    speed. Algorithms can be trained to handle novel situations like this using synthetic
    data](img/B19297_07_3.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 一辆自动驾驶汽车高速撞上一辆翻车的图片。可以使用合成数据训练算法来处理这种新颖的情况](img/B19297_07_3.jpg)'
- en: Figure 7.3 – An autonomous vehicle crashes into an overturned truck at high
    speed. Algorithms can be trained to handle novel situations like this using synthetic
    data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 一辆自动驾驶汽车高速撞上一辆翻车的图片。可以使用合成数据训练算法来处理这种新颖的情况
- en: In this very unlikely but highly dangerous scenario, the car’s algorithms are
    not equipped to correctly assess the statistical probability that the object in
    front of it is blocking the road.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种极不可能但非常危险的场景中，汽车的算法没有装备正确评估前方物体阻挡道路的统计概率。
- en: This is a scenario where synthetic data proves highly valuable. As we have just
    learned, even best-in-class computer vision algorithms have a high degree of sensitivity
    to variations in the position of common objects. Therefore, objects must be introduced
    in various positions and lighting conditions in the training data to cover all
    possible combinations, especially highly improbable ones.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种合成数据证明非常有价值的情况。正如我们刚刚学到的，即使是最好的计算机视觉算法对常见物体位置的变化也具有高度敏感性。因此，必须在训练数据中引入各种位置和光照条件，以覆盖所有可能的组合，特别是高度不可能的组合。
- en: When we use machine learning to figure out what’s happening in an image, we
    are extracting the concave and convex curves within the image, also known as the
    features within a deep neural network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用机器学习来分析图像中的情况时，我们正在提取图像中的凹凸曲线，也称为深度神经网络中的特征。
- en: To create these curves from a synthetic data perspective, you would simply be
    recreating those formations within images. This would typically involve flipping,
    rotating, zooming, cropping, making light changes, and resizing images to create
    slight variations on the same scenario.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从合成数据的角度创建这些曲线，你实际上是在图像中重新创建那些形态。这通常包括翻转、旋转、缩放、裁剪、调整光线变化以及调整图像大小以在相同场景中创建轻微的变化。
- en: In the example of the Tesla accident, we might create images with the truck
    driving as normal, rolled on its side, rolled on its back, driving in the wrong
    direction, in the dark, partially covered by other objects, and so on. These scenarios
    are hard to get a hold of in real-life imagery, yet they’re very important to
    be able to deal with when the situation arises.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在特斯拉事故的例子中，我们可能会创建卡车正常行驶、侧翻、仰翻、逆向行驶、在黑暗中、部分被其他物体覆盖等图像。这些场景在现实生活中的图像中很难获得，但它们在出现这种情况时非常重要。
- en: Generating synthetic data using generative adversarial networks (GANs)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用生成对抗网络（GANs）生成合成数据
- en: GANs are common tools for generating synthetic image data with properties similar
    to real-world data. GANs were invented by computer scientist Ian Goodfellow in
    2014 and have since led to an explosion in generative models that can create all
    sorts of content, including text, art, video, and images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是生成具有类似真实世界数据特性的合成图像数据的常用工具。GANs由计算机科学家伊恩·古德费洛（Ian Goodfellow）于2014年发明，并自此以来导致了能够创建各种内容的生成模型爆炸式增长，包括文本、艺术、视频和图像。
- en: GANs are a form of unsupervised learning where a generator model is pitted against
    a discriminator model (hence the “adversarial” aspect). Both models are neural
    networks that compete against each other to turn the exercise into a “pseudo-supervised”
    learning problem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是一种无监督学习形式，其中生成器模型与判别器模型相对抗（因此有“对抗”这一方面）。这两个模型都是神经网络，它们相互竞争，将练习变成一个“伪监督”学习问题。
- en: The generator identifies patterns in the original dataset that are then used
    to generate synthetic output that could have conceivably existed in the input
    data. The generated examples become negative training samples for the discriminator
    model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器识别原始数据集中的模式，然后使用这些模式生成可能存在于输入数据中的合成输出。生成的示例成为判别器模型的负训练样本。
- en: It is the discriminator’s job to classify the newly generated data as fake or
    real. This zero-sum contest continues until the discriminator picks *fake* observations
    as *real* close to 50% of the time.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的任务是分类新生成数据为虚假或真实。这种零和竞赛持续进行，直到判别器将*虚假*观察结果作为*真实*的接近50%的时间。
- en: 'Mathematically, the training process for a GAN can be thought of as minimizing
    a loss function that measures the difference between the generated examples and
    the real examples. This loss function is typically a combination of two terms:
    one that measures how well the generative model can produce examples that are
    similar to the real examples, and one that measures how well the discriminative
    model can distinguish between real and generated examples.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，GAN的训练过程可以被视为最小化一个损失函数，该函数衡量生成示例与真实示例之间的差异。这个损失函数通常是两个术语的组合：一个衡量生成模型产生与真实示例相似的示例的能力，另一个衡量判别模型区分真实和生成示例的能力。
- en: By training the two parts of the GAN in this way, the generative model can learn
    the patterns and features of the real examples in the training dataset, and then
    use that information to generate new examples that are similar to the real ones.
    This allows GANs to be used for a wide range of applications, including image
    generation, text generation, and many others.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以这种方式训练GAN的两个部分，生成模型可以学习训练数据集中真实示例的模式和特征，然后使用这些信息生成与真实示例相似的新示例。这使得GANs可以用于广泛的用途，包括图像生成、文本生成等。
- en: '*Figure 7**.4* provides a conceptual illustration of how GANs iterate through
    a large number of mini-contests to arrive at a model that can generate very realistic
    outputs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7**.4*提供了一个概念图，说明了GANs如何通过大量的小型竞赛迭代，最终到达一个可以生成非常逼真输出的模型：'
- en: '![Figure 7.4 – A conceptual illustration of how a GAN works](img/B19297_07_4.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – GAN工作原理的概念图](img/B19297_07_4.jpg)'
- en: Figure 7.4 – A conceptual illustration of how a GAN works
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – GAN工作原理的概念图
- en: The generator starts with some very basic presentations of the desired output
    but gets better at fooling the discriminator as it iterates through many examples.
    Training is completed when the discriminator struggles to recognize real from
    fake.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器最初以一些非常基本的输出形式开始，但随着它通过许多示例进行迭代，它欺骗判别器的技巧会变得更好。当判别器难以识别真实与虚假时，训练完成。
- en: The progressive growth of GANs
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GANs的渐进式增长
- en: As you can imagine, the first few examples that are created by the generator
    will be relatively easy for the discriminator to pick. There is a lot for the
    generator model to learn and it can be challenging to make the GAN follow a learning
    path we would like it to take.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，生成器最初创建的几个示例对判别器来说相对容易识别。生成器模型有很多要学习的东西，并且使GAN遵循我们希望其采取的学习路径可能具有挑战性。
- en: GANs are inherently unstable models, especially when it comes to generating
    complex structures, such as images. To fool the discriminator, the generator must
    pick up the small details and larger structures of an image, which can be difficult
    on high-resolution images. If the generator can’t do this, it will get stuck in
    a no-man’s land of never fooling the discriminator.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GANs本质上是具有不稳定性的模型，尤其是在生成复杂结构，如图像时。为了欺骗判别器，生成器必须捕捉到图像的细节和较大结构，这在高分辨率图像上可能很困难。如果生成器做不到这一点，它将陷入一个无法欺骗判别器的无人地带。
- en: Another challenge is that large images require lots of computer memory. As a
    result, the batch size (the number of images used to update model weights each
    training iteration) must often be reduced to make sure the images will fit into
    memory. Again, this makes the training process less stable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是，大图像需要大量的计算机内存。因此，批大小（每次训练迭代中用于更新模型权重的图像数量）必须经常减少，以确保图像可以装入内存。这再次使得训练过程变得不稳定。
- en: A solution to these problems is to progressively increase the detail and complexity
    of the model’s input and output. Progressive growing was first proposed by NVIDIA
    researchers Karras et al.7 in 2017, and is a technique for training GANs that
    allows the model to gradually increase the resolution of the generated images
    over many iterations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法是通过逐步增加模型输入和输出的细节和复杂性。渐进增长最早由NVIDIA研究人员Karras等人于2017年提出，是一种训练GANs的技术，允许模型在多次迭代中逐渐增加生成图像的分辨率。
- en: Under progressive growing, the model is trained using a step-wise approach.
    First, the generator and discriminator models are trained with low-resolution
    images and seek to improve image quality by changing their parameters to optimize
    loss functions. Then, the resolution of the generated images is increased while
    fine-tuning occurs based on the understanding gathered from the initial training
    stage until the desired resolution is reached. In other words, the model learns
    in steps, rather than all at once.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在渐进增长过程中，模型使用逐步方法进行训练。首先，生成器和判别器模型使用低分辨率图像进行训练，并通过改变参数以优化损失函数来提高图像质量。然后，在基于初始训练阶段收集的理解的基础上进行微调，同时提高生成图像的分辨率，直到达到所需的分辨率。换句话说，模型是分步骤学习的，而不是一次性学习。
- en: Karras et al. propose training both the generator and discriminator with a batch
    of low-resolution images of 4x4 pixels. Then, a new sampling layer is used to
    gradually grow the image complexity to 8x8, using nearest neighbor interpolation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Karras等人提出使用一批4x4像素的低分辨率图像来训练生成器和判别器。然后，使用一个新的采样层逐渐增加图像的复杂度到8x8，采用最近邻插值。
- en: New network layers are introduced gradually to create minimal disruption between
    resolution layers. This approach allows for the smooth and seamless integration
    of newer components into the existing infrastructure.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 逐渐引入新的网络层以在分辨率层之间创建最小的干扰。这种方法允许新组件平滑且无缝地集成到现有基础设施中。
- en: The gradual phasing in of a new block of layers is done by adding higher-resolution
    inputs to the existing input or output layer. The relative influence of the new
    outputs is controlled using a weighting, α, where the weight of the original output
    is 1 - α . As α increases, the old layer is gradually faded out, while the new
    layer takes over.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向现有的输入或输出层添加更高分辨率的输入来实现新的一层层的逐渐引入。新输出的相对影响通过一个权重α来控制，其中原始输出的权重为1 - α。随着α的增加，旧层逐渐淡出，而新层接管。
- en: 'This process continues until the desired image resolution is reached. *Figure
    7**.5*, from Kerras et al., highlights the process of progressively growing from
    4x4 to 1,024x1,024-pixel images:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程一直持续到达到所需的图像分辨率。*图7.5*，来自Kerras等人，突出了从4x4到1,024x1,024像素图像的渐进增长过程：
- en: '![Figure 7.5 – Visualization of the progressive growth of a GAN from Kerras
    et al. (2017)](img/B19297_07_5.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 来自Kerras等人（2017）的GAN渐进增长可视化](img/B19297_07_5.jpg)'
- en: Figure 7.5 – Visualization of the progressive growth of a GAN from Kerras et
    al. (2017)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 来自Kerras等人（2017）的GAN渐进增长可视化
- en: This means that the model can first learn about the big picture of the image,
    and then focus on smaller details. This typically yields better results than trying
    to learn everything at once. By leveraging this approach, GANs can grasp the essential
    architecture and characteristics of low-resolution datasets, thus creating higher-quality
    images with greater precision.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型可以先了解图像的整体情况，然后专注于更小的细节。这种方法通常比一次性学习所有内容要产生更好的结果。通过利用这种方法，GAN可以掌握低分辨率数据集的基本架构和特征，从而以更高的精度创建出更高质量的图像。
- en: Achieving greater accuracy with StyleGANs
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用StyleGANs实现更高的精度
- en: The research team at NVIDIA built on their progressive GAN architecture to introduce
    the first StyleGAN in December 20188\. Since then, StyleGAN-2 and StyleGAN-3 architectures
    have been released. These incremental upgrades resolved some systemic issues in
    the output from the original StyleGAN, but are otherwise similar in structure.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 英伟达的研究团队在其渐进式GAN架构的基础上，于2018年12月推出了第一个StyleGAN。从那时起，已经发布了StyleGAN-2和StyleGAN-3架构。这些增量升级解决了原始StyleGAN输出中的一些系统性问题，但在结构上基本相似。
- en: The primary innovation of StyleGANs is the ability to control the *style* of
    the output created by the generator model. The new architecture allows the generator
    to automatically separate broader features from stochastic/random features in
    an image. Examples of broad features are a person’s pose and identity; hair and
    freckles are considered stochastic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN的主要创新是能够控制生成模型创建的输出**风格**。新的架构允许生成模型自动将图像中的更广泛特征与随机/随机特征分开。广泛特征的例子包括一个人的姿态和身份；头发和雀斑被认为是随机的。
- en: Before the introduction of StyleGANs, the inner workings of image generators
    were partially a mystery and therefore hard to control. With no effective method
    to compare different images produced by various models and a limited understanding
    of how features originated, the original GAN generators were black boxes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在StyleGANs引入之前，图像生成器的内部工作原理部分是神秘的，因此难以控制。没有有效的比较不同模型产生的不同图像的方法，以及对特征起源的有限理解，原始GAN生成器是黑盒。
- en: 'Let’s take a look at a comparison between the **progressive GAN** (**ProGAN**)
    and StyleGAN architectures in *Figure 7**.6* to understand why StyleGAN has been
    so successful in generating highly realistic synthetic images:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图7.6中**渐进式GAN**（**ProGAN**）和StyleGAN架构的比较，以了解为什么StyleGAN在生成高度逼真的合成图像方面如此成功：
- en: '![Figure 7.6 – Comparison between ProGAN (a) and StyleGAN (b) from Kerras et
    al. (2018)](img/B19297_07_6.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – Kerras等人（2018）中ProGAN（a）和StyleGAN（b）的比较](img/B19297_07_6.jpg)'
- en: Figure 7.6 – Comparison between ProGAN (a) and StyleGAN (b) from Kerras et al.
    (2018)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – Kerras等人（2018）中ProGAN（a）和StyleGAN（b）的比较
- en: While ProGANs use a progressive training methodology to grow the resolution
    of generated images layer by layer, StyleGAN gives users more control over the
    generated images via the use of a Mapping Network and Synthesis Network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ProGAN使用渐进式训练方法逐层提升生成图像的分辨率，但StyleGAN通过使用映射网络和综合网络，使用户能够对生成图像有更多的控制。
- en: StyleGAN’s Mapping Network is a type of neural network that maps a low-dimensional
    vector, *z*, to an intermediate latent space, *w*. This is known as disentangled
    representation learning. By disentangling the features in these two spaces, users
    can more easily control the different aspects of the generated image, such as
    its physiology, hairstyle, or clothing. In simpler terms, it allows for separated
    control over high-level features of the image rather than specific pixel values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN的映射网络是一种神经网络，它将低维向量 *z* 映射到中间潜在空间 *w*。这被称为解耦表示学习。通过解耦这两个空间中的特征，用户可以更容易地控制生成图像的不同方面，例如其生理结构、发型或服装。简单来说，它允许对图像的高级特征进行分离控制，而不是具体的像素值。
- en: The Synthesis Network is a deep convolutional neural network that works by receiving
    style vectors, *w*, as input and returns an output image. In synthesizing a realistic
    image, features are pulled from the feature vector and applied to the image layer
    by layer, beginning with the lowest layer and progressing one layer at a time
    to higher resolutions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 综合网络是一种深度卷积神经网络，它通过接收风格向量 *w* 作为输入并返回输出图像来工作。在合成逼真图像时，特征从特征向量中提取并逐层应用于图像层，从最低层开始，每次只提升一层到更高的分辨率。
- en: The Synthesis Network interacts with a learned **adaptive instance normalization**
    (**AdaIN**) module that rescales image features to increase diversity in image
    outputs. The module accepts the feature vector and a style vector as inputs and
    adjusts image features’ scaling and bias by subtracting the feature map’s mean
    and dividing it by the standard deviation. As a result, StyleGAN can produce highly
    detailed images by focusing on specific features such as hairstyle or eye color.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges of GANs
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although GANs are a wonderful addition to the machine learning toolbox, they
    are not without their challenges.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs are based on zero-sum game theory. Essentially, if one player triumphs,
    then the other will be defeated. This kind of situation is also known as minimax:
    your opponent looks to maximize their output while you seek to minimize it. The
    theory behind GANs states that the game between the generator and discriminator
    models will continue until a Nash Equilibrium is reached.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The Nash Equilibrium is an important solution concept within economics, politics,
    and evolutionary biology that can be seen in a wide variety of real-world scenarios.
    A Nash Equilibrium is a situation where no player has an incentive to do something
    different than what they are already doing. This is because they have considered
    what everyone else is doing and they think that their current strategy is the
    best possible option.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, all players are said to be at equilibrium as they have no
    incentive to change their behavior because any changes made by one player will
    likely lead to a worse outcome for that particular player. Therefore, it is in
    each individual’s best interest not to make any sudden changes in this type of
    equilibrium situation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where competing firms are trying to set prices
    for their products or services. If each firm sets its price too high, it may lose
    customers to its competitors. However, if each firm sets its price too low, it
    will not be able to cover its costs and make a profit. Thus, the Nash Equilibrium
    for this situation is for each firm to set their prices at a level that is low
    enough to deter customers from buying from their competitors without being so
    low that they are unable to make a profit.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Although this theory can work, it is often difficult to achieve in practice.
    There is no guarantee that cost functions will *converge* and find a Nash Equilibrium.
    In this situation, the game continues indefinitely.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, when one agent outmatches the other in terms of power and efficacy,
    the learning signal for their counterpart becomes useless; consequently, no knowledge
    is gained by either side. The most common scenario is that the discriminator becomes
    so good at picking the generator’s faults that the generator never learns how
    to advance in the game.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: One of the main challenges with GANs is called mode collapse. Like any other
    statistical model, GANs tend to find the easiest way through the underlying data,
    which can lead to the overrepresentation of modal observations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Mode collapse is another common issue that occurs when the generator produces
    an especially plausible output, which causes it to only produce that output. Once
    this happens, the discriminator is more likely to fall into a local minimum, unable
    to find a better output that it deems valid.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the generator is driven to tailor its outputs toward the criteria
    used by this static discriminator rather than attempting to create realistic or
    dynamic outputs. As such, generators tend to “over-optimize” for a particular
    outcome, as determined by their single discriminator.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: An example of mode collapse is illustrated in *Figure 7**.7* and is from Metz
    et al. (2017)9\. In this example, the researchers used the MNIST dataset of handwritten
    digits to train two different GANs. The MNIST dataset contains 10 different modes
    that represent digits 0-9.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The top four quadrants of numbers have been successfully generated (using an
    unrolled GAN training method) to look like real handwritten digits with a representation
    of all possible digits. The bottom four quadrants, on the other hand (generated
    using the original GAN architecture), have suffered from mode collapse early on
    in the process and produce only representations of the number 6:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Two different GAN architectures trained on the MNIST dataset
    from Metz et al. (2017)](img/B19297_07_7.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Two different GAN architectures trained on the MNIST dataset from
    Metz et al. (2017)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Since the development of the original GAN architecture in 2014, several new
    GAN variants have been introduced to deal with mode collapse. As a result, this
    is now a less common issue, but it’s something to always watch out for.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The mode collapse in the MNIST example is relatively easy to spot, but it is
    important to note that GANs may potentially preserve and exacerbate existing biases
    in more subtle ways.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In a 2021 study (Jain et at, 2021)10, researchers from Arizona State University
    and Rensselaer Polytechnic Institute assessed the performance of GANs in generating
    synthetic facial data. The researchers wanted to test whether GANs would exacerbate
    the modal facial characteristics from a (naturally) biased input dataset, thus
    increasing the most common features in the synthetic output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Two model architectures were compared: **deep convolutional GAN** (**DCGAN**)
    and ProGAN. The experiment involved a dataset of 17,245 images of engineering
    professors from US universities, of which 80% were classified as male and 76%
    were classified as white. The experiment used human classifiers (Turkers) to classify
    the faces in the original dataset and those produced by the GANs.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.8* shows the outcomes of two of the GANs tested against the original
    dataset. The DCGAN model resulted in heavily biased data generation, with a large
    overrepresentation of males and whites in the synthetic images. While the ProGAN
    model carried less of a bias, it still wasn’t a reasonable representation of the
    features in the original dataset.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Both DCGAN and ProGAN penalized images with mostly feminine features. DCGAN
    generated the most biased output, reducing the percentage of feminine faces from
    20% to 6.67%.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison to the 24% non-white faces in the original dataset, both DCGAN
    and ProGAN reduced that rate significantly – 1.33%% for DCGAN and 11.33% for ProGAN,
    respectively:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Comparison of gender and skin color distributions from synthetic
    facial images generated by GANs versus the original dataset. Source: Jain et al.,
    2021](img/B19297_07_8.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8 – Comparison of gender and skin color distributions from synthetic
    facial images generated by GANs versus the original dataset. Source: Jain et al.,
    2021'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In other words, GANs can produce highly realistic synthetic datasets that may
    still be a biased representation of the latent features in the original dataset.
    Ironically, this partial mode collapse may produce more realistic images because
    the GAN has specialized to perform well for certain dimensions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Following our principles of data-centric machine learning, it is important to
    identify modal bias across all dimensions of a synthetic dataset to ensure it
    is useful and ethically appropriate for your intended purpose. It is typically
    a process of trial and error to empirically validate and remove bias in GAN outputs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a notable weakness of GANs is that generating high-quality outputs
    requires a lot of computational resources, and without these, the images that
    are produced may appear blurry or unrealistic. Furthermore, without an experienced
    person choosing the appropriate directions to use for image generation, using
    GANs may not produce the desired outcome.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image augmentation with a practical example
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the complexity involved in synthetic image data generation, we want
    to finish this section with a practical example that gets you inspired to apply
    these techniques in your work.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover data augmentation, a mechanism for generating
    synthetic data for image data. We will use a pre-trained Xception model that was
    trained on ImageNet data and fine-tune it to accommodate clothing examples. We
    will achieve this by applying transfer learning to fine-tune the clothing examples,
    and then generate synthetic data to enhance its performance. With transfer learning,
    we can freeze the pre-trained layers of the network and only train the new layers
    by updating the final output. This helps the model to quickly adapt to the new
    dataset with less training time.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: By applying transfer learning and image augmentation techniques on top of a
    pre-trained Xception model, we can generate synthetic data that can improve the
    performance of the model on new data. This approach is widely used in various
    applications, including image classification, object detection, and segmentation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: To start, we need to load the pre-trained Xception model using the TensorFlow
    library. We can do this by simply importing the Xception model from the TensorFlow
    module and setting the `include_top` parameter to `False` to exclude the top layer
    of the model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Next, we must add a custom classifier on top of the pre-trained Xception model.
    We can achieve this by adding a few dense layers and a final output layer with
    the number of classes equal to the number of clothing categories we want to classify.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the model’s performance, we will apply image augmentation
    using the out-of-the-box TensorFlow features. This can include rotation, zooming,
    flipping, and other techniques to create variations in the training data and make
    the model more robust.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will train the model using the augmented training data and evaluate
    its performance on the validation set. We can fine-tune the hyperparameters of
    the model and the augmentation techniques to achieve better accuracy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: This example has been adapted from [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111)
    of *Machine Learning Bookcamp* ([https://livebook.manning.com/book/machine-learning-bookcamp/chapter-7/](http://book/machine-learning-bookcamp/chapter-7/))
    and has also been made available by the author of the book through a course run
    by the author’s company, DataTalks Club (https://github.com/alexeygrigorev/mlbookcamp-code).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will import all the necessary libraries:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will download the data by cloning the dataset from its respective
    GitHub repository:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will load an image of pants that we have downloaded from the clothing
    dataset repository. To load the image, we will use the `load_img` function:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will display the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – A pair of pants as output from the load_img function](img/B19297_07_9.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – A pair of pants as output from the load_img function
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will represent the image as a NumPy array as this is the format in
    which the model expects the image. We also need to ensure that all images are
    passed to the model with the same dimensions, so we will choose a standard of
    (299, 299, 3), which means 299 pixels top to bottom, 299 pixels left to right,
    and three color channels, which are red, green, and blue. Each pixel will be represented
    three times with values from 0-255 for each color channel.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must load the preceding image with a target size of (299,299):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s see the image with our new dimension:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Our image of a pair of pants, resized to be 299x299 pixels](img/B19297_07_10.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Our image of a pair of pants, resized to be 299x299 pixels
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have ensured the dimensions, we will load the pretrained Xception
    model and specify the input format, which is the same as the dimensions specified
    for the preceding image. Once we’ve loaded the model, we will score the image
    and check the prediction:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The model expects the list of images in a particular format; hence, we will
    pass the previous image as a list and convert it into a NumPy array. Then, we
    will preprocess the image using `preprocess_input` and use and then score the
    input. The prediction will be stored in `pred`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `preprocess_input` function is required to preprocess the image data required
    by the model since when the Xception model was trained, the input values were
    transformed from 0-255 and scaled to have values between -1 and 1\. This is important
    because the distribution of color scales may affect the prediction. Imagine that
    red color scales were between 0-100, while blue color scales were between 200-300;
    this may have led to an unstable model. Hence, scaling is important. Without the
    correct preprocessing, the predictions won’t make sense.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will decode these predictions with a convenience function. The model
    will provide a probability of the top five labels out of 1,000 different classes
    since the Xception model was trained to predict 1,000 labels. We don’t believe
    the target labels consist of clothing examples, so after the next few steps, we
    will move on to transfer learning. To decode the predictions, we will use the
    `decode_predictions` function. `decode_predictions` is a convenience function
    that provides predictions in such a format that they can be easily understood:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s quite clear that the image of the pants is closest to the `jean` label
    and then the `swimming_trunks` label, as per the labels used in the Xception model.
    However, in the training data, there is no `jean` or `swimming_trunks` label.
    Next, we will extract the training data and make sure it is preprocessed before
    we pass it for training.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will use transfer learning and leverage the entire training data.
    We will use `ImageDataGenerator` to help process the input data that the model
    requires and then create training and validation datasets. The training data will
    consist of 3,041 images, while the validation data will consist of 341 images.
    For preprocessing, we will use 150x150 pixels instead of 299x299 pixels to reduce
    the training time:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will use the `flow_from_directory` property to process the entire
    training and validation dataset. We will use `seed=42` to ensure data is passed
    to the network, it’s passed with the same randomization, and the result at each
    training layer is reproducible. For the validation data, we will utilize `shuffle=False`
    to ensure that there is no randomization at each training step but data is passed
    sequentially:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can view the target classes of our dataset:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we will apply transfer learning. To do so, we must first extract the
    base layer – in other words, we must extract the convolutional layer of the Xception
    model and ensure it is frozen. This will ensure we get access to the feature map
    of the image data of 1 million+ images. To do so, we will use the `include_top=False`
    parameter, which will ignore the dense layers and return the bottom layer of the
    convolutional neural network. Next, we will build a custom dense layer with the
    10 class labels highlighted previously and train the dense layer for our use case:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will build the architecture of the dense layer and combine it with
    the base layer. First, we will define the input standard such that the base model
    can provide the vector that’s suitable for input of the same standard. We will
    use (150,150,3) so that the model can be trained faster as more pixels can slow
    down the training process. Next, we will transform the vector from a base layer
    into a two-dimensional array. For that, we will use a method called pooling, which
    can help reduce the spatial size of feature maps but still retain the key information
    about the base layer. After that, we will create a dense layer where we specify
    the number of outputs, which is 10 in this case, and apply softmax activation
    to return a probability.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can define the loss function, utilize the Adam optimizer
    with a learning rate of 0.005, and choose accuracy as a metric:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will compile the model with the optimizer and loss to achieve the
    best accuracy:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we will train the network with the clothing dataset:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Looking at these results, it is clear that the model is overfitted since the
    model achieved 92.7% accuracy on the training data and only 81.52% accuracy on
    the validation data, which is almost a 10-11% difference at the end of 10 epochs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: We could try training dense layers with different learning rates to obtain a
    more generalized model. A learning rate controls how quickly we want the model
    to learn from training data and adjust its weights to fit the data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: A low learning rate is like watching a video at a slow pace to ensure most of
    the details are covered, whereas a high learning rate is like watching a video
    at a faster pace and some details may be missed. An optimized learning rate is
    crucial for successful training, but it often requires tuning to find a balance
    between convergence speed and accuracy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce overfitting is by adjusting the dropout rate, which is
    a regularization technique in which a percentage of data is omitted at random.
    Both adjustments require experimentation and are more model-centric approaches.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Following a data-centric approach, we want to test more data examples or better
    data examples. We need examples where the model doesn’t try to memorize specific
    pixels so that if it sees 120 red in the 130th-pixel location, it starts believing
    the image is of pants. Hence, to ensure a well-generalized model, we could leverage
    data augmentation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Concerning data-centric AI, data augmentation can be referred to as a technique
    to artificially increase the size and diversity of a training dataset by applying
    various transformations to the existing data. These transformations can include
    rotation, scaling, cropping, flipping, adding noise, and many others, depending
    on the type of data being augmented.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: There are some common augmentation techniques, such as creating different angles
    of the image, shifting images, zooming images in and out, flipping them upside
    down, and more. However, before applying augmenting techniques, we must first
    consider different ways data will be generated. For instance, if users don’t generate
    pictures upside down, then augmenting images to create flipped images may only
    add noise and not provide a good signal to the model. For the following example,
    we will apply zoom augmentation, where images will be zoomed in and out a little,
    some shifting where clothing in the images will shift close to the edges, and
    apply vertical flips so that if some images are taken using a mirror, we can capture
    additional data for each scenario. We will achieve this by updating the image
    generator function, adding these extra parameters, and then training the model
    on the best parameters.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply a rotation range of 10, a shear range of 10, a width and height
    shift range of 0.2, a zoom range of 0.1, and vertical flip. To achieve this, we
    will tweak the `ImageDataGenerator` function as this will create more examples
    of the training data under the hood:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will also create a function that will take in the learning rate and return
    the model and its parameters:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will use this function and pass `0.005` as the learning rate and add
    50 epochs since we have generated a lot more data for augmentation:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The model achieved lower training accuracy, but the model is more generalizable
    and not overfitted. Also, note that the best validation accuracy was achieved
    at the 32nd epoch. However, it is difficult to note at which epoch a model will
    achieve the best accuracy.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we can further utilize the model checkpoint functionality to
    ensure only a model that achieves a minimum validation accuracy of 78% at a given
    epoch will be created and saved, and only when a previous best accuracy is surpassed
    will a new model be created. We can then use these saved models to score test
    data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will add a dropout rate of 0.2 and an inner layer of 50\.
    We will update the function for training the network and add the checkpoint functionality.
    Once the checkpoint has been created, we’ll add it as a callback to the `fit`
    function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to utilize hyperparameterization with the learning rate, dropout
    rate, and inner layer while using the checkpoint to ensure the best accuracy.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a function that takes three inputs – the learning rate,
    the inner layer, and the dropout rate:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we will define the checkpoint. This is where we will save all the models
    with various epochs where the validation accuracy has reached a minimum of 78%.
    Then, we will add this checkpoint to the callback of the fitting function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that the best model has been trained and saved, we will import the saved
    model, score all the data, and calculate the test accuracy:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The model that we used to predict the test data had a training accuracy of 79.16%
    and a validation accuracy of 81.23%. The test accuracy we achieved was only 77.15%,
    which can be improved iteratively – but beyond the scope of this example – by
    utilizing hyperparameter tuning for data augmentation parameters and model-centric
    parameters, which is encouraged in real life. However, due to computing and time
    constraints, this is outside the scope of this book.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will extract the target labels and build a function to provide a predicted
    probability score along with the relevant classes. First, we will load the image
    and preprocess it, and then we will score it:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we will extract a random image of pants and run it through the model
    to get the prediction:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will result in the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – A randomly selected pair of pants from the clothing dataset](img/B19297_07_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – A randomly selected pair of pants from the clothing dataset
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will load and preprocess the image before scoring it. Finally, we
    will decode the predictions to extract the final prediction. For this, we will
    leverage the functions we created earlier:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: According to the model, the image has a probability of 99% to be classified
    as a pair of pants, which is quite accurate.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have been able to demonstrate, through the data-centric
    technique of data augmentation, how to generalize the model. We believe that by
    iterating over data augmentation parameters, we can further improve the quality
    of the model. Once the parameters have been tuned, we recommend that practitioners
    combine model-centric techniques such as regularization, the learning rate, inner
    layers, and the dropout rate to further tune and improve the model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to exploring another topic that relies on unstructured
    data: synthetic data for text and natural language processing.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic text data is typically used to increase the depth and breadth of written
    words and sentences with a similar semantic meaning to real-life observations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The most common augmentation techniques that are used to create synthetic data
    for natural language processing involve replacing words with synonyms, randomly
    shuffling the position of words in a sentence, and inserting or deleting words
    in a sentence.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: For example, the sentence “I love drinking tea” could be transformed into “I
    take great pleasure in consuming tea” without losing the contextual meaning of
    the statement. This is an example of *synonym replacement*, where “love” has been
    replaced with “take great pleasure in” and “drinking” has been replaced with “consuming.”
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*Back translation* is another NLP technique that involves translating a sentence
    in one language into another language, and then back into the original language.
    Often, this will generate slightly different sentence structures with a similar
    semantic meaning, which makes it a great way to combat overfitting while increasing
    the size of your training data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate a simple example of how to perform back translation using
    *Hugging Face Transformers* – specifically, the *MarianMT* suite of language models.
    MarianMT models were first created by Jörg Tiedemann using the Marian C++ library
    for fast training and translation but are now offered through the Hugging Face
    suite of Python libraries.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the resource offers 1,440 transformer encoder-decoder
    models, each with six layers. These models support various language pairs, based
    on the *Helsinki-NLP* framework developed by the Language Technology Research
    Group at the University of Helsinki in Finland.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we want to translate the following three sentences from English
    into Spanish, and then use the same technique to translate the Spanish sentences
    back into English:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The man glanced suspiciously at the door
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peter thought he looked very cool
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most individuals are rather nice
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to generate similar sentences with the same semantic meaning but
    slightly different wording as this synthetic data will give our eventual model
    more data to learn from.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll install the required Python libraries:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we’ll import the `MarianMTModel` and `MarianTokenizer` packages from
    the `transformers` library and define our input text string as `src_text`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, we’ll define our `translator` model using the `Helsinki-NLP/opus-mt-en-es`
    language model, which translates from English into Spanish. The `MarianTokenizer`
    and `MarianMTModel` functions are used to define and execute our tokenizer and
    translation model, respectively.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output is stored as `trans_out`, which is then used as the input
    for our back translation model:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this basic example, we simply repeat the same modeling exercise in reverse
    to produce slightly altered versions of the original input sentences. We use ‘`Helsinki-NLP/opus-mt-es-en`’
    to translate back into English:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following table shows the original input sentences against the model’s
    output. The generated sentences have slightly different wording, but generally,
    they have the same semantic meaning as the originals. To use these sentences in
    a training dataset for a supervised model, they must inherit the labels of their
    original “parent” sentences:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input Sentence** | **Back Translation** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| The man glanced suspiciously at the door | The man looked suspiciously at
    the door |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Peter thought he looked very cool | Peter thought he looked great |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Most individuals are rather nice | Most individuals are quite pleasant |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Examples of back translation using Hugging Face Transformers
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Privacy preservation
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data is also extremely useful for protecting the privacy and identity
    of individuals. The main aim of using synthetic data for privacy preservation
    is to make it impossible to identify individuals in a dataset while still keeping
    the statistical properties of the original dataset (close to) intact.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data is an excellent option for privacy preservation since it allows
    information to be shared without revealing private or sensitive information. To
    achieve this, we must create data that resembles the original but does not contain
    any personally identifiable information.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The use of synthetic data allows organizations to share data for research or
    other purposes without compromising the privacy of individuals. There are several
    benefits to using synthetic data for privacy preservation – for example, you can
    reduce the risk of data breaches since the data contains no personal or sensitive
    information.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy regulations around the world are increasingly making it mandatory
    to protect individuals’ privacy when using consumer data for analytical purposes.
    Synthetic data can be used to comply with privacy regulations, such as GDPR in
    the European Union or the HIPAA privacy rule in the United States, which sets
    standards for protecting personal data and preventing it from being shared without
    consent.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In general, synthetic data is a useful tool for preserving privacy because it
    allows organizations to share data without revealing sensitive or personal information.
    It is particularly useful for managing the trade-off between data quality and
    individual privacy in machine learning, making it an integral part of the data-centric
    toolbox.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, a bank that wants to use sensitive customer data for
    analytical activities such as churn modeling, fraud detection, and credit assessments.
    Using customer data for these activities typically brings about many compliance
    risks and mandated requirements that must be managed to avoid privacy breaches
    and heavy fines from regulators.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: By having pre-generated synthetic datasets at hand, data scientists from various
    parts of the business can quickly and safely build models that would yield similar
    results to models built on real-world data. By using *appropriately constructed*
    synthetic data, the organization avoids going through cumbersome compliance and
    governance processes every time a new model is built and productionized.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: However, this doesn’t mean the use of privacy-preserving data is without its
    risks. It may happen, for example, that the generative model overfits the original
    data and produces synthetic instances too close to the original data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Also, although synthetic data may appear anonymous, there may be instances where
    sophisticated hacks can reveal the identities of individuals. The aim of privacy-preserving
    synthetic data is to limit the risk of this happening.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore some common privacy disclosure scenarios to understand these risks
    and how we might limit them.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Types of privacy disclosure
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further understand and appreciate the usefulness of synthetic data for privacy
    preservation, let’s have a look at three different types of privacy disclosure
    that can occur. This is not an exhaustive list of potential disclosure events,
    but it does help build an understanding of the potential and limitations of using
    synthetic data for this purpose.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct identity disclosure** is the most obvious type of privacy disclosure.
    This is where an external adversary, such as a hacker, tries to gain information
    by matching the identity of an individual to records of private information. An
    example of this could be matching a person’s identity with medical records.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '**Inferential identity disclosure** is a form of data privacy breach where
    certain pieces of personal information can be derived from data that has been
    made publicly available, without explicitly revealing an individual’s identity.
    This type of privacy breach occurs when an attacker uses statistical analysis
    to infer characteristics about an individual by analyzing patterns and correlations
    in a dataset. For example, an attacker may be able to determine the gender of
    an individual from publicly available data by analyzing patterns between particular
    characteristics and the corresponding gender.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Fully synthetic data, by design, makes direct identity disclosure almost impossible.
    However, an attacker could use the analysis of a synthetic dataset to infer information
    about a particular group of people, despite not being able to identify individuals
    in the dataset.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: For example, say an original dataset contains sensitive medical information.
    The synthetic version of this data preserves the same statistical properties as
    the original data. With basic statistical methods or more advanced machine learning
    models, an adversary can identify groups of people with similar characteristics
    and deduce their risk of a certain disease. An adversary could then leverage this
    knowledge to infer the risk of any individual that shares those same characteristics,
    without any direct identity disclosure taking place.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Another example of inferential disclosure is when an attacker can infer someone’s
    financial status or income level based on certain behaviors, such as shopping
    habits or credit card usage patterns. In addition, an attacker may be able to
    determine the medical history of a person by analyzing health insurance claims
    and other related records. This can lead to serious consequences, such as discrimination
    or exploitation of sensitive information. Therefore, organizations must take appropriate
    steps to protect their data from inferential disclosure to ensure that it is not
    used for malicious purposes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '**Membership inference attacks** are similar to inferential disclosure, but
    they are not exactly the same. Rather than inferring personal information about
    an individual based on a group that shares similar characteristics, membership
    inference attacks aim to deduce if an individual who was present in the original
    dataset was used to create the synthetic dataset. This presents a huge privacy
    risk as, for example, it may reveal that someone has a certain illness without
    ever having disclosed their medical information. Preventing these attacks through
    synthetic data is difficult as the statistical properties of the original dataset
    have been maintained.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: In other words, synthetic data is a potent weapon against direct identity disclosure
    but does not remove the risk of identity disclosure entirely. Let’s examine why
    synthetic data is still superior to traditional identity-masking techniques.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Why we need synthetic data for privacy preservation
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional data de-identification techniques rely on two main approaches:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**Anonymization**: This is the simplest form of de-identification and is where
    columns containing direct (customer ID, name, address) and quasi-identifiers (ZIP
    code, birth date) and other sensitive information are removed, hashed, encrypted,
    or masked. Metrics such as k-anonymity, l-diversity, and t-closeness are then
    used to validate the level of privacy preservation in a given dataset.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differential privacy**: An algorithm for differential privacy uses statistical
    distributions such as Gaussian and Laplace to add randomly generated noise to
    the identifying features in a dataset. As a consequence, individuals’ privacy
    will be protected because identifying information is concealed behind the noise.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these techniques lower the risk of individuals being identified directly,
    they aren’t necessarily enough to completely remove it.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: A 2019 study by Rocher et al11 demonstrated that 99.98% of Americans could be
    re-identified with no more than 15 demographic attributes based on a sample size
    of the population of Massachusetts. The authors conclude that “*heavily sampled
    anonymized datasets are unlikely to satisfy the modern standards for anonymization
    set forth by GDPR and seriously challenge the technical and legal adequacy of
    the de-identification* *release-and-forget model.*”
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Another study, this time by Sweeney, 2000,12 found that 87% of the population
    in the US had reported characteristics that likely made them unique based only
    on ZIP code, gender, and date of birth. 53% of the US population is identifiable
    by only location, gender, and date of birth, where “location” is the city, town,
    or municipality where the person lives. 18% of the population are identifiable
    based on a combination of their county, gender, and date of birth.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it is quite possible to identify unique individuals based on
    only a few quasi-identifiers. By using synthetically generated data, we can remove
    these individual combinations from the dataset while preserving the overall statistical
    properties of the data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine how this is done.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic data for privacy preservation
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we create synthetic data for privacy preservation, we have three goals:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the utility of the original data by reflecting its statistical properties
    in the synthetic dataset.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure the data structure is the same as the original data. This means that
    we can use the same code and tools on synthetic data as on the original data,
    without needing to change anything.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should not be possible to tell which real-world individuals were part of
    the original dataset when using privacy-preserving synthetic data.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth noting that there are different ways to create synthetic data. Partial
    synthetic data just replaces some of the data with synthetic data, while fully
    synthetic information is created from scratch, without any of the original data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the approach taken, fully synthetic information can provide a stronger
    guarantee against personal identity breaches, without sacrificing much in terms
    of usability and convenience.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: A great way for you to start practicing synthetic data generation is through
    the tools created by the **Synthetic Data Vault** (**SDV**) project. The project
    was first established by MIT’s Data to AI Lab in 2016 and is a comprehensive ecosystem
    of Python libraries that allows users to learn single-table, multi-table, and
    time series datasets, which can then be used as the basis for generating synthetic
    data that replicates the format and statistical properties of the original data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this project, it’s possible to easily supplement, augment, and – in
    some cases – replace real data with synthetic data when training machine learning
    models. Additionally, it enables machine learning models or other data-dependent
    software systems to be tested without the risk of exposure that comes with sharing
    actual data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The SDV suite is comprised of several probabilistic graphical modeling and deep
    learning-based techniques. They are used to generate hierarchical generative models
    and recursive sampling algorithms, which enable synthetic versions of a variety
    of data structures.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We will use two different techniques from the SDV suite – `GaussianCopula` and
    `CopulaGAN` – to illustrate how to generate synthetic data for privacy preservation
    purposes. Then, we’ll briefly look at how to measure the *quality* and *score*
    using metrics and charts.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: GaussianCopula
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A copula is a tool that’s used to measure the dependence among random variables.
    `GaussianCopula` is a collection of multiple (that is, multivariate) normally
    distributed pieces of data. Taken together as one set, the copula lets us describe
    how these independent normal distributions are related by showing how changes
    in one element in the set affect the others – that is, their *marginal distributions*.
    This is important because this exercise aims to augment any one unique combination
    of variables while preserving the overall statistical properties of the dataset.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Example GaussianCopula Python program
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will write a sample program to show how this works. It will do the
    following:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Load a sample dataset and then calculate the `GaussianCopula` model.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use that model to generate some sample data given the `GaussianCopula` model.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the output and its statistical properties to understand how our model
    is performing.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we must install the necessary Python packages – `sdv` and `pandas`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For this exercise, we will use the publicly available *Adult* dataset, also
    known as the *Census Income* dataset. To get started, download it from [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data](http://ml/machine-learning-databases/adult/adult.data).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use standard `pandas` functions to create a DataFrame, `df`, from the
    preceding URL:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will output the following DataFrame:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – The first five rows of the Adult dataset – our input dataset
    for synthetic data generation](img/B19297_07_12.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The first five rows of the Adult dataset – our input dataset for
    synthetic data generation
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: With our DataFrame created, we will import `SingleTableMetadata`, which is a
    class that provides methods to manage metadata about a single table of data, such
    as the names and types of columns, relationships between columns, and more. SDV’s
    modeling suite needs this metadata object as input.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will use the `detect_from_dataframe()` method to analyze the pandas
    `df` DataFrame and automatically detect and set metadata about the table.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will load the appropriate APIs and objects from SDV and instantiate
    the `GaussianCopula` model. Then, we will use the `fit()` method to generate the
    model:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Normally, we would take a sample of the input dataset that is smaller than the
    full input dataset to generate the model, but in this case, we’ll take the entire
    input data since it isn’t too large.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s generate the synthetic dataset:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once we have generated our synthetic data, we can assess its quality by comparing
    it to the attributes of the real data. This can be done by using several quality
    metrics.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and do that.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Calculating quality scores
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To measure the quality of the synthetic data, we can use various *score* metrics
    from the *SDV* package. The definition and interpretation of the scores vary depending
    on which metric we are looking at.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some relevant metrics for measuring statistical similarity between
    original and synthetic data, as well as the risk of inference attacks being successful.
    Scores range between 0 and 1\. The interpretation of 0 or 1 varies according to
    what metric you are using:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '`BoundaryAdherence`: This describes whether the synthetic data lies within
    the range of the max and min for a column in the real data. 1 means yes and 0
    means no.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StatisticSimilarity`: This compares the mean, median, and standard deviation
    in a column between real and synthetic data.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CategoricalCAP`: This is the risk of disclosing private information using
    an inference attack – that is, a hacker knows some of the real data and can match
    it up with the synthetic. A score of 1 means there is a high risk.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Data Likelihood`: This calculates how likely it is that the data will match
    observations in the original data. This is similar to the `Detection` metric,
    which asks whether the machine learning model can tell which is the original dataset
    and which is the fabricated one.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KSComplement`: This shows whether the column shape of the real and synthetic
    data are the same using the **Kolmogorov-Smirnov** (**K-S**) test. The K-S test
    measures the maximum distance between the **cumulative distribution function**
    (**CDF**) of the two datasets. However, it uses its complement (the 1 - KS statistic).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MissingValueSimilarity`: This measures the proportion of missing data in the
    real and synthetic datasets.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for showing all of these metrics is nearly the same. Simply call the
    appropriate package, then run the `compute` method:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is an example for `MissingValueSimilarity`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output score is equal to 1.0, which means the model has successfully matched
    the proportion of missing values in the synthetic dataset.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying and visualizing data quality
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also want to quantify and visualize the quality of our synthetic data compared
    to the original set. For this purpose, we’ll use the *diagnostic* and *quality*
    reports from the SDV library.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagnostic report should always produce a score of 100%, which tells us
    that primary keys are unique and non-null, continuous values in the synthetic
    data adhere to the min/max range in the original data, discrete values line up
    with the same categories across real and synthetic data, and column names are
    the same:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here’s our output:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The SDV quality report evaluates how well your synthetic data captures the mathematical
    properties of our original real data. It does this through a set of metrics that
    measure various aspects of the *fidelity* between the two datasets. **Data fidelity**
    refers to how accurate a dataset is at representing the features of its source.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The report provides an overview of the results, as well as detailed visualizations
    and explanations for each metric so that you can quickly understand the strengths
    and weaknesses of your synthetic data. By understanding how well your synthetic
    data captures the mathematical properties of the real data, you can take steps
    to improve it if needed.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The SDMetrics quality report is a valuable tool that helps you ensure your
    synthetic data is as accurate and reliable as possible. Here’s how we can use
    it:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code produces a quality report with various metrics and visualizations
    that show the overall similarities between the original and synthetic data:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here are a couple of important metrics to know about:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '`Column Shapes`: A column’s shape tells us how data is distributed. A higher
    score means that the real and synthetic data are more similar. A separate column
    shape score for every column is calculated, but the final score is the average
    of all columns.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Column Pair Trends`: The correlation between two columns indicates how their
    trends compare to each other; the higher the score, the more similar those trends
    are. A score is produced for each column pair in the data, while the final score
    is the average of all columns. This is an important score that tells us whether
    our synthetic data has captured the relationships between variables in the original
    dataset.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also visualize the dimensions of these metrics with the `get_visualization`
    command:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will generate the following plot:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – A correlation matrix comparing column pair trends between the
    original and synthetic datasets](img/B19297_07_13.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – A correlation matrix comparing column pair trends between the
    original and synthetic datasets
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, most column pairs have a high similarity score, but the `capital-gain`
    column is far apart. We can use the following code to visualize the real and synthetic
    `capital-gain` column distributions side by side:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is generated as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.14 – A comparison of the distribution of the real and synthetic\
    \ capital-gain columns. The GaussianCopula model hasn’t done a good job of matching\
    \ the distribution\uFEFF](img/B19297_07_14.jpg)"
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – A comparison of the distribution of the real and synthetic capital-gain
    columns. The GaussianCopula model hasn’t done a good job of matching the distribution
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we would test various column distribution functions to find a
    better match for this particular column. In this example, we used the `GaussianCopula`
    function to create a synthetic dataset. However, the SDV library contains several
    other distributions that can be useful, depending on the characteristics of your
    original dataset. Let’s explore how to change the default distribution.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Varying column distribution functions
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `GaussianCopula` function determines which statistical distribution best
    describes each copula, but it doesn’t always get it right. Luckily, we can override
    the preselection and pick our preferred distribution.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following choices:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian (normal) distribution**: Use this if your data is continuous and
    symmetrically distributed around the mean. It’s often used for naturally occurring
    data, such as the heights or weights of a population.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma distribution**: This is used for positive-only, skewed data. It’s often
    used for things such as wait times or service times.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beta distribution**: This is used for variables that are bounded between
    0 and 1, such as proportions or probabilities.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Student’s t-distribution**: This is similar to the Gaussian distribution
    but has heavier tails. It’s often used when the sample size is small or the standard
    deviation is unknown.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian kernel density estimation** (**KDE**): Use this for non-parametric
    data – that is, when you don’t know or want to assume a specific distribution.
    The KDE uses the data itself to estimate its distribution.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Truncated Gaussian distribution**: Use this when you have data that follows
    a Gaussian distribution but is bounded within a specific range.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, here is how to show the distributions it calculated:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This statement produces a detailed list of all the columns in the dataset. Instead
    of showing the output here, the model defaulted to a beta distribution for all
    columns.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'To change a distribution function for a given column, just create a model again
    but this time explicitly apply a specific distribution to that column. In this
    case, we will apply a gamma distribution to the `capital-gain` column:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The resulting output is a new synthetic dataset with a `capital-gain` column
    distribution much closer to the real data:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – A new comparison of the capital-gain columns. Using the gamma
    distribution on this column improved the similarity between the synthetic and
    original data](img/B19297_07_15.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – A new comparison of the capital-gain columns. Using the gamma
    distribution on this column improved the similarity between the synthetic and
    original data
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Another useful package from the SDV library is `CopulaGAN`. This algorithm is
    a blend of the `GaussianCopula` and `CTGAN` algorithms. Let’s compare the performance
    of `CopulaGAN` to `GaussianCopula` on the *Adult* dataset.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: CopulaGAN code example
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopulaGAN` is a variation of `GaussianCopula` that can yield better results
    using a simplified GAN model. We will compare the two models in this section,
    but first, here is the code to generate `CopulaGAN` using the same input dataset
    and metadata object:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Measuring data quality from CopulaGAN
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let’s look at data quality regarding the `CopulaGAN` model, repeating
    some of the same techniques we used with `GaussianCopula`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*Figure 7**.16* provides a visual representation of the column pair trends
    of the original and synthetic datasets:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Column pair trends for the Adult dataset using CopulaGAN](img/B19297_07_16.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Column pair trends for the Adult dataset using CopulaGAN
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between GaussianCopula and CopulaGAN
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopulaGAN` is a hybrid AI model that combines the human accessibility of Gaussian
    copulas with the robust accuracy of GANs13.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: A GAN is a deep learning algorithm. If you’ve worked with neural networks, you
    know that they are a kind of black box, meaning the coefficients of the nodes
    in the network are functions and not numbers. They are very hard to explain or
    understand compared to, for example, a polynomial or linear model.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianCopula` is easier to explain. It works by trying different known statistical
    distributions (normal, Weibull, and others), which is very useful for known or
    easily observable distributions. Then, for each column, it picks the one that
    matches the closest.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'The team behind the SDV project developed `CopulaGAN` to get the best of both
    worlds: a more accurate model that is still explainable.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table compares the results from our two models in the previous
    examples. `CopulaGAN` achieved a higher overall quality score because it was able
    to match the column shapes more:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **GaussianCopula** | **CopulaGAN** |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| **Overall** **Quality Score** | 84.6% | 87.39% |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| **Column Shapes** | 87.57% | 91.75% |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| **Column** **Pair Trends** | 81.63% | 83.04% |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Data quality comparison of the GaussianCopula and CopulaGAN algorithms
    on the Adult dataset
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Of course, quality is a highly complex topic and our example is not exhaustive
    in that regard. You would have to look at the scores across all columns and all
    the different types of scores to validate the accuracy of the synthetically generated
    data. In other words, you cannot say for definite that `CopulaGAN` is more accurate
    in all cases without doing a deeper review of the variables in the dataset. This
    is particularly important when you are dealing with high-stakes datasets and use
    cases.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: One additional metric to consider is run speed. Anecdotally, when we wrote this
    example, `CopulaGAN` took 1 hour to complete, while `GaussianCopula` took 15 seconds
    to complete.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Validating the privacy of our new dataset
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have constructed a synthetic dataset for our use case, we need to
    ensure we have prevented the ability to re-identify individuals from the original
    dataset.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: To know the likelihood that individuals can be re-identified, we need an accurate
    measure of the difference or “distance” between the original and synthetic records.
    The farther the two are apart, the less probable that they can be identified as
    one entity. If we are discussing personal information in tabular form, we need
    a methodology for measuring the distance between qualitative and quantitative
    attributes alike.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: To accurately measure the closeness of two rows within a dataset containing
    both qualitative and quantitative information, we can utilize a similarity coefficient
    called Gower’s distance.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance is a unique type of distance measure that differs from distance
    measures. It stands out in terms of its ability to calculate the difference between
    two entities with both numerical and categorical values. This is important because
    many common clustering algorithms, such as K-means clustering, only work when
    all of the variables are numeric.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance returns a similarity coefficient between 0 (indicating identical
    observations) and 1 (showing that they are at the maximum distance).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a set of *p* features in the original (*o*) and synthetic (*s*)
    datasets:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: For ordinal numbers, the distance from one feature to the other is simply the
    absolute value of their difference divided by the range of that variable. We divide
    by the range to normalize the data so that large numbers won’t be given greater
    weight than small ones.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variables are turned into numbers so that we can do math. The formula
    is simple – if those values are the same, their distance is 0; otherwise, it is
    1.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gower’s distance is the sum of the distances divided by the number of features
    – the average of the terms. Since we divide these differences by the number of
    features, this is the same as saying Gower’s distance is the average distance.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Then, we make a definition for **closeness** and call it the **distance to the
    closest record**. For every element in *s*, the closest row in *o* is the one
    with the minimum Gower’s distance. A distance of 0 means that two rows of data
    are the same, while a distance of 1 means that two rows are as different as possible
    given the observations in the dataset we’re using.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Let’s practice applying Gower’s distance using the `Gower` Python package.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance Python example
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our Gower’s distance practice example, we’ll use the *Adult* dataset and
    compare the synthetic output generated by `CopulaGAN` to the original data. We
    recommend using a small subset of the *Adult* dataset (for example, 1,000 rows)
    to practice as Gower’s matrix calculation can take a long time to run on larger
    sets.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create the DataFrame for our model based on the top 1,000 rows
    from the existing `df` DataFrame, which contains the full *Adult* dataset. Then,
    we’ll fit a `GaussianCopula` model on this dataset and generate a new synthetic
    dataset called `synthetic`:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we’ll install the `Gower` package and calculate the Gower’s distance
    matrix between our two datasets:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This will generate results similar to the following, where the distance between
    each row in the dataset is calculated:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The resulting gowerMatrix](img/B19297_07_17.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The resulting gowerMatrix
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll use the `gower_topn()` function to find the top *n* (in this case,
    10) closest (that is, most similar) rows.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the synthetic dataset passes our test, we must make sure none of
    its values are equal to 0; otherwise, this would indicate that some rows in the
    synthetic data resemble those from the original. Generally speaking, we want the
    top values to be sufficiently distanced from 0 as this reduces the risk of reidentification:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The result is the index of the top 10 closest rows and their Gower’s distance.
    In this case, the smallest distance between two rows in our datasets is 0.02205,
    which means our synthetic dataset is not sufficiently different from the original
    at the individual row level:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In this case, we would have more work to do to reduce the similarity between
    sets. Here are a few techniques you could use to achieve this:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianCopula` or test other synthesizers in the SDV catalog, such as CTGAN
    or `CopulaGAN`.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add noise**: You can add random noise to the synthetic data. This will make
    the synthetic data more “unique” compared to the original data.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform feature transformation**: Apply some kind of transformation (for
    example, logarithmic, square root, exponential, and so on) to the features in
    the synthetic dataset.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform data augmentation**: Generate new synthetic data points that are
    not direct copies of the real data. You can do this by using techniques such as
    the **Synthetic Minority Over-Sampling Technique** (**SMOTE**) or **Adaptive Synthetic
    Sampling** (**ADASYN**), both of which we’ll discuss later in this chapter.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that while your goal is to reduce similarity, you also want the synthetic
    data to be useful and representative of the real data. If you make the synthetic
    data too dissimilar, it may not serve its intended purpose.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, Gower’s distance can also be used to find rows of data that
    are very similar to each other, which is useful for tasks such as creating lookalike
    audiences, clustering, or identifying at-risk populations.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: For instance, imagine that you have just run a very successful email marketing
    campaign to a group of customers and you want to expand the campaign to customers
    who look like the ones in the original target group.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: To do this, simply calculate Gower’s distance between customers in the original
    target group and the rest of your customer base, and pick a target group based
    on the lowest Gower’s distances.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Using synthetic data to improve model performance
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070)*, Techniques for Data Cleaning*
    and [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for Programmatic
    Labeling in Machine Learning*, we dealt with improving model performance by refining
    data quality. However, there are times when improving data quality may not be
    enough, especially when datasets are small. In such situations, we can take advantage
    of generating synthetic data to boost model performance.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: As we covered previously in this chapter, synthetic data can help with generating
    more training examples, as well as generalizing the performance of the model by
    providing more examples of different variations and distributions of the data.
    Both of these uses can make the model more robust and less likely to overfit the
    training data.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: With imbalanced datasets, a model gets biased toward the majority class as there
    are more examples of one class over another. This is the problem with the loan
    prediction dataset, where 30% of the data belongs to the minority class.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover generating synthetic data for the minority class
    so that the model can generalize further and model performance metrics can improve.
    We will stick with a decision tree model and use synthetic data generation to
    further improve the signal strength of the data. We will use the `imblearn` library
    from Python to generate synthetic data.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the library and two oversampling methods, `SMOTE` and `ADASYN`,
    to oversample the minority class. We will also leverage the `Counter` method to
    count data samples pre- and post-synthetic data generation:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Both the SMOTE and ADASYN algorithms are used to generate synthetic data. However,
    ADASYN is more robust as it considers the density of points to generate synthetic
    data. SMOTE may generate synthetic data around the minority class, but it does
    so uniformly without considering how rare a data point is.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE creates synthetic samples by randomly selecting pairs of minority-class
    samples and interpolating new samples around the existing samples. This technique
    spreads out further into the space to increase the number of minority class samples.
    However, as the samples are chosen randomly, no weighting is given to rare sample
    points.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ADASYN considers rare data points in the feature space by
    computing the density distribution of the minority class samples. It generates
    synthetic samples in regions of the feature space where the density is low to
    ensure that synthetic samples are generated where they are most needed to balance
    the dataset. ADASYN uses the k-nearest neighbors algorithm to estimate the density
    distribution of the minority class samples. For each minority class sample, ADASYN
    computes the density based on the number of k-nearest neighbors that belong to
    the minority class. The value of k is a user-defined parameter, typically set
    to a small value such as 5 to 10\. The density is the average distance from k
    nearest points. A higher average distance means lower density, and vice versa.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: We iterate over different thresholds of the algorithm parameters, such as the
    number of nearest neighbors and the percentage of data to oversample. This helps
    us find the best parameters to generate the optimal number of samples so that
    both the test ROC and test accuracy get the maximum boost. Then, we combine those
    results in a DataFrame and choose the best parameters. This is done to measure
    the performance of the model that is using synthetic data generation.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will only use ADASYN, but we encourage you to try different
    techniques, including SMOTE, for the problem at hand:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we must generate a DataFrame that contains our generated combinations
    of model parameters and model performance metrics and sort it by test accuracy.
    The DataFrame indicates that an oversampling strategy with a ratio of 75% for
    the minority to majority class, and a nearest neighbors value of 7, will provide
    the best accuracy and ROC score:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Output DataFrame](img/B19297_07_18.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Output DataFrame
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must apply the parameters from our highest-performing oversampling
    strategy and retrain the decision tree model:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ADASYN increased the number of minority class samples from 173 to 321 using
    synthetic data generation, which boosted the test accuracy to 83.8%. This is an
    almost 2% increase in accuracy. The ROC score was also boosted to 86.2%, which
    is a further increase of 4.4%.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: These results demonstrate that synthetic data generation can provide significant
    gains in model performance, even for small datasets. However, it is important
    to note that this may not always be the case, especially if error analysis suggests
    that adding new data doesn’t contribute to an improvement in model performance.
    In such cases, you may turn to collecting more data or features, or even performing
    feature engineering, before moving on to synthetic data generation.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: When should you use synthetic data?
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve established that synthetic data can be used for several purposes,
    but how do you decide whether to use synthetic data for your project or not?
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: For businesses seeking to gain an edge over their competitors through innovative
    or unconventional approaches, synthetic data provides an accessible middle ground
    between experimentation and reality. For governmental organizations wanting to
    learn from their vast stores of population data, synthetic data allows highly
    sensitive datasets to be analyzed without compromising individual privacy.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation and exploring the boundaries of your data (synthetic or real)
    can be incredibly valuable, but the benefit of introducing synthetic data should
    always be assessed against the cost and risk of making damaging predictions with
    that same data.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: The central question is, “*What is the acceptable cost of an experiment?*,”
    especially if it includes human collateral damage or reputational or financial
    loss.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: In our opinion, synthetic data should be used when obtaining real-world data
    may be difficult, expensive, or unethical. The most common and practical use cases
    for synthetic data are for preserving the privacy of individuals and for creating
    simulations that are very difficult or impossible in traditional test environments.
    For these use cases, the benefits are more likely to outweigh the risks of using
    synthetic data, but that is not a guarantee, so make sure you manage risks appropriately.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: The main risks to mitigate are perpetuation and exacerbation of bias. Machine
    learning models are inherently prone to overfitting and finding the “easiest”
    path through the data, so synthetic datasets should be rigorously tested to ensure
    they are fit for purpose.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data can also accelerate the process of testing and training machine
    learning models, saving companies time and money in their development and deployment
    cycles. Furthermore, synthetic data is a useful tool for creating simulations
    that are not possible in traditional test environments.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that using synthetic data is typically just one of many avenues
    to take when building models or improving the accuracy of your predictions. It
    should only be used when the potential risk and effect on those impacted is understood
    and managed appropriately. On the other hand, if you can mitigate this risk –
    or in some cases, avoid “real-world” risks altogether – then it is a wonderful
    tool to have in your toolkit.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided a primer on synthetic data and its common uses.
    Synthetic data is a key part of the data-centric toolkit because it gives us yet
    another avenue to much better input data, especially when collecting new data
    is not feasible.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a clear understanding of the fundamentals of synthetic
    data and its potential applications. Synthetic data is often used for computer
    vision, natural language processing, and privacy protection applications. However,
    the potential of synthetic data goes well beyond these three realms.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Whole books have been dedicated to the topic of synthetic data and we recommend
    that you dive deeper into the subject if you want to become a true expert in synthetic
    data generation.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll explore another powerful technique for improving
    your data without the need for collecting new data: programmatic labeling.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://datagen.tech/guides/synthetic-data/synthetic-data](https://datagen.tech/guides/synthetic-data/synthetic-data),
    viewed on 12 November 2022'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/](https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/)'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://unity.com/our-company](https://unity.com/our-company), viewed on 15
    November 2022'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: https://venturebeat.com/ai/unitys-danny-lange-explains-why-synthetic-data-is-better-than-the-real-thing-at-transform-2021-2/,
    viewed on 15 November 2022
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alcorn, M A et al 2019, *Strike (with) a Pose: Neural Networks Are Easily Fooled
    by Strange Poses of Familiar Objects*, viewed 13 November 2022: [https://arxiv.org/pdf/1811.11553.pdf](http://pdf/1811.11553.pdf)'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.tesla.com/VehicleSafetyReport](https://www.tesla.com/VehicleSafetyReport),
    viewed 13 November 2022'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Karras T, Aila T, Laine S, Lethtinen J, 2017, *Progressive Growing of GANs
    for Improved Quality, Stability, and* *Variation*: [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Karras T, Aila T, Laine S 2018, *A Style-Based Generator Architecture for Generative
    Adversarial* *Networks*: [https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Metz L, Poole B, Pfau D, Sohl-Dickstein J 2017, *Unrolled Generative Adversarial
    Networks*, ICLR 2017: [https://arxiv.org/pdf/1611.02163.pdf](https://arxiv.org/pdf/1611.02163.pdf)'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jain N, Olmo A, Sengupta S, Manikonda L, Kambhampati S, 2021, *Imperfect ImaGANation:
    Implications of GANs Exacerbating Biases on Facial Data*, ICLR 2021 Workshop on
    Synthetic Data Generation – Quality, Privacy, Bias: [https://arxiv.org/pdf/2001.09528.pdf](https://arxiv.org/pdf/2001.09528.pdf
    )'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rocher L, Hendrickx J M, de Montjoye Y A 2019, *Estimating the success of re-identifications
    in incomplete datasets using generative* *models*: [https://www.nature.com/articles/s41467-019-10933-3](https://www.nature.com/articles/s41467-019-10933-3
    )'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'L. Sweeney, *Simple Demographics Often Identify People Uniquely, Carnegie Mellon
    University*, Data Privacy Working Paper 3\. Pittsburgh 2000: [https://dataprivacylab.org/projects/identifiability/paper1.pdf](https://dataprivacylab.org/projects/identifiability/paper1.pdf)'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://mobile.twitter.com/sdv_dev/status/1519747462088507393](https://mobile.twitter.com/sdv_dev/status/1519747462088507393),
    viewed on 25 January 2023'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
