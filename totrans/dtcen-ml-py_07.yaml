- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Synthetic Data in Data-Centric Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we discussed various approaches to improving data quality
    for machine learning purposes through better collection and labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Although human labelers, data ownership, and technical data quality improvement
    practices are critical to data centricity, there are limits to the kind of labeling
    and data creation that can be performed by individuals or through empirical observation.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data has the potential to fill in these gaps and produce comprehensive
    training data at a fraction of the cost and time of other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter provides an introduction to synthetic data generation. We will
    cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What synthetic data is and why it matters for data centricity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How synthetic data is being used to generate better models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common techniques used to generate synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The risks and challenges with synthetic data use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by defining what synthetic data is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthetic data is artificially created data that, if done right, contains all
    the characteristics of production data.
  prefs: []
  type: TYPE_NORMAL
- en: The reason it’s called synthetic data is that it doesn’t have a physical existence
    – that is, it doesn’t come from real-life observations or experiments that we
    create to gather data that we subsequently use to run analysis or build machine
    learning models on.
  prefs: []
  type: TYPE_NORMAL
- en: A foundational principle of machine learning is that you need a lot of data,
    ranging from thousands to billions of observations. The amount you need depends
    on your model.
  prefs: []
  type: TYPE_NORMAL
- en: As we have outlined many times already, when the required volume of data is
    difficult to come by, one approach is to improve the signal in your data to make
    it possible to produce accurate and relevant outputs, even on smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to create synthetic data to cover the gaps. A major benefit
    of synthetic data is its scalability. Real training data is collected linearly,
    one example at a time, which can be both time-consuming and expensive.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, synthetic data can be generated in very large quantities in a relatively
    short time and typically at a lower cost. As an example, a training image that
    may cost $5 if it’s obtained from a labeling service might cost $0.05 if it’s
    produced artificially1.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data is touted as the answer to many challenges in the development
    of more powerful machine learning and AI solutions. From solving privacy issues
    to inexpensively generating rare but important observations for your modeling
    and training data, synthetic data can fill the gaps where real-world data falls
    short.
  prefs: []
  type: TYPE_NORMAL
- en: According to Gartner predictions2, 60% of the data used in AI and analytics
    projects will be synthetically generated rather than gathered through real-world
    observations by 2024.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, the use of data for analytical purposes has been driven by the
    data we have available and its limitations. We might imagine the perfect data
    solution, but often, the depth, breadth, reliability, and privacy constraints
    of a dataset limit what we can do in reality. To a large extent, this is what
    synthetic data aims to fix.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to create synthetic data, and to some extent, the technical
    creation of the data is the least complex part. Validating whether a synthetic
    dataset is a relevant reflection of potential real-world scenarios and defending
    against unwanted bias can be time-consuming and challenging.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to use synthetic data for your next project, the first important
    question is always, “*What are you going to use the data for?*” The answer to
    this question determines your data needs, which, in turn, will highlight your
    data gaps.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the typical reasons for using synthetic data and
    explore some common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The use case for synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The reasons for using synthetic data generally fall into the following four
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability**: Synthetic data creation is used to compensate for the lack
    of data in a domain. It may be that we have imbalanced classes in a dataset compared
    to the real-life distribution, so to make those classes balanced, we create synthetic
    data to compensate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: It can be very costly and time-consuming to collect certain types
    of data, in which case it can be useful to generate synthetic data to reduce the
    time and cost spent on a project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk management**: In some cases, synthetic data can also be used to lower
    the risk of human or financial damage. An example of this is flight simulators,
    which are used to train new and experienced pilots in all sorts of situations.
    Training pilots in a simulated environment allows us to safely and knowingly introduce
    rare events that would be hard to create in a natural environment without unacceptable
    risk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and legal compliance**: The data you need may already exist but
    it is unsafe or illegal to use it for machine learning purposes. For example,
    some regulations, such as Europe’s **General Data Protection Regulation** (**GDPR**),
    forbid the use of certain kinds of data without clear consent from the underlying
    individual. Alternatively, it might just be too slow and cumbersome to get signoff
    in your organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some examples of common and potential use cases for synthetic data:'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision and image and video processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy preservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correcting bias (discussed in [*Chapter 8*](B19297_08.xhtml#_idTextAnchor125)*,
    Techniques for Identifying and* *Removing Bias*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving data quality or gaps (more cheaply)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing modeling data volumes for rare events (discussed in [*Chapter 9*](B19297_09.xhtml#_idTextAnchor141)*,
    Dealing with Edge Cases and Rare Events in* *Machine Learning*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore some of these topics in this chapter to illustrate how synthetic
    data can be used as part of your model development strategy.
  prefs: []
  type: TYPE_NORMAL
- en: To set the scene, let’s look at an example of just how powerful synthetic data
    can be in the right setting, courtesy of the world’s leading computer games software
    development company, Unity Technologies. By way of background, the Unity platform
    was used to create 72% of the top 1,000 mobile phone games and 50% of all computer
    games across mobile, PC, and consoles in 20213.
  prefs: []
  type: TYPE_NORMAL
- en: The users of Unity’s technology have improved object recognition rates from
    70% to almost 100% simply by augmenting real-world data with synthetic data. Synthetic
    data adds a lot more variety and many more scenarios to the training data, which
    enables objects to be recognized from many angles4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity’s Vice President of AI and machine learning, David Lange, says the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “*We’re using the Unity engine to recreate three-dimensional worlds with objects
    in there. Then, we can generate synthetic images that look very much like what
    they would look like in the real world,* *perfectly labeled.*
  prefs: []
  type: TYPE_NORMAL
- en: “*Real-world data is really just a snapshot of the situation. What you can do
    with the synthetic data is augment that real world with special use cases, special
    situations, special events. You can improve the diversity of your data by adding
    synthetic data to* *your dataset.*
  prefs: []
  type: TYPE_NORMAL
- en: “*We can create improbable situations because it’s not going to cost us anything
    in milliseconds, rather than trying to stage them in reality. The ease with which
    you can create all these scenarios is driving the use of* *synthetic data.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'David Lange shares the view of Gartner in that synthetic data is going to be
    the predominant raw material for machine learning in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: “*I believe that the vast majority of training data will be synthetic. You have
    to have the real world as a baseline, but synthetic data eliminates privacy concerns
    because there are no real people involved. You can eliminate bias. You can do
    your data analytics and ensure that your data represents the real world in a very
    even way, better than the real* *world does.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'Take note of the benefits of synthetic data that David Lange mentions:'
  prefs: []
  type: TYPE_NORMAL
- en: Objects can be perfectly labeled, thereby avoiding the labeling ambiguities
    discussed in previous chapters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diversity of the dataset can be increased substantially to cover slight
    variations in probable scenarios, as well as rare events and edge cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets can be scaled quickly and cheaply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and privacy concerns can be reduced because data is cleaner and depersonalized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dig deeper into the various uses of synthetic data to understand the possibilities,
    benefits, risks, and constraints associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data for computer vision and image and video processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, the most prevalent use of synthetic data is in computer
    vision problems. This is because we can often create this type of data with limited
    risk, while outliers (often rare but impactful events) can be particularly hard
    to get hold of in image data.
  prefs: []
  type: TYPE_NORMAL
- en: A common challenge in computer vision (and most other machine learning problems
    for that matter) is that real-world data typically contains a large proportion
    of observations describing the most probable scenarios, and very few or no examples
    of rare events.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, real-world data can be difficult, expensive, or outright dangerous
    to collect. As an example, autonomous vehicle models can’t be trained to avoid
    car crashes by putting real cars into dangerous situations. Instead, these crashes
    and other rare but significant events must be simulated.
  prefs: []
  type: TYPE_NORMAL
- en: A common problem for image classification algorithms is recognizing familiar
    objects in slightly unfamiliar positions or environments. Because machine learning
    algorithms don’t reason by logic or abstraction, even models that perform very
    well on both training and test datasets will often fail to generalize to out-of-distribution
    observations. This is true whether these observations are introduced as an adversarial
    test of model performance or occur naturally.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* provides a simplified example of this phenomenon. A square that
    is rotated 45 degrees may be interpreted by some – humans and algorithms alike
    – as a diamond, and not simply a tilted square with the same dimensions as a square
    positioned on its side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – These two squares are either identical or different, depending
    on the rules we use to interpret them](img/B19297_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – These two squares are either identical or different, depending
    on the rules we use to interpret them
  prefs: []
  type: TYPE_NORMAL
- en: The implications of this bias are often substantial. In an analysis of deep
    neural networks’ performance on images from the ImageNet dataset, Alcorn et al.
    (2019)5 describe how the common image classifiers *Google Inception-v3*, *AlexNet*,
    and *ResNet-50* can easily be fooled by slight changes to the positioning of an
    object within an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7**.2*, the images in column (d) are real photographs collected
    from the internet, whereas columns (a) to (c) are out-of-distribution images of
    the same objects in unusual positions. The main object within the image is flipped
    and rotated, but the background is kept constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Deep neural networks can easily be fooled when familiar objects
    are in uncommon positions. Column (d) represents real-life images, while columns
    (a) to (c) are synthetic](img/B19297_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Deep neural networks can easily be fooled when familiar objects
    are in uncommon positions. Column (d) represents real-life images, while columns
    (a) to (c) are synthetic
  prefs: []
  type: TYPE_NORMAL
- en: The authors then used *Inception-v3* to classify these images, with the resulting
    label and confidence score depicted under each image. In these examples, the algorithm
    was able to classify with a high degree of accuracy and confidence when objects
    were in a commonly observed position in real-life scenarios. However, the algorithm
    misclassified images with a high degree of confidence when objects were being
    flipped, rotated, or moved very close.
  prefs: []
  type: TYPE_NORMAL
- en: Not only did the algorithm misclassify objects, but it also confidently mislabeled
    them as objects they are not. A rolling bus and a punching bag are like chalk
    and cheese, and an overturned scooter is nowhere close to looking like a parachute.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to recognize familiar objects in unfamiliar positions is especially
    critical when it comes to observing and classifying moving objects. In the example
    of self-driving cars, algorithmic misinterpretations introduce novel and unexpected
    events into the traffic environment, even though these vehicles are statistically
    safer than human drivers6.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image, which was captured from a traffic camera in Taiwan, shows
    an example of this issue. A truck has overturned on a busy highway, and an autonomous
    Tesla sedan doesn’t recognize the truck as an obstacle in the way and crashes
    into the truck at high speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – An autonomous vehicle crashes into an overturned truck at high
    speed. Algorithms can be trained to handle novel situations like this using synthetic
    data](img/B19297_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – An autonomous vehicle crashes into an overturned truck at high
    speed. Algorithms can be trained to handle novel situations like this using synthetic
    data
  prefs: []
  type: TYPE_NORMAL
- en: In this very unlikely but highly dangerous scenario, the car’s algorithms are
    not equipped to correctly assess the statistical probability that the object in
    front of it is blocking the road.
  prefs: []
  type: TYPE_NORMAL
- en: This is a scenario where synthetic data proves highly valuable. As we have just
    learned, even best-in-class computer vision algorithms have a high degree of sensitivity
    to variations in the position of common objects. Therefore, objects must be introduced
    in various positions and lighting conditions in the training data to cover all
    possible combinations, especially highly improbable ones.
  prefs: []
  type: TYPE_NORMAL
- en: When we use machine learning to figure out what’s happening in an image, we
    are extracting the concave and convex curves within the image, also known as the
    features within a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: To create these curves from a synthetic data perspective, you would simply be
    recreating those formations within images. This would typically involve flipping,
    rotating, zooming, cropping, making light changes, and resizing images to create
    slight variations on the same scenario.
  prefs: []
  type: TYPE_NORMAL
- en: In the example of the Tesla accident, we might create images with the truck
    driving as normal, rolled on its side, rolled on its back, driving in the wrong
    direction, in the dark, partially covered by other objects, and so on. These scenarios
    are hard to get a hold of in real-life imagery, yet they’re very important to
    be able to deal with when the situation arises.
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic data using generative adversarial networks (GANs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GANs are common tools for generating synthetic image data with properties similar
    to real-world data. GANs were invented by computer scientist Ian Goodfellow in
    2014 and have since led to an explosion in generative models that can create all
    sorts of content, including text, art, video, and images.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are a form of unsupervised learning where a generator model is pitted against
    a discriminator model (hence the “adversarial” aspect). Both models are neural
    networks that compete against each other to turn the exercise into a “pseudo-supervised”
    learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: The generator identifies patterns in the original dataset that are then used
    to generate synthetic output that could have conceivably existed in the input
    data. The generated examples become negative training samples for the discriminator
    model.
  prefs: []
  type: TYPE_NORMAL
- en: It is the discriminator’s job to classify the newly generated data as fake or
    real. This zero-sum contest continues until the discriminator picks *fake* observations
    as *real* close to 50% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the training process for a GAN can be thought of as minimizing
    a loss function that measures the difference between the generated examples and
    the real examples. This loss function is typically a combination of two terms:
    one that measures how well the generative model can produce examples that are
    similar to the real examples, and one that measures how well the discriminative
    model can distinguish between real and generated examples.'
  prefs: []
  type: TYPE_NORMAL
- en: By training the two parts of the GAN in this way, the generative model can learn
    the patterns and features of the real examples in the training dataset, and then
    use that information to generate new examples that are similar to the real ones.
    This allows GANs to be used for a wide range of applications, including image
    generation, text generation, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.4* provides a conceptual illustration of how GANs iterate through
    a large number of mini-contests to arrive at a model that can generate very realistic
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – A conceptual illustration of how a GAN works](img/B19297_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – A conceptual illustration of how a GAN works
  prefs: []
  type: TYPE_NORMAL
- en: The generator starts with some very basic presentations of the desired output
    but gets better at fooling the discriminator as it iterates through many examples.
    Training is completed when the discriminator struggles to recognize real from
    fake.
  prefs: []
  type: TYPE_NORMAL
- en: The progressive growth of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you can imagine, the first few examples that are created by the generator
    will be relatively easy for the discriminator to pick. There is a lot for the
    generator model to learn and it can be challenging to make the GAN follow a learning
    path we would like it to take.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are inherently unstable models, especially when it comes to generating
    complex structures, such as images. To fool the discriminator, the generator must
    pick up the small details and larger structures of an image, which can be difficult
    on high-resolution images. If the generator can’t do this, it will get stuck in
    a no-man’s land of never fooling the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge is that large images require lots of computer memory. As a
    result, the batch size (the number of images used to update model weights each
    training iteration) must often be reduced to make sure the images will fit into
    memory. Again, this makes the training process less stable.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to these problems is to progressively increase the detail and complexity
    of the model’s input and output. Progressive growing was first proposed by NVIDIA
    researchers Karras et al.7 in 2017, and is a technique for training GANs that
    allows the model to gradually increase the resolution of the generated images
    over many iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Under progressive growing, the model is trained using a step-wise approach.
    First, the generator and discriminator models are trained with low-resolution
    images and seek to improve image quality by changing their parameters to optimize
    loss functions. Then, the resolution of the generated images is increased while
    fine-tuning occurs based on the understanding gathered from the initial training
    stage until the desired resolution is reached. In other words, the model learns
    in steps, rather than all at once.
  prefs: []
  type: TYPE_NORMAL
- en: Karras et al. propose training both the generator and discriminator with a batch
    of low-resolution images of 4x4 pixels. Then, a new sampling layer is used to
    gradually grow the image complexity to 8x8, using nearest neighbor interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: New network layers are introduced gradually to create minimal disruption between
    resolution layers. This approach allows for the smooth and seamless integration
    of newer components into the existing infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The gradual phasing in of a new block of layers is done by adding higher-resolution
    inputs to the existing input or output layer. The relative influence of the new
    outputs is controlled using a weighting, α, where the weight of the original output
    is 1 - α . As α increases, the old layer is gradually faded out, while the new
    layer takes over.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process continues until the desired image resolution is reached. *Figure
    7**.5*, from Kerras et al., highlights the process of progressively growing from
    4x4 to 1,024x1,024-pixel images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Visualization of the progressive growth of a GAN from Kerras
    et al. (2017)](img/B19297_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Visualization of the progressive growth of a GAN from Kerras et
    al. (2017)
  prefs: []
  type: TYPE_NORMAL
- en: This means that the model can first learn about the big picture of the image,
    and then focus on smaller details. This typically yields better results than trying
    to learn everything at once. By leveraging this approach, GANs can grasp the essential
    architecture and characteristics of low-resolution datasets, thus creating higher-quality
    images with greater precision.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving greater accuracy with StyleGANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The research team at NVIDIA built on their progressive GAN architecture to introduce
    the first StyleGAN in December 20188\. Since then, StyleGAN-2 and StyleGAN-3 architectures
    have been released. These incremental upgrades resolved some systemic issues in
    the output from the original StyleGAN, but are otherwise similar in structure.
  prefs: []
  type: TYPE_NORMAL
- en: The primary innovation of StyleGANs is the ability to control the *style* of
    the output created by the generator model. The new architecture allows the generator
    to automatically separate broader features from stochastic/random features in
    an image. Examples of broad features are a person’s pose and identity; hair and
    freckles are considered stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: Before the introduction of StyleGANs, the inner workings of image generators
    were partially a mystery and therefore hard to control. With no effective method
    to compare different images produced by various models and a limited understanding
    of how features originated, the original GAN generators were black boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a comparison between the **progressive GAN** (**ProGAN**)
    and StyleGAN architectures in *Figure 7**.6* to understand why StyleGAN has been
    so successful in generating highly realistic synthetic images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Comparison between ProGAN (a) and StyleGAN (b) from Kerras et
    al. (2018)](img/B19297_07_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Comparison between ProGAN (a) and StyleGAN (b) from Kerras et al.
    (2018)
  prefs: []
  type: TYPE_NORMAL
- en: While ProGANs use a progressive training methodology to grow the resolution
    of generated images layer by layer, StyleGAN gives users more control over the
    generated images via the use of a Mapping Network and Synthesis Network.
  prefs: []
  type: TYPE_NORMAL
- en: StyleGAN’s Mapping Network is a type of neural network that maps a low-dimensional
    vector, *z*, to an intermediate latent space, *w*. This is known as disentangled
    representation learning. By disentangling the features in these two spaces, users
    can more easily control the different aspects of the generated image, such as
    its physiology, hairstyle, or clothing. In simpler terms, it allows for separated
    control over high-level features of the image rather than specific pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: The Synthesis Network is a deep convolutional neural network that works by receiving
    style vectors, *w*, as input and returns an output image. In synthesizing a realistic
    image, features are pulled from the feature vector and applied to the image layer
    by layer, beginning with the lowest layer and progressing one layer at a time
    to higher resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: The Synthesis Network interacts with a learned **adaptive instance normalization**
    (**AdaIN**) module that rescales image features to increase diversity in image
    outputs. The module accepts the feature vector and a style vector as inputs and
    adjusts image features’ scaling and bias by subtracting the feature map’s mean
    and dividing it by the standard deviation. As a result, StyleGAN can produce highly
    detailed images by focusing on specific features such as hairstyle or eye color.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although GANs are a wonderful addition to the machine learning toolbox, they
    are not without their challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs are based on zero-sum game theory. Essentially, if one player triumphs,
    then the other will be defeated. This kind of situation is also known as minimax:
    your opponent looks to maximize their output while you seek to minimize it. The
    theory behind GANs states that the game between the generator and discriminator
    models will continue until a Nash Equilibrium is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: The Nash Equilibrium is an important solution concept within economics, politics,
    and evolutionary biology that can be seen in a wide variety of real-world scenarios.
    A Nash Equilibrium is a situation where no player has an incentive to do something
    different than what they are already doing. This is because they have considered
    what everyone else is doing and they think that their current strategy is the
    best possible option.
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, all players are said to be at equilibrium as they have no
    incentive to change their behavior because any changes made by one player will
    likely lead to a worse outcome for that particular player. Therefore, it is in
    each individual’s best interest not to make any sudden changes in this type of
    equilibrium situation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where competing firms are trying to set prices
    for their products or services. If each firm sets its price too high, it may lose
    customers to its competitors. However, if each firm sets its price too low, it
    will not be able to cover its costs and make a profit. Thus, the Nash Equilibrium
    for this situation is for each firm to set their prices at a level that is low
    enough to deter customers from buying from their competitors without being so
    low that they are unable to make a profit.
  prefs: []
  type: TYPE_NORMAL
- en: Although this theory can work, it is often difficult to achieve in practice.
    There is no guarantee that cost functions will *converge* and find a Nash Equilibrium.
    In this situation, the game continues indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, when one agent outmatches the other in terms of power and efficacy,
    the learning signal for their counterpart becomes useless; consequently, no knowledge
    is gained by either side. The most common scenario is that the discriminator becomes
    so good at picking the generator’s faults that the generator never learns how
    to advance in the game.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main challenges with GANs is called mode collapse. Like any other
    statistical model, GANs tend to find the easiest way through the underlying data,
    which can lead to the overrepresentation of modal observations.
  prefs: []
  type: TYPE_NORMAL
- en: Mode collapse is another common issue that occurs when the generator produces
    an especially plausible output, which causes it to only produce that output. Once
    this happens, the discriminator is more likely to fall into a local minimum, unable
    to find a better output that it deems valid.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the generator is driven to tailor its outputs toward the criteria
    used by this static discriminator rather than attempting to create realistic or
    dynamic outputs. As such, generators tend to “over-optimize” for a particular
    outcome, as determined by their single discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: An example of mode collapse is illustrated in *Figure 7**.7* and is from Metz
    et al. (2017)9\. In this example, the researchers used the MNIST dataset of handwritten
    digits to train two different GANs. The MNIST dataset contains 10 different modes
    that represent digits 0-9.
  prefs: []
  type: TYPE_NORMAL
- en: 'The top four quadrants of numbers have been successfully generated (using an
    unrolled GAN training method) to look like real handwritten digits with a representation
    of all possible digits. The bottom four quadrants, on the other hand (generated
    using the original GAN architecture), have suffered from mode collapse early on
    in the process and produce only representations of the number 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Two different GAN architectures trained on the MNIST dataset
    from Metz et al. (2017)](img/B19297_07_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Two different GAN architectures trained on the MNIST dataset from
    Metz et al. (2017)
  prefs: []
  type: TYPE_NORMAL
- en: Since the development of the original GAN architecture in 2014, several new
    GAN variants have been introduced to deal with mode collapse. As a result, this
    is now a less common issue, but it’s something to always watch out for.
  prefs: []
  type: TYPE_NORMAL
- en: The mode collapse in the MNIST example is relatively easy to spot, but it is
    important to note that GANs may potentially preserve and exacerbate existing biases
    in more subtle ways.
  prefs: []
  type: TYPE_NORMAL
- en: In a 2021 study (Jain et at, 2021)10, researchers from Arizona State University
    and Rensselaer Polytechnic Institute assessed the performance of GANs in generating
    synthetic facial data. The researchers wanted to test whether GANs would exacerbate
    the modal facial characteristics from a (naturally) biased input dataset, thus
    increasing the most common features in the synthetic output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two model architectures were compared: **deep convolutional GAN** (**DCGAN**)
    and ProGAN. The experiment involved a dataset of 17,245 images of engineering
    professors from US universities, of which 80% were classified as male and 76%
    were classified as white. The experiment used human classifiers (Turkers) to classify
    the faces in the original dataset and those produced by the GANs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.8* shows the outcomes of two of the GANs tested against the original
    dataset. The DCGAN model resulted in heavily biased data generation, with a large
    overrepresentation of males and whites in the synthetic images. While the ProGAN
    model carried less of a bias, it still wasn’t a reasonable representation of the
    features in the original dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Both DCGAN and ProGAN penalized images with mostly feminine features. DCGAN
    generated the most biased output, reducing the percentage of feminine faces from
    20% to 6.67%.
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison to the 24% non-white faces in the original dataset, both DCGAN
    and ProGAN reduced that rate significantly – 1.33%% for DCGAN and 11.33% for ProGAN,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Comparison of gender and skin color distributions from synthetic
    facial images generated by GANs versus the original dataset. Source: Jain et al.,
    2021](img/B19297_07_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8 – Comparison of gender and skin color distributions from synthetic
    facial images generated by GANs versus the original dataset. Source: Jain et al.,
    2021'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, GANs can produce highly realistic synthetic datasets that may
    still be a biased representation of the latent features in the original dataset.
    Ironically, this partial mode collapse may produce more realistic images because
    the GAN has specialized to perform well for certain dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Following our principles of data-centric machine learning, it is important to
    identify modal bias across all dimensions of a synthetic dataset to ensure it
    is useful and ethically appropriate for your intended purpose. It is typically
    a process of trial and error to empirically validate and remove bias in GAN outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a notable weakness of GANs is that generating high-quality outputs
    requires a lot of computational resources, and without these, the images that
    are produced may appear blurry or unrealistic. Furthermore, without an experienced
    person choosing the appropriate directions to use for image generation, using
    GANs may not produce the desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image augmentation with a practical example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the complexity involved in synthetic image data generation, we want
    to finish this section with a practical example that gets you inspired to apply
    these techniques in your work.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover data augmentation, a mechanism for generating
    synthetic data for image data. We will use a pre-trained Xception model that was
    trained on ImageNet data and fine-tune it to accommodate clothing examples. We
    will achieve this by applying transfer learning to fine-tune the clothing examples,
    and then generate synthetic data to enhance its performance. With transfer learning,
    we can freeze the pre-trained layers of the network and only train the new layers
    by updating the final output. This helps the model to quickly adapt to the new
    dataset with less training time.
  prefs: []
  type: TYPE_NORMAL
- en: By applying transfer learning and image augmentation techniques on top of a
    pre-trained Xception model, we can generate synthetic data that can improve the
    performance of the model on new data. This approach is widely used in various
    applications, including image classification, object detection, and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we need to load the pre-trained Xception model using the TensorFlow
    library. We can do this by simply importing the Xception model from the TensorFlow
    module and setting the `include_top` parameter to `False` to exclude the top layer
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we must add a custom classifier on top of the pre-trained Xception model.
    We can achieve this by adding a few dense layers and a final output layer with
    the number of classes equal to the number of clothing categories we want to classify.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the model’s performance, we will apply image augmentation
    using the out-of-the-box TensorFlow features. This can include rotation, zooming,
    flipping, and other techniques to create variations in the training data and make
    the model more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will train the model using the augmented training data and evaluate
    its performance on the validation set. We can fine-tune the hyperparameters of
    the model and the augmentation techniques to achieve better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This example has been adapted from [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111)
    of *Machine Learning Bookcamp* ([https://livebook.manning.com/book/machine-learning-bookcamp/chapter-7/](http://book/machine-learning-bookcamp/chapter-7/))
    and has also been made available by the author of the book through a course run
    by the author’s company, DataTalks Club (https://github.com/alexeygrigorev/mlbookcamp-code).
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will import all the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will download the data by cloning the dataset from its respective
    GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will load an image of pants that we have downloaded from the clothing
    dataset repository. To load the image, we will use the `load_img` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – A pair of pants as output from the load_img function](img/B19297_07_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – A pair of pants as output from the load_img function
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will represent the image as a NumPy array as this is the format in
    which the model expects the image. We also need to ensure that all images are
    passed to the model with the same dimensions, so we will choose a standard of
    (299, 299, 3), which means 299 pixels top to bottom, 299 pixels left to right,
    and three color channels, which are red, green, and blue. Each pixel will be represented
    three times with values from 0-255 for each color channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must load the preceding image with a target size of (299,299):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the image with our new dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Our image of a pair of pants, resized to be 299x299 pixels](img/B19297_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Our image of a pair of pants, resized to be 299x299 pixels
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have ensured the dimensions, we will load the pretrained Xception
    model and specify the input format, which is the same as the dimensions specified
    for the preceding image. Once we’ve loaded the model, we will score the image
    and check the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The model expects the list of images in a particular format; hence, we will
    pass the previous image as a list and convert it into a NumPy array. Then, we
    will preprocess the image using `preprocess_input` and use and then score the
    input. The prediction will be stored in `pred`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `preprocess_input` function is required to preprocess the image data required
    by the model since when the Xception model was trained, the input values were
    transformed from 0-255 and scaled to have values between -1 and 1\. This is important
    because the distribution of color scales may affect the prediction. Imagine that
    red color scales were between 0-100, while blue color scales were between 200-300;
    this may have led to an unstable model. Hence, scaling is important. Without the
    correct preprocessing, the predictions won’t make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will decode these predictions with a convenience function. The model
    will provide a probability of the top five labels out of 1,000 different classes
    since the Xception model was trained to predict 1,000 labels. We don’t believe
    the target labels consist of clothing examples, so after the next few steps, we
    will move on to transfer learning. To decode the predictions, we will use the
    `decode_predictions` function. `decode_predictions` is a convenience function
    that provides predictions in such a format that they can be easily understood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It’s quite clear that the image of the pants is closest to the `jean` label
    and then the `swimming_trunks` label, as per the labels used in the Xception model.
    However, in the training data, there is no `jean` or `swimming_trunks` label.
    Next, we will extract the training data and make sure it is preprocessed before
    we pass it for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will use transfer learning and leverage the entire training data.
    We will use `ImageDataGenerator` to help process the input data that the model
    requires and then create training and validation datasets. The training data will
    consist of 3,041 images, while the validation data will consist of 341 images.
    For preprocessing, we will use 150x150 pixels instead of 299x299 pixels to reduce
    the training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the `flow_from_directory` property to process the entire
    training and validation dataset. We will use `seed=42` to ensure data is passed
    to the network, it’s passed with the same randomization, and the result at each
    training layer is reproducible. For the validation data, we will utilize `shuffle=False`
    to ensure that there is no randomization at each training step but data is passed
    sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can view the target classes of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will apply transfer learning. To do so, we must first extract the
    base layer – in other words, we must extract the convolutional layer of the Xception
    model and ensure it is frozen. This will ensure we get access to the feature map
    of the image data of 1 million+ images. To do so, we will use the `include_top=False`
    parameter, which will ignore the dense layers and return the bottom layer of the
    convolutional neural network. Next, we will build a custom dense layer with the
    10 class labels highlighted previously and train the dense layer for our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will build the architecture of the dense layer and combine it with
    the base layer. First, we will define the input standard such that the base model
    can provide the vector that’s suitable for input of the same standard. We will
    use (150,150,3) so that the model can be trained faster as more pixels can slow
    down the training process. Next, we will transform the vector from a base layer
    into a two-dimensional array. For that, we will use a method called pooling, which
    can help reduce the spatial size of feature maps but still retain the key information
    about the base layer. After that, we will create a dense layer where we specify
    the number of outputs, which is 10 in this case, and apply softmax activation
    to return a probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can define the loss function, utilize the Adam optimizer
    with a learning rate of 0.005, and choose accuracy as a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compile the model with the optimizer and loss to achieve the
    best accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will train the network with the clothing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Looking at these results, it is clear that the model is overfitted since the
    model achieved 92.7% accuracy on the training data and only 81.52% accuracy on
    the validation data, which is almost a 10-11% difference at the end of 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: We could try training dense layers with different learning rates to obtain a
    more generalized model. A learning rate controls how quickly we want the model
    to learn from training data and adjust its weights to fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: A low learning rate is like watching a video at a slow pace to ensure most of
    the details are covered, whereas a high learning rate is like watching a video
    at a faster pace and some details may be missed. An optimized learning rate is
    crucial for successful training, but it often requires tuning to find a balance
    between convergence speed and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce overfitting is by adjusting the dropout rate, which is
    a regularization technique in which a percentage of data is omitted at random.
    Both adjustments require experimentation and are more model-centric approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Following a data-centric approach, we want to test more data examples or better
    data examples. We need examples where the model doesn’t try to memorize specific
    pixels so that if it sees 120 red in the 130th-pixel location, it starts believing
    the image is of pants. Hence, to ensure a well-generalized model, we could leverage
    data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Concerning data-centric AI, data augmentation can be referred to as a technique
    to artificially increase the size and diversity of a training dataset by applying
    various transformations to the existing data. These transformations can include
    rotation, scaling, cropping, flipping, adding noise, and many others, depending
    on the type of data being augmented.
  prefs: []
  type: TYPE_NORMAL
- en: There are some common augmentation techniques, such as creating different angles
    of the image, shifting images, zooming images in and out, flipping them upside
    down, and more. However, before applying augmenting techniques, we must first
    consider different ways data will be generated. For instance, if users don’t generate
    pictures upside down, then augmenting images to create flipped images may only
    add noise and not provide a good signal to the model. For the following example,
    we will apply zoom augmentation, where images will be zoomed in and out a little,
    some shifting where clothing in the images will shift close to the edges, and
    apply vertical flips so that if some images are taken using a mirror, we can capture
    additional data for each scenario. We will achieve this by updating the image
    generator function, adding these extra parameters, and then training the model
    on the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply a rotation range of 10, a shear range of 10, a width and height
    shift range of 0.2, a zoom range of 0.1, and vertical flip. To achieve this, we
    will tweak the `ImageDataGenerator` function as this will create more examples
    of the training data under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create a function that will take in the learning rate and return
    the model and its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use this function and pass `0.005` as the learning rate and add
    50 epochs since we have generated a lot more data for augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The model achieved lower training accuracy, but the model is more generalizable
    and not overfitted. Also, note that the best validation accuracy was achieved
    at the 32nd epoch. However, it is difficult to note at which epoch a model will
    achieve the best accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we can further utilize the model checkpoint functionality to
    ensure only a model that achieves a minimum validation accuracy of 78% at a given
    epoch will be created and saved, and only when a previous best accuracy is surpassed
    will a new model be created. We can then use these saved models to score test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will add a dropout rate of 0.2 and an inner layer of 50\.
    We will update the function for training the network and add the checkpoint functionality.
    Once the checkpoint has been created, we’ll add it as a callback to the `fit`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to utilize hyperparameterization with the learning rate, dropout
    rate, and inner layer while using the checkpoint to ensure the best accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a function that takes three inputs – the learning rate,
    the inner layer, and the dropout rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the checkpoint. This is where we will save all the models
    with various epochs where the validation accuracy has reached a minimum of 78%.
    Then, we will add this checkpoint to the callback of the fitting function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the best model has been trained and saved, we will import the saved
    model, score all the data, and calculate the test accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The model that we used to predict the test data had a training accuracy of 79.16%
    and a validation accuracy of 81.23%. The test accuracy we achieved was only 77.15%,
    which can be improved iteratively – but beyond the scope of this example – by
    utilizing hyperparameter tuning for data augmentation parameters and model-centric
    parameters, which is encouraged in real life. However, due to computing and time
    constraints, this is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will extract the target labels and build a function to provide a predicted
    probability score along with the relevant classes. First, we will load the image
    and preprocess it, and then we will score it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will extract a random image of pants and run it through the model
    to get the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – A randomly selected pair of pants from the clothing dataset](img/B19297_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – A randomly selected pair of pants from the clothing dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will load and preprocess the image before scoring it. Finally, we
    will decode the predictions to extract the final prediction. For this, we will
    leverage the functions we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: According to the model, the image has a probability of 99% to be classified
    as a pair of pants, which is quite accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have been able to demonstrate, through the data-centric
    technique of data augmentation, how to generalize the model. We believe that by
    iterating over data augmentation parameters, we can further improve the quality
    of the model. Once the parameters have been tuned, we recommend that practitioners
    combine model-centric techniques such as regularization, the learning rate, inner
    layers, and the dropout rate to further tune and improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to exploring another topic that relies on unstructured
    data: synthetic data for text and natural language processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic text data is typically used to increase the depth and breadth of written
    words and sentences with a similar semantic meaning to real-life observations.
  prefs: []
  type: TYPE_NORMAL
- en: The most common augmentation techniques that are used to create synthetic data
    for natural language processing involve replacing words with synonyms, randomly
    shuffling the position of words in a sentence, and inserting or deleting words
    in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the sentence “I love drinking tea” could be transformed into “I
    take great pleasure in consuming tea” without losing the contextual meaning of
    the statement. This is an example of *synonym replacement*, where “love” has been
    replaced with “take great pleasure in” and “drinking” has been replaced with “consuming.”
  prefs: []
  type: TYPE_NORMAL
- en: '*Back translation* is another NLP technique that involves translating a sentence
    in one language into another language, and then back into the original language.
    Often, this will generate slightly different sentence structures with a similar
    semantic meaning, which makes it a great way to combat overfitting while increasing
    the size of your training data.'
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate a simple example of how to perform back translation using
    *Hugging Face Transformers* – specifically, the *MarianMT* suite of language models.
    MarianMT models were first created by Jörg Tiedemann using the Marian C++ library
    for fast training and translation but are now offered through the Hugging Face
    suite of Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the resource offers 1,440 transformer encoder-decoder
    models, each with six layers. These models support various language pairs, based
    on the *Helsinki-NLP* framework developed by the Language Technology Research
    Group at the University of Helsinki in Finland.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we want to translate the following three sentences from English
    into Spanish, and then use the same technique to translate the Spanish sentences
    back into English:'
  prefs: []
  type: TYPE_NORMAL
- en: The man glanced suspiciously at the door
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peter thought he looked very cool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most individuals are rather nice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to generate similar sentences with the same semantic meaning but
    slightly different wording as this synthetic data will give our eventual model
    more data to learn from.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll install the required Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll import the `MarianMTModel` and `MarianTokenizer` packages from
    the `transformers` library and define our input text string as `src_text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll define our `translator` model using the `Helsinki-NLP/opus-mt-en-es`
    language model, which translates from English into Spanish. The `MarianTokenizer`
    and `MarianMTModel` functions are used to define and execute our tokenizer and
    translation model, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output is stored as `trans_out`, which is then used as the input
    for our back translation model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this basic example, we simply repeat the same modeling exercise in reverse
    to produce slightly altered versions of the original input sentences. We use ‘`Helsinki-NLP/opus-mt-es-en`’
    to translate back into English:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the original input sentences against the model’s
    output. The generated sentences have slightly different wording, but generally,
    they have the same semantic meaning as the originals. To use these sentences in
    a training dataset for a supervised model, they must inherit the labels of their
    original “parent” sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input Sentence** | **Back Translation** |'
  prefs: []
  type: TYPE_TB
- en: '| The man glanced suspiciously at the door | The man looked suspiciously at
    the door |'
  prefs: []
  type: TYPE_TB
- en: '| Peter thought he looked very cool | Peter thought he looked great |'
  prefs: []
  type: TYPE_TB
- en: '| Most individuals are rather nice | Most individuals are quite pleasant |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Examples of back translation using Hugging Face Transformers
  prefs: []
  type: TYPE_NORMAL
- en: Privacy preservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data is also extremely useful for protecting the privacy and identity
    of individuals. The main aim of using synthetic data for privacy preservation
    is to make it impossible to identify individuals in a dataset while still keeping
    the statistical properties of the original dataset (close to) intact.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data is an excellent option for privacy preservation since it allows
    information to be shared without revealing private or sensitive information. To
    achieve this, we must create data that resembles the original but does not contain
    any personally identifiable information.
  prefs: []
  type: TYPE_NORMAL
- en: The use of synthetic data allows organizations to share data for research or
    other purposes without compromising the privacy of individuals. There are several
    benefits to using synthetic data for privacy preservation – for example, you can
    reduce the risk of data breaches since the data contains no personal or sensitive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy regulations around the world are increasingly making it mandatory
    to protect individuals’ privacy when using consumer data for analytical purposes.
    Synthetic data can be used to comply with privacy regulations, such as GDPR in
    the European Union or the HIPAA privacy rule in the United States, which sets
    standards for protecting personal data and preventing it from being shared without
    consent.
  prefs: []
  type: TYPE_NORMAL
- en: In general, synthetic data is a useful tool for preserving privacy because it
    allows organizations to share data without revealing sensitive or personal information.
    It is particularly useful for managing the trade-off between data quality and
    individual privacy in machine learning, making it an integral part of the data-centric
    toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, a bank that wants to use sensitive customer data for
    analytical activities such as churn modeling, fraud detection, and credit assessments.
    Using customer data for these activities typically brings about many compliance
    risks and mandated requirements that must be managed to avoid privacy breaches
    and heavy fines from regulators.
  prefs: []
  type: TYPE_NORMAL
- en: By having pre-generated synthetic datasets at hand, data scientists from various
    parts of the business can quickly and safely build models that would yield similar
    results to models built on real-world data. By using *appropriately constructed*
    synthetic data, the organization avoids going through cumbersome compliance and
    governance processes every time a new model is built and productionized.
  prefs: []
  type: TYPE_NORMAL
- en: However, this doesn’t mean the use of privacy-preserving data is without its
    risks. It may happen, for example, that the generative model overfits the original
    data and produces synthetic instances too close to the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Also, although synthetic data may appear anonymous, there may be instances where
    sophisticated hacks can reveal the identities of individuals. The aim of privacy-preserving
    synthetic data is to limit the risk of this happening.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore some common privacy disclosure scenarios to understand these risks
    and how we might limit them.
  prefs: []
  type: TYPE_NORMAL
- en: Types of privacy disclosure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further understand and appreciate the usefulness of synthetic data for privacy
    preservation, let’s have a look at three different types of privacy disclosure
    that can occur. This is not an exhaustive list of potential disclosure events,
    but it does help build an understanding of the potential and limitations of using
    synthetic data for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct identity disclosure** is the most obvious type of privacy disclosure.
    This is where an external adversary, such as a hacker, tries to gain information
    by matching the identity of an individual to records of private information. An
    example of this could be matching a person’s identity with medical records.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inferential identity disclosure** is a form of data privacy breach where
    certain pieces of personal information can be derived from data that has been
    made publicly available, without explicitly revealing an individual’s identity.
    This type of privacy breach occurs when an attacker uses statistical analysis
    to infer characteristics about an individual by analyzing patterns and correlations
    in a dataset. For example, an attacker may be able to determine the gender of
    an individual from publicly available data by analyzing patterns between particular
    characteristics and the corresponding gender.'
  prefs: []
  type: TYPE_NORMAL
- en: Fully synthetic data, by design, makes direct identity disclosure almost impossible.
    However, an attacker could use the analysis of a synthetic dataset to infer information
    about a particular group of people, despite not being able to identify individuals
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say an original dataset contains sensitive medical information.
    The synthetic version of this data preserves the same statistical properties as
    the original data. With basic statistical methods or more advanced machine learning
    models, an adversary can identify groups of people with similar characteristics
    and deduce their risk of a certain disease. An adversary could then leverage this
    knowledge to infer the risk of any individual that shares those same characteristics,
    without any direct identity disclosure taking place.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of inferential disclosure is when an attacker can infer someone’s
    financial status or income level based on certain behaviors, such as shopping
    habits or credit card usage patterns. In addition, an attacker may be able to
    determine the medical history of a person by analyzing health insurance claims
    and other related records. This can lead to serious consequences, such as discrimination
    or exploitation of sensitive information. Therefore, organizations must take appropriate
    steps to protect their data from inferential disclosure to ensure that it is not
    used for malicious purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Membership inference attacks** are similar to inferential disclosure, but
    they are not exactly the same. Rather than inferring personal information about
    an individual based on a group that shares similar characteristics, membership
    inference attacks aim to deduce if an individual who was present in the original
    dataset was used to create the synthetic dataset. This presents a huge privacy
    risk as, for example, it may reveal that someone has a certain illness without
    ever having disclosed their medical information. Preventing these attacks through
    synthetic data is difficult as the statistical properties of the original dataset
    have been maintained.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, synthetic data is a potent weapon against direct identity disclosure
    but does not remove the risk of identity disclosure entirely. Let’s examine why
    synthetic data is still superior to traditional identity-masking techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Why we need synthetic data for privacy preservation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional data de-identification techniques rely on two main approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anonymization**: This is the simplest form of de-identification and is where
    columns containing direct (customer ID, name, address) and quasi-identifiers (ZIP
    code, birth date) and other sensitive information are removed, hashed, encrypted,
    or masked. Metrics such as k-anonymity, l-diversity, and t-closeness are then
    used to validate the level of privacy preservation in a given dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differential privacy**: An algorithm for differential privacy uses statistical
    distributions such as Gaussian and Laplace to add randomly generated noise to
    the identifying features in a dataset. As a consequence, individuals’ privacy
    will be protected because identifying information is concealed behind the noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these techniques lower the risk of individuals being identified directly,
    they aren’t necessarily enough to completely remove it.
  prefs: []
  type: TYPE_NORMAL
- en: A 2019 study by Rocher et al11 demonstrated that 99.98% of Americans could be
    re-identified with no more than 15 demographic attributes based on a sample size
    of the population of Massachusetts. The authors conclude that “*heavily sampled
    anonymized datasets are unlikely to satisfy the modern standards for anonymization
    set forth by GDPR and seriously challenge the technical and legal adequacy of
    the de-identification* *release-and-forget model.*”
  prefs: []
  type: TYPE_NORMAL
- en: Another study, this time by Sweeney, 2000,12 found that 87% of the population
    in the US had reported characteristics that likely made them unique based only
    on ZIP code, gender, and date of birth. 53% of the US population is identifiable
    by only location, gender, and date of birth, where “location” is the city, town,
    or municipality where the person lives. 18% of the population are identifiable
    based on a combination of their county, gender, and date of birth.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it is quite possible to identify unique individuals based on
    only a few quasi-identifiers. By using synthetically generated data, we can remove
    these individual combinations from the dataset while preserving the overall statistical
    properties of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic data for privacy preservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we create synthetic data for privacy preservation, we have three goals:'
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the utility of the original data by reflecting its statistical properties
    in the synthetic dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure the data structure is the same as the original data. This means that
    we can use the same code and tools on synthetic data as on the original data,
    without needing to change anything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should not be possible to tell which real-world individuals were part of
    the original dataset when using privacy-preserving synthetic data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth noting that there are different ways to create synthetic data. Partial
    synthetic data just replaces some of the data with synthetic data, while fully
    synthetic information is created from scratch, without any of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the approach taken, fully synthetic information can provide a stronger
    guarantee against personal identity breaches, without sacrificing much in terms
    of usability and convenience.
  prefs: []
  type: TYPE_NORMAL
- en: A great way for you to start practicing synthetic data generation is through
    the tools created by the **Synthetic Data Vault** (**SDV**) project. The project
    was first established by MIT’s Data to AI Lab in 2016 and is a comprehensive ecosystem
    of Python libraries that allows users to learn single-table, multi-table, and
    time series datasets, which can then be used as the basis for generating synthetic
    data that replicates the format and statistical properties of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this project, it’s possible to easily supplement, augment, and – in
    some cases – replace real data with synthetic data when training machine learning
    models. Additionally, it enables machine learning models or other data-dependent
    software systems to be tested without the risk of exposure that comes with sharing
    actual data.
  prefs: []
  type: TYPE_NORMAL
- en: The SDV suite is comprised of several probabilistic graphical modeling and deep
    learning-based techniques. They are used to generate hierarchical generative models
    and recursive sampling algorithms, which enable synthetic versions of a variety
    of data structures.
  prefs: []
  type: TYPE_NORMAL
- en: We will use two different techniques from the SDV suite – `GaussianCopula` and
    `CopulaGAN` – to illustrate how to generate synthetic data for privacy preservation
    purposes. Then, we’ll briefly look at how to measure the *quality* and *score*
    using metrics and charts.
  prefs: []
  type: TYPE_NORMAL
- en: GaussianCopula
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A copula is a tool that’s used to measure the dependence among random variables.
    `GaussianCopula` is a collection of multiple (that is, multivariate) normally
    distributed pieces of data. Taken together as one set, the copula lets us describe
    how these independent normal distributions are related by showing how changes
    in one element in the set affect the others – that is, their *marginal distributions*.
    This is important because this exercise aims to augment any one unique combination
    of variables while preserving the overall statistical properties of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Example GaussianCopula Python program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will write a sample program to show how this works. It will do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a sample dataset and then calculate the `GaussianCopula` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use that model to generate some sample data given the `GaussianCopula` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the output and its statistical properties to understand how our model
    is performing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we must install the necessary Python packages – `sdv` and `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: For this exercise, we will use the publicly available *Adult* dataset, also
    known as the *Census Income* dataset. To get started, download it from [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data](http://ml/machine-learning-databases/adult/adult.data).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use standard `pandas` functions to create a DataFrame, `df`, from the
    preceding URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – The first five rows of the Adult dataset – our input dataset
    for synthetic data generation](img/B19297_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The first five rows of the Adult dataset – our input dataset for
    synthetic data generation
  prefs: []
  type: TYPE_NORMAL
- en: With our DataFrame created, we will import `SingleTableMetadata`, which is a
    class that provides methods to manage metadata about a single table of data, such
    as the names and types of columns, relationships between columns, and more. SDV’s
    modeling suite needs this metadata object as input.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will use the `detect_from_dataframe()` method to analyze the pandas
    `df` DataFrame and automatically detect and set metadata about the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will load the appropriate APIs and objects from SDV and instantiate
    the `GaussianCopula` model. Then, we will use the `fit()` method to generate the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Normally, we would take a sample of the input dataset that is smaller than the
    full input dataset to generate the model, but in this case, we’ll take the entire
    input data since it isn’t too large.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s generate the synthetic dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once we have generated our synthetic data, we can assess its quality by comparing
    it to the attributes of the real data. This can be done by using several quality
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and do that.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating quality scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To measure the quality of the synthetic data, we can use various *score* metrics
    from the *SDV* package. The definition and interpretation of the scores vary depending
    on which metric we are looking at.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some relevant metrics for measuring statistical similarity between
    original and synthetic data, as well as the risk of inference attacks being successful.
    Scores range between 0 and 1\. The interpretation of 0 or 1 varies according to
    what metric you are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BoundaryAdherence`: This describes whether the synthetic data lies within
    the range of the max and min for a column in the real data. 1 means yes and 0
    means no.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StatisticSimilarity`: This compares the mean, median, and standard deviation
    in a column between real and synthetic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CategoricalCAP`: This is the risk of disclosing private information using
    an inference attack – that is, a hacker knows some of the real data and can match
    it up with the synthetic. A score of 1 means there is a high risk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Data Likelihood`: This calculates how likely it is that the data will match
    observations in the original data. This is similar to the `Detection` metric,
    which asks whether the machine learning model can tell which is the original dataset
    and which is the fabricated one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KSComplement`: This shows whether the column shape of the real and synthetic
    data are the same using the **Kolmogorov-Smirnov** (**K-S**) test. The K-S test
    measures the maximum distance between the **cumulative distribution function**
    (**CDF**) of the two datasets. However, it uses its complement (the 1 - KS statistic).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MissingValueSimilarity`: This measures the proportion of missing data in the
    real and synthetic datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for showing all of these metrics is nearly the same. Simply call the
    appropriate package, then run the `compute` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example for `MissingValueSimilarity`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The output score is equal to 1.0, which means the model has successfully matched
    the proportion of missing values in the synthetic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying and visualizing data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also want to quantify and visualize the quality of our synthetic data compared
    to the original set. For this purpose, we’ll use the *diagnostic* and *quality*
    reports from the SDV library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagnostic report should always produce a score of 100%, which tells us
    that primary keys are unique and non-null, continuous values in the synthetic
    data adhere to the min/max range in the original data, discrete values line up
    with the same categories across real and synthetic data, and column names are
    the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The SDV quality report evaluates how well your synthetic data captures the mathematical
    properties of our original real data. It does this through a set of metrics that
    measure various aspects of the *fidelity* between the two datasets. **Data fidelity**
    refers to how accurate a dataset is at representing the features of its source.
  prefs: []
  type: TYPE_NORMAL
- en: The report provides an overview of the results, as well as detailed visualizations
    and explanations for each metric so that you can quickly understand the strengths
    and weaknesses of your synthetic data. By understanding how well your synthetic
    data captures the mathematical properties of the real data, you can take steps
    to improve it if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SDMetrics quality report is a valuable tool that helps you ensure your
    synthetic data is as accurate and reliable as possible. Here’s how we can use
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a quality report with various metrics and visualizations
    that show the overall similarities between the original and synthetic data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a couple of important metrics to know about:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Column Shapes`: A column’s shape tells us how data is distributed. A higher
    score means that the real and synthetic data are more similar. A separate column
    shape score for every column is calculated, but the final score is the average
    of all columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Column Pair Trends`: The correlation between two columns indicates how their
    trends compare to each other; the higher the score, the more similar those trends
    are. A score is produced for each column pair in the data, while the final score
    is the average of all columns. This is an important score that tells us whether
    our synthetic data has captured the relationships between variables in the original
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also visualize the dimensions of these metrics with the `get_visualization`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – A correlation matrix comparing column pair trends between the
    original and synthetic datasets](img/B19297_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – A correlation matrix comparing column pair trends between the
    original and synthetic datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, most column pairs have a high similarity score, but the `capital-gain`
    column is far apart. We can use the following code to visualize the real and synthetic
    `capital-gain` column distributions side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.14 – A comparison of the distribution of the real and synthetic\
    \ capital-gain columns. The GaussianCopula model hasn’t done a good job of matching\
    \ the distribution\uFEFF](img/B19297_07_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – A comparison of the distribution of the real and synthetic capital-gain
    columns. The GaussianCopula model hasn’t done a good job of matching the distribution
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we would test various column distribution functions to find a
    better match for this particular column. In this example, we used the `GaussianCopula`
    function to create a synthetic dataset. However, the SDV library contains several
    other distributions that can be useful, depending on the characteristics of your
    original dataset. Let’s explore how to change the default distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Varying column distribution functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `GaussianCopula` function determines which statistical distribution best
    describes each copula, but it doesn’t always get it right. Luckily, we can override
    the preselection and pick our preferred distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian (normal) distribution**: Use this if your data is continuous and
    symmetrically distributed around the mean. It’s often used for naturally occurring
    data, such as the heights or weights of a population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma distribution**: This is used for positive-only, skewed data. It’s often
    used for things such as wait times or service times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beta distribution**: This is used for variables that are bounded between
    0 and 1, such as proportions or probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Student’s t-distribution**: This is similar to the Gaussian distribution
    but has heavier tails. It’s often used when the sample size is small or the standard
    deviation is unknown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian kernel density estimation** (**KDE**): Use this for non-parametric
    data – that is, when you don’t know or want to assume a specific distribution.
    The KDE uses the data itself to estimate its distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Truncated Gaussian distribution**: Use this when you have data that follows
    a Gaussian distribution but is bounded within a specific range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, here is how to show the distributions it calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This statement produces a detailed list of all the columns in the dataset. Instead
    of showing the output here, the model defaulted to a beta distribution for all
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change a distribution function for a given column, just create a model again
    but this time explicitly apply a specific distribution to that column. In this
    case, we will apply a gamma distribution to the `capital-gain` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is a new synthetic dataset with a `capital-gain` column
    distribution much closer to the real data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – A new comparison of the capital-gain columns. Using the gamma
    distribution on this column improved the similarity between the synthetic and
    original data](img/B19297_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – A new comparison of the capital-gain columns. Using the gamma
    distribution on this column improved the similarity between the synthetic and
    original data
  prefs: []
  type: TYPE_NORMAL
- en: Another useful package from the SDV library is `CopulaGAN`. This algorithm is
    a blend of the `GaussianCopula` and `CTGAN` algorithms. Let’s compare the performance
    of `CopulaGAN` to `GaussianCopula` on the *Adult* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: CopulaGAN code example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopulaGAN` is a variation of `GaussianCopula` that can yield better results
    using a simplified GAN model. We will compare the two models in this section,
    but first, here is the code to generate `CopulaGAN` using the same input dataset
    and metadata object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Measuring data quality from CopulaGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let’s look at data quality regarding the `CopulaGAN` model, repeating
    some of the same techniques we used with `GaussianCopula`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7**.16* provides a visual representation of the column pair trends
    of the original and synthetic datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Column pair trends for the Adult dataset using CopulaGAN](img/B19297_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Column pair trends for the Adult dataset using CopulaGAN
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between GaussianCopula and CopulaGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopulaGAN` is a hybrid AI model that combines the human accessibility of Gaussian
    copulas with the robust accuracy of GANs13.'
  prefs: []
  type: TYPE_NORMAL
- en: A GAN is a deep learning algorithm. If you’ve worked with neural networks, you
    know that they are a kind of black box, meaning the coefficients of the nodes
    in the network are functions and not numbers. They are very hard to explain or
    understand compared to, for example, a polynomial or linear model.
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianCopula` is easier to explain. It works by trying different known statistical
    distributions (normal, Weibull, and others), which is very useful for known or
    easily observable distributions. Then, for each column, it picks the one that
    matches the closest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The team behind the SDV project developed `CopulaGAN` to get the best of both
    worlds: a more accurate model that is still explainable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table compares the results from our two models in the previous
    examples. `CopulaGAN` achieved a higher overall quality score because it was able
    to match the column shapes more:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **GaussianCopula** | **CopulaGAN** |'
  prefs: []
  type: TYPE_TB
- en: '| **Overall** **Quality Score** | 84.6% | 87.39% |'
  prefs: []
  type: TYPE_TB
- en: '| **Column Shapes** | 87.57% | 91.75% |'
  prefs: []
  type: TYPE_TB
- en: '| **Column** **Pair Trends** | 81.63% | 83.04% |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Data quality comparison of the GaussianCopula and CopulaGAN algorithms
    on the Adult dataset
  prefs: []
  type: TYPE_NORMAL
- en: Of course, quality is a highly complex topic and our example is not exhaustive
    in that regard. You would have to look at the scores across all columns and all
    the different types of scores to validate the accuracy of the synthetically generated
    data. In other words, you cannot say for definite that `CopulaGAN` is more accurate
    in all cases without doing a deeper review of the variables in the dataset. This
    is particularly important when you are dealing with high-stakes datasets and use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: One additional metric to consider is run speed. Anecdotally, when we wrote this
    example, `CopulaGAN` took 1 hour to complete, while `GaussianCopula` took 15 seconds
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the privacy of our new dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have constructed a synthetic dataset for our use case, we need to
    ensure we have prevented the ability to re-identify individuals from the original
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To know the likelihood that individuals can be re-identified, we need an accurate
    measure of the difference or “distance” between the original and synthetic records.
    The farther the two are apart, the less probable that they can be identified as
    one entity. If we are discussing personal information in tabular form, we need
    a methodology for measuring the distance between qualitative and quantitative
    attributes alike.
  prefs: []
  type: TYPE_NORMAL
- en: To accurately measure the closeness of two rows within a dataset containing
    both qualitative and quantitative information, we can utilize a similarity coefficient
    called Gower’s distance.
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance is a unique type of distance measure that differs from distance
    measures. It stands out in terms of its ability to calculate the difference between
    two entities with both numerical and categorical values. This is important because
    many common clustering algorithms, such as K-means clustering, only work when
    all of the variables are numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance returns a similarity coefficient between 0 (indicating identical
    observations) and 1 (showing that they are at the maximum distance).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a set of *p* features in the original (*o*) and synthetic (*s*)
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: For ordinal numbers, the distance from one feature to the other is simply the
    absolute value of their difference divided by the range of that variable. We divide
    by the range to normalize the data so that large numbers won’t be given greater
    weight than small ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variables are turned into numbers so that we can do math. The formula
    is simple – if those values are the same, their distance is 0; otherwise, it is
    1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gower’s distance is the sum of the distances divided by the number of features
    – the average of the terms. Since we divide these differences by the number of
    features, this is the same as saying Gower’s distance is the average distance.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we make a definition for **closeness** and call it the **distance to the
    closest record**. For every element in *s*, the closest row in *o* is the one
    with the minimum Gower’s distance. A distance of 0 means that two rows of data
    are the same, while a distance of 1 means that two rows are as different as possible
    given the observations in the dataset we’re using.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s practice applying Gower’s distance using the `Gower` Python package.
  prefs: []
  type: TYPE_NORMAL
- en: Gower’s distance Python example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our Gower’s distance practice example, we’ll use the *Adult* dataset and
    compare the synthetic output generated by `CopulaGAN` to the original data. We
    recommend using a small subset of the *Adult* dataset (for example, 1,000 rows)
    to practice as Gower’s matrix calculation can take a long time to run on larger
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create the DataFrame for our model based on the top 1,000 rows
    from the existing `df` DataFrame, which contains the full *Adult* dataset. Then,
    we’ll fit a `GaussianCopula` model on this dataset and generate a new synthetic
    dataset called `synthetic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll install the `Gower` package and calculate the Gower’s distance
    matrix between our two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate results similar to the following, where the distance between
    each row in the dataset is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The resulting gowerMatrix](img/B19297_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The resulting gowerMatrix
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll use the `gower_topn()` function to find the top *n* (in this case,
    10) closest (that is, most similar) rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the synthetic dataset passes our test, we must make sure none of
    its values are equal to 0; otherwise, this would indicate that some rows in the
    synthetic data resemble those from the original. Generally speaking, we want the
    top values to be sufficiently distanced from 0 as this reduces the risk of reidentification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the index of the top 10 closest rows and their Gower’s distance.
    In this case, the smallest distance between two rows in our datasets is 0.02205,
    which means our synthetic dataset is not sufficiently different from the original
    at the individual row level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we would have more work to do to reduce the similarity between
    sets. Here are a few techniques you could use to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianCopula` or test other synthesizers in the SDV catalog, such as CTGAN
    or `CopulaGAN`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add noise**: You can add random noise to the synthetic data. This will make
    the synthetic data more “unique” compared to the original data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform feature transformation**: Apply some kind of transformation (for
    example, logarithmic, square root, exponential, and so on) to the features in
    the synthetic dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perform data augmentation**: Generate new synthetic data points that are
    not direct copies of the real data. You can do this by using techniques such as
    the **Synthetic Minority Over-Sampling Technique** (**SMOTE**) or **Adaptive Synthetic
    Sampling** (**ADASYN**), both of which we’ll discuss later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that while your goal is to reduce similarity, you also want the synthetic
    data to be useful and representative of the real data. If you make the synthetic
    data too dissimilar, it may not serve its intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, Gower’s distance can also be used to find rows of data that
    are very similar to each other, which is useful for tasks such as creating lookalike
    audiences, clustering, or identifying at-risk populations.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, imagine that you have just run a very successful email marketing
    campaign to a group of customers and you want to expand the campaign to customers
    who look like the ones in the original target group.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, simply calculate Gower’s distance between customers in the original
    target group and the rest of your customer base, and pick a target group based
    on the lowest Gower’s distances.
  prefs: []
  type: TYPE_NORMAL
- en: Using synthetic data to improve model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070)*, Techniques for Data Cleaning*
    and [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for Programmatic
    Labeling in Machine Learning*, we dealt with improving model performance by refining
    data quality. However, there are times when improving data quality may not be
    enough, especially when datasets are small. In such situations, we can take advantage
    of generating synthetic data to boost model performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we covered previously in this chapter, synthetic data can help with generating
    more training examples, as well as generalizing the performance of the model by
    providing more examples of different variations and distributions of the data.
    Both of these uses can make the model more robust and less likely to overfit the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: With imbalanced datasets, a model gets biased toward the majority class as there
    are more examples of one class over another. This is the problem with the loan
    prediction dataset, where 30% of the data belongs to the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover generating synthetic data for the minority class
    so that the model can generalize further and model performance metrics can improve.
    We will stick with a decision tree model and use synthetic data generation to
    further improve the signal strength of the data. We will use the `imblearn` library
    from Python to generate synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the library and two oversampling methods, `SMOTE` and `ADASYN`,
    to oversample the minority class. We will also leverage the `Counter` method to
    count data samples pre- and post-synthetic data generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Both the SMOTE and ADASYN algorithms are used to generate synthetic data. However,
    ADASYN is more robust as it considers the density of points to generate synthetic
    data. SMOTE may generate synthetic data around the minority class, but it does
    so uniformly without considering how rare a data point is.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE creates synthetic samples by randomly selecting pairs of minority-class
    samples and interpolating new samples around the existing samples. This technique
    spreads out further into the space to increase the number of minority class samples.
    However, as the samples are chosen randomly, no weighting is given to rare sample
    points.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ADASYN considers rare data points in the feature space by
    computing the density distribution of the minority class samples. It generates
    synthetic samples in regions of the feature space where the density is low to
    ensure that synthetic samples are generated where they are most needed to balance
    the dataset. ADASYN uses the k-nearest neighbors algorithm to estimate the density
    distribution of the minority class samples. For each minority class sample, ADASYN
    computes the density based on the number of k-nearest neighbors that belong to
    the minority class. The value of k is a user-defined parameter, typically set
    to a small value such as 5 to 10\. The density is the average distance from k
    nearest points. A higher average distance means lower density, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: We iterate over different thresholds of the algorithm parameters, such as the
    number of nearest neighbors and the percentage of data to oversample. This helps
    us find the best parameters to generate the optimal number of samples so that
    both the test ROC and test accuracy get the maximum boost. Then, we combine those
    results in a DataFrame and choose the best parameters. This is done to measure
    the performance of the model that is using synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will only use ADASYN, but we encourage you to try different
    techniques, including SMOTE, for the problem at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must generate a DataFrame that contains our generated combinations
    of model parameters and model performance metrics and sort it by test accuracy.
    The DataFrame indicates that an oversampling strategy with a ratio of 75% for
    the minority to majority class, and a nearest neighbors value of 7, will provide
    the best accuracy and ROC score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Output DataFrame](img/B19297_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Output DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must apply the parameters from our highest-performing oversampling
    strategy and retrain the decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: ADASYN increased the number of minority class samples from 173 to 321 using
    synthetic data generation, which boosted the test accuracy to 83.8%. This is an
    almost 2% increase in accuracy. The ROC score was also boosted to 86.2%, which
    is a further increase of 4.4%.
  prefs: []
  type: TYPE_NORMAL
- en: These results demonstrate that synthetic data generation can provide significant
    gains in model performance, even for small datasets. However, it is important
    to note that this may not always be the case, especially if error analysis suggests
    that adding new data doesn’t contribute to an improvement in model performance.
    In such cases, you may turn to collecting more data or features, or even performing
    feature engineering, before moving on to synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: When should you use synthetic data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve established that synthetic data can be used for several purposes,
    but how do you decide whether to use synthetic data for your project or not?
  prefs: []
  type: TYPE_NORMAL
- en: For businesses seeking to gain an edge over their competitors through innovative
    or unconventional approaches, synthetic data provides an accessible middle ground
    between experimentation and reality. For governmental organizations wanting to
    learn from their vast stores of population data, synthetic data allows highly
    sensitive datasets to be analyzed without compromising individual privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation and exploring the boundaries of your data (synthetic or real)
    can be incredibly valuable, but the benefit of introducing synthetic data should
    always be assessed against the cost and risk of making damaging predictions with
    that same data.
  prefs: []
  type: TYPE_NORMAL
- en: The central question is, “*What is the acceptable cost of an experiment?*,”
    especially if it includes human collateral damage or reputational or financial
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: In our opinion, synthetic data should be used when obtaining real-world data
    may be difficult, expensive, or unethical. The most common and practical use cases
    for synthetic data are for preserving the privacy of individuals and for creating
    simulations that are very difficult or impossible in traditional test environments.
    For these use cases, the benefits are more likely to outweigh the risks of using
    synthetic data, but that is not a guarantee, so make sure you manage risks appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: The main risks to mitigate are perpetuation and exacerbation of bias. Machine
    learning models are inherently prone to overfitting and finding the “easiest”
    path through the data, so synthetic datasets should be rigorously tested to ensure
    they are fit for purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data can also accelerate the process of testing and training machine
    learning models, saving companies time and money in their development and deployment
    cycles. Furthermore, synthetic data is a useful tool for creating simulations
    that are not possible in traditional test environments.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that using synthetic data is typically just one of many avenues
    to take when building models or improving the accuracy of your predictions. It
    should only be used when the potential risk and effect on those impacted is understood
    and managed appropriately. On the other hand, if you can mitigate this risk –
    or in some cases, avoid “real-world” risks altogether – then it is a wonderful
    tool to have in your toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided a primer on synthetic data and its common uses.
    Synthetic data is a key part of the data-centric toolkit because it gives us yet
    another avenue to much better input data, especially when collecting new data
    is not feasible.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a clear understanding of the fundamentals of synthetic
    data and its potential applications. Synthetic data is often used for computer
    vision, natural language processing, and privacy protection applications. However,
    the potential of synthetic data goes well beyond these three realms.
  prefs: []
  type: TYPE_NORMAL
- en: Whole books have been dedicated to the topic of synthetic data and we recommend
    that you dive deeper into the subject if you want to become a true expert in synthetic
    data generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll explore another powerful technique for improving
    your data without the need for collecting new data: programmatic labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://datagen.tech/guides/synthetic-data/synthetic-data](https://datagen.tech/guides/synthetic-data/synthetic-data),
    viewed on 12 November 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/](https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://unity.com/our-company](https://unity.com/our-company), viewed on 15
    November 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: https://venturebeat.com/ai/unitys-danny-lange-explains-why-synthetic-data-is-better-than-the-real-thing-at-transform-2021-2/,
    viewed on 15 November 2022
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alcorn, M A et al 2019, *Strike (with) a Pose: Neural Networks Are Easily Fooled
    by Strange Poses of Familiar Objects*, viewed 13 November 2022: [https://arxiv.org/pdf/1811.11553.pdf](http://pdf/1811.11553.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.tesla.com/VehicleSafetyReport](https://www.tesla.com/VehicleSafetyReport),
    viewed 13 November 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Karras T, Aila T, Laine S, Lethtinen J, 2017, *Progressive Growing of GANs
    for Improved Quality, Stability, and* *Variation*: [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Karras T, Aila T, Laine S 2018, *A Style-Based Generator Architecture for Generative
    Adversarial* *Networks*: [https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Metz L, Poole B, Pfau D, Sohl-Dickstein J 2017, *Unrolled Generative Adversarial
    Networks*, ICLR 2017: [https://arxiv.org/pdf/1611.02163.pdf](https://arxiv.org/pdf/1611.02163.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jain N, Olmo A, Sengupta S, Manikonda L, Kambhampati S, 2021, *Imperfect ImaGANation:
    Implications of GANs Exacerbating Biases on Facial Data*, ICLR 2021 Workshop on
    Synthetic Data Generation – Quality, Privacy, Bias: [https://arxiv.org/pdf/2001.09528.pdf](https://arxiv.org/pdf/2001.09528.pdf
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rocher L, Hendrickx J M, de Montjoye Y A 2019, *Estimating the success of re-identifications
    in incomplete datasets using generative* *models*: [https://www.nature.com/articles/s41467-019-10933-3](https://www.nature.com/articles/s41467-019-10933-3
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'L. Sweeney, *Simple Demographics Often Identify People Uniquely, Carnegie Mellon
    University*, Data Privacy Working Paper 3\. Pittsburgh 2000: [https://dataprivacylab.org/projects/identifiability/paper1.pdf](https://dataprivacylab.org/projects/identifiability/paper1.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://mobile.twitter.com/sdv_dev/status/1519747462088507393](https://mobile.twitter.com/sdv_dev/status/1519747462088507393),
    viewed on 25 January 2023'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
