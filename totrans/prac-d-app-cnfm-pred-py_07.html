<html><head></head><body>
<div id="_idContainer042">
<h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-76"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.2.1">Conformal Prediction for Regression</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will cover conformal prediction for </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">regression problems.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Regression is a cornerstone of machine learning, enabling us to predict continuous outcomes from given data. </span><span class="koboSpan" id="kobo.5.2">However, as with many predictive tasks, the predictions are never free from uncertainty. </span><span class="koboSpan" id="kobo.5.3">Traditional regression techniques give us a point estimate but fail to measure the uncertainty. </span><span class="koboSpan" id="kobo.5.4">This is where the power of conformal prediction comes into play, extending our regression models to produce well-calibrated </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">prediction intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">This chapter delves deep into conformal prediction tailored specifically for regression problems. </span><span class="koboSpan" id="kobo.7.2">By understanding and appreciating the importance of quantifying uncertainty, we will explore how conformal prediction augments regression to provide not just a point prediction but an entire interval or even a distribution where the actual outcome will likely fall with pre-specified confidence. </span><span class="koboSpan" id="kobo.7.3">This is invaluable in many real-world scenarios, especially when making decisions based on predictions where stakes are high and being “approximately right” isn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">good enough.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">We will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">the chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Uncertainty quantification for </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">regression problems</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Various approaches to produce </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">prediction intervals</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Conformal prediction for </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">regression problems</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Building prediction intervals and predictive distributions using </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">conformal prediction</span></span></li>
</ul>
<h1 id="_idParaDest-77"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.19.1">Uncertainty quantification for regression problems</span></h1>
<p><span class="koboSpan" id="kobo.20.1">After completing this chapter, whenever you predict any continuous variable, you’ll be equipped to add a layer of robustness and reliability to your predictions. </span><span class="koboSpan" id="kobo.20.2">Understanding and quantifying this uncertainty is crucial for </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">several reasons:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.22.1">Model interpretability and trust</span></strong><span class="koboSpan" id="kobo.23.1">: Uncertainty quantification helps us understand the reliability of our model predictions. </span><span class="koboSpan" id="kobo.23.2">By providing a range of possible outcomes, we can build trust in our model’s predictions and interpret them </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">more effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.25.1">Decision-making</span></strong><span class="koboSpan" id="kobo.26.1">: In many practical applications of regression analysis, decision-makers must rely on something other than point estimates. </span><span class="koboSpan" id="kobo.26.2">They often need to know the range within which the actual value will likely fall with a certain probability. </span><span class="koboSpan" id="kobo.26.3">This range, or prediction interval, provides crucial information about the uncertainty of the prediction and aids in </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">risk management.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.28.1">Model improvement</span></strong><span class="koboSpan" id="kobo.29.1">: Uncertainty can highlight the areas where the model may benefit from additional data or feature engineering. </span><span class="koboSpan" id="kobo.29.2">High uncertainty may indicate that the model needs help capturing the underlying relationship, suggesting the need for model revision or </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">additional data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.31.1">Outlier detection</span></strong><span class="koboSpan" id="kobo.32.1">: Uncertainty quantification can also help us identify outliers or anomalies in the data. </span><span class="koboSpan" id="kobo.32.2">Observations associated with high predictive uncertainty may be outliers or indicate a novel situation not captured during </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">model training.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.34.1">Therefore, uncertainty </span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.35.1">quantification forms an essential part of regression problems. </span><span class="koboSpan" id="kobo.35.2">It provides a more holistic picture of predictive performance, allows for better risk management, and improves model trust and interpretability. </span><span class="koboSpan" id="kobo.35.3">Conformal prediction for regression, a framework that we will discuss in this chapter, is the approach to efficiently quantifying uncertainty in </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">regression problems.</span></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.37.1">Understanding the types and sources of uncertainty in regression modeling</span></h2>
<p><span class="koboSpan" id="kobo.38.1">Uncertainty </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.39.1">in regression modeling can arise from several sources and manifests in different ways. </span><span class="koboSpan" id="kobo.39.2">Broadly, these uncertainties can be classified into two main types – </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">aleatoric</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.41.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.42.1">epistemic</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.44.1">Aleatoric uncertainty</span></strong><span class="koboSpan" id="kobo.45.1">: This </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.46.1">type of uncertainty is often called “inherent,” “irreducible,” or “random” uncertainty. </span><span class="koboSpan" id="kobo.46.2">It arises due to the inherent </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.47.1">variability in the data itself, which is usually beyond our control. </span><span class="koboSpan" id="kobo.47.2">This uncertainty would not be eliminated if we were to collect more data or improve our measurements. </span><span class="koboSpan" id="kobo.47.3">Aleatoric uncertainty reflects the randomness, variability, or heterogeneity in our </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">sample population.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Epistemic uncertainty</span></strong><span class="koboSpan" id="kobo.50.1">: Also known as “reducible” or “systematic” uncertainty, this type </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.51.1">originates from the lack of knowledge </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.52.1">about the system or process under study. </span><span class="koboSpan" id="kobo.52.2">It could be due to insufficient data, measurement errors, or incorrect assumptions about the underlying data distribution or model structure. </span><span class="koboSpan" id="kobo.52.3">Unlike aleatoric uncertainty, epistemic uncertainty can be reduced with more information </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">or data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.54.1">The primary sources of these uncertainties in regression modeling are </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Data uncertainty</span></strong><span class="koboSpan" id="kobo.57.1">: This includes measurement errors, missing values, and variability </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.58.1">in the data. </span><span class="koboSpan" id="kobo.58.2">Data might be collected under different conditions or sources, adding more uncertainty to </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">the dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Model uncertainty</span></strong><span class="koboSpan" id="kobo.61.1">: This comes from the model’s inability to precisely capture the </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.62.1">proper relationship between predictors and the outcome. </span><span class="koboSpan" id="kobo.62.2">Every model makes certain assumptions (for example, linearity, independence, normality, and so on), and any violation of these assumptions </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">introduces uncertainty.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.64.1">Parameter uncertainty</span></strong><span class="koboSpan" id="kobo.65.1">: Every </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.66.1">regression model involves estimating parameters (for example, coefficients). </span><span class="koboSpan" id="kobo.66.2">There’s always some uncertainty about these estimates, which can contribute to the overall uncertainty </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">in predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Structural uncertainty</span></strong><span class="koboSpan" id="kobo.69.1">: This </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.70.1">refers to uncertainty due to the choice of a specific model form. </span><span class="koboSpan" id="kobo.70.2">Different model structures or types (for example, linear regression, polynomial regression, and so on) might lead to different interpretations </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">and predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Residual uncertainty</span></strong><span class="koboSpan" id="kobo.73.1">: This comes from the residuals or errors of the model. </span><span class="koboSpan" id="kobo.73.2">It </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.74.1">represents the difference between the observed outcomes and the outcomes predicted by </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">the model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.76.1">In the context of regression modeling, recognizing these types and sources of uncertainty can help interpret the model’s predictions more accurately. </span><span class="koboSpan" id="kobo.76.2">It can guide the process of model refinement </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">and validation.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.78.1">The concept of prediction intervals</span></h2>
<p><span class="koboSpan" id="kobo.79.1">A prediction interval is an interval estimate associated with a regression prediction, indicating </span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.80.1">a range within which the actual outcome will likely fall with a certain probability. </span><span class="koboSpan" id="kobo.80.2">While a point prediction gives us a singular value as the most likely outcome, a prediction interval offers a range, providing a clearer picture of the uncertainty associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">that prediction.</span></span></p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.82.1">Why do we need prediction intervals?</span></h2>
<p><span class="koboSpan" id="kobo.83.1">Let’s </span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.84.1">take </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">a look:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Quantification of uncertainty</span></strong><span class="koboSpan" id="kobo.87.1">: The primary reason for employing prediction intervals is to quantify our predictions’ uncertainty. </span><span class="koboSpan" id="kobo.87.2">No matter how sophisticated the model is, every prediction comes with inherent variability. </span><span class="koboSpan" id="kobo.87.3">By using prediction intervals, we can communicate this </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">variability effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.89.1">Risk management</span></strong><span class="koboSpan" id="kobo.90.1">: In various industries, particularly finance, healthcare, and engineering, understanding the range of potential outcomes is crucial for risk assessment and mitigation. </span><span class="koboSpan" id="kobo.90.2">A prediction interval helps decision-makers weigh their actions’ potential risks </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">and benefits.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.92.1">Model transparency</span></strong><span class="koboSpan" id="kobo.93.1">: Providing an interval instead of just a point estimate can enhance the transparency of the model. </span><span class="koboSpan" id="kobo.93.2">Stakeholders can gauge not only what the model predicts but also the confidence the model has in </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">that prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.95.1">Guided decision-making</span></strong><span class="koboSpan" id="kobo.96.1">: Decision-makers can act more decisively when they understand </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.97.1">the worst-case and best-case scenarios. </span><span class="koboSpan" id="kobo.97.2">For example, knowing the lower and upper bounds of predicted sales in sales forecasting can help </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">allocate resources.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.99.1">Understanding the necessity for prediction intervals in various contexts paves the way for a deeper discussion of their nature, particularly in comparision with </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">confidence intervals.</span></span></p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.101.1">How is it different from a confidence interval?</span></h2>
<p><span class="koboSpan" id="kobo.102.1">This distinction is crucial. </span><span class="koboSpan" id="kobo.102.2">A confidence interval pertains to the uncertainty regarding a population </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.103.1">parameter based on a sample statistic. </span><span class="koboSpan" id="kobo.103.2">For instance, we might use a confidence interval to estimate the mean value of a population based on a sample mean. </span><span class="koboSpan" id="kobo.103.3">On the other hand, a prediction interval is about predicting a single future observation and quantifying the uncertainty around that </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">individual prediction.</span></span></p>
<h3><span class="koboSpan" id="kobo.105.1">The components of a prediction interval</span></h3>
<p><span class="koboSpan" id="kobo.106.1">A prediction </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.107.1">interval typically has the </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">following components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.109.1">Lower bound</span></strong><span class="koboSpan" id="kobo.110.1">: The </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.111.1">minimum value within the </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">predicted range.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">Upper bound</span></strong><span class="koboSpan" id="kobo.114.1">: The </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.115.1">maximum value within the </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">expected range.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.117.1">Coverage probability</span></strong><span class="koboSpan" id="kobo.118.1">: The </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.119.1">probability of the actual outcome falling within the prediction interval. </span><span class="koboSpan" id="kobo.119.2">Commonly used probabilities are 90%, 95%, </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">and 99%.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.121.1">There are various approaches to producing prediction intervals. </span><span class="koboSpan" id="kobo.121.2">Quantifying uncertainty in regression </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.122.1">models is crucial for understanding the reliability of predictions. </span><span class="koboSpan" id="kobo.122.2">Here are some of the most </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">used techniques:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.124.1">Confidence and prediction intervals</span></strong><span class="koboSpan" id="kobo.125.1">: These are fundamental techniques for quantifying uncertainty. </span><span class="koboSpan" id="kobo.125.2">Confidence intervals provide a range of values where we expect the true regression parameters to fall, given a certain confidence level. </span><span class="koboSpan" id="kobo.125.3">Prediction intervals, on the other hand, give a range for predicting a new observation, incorporating both the uncertainty in the estimate of the mean function and the randomness of the </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">new observation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.127.1">Resampling methods</span></strong><span class="koboSpan" id="kobo.128.1">: Techniques such as bootstrapping and cross-validation can provide empirical estimates of model uncertainty. </span><span class="koboSpan" id="kobo.128.2">Bootstrapping, for example, involves repeatedly sampling from the dataset with replacement and recalculating the regression estimates to get an empirical distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">the estimates.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.130.1">Bayesian methods</span></strong><span class="koboSpan" id="kobo.131.1">: Bayesian regression analysis provides a probabilistic framework </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.132.1">for quantifying uncertainty. </span><span class="koboSpan" id="kobo.132.2">Instead of single-point estimates, Bayesian regression gives a posterior distribution for the model parameters, which can be used to construct </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">prediction intervals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.134.1">Conformal prediction</span></strong><span class="koboSpan" id="kobo.135.1">: Conformal prediction is a more recent approach that measures the certainty of predictions made by machine learning algorithms. </span><span class="koboSpan" id="kobo.135.2">It builds prediction regions that attain valid coverage in finite samples without making assumptions about </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">data distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.137.1">Quantile regression</span></strong><span class="koboSpan" id="kobo.138.1">: Unlike standard regression techniques, which model the conditional mean of the response variable given specific values of predictor variables, quantile regression models the conditional median or other quantiles. </span><span class="koboSpan" id="kobo.138.2">It can provide a more comprehensive view of the possible outcomes and their </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">associated probabilities.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.140.1">Monte Carlo methods</span></strong><span class="koboSpan" id="kobo.141.1">: Monte Carlo methods are a class of computational algorithms that use random sampling to obtain numerical results. </span><span class="koboSpan" id="kobo.141.2">In the context of uncertainty quantification, Monte Carlo methods can be used to propagate the uncertainties from the input variables to the </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">response variable.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.143.1">Sensitivity analysis</span></strong><span class="koboSpan" id="kobo.144.1">: Sensitivity analysis is a technique that’s used to determine how different values of an independent variable will impact a particular dependent variable under a given set of assumptions. </span><span class="koboSpan" id="kobo.144.2">This technique is used within specific boundaries that depend on one or more </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">input variables.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.146.1">Understanding the inherent value and significance of prediction intervals makes it vital to discern the methodologies and tools to help us generate them. </span><span class="koboSpan" id="kobo.146.2">While traditional statistical methods have their merits, the dynamic landscape of data-driven industries necessitates </span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.147.1">more adaptive and reliable techniques. </span><span class="koboSpan" id="kobo.147.2">Conformal prediction, with its roots grounded in algorithmic randomness and validity, offers an enticing approach. </span><span class="koboSpan" id="kobo.147.3">As we transition into the next section, we will explore how conformal prediction tailors itself to regression problems, ensuring that our prediction intervals are accurate and theoretically sound. </span><span class="koboSpan" id="kobo.147.4">Let’s dive in and unveil the intricacies of conformal prediction in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">of regression.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.149.1">Conformal prediction for regression problems</span></h1>
<p><span class="koboSpan" id="kobo.150.1">In the </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.151.1">preceding chapters, we investigated </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.152.1">the numerous advantages that conformal prediction provides. </span><span class="koboSpan" id="kobo.152.2">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.154.1">Validity and calibration</span></strong><span class="koboSpan" id="kobo.155.1">: Conformal prediction maintains its validity and calibration, irrespective of the dataset’s size. </span><span class="koboSpan" id="kobo.155.2">This makes it a robust method for prediction across different </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">dataset sizes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.157.1">Distribution-free nature</span></strong><span class="koboSpan" id="kobo.158.1">: One of the significant benefits of conformal prediction is its distribution-free nature. </span><span class="koboSpan" id="kobo.158.2">It makes no specific assumptions about the underlying data distribution, making it a flexible and versatile tool for many </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">prediction problems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Compatibility with various predictors</span></strong><span class="koboSpan" id="kobo.161.1">: Conformal prediction can seamlessly integrate with any point predictor, irrespective of its nature. </span><span class="koboSpan" id="kobo.161.2">This property enhances its adaptability and widens its scope of application in </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">diverse domains.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.163.1">Non-intrusiveness</span></strong><span class="koboSpan" id="kobo.164.1">: The conformal prediction framework is non-intrusive, implying that it does not interfere with or alter the original prediction model. </span><span class="koboSpan" id="kobo.164.2">Instead, it is an additional layer that quantifies uncertainty, providing a holistic perspective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">model’s predictions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.166.1">In regression analysis, one paramount concern is the validity and calibration of various uncertainty quantification methods. </span><span class="koboSpan" id="kobo.166.2">These qualities are particularly essential in generating prediction intervals, where it’s expected that these intervals provide coverage </span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.167.1">that matches the specified confidence level, which can sometimes </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">be challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">In the </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.170.1">comprehensive study titled </span><em class="italic"><span class="koboSpan" id="kobo.171.1">Valid prediction intervals for regression problems</span></em><span class="koboSpan" id="kobo.172.1">, by Nicolas Dewolf, Bernard De Baets, and Willem Waegeman (</span><a href="https://arxiv.org/abs/2107.00363"><span class="koboSpan" id="kobo.173.1">https://arxiv.org/abs/2107.00363</span></a><span class="koboSpan" id="kobo.174.1">), the authors explored four broad categories of methods designed for estimating prediction intervals in regression scenarios – Bayesian methods, ensemble methods, direct estimation methods, and conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">prediction methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.176.1">Bayesian methods</span></strong><span class="koboSpan" id="kobo.177.1">: This category encompasses techniques that use Bayes’ theorem </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.178.1">to predict the posterior probability of the intervals. </span><span class="koboSpan" id="kobo.178.2">These methods can give robust prediction intervals by modeling the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">output distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.180.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.181.1">: Ensemble </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.182.1">methods such as random forest or bagging can generate prediction intervals by leveraging the variability among individual models in </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">the ensemble.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.184.1">Direct estimation methods</span></strong><span class="koboSpan" id="kobo.185.1">: These </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.186.1">techniques involve the direct calculation of prediction intervals. </span><span class="koboSpan" id="kobo.186.2">They often necessitate specific assumptions about the underlying data or the </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">error distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Conformal prediction methods</span></strong><span class="koboSpan" id="kobo.189.1">: Conformal prediction stands out for its distribution-free </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.190.1">nature and ability to provide valid prediction intervals across </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">diverse scenarios.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.192.1">The authors of this study underscore that the adoption of artificial intelligence systems by humans is significantly tied to the reliability these systems can offer. </span><span class="koboSpan" id="kobo.192.2">The reliability here refers not only to producing accurate point predictions but also to the system’s ability to gauge and communicate its uncertainty level accurately. </span><span class="koboSpan" id="kobo.192.3">Thus, these systems should be adept at highlighting their areas of knowledge and limitations, particularly the aspects they need </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">clarification on.</span></span></p>
<p><span class="koboSpan" id="kobo.194.1">Converting uncertainties or “what they do not know” becomes even more crucial in real-world applications, where predictions often drive significant decisions. </span><span class="koboSpan" id="kobo.194.2">Therefore, uncertainty quantification aids in making informed and risk-aware decisions, contributing to the broader acceptance and trust in artificial </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">intelligence systems.</span></span></p>
<p><span class="koboSpan" id="kobo.196.1">Calibration refers to the degree to which the actual coverage of a prediction interval matches its nominal coverage level. </span><span class="koboSpan" id="kobo.196.2">In other words, a prediction interval is well-calibrated if </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.197.1">it contains the true value of the response variable with the expected frequency. </span><span class="koboSpan" id="kobo.197.2">Calibration is essential because it ensures that </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.198.1">the prediction intervals are not too narrow or too broad and that they provide accurate information about the uncertainty associated with the predictions. </span><span class="koboSpan" id="kobo.198.2">In this study, the authors use conformal prediction as a general calibration procedure to ensure the prediction intervals </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">are well-calibrated.</span></span></p>
<p><span class="koboSpan" id="kobo.200.1">Methods other than conformal prediction can suffer from calibration issues because they may make assumptions about the distribution of the errors or the model parameters that do not hold in practice. </span><span class="koboSpan" id="kobo.200.2">For example, Bayesian methods might assume that the errors are normally distributed with a fixed variance, which may not be the case. </span><span class="koboSpan" id="kobo.200.3">Ensemble methods, on the other hand, may not consider the correlation between the predictions of the individual models in the ensemble. </span><span class="koboSpan" id="kobo.200.4">Outliers or other noise sources in the data may also affect direct estimation methods. </span><span class="koboSpan" id="kobo.200.5">In addition, some interval estimators can, at best, be asymptotically valid; since this is only guaranteed for infinitely large datasets, there is no guarantee that it will hold for real datasets of final size, especially for medium-sized and </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">smaller datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">These issues can lead to prediction intervals that are poorly calibrated, meaning that they do not provide accurate information about the uncertainty associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">the predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">Conversely, conformal prediction is a non-parametric method that does not make any assumptions about the distribution of the errors or the model parameters. </span><span class="koboSpan" id="kobo.204.2">Instead, it uses the data to construct prediction intervals that are guaranteed to be well-calibrated, regardless of the underlying distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">the errors.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">The following table provides a summary of the characteristics of the four classes of methods for constructing prediction intervals in a </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">regression setting:</span></span></p>
<table class="T---Table _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.208.1">Method</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.209.1">Marginal Validity</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.210.1">Scalability</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.211.1">Domain Knowledge</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.212.1">Validation Set</span></strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.213.1">Bayesian methods</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.214.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.215.1">Only scalable with </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">approximate inference</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.217.1">Yes</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.218.1">No</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.219.1">Ensemble methods</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.220.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.221.1">Yes (when scalable models </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">are used)</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.223.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.224.1">No</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.225.1">Direct </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">interval estimation</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.227.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.228.1">Yes</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.229.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.230.1">Yes</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.231.1">Conformal prediction</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.232.1">Yes</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.233.1">Yes (</span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">for ICP)</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.235.1">No</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.236.1">Yes</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.237.1">Table 7.1 – Summary of the uncertainty quantification methods</span></p>
<p><span class="koboSpan" id="kobo.238.1">For each </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.239.1">class of methods, this table indicates whether the method has marginal validity (meaning that it does not require </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.240.1">any assumptions about the distribution of the errors or the model parameters), whether it is scalable (meaning that it can be applied to large datasets), whether it requires domain knowledge (meaning that it requires knowledge of the specific problem domain), and whether it requires a validation set (meaning that it requires a separate dataset to evaluate the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">the method).</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">This table shows </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.244.1">Bayesian methods do not have marginal validity, are only scalable with approximate inference, require domain knowledge, and do not require a </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">validation set</span></span></li>
<li><span class="koboSpan" id="kobo.246.1">Ensemble methods do not have marginal validity, are scalable, do not require domain knowledge, and do not require a </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">validation set</span></span></li>
<li><span class="koboSpan" id="kobo.248.1">Direct interval estimation methods do not have marginal validity, are scalable, do not require domain knowledge, and require a </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">validation set</span></span></li>
<li><span class="koboSpan" id="kobo.250.1">Conformal prediction has marginal validity, is scalable, does not require domain knowledge, and </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.251.1">requires a validation set (in the </span><strong class="bold"><span class="koboSpan" id="kobo.252.1">inductive conformal prediction</span></strong><span class="koboSpan" id="kobo.253.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.254.1">ICP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">) version)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.256.1">Several types of prediction interval estimators for regression problems were reviewed and compared, </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Bayesian methods</span></strong><span class="koboSpan" id="kobo.259.1">: Gaussian </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.260.1">process and </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">approximate GP</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.262.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.263.1">: Dropout </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.264.1">ensemble, deep ensembles, and </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">mean-variance estimator</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.266.1">Direct interval estimation methods</span></strong><span class="koboSpan" id="kobo.267.1">: Neural </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.268.1">network </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">quantile regression</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.270.1">Conformal prediction</span></strong><span class="koboSpan" id="kobo.271.1">: Neural </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.272.1">networks and </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">random forest</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.274.1">This comparison was based on two main properties – the </span><em class="italic"><span class="koboSpan" id="kobo.275.1">coverage degree</span></em><span class="koboSpan" id="kobo.276.1"> and the </span><em class="italic"><span class="koboSpan" id="kobo.277.1">average width of the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.278.1">prediction intervals</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">Before calibration, the performance of prediction interval estimators varied significantly across different benchmark datasets, with large fluctuations in performance from one dataset to another. </span><span class="koboSpan" id="kobo.280.2">This is due to the violation of certain assumptions that are inherent to some </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.281.1">classes of methods. </span><span class="koboSpan" id="kobo.281.2">For example, some methods may perform well on datasets with normally distributed errors, but poorly </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.282.1">on datasets with strongly skewed errors. </span><span class="koboSpan" id="kobo.282.2">Similarly, some methods may perform well on datasets with low levels of noise, but poorly on datasets with high levels </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">of noise.</span></span></p>
<p><span class="koboSpan" id="kobo.284.1">The paper also found that the performance of the different methods for constructing prediction intervals depends on several factors, such as the size of the dataset, the complexity of the model, and the degree of skewness in </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.286.1">For example, the paper found that Bayesian methods tend to perform better on small datasets, while ensemble methods tend to perform well on large datasets. </span><span class="koboSpan" id="kobo.286.2">The paper also found that the mean-variance estimator, which is a type of ensemble method, can be sensitive to the normality assumption and may perform poorly on strongly </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">skewed datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">Finally, the paper found that direct interval estimation methods, such as neural network quantile regression, can be computationally expensive and may require many training samples to achieve </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">good performance.</span></span></p>
<p><span class="koboSpan" id="kobo.290.1">The conformal prediction framework was used for post-hoc calibration, and it was found that all methods attained the desired coverage after calibration, and in certain cases, the calibrated models even produced intervals with a smaller </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">average width.</span></span></p>
<p><span class="koboSpan" id="kobo.292.1">The authors illustrated how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step and show that it can improve the performance of these methods on a wide range of datasets, making conformal prediction a promising framework for constructing prediction intervals in </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.293.1">a regression setting. </span><span class="koboSpan" id="kobo.293.2">In particular, the paper showed that conformal prediction can be used as a general calibration </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.294.1">procedure for methods that deliver poor results without a calibration step. </span><span class="koboSpan" id="kobo.294.2">The paper also found that conformal prediction has several significant advantages over other methods for constructing prediction intervals. </span><span class="koboSpan" id="kobo.294.3">For example, conformal prediction has marginal validity, meaning that it does not require any assumptions about the distribution of the errors or the model parameters. </span><span class="koboSpan" id="kobo.294.4">Additionally, conformal prediction is scalable, meaning that it can be applied to large datasets. </span><span class="koboSpan" id="kobo.294.5">Finally, conformal prediction does not require domain knowledge, meaning that it does not require knowledge of the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">problem domain.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">Conformal prediction is a powerful framework for quantifying uncertainty in machine learning predictions. </span><span class="koboSpan" id="kobo.296.2">Its principles can be applied to various types of problems involving uncertainty quantification, including regression, which involves predicting a continuous </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">output variable.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">When applied to regression problems, conformal prediction provides prediction intervals rather than point predictions. </span><span class="koboSpan" id="kobo.298.2">These prediction intervals offer a range of plausible values for the target variable, with a specified confidence level. </span><span class="koboSpan" id="kobo.298.3">For example, a 95% prediction interval indicates that we can be 95% confident that the true value of the target variable falls within </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">this range.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">The most compelling feature of conformal prediction in regression scenarios is its validity, which refers to the fact that if we claim a 95% confidence level, the true value will indeed be in the prediction interval 95% of the time. </span><span class="koboSpan" id="kobo.300.2">Importantly, this validity is assured for unseen test data, any sample size (not just for large or infinite samples), and any underlying point </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">regressor model.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">Moreover, conformal prediction for regression is non-parametric, which means it does not make any specific assumptions about the underlying data distribution. </span><span class="koboSpan" id="kobo.302.2">This makes it broadly applicable across different types of regression problems </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">and datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">To implement conformal prediction for regression, we need a nonconformity measure, which quantifies how much each observation deviates from the norm. </span><span class="koboSpan" id="kobo.304.2">Common choices for regression </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.305.1">include the absolute residuals. </span><span class="koboSpan" id="kobo.305.2">Once we </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.306.1">have the nonconformity scores, we can generate prediction intervals based on the ordered nonconformity scores of the </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">calibration set.</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">Different variants of conformal prediction can be used in regression, such as transductive (full) conformal prediction, ICP, which is more computationally efficient than classical TCP, jackknife+, and cross-conformal methods, which offer improved precision </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">and robustness.</span></span></p>
<p><span class="koboSpan" id="kobo.310.1">Overall, conformal prediction provides a flexible, robust, and reliable approach to uncertainty quantification in regression problems, offering valid and well-calibrated </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">prediction intervals.</span></span></p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.312.1">Building prediction intervals and predictive distributions using conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.313.1">ICP is a computationally efficient variant of the original transductive conformal prediction </span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.314.1">framework. </span><span class="koboSpan" id="kobo.314.2">Like all other models from the conformal prediction family, ICP is model-agnostic in terms of the underlying point prediction </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.315.1">model and data </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.316.1">distribution and comes with automatic validity guarantees for final samples of </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">any size.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">The key advantage of ICP compared to the original variant of conformal prediction (transductive conformal prediction) is that ICP requires training the underlying regression model only once, leading to efficient computations during the calibration and prediction phases. </span><span class="koboSpan" id="kobo.318.2">ICP is highly computationally efficient as the conformal layer requires very little additional computation overhead compared to training the </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">underlying model.</span></span></p>
<p><span class="koboSpan" id="kobo.320.1">The ICP process involves splitting the dataset into a proper training set and a calibration set. </span><span class="koboSpan" id="kobo.320.2">The training set is used to create the initial point prediction model, while the calibration set is utilized to calculate conformity scores and produce the prediction intervals of the </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">unseen points.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">ICP </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.323.1">automatically guarantees </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.324.1">validity and ensures that prediction intervals include actual test points with a specified degree of </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">selected coverage.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">ICP’s efficiency and flexibility have made it a popular choice for uncertainty estimation in </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">various applications.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">We will use the notebook at https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb to illustrate how to </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">use ICP:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.330.1"><img alt="Figure 7.1 – Predictions by ﻿RandomForestRegressor on the test set" src="image/B19925_07_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.331.1">Figure 7.1 – Predictions by RandomForestRegressor on the test set</span></p>
<p><span class="koboSpan" id="kobo.332.1">Let’s </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.333.1">generate prediction </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.334.1">intervals by utilizing </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">RandomForestRegressor</span></strong><span class="koboSpan" id="kobo.336.1"> as the core predictive model, and employ ICP to transform the point forecasts made by the base machine learning model into well-calibrated </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">prediction intervals:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.338.1">Model training</span></strong><span class="koboSpan" id="kobo.339.1">: Initiate the process by training the chosen model using the proper </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">training dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.341.1">model = RandomForestRegressor(n_jobs=-1)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.342.1">model.fit(X_proper_train, y_proper_train)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.343.1">y_pred_calib = model.predict(X_calib)</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.344.1">Making predictions</span></strong><span class="koboSpan" id="kobo.345.1">: Utilize the trained model to generate predictions on the calibration and </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">test datasets:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.347.1">y_pred_calib = model.predict(X_calib)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.348.1">y_pred_test = model.predict(X_test)</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.349.1">Nonconformity metric calculation</span></strong><span class="koboSpan" id="kobo.350.1">: For every observation within the calibration set, compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">nonconformity metric:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.352.1">y_calib_error = np.abs(y_calib - y_pred_calib)</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.353.1">Quantile calculation</span></strong><span class="koboSpan" id="kobo.354.1">: Determine the quantile of the nonconformity metrics using </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.355.1">the formula </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.356.1">designed for final sample correction. </span><span class="koboSpan" id="kobo.356.2">The formula includes the final sample </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">correction factor:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.358.1">q_yhat_calib = np.quantile(y_calib_error,np.ceil((n_calib+1)*(1-alpha))/n_calib)</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.359.1">Form prediction intervals</span></strong><span class="koboSpan" id="kobo.360.1">: Using the quantile you calculated in the previous step, establish prediction intervals for the test set. </span><span class="koboSpan" id="kobo.360.2">These intervals should be based on the point predictions made by the </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">underlying model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.362.1">y_hat_test_lower = y_pred_test - q_yhat_calib</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.363.1">y_hat_test_upper = y_pred_test + q_yhat_calib</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.364.1">We can illustrate the prediction outcomes on a plot that encompasses point estimates, actual values, and prediction intervals yielded </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">by ICP:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.366.1"><img alt="Figure 7.2 – Actual versus predicted values with a prediction interval" src="image/B19925_07_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.367.1">Figure 7.2 – Actual versus predicted values with a prediction interval</span></p>
<p><span class="koboSpan" id="kobo.368.1">We can </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.369.1">confirm that in ICP, the </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.370.1">width of the prediction interval remains constant (</span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">as designed).</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">ICP for regression offers several advantages and a few drawbacks. </span><span class="koboSpan" id="kobo.372.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">consider these.</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">We’ll </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.375.1">cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">advantages first:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.377.1">Model-agnostic</span></strong><span class="koboSpan" id="kobo.378.1">: ICP can be applied to any existing regression model. </span><span class="koboSpan" id="kobo.378.2">This means it’s a flexible method that can be used to enhance a wide variety of </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">regression models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.380.1">Efficiency</span></strong><span class="koboSpan" id="kobo.381.1">: ICP only requires the underlying regression model to be trained once, leading to efficient computations during the calibration and prediction phases. </span><span class="koboSpan" id="kobo.381.2">This makes it computationally more efficient compared to the original transductive conformal prediction framework, which requires the model to be retrained for every </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">new prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.383.1">Validity</span></strong><span class="koboSpan" id="kobo.384.1">: ICP comes with automatic validity guarantees. </span><span class="koboSpan" id="kobo.384.2">If the data distribution is exchangeable (that is, the order of the data points does not matter), then the prediction </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.385.1">intervals produced by ICP will have the desired </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">coverage probability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.387.1">Distribution-free</span></strong><span class="koboSpan" id="kobo.388.1">: ICP does not make any assumptions about the distribution of the data. </span><span class="koboSpan" id="kobo.388.2">This means it can be applied even when the data does not follow any known </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">statistical distribution.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.390.1">Now, let’s </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.391.1">cover </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">the disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.393.1">Performance dependency</span></strong><span class="koboSpan" id="kobo.394.1">: The effectiveness of ICP relies heavily on the performance of the underlying regression model. </span><span class="koboSpan" id="kobo.394.2">If the underlying model does not fit the data well, the prediction intervals produced by ICP may be </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">too wide.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.396.1">Assumption of exchangeability</span></strong><span class="koboSpan" id="kobo.397.1">: ICP’s validity guarantees depend on the assumption of exchangeability, which may not hold in many real-world scenarios (for example, when dealing with time </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">series data).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.399.1">Lack of adaptivity</span></strong><span class="koboSpan" id="kobo.400.1">: Unlike other conformal prediction methods, such as </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">conformalized quantile regression</span></strong><span class="koboSpan" id="kobo.402.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.403.1">CQR</span></strong><span class="koboSpan" id="kobo.404.1">) and jackknife+ methods, which we’ll </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.405.1">cover later in this chapter, ICP isn’t inherently adaptive. </span><span class="koboSpan" id="kobo.405.2">It does not dynamically adjust to the data’s complexity or structure. </span><span class="koboSpan" id="kobo.405.3">For example, it won’t naturally produce narrower intervals in regions of the data where the model is more confident and wider intervals where the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">less confident.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.407.1">In the </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.408.1">ensuing section, we’ll deep dive into one of the most popular conformal prediction models – CQR. </span><span class="koboSpan" id="kobo.408.2">This is </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.409.1">a sophisticated technique that amalgamates conformal prediction’s robustness with the precision of quantile regression. </span><span class="koboSpan" id="kobo.409.2">This fusion facilitates the generation of reliable prediction intervals and ensures that these intervals are optimally tuned to encapsulate the true values with a high degree of certainty. </span><span class="koboSpan" id="kobo.409.3">By harnessing the strengths of both conformal prediction and quantile regression, CQR emerges as a formidable tool for constructing well-calibrated prediction intervals, thereby augmenting the interpretability and trustworthiness of predictive models. </span><span class="koboSpan" id="kobo.409.4">As we traverse this section, we will unravel the mechanics of CQR, elucidate its advantages, and illustrate its application in real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">predictive scenarios.</span></span></p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.411.1">Mechanics of CQR</span></h1>
<p><span class="koboSpan" id="kobo.412.1">In the previous section, we observed that ICP generates prediction intervals of uniform width. </span><span class="koboSpan" id="kobo.412.2">Consequently, it doesn’t adjust adaptively to heteroscedastic data, where the variability </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.413.1">of the response variable isn’t constant across different regions of </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.415.1">In many cases, not only it is crucial to ensure valid coverage in final samples but it is also beneficial to generate the most concise prediction intervals for each point within the input space. </span><span class="koboSpan" id="kobo.415.2">This helps maintain the informativeness of these intervals. </span><span class="koboSpan" id="kobo.415.3">When dealing with heteroscedastic data, the model should be capable of adjusting the length of prediction intervals to match the local variability associated with each point in the </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">feature space.</span></span></p>
<p><span class="koboSpan" id="kobo.417.1">CQR (developed by Yaniv Romano, Evan Patterson, and Emmanuel Candes and published in the paper </span><em class="italic"><span class="koboSpan" id="kobo.418.1">Conformalized Quantile Regression</span></em><span class="koboSpan" id="kobo.419.1"> (</span><a href="https://arxiv.org/abs/1905.03222"><span class="koboSpan" id="kobo.420.1">https://arxiv.org/abs/1905.03222</span></a><span class="koboSpan" id="kobo.421.1">)) is one of the most popular and widely adopted conformal prediction models. </span><span class="koboSpan" id="kobo.421.2">It was specifically designed to address the need for adaptivity by employing quantile regression as the underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">regression model.</span></span></p>
<p><span class="koboSpan" id="kobo.423.1">Roger Koenker developed quantile regression – a statistical method that estimates the conditional quantiles of a response variable given a set of features. </span><span class="koboSpan" id="kobo.423.2">Unlike classical regression, which focuses on estimating the conditional mean, quantile regression provides a more complete picture of the relationship between the predictors and the response by estimating specified quantiles. </span><span class="koboSpan" id="kobo.423.3">Quantile regression provides adaptivity by allowing quantiles to adjust to the local variability of the data. </span><span class="koboSpan" id="kobo.423.4">This is particularly important when the data is heteroscedastic, meaning that the variance of the response variable changes across the range of the predictors. </span><span class="koboSpan" id="kobo.423.5">However, unlike models in the conformal prediction framework, quantile regression does not have automatic </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">validity guarantees.</span></span></p>
<p><span class="koboSpan" id="kobo.425.1">By combining the concept of quantile regression with the conformal prediction framework, CQR inherits the distribution-free validity guarantees for finite samples from conformal prediction, as well as the statistical efficiency and adaptivity of </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">quantile regression.</span></span></p>
<p><span class="koboSpan" id="kobo.427.1">Combining conformal prediction with quantile regression to create CQR provides several advantages </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.428.1">over </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">existing methods:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.430.1">Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. </span><span class="koboSpan" id="kobo.430.2">This means that the prediction intervals produced by CQR are guaranteed to contain the true response value with a certain probability, regardless of the underlying distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">the data.</span></span></li>
<li><span class="koboSpan" id="kobo.432.1">Quantile regression provides a flexible and efficient way to estimate the conditional quantiles of the response variable, which allows CQR to adjust the length of the prediction intervals to the local variability of the data. </span><span class="koboSpan" id="kobo.432.2">This adaptivity is particularly important when the data is heteroscedastic, meaning that the variance of the response variable changes across the range of the predictors. </span><span class="koboSpan" id="kobo.432.3">By estimating the conditional quantiles at each query point in predictor space, CQR can construct prediction intervals that are shorter and more informative than those obtained from classical </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">regression methods.</span></span></li>
<li><span class="koboSpan" id="kobo.434.1">According to the results published in the paper </span><em class="italic"><span class="koboSpan" id="kobo.435.1">Conformalized Quantile Regression</span></em><span class="koboSpan" id="kobo.436.1">, CQR tends to produce shorter intervals than ICP and is adaptive. </span><span class="koboSpan" id="kobo.436.2">This is because CQR can adjust the length of the prediction intervals to the local variability of the data using </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">quantile regression.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.438.1">We will now describe the mechanics of CRQ, starting with classical </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">quantile regression.</span></span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.440.1">Quantile regression</span></h2>
<p><span class="koboSpan" id="kobo.441.1">Quantile regression is a statistical method that estimates the conditional quantiles of a response </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.442.1">variable given a set of predictor variables (features). </span><span class="koboSpan" id="kobo.442.2">Unlike classical regression, which focuses on estimating </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.443.1">the conditional mean, quantile regression provides a more complete picture of the relationship between the predictors and the response by estimating the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">conditional distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.445.1">At a high level, quantile regression minimizes a loss function that measures the difference between the observed response values and the predicted quantiles. </span><span class="koboSpan" id="kobo.445.2">The loss function that’s used in quantile regression is typically pinball loss, a piecewise linear function that places more weight on the residuals at the specified </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">quantile level:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.447.1"><img alt="Figure 7.3 – Visualization of the pinball loss function, where y=y_hat" src="image/B19925_07_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.448.1">Figure 7.3 – Visualization of the pinball loss function, where y=y_hat</span></p>
<p><span class="koboSpan" id="kobo.449.1">Under the hood, quantile regression can be implemented using a variety of algorithms, including linear quantile regression, neural networks, quantile random forest, and gradient </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">boosting methods.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">One common </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.452.1">strategy for estimating uncertainty with quantile </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.453.1">regression involves calculating the lower (</span><em class="italic"><span class="koboSpan" id="kobo.454.1">q_lo</span></em><span class="koboSpan" id="kobo.455.1">) and upper (</span><em class="italic"><span class="koboSpan" id="kobo.456.1">q_hi</span></em><span class="koboSpan" id="kobo.457.1">) quantiles for each X value in the test dataset, and then outputting [</span><em class="italic"><span class="koboSpan" id="kobo.458.1">q_lo, q_hi</span></em><span class="koboSpan" id="kobo.459.1">] as a prediction interval. </span><span class="koboSpan" id="kobo.459.2">While this method can sometimes perform well and adapt to heteroscedasticity, it doesn’t come with guarantees for achieving the desired coverage. </span><span class="koboSpan" id="kobo.459.3">Without such assurance for final samples, the outcomes of using such strategies could be catastrophic, particularly in critical applications such as healthcare, finance, and autonomous vehicles. </span><span class="koboSpan" id="kobo.459.4">This is confirmed by the results of the experiments in the paper, which show that prediction intervals produced by neural networks can significantly undercover </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">actual values.</span></span></p>
<p><span class="koboSpan" id="kobo.461.1">Several methods exist to provide asymptotic consistent results for quantile regression, including for related methods such as quantile random forests. </span><span class="koboSpan" id="kobo.461.2">However, none of these methods provide validity guarantees in </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">final samples.</span></span></p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.463.1">CQR</span></h2>
<p><span class="koboSpan" id="kobo.464.1">In previous chapters, we discussed how conformal prediction, in both its transductive and inductive forms, can offer validity guarantees in final samples. </span><span class="koboSpan" id="kobo.464.2">While both variants of conformal </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.465.1">prediction can be applied to quantile regression, considering the prevalent use of ICP, we will focus solely on the application of ICP in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">of CQR.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">The following figure from </span><a href="https://arxiv.org/abs/1905.03222"><span class="koboSpan" id="kobo.468.1">https://arxiv.org/abs/1905.03222</span></a><span class="koboSpan" id="kobo.469.1"> presents a comparison between the prediction intervals produced by the standard ICP model (known as split-conformal), its locally adaptive variant, </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">and CQR:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.471.1"><img alt="Figure 7.4 – Prediction intervals on simulated heteroscedastic data with outliers – (a) the standard split conformal method, (b) its locally adaptive variant, and (c) CQR. The interval length as a function of X is shown in (d). The target coverage rate is 90%" src="image/B19925_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.472.1">Figure 7.4 – Prediction intervals on simulated heteroscedastic data with outliers – (a) the standard split conformal method, (b) its locally adaptive variant, and (c) CQR. </span><span class="koboSpan" id="kobo.472.2">The interval length as a function of X is shown in (d). </span><span class="koboSpan" id="kobo.472.3">The target coverage rate is 90%</span></p>
<p><span class="koboSpan" id="kobo.473.1">The heteroscedastic dataset, which contains outliers, is modeled by three different methods, all achieving a user-specified coverage of 90%. </span><span class="koboSpan" id="kobo.473.2">As we discussed when we talked about ICP, ICP (split-conformal) generates prediction intervals of a constant width, indicating its non-adaptive nature. </span><span class="koboSpan" id="kobo.473.3">The locally weighted variant of ICP shows partial </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.474.1">adaptivity. </span><span class="koboSpan" id="kobo.474.2">CQR, however, is fully adaptive and notably produces prediction intervals with the shortest </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">average length.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">Let’s describe the steps involved </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">in CQR:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.478.1">Split the data into a proper training set and a calibration set. </span><span class="koboSpan" id="kobo.478.2">The proper training set is used to fit the quantile regression model, while the calibration set is used to construct the </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">prediction intervals.</span></span></li>
<li><span class="koboSpan" id="kobo.480.1">Fit the quantile regression model to the proper training set using any algorithm for quantile regression, such as random forest or deep neural networks. </span><span class="koboSpan" id="kobo.480.2">This step involves estimating the conditional quantiles of the response variable given the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">predictor variables.</span></span></li>
<li><span class="koboSpan" id="kobo.482.1">Compute conformity scores that quantify the errors made by the prediction interval obtained from quantile regression [</span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">q_lo</span></strong><span class="koboSpan" id="kobo.484.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">q_hi</span></strong><span class="koboSpan" id="kobo.486.1">] by computing nonconformity scores using </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.487.1">E</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.488.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.489.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.490.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.491.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.492.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.493.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.494.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.495.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.496.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.497.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.498.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.499.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.500.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.501.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.502.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.503.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.504.1">l</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.505.1">o</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.506.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.507.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.508.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.509.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.510.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.511.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.512.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.513.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.514.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.515.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.516.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.517.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.518.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.519.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.520.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.521.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.522.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.523.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.524.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.525.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.526.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.527.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.528.1">h</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.529.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.530.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.531.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.532.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.533.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.534.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.535.1">}</span></span><span class="koboSpan" id="kobo.536.1">. </span><span class="koboSpan" id="kobo.536.2">The nonconformity score can be interpreted as follows – if the actual label, y, falls below the lower bound of the interval, the nonconformity score corresponds to the distance from </span><em class="italic"><span class="koboSpan" id="kobo.537.1">y</span></em><span class="koboSpan" id="kobo.538.1"> to the lower bound. </span><span class="koboSpan" id="kobo.538.2">Conversely, if </span><em class="italic"><span class="koboSpan" id="kobo.539.1">y</span></em><span class="koboSpan" id="kobo.540.1"> exceeds the upper bound, the nonconformity score is the distance from </span><em class="italic"><span class="koboSpan" id="kobo.541.1">y</span></em><span class="koboSpan" id="kobo.542.1"> to the upper bound. </span><span class="koboSpan" id="kobo.542.2">However, if the actual label, </span><em class="italic"><span class="koboSpan" id="kobo.543.1">y</span></em><span class="koboSpan" id="kobo.544.1">, is within the interval, the nonconformity score is considered negative and corresponds to the distance from </span><em class="italic"><span class="koboSpan" id="kobo.545.1">y</span></em><span class="koboSpan" id="kobo.546.1"> to the </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">nearest bound.</span></span></li>
<li><span class="koboSpan" id="kobo.548.1">Compute the prediction intervals for each test point by using empirical quantiles of </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.549.1">E</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.550.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.551.1">i</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.553.1">The last step of the formula for prediction intervals is </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">as follows:</span></span></p><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.556.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.557.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.558.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.559.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.560.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.561.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.562.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.563.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.564.1">[</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.565.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.566.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.567.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.568.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.569.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.570.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.571.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.572.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.573.1">l</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.574.1">o</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.575.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.576.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.577.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.578.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.579.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.580.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.581.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.582.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.583.1">Q</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.584.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.585.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.586.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.587.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.588.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.589.1">E</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.590.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.591.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.592.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.593.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.594.1">)</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.595.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.596.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.597.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.598.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.599.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.600.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.601.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.602.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.603.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.604.1">h</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.605.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.606.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.607.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.608.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.609.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.610.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.611.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.612.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.613.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.614.1">Q</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.615.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.616.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.617.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.618.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.619.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.620.1">E</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.621.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.622.1">ℐ</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.623.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.624.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.625.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.626.1">]</span></span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.627.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.628.1">Q</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.629.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.630.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.631.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.632.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.633.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.634.1">E</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.635.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.636.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.637.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.638.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.639.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.640.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.641.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.642.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.643.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.644.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.645.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.646.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.647.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.648.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.649.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.650.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.651.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.652.1">|</span></span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.653.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.654.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.655.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.656.1">|</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.657.1">)</span></span><span class="koboSpan" id="kobo.658.1"> is the empirical quantile of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.659.1">E</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.660.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.661.1">i</span></span><span class="koboSpan" id="kobo.662.1">, </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.663.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.664.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.665.1">2</span></span><span class="koboSpan" id="kobo.666.1"> is the calibration set, and </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.667.1">|</span></span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.668.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.669.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.670.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.671.1">|</span></span><span class="koboSpan" id="kobo.672.1"> is just a mathematical notation for the number of elements in the </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">calibration set.</span></span></p></li>
</ol>
<p><span class="koboSpan" id="kobo.674.1">The conformal prediction idea here is similar to what we did in ICP – we simply use the calibration set to compute some form of nonconformity metric and then use the quantiles </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.675.1">of the nonconformity metric to produce uncertainty intervals around the point forecast from the trained point prediction model. </span><span class="koboSpan" id="kobo.675.2">The only difference is that instead of adjusting the point predictions produced by the regression model, we adjust quantiles produced by the underlying statistical, machine learning, or deep learning model. </span><span class="koboSpan" id="kobo.675.3">Otherwise, the ideas and the mechanism of quantile calculation are the same as in ICP, given the definition of the nonconformity metric that we </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">have described.</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">The key result in the paper is that if the data exchangeable, then the prediction interval constructed by the split (inductive) CQR algorithm satisfies the property of validity – that is, </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.678.1">ℙ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.679.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.680.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.681.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.682.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.683.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.684.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.685.1">∈</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.686.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.687.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.688.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.689.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.690.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.691.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.692.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.693.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.694.1">}</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.695.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.696.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.697.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.698.1">α</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.700.1">This is like in all other models in conformal prediction and says that given the user-specified confidence level, </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.701.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.702.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.703.1">α</span></span><span class="koboSpan" id="kobo.704.1">, the probability of having the actual value of </span><em class="italic"><span class="koboSpan" id="kobo.705.1">y</span></em><span class="koboSpan" id="kobo.706.1"> within the constructed prediction interval is guaranteed to exceed </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.707.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.708.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.709.1">α</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.710.1">.</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.711.1">So, if the specified user confidence is 90%, the actual points are guaranteed to lie within the prediction interval 90% of </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">the time.</span></span></p>
<p><span class="koboSpan" id="kobo.713.1">The additional bonus point is that if the nonconformity scores, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.714.1">E</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.715.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.716.1">i</span></span><span class="koboSpan" id="kobo.717.1">, are almost surely distinct, then the coverage is also bounded as </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.718.1">ℙ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.719.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.720.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.721.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.722.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.723.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.724.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.725.1">∈</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.726.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.727.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.728.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.729.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.730.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.731.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.732.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.733.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.734.1">}</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.735.1">≤</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.736.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.737.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.738.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.739.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.740.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.741.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.742.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.743.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.744.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.745.1">|</span></span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.746.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.747.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.748.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.749.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.750.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.751.1">1</span></span><span class="koboSpan" id="kobo.752.1">, where </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.753.1">|</span></span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.754.1">ℐ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.755.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.756.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.757.1">|</span></span><span class="koboSpan" id="kobo.758.1"> is the size of the calibration dataset. </span><span class="koboSpan" id="kobo.758.2">By way of example, for 500 points in the calibration dataset, the coverage is guaranteed to be bound between 90% </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">and ~90.2%.</span></span></p>
<p><span class="koboSpan" id="kobo.760.1">To summarize, CQR tends to produce shorter intervals compared to ICP. </span><span class="koboSpan" id="kobo.760.2">This is because CQR adjusts the length of the prediction intervals to the local variability of the data using quantile regression, which is particularly important when the data </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">is heteroscedastic.</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">Overall, the combination of conformal prediction and quantile regression in CQR provides a powerful and flexible framework for constructing prediction intervals that are both distribution-free and adaptive </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">to heteroscedasticity.</span></span></p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.764.1">Jackknife+</span></h2>
<p><span class="koboSpan" id="kobo.765.1">We will now describe another widely used conformal prediction method for regression, known as jackknife+. </span><span class="koboSpan" id="kobo.765.2">The </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.766.1">description of the jackknife+ technique aligns closely </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.767.1">with the details outlined in the seminal paper </span><em class="italic"><span class="koboSpan" id="kobo.768.1">Predictive Inference with the Jackknife+</span></em><span class="koboSpan" id="kobo.769.1">, where this method was </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">first introduced.</span></span></p>
<p><span class="koboSpan" id="kobo.771.1">We aim to fit a regression function to the training data, which consists of pairs of features (</span><em class="italic"><span class="koboSpan" id="kobo.772.1">X</span></em><em class="italic"><span class="koboSpan" id="kobo.773.1">i</span></em><span class="koboSpan" id="kobo.774.1">, </span><em class="italic"><span class="koboSpan" id="kobo.775.1">Y</span></em><em class="italic"><span class="koboSpan" id="kobo.776.1">i</span></em><span class="koboSpan" id="kobo.777.1">). </span><span class="koboSpan" id="kobo.777.2">Our goal is to predict the output, Y</span><span class="subscript"><span class="koboSpan" id="kobo.778.1">n+1</span></span><span class="koboSpan" id="kobo.779.1">, given a new feature vector, X</span><span class="subscript"><span class="koboSpan" id="kobo.780.1">n+1</span></span><span class="koboSpan" id="kobo.781.1">=x, and generate a corresponding uncertainty interval for this test point. </span><span class="koboSpan" id="kobo.781.2">We require that the interval contains the true Y</span><span class="subscript"><span class="koboSpan" id="kobo.782.1">n+1</span></span><span class="koboSpan" id="kobo.783.1"> given the specified target coverage – that is, </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.784.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.785.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.786.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.787.1">:</span></span> <span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.788.1">ℙ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.789.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.790.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.791.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.792.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.793.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.794.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.795.1">∈</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.796.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.797.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.798.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.799.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.800.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.801.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.802.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.803.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.804.1">}</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.805.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.806.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.807.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.808.1">α</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.810.1">A naive solution could be to use the residuals after fitting the underlying regressor on the training data, </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.811.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.812.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.813.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.814.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.815.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.816.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.817.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.818.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.819.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.820.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.821.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.822.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.823.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.824.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.825.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.826.1">|</span></span><span class="koboSpan" id="kobo.827.1">, and compute the quantile of the residuals to estimate the width of the prediction interval on the new test point, </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.829.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.830.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.831.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.832.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.833.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.834.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.835.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.836.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.837.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.838.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.839.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.840.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.841.1">±</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.842.1">(</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.843.1">the</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.844.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.845.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.846.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.847.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.848.1">)</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.849.1">quantile</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.850.1">of</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.851.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.852.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.853.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.854.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.855.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.856.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.857.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.858.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.859.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.860.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.861.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.862.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.863.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.864.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.865.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.866.1">|</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.867.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.868.1">…</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.869.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.870.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.871.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.872.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.873.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.874.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.875.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.876.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.877.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.878.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.879.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.880.1">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.881.1">X</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.882.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.883.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.884.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.885.1">|</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.886.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.887.1">However, in practice, such an approach will underestimate uncertainty due to overfitting as the residuals on the training set are typically smaller than residuals on unseen </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">test data.</span></span></p>
<p><span class="koboSpan" id="kobo.889.1">To circumvent overfitting, statisticians developed a robust technique known as the jackknife. </span><span class="koboSpan" id="kobo.889.2">This technique was originally designed to reduce bias and estimate variance. </span><span class="koboSpan" id="kobo.889.3">It functions by iteratively omitting one observation from the dataset and recalculating the model. </span><span class="koboSpan" id="kobo.889.4">This offers an empirical approach to evaluate the model’s stability and resilience to individual </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">data points:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.891.1"><img alt="Figure 7.5 – Illustration of the ﻿jackknife prediction method" src="image/B19925_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.892.1">Figure 7.5 – Illustration of the jackknife prediction method</span></p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.893.1">Jackknife regression</span></h2>
<p><span class="koboSpan" id="kobo.894.1">Each time jackknife regression is performed, it fits the model to all data points, excluding </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.895.1">the one specified by (X</span><span class="subscript"><span class="koboSpan" id="kobo.896.1">i</span></span><span class="koboSpan" id="kobo.897.1">, Y</span><span class="subscript"><span class="koboSpan" id="kobo.898.1">i</span></span><span class="koboSpan" id="kobo.899.1">). </span><span class="koboSpan" id="kobo.899.2">This process allows jackknife regression to estimate the leave-one-out residual </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.900.1">denoted by </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.901.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.902.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.903.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.904.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.905.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.906.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.907.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.908.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.909.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.910.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.911.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.912.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.913.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.914.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.915.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.916.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.917.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.918.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.919.1">|</span></span><span class="koboSpan" id="kobo.920.1">. </span><span class="koboSpan" id="kobo.920.2">By treating these leave-one-out residuals as nonconformity scores, we can estimate the </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.921.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.922.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.923.1">α</span></span><span class="koboSpan" id="kobo.924.1">  quantile and form prediction intervals similarly to ICP. </span><span class="koboSpan" id="kobo.924.2">Conceptually, this method is expected to achieve the desired coverage as it effectively tackles the issue of overfitting. </span><span class="koboSpan" id="kobo.924.3">This is because the residuals</span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.925.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.926.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.927.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.928.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.929.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.930.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.931.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.932.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.933.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.934.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.935.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.936.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.937.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.938.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.939.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.940.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.941.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.942.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.943.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.944.1">|</span></span><span class="koboSpan" id="kobo.945.1">, are computed in an out-of-sample manner, thus providing a more realistic evaluation of </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">However, the jackknife procedure does not have universal theoretical guarantees and can have poor coverage properties in certain cases, particularly when the data is highly skewed or has heavy tails. </span><span class="koboSpan" id="kobo.947.2">Although there are statistical results under asymptotic settings or assumptions of stability regarding the jackknife regression algorithm, it is clear that in situations where the jackknife estimator is unstable, the jackknife method can result in a loss of </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">predictive coverage.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">In some cases, the jackknife method can even have zero coverage, meaning that the true value is not contained in the estimated prediction interval. </span><span class="koboSpan" id="kobo.949.2">Additionally, the jackknife method can be computationally intensive as it requires fitting the model multiple times on </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">reduced datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.951.1">These challenges prompted the creation of an improved variant known as jackknife+. </span><span class="koboSpan" id="kobo.951.2">This method not only aims to bolster the original jackknife method’s coverage properties and computational efficiency but also aligns with the conformal prediction family of methods. </span><span class="koboSpan" id="kobo.951.3">As a result, jackknife+ benefits from all the robust features of the conformal prediction framework. </span><span class="koboSpan" id="kobo.951.4">It guarantees validity even in final samples of any size, exhibits a distribution-free nature, and is versatile enough to be applied to any </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">regression model.</span></span></p>
<p><span class="koboSpan" id="kobo.953.1">The main difference between the jackknife and jackknife+ methods for constructing predictive intervals is that the jackknife+ method uses leave-one-out predictions at the test point to account for the variability in the fitted regression function, in addition to the quantiles of leave-one-out residuals used by the jackknife method. </span><span class="koboSpan" id="kobo.953.2">This modification allows the jackknife+ method to provide rigorous coverage guarantees, regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. </span><span class="koboSpan" id="kobo.953.3">In contrast, the original jackknife method offers no theoretical guarantees without stability assumptions and can sometimes have poor coverage properties. </span><span class="koboSpan" id="kobo.953.4">Leave-one-out predictions capture the uncertainty in the fitted model’s predictions at the target point, allowing the intervals to adapt based on model variability. </span><span class="koboSpan" id="kobo.953.5">In contrast, the original jackknife method offers no theoretical guarantees without stability assumptions and can sometimes have poor </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">coverage properties.</span></span></p>
<p><span class="koboSpan" id="kobo.955.1">The jackknife+ method is related to cross-conformal prediction proposed by Vovk in that both methods </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.956.1">aim to construct predictive intervals that provide </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.957.1">rigorous coverage guarantees, regardless of the distribution of the data points, for any algorithm that treats the training </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">points symmetrically.</span></span></p>
<p><span class="koboSpan" id="kobo.959.1">However, the jackknife+ method differs from cross-conformal prediction in that it uses leave-one-out predictions at the test point to account for the variability in the fitted regression function and the quantiles of leave-one-out residuals used by </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">cross-conformal prediction.</span></span></p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.961.1">Jackknife+ regression</span></h2>
<p><span class="koboSpan" id="kobo.962.1">To recap, ICP provides validity guarantees at a user-specified </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.963.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.964.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.965.1">α</span></span><span class="koboSpan" id="kobo.966.1"> confidence level and is </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.967.1">highly computationally efficient as it does not necessitate retraining the base regression model. </span><span class="koboSpan" id="kobo.967.2">However, these advantages come at the expense of needing to divide the data into a separate </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.968.1">calibration dataset. </span><span class="koboSpan" id="kobo.968.2">As this reduces the amount of data available for training the underlying regressor, it can lead to a less optimal fit for the regression model and subsequently wider prediction intervals, particularly if the original dataset is small. </span><span class="koboSpan" id="kobo.968.3">Conversely, if the calibration set is small, it could result in </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">higher variability.</span></span></p>
<p><span class="koboSpan" id="kobo.970.1">Jackknife+ is a modification of jackknife. </span><span class="koboSpan" id="kobo.970.2">Similar to jackknife, we fit the regressor </span><em class="italic"><span class="koboSpan" id="kobo.971.1">n</span></em><span class="koboSpan" id="kobo.972.1"> times for each of the </span><em class="italic"><span class="koboSpan" id="kobo.973.1">n</span></em><span class="koboSpan" id="kobo.974.1"> points in </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">the dataset:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.976.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.977.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.978.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.979.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.980.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.981.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.982.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.983.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.984.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended"><span class="koboSpan" id="kobo.985.1">𝒜</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.986.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.987.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.988.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.989.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.990.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.991.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.992.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.993.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.994.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.995.1">)</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.996.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.997.1">…</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.998.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.999.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1000.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1001.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1002.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1003.1">−</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1004.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1005.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1006.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1007.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1008.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1009.1">−</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1010.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1011.1">)</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1012.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1013.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1014.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1015.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1016.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1017.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1018.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1019.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1020.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1021.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1022.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1023.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1024.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1025.1">)</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1026.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1027.1">…</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1028.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1029.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1030.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1031.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1032.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1033.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1034.1">Y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1035.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1036.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1037.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1038.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.1039.1">The primary distinction between the jackknife and jackknife+ methods lies in the latter’s utilization of leave-one-out predictions at the test point, in addition to the quantiles of leave-one-out residuals that the jackknife </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">method employs.</span></span></p>
<p><span class="koboSpan" id="kobo.1041.1">The prediction interval generated by the jackknife+ method leverages quantiles to construct prediction intervals. </span><span class="koboSpan" id="kobo.1041.2">However, it not only examines the quantiles of leave-one-out prediction errors but it also considers the </span><em class="italic"><span class="koboSpan" id="kobo.1042.1">n</span></em><span class="koboSpan" id="kobo.1043.1"> predictions produced by the regression model for </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.1044.1">the point prediction of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1045.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1046.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1047.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1048.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1049.1">1</span></span><span class="koboSpan" id="kobo.1050.1">. </span><span class="koboSpan" id="kobo.1050.2">This approach effectively broadens the scope of the predictive model, taking into account both error estimations </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.1051.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">individual predictions:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1053.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1054.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1055.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1056.1">C</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1057.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1058.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1059.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1060.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1061.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1062.1"> </span></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.1063.1">jackknife</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1064.1">+</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1065.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1066.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1067.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1068.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1069.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1070.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1071.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1072.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1073.1">[</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1074.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1075.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1076.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1077.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1078.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1079.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1080.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1081.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1082.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1083.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1084.1">−</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1085.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1086.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1087.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1088.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1089.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1090.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1091.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1092.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1093.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1094.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1095.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1096.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1097.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1098.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1099.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1100.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1101.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1102.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1103.1">R</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1104.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1105.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1106.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1107.1">L</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1108.1">O</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1109.1">O</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1110.1">}</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1111.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1112.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1113.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1114.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1115.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1116.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1117.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1118.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1119.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1120.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1121.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1122.1">+</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1123.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1124.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1125.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1126.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1127.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1128.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1129.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1130.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1131.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1132.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1133.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1134.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1135.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1136.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1137.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1138.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1139.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1140.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1141.1">R</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1142.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1143.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1144.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1145.1">L</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1146.1">O</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1147.1">O</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1148.1">}</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1149.1">]</span></span></span></p>
<p><span class="koboSpan" id="kobo.1150.1">Compare this formula with the formula </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">for jackknife:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1152.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1153.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1154.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1155.1">C</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1156.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1157.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1158.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1159.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1160.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1161.1"> </span></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.1162.1">jackknife</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1163.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1164.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1165.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1166.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1167.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1168.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1169.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1170.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1171.1">[</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1172.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1173.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1174.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1175.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1176.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1177.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1178.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1179.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1180.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1181.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1182.1">−</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1183.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1184.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1185.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1186.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1187.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1188.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1189.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1190.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1191.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1192.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1193.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1194.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1195.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1196.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1197.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1198.1">R</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1199.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1200.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1201.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1202.1">L</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1203.1">O</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1204.1">O</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1205.1">}</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1206.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1207.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1208.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1209.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1210.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1211.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1212.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1213.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1214.1">,</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1215.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1216.1"> </span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1217.1">+</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1218.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1219.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1220.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1221.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1222.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1223.1">μ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1224.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1225.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1226.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1227.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1228.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1229.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1230.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1231.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1232.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1233.1">R</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1234.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1235.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1236.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1237.1">L</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1238.1">O</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1239.1">O</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1240.1">}</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1241.1">]</span></span></span></p>
<p><span class="koboSpan" id="kobo.1242.1">The significant divergence between jackknife and jackknife+ lies in the manner in which point predictions are made for the  </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1243.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1244.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1245.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1246.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1247.1">1</span></span><span class="koboSpan" id="kobo.1248.1"> test point. </span><span class="koboSpan" id="kobo.1248.2">While the jackknife model makes the prediction only once, the jackknife+ model makes </span><em class="italic"><span class="koboSpan" id="kobo.1249.1">n</span></em><span class="koboSpan" id="kobo.1250.1"> predictions, each time fitting the model to </span><em class="italic"><span class="koboSpan" id="kobo.1251.1">n-1</span></em><span class="koboSpan" id="kobo.1252.1"> data points while excluding one point at </span><span class="No-Break"><span class="koboSpan" id="kobo.1253.1">a time.</span></span></p>
<p><span class="koboSpan" id="kobo.1254.1">This structure effectively accommodates potential instability in the regression algorithm, an issue that previously hindered jackknife from fulfilling theoretical validity guarantees. </span><span class="koboSpan" id="kobo.1254.2">In scenarios where the regression model is highly sensitive to the training data, the output can vary substantially. </span><span class="koboSpan" id="kobo.1254.3">Thus, jackknife+ provides a more nuanced and flexible prediction model, catering to possible variability in </span><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.1256.1">The following figure presents a comparative illustration of prediction intervals produced by both the jackknife and </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">jackknife+ models:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.1258.1"><img alt="Figure 7.6 – Prediction intervals produced by jackknife and jackknife+" src="image/B19925_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1259.1">Figure 7.6 – Prediction intervals produced by jackknife and jackknife+</span></p>
<p><span class="koboSpan" id="kobo.1260.1">In scenarios where the regression algorithm is stable, both models perform quite similarly, delivering empirical coverage approximately equal to </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1261.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1262.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1263.1">α</span></span><span class="koboSpan" id="kobo.1264.1">. </span><span class="koboSpan" id="kobo.1264.2">However, in situations </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.1265.1">where the regression model is unstable and sensitive to training data, to the extent that removing </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.1266.1">a single data point can significantly alter the predicted value at </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1267.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1268.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1269.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1270.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1271.1">1</span></span><span class="koboSpan" id="kobo.1272.1">, the output from both models can </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">diverge significantly.</span></span></p>
<p><span class="koboSpan" id="kobo.1274.1">Contrary to the jackknife method, which lacks theoretical validity guarantees, the jackknife+ model will, even in the worst-case scenarios, assure coverage of at least 1-2\alpha. </span><span class="koboSpan" id="kobo.1274.2">Furthermore, in most practical scenarios, barring instances involving instability, the jackknife+ model is expected to provide empirical coverage of 1-\alpha, making it a robust method for </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">uncertainty prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.1276.1">While the jackknife method provides a robust way of calculating nonconformity scores, it has a significant computational overhead as it requires retraining the model for each instance in the calibration set. </span><span class="koboSpan" id="kobo.1276.2">This is where the jackknife+ method </span><span class="No-Break"><span class="koboSpan" id="kobo.1277.1">comes in.</span></span></p>
<p><span class="koboSpan" id="kobo.1278.1">The jackknife+ method improves upon the jackknife method by allowing for the calculation of nonconformity scores without the need to retrain the model for each instance in the calibration set. </span><span class="koboSpan" id="kobo.1278.2">This is achieved by adjusting the nonconformity score calculation to account for the influence of each example on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">model’s predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.1280.1">First, the jackknife+ method trains the model on the entire training dataset. </span><span class="koboSpan" id="kobo.1280.2">Then, for each instance in the calibration set, the method calculates an adjusted prediction that approximates the prediction that would have been made if the instance had been left out when </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.1281.1">training the model. </span><span class="koboSpan" id="kobo.1281.2">The nonconformity score for each instance is the absolute difference between its actual value and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">adjusted prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.1283.1">The </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.1284.1">primary benefit of the Jackknife+ method is computational efficiency as it doesn’t require the model to be retrained for each instance in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1285.1">calibration set.</span></span></p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.1286.1">Conformal predictive distributions</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1287.1">Conformal predictive distribution</span></strong><span class="koboSpan" id="kobo.1288.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1289.1">CPD</span></strong><span class="koboSpan" id="kobo.1290.1">) is an innovative method that applies the principles </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.1291.1">of conformal prediction to generate predictive distributions. </span><span class="koboSpan" id="kobo.1291.2">These distributions provide a comprehensive view of prediction uncertainty, offering not just interval estimates but a complete distribution over all </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">potential outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.1293.1">The concept of CPD was first introduced in the paper </span><em class="italic"><span class="koboSpan" id="kobo.1294.1">Nonparametric predictive distributions based on Conformal Prediction</span></em><span class="koboSpan" id="kobo.1295.1"> (</span><a href="https://link.springer.com/article/10.1007/s10994-018-5755-8"><span class="koboSpan" id="kobo.1296.1">https://link.springer.com/article/10.1007/s10994-018-5755-8</span></a><span class="koboSpan" id="kobo.1297.1">), by Vladimir Vovk, Jieli Shen, Valery Manokhin, and Min-ge Xie. </span><span class="koboSpan" id="kobo.1297.2">In this paper, the authors applied conformal prediction to derive valid predictive distributions under a </span><span class="No-Break"><span class="koboSpan" id="kobo.1298.1">nonparametric assumption.</span></span></p>
<p><span class="koboSpan" id="kobo.1299.1">In a </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.1300.1">subsequent paper, </span><em class="italic"><span class="koboSpan" id="kobo.1301.1">Conformal predictive distributions with kernels</span></em><span class="koboSpan" id="kobo.1302.1"> (</span><a href="https://arxiv.org/abs/1710.08894"><span class="koboSpan" id="kobo.1303.1">https://arxiv.org/abs/1710.08894</span></a><span class="koboSpan" id="kobo.1304.1">), by Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alex Gammerman, the authors reviewed the history of predictive distributions in statistics and discussed two key developments. </span><span class="koboSpan" id="kobo.1304.2">The first was the integration of predictive distributions into machine learning, and the second was the combination of predictive distributions with </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">kernel methods.</span></span></p>
<p><span class="koboSpan" id="kobo.1306.1">In </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.1307.1">the paper </span><em class="italic"><span class="koboSpan" id="kobo.1308.1">Cross-conformal predictive distributions</span></em><span class="koboSpan" id="kobo.1309.1"> (</span><a href="http://proceedings.mlr.press/v91/vovk18a.html"><span class="koboSpan" id="kobo.1310.1">http://proceedings.mlr.press/v91/vovk18a.html</span></a><span class="koboSpan" id="kobo.1311.1">), by Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman, the authors extended CPD to any underlying model, whether it be statistical, machine learning, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">deep learning.</span></span></p>
<p><span class="koboSpan" id="kobo.1313.1">Inductive (split) conformal predictive systems are computationally efficient versions of conformal predictive systems that output probability distributions for labels of test observations in regression problems. </span><span class="koboSpan" id="kobo.1313.2">These systems provide additional information that can be useful </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">in decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.1315.1">Cross-conformal predictive systems are a novel application of conformal prediction that facilitates automatic decision-making. </span><span class="koboSpan" id="kobo.1315.2">They are built on top of split conformal predictive systems, and while they can lose their validity in principle, they usually satisfy the validity </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.1316.1">requirement in practice. </span><span class="koboSpan" id="kobo.1316.2">Therefore, cross-conformal predictive systems differ from traditional conformal predictors in that they are more computationally efficient versions that can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.1317.1">automatic decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.1318.1">The subsequent figure depicts the CPD prediction process for a single </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">test instance:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.1320.1"><img alt="Figure 7.7 – CPD" src="image/B19925_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1321.1">Figure 7.7 – CPD</span></p>
<p><span class="koboSpan" id="kobo.1322.1">Let’s apply these concepts in practice; we will use the notebook </span><em class="italic"><span class="koboSpan" id="kobo.1323.1">Conformal Prediction for </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1324.1">Regression</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1325.1"> (</span></span><a href="https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.1326.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_07.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1327.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.1328.1">We will follow the MAPIE tutorial for CQR. </span><span class="koboSpan" id="kobo.1328.2">The target variable of this dataset is the median house value for the California districts. </span><span class="koboSpan" id="kobo.1328.3">This dataset comprises eight features, including variables such as the house’s age, the neighborhood’s median income, the average number of rooms or bedrooms, and even the location in latitude and longitude. </span><span class="koboSpan" id="kobo.1328.4">In total, there are around 20k observations. </span><span class="koboSpan" id="kobo.1328.5">We compute the correlation between features and </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.1329.1">also between features and the target. </span><span class="koboSpan" id="kobo.1329.2">As is evident from the analysis, the most significant correlation of house prices is with the neighborhood’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">median income:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.1331.1"><img alt="Figure 7.8 – California housing – correlation matrix" src="image/B19925_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1332.1">Figure 7.8 – California housing – correlation matrix</span></p>
<p><span class="koboSpan" id="kobo.1333.1">We can also plot the distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.1334.1">house prices:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.1335.1"><img alt="Figure 7.9 – California housing – histogram of house prices" src="image/B19925_07_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1336.1">Figure 7.9 – California housing – histogram of house prices</span></p>
<p><span class="koboSpan" id="kobo.1337.1">Now, we </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.1338.1">can train and optimize the </span><span class="No-Break"><span class="koboSpan" id="kobo.1339.1">underlying model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1340.1">estimator = LGBMRegressor(objective='quantile', alpha=0.5, random_state=random_state)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1341.1">params_distributions = dict(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1342.1">    num_leaves=randint(low=10, high=50),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1343.1">    max_depth=randint(low=3, high=20),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1344.1">    n_estimators=randint(low=50, high=300),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1345.1">    learning_rate=uniform()</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1346.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1347.1">optim_model = RandomizedSearchCV(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1348.1">    estimator,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1349.1">    param_distributions=params_distributions,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1350.1">    n_jobs=-1,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1351.1">    n_iter=100,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1352.1">    cv=KFold(n_splits=5, shuffle=True),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1353.1">    verbose=0</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1354.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1355.1">optim_model.fit(X_train, y_train)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.1356.1">estimator = optim_model.best_estimator_</span></pre>
<p><span class="koboSpan" id="kobo.1357.1">Several </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.1358.1">methods are used to produce probabilistic predictions, including CQR and a variant </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">of jackknife.</span></span></p>
<p><span class="koboSpan" id="kobo.1360.1">We can plot the results that are produced by using </span><span class="No-Break"><span class="koboSpan" id="kobo.1361.1">different methods:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.1362.1"><img alt="Figure 7.10 – Predicting California housing prices using various methods" src="image/B19925_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1363.1">Figure 7.10 – Predicting California housing prices using various methods</span></p>
<p><span class="koboSpan" id="kobo.1364.1">We observe </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.1365.1">increased flexibility in the prediction intervals for CQR compared to other methods that maintain a fixed interval width. </span><span class="koboSpan" id="kobo.1365.2">Specifically, as the prices rise, the prediction intervals expand correspondingly. </span><span class="koboSpan" id="kobo.1365.3">To substantiate these observations, we will examine the conditional coverage and interval width across these intervals, segmented </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">by quantiles:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.1367.1"><img alt="Figure 7.11 – Prediction interval coverage by binned house prices depending on price level" src="image/B19925_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1368.1">Figure 7.11 – Prediction interval coverage by binned house prices depending on price level</span></p>
<p><span class="koboSpan" id="kobo.1369.1">As we can see, CQR adjusts to larger prices more adeptly. </span><span class="koboSpan" id="kobo.1369.2">Its conditional coverage closely aligns </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.1370.1">with the target coverage for higher prices and lower prices where other methods exceed the necessary coverage. </span><span class="koboSpan" id="kobo.1370.2">This adaptation is likely to influence the widths of </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">the intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.1372.1">We can also plot interval width </span><span class="No-Break"><span class="koboSpan" id="kobo.1373.1">by bin:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.1374.1"><img alt="Figure 7.12 – Prediction of interval width by binned house prices depending on house price level" src="image/B19925_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1375.1">Figure 7.12 – Prediction of interval width by binned house prices depending on house price level</span></p>
<p><span class="koboSpan" id="kobo.1376.1">Now, we can look at CPD in action using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1377.1">same notebook:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1378.1">We will demonstrate the use of CPD with the Crepes package. </span><span class="koboSpan" id="kobo.1378.2">For a quick start guide on the Crepes package, refer to https://github.com/henrikbostrom/crepes. </span><span class="koboSpan" id="kobo.1378.3">We will wrap the standard random forest regressor using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1379.1">WrapRegressor</span></strong><span class="koboSpan" id="kobo.1380.1"> class from Crepes and fit it (in the usual way) to the proper </span><span class="No-Break"><span class="koboSpan" id="kobo.1381.1">training set:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1382.1">rf = WrapRegressor(RandomForestRegressor())</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1383.1">rf.fit(X_prop_train, y_prop_train)</span></pre></li>
<li><span class="koboSpan" id="kobo.1384.1">Then, we will calibrate the regressor using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">calibration set:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1386.1">rf.calibrate(X_cal, y_cal)</span></pre></li>
<li><span class="koboSpan" id="kobo.1387.1">The conformal regressor is now ready to generate prediction intervals for the test set, utilizing a 99% confidence level. </span><span class="koboSpan" id="kobo.1387.2">The output will be a NumPy array. </span><span class="koboSpan" id="kobo.1387.3">Each row </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.1388.1">corresponds to a test instance, with two columns indicating each prediction interval’s lower and </span><span class="No-Break"><span class="koboSpan" id="kobo.1389.1">upper bounds:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1390.1">rf.predict_int(X_test, confidence=0.99)</span></pre></li>
<li><span class="koboSpan" id="kobo.1391.1">We can specify that we wish to trim the intervals to omit impossible values – in this scenario, values below 0. </span><span class="koboSpan" id="kobo.1391.2">Using the default confidence level (0.95), the resulting output intervals will be slightly </span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">more concise:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1393.1">rf.predict_int(X_test, y_min=0)</span></pre></li>
<li><span class="koboSpan" id="kobo.1394.1">We will employ </span><strong class="source-inline"><span class="koboSpan" id="kobo.1395.1">DifficultyEstimator()</span></strong><span class="koboSpan" id="kobo.1396.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1397.1">crepes</span></strong><span class="koboSpan" id="kobo.1398.1"> to make intervals more adaptive. </span><span class="koboSpan" id="kobo.1398.2">Here, the difficulty is estimated by the standard deviation of the target of the default k=25 nearest neighbors in the proper training set for each object in the calibration set. </span><span class="koboSpan" id="kobo.1398.3">First, we will obtain the difficulty estimates for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1399.1">calibration set:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1400.1">de = DifficultyEstimator()</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1401.1">de.fit(X_prop_train, y=y_prop_train)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1402.1">sigmas_cal = de.apply(X_cal)</span></pre></li>
<li><span class="koboSpan" id="kobo.1403.1">These can now be used for the calibration, which will produce a normalized </span><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">conformal regressor:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1405.1">rf.calibrate(X_cal, y_cal, sigmas=sigmas_cal)</span></pre></li>
<li><span class="koboSpan" id="kobo.1406.1">We also need difficulty estimates for the test set, which we provide as input </span><span class="No-Break"><span class="koboSpan" id="kobo.1407.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1408.1">predict_int</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1409.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1410.1">sigmas_test = de.apply(X_test)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1411.1">crepes_predictions = rf.predict_int(X_test, sigmas=sigmas_test, y_min=0)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1412.1">We </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.1413.1">will get the </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">following output:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.1415.1"><img alt="Figure 7.13 – Actual versus predicted values with prediction interval coverage" src="image/B19925_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1416.1">Figure 7.13 – Actual versus predicted values with prediction interval coverage</span></p>
<h3><span class="koboSpan" id="kobo.1417.1">CPD (conformal predictive systems) in the Crepes package</span></h3>
<p><span class="koboSpan" id="kobo.1418.1">CPD yields cumulative distribution functions (conformal predictive distributions). </span><span class="koboSpan" id="kobo.1418.2">These not only </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.1419.1">allow us to create </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.1420.1">prediction intervals but also enable us to derive percentiles, calibrated point predictions, and </span><em class="italic"><span class="koboSpan" id="kobo.1421.1">p</span></em><span class="koboSpan" id="kobo.1422.1"> values for specific target values. </span><span class="koboSpan" id="kobo.1422.2">Let’s explore how to </span><span class="No-Break"><span class="koboSpan" id="kobo.1423.1">accomplish this.</span></span></p>
<p><span class="koboSpan" id="kobo.1424.1">The only modification that’s required is to pass </span><strong class="source-inline"><span class="koboSpan" id="kobo.1425.1">cps=True</span></strong><span class="koboSpan" id="kobo.1426.1"> to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1427.1">calibrate</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1428.1"> method:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1429.1">For instance, we can establish normalized Mondrian conformal predictive systems by supplying both </span><strong class="source-inline"><span class="koboSpan" id="kobo.1430.1">bins</span></strong><span class="koboSpan" id="kobo.1431.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1432.1">sigmas</span></strong><span class="koboSpan" id="kobo.1433.1"> to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1434.1">calibrate</span></strong><span class="koboSpan" id="kobo.1435.1"> method. </span><span class="koboSpan" id="kobo.1435.2">In this case, we will examine Mondrian categories formed by binning the </span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">point predictions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1437.1">bins_cal, bin_thresholds = binning(rf.predict(X_cal), bins=5)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1438.1">rf.calibrate(X_cal, y_cal, sigmas=sigmas_cal, bins=bins_cal, cps=True)</span></pre></li>
<li><span class="koboSpan" id="kobo.1439.1">By supplying </span><strong class="source-inline"><span class="koboSpan" id="kobo.1440.1">bins</span></strong><span class="koboSpan" id="kobo.1441.1"> (and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1442.1">sigmas</span></strong><span class="koboSpan" id="kobo.1443.1">) for the test objects, we can make predictions using the conformal predictive system by utilizing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1444.1">predict_cps</span></strong><span class="koboSpan" id="kobo.1445.1"> method. </span><span class="koboSpan" id="kobo.1445.2">This method offers flexible control over the output. </span><span class="koboSpan" id="kobo.1445.3">In this instance, we seek to obtain prediction intervals with </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">95% confidence:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1447.1">bins_test = binning(rf.predict(X_test), bins=bin_thresholds)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1448.1">rf.predict_cps(X_test, sigmas=sigmas_test, bins=bins_test,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.1449.1">               lower_percentiles=2.5, higher_percentiles=97.5, y_min=0)</span></pre></li>
<li><span class="koboSpan" id="kobo.1450.1">We can instruct the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1451.1">predict_cps</span></strong><span class="koboSpan" id="kobo.1452.1"> method to return the complete CPD for each </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.1453.1">test instance, as delineated by the threshold values, by setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.1454.1">return_cpds=True</span></strong><span class="koboSpan" id="kobo.1455.1">. </span><span class="koboSpan" id="kobo.1455.2">The structure of these distributions differs based on the </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.1456.1">type of conformal predictive system. </span><span class="koboSpan" id="kobo.1456.2">For standard and normalized CPS, the output is an array with a row for each test instance and a column for each calibration instance (residual). </span><span class="koboSpan" id="kobo.1456.3">Conversely, for a Mondrian CPS, the default output is a vector with one CPD for each test instance, as the number of values may fluctuate </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">between categories:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1458.1">cpds = rf.predict_cps(X_test, sigmas=sigmas_test, bins=bins_test, return_cpds=True)</span></pre></li>
<li><span class="koboSpan" id="kobo.1459.1">We can plot CPD for a random object from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1460.1">test set:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.1461.1"><img alt="Figure 7.14 – CPD for a test object" src="image/B19925_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1462.1">Figure 7.14 – CPD for a test object</span></p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.1463.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1464.1">This chapter explored uncertainty quantification for regression problems, a critical aspect of data science and machine learning. </span><span class="koboSpan" id="kobo.1464.2">It highlighted the importance of uncertainty and the methods to handle it effectively to make more reliable predictions </span><span class="No-Break"><span class="koboSpan" id="kobo.1465.1">and decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.1466.1">One of the significant sections of this chapter was dedicated to various approaches that can be used to produce prediction intervals. </span><span class="koboSpan" id="kobo.1466.2">It systematically broke down and explained diverse methods, elucidating how each works and their advantages and disadvantages. </span><span class="koboSpan" id="kobo.1466.3">This detailed analysis aids in understanding the mechanisms behind these approaches and their practical application in real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1">regression problems.</span></span></p>
<p><span class="koboSpan" id="kobo.1468.1">Furthermore, this chapter discussed building prediction intervals and predictive distributions using conformal prediction. </span><span class="koboSpan" id="kobo.1468.2">We provided a step-by-step guide to constructing these intervals and distributions. </span><span class="koboSpan" id="kobo.1468.3">This chapter also offered practical insights and tips for effectively utilizing conformal prediction to achieve more reliable and trustworthy predictions in </span><span class="No-Break"><span class="koboSpan" id="kobo.1469.1">regression problems.</span></span></p>
<p><span class="koboSpan" id="kobo.1470.1">In addition, we delved into advanced topics such as CQR, jackknife+, and CPD. </span><span class="koboSpan" id="kobo.1470.2">These advanced techniques were broken down and explained in detail, helping you grasp their complexity and utility in handling </span><span class="No-Break"><span class="koboSpan" id="kobo.1471.1">regression problems.</span></span></p>
<p><span class="koboSpan" id="kobo.1472.1">Practical illustration was crucial in this chapter, offering hands-on experience and insights. </span><span class="koboSpan" id="kobo.1472.2">This chapter utilized housing price datasets to demonstrate the application of the discussed models and techniques. </span><span class="koboSpan" id="kobo.1472.3">Libraries such as MAPIE and Crepes were applied to the datasets, providing you with practical knowledge and experience beyond </span><span class="No-Break"><span class="koboSpan" id="kobo.1473.1">theoretical understanding.</span></span></p>
<p><span class="koboSpan" id="kobo.1474.1">In conclusion, this chapter provided a comprehensive and practical guide that covered various topics related to uncertainty quantification, prediction intervals, and conformal prediction for regression problems. </span><span class="koboSpan" id="kobo.1474.2">The realistic illustrations, using real-world datasets and libraries, further enhanced the learning experience, making this chapter a valuable resource for anyone looking to deepen their understanding and strengthen their skills in these </span><span class="No-Break"><span class="koboSpan" id="kobo.1475.1">critical areas.</span></span></p>
</div>
</body></html>