<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Introducing the Probability Theory"><div class="titlepage" id="aid-DB7S2"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Introducing the Probability Theory</h1></div></div></div><p>Bayesian inference is a method of learning about the relationship between variables from data, in the presence of uncertainty, in real-world problems. It is one of the frameworks of probability theory. Any reader interested in Bayesian inference should have a good knowledge of probability theory to understand and use Bayesian inference. This chapter covers an overview of probability theory, which will be sufficient to understand the rest of the chapters in this book.</p><p>It was Pierre-Simon Laplace who first proposed a formal definition of probability with mathematical rigor. This definition is called the <a id="id0" class="indexterm"/>
<span class="emphasis"><em>Classical Definition</em></span> and it states the following:</p><div class="blockquote"><table border="0" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><span class="emphasis"><em>Pierre-Simon Laplace, A Philosophical Essay on Probabilities</em></span></span></td></tr></table></div><p>What this definition means is that, if a random experiment can result in <span class="inlinemediaobject"><img src="../Images/image00166.jpeg" alt="Introducing the Probability Theory"/></span> mutually exclusive and equally likely outcomes, the probability of the event <span class="inlinemediaobject"><img src="../Images/image00167.jpeg" alt="Introducing the Probability Theory"/></span> is given by:</p><div class="mediaobject"><img src="../Images/image00168.jpeg" alt="Introducing the Probability Theory"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00169.jpeg" alt="Introducing the Probability Theory"/></span> is the number of occurrences of the event <span class="inlinemediaobject"><img src="../Images/image00167.jpeg" alt="Introducing the Probability Theory"/></span>.</p><p>To illustrate<a id="id1" class="indexterm"/> this concept, let us take a simple example of a rolling dice. If the dice is a fair dice, then all the faces will have an equal chance of showing up when the dice is rolled. Then, the probability of each face showing up is 1/6. However, when one rolls the dice 100 times, all the faces will not come in equal proportions of 1/6 due to random fluctuations. The estimate of probability of each face is the number of times the face shows up divided by the number of rolls. As the denominator is very large, this ratio will be close to 1/6.</p><p>In the long run, this classical definition treats the probability of an uncertain event as the relative frequency of its occurrence. This is also called a <a id="id2" class="indexterm"/>
<span class="strong"><strong>frequentist</strong></span> approach to probability. Although this approach is suitable for a large class of problems, there are cases where this type of approach cannot be used. As an example, consider the following question: <span class="emphasis"><em>Is Pataliputra the name of an ancient city or a king?</em></span> In such cases, we have a degree of belief in various plausible answers, but it is not based on counts in the outcome of an experiment (in the Sanskrit language <span class="emphasis"><em>Putra</em></span> means son, therefore some people may believe that Pataliputra is the name of an ancient king in India, but it is a city).</p><p>Another example is, <span class="emphasis"><em>What is the chance of the Democratic Party winning the election in 2016 in America?</em></span> Some people may believe it is 1/2 and some people may believe it is 2/3. In this case, probability is defined as the <span class="strong"><strong>degree of belief</strong></span><a id="id3" class="indexterm"/> of a person in the outcome of an uncertain event. This is called the <a id="id4" class="indexterm"/>
<span class="strong"><strong>subjective</strong></span> definition of probability.</p><p>One of the limitations of the classical or frequentist definition of probability is that it cannot address subjective probabilities. As we will see later in this book, Bayesian inference is a natural framework for treating both frequentist and subjective interpretations of probability.</p><div class="section" title="Probability distributions"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Probability distributions</h1></div></div></div><p>In both classical <a id="id5" class="indexterm"/>and Bayesian approaches, a probability distribution function is the central quantity, which captures all of the information about the relationship between variables in the presence of uncertainty. A probability distribution assigns a probability value to each measurable subset of outcomes of a random experiment. The <a id="id6" class="indexterm"/>variable involved could be discrete or continuous, and univariate or multivariate. Although people use slightly different terminologies, the commonly used probability distributions for the different types of random variables are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Probability mass function</strong></span> (<span class="strong"><strong>pmf</strong></span>) for <a id="id7" class="indexterm"/>discrete numerical random variables</li><li class="listitem"><span class="strong"><strong>Categorical distribution</strong></span> for<a id="id8" class="indexterm"/> categorical random variables</li><li class="listitem"><span class="strong"><strong>Probability density function</strong></span> (<span class="strong"><strong>pdf</strong></span>) for <a id="id9" class="indexterm"/>continuous random variables</li></ul></div><p>One of the well-known distribution functions is the normal or Gaussian distribution, which is named after Carl Friedrich Gauss, a famous German mathematician and physicist. It is also known by the name <span class="emphasis"><em>bell curve</em></span> because of its shape. The mathematical form of this distribution is given by:</p><div class="mediaobject"><img src="../Images/image00170.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00171.jpeg" alt="Probability distributions"/></span> is the mean or location parameter and <span class="inlinemediaobject"><img src="../Images/image00172.jpeg" alt="Probability distributions"/></span> is the standard deviation or scale parameter (<span class="inlinemediaobject"><img src="../Images/image00173.jpeg" alt="Probability distributions"/></span> is called variance). The following graphs show what the distribution looks like for different values of location and scale parameters:</p><div class="mediaobject"><img src="../Images/image00174.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>One can see<a id="id10" class="indexterm"/> that as the mean changes, the location of the peak of the distribution changes. Similarly, when the standard deviation changes, the width of the distribution also changes.</p><p>Many natural datasets follow normal distribution because, according to the <a id="id11" class="indexterm"/>
<span class="strong"><strong>central limit theorem</strong></span>, any random variable that can be composed as a mean of independent random variables will have a normal distribution. This is irrespective of the form of the distribution of this random variable, as long as they have finite mean and variance and all are drawn from the same original distribution. A normal distribution is also very popular among data scientists because in many statistical inferences, theoretical results can be derived if the underlying distribution is normal.</p><p>Now, let us look at the multidimensional version of normal distribution. If the random variable is an N-dimensional vector, <span class="emphasis"><em>x</em></span> is denoted by:</p><div class="mediaobject"><img src="../Images/image00175.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>Then, the corresponding normal distribution is given by:</p><div class="mediaobject"><img src="../Images/image00176.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00177.jpeg" alt="Probability distributions"/></span> corresponds to the mean (also called location) and <span class="inlinemediaobject"><img src="../Images/image00178.jpeg" alt="Probability distributions"/></span> is an <span class="emphasis"><em>N x N</em></span> covariance matrix (also called scale).</p><p>To get a better<a id="id12" class="indexterm"/> understanding of the multidimensional normal distribution, let us take the case of two dimensions. In this case, <span class="inlinemediaobject"><img src="../Images/image00179.jpeg" alt="Probability distributions"/></span> and the covariance matrix is given by:</p><div class="mediaobject"><img src="../Images/image00180.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00181.jpeg" alt="Probability distributions"/></span> and <span class="inlinemediaobject"><img src="../Images/image00182.jpeg" alt="Probability distributions"/></span> are the variances along <span class="inlinemediaobject"><img src="../Images/image00183.jpeg" alt="Probability distributions"/></span> and <span class="inlinemediaobject"><img src="../Images/image00184.jpeg" alt="Probability distributions"/></span> directions, and <span class="inlinemediaobject"><img src="../Images/image00185.jpeg" alt="Probability distributions"/></span> is the correlation between <span class="inlinemediaobject"><img src="../Images/image00183.jpeg" alt="Probability distributions"/></span> and <span class="inlinemediaobject"><img src="../Images/image00184.jpeg" alt="Probability distributions"/></span>. A plot of two-dimensional normal distribution for <span class="inlinemediaobject"><img src="../Images/image00186.jpeg" alt="Probability distributions"/></span>, <span class="inlinemediaobject"><img src="../Images/image00187.jpeg" alt="Probability distributions"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00188.jpeg" alt="Probability distributions"/></span> is shown in the following image:</p><div class="mediaobject"><img src="../Images/image00189.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>If <span class="inlinemediaobject"><img src="../Images/image00190.jpeg" alt="Probability distributions"/></span>, then the two-dimensional normal distribution will be reduced to the product of two one-dimensional <a id="id13" class="indexterm"/>normal distributions, since <span class="inlinemediaobject"><img src="../Images/image00178.jpeg" alt="Probability distributions"/></span> would become diagonal in this case. The following 2D projections of normal distribution for the same values of <span class="inlinemediaobject"><img src="../Images/image00181.jpeg" alt="Probability distributions"/></span> and <span class="inlinemediaobject"><img src="../Images/image00182.jpeg" alt="Probability distributions"/></span> but with <span class="inlinemediaobject"><img src="../Images/image00188.jpeg" alt="Probability distributions"/></span> and <span class="inlinemediaobject"><img src="../Images/image00191.jpeg" alt="Probability distributions"/></span> illustrate this case:</p><div class="mediaobject"><img src="../Images/image00192.jpeg" alt="Probability distributions"/></div><p style="clear:both; height: 1em;"> </p><p>The high correlation between <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> in the first case forces most of the data points along the 45 degree <a id="id14" class="indexterm"/>line and makes the distribution more anisotropic; whereas, in the second case, when the correlation is zero, the distribution is more isotropic.</p><p>We will briefly review some of the other well-known distributions used in Bayesian inference here.</p></div></div>
<div class="section" title="Conditional probability" id="aid-E9OE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Conditional probability</h1></div></div></div><p>Often, one<a id="id15" class="indexterm"/> would be interested in finding the probability of the occurrence of a set of random variables when other random variables in the problem are held fixed. As an example of population health study, one would be interested in finding what is the probability of a person, in the age range 40-50, developing heart disease with high blood pressure and diabetes. Questions such as these can be modeled using conditional probability, which is defined as the probability of an event, given that another event has happened. More formally, if we take the variables <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span>, this definition can be rewritten as follows:</p><div class="mediaobject"><img src="../Images/image00193.jpeg" alt="Conditional probability"/></div><p style="clear:both; height: 1em;"> </p><p>Similarly:</p><div class="mediaobject"><img src="../Images/image00194.jpeg" alt="Conditional probability"/></div><p style="clear:both; height: 1em;"> </p><p>The following <a id="id16" class="indexterm"/>Venn diagram explains the concept more clearly:</p><div class="mediaobject"><img src="../Images/image00195.jpeg" alt="Conditional probability"/></div><p style="clear:both; height: 1em;"> </p><p>In Bayesian inference, we are interested in conditional probabilities corresponding to multivariate distributions. If <span class="inlinemediaobject"><img src="../Images/image00196.jpeg" alt="Conditional probability"/></span> denotes the entire random variable set, then the conditional probability of <span class="inlinemediaobject"><img src="../Images/image00197.jpeg" alt="Conditional probability"/></span>, given that <span class="inlinemediaobject"><img src="../Images/image00198.jpeg" alt="Conditional probability"/></span> is fixed at some value, is given by the ratio of joint probability of <span class="inlinemediaobject"><img src="../Images/image00196.jpeg" alt="Conditional probability"/></span> and joint probability of <span class="inlinemediaobject"><img src="../Images/image00198.jpeg" alt="Conditional probability"/></span>:</p><div class="mediaobject"><img src="../Images/image00199.jpeg" alt="Conditional probability"/></div><p style="clear:both; height: 1em;"> </p><p>In the case of two-dimensional<a id="id17" class="indexterm"/> normal distribution, the conditional probability of interest is as follows:</p><div class="mediaobject"><img src="../Images/image00200.jpeg" alt="Conditional probability"/></div><p style="clear:both; height: 1em;"> </p><p>It can be shown that (exercise 2 in the <span class="emphasis"><em>Exercises</em></span> section of this chapter) the RHS can be simplified, resulting in an expression for <span class="inlinemediaobject"><img src="../Images/image00201.jpeg" alt="Conditional probability"/></span> in the form of a normal distribution again with the mean <span class="inlinemediaobject"><img src="../Images/image00202.jpeg" alt="Conditional probability"/></span> and variance <span class="inlinemediaobject"><img src="../Images/image00203.jpeg" alt="Conditional probability"/></span>.</p></div>
<div class="section" title="Bayesian theorem" id="aid-F8901"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Bayesian theorem</h1></div></div></div><p>From <a id="id18" class="indexterm"/>the definition of the conditional probabilities <span class="inlinemediaobject"><img src="../Images/image00204.jpeg" alt="Bayesian theorem"/></span> and <span class="inlinemediaobject"><img src="../Images/image00205.jpeg" alt="Bayesian theorem"/></span>, it is easy to show the following:</p><div class="mediaobject"><img src="../Images/image00206.jpeg" alt="Bayesian theorem"/></div><p style="clear:both; height: 1em;"> </p><p>Rev. Thomas Bayes (1701–1761) used this rule and formulated his famous Bayes theorem that <a id="id19" class="indexterm"/>can be interpreted if <span class="inlinemediaobject"><img src="../Images/image00207.jpeg" alt="Bayesian theorem"/></span> represents the initial degree of belief (or prior probability) in the value of a random variable <span class="emphasis"><em>A</em></span> before observing <span class="emphasis"><em>B</em></span>; then, its posterior probability or degree of belief after accounted for <span class="emphasis"><em>B</em></span> will get updated according to the preceding equation. So, the Bayesian inference essentially corresponds to updating beliefs about an uncertain system after having made some observations about it. In the sense, this is also how we human beings learn about the world. For example, before we visit a new city, we will have certain prior knowledge about the place after reading from books or on the Web. </p><p>However, soon after we reach the place, this belief will get updated based on our initial experience of the place. We continuously update the belief as we explore the new city more and more. We will describe Bayesian inference more in detail in <a class="link" title="Chapter 3. Introducing Bayesian Inference" href="part0030.xhtml#aid-SJGS2">Chapter 3</a>, <span class="emphasis"><em>Introducing Bayesian Inference</em></span>.</p></div>
<div class="section" title="Marginal distribution" id="aid-G6PI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Marginal distribution</h1></div></div></div><p>In many <a id="id20" class="indexterm"/>situations, we are interested only in the probability distribution of a subset of random variables. For example, in the heart disease problem mentioned in the previous section, if we want to infer the probability of people in a population having a heart disease as a function of their age only, we need to integrate out the effect of other random variables such as blood pressure and diabetes. This is<a id="id21" class="indexterm"/> called <span class="strong"><strong>marginalization</strong></span>:</p><div class="mediaobject"><img src="../Images/image00208.jpeg" alt="Marginal distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Or:</p><div class="mediaobject"><img src="../Images/image00209.jpeg" alt="Marginal distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Note that <a id="id22" class="indexterm"/>marginal distribution is very different from conditional distribution. In conditional probability, we are finding the probability of a subset of random variables with values of other random variables fixed (conditioned) at a given value. In the case of marginal distribution, we are eliminating the effect of a subset of random variables by integrating them out (in the sense averaging their effect) from the joint distribution. For example, in the case of two-dimensional normal distribution, marginalization with respect to one variable will result in a one-dimensional normal distribution of the other variable, as follows:</p><div class="mediaobject"><img src="../Images/image00210.jpeg" alt="Marginal distribution"/></div><p style="clear:both; height: 1em;"> </p><p>The details of this integration is given as an exercise (exercise 3 in the <span class="emphasis"><em>Exercises</em></span> section of this chapter).</p></div>
<div class="section" title="Expectations and covariance"><div class="titlepage" id="aid-H5A42"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Expectations and covariance</h1></div></div></div><p>Having known the <a id="id23" class="indexterm"/>distribution of a set of random variables <span class="inlinemediaobject"><img src="../Images/image00211.jpeg" alt="Expectations and covariance"/></span>, what one would be typically interested in for real-life applications is to be able to estimate the average values of these random variables and the correlations between them. These are computed formally using the following expressions:</p><div class="mediaobject"><img src="../Images/image00212.jpeg" alt="Expectations and covariance"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00213.jpeg" alt="Expectations and covariance"/></div><p style="clear:both; height: 1em;"> </p><p>For example, in the case of two-dimensional normal distribution, if we are interested in finding the correlation<a id="id24" class="indexterm"/> between the variables <span class="inlinemediaobject"><img src="../Images/image00183.jpeg" alt="Expectations and covariance"/></span> and <span class="inlinemediaobject"><img src="../Images/image00184.jpeg" alt="Expectations and covariance"/></span>, it can be formally computed from the joint distribution using the following formula:</p><div class="mediaobject"><img src="../Images/image00214.jpeg" alt="Expectations and covariance"/></div><p style="clear:both; height: 1em;"> </p><div class="section" title="Binomial distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Binomial distribution</h2></div></div></div><p>A<a id="id25" class="indexterm"/> binomial distribution<a id="id26" class="indexterm"/> is a discrete distribution that gives the probability of heads in <span class="emphasis"><em>n</em></span> independent trials where each trial has one of two possible outcomes, heads or tails, with the probability of heads being <span class="emphasis"><em>p</em></span>. Each of the trials is called a Bernoulli trial. The functional form of the binomial distribution is given by:</p><div class="mediaobject"><img src="../Images/image00215.jpeg" alt="Binomial distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00216.jpeg" alt="Binomial distribution"/></span> denotes the probability of having <span class="emphasis"><em>k</em></span> heads in <span class="emphasis"><em>n</em></span> trials. The mean of the binomial distribution is given by <span class="emphasis"><em>np</em></span> and variance is given by <span class="emphasis"><em>np(1-p)</em></span>. Have a look at the following graphs:</p><div class="mediaobject"><img src="../Images/image00217.jpeg" alt="Binomial distribution"/></div><p style="clear:both; height: 1em;"> </p><p>The preceding <a id="id27" class="indexterm"/>graphs show the binomial distribution for two values of <span class="emphasis"><em>n</em></span>; 100 and 1000 for <span class="emphasis"><em>p = 0.7</em></span>. As you can see, when <span class="emphasis"><em>n</em></span> becomes large, the Binomial distribution becomes sharply peaked. It can be shown that, in the large <span class="emphasis"><em>n</em></span> limit, a<a id="id28" class="indexterm"/> binomial distribution can be approximated using a normal distribution with mean <span class="emphasis"><em>np</em></span> and variance <span class="emphasis"><em>np(1-p)</em></span>. This is a characteristic shared by many discrete distributions that, in the large <span class="emphasis"><em>n</em></span> limit, they can be approximated by some continuous distributions.</p></div><div class="section" title="Beta distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Beta distribution</h2></div></div></div><p>The<a id="id29" class="indexterm"/> Beta distribution <a id="id30" class="indexterm"/>denoted by <span class="inlinemediaobject"><img src="../Images/image00218.jpeg" alt="Beta distribution"/></span> is a function of the power of <span class="inlinemediaobject"><img src="../Images/image00219.jpeg" alt="Beta distribution"/></span>, and its reflection <span class="inlinemediaobject"><img src="../Images/image00220.jpeg" alt="Beta distribution"/></span> is given by:</p><div class="mediaobject"><img src="../Images/image00221.jpeg" alt="Beta distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00222.jpeg" alt="Beta distribution"/></span> are parameters that determine the shape of the distribution function and <span class="inlinemediaobject"><img src="../Images/image00223.jpeg" alt="Beta distribution"/></span> is the Beta function given by the ratio of Gamma functions: <span class="inlinemediaobject"><img src="../Images/image00224.jpeg" alt="Beta distribution"/></span>.</p><p>The Beta distribution is a very important distribution in Bayesian inference. It is the conjugate prior probability distribution (which will be defined more precisely in the next chapter) for binomial, Bernoulli, negative binomial, and geometric distributions. It is used for modeling the random behavior of percentages and proportions. For example, the Beta distribution has been used for modeling <span class="strong"><strong>allele</strong></span> frequencies<a id="id31" class="indexterm"/> in population genetics, time allocation in project management, the proportion of minerals in rocks, and heterogeneity in the probability of HIV transmission.</p></div><div class="section" title="Gamma distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Gamma distribution</h2></div></div></div><p>The <a id="id32" class="indexterm"/>Gamma distribution denoted by <span class="inlinemediaobject"><img src="../Images/image00225.jpeg" alt="Gamma distribution"/></span> is <a id="id33" class="indexterm"/>another common distribution used in Bayesian inference. It is used for modeling the waiting times such as survival rates. Special cases of the Gamma distribution are the well-known Exponential and Chi-Square distributions.</p><p>In Bayesian<a id="id34" class="indexterm"/> inference, the Gamma distribution is used <a id="id35" class="indexterm"/>as a conjugate prior for the inverse of variance of a one-dimensional normal distribution or parameters such as the rate (<span class="inlinemediaobject"><img src="../Images/image00226.jpeg" alt="Gamma distribution"/></span>) of an exponential or Poisson distribution.</p><p>The mathematical form of a Gamma distribution is given by:</p><div class="mediaobject"><img src="../Images/image00227.jpeg" alt="Gamma distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00228.jpeg" alt="Gamma distribution"/></span> and <span class="inlinemediaobject"><img src="../Images/image00229.jpeg" alt="Gamma distribution"/></span> are the shape and rate parameters, respectively (both take values greater than zero). There is also a form in terms of the scale parameter <span class="inlinemediaobject"><img src="../Images/image00230.jpeg" alt="Gamma distribution"/></span>, which is common in <a id="id36" class="indexterm"/>
<span class="strong"><strong>econometrics</strong></span>. Another related distribution is the Inverse-Gamma distribution that is the distribution of the reciprocal of a variable that is distributed according to the Gamma distribution. It's mainly used in Bayesian inference as the conjugate prior distribution for the variance of a one-dimensional normal distribution.</p></div><div class="section" title="Dirichlet distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Dirichlet distribution</h2></div></div></div><p>The <a id="id37" class="indexterm"/>Dirichlet distribution<a id="id38" class="indexterm"/> is a multivariate analogue of the Beta distribution. It is commonly used in Bayesian inference as the conjugate prior distribution for multinomial distribution and categorical distribution. The main reason for this is that it is easy to implement inference techniques, such as Gibbs sampling, on the Dirichlet-multinomial distribution.</p><p>The Dirichlet distribution of order <span class="inlinemediaobject"><img src="../Images/image00231.jpeg" alt="Dirichlet distribution"/></span> is defined over an open <span class="inlinemediaobject"><img src="../Images/image00232.jpeg" alt="Dirichlet distribution"/></span> dimensional simplex as <a id="id39" class="indexterm"/>follows:</p><div class="mediaobject"><img src="../Images/image00233.jpeg" alt="Dirichlet distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00234.jpeg" alt="Dirichlet distribution"/></span>, <span class="inlinemediaobject"><img src="../Images/image00235.jpeg" alt="Dirichlet distribution"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00236.jpeg" alt="Dirichlet distribution"/></span>.</p></div><div class="section" title="Wishart distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Wishart distribution</h2></div></div></div><p>The <a id="id40" class="indexterm"/>Wishart distribution<a id="id41" class="indexterm"/> is a multivariate generalization of the Gamma distribution. It is defined over symmetric non-negative matrix-valued random variables. In Bayesian inference, it is used as the conjugate prior to estimate the distribution of inverse of the covariance matrix <span class="inlinemediaobject"><img src="../Images/image00237.jpeg" alt="Wishart distribution"/></span> (or precision matrix) of the normal distribution. When we discussed Gamma distribution, we said it is used as a conjugate distribution for the inverse of the variance of the one-dimensional normal distribution.</p><p>The mathematical definition of the Wishart distribution is as follows:</p><div class="mediaobject"><img src="../Images/image00238.jpeg" alt="Wishart distribution"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00239.jpeg" alt="Wishart distribution"/></span> denotes the determinant of the matrix <span class="inlinemediaobject"><img src="../Images/image00240.jpeg" alt="Wishart distribution"/></span> of dimension <span class="inlinemediaobject"><img src="../Images/image00241.jpeg" alt="Wishart distribution"/></span> and <span class="inlinemediaobject"><img src="../Images/image00242.jpeg" alt="Wishart distribution"/></span> is the degrees of freedom.</p><p>A special <a id="id42" class="indexterm"/>case<a id="id43" class="indexterm"/> of the Wishart distribution is when <span class="inlinemediaobject"><img src="../Images/image00243.jpeg" alt="Wishart distribution"/></span> corresponds to the well-known Chi-Square distribution function with <span class="inlinemediaobject"><img src="../Images/image00244.jpeg" alt="Wishart distribution"/></span> degrees of freedom.</p><p>Wikipedia gives a list of more than 100 useful distributions that are commonly used by statisticians (reference 1 in the <span class="emphasis"><em>Reference</em></span> section of this chapter). Interested readers should refer to this article.</p></div></div>
<div class="section" title="Exercises" id="aid-I3QM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
By using <a id="id44" class="indexterm"/>the definition of conditional probability, show that any multivariate joint distribution of N random variables <span class="inlinemediaobject"><img src="../Images/image00197.jpeg" alt="Exercises"/></span> has the following trivial factorization:
<div class="mediaobject"><img src="../Images/image00245.jpeg" alt="Exercises"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">The bivariate normal distribution is given by:<div class="mediaobject"><img src="../Images/image00246.jpeg" alt="Exercises"/></div><p style="clear:both; height: 1em;"> </p><p>Here:</p><div class="mediaobject"><img src="../Images/image00247.jpeg" alt="Exercises"/></div><p style="clear:both; height: 1em;"> </p><p>By using the definition of conditional probability, show that the conditional distribution <span class="inlinemediaobject"><img src="../Images/image00201.jpeg" alt="Exercises"/></span> can be written as a normal distribution of the form <span class="inlinemediaobject"><img src="../Images/image00248.jpeg" alt="Exercises"/></span> where <span class="inlinemediaobject"><img src="../Images/image00249.jpeg" alt="Exercises"/></span> and <span class="inlinemediaobject"><img src="../Images/image00203.jpeg" alt="Exercises"/></span>.</p></li><li class="listitem">By using <a id="id45" class="indexterm"/>explicit integration of the expression in exercise 2, show that the marginalization of bivariate normal distribution will result in univariate normal distribution.</li><li class="listitem">In the following table, a dataset containing the measurements of petal and sepal sizes of 15 different Iris flowers are shown (taken from the Iris dataset, UCI machine learning dataset repository). All units are in cms:<div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Sepal Length </p>
</th><th valign="bottom">
<p>Sepal Width</p>
</th><th valign="bottom">
<p>Petal Length</p>
</th><th valign="bottom">
<p>Petal Width</p>
</th><th valign="bottom">
<p>Class of Flower</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>5.1</p>
</td><td valign="top">
<p>3.5</p>
</td><td valign="top">
<p>1.4</p>
</td><td valign="top">
<p>0.2</p>
</td><td valign="top">
<p>Iris-setosa</p>
</td></tr><tr><td valign="top">
<p>4.9</p>
</td><td valign="top">
<p>3</p>
</td><td valign="top">
<p>1.4</p>
</td><td valign="top">
<p>0.2</p>
</td><td valign="top">
<p>Iris-setosa</p>
</td></tr><tr><td valign="top">
<p>4.7</p>
</td><td valign="top">
<p>3.2</p>
</td><td valign="top">
<p>1.3</p>
</td><td valign="top">
<p>0.2</p>
</td><td valign="top">
<p>Iris-setosa</p>
</td></tr><tr><td valign="top">
<p>4.6</p>
</td><td valign="top">
<p>3.1</p>
</td><td valign="top">
<p>1.5</p>
</td><td valign="top">
<p>0.2</p>
</td><td valign="top">
<p>Iris-setosa</p>
</td></tr><tr><td valign="top">
<p>5</p>
</td><td valign="top">
<p>3.6</p>
</td><td valign="top">
<p>1.4</p>
</td><td valign="top">
<p>0.2</p>
</td><td valign="top">
<p>Iris-setosa</p>
</td></tr><tr><td valign="top">
<p>7</p>
</td><td valign="top">
<p>3.2</p>
</td><td valign="top">
<p>4.7</p>
</td><td valign="top">
<p>1.4</p>
</td><td valign="top">
<p>Iris-versicolor</p>
</td></tr><tr><td valign="top">
<p>6.4</p>
</td><td valign="top">
<p>3.2</p>
</td><td valign="top">
<p>4.5</p>
</td><td valign="top">
<p>1.5</p>
</td><td valign="top">
<p>Iris-versicolor</p>
</td></tr><tr><td valign="top">
<p>6.9</p>
</td><td valign="top">
<p>3.1</p>
</td><td valign="top">
<p>4.9</p>
</td><td valign="top">
<p>1.5</p>
</td><td valign="top">
<p>Iris-versicolor</p>
</td></tr><tr><td valign="top">
<p>5.5</p>
</td><td valign="top">
<p>2.3</p>
</td><td valign="top">
<p>4</p>
</td><td valign="top">
<p>1.3</p>
</td><td valign="top">
<p>Iris-versicolor</p>
</td></tr><tr><td valign="top">
<p>6.5</p>
</td><td valign="top">
<p>2.8</p>
</td><td valign="top">
<p>4.6</p>
</td><td valign="top">
<p>1.5</p>
</td><td valign="top">
<p>Iris-versicolor</p>
</td></tr><tr><td valign="top">
<p>6.3</p>
</td><td valign="top">
<p>3.3</p>
</td><td valign="top">
<p>6</p>
</td><td valign="top">
<p>2.5</p>
</td><td valign="top">
<p>Iris-virginica</p>
</td></tr><tr><td valign="top">
<p>5.8</p>
</td><td valign="top">
<p>2.7</p>
</td><td valign="top">
<p>5.1</p>
</td><td valign="top">
<p>1.9</p>
</td><td valign="top">
<p>Iris-virginica</p>
</td></tr><tr><td valign="top">
<p>7.1</p>
</td><td valign="top">
<p>3</p>
</td><td valign="top">
<p>5.9</p>
</td><td valign="top">
<p>2.1</p>
</td><td valign="top">
<p>Iris-virginica</p>
</td></tr><tr><td valign="top">
<p>6.3</p>
</td><td valign="top">
<p>2.9</p>
</td><td valign="top">
<p>5.6</p>
</td><td valign="top">
<p>1.8</p>
</td><td valign="top">
<p>Iris-virginica</p>
</td></tr><tr><td valign="top">
<p>6.5</p>
</td><td valign="top">
<p>3</p>
</td><td valign="top">
<p>5.8</p>
</td><td valign="top">
<p>2.2</p>
</td><td valign="top">
<p>Iris-virginica</p>
</td></tr></tbody></table></div><p>Answer<a id="id46" class="indexterm"/> the following questions:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">What is the probability of finding flowers with a sepal length more than 5 cm and a sepal width less than 3 cm?</li><li class="listitem">What is the probability of finding flowers with a petal length less than 1.5 cm; given that petal width is equal to 0.2 cm?</li><li class="listitem">What is the probability of finding flowers with a sepal length less than 6 cm and a petal width less than 1.5 cm; given that the class of the flower is Iris-versicolor?</li></ol><div style="height:10px; width: 1px"/></div></li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-J2B81"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><a class="ulink" href="http://en.wikipedia.org/wiki/List_of_probability_distributions">http://en.wikipedia.org/wiki/List_of_probability_distributions</a></li><li class="listitem">Feller W. <span class="emphasis"><em>An Introduction to Probability Theory and Its Applications</em></span>. Vol. 1. Wiley Series in Probability and Mathematical Statistics. 1968. ISBN-10: 0471257087</li><li class="listitem">Jayes E.T. <span class="emphasis"><em>Probability Theory: The Logic of Science</em></span>. Cambridge University Press. 2003. ISBN-10: 0521592712</li><li class="listitem">Radziwill N.M. <span class="emphasis"><em>Statistics (The Easier Way) with R: an informal text on applied statistics</em></span>. Lapis Lucera. 2015. ISBN-10: 0692339426</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-K0RQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Summary</h1></div></div></div><p>To summarize this chapter, we discussed elements of probability theory; particularly those aspects required for learning Bayesian inference. Due to lack of space, we have not covered many elementary aspects of this subject. There are some excellent books on this subject, for example, books by William Feller (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter), E. T. Jaynes (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter), and M. Radziwill (reference 4 in the <span class="emphasis"><em>References</em></span> section of this chapter). Readers are encouraged to read these to get a more in-depth understanding of probability theory and how it can be applied in real-life situations.</p><p>In the next chapter, we will introduce the R programming language that is the most popular open source framework for data analysis and Bayesian inference in particular.</p></div></body></html>