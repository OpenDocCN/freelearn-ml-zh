<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 12. Augmented Reality"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Augmented Reality</h1></div></div></div><p>In this chapter, you are going to learn about augmented reality and how you can use it to build cool applications. We will discuss pose estimation and plane tracking. You will learn how to map the coordinates from 2D to 3D, and how we can overlay graphics on top of a live video.</p><p>By the end of this chapter, you will know:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is the premise of augmented reality</li><li class="listitem" style="list-style-type: disc">What is pose estimation</li><li class="listitem" style="list-style-type: disc">How to track a planar object</li><li class="listitem" style="list-style-type: disc">How to map coordinates from 3D to 2D</li><li class="listitem" style="list-style-type: disc">How to overlay graphics on top of a video in real time</li></ul></div><div class="section" title="What is the premise of augmented reality?"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec91"/>What is the premise of augmented reality?</h1></div></div></div><p>Before we jump into all the fun stuff, let's understand what augmented reality means. You would have probably seen the term "augmented reality" being used in a variety of contexts. So, we should understand the premise of augmented reality before we start discussing the implementation details. Augmented Reality refers to the superposition of computer-generated input such as imagery, sounds, graphics, and text on top of the real world.</p><p>Augmented reality<a id="id396" class="indexterm"/> tries to blur the line between what's real and what's computer-generated by seamlessly merging the information and enhancing what we see and feel. It is actually closely related to a concept called mediated reality where a computer modifies our view of the reality. As a result of this, the technology works by enhancing our current perception of reality. Now the challenge here is to make it look seamless to the user. It's easy to just overlay something on top of the input video, but we need to make it look like it is part of the video. The user should feel that the computer-generated input is closely following the real world. This is what we want to achieve when we build an augmented reality system.</p><p>Computer vision research in this context explores how we can apply computer-generated imagery to live video streams so that we can enhance the perception of the real world. Augmented reality technology has a wide variety of applications including, but not limited to, head-mounted displays, automobiles, data visualization, gaming, construction, and so on. Now<a id="id397" class="indexterm"/> that we have powerful smartphones and smarter machines, we can build high-end augmented reality applications with ease.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="What does an augmented reality system look like?"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec92"/>What does an augmented reality system look like?</h1></div></div></div><p>Let's consider the following figure:</p><div class="mediaobject"><img src="images/B04554_12_01.jpg" alt="What does an augmented reality system look like?"/></div><p>As we can see here, the<a id="id398" class="indexterm"/> camera captures the real world video to get the reference point. The graphics system generates the virtual objects that need to be overlaid on top of the video. Now the video-merging block is where all the magic happens. This block should be smart enough to understand how to overlay the virtual objects on top of the real world in the best way possible.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Geometric transformations for augmented reality"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec93"/>Geometric transformations for augmented reality</h1></div></div></div><p>The outcome <a id="id399" class="indexterm"/>of augmented reality is amazing, but there<a id="id400" class="indexterm"/> are a lot of mathematical things going on underneath. Augmented reality utilizes a lot of geometric transformations and the associated mathematical functions to make sure everything looks seamless. When talking about a live video for augmented reality, we need to precisely register the virtual objects on top of the real world. To understand it better, let's think of it as an alignment of two cameras—the real one through which we see the world, and the virtual one that projects the computer generated graphical objects.</p><p>In order to build an<a id="id401" class="indexterm"/> augmented reality system, the following geometric transformations need to be established:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Object-to-scene</strong></span>: This<a id="id402" class="indexterm"/> transformation<a id="id403" class="indexterm"/> refers to transforming the 3D coordinates of a virtual object and expressing them in the coordinate frame of our real-world scene. This ensures that we are positioning the virtual object in the right location.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scene-to-camera</strong></span>: This <a id="id404" class="indexterm"/>transformation <a id="id405" class="indexterm"/>refers to the pose of the camera in the real world. By "pose", we mean the orientation and location of the camera. We need to estimate the point of view of the camera so that we know how to overlay the virtual object.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Camera-to-image</strong></span>: This <a id="id406" class="indexterm"/>refers to the <a id="id407" class="indexterm"/>calibration parameters of the camera. This defines how we can project a 3D object onto a 2D image plane. This is the image that we will actually see in the end.</li></ul></div><p>Consider the following image:</p><div class="mediaobject"><img src="images/B04554_12_02.jpg" alt="Geometric transformations for augmented reality"/></div><p>As we can see here, the <a id="id408" class="indexterm"/>car is trying to fit into the scene but it looks very artificial. If we don't convert the coordinates in the right way, it looks unnatural. This is what we were talking about in the object-to-scene transformation! Once we transform the 3D coordinates of the virtual object into the coordinate frame of the real world, we need to estimate the pose of the camera:</p><div class="mediaobject"><img src="images/B04554_12_03.jpg" alt="Geometric transformations for augmented reality"/></div><p>We need to understand the position and rotation of the camera because that's what the user will see. Once we estimate the camera pose, we are ready to put this 3D scene on a 2D image.</p><div class="mediaobject"><img src="images/B04554_12_04.jpg" alt="Geometric transformations for augmented reality"/></div><p>Once we have these <a id="id409" class="indexterm"/>transformations, we can build the complete system.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is pose estimation?"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec94"/>What is pose estimation?</h1></div></div></div><p>Before we proceed, we<a id="id410" class="indexterm"/> need to understand how to estimate the camera pose. This is a very critical step in an augmented reality system and we need to get it right if we want our experience to be seamless. In the world of augmented reality, we overlay graphics on top of an object in real time. In order to do that, we need to know the location and orientation of the camera, and we need to do it quickly. This is where pose estimation becomes very important. If you don't track the pose correctly, the overlaid graphics will not look natural.</p><p>Consider the following image:</p><div class="mediaobject"><img src="images/B04554_12_05.jpg" alt="What is pose estimation?"/></div><p>The arrow line represents that the surface is normal. Let's say the object changes its orientation:</p><div class="mediaobject"><img src="images/B04554_12_06.jpg" alt="What is pose estimation?"/></div><p>Now even though the<a id="id411" class="indexterm"/> location is the same, the orientation has changed. We need to have this information so that the overlaid graphics looks natural. We need to make sure that it's aligned to this orientation as well as position.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="How to track planar objects?"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec95"/>How to track planar objects?</h1></div></div></div><p>Now that you understand <a id="id412" class="indexterm"/>what pose estimation is, let's see how you can use it to track planar objects. Let's consider the following planar object:</p><div class="mediaobject"><img src="images/B04554_12_07.jpg" alt="How to track planar objects?"/></div><p>Now if we extract <a id="id413" class="indexterm"/>feature points from this image, we will see something like this:</p><div class="mediaobject"><img src="images/B04554_12_08.jpg" alt="How to track planar objects?"/></div><p>Let's tilt the<a id="id414" class="indexterm"/> cardboard:</p><div class="mediaobject"><img src="images/B04554_12_09.jpg" alt="How to track planar objects?"/></div><p>As we can see, the <a id="id415" class="indexterm"/>cardboard is tilted in this image. Now if we want to make sure our virtual object is overlaid on top of this surface, we need to gather this planar tilt information. One way to do this is by using the relative positions of those feature points. If we extract the feature points from the preceding image, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_10.jpg" alt="How to track planar objects?"/></div><p>As you can see, the <a id="id416" class="indexterm"/>feature points got closer horizontally on the far end of the plane as compared to the ones on the near end.</p><div class="mediaobject"><img src="images/B04554_12_11.jpg" alt="How to track planar objects?"/></div><p>So we can utilize this<a id="id417" class="indexterm"/> information to extract the orientation information from the image. If you remember, we discussed perspective transformation in detail when we were discussing geometric transformations as well as panoramic imaging. All we need to do is use those two sets of points and extract the homography matrix. This homography matrix will tell us how the cardboard turned.</p><p>Consider the following image:</p><div class="mediaobject"><img src="images/B04554_12_12.jpg" alt="How to track planar objects?"/></div><p>We start by selecting the region of interest.</p><div class="mediaobject"><img src="images/B04554_12_13.jpg" alt="How to track planar objects?"/></div><p>We then extract <a id="id418" class="indexterm"/>feature points from this region of interest. Since we are tracking planar objects, the algorithm assumes that this region of interest is a plane. That was obvious, but it's better to state it explicitly! So make sure you have a cardboard in your hand when you select this region of interest. Also, it'll be better if the cardboard has a bunch of patterns and distinctive points so that it's easy to detect and track the feature points on it.</p><p>Let the tracking begin! We'll move the cardboard around to see what happens:</p><div class="mediaobject"><img src="images/B04554_12_14.jpg" alt="How to track planar objects?"/></div><p>As you can see, the<a id="id419" class="indexterm"/> feature points are being tracked inside the region of interest. Let's tilt it and see what happens:</p><div class="mediaobject"><img src="images/B04554_12_15.jpg" alt="How to track planar objects?"/></div><p>Looks like the feature points are being tracked properly. As we can see, the overlaid rectangle is<a id="id420" class="indexterm"/> changing its orientation according to the surface of the cardboard.</p><p>Here is the code to do it:</p><div class="informalexample"><pre class="programlisting">import sys
from collections import namedtuple

import cv2
import numpy as np

class PoseEstimator(object):
    def __init__(self):
        # Use locality sensitive hashing algorithm
        flann_params = dict(algorithm = 6, table_number = 6,
                key_size = 12, multi_probe_level = 1)

        self.min_matches = 10
        self.cur_target = namedtuple('Current', 'image, rect, keypoints, descriptors, data')
        self.tracked_target = namedtuple('Tracked', 'target, points_prev, points_cur, H, quad')

        self.feature_detector = cv2.ORB(nfeatures=1000)
        self.feature_matcher = cv2.FlannBasedMatcher(flann_params, {})
        self.tracking_targets = []

    # Function to add a new target for tracking
    def add_target(self, image, rect, data=None):
        x_start, y_start, x_end, y_end = rect
        keypoints, descriptors = [], []
        for keypoint, descriptor in zip(*self.detect_features(image)):
            x, y = keypoint.pt
            if x_start &lt;= x &lt;= x_end and y_start &lt;= y &lt;= y_end:
                keypoints.append(keypoint)
                descriptors.append(descriptor)

        descriptors = np.array(descriptors, dtype='uint8')
        self.feature_matcher.add([descriptors])
        target = self.cur_target(image=image, rect=rect, keypoints=keypoints,
                    descriptors=descriptors, data=None)
        self.tracking_targets.append(target)

    # To get a list of detected objects
    def track_target(self, frame):
        self.cur_keypoints, self.cur_descriptors = self.detect_features(frame)
        if len(self.cur_keypoints) &lt; self.min_matches:
            return []

        matches = self.feature_matcher.knnMatch(self.cur_descriptors, k=2)
        matches = [match[0] for match in matches if len(match) == 2 and
                    match[0].distance &lt; match[1].distance * 0.75]
        if len(matches) &lt; self.min_matches:
            return []

        matches_using_index = [[] for _ in xrange(len(self.tracking_targets))]
        for match in matches:
            matches_using_index[match.imgIdx].append(match)

        tracked = []
        for image_index, matches in enumerate(matches_using_index):
            if len(matches) &lt; self.min_matches:
                continue

            target = self.tracking_targets[image_index]
            points_prev = [target.keypoints[m.trainIdx].pt for m in matches]
            points_cur = [self.cur_keypoints[m.queryIdx].pt for m in matches]
            points_prev, points_cur = np.float32((points_prev, points_cur))
            H, status = cv2.findHomography(points_prev, points_cur, cv2.RANSAC, 3.0)
            status = status.ravel() != 0
            if status.sum() &lt; self.min_matches:
                continue

            points_prev, points_cur = points_prev[status], points_cur[status]

            x_start, y_start, x_end, y_end = target.rect
            quad = np.float32([[x_start, y_start], [x_end, y_start], [x_end, y_end], [x_start, y_end]])
            quad = cv2.perspectiveTransform(quad.reshape(1, -1, 2), H).reshape(-1, 2)

            track = self.tracked_target(target=target, points_prev=points_prev,
                        points_cur=points_cur, H=H, quad=quad)
            tracked.append(track)

        tracked.sort(key = lambda x: len(x.points_prev), reverse=True)
        return tracked

    # Detect features in the selected ROIs and return the keypoints and descriptors
    def detect_features(self, frame):
        keypoints, descriptors = self.feature_detector.detectAndCompute(frame, None)
        if descriptors is None:
            descriptors = []

        return keypoints, descriptors

    # Function to clear all the existing targets
    def clear_targets(self):
        self.feature_matcher.clear()
        self.tracking_targets = []

class VideoHandler(object):
    def __init__(self):
        self.cap = cv2.VideoCapture(0)
        self.paused = False
        self.frame = None
        self.pose_tracker = PoseEstimator()

        cv2.namedWindow('Tracker')
        self.roi_selector = ROISelector('Tracker', self.on_rect)

    def on_rect(self, rect):
        self.pose_tracker.add_target(self.frame, rect)

    def start(self):
        while True:
            is_running = not self.paused and self.roi_selector.selected_rect is None

            if is_running or self.frame is None:
                ret, frame = self.cap.read()
                scaling_factor = 0.5
                frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor,
                        interpolation=cv2.INTER_AREA)
                if not ret:
                    break

                self.frame = frame.copy()

            img = self.frame.copy()
            if is_running:
                tracked = self.pose_tracker.track_target(self.frame)
                for item in tracked:
                    cv2.polylines(img, [np.int32(item.quad)], True, (255, 255, 255), 2)
                    for (x, y) in np.int32(item.points_cur):
                        cv2.circle(img, (x, y), 2, (255, 255, 255))

            self.roi_selector.draw_rect(img)
            cv2.imshow('Tracker', img)
            ch = cv2.waitKey(1)
            if ch == ord(' '):
                self.paused = not self.paused
            if ch == ord('c'):
                self.pose_tracker.clear_targets()
            if ch == 27:
                break

class ROISelector(object):
    def __init__(self, win_name, callback_func):
        self.win_name = win_name
        self.callback_func = callback_func
        cv2.setMouseCallback(self.win_name, self.on_mouse_event)
        self.selection_start = None
        self.selected_rect = None

    def on_mouse_event(self, event, x, y, flags, param):
        if event == cv2.EVENT_LBUTTONDOWN:
            self.selection_start = (x, y)

        if self.selection_start:
            if flags &amp; cv2.EVENT_FLAG_LBUTTON:
                x_orig, y_orig = self.selection_start
                x_start, y_start = np.minimum([x_orig, y_orig], [x, y])
                x_end, y_end = np.maximum([x_orig, y_orig], [x, y])
                self.selected_rect = None
                if x_end &gt; x_start and y_end &gt; y_start:
                    self.selected_rect = (x_start, y_start, x_end, y_end)
            else:
                rect = self.selected_rect
                self.selection_start = None
                self.selected_rect = None
                if rect:
                    self.callback_func(rect)

    def draw_rect(self, img):
        if not self.selected_rect:
            return False

        x_start, y_start, x_end, y_end = self.selected_rect
        cv2.rectangle(img, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)
        return True

if __name__ == '__main__':
    VideoHandler().start()</pre></div><div class="section" title="What happened inside the code?"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec49"/>What happened inside the code?</h2></div></div></div><p>To start with, we <a id="id421" class="indexterm"/>have a <code class="literal">PoseEstimator</code> class that does all the heavy lifting here. We need something to detect the features in the image and something to match the features between successive images. So we use the ORB feature detector and the Flann feature matcher. As we can see, we initialize the class with these parameters in the constructor.</p><p>Whenever we select a region of interest, we call the <code class="literal">add_target</code> method to add that to our list of tracking targets. This method just extracts the features from that region of interest and stores in one of the class variables. Now that we have a target, we are ready to track it!</p><p>The <code class="literal">track_target</code> method handles all the tracking. We take the current frame and extract all the keypoints. However, we are not really interested in all the keypoints in the current frame of the video. We just want the keypoints that belong to our target object. So now, our job is to find the closest keypoints in the current frame.</p><p>We now have a<a id="id422" class="indexterm"/> set of keypoints in the current frame and we have another set of keypoints from our target object in the previous frame. The next step is to extract the homography matrix from these matching points. This homography matrix tells us how to transform the overlaid rectangle so that it's aligned with the cardboard surface. We just need to take this homography matrix and apply it to the overlaid rectangle to obtain the new positions of all its points.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="How to augment our reality?"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec96"/>How to augment our reality?</h1></div></div></div><p>Now that we<a id="id423" class="indexterm"/> know how to track planar objects, let's see how to overlay 3D objects on top of the real world. The objects are 3D but the video on our screen is 2D. So the first step here is to understand how to map those 3D objects to 2D surfaces so that it looks realistic. We just need to project those 3D points onto planar surfaces.</p><div class="section" title="Mapping coordinates from 3D to 2D"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec50"/>Mapping coordinates from 3D to 2D</h2></div></div></div><p>Once we estimate the <a id="id424" class="indexterm"/>pose, we project the points from the 3D to the 2D. Consider the following image:</p><div class="mediaobject"><img src="images/B04554_12_16.jpg" alt="Mapping coordinates from 3D to 2D"/></div><p>As we can see here, the <a id="id425" class="indexterm"/>TV remote control is a 3D object but we are seeing it on a 2D plane. Now if we move it around, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_17.jpg" alt="Mapping coordinates from 3D to 2D"/></div><p>This 3D object is still <a id="id426" class="indexterm"/>on a 2D plane. The object has moved to a different location and the distance from the camera has changed as well. How do we compute these coordinates? We need a mechanism to map this 3D object onto the 2D surface. This is where the 3D to 2D projection becomes really important.</p><p>We just need to estimate the initial camera pose to start with. Now, let's assume that the intrinsic parameters of the camera are already known. So we can just use the <code class="literal">solvePnP</code> function in OpenCV to estimate the camera's pose. This function is used to estimate the object's pose using a set of points. You can <a id="id427" class="indexterm"/>read more about it at <a class="ulink" href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#bool%20solvePnP(InputArray%20objectPoints,%20InputArray%20imagePoints,%20InputArray%20cameraMatrix,%20InputArray%20distCoeffs,%20OutputArray%20rvec,%20OutputArray%20tvec,%20bool%20useExtrinsicGuess,%20int%20flags)">http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#bool solvePnP(InputArray objectPoints, InputArray imagePoints, InputArray cameraMatrix, InputArray distCoeffs, OutputArray rvec, OutputArray tvec, bool useExtrinsicGuess, int flags)</a>. Once we do this, we need to project these points onto 2D. We use the OpenCV function <code class="literal">projectPoints</code> to do this. This function calculates the projections of those 3D points onto the 2D plane.</p></div><div class="section" title="How to overlay 3D objects on a video?"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec51"/>How to overlay 3D objects on a video?</h2></div></div></div><p>Now that we have all <a id="id428" class="indexterm"/>the<a id="id429" class="indexterm"/> different blocks, we are ready to build the final system. Let's say we want to overlay a pyramid on top of our cardboard as shown here:</p><div class="mediaobject"><img src="images/B04554_12_18.jpg" alt="How to overlay 3D objects on a video?"/></div><p>Let's tilt the cardboard to see what happens:</p><div class="mediaobject"><img src="images/B04554_12_19.jpg" alt="How to overlay 3D objects on a video?"/></div><p>Looks like the <a id="id430" class="indexterm"/>pyramid is following the surface. Let's add a <a id="id431" class="indexterm"/>second target:</p><div class="mediaobject"><img src="images/B04554_12_20.jpg" alt="How to overlay 3D objects on a video?"/></div><p>You can keep<a id="id432" class="indexterm"/> adding more targets and all those pyramids will <a id="id433" class="indexterm"/>be tracked nicely. Let's see how to do this using OpenCV Python. Make sure to save the previous file as <code class="literal">pose_estimation.py</code> because we will be importing a couple of classes from there:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

from pose_estimation import PoseEstimator, ROISelector

class Tracker(object):
    def __init__(self):
        self.cap = cv2.VideoCapture(0)
        self.frame = None
        self.paused = False
        self.tracker = PoseEstimator()

        cv2.namedWindow('Augmented Reality')
        self.roi_selector = ROISelector('Augmented Reality', self.on_rect)

        self.overlay_vertices = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0],
                               [0.5, 0.5, 4]])
        self.overlay_edges = [(0, 1), (1, 2), (2, 3), (3, 0),
                    (0,4), (1,4), (2,4), (3,4)]
        self.color_base = (0, 255, 0)
        self.color_lines = (0, 0, 0)

    def on_rect(self, rect):
        self.tracker.add_target(self.frame, rect)

    def start(self):
        while True:
            is_running = not self.paused and self.roi_selector.selected_rect is None
            if is_running or self.frame is None:
                ret, frame = self.cap.read()
                scaling_factor = 0.5
                frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor,
                        interpolation=cv2.INTER_AREA)
                if not ret:
                    break

                self.frame = frame.copy()

            img = self.frame.copy()
            if is_running:
                tracked = self.tracker.track_target(self.frame)
                for item in tracked:
                    cv2.polylines(img, [np.int32(item.quad)], True, self.color_lines, 2)
                    for (x, y) in np.int32(item.points_cur):
                        cv2.circle(img, (x, y), 2, self.color_lines)

                    self.overlay_graphics(img, item)

            self.roi_selector.draw_rect(img)
            cv2.imshow('Augmented Reality', img)
            ch = cv2.waitKey(1)
            if ch == ord(' '):
                self.paused = not self.paused
            if ch == ord('c'):
                self.tracker.clear_targets()
            if ch == 27:
                break

    def overlay_graphics(self, img, tracked):
        x_start, y_start, x_end, y_end = tracked.target.rect
        quad_3d = np.float32([[x_start, y_start, 0], [x_end, y_start, 0],
                    [x_end, y_end, 0], [x_start, y_end, 0]])
        h, w = img.shape[:2]
        K = np.float64([[w, 0, 0.5*(w-1)],
                        [0, w, 0.5*(h-1)],
                        [0, 0, 1.0]])
        dist_coef = np.zeros(4)
        ret, rvec, tvec = cv2.solvePnP(quad_3d, tracked.quad, K, dist_coef)
        verts = self.overlay_vertices * [(x_end-x_start), (y_end-y_start),
                    -(x_end-x_start)*0.3] + (x_start, y_start, 0)
        verts = cv2.projectPoints(verts, rvec, tvec, K, dist_coef)[0].reshape(-1, 2)

        verts_floor = np.int32(verts).reshape(-1,2)
        cv2.drawContours(img, [verts_floor[:4]], -1, self.color_base, -3)
        cv2.drawContours(img, [np.vstack((verts_floor[:2], verts_floor[4:5]))],
                    -1, (0,255,0), -3)
        cv2.drawContours(img, [np.vstack((verts_floor[1:3], verts_floor[4:5]))],
                    -1, (255,0,0), -3)
        cv2.drawContours(img, [np.vstack((verts_floor[2:4], verts_floor[4:5]))],
                    -1, (0,0,150), -3)
        cv2.drawContours(img, [np.vstack((verts_floor[3:4], verts_floor[0:1],
                    verts_floor[4:5]))], -1, (255,255,0), -3)

        for i, j in self.overlay_edges:
            (x_start, y_start), (x_end, y_end) = verts[i], verts[j]
            cv2.line(img, (int(x_start), int(y_start)), (int(x_end), int(y_end)), self.color_lines, 2)

if __name__ == '__main__':
    Tracker().start()</pre></div></div><div class="section" title="Let's look at the code"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec52"/>Let's look at the code</h2></div></div></div><p>The class <code class="literal">Tracker</code> is used to<a id="id434" class="indexterm"/> perform all the computations here. We initialize <a id="id435" class="indexterm"/>the class with the pyramid structure that is defined using edges and vertices. The logic that we use to track the surface is the same as we discussed earlier because we are using the same class. We just need to use <code class="literal">solvePnP</code> and <code class="literal">projectPoints</code> to map the 3D pyramid to the 2D surface.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Let's add some movements"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec97"/>Let's add some movements</h1></div></div></div><p>Now that we know how to add a virtual pyramid, let's see if we can add some movements. Let's see how we can <a id="id436" class="indexterm"/>dynamically change the height of the pyramid. When you start, the pyramid will look like this:</p><div class="mediaobject"><img src="images/B04554_12_21.jpg" alt="Let's add some movements"/></div><p>If you wait for some time, the pyramid gets taller and it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_22.jpg" alt="Let's add some movements"/></div><p>Let's see how to do <a id="id437" class="indexterm"/>it in OpenCV Python. Inside the augmented reality code that we just discussed, add the following snippet at the end of the <code class="literal">__init__</code> method in the <code class="literal">Tracker</code> class:</p><div class="informalexample"><pre class="programlisting">self.overlay_vertices = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0], [0.5, 0.5, 4]])
self.overlay_edges = [(0, 1), (1, 2), (2, 3), (3, 0),
            (0,4), (1,4), (2,4), (3,4)]
self.color_base = (0, 255, 0)
self.color_lines = (0, 0, 0)

self.graphics_counter = 0
self.time_counter = 0</pre></div><p>Now that we have the structure, we need to add the code to dynamically change the height. Replace the <code class="literal">overlay_graphics()</code> method with the following method:</p><div class="informalexample"><pre class="programlisting">def overlay_graphics(self, img, tracked):
    x_start, y_start, x_end, y_end = tracked.target.rect
    quad_3d = np.float32([[x_start, y_start, 0], [x_end, y_start, 0],
                [x_end, y_end, 0], [x_start, y_end, 0]])
    h, w = img.shape[:2]
    K = np.float64([[w, 0, 0.5*(w-1)],
                    [0, w, 0.5*(h-1)],
                    [0, 0, 1.0]])
    dist_coef = np.zeros(4)
    ret, rvec, tvec = cv2.solvePnP(quad_3d, tracked.quad, K, dist_coef)

    self.time_counter += 1
    if not self.time_counter % 20:
        self.graphics_counter = (self.graphics_counter + 1) % 8

    self.overlay_vertices = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0],
                           [0.5, 0.5, self.graphics_counter]])

    verts = self.overlay_vertices * [(x_end-x_start), (y_end-y_start),
                -(x_end-x_start)*0.3] + (x_start, y_start, 0)
    verts = cv2.projectPoints(verts, rvec, tvec, K, dist_coef)[0].reshape(-1, 2)

    verts_floor = np.int32(verts).reshape(-1,2)
    cv2.drawContours(img, [verts_floor[:4]], -1, self.color_base, -3)
    cv2.drawContours(img, [np.vstack((verts_floor[:2], verts_floor[4:5]))],
                -1, (0,255,0), -3)
    cv2.drawContours(img, [np.vstack((verts_floor[1:3], verts_floor[4:5]))],
                -1, (255,0,0), -3)
    cv2.drawContours(img, [np.vstack((verts_floor[2:4], verts_floor[4:5]))],
                -1, (0,0,150), -3)
    cv2.drawContours(img, [np.vstack((verts_floor[3:4], verts_floor[0:1],
                verts_floor[4:5]))], -1, (255,255,0), -3)

    for i, j in self.overlay_edges:
        (x_start, y_start), (x_end, y_end) = verts[i], verts[j]
        cv2.line(img, (int(x_start), int(y_start)), (int(x_end), int(y_end)), self.color_lines, 2)</pre></div><p>Now that we <a id="id438" class="indexterm"/>know how to change the height, let's go ahead and make the pyramid dance for us. We can make the tip of the pyramid oscillate in a nice periodic fashion. So when you start, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_23.jpg" alt="Let's add some movements"/></div><p>If you wait for<a id="id439" class="indexterm"/> some time, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_24.jpg" alt="Let's add some movements"/></div><p>You can look at <code class="literal">augmented_reality_motion.py</code> for the implementation details.</p><p>In our next<a id="id440" class="indexterm"/> experiment, we will make the whole pyramid move around the region of interest. We can make it move in any way we want. Let's start by adding linear diagonal movement around our selected region of interest. When you start, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_25.jpg" alt="Let's add some movements"/></div><p>After some time, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_26.jpg" alt="Let's add some movements"/></div><p>Refer <a id="id441" class="indexterm"/>to <code class="literal">augmented_reality_dancing.py</code> to see how to change the <code class="literal">overlay_graphics()</code> method to make it dance. Let's see if we can make the pyramid go around in circles around our region of interest. When you start, it will look like this:</p><div class="mediaobject"><img src="images/B04554_12_27.jpg" alt="Let's add some movements"/></div><p>After some<a id="id442" class="indexterm"/> time, it will move to a new position:</p><div class="mediaobject"><img src="images/B04554_12_28.jpg" alt="Let's add some movements"/></div><p>You can<a id="id443" class="indexterm"/> refer to <code class="literal">augmented_reality_circular_motion.py</code> to see how to make this happen. You can make it do anything you want. You just need to come up with the right mathematical formula and the pyramid will literally dance to your tune! You can also try out other virtual objects to see what you can with it. There are a lot of things you can do with a lot of different objects. These examples provide a good reference point, on top of which you can build many interesting augmented reality applications.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec98"/>Summary</h1></div></div></div><p>In this chapter, you learned about the premise of augmented reality and understood what an augmented reality system looks like. We discussed the geometric transformations required for augmented reality. You learned how to use those transformations to estimate the camera pose. You learned how to track planar objects. We discussed how we can add virtual objects on top of the real world. You learned how to modify the virtual objects in different ways to add cool effects. Remember that the world of computer vision is filled with endless possibilities! This book is designed to teach you the necessary skills to get started on a wide variety of projects. Now it's up to you and your imagination to use the skills you have acquired here to build something unique and interesting.</p></div></div>
</body></html>