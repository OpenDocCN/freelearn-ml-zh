- en: Scala for Tree-Based Ensemble Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we solved both classification and regression problems
    using linear models. We also used logistic regression, support vector machine,
    and Naive Bayes. However, in both cases, we haven't experienced good accuracy
    because our models showed low confidence.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, tree-based and tree ensemble classifiers are really useful,
    robust, and widely used for both classification and regression tasks. This chapter
    will provide a quick glimpse at developing these classifiers and regressors using
    tree-based and ensemble techniques, such as **decision trees** (**DTs**), **random
    forests** (**RF**), and **gradient boosted trees** (**GBT**), for both classification
    and regression. More specifically, we will revisit and solve both the regression
    (from [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala for Regression
    Analysis*) and classification (from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*) problems we discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and tree ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees for supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosted trees for supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest for supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following playlist to see the Code in Action video for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2WhQf2i](http://bit.ly/2WhQf2i)'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and tree ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DTs normally fall under supervised learning techniques, which are used to identify
    and solve problems related to classification and regression. As the name indicates,
    DTs have various branches—where each branch indicates a possible decision, appearance,
    or reaction in terms of statistical probability. In terms of features, DTs are
    split into two main types: the training set and the test set, which helps produce
    a good update on the predicted labels or classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both binary and multiclass classification problems can be handled by DT algorithms,
    which is one of the reasons it is used across problems. For instance, for the
    admission example we introduced in [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, DTs learn from the admission data to approximate
    a sine curve with a set of `if...else` decision rules, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b07f5aa1-744a-4782-8fc6-4ac4bdab1cc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating decision rules using DTs based on university admission data
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the bigger the tree, the more complex the decision rules and the
    more fitted the model is. Another exciting power of DTs is that they can be used
    to solve both classification and regression problems. Now let''s see some pros
    and cons of DTs. The two widely-used tree-based ensemble techniques are RF and
    GBT. The main difference between these two techniques is the way and order in
    which trees are trained:'
  prefs: []
  type: TYPE_NORMAL
- en: RFs train each tree independently but based on a random sample of the data.
    These random samples help to make the model more robust than a single DT, and
    hence it is less likely to have an overload on the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GBTs train one tree at a time. The errors created by the trees trained previously
    will be rectified by every new tree that is trained. As more trees are added,
    the model becomes more expressive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RFs take a subset of observations and a subset of variables to build, which
    is an ensemble of DTs. These trees are actually trained on different parts of
    the same training set, but individual trees grow very deep tends to learn from
    highly unpredictable patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes very deep trees are responsible for overfitting problems in DT models.
    In addition, these biases can make the classifier a low performer even if the
    quality of the features represented is good with respect to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'When DTs are built, RFs integrate them together to get a more accurate and
    stable prediction. RFs helps to average multiple DTs together, with the goal of
    reducing the variance to ensure consistency by computing proximities between pairs
    of cases. This is a direct consequence on RF too. By maximum voting from a panel
    of independent jurors, we get the final prediction that is better than the best
    jury. The following figure shows how the decisions from two forests are ensembled
    together to get the final prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/debf3299-9124-459b-92b1-c5682ebce220.png)'
  prefs: []
  type: TYPE_IMG
- en: Tree-based ensemble and its assembling technique
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, both RF and GBT produce a weighted collection of DT, which is followed
    by predicting the combining results from the individual trees of each ensemble
    model. When using these approaches (as a classifier or regressor), the parameter
    settings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the number of trees is 1, no bootstrapping is applied. If the number of trees
    is greater than 1, bootstrapping is applied, with `auto`, `all`, `sqrt`, `log2`,
    and one-third being the supported values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supported numerical values are [0.0-1.0] and [1-n]. If `numTrees` is `1`,
    `featureSubsetStrategy` is set to be `all`. If `numTrees > 1` (for RF), `featureSubsetStrategy`
    is set to be `sqrt` for classification. If `featureSubsetStrategy` is chosen as
    `auto`, the algorithm infers the best feature subset strategy automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impurity criterion is used only for the information-gain calculation, with
    gini and variance as the supported values for classification and regression, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxDepth` is the maximum depth of the tree (for example, depth 0 means 1 leaf
    node, depth 1 means 1 internal node and 2 leaf nodes, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxBins` signifies the maximum number of bins used to split the features,
    where the suggested value is 100 to get better results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've already dealt with both regression analysis and classification
    problems, let's see how to use DT, RF, and GBT more comfortably to solve these
    problems. Let's get started with DT.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees for supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll see how to use DTs to solve both regression and classification
    problems. In the previous two chapters, [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml),
    *Scala for Regression Analysis*, and [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, we solved customer churn and insurance-severity
    claim problems. Those were classification and regression problems, respectively.
    In both approaches, we used other classic models. However, we'll see how we can
    solve them with tree-based and ensemble techniques. We'll use the DT implementation
    from the Apache Spark ML package in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, we know the customer churn prediction problem in [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, and we know the data as well. We also know the working
    principle of DTs. So we can directly move to the coding part using the Spark based
    implementation of DTs. First we create a `DecisionTreeClassifier` estimator by
    instantiating the `DecisionTreeClassifier` interface. Additionally, we need to
    specify the label and feature vector columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed in the previous chapters, we have three transformers (`ipindexer`,
    `labelindexer`, and `assembler`) and an estimator (`dTree`). We can now chain
    them together in a single pipeline so that each of them will act as a stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we would like to perform hyperparameter tuning and cross-validation,
    we will have to create a `paramGrid` variable, which will be used for grid search
    over the hyperparameter space during the K-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: More specifically, this will search for the DT's `impurity`, `maxBins`, and
    `maxDepth` for the best model. The maximum number of bins is used for separating
    continuous features and for choosing how to split on features at each node. Combined,
    the algorithm searches through the DT's `maxDepth` and `maxBins` parameters for
    the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code segment, we''re creating a progressive `paramGrid` variable,
    where we specify the combination as a list of string or integer values. That means
    we are creating the grid space with different hyperparameter combinations. This
    will help us to provide the best model, consisting of optimal hyperparameters.
    However, for that, we need to have a `BinaryClassificationEvaluator` evaluator
    to evaluate each model and pick the best one during the cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `CrossValidator` to perform 10-fold cross validation to select the best
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now call the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to evaluate the predictive power of the DT model on the test
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2d26de4-9bd0-4238-a36d-ddf1cba4948d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, based on the preceding prediction DataFrame, it is really difficult
    to guess the classification''s accuracy. But in the second step, the evaluation
    is done using `BinaryClassificationEvaluator` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide an output with an accuracy value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we get about 84% classification accuracy from our binary classification
    model. Just like with SVM and LR, we will observe the area under the precision-recall
    curve and the area under the **receiver operating characteristic** (**ROC**) curve
    based on the following RDD, which contains the raw scores on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding RDD can be used for computing the previously mentioned two performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 84% accuracy but only 67% precision, which
    is much better than that of SVM and LR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate some more metrics, for example, false and true positive,
    and false and true negative, as these predictions are also useful to evaluate
    the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s observe how high the model confidence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Fantastic! We achieved only 70% accuracy, which is probably why we had a low
    number of trees, but for what factors?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see at what level we achieved the best model after the cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the following output, we achieved the best tree model at `depth
    5` with `53 nodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract those moves taken (that is, decisions) during tree construction
    by showing the tree. This tree helps us to find the most valuable features in
    our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following output, the `toDebugString()` method prints the tree''s decision
    nodes and final the prediction outcomes at the end leaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see that certain features (`3` and `11` in our case) are used for
    decision making—that is, the two most important reasons customers are likely to
    churn. But what are those two features? Let''s see them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the following output, feature 3 and 11 were most important predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The customer service calls and total day minutes are selected by DTs, since
    they provide an automated mechanism for determining the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala for Learning
    Classification*, we learned how to predict the problem regarding slowness in traffic.
    We applied **linear regression** (**LR**) and generalized linear regression to
    solve this problem. Also, we knew the data very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated earlier, DT also can provide very powerful responses and performance
    in the case of a regression problem. Similar to `DecisionTreeClassifier`, a `DecisionTreeRegressor`
    estimator can be instantiated with the `DecisionTreeRegressor()` method. Additionally,
    we need to explicitly specify the label and feature columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can set the max bins, number of trees, max depth, and impurity while instantiating
    the preceding estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since we''ll perform k-fold cross-validation, we can set those parameters
    while creating `paramGrid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as a part of model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust number of folds based on your settings
    and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train DT regression model with cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the fitted model, we can make predictions. So let''s start
    evaluating the model on the train and validation set and calculate RMSE, MSE,
    MAE, R squared, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the best-fitted and cross-validated model, we can expect a good
    prediction accuracy. Let''s observe the result on the train and the validation
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have managed to compute the raw prediction on the train set and the
    test set, and we can see the improvements compared to LR regression model. Let''s
    hunt for the model that will help us to achieve better accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can see how the decisions were made by observing DTs in the
    forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With DTs, it is possible to measure the feature importance, so that in a later
    stage we can decide which features to use and which ones to drop from the DataFrame.
    Let''s find the feature importance out of the best model we just created before
    for all the features that are arranged in an ascending order as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the feature importance generated by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The last result is important to understand the feature importance. As you can
    see, RF has ranked some features to be more important. For example, the last few
    features are the most important ones, while eight of them are less important.
    We can drop those unimportant columns and train the DT model again to observe
    whether there is any greater reduction of MAE and increase in R-squared on the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted trees for supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll see how to use GBT to solve both regression and classification
    problems. In the previous two chapters, [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala
    for Regression Analysis*, and [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, we solved the customer churn and insurance severity
    claim problems, which were classification and regression problem, respectively.
    In both approaches, we used other classic models. However, we'll see how we can
    solve them with tree-based and ensemble techniques. We'll use the GBT implementation
    from the Spark ML package in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted trees for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know the customer churn prediction problem from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, and we know the data well. We already know
    the working principles of RF, so let''s start using the Spark-based implementation
    of RF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a `GBTClassifier` estimator by invoking the `GBTClassifier()` interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have three transformers and an estimator ready. Chain in a single pipeline,
    that is, each of them acts a stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `paramGrid` variable to perform such a grid search over the hyperparameter
    space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CrossValidator` for performing 10-fold cross validation for best
    model selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s call now the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and DT classifier, is executed multiple times—each
    time with a different hyperparameter vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to evaluate the predictive power of DT model on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the test set with the model pipeline, which will update the features
    as per the same mechanism we described in the preceding feature engineering step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5764c41b-e8ca-46e1-8139-1361a875ed98.png)'
  prefs: []
  type: TYPE_IMG
- en: However, after seeing the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in the second step, in the evaluation is done using `BinaryClassificationEvaluator`
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the classification accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So we get about 87% classification accuracy from our binary classification
    model. Just like with SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD, which contains
    the raw scores on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding RDD can be used for computing the previously mentioned performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will share the value in terms of accuracy and prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 87% accuracy but only 73% precision, which
    is much better than SVM and LR. Then we calculate some more false and true metrics.
    Positive and negative predictions can also be useful to evaluate the model''s
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s observe how high the model confidence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s take a look at the true positive, false positive, true negative,
    and false negative rates. Additionally, we see the MCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: These rates looks promising as we experienced positive MCC that shows mostly
    positive correlation indicating a robust classifier. Now, similar to DTs, RFs
    can be debugged during the classification. For the tree to be printed and to select
    the most important features, run the last few lines of code in the DT. Note that
    we still confine the hyperparameter space with `numTrees`, `maxBins`, and `maxDepth`
    by limiting them to `7`. Remember that bigger trees will most likely perform better.
    Therefore, feel free to play around with this code and add features, and also
    use a bigger hyperparameter space, for instance, bigger trees.
  prefs: []
  type: TYPE_NORMAL
- en: GBTs for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To reduce the size of a loss function, GBTs will train many DTs. For each instance,
    the algorithm will use the ensemble that is currently available to predict the
    label of each training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to decision trees, GBTs can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Handle both categorical and numerical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be used for both binary classification and regression (multiclass classification
    is not yet supported)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not require feature scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture non-linearity and feature interactions from very high-dimensional datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suppose we have *N* data instances (being *x[i]* = features of instance *i*)
    and *y* is the label (being *y[i]* = label of instance *i*), then *f(x[i])* is
    GBT model''s predicted label for instance *i*, which tries to minimize any of
    the following losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65f6f4b8-e90a-47da-a3c0-030f4ff1d493.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/880a15a7-d9c3-4c57-ac07-1527f8b1fb83.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a742e469-420d-40f4-aa94-c9bbef6a0223.png)'
  prefs: []
  type: TYPE_IMG
- en: The first equation is called the *log* loss, which is twice the binomial negative
    *log* likelihood. The second one called squared error is commonly referred to
    as *L2* loss and the default loss for GBT-based regression task. Finally, the
    third, called absolute error, is commonly referred to as *L1* loss and is recommended
    if the data points have many outliers and robust than squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know the minimum working principle of the GBT regression algorithm,
    we can get started. Let''s instantiate a `GBTRegressor` estimator by invoking
    the `GBTRegressor()` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can set the max bins, number of trees, max depth, and impurity when instantiating
    the preceding estimator. However, since we''ll perform k-fold cross-validation,
    we can set those parameters while creating the `paramGrid` variable too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '**Validation while training**: Gradient boosting can overfit, especially when
    you train your model with more trees. In order to prevent this issue, it is useful
    to validate (for example, using cross-validation) while carrying out the training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as part of the model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust the number of folds based on your
    settings and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train the `RandomForestRegression` model with cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the fitted model, we can make predictions. Let''s start evaluating
    the model on the train and validation sets and calculate RMSE, MSE, MAE, and R
    squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the best-fitted and cross-validated model, we can expect a high
    prediction accuracy. Now let''s observe the result on the train and the validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    set, and we can see the improvements compared to the LR, DT, and GBT regression
    models. Let''s hunt for the model that helps us to achieve better accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can see how the decisions were made by observing the DTs in
    the forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following output, the toDebugString() method prints the tree''s decision
    nodes and final prediction outcomes at the end leaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'With random forest, it is possible to measure the feature importance so that
    in a later stage, we can decide which features to use and which ones to drop from
    the DataFrame. Let''s find the feature importance out of the best model we just
    created before for all the features that are arranged in an ascending order as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the feature importance generated by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The last result is important to understand the feature importance. As you can
    see, the RF has ranked some features that looks to be more important. For example,
    the last two features are the most important and the first two are less important.
    We can drop some unimportant columns and train the RF model to observe whether
    there is any reduction in the R-squared and MAE values on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest for supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll see how to use RF to solve both regression and classification
    problems. We''ll use DT implementation from the Spark ML package in Scala. Although
    both GBT and RF are ensembles of trees, the training processes are different.
    For instance, RF uses the bagging technique to perform the example, while GBT
    uses boosting. Nevertheless, there are several practical trade-offs between both
    the ensembles that can pose a dilemma about what to choose. However, RF would
    be the winner in most of the cases. Here are some justifications:'
  prefs: []
  type: TYPE_NORMAL
- en: GBTs train one tree at a time, but RF can train multiple trees in parallel.
    So the training time is lower with RF. However, in some special cases, training
    and using a smaller number of trees with GBTs is faster and more convenient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RFs are less prone to overfitting. In other words, RFs reduces variance with
    more trees, but GBTs reduce bias with more trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RFs can be easier to tune since performance improves monotonically with the
    number of trees, but GBTs perform badly with an increased number of trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're familiar with the customer churn prediction problem from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, and we also know the data well. We also know
    the working principle of RF. So, we can directly jump into coding using the Spark-based
    implementation of RFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get started by instantiating a `RandomForestClassifier` estimator by invoking
    the `RandomForestClassifier()` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline, that is, each of them acts as a stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define `paramGrid` to perform a grid search over the hyperparameter
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CrossValidator` to perform 10-fold cross validation to select the
    best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s call now the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Now it's time to evaluate the predictive power of the DT model on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we need to transform the test set with the model pipeline,
    which will map the features according to the same mechanism we described in the
    feature engineering step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/779f9ae9-174a-49f5-a1af-6bc666d4286d.png)'
  prefs: []
  type: TYPE_IMG
- en: However, based on the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in the second step, the evaluation is done using `BinaryClassificationEvaluator`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'So we get about 87% classification accuracy from our binary classification
    model. Now, similar to SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD, which contains
    the raw scores on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding RDD can be used to compute the previously mentioned performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 88% accuracy but only 73% precision, which
    is much better than SVM and LR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we calculate some more metrics, for example, false and true positive and
    negative predictions, which will be useful to evaluate the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s observe how high the model confidence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s take a look at the true positive, false positive, true negative,
    and false negative rates. Additionally, we see the MCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Just like DT and GBT, RF not only shows robust performance but also a slightly
    improved performance. And like DT and GBT, RF can be debugged to get the DT that
    was constructed during the classification. For the tree to be printed and the
    most important features selected, try the last few lines of code in the DT, and
    you're done.
  prefs: []
  type: TYPE_NORMAL
- en: Can you guess how many different models were trained? Well, we have 10-folds
    on cross-validation and 5-dimensional hyperparameter space cardinalities between
    2 and 7\. Now let's do some simple math: *10 * 7 * 5 * 2 * 3 * 6 = 12,600* models!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to use RF in a classification setting, let's see another
    example of regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since RF is fast and scalable enough for a large-scale dataset, Spark-based
    implementations of RF help you achieve massive scalability. Fortunately, we already
    know the working principles of RF.
  prefs: []
  type: TYPE_NORMAL
- en: If the proximities are calculated in RF, the storage requirements also grow
    exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can jump directly into coding using the Spark-based implementation of RF
    for regression. We get started by instantiating a `RandomForestClassifier` estimator
    by invoking the `RandomForestClassifier()` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a grid space by specifying some hyperparameters, such as
    the max number of bins, max depth of the trees, number of trees, and impurity
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as part of the model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust the number of folds based on your
    settings and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train the random forest regression model with cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the fitted model, we can make predictions. Let''s start evaluating
    the model on the train and validation sets and calculate RMSE, MSE, MAE, and R
    squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the best-fitted and cross-validated model, we can expect a good
    prediction accuracy. Now let''s observe the result on the train and validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    sets, and we can see the improvements compared to the LR, DT, and GBT regression
    models. Let''s hunt for the model that helps us to achieve better accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can see how the decisions were made by seeing DTs in the forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following output, the `toDebugString()` method prints the tree''s decision
    nodes and final prediction outcomes at the end leaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'With RF, it is possible to measure the feature importance so that at a later
    stage, we can decide which features to use and which ones to drop from the DataFrame.
    Let''s find the feature importance out of the best model we just created before
    we arrange all the feature in an ascending order as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the feature importance generated by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The last result is important for understanding the feature importance. As seen,
    some features have higher weights than others. Even some of these have zero weights.
    Higher the weights the higher the importance of a feature. For example, the last
    two features are the most important, and the first two are less important. We
    can drop some unimportant columns and train the RF model to observe whether there
    is any reduction in the R-squared and MAE values on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have mostly covered classic and tree-based algorithms for both regression
    and classification. We saw that the ensemble technique showed the best performance
    compared to classic algorithms. However, there are other algorithms, such as one-vs-rest
    algorithm, which work for solving classification problems using other classifiers,
    such as logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, neural-network-based approaches, such as **multilayer perceptron**
    (**MLP**), **convolutional neural network** (**CNN**), and **recurrent neural
    network** (**RNN**), can also be used to solve supervised learning problems. However,
    as expected, these algorithms require a large number of training samples and a
    large computing infrastructure. The datasets we used so far throughout the examples
    had a few samples. Moreover, those were not so high dimensional. This doesn't
    mean that we cannot use them to solve these two problems; we can, but this results
    in huge overfitting due to a lack of training samples.
  prefs: []
  type: TYPE_NORMAL
- en: How do we fix this issue? Well, we can either search for other datasets or generate
    training data randomly. We'll discuss and show how we can train neural-network-based
    deep learning models to solve other problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had a brief introduction to powerful tree-based algorithms,
    such as DTs, GBT, and RF, for solving both classification and regression tasks.
    We saw how to develop these classifiers and regressors using tree-based and ensemble
    techniques. Through two real-world classification and regression problems, we
    saw how tree ensemble techniques outperform DT-based classifiers or regressors.
  prefs: []
  type: TYPE_NORMAL
- en: We covered supervised learning for both classification and regression on structured
    and labeled data. However, with the rise of cloud computing, IoT, and social media,
    unstructured data is growing unprecedentedly, giving more than 80% data, most
    of which is unlabeled.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning techniques, such as clustering analysis and dimensionality
    reduction, are key applications in data-driven research and industry settings
    to find hidden structures from unstructured datasets automatically. There are
    many clustering algorithms, such as k-means and bisecting k-means. However, these
    algorithms cannot perform well with high-dimensional input datasets and often
    suffer from the *curse of dimensionality*. Reducing the dimensionality using algorithms
    such as **principal component analysis** (**PCA**) and feeding the latent data
    is useful for clustering billions of data points.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use one kind of genomic data to cluster a population
    according to their predominant ancestry, also called geographic ethnicity. We
    will also learn how to evaluate the clustering analysis result and about the dimensionality
    reduction technique to avoid the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
