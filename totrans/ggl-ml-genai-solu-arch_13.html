<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer158" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-226"><a id="_idTextAnchor288" class="calibre6 pcalibre pcalibre1"/>11</h1>
<h1 id="_idParaDest-227" class="calibre5"><a id="_idTextAnchor289" class="calibre6 pcalibre pcalibre1"/>Machine Learning Engineering and MLOps with Google Cloud</h1>
<p class="calibre3">It’s generally estimated that almost 90% of data science projects never make it to production. Data scientists spend a lot of time training and experimenting with models in the lab, but often don’t succeed in bringing those workloads out into the real world. A major reason for this is because, as we have discussed in the previous chapters of this book, there are difficult challenges at every step in the model development lifecycle. Following on from our previous chapter, we will now dive into more detail on deployment concepts and challenges, and describe the importance of <strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>) in addressing these challenges for large-scale production <span>AI/ML workloads.</span></p>
<p class="calibre3">Specifically, this chapter will cover the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">An introduction <span>to MLOps</span></li>
<li class="calibre8">Why MLOps is needed for deploying large-scale <span>ML workloads</span></li>
<li class="calibre8"><span>MLOps tools</span></li>
<li class="calibre8">Implementing MLOps on Google Cloud using Vertex <span>AI Pipelines</span></li>
</ul>
<p class="calibre3">While we’ve already touched on some of these concepts so far in the book, we’ll kick off this chapter with a more formal introduction <span>to MLOps.</span></p>
<h1 id="_idParaDest-228" class="calibre5"><a id="_idTextAnchor290" class="calibre6 pcalibre pcalibre1"/>An introduction to MLOps</h1>
<p class="calibre3">MLOps is an extension of the<a id="_idIndexMarker1199" class="calibre6 pcalibre pcalibre1"/> DevOps concept from the software development industry but with a specific focus on managing and automating ML model and project lifecycles. It goes beyond the tactical steps required to create machine learning models and addresses requirements that come to light when companies need to manage the entire lifecycle of data science use cases at scale. This is a good time to reflect on the ML model lifecycle stages we outlined in previous chapters in this book, as depicted in <span><em class="italic">Figure 11</em></span><span><em class="italic">.1</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer150">
<img alt="Figure 11.1: Data science lifecycle stages" src="image/B18143_11_1.jpg" class="calibre147"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.1: Data science lifecycle stages</p>
<p class="calibre3">At a high level, MLOps aims to automate all of the various steps in the ML model lifecycle, such as data collection, data<a id="_idIndexMarker1200" class="calibre6 pcalibre pcalibre1"/> cleaning and preprocessing, model training, model evaluation, model deployment, and model monitoring. As such, it can be seen as a type of engineering culture that aims to unify ML system development and ML system day-to-day operations. This includes a practice for collaboration and communication among all relevant stakeholders in an ML project, such as a company’s data scientists, ML engineers, data engineers, business analysts, and operations staff, to help manage the overall machine learning lifecycle on an <span>ongoing basis.</span></p>
<p class="calibre3">In this context, managing the ML lifecycle includes defining, implementing, testing, deploying, monitoring, and managing ML models to ensure that they work reliably and efficiently in a <span>production environment.</span></p>
<p class="calibre3">In another similarity to DevOps, MLOps practices also encompass the concepts of continuous integration, continuous delivery, and continuous deployment (CI/CD), but with a specific focus on how ML models are developed. This ensures that new changes to the models are correctly integrated, thoroughly tested, and can be deployed to production in a systematic, reliable, and repeatable way. The goal in this regard is to ensure that any part of the ML lifecycle (data preprocessing, training, etc.) can be repeated at a later point with the same results. Just as with CI/CD in the context of software DevOps, this includes automating steps such as validation checks and integration tests, but in the case of ML model development, it also adds extra components such as data quality checks and model <span>quality evaluations.</span></p>
<p class="calibre3">As we discussed in previous chapters, deploying a model to production is a great achievement, but the work doesn’t stop there. Once ML models are deployed to production, they need to be continuously monitored to ensure their performance does not degrade over time due to changes in the underlying data or other factors. If any issues are detected, the models need to be updated or replaced with newer models. MLOps also includes tools to automate that part of the process, such as automatically retraining (for example, with fresh data), validating, and deploying a new <span>model version.</span></p>
<p class="calibre3">Another important component of this is keeping track of various versions of the artifacts (e.g., data, models, and hyperparameter values) produced at various stages in our data science project. Not only does this enable easy rollback to previous versions if required, but it also provides an audit trail for compliance purposes, and to help with model explainability and transparency, as well as appropriate governance mechanisms, helping to ensure that ML models are used responsibly <span>and ethically.</span></p>
<p class="calibre3">Now that we understand<a id="_idIndexMarker1201" class="calibre6 pcalibre pcalibre1"/> what MLOps is, let’s dive into more detail on why it’s especially important when developing, deploying, and managing models at a <span>large scale.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Later in this book, we will discuss generative AI/ML. There are particular types of ML models that are used in generative AI/ML that are called <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>). These models also have<a id="_idIndexMarker1202" class="calibre6 pcalibre pcalibre1"/> additional kinds of resources and artifacts associated with them, and a new term, LLMOps has emerged in the industry to refer to the operationalization of those workloads. For now, we will focus on traditional MLOps. While many of these concepts also apply to LLMOps workloads, we will discuss the additional considerations for LLMOps in <span>later chapters.</span></p>
<h1 id="_idParaDest-229" class="calibre5"><a id="_idTextAnchor291" class="calibre6 pcalibre pcalibre1"/>Why MLOps is needed for deploying large-scale ML workloads</h1>
<p class="calibre3">The most important aspect <a id="_idIndexMarker1203" class="calibre6 pcalibre pcalibre1"/>of MLOps is that it helps organizations develop ML models in a faster, more efficient, and reliable manner, and it allows data science teams to experiment and innovate while also meeting <span>operational requirements.</span></p>
<p class="calibre3">We know by now that ML has become an essential component of many industries and sectors, providing invaluable insights and decision-making capabilities, but that deploying ML models, especially at scale, presents many challenges. Some of these are challenges that can only be solved by MLOps, and we dive into more detail on such challenges in this section, as well as providing examples of how MLOps helps to <span>address them.</span></p>
<p class="calibre3">Before we dive in, I’m going to point out that the kinds of challenges we will discuss in this section actually apply to any industry that creates products at a large scale, whether those products are cars, safety pins, toys, or machine <span>learning models.</span></p>
<p class="calibre3">I’m going to take an analogy from the automobile industry in order to highlight the kinds of concepts that are required for <a id="_idIndexMarker1204" class="calibre6 pcalibre pcalibre1"/>large-scale production of any type of product. Consider that before cars were invented, a major form of transport would have been horse-drawn carts, and such carts would have been made in a kind of workshop like the one shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.2</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer151">
<img alt="Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)" src="image/B18143_11_2.jpg" class="calibre148"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)</p>
<p class="calibre3">Notice the characteristics of the production environment shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.2</em></span><span>:</span></p>
<ul class="calibre16">
<li class="calibre8">It’s a small environment, which could only accommodate a few people <span>working together</span></li>
<li class="calibre8">Whoever works here has lots of random tools lying around, and there doesn’t seem to be much <span>standardization implemented</span></li>
</ul>
<p class="calibre3">With those kinds of characteristics, any kind of large-scale collaboration would not be possible, and at most, only three or four people could work together on creating a product. In such circumstances, a company could not possibly create a large number of products every month (for example). Bear in mind that this is how most companies begin to implement data science projects, in which data scientists perform various experiments and develop models on their own computers, using any random tools based on personal <a id="_idIndexMarker1205" class="calibre6 pcalibre pcalibre1"/>preferences, and trying to share learnings and artifacts with colleagues in a non-standardized and <span>unsystematic way.</span></p>
<p class="calibre3">To overcome these kinds of challenges, the automotive industry invented the concept of the assembly line, and some of the first assembly lines would have looked similar to the one shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.3</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer152">
<img alt="Figure 11.3: Assembly line (source:  https://pxhere.com/en/photo/798929)" src="image/B18143_11_3.jpg" class="calibre149"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.3: Assembly line (source:  https://pxhere.com/en/photo/798929)</p>
<p class="calibre3">Notice the characteristics of the production environment shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<ul class="calibre16">
<li class="calibre8">The environment can accommodate many people <span>working together</span></li>
<li class="calibre8">The tools and manufacturing processes have <span>been standardized</span></li>
</ul>
<p class="calibre3">This kind of environment enables large-scale collaboration, and it’s possible to produce many more products in any <span>given timeframe.</span></p>
<p class="calibre3">As automotive companies continued to evolve, they continued to apply more standardization and<a id="_idIndexMarker1206" class="calibre6 pcalibre pcalibre1"/> automation to each step in the process, leading to more efficient processes and reproducibility, until today’s assembly lines look like the one shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.4</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer153">
<img alt="Figure 11.4: Modern assembly line (source:  https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)" src="image/B18143_11_4.jpg" class="calibre150"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.4: Modern assembly line (source:  https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)</p>
<p class="calibre3">As we can see in <span><em class="italic">Figure 11</em></span><em class="italic">.4</em>, the production process has become highly automated and more efficient to operate at scale. With this analogy in mind, let’s look at how this idea maps to the concepts and steps in the ML model <span>development lifecycle.</span></p>
<h2 id="_idParaDest-230" class="calibre9"><a id="_idTextAnchor292" class="calibre6 pcalibre pcalibre1"/>Model management and versioning</h2>
<p class="calibre3">As an organization scales up<a id="_idIndexMarker1207" class="calibre6 pcalibre pcalibre1"/> its ML efforts, the number of models in operation can increase exponentially. Managing these models, keeping track of their versions, and knowing when to update or retire them can become quite difficult. Without a proper versioning and management system, ML models and their corresponding datasets can become disorganized, which leads to confusion and errors in the model development process. In fact, when we remember that many large companies have hundreds or even thousands of models in production, and are constantly developing new versions of those models to improve their performance, managing all of those models without specialized tools is basically impossible. MLOps tools<a id="_idIndexMarker1208" class="calibre6 pcalibre pcalibre1"/> provide mechanisms to facilitate easier model management and versioning, allowing teams to track and control their models in an organized and <span>efficient way.</span></p>
<h2 id="_idParaDest-231" class="calibre9"><a id="_idTextAnchor293" class="calibre6 pcalibre pcalibre1"/>Productivity and automation</h2>
<p class="calibre3">Training, testing, deploying, and <a id="_idIndexMarker1209" class="calibre6 pcalibre pcalibre1"/>retraining models often involves many repetitive tasks, and as the scale of ML workloads increases, the time and effort required to manually manage these processes can become prohibitive. Considering that MLOps introduces automation at various stages of the ML lifecycle, such as data preprocessing, model training, testing, deployment, and monitoring, this frees up the data science teams to focus on more valuable tasks. Another important aspect of automation is that it reduces the likelihood of human error, which can help to avoid business-affecting mistakes, and <span>increase productivity.</span></p>
<h2 id="_idParaDest-232" class="calibre9"><a id="_idTextAnchor294" class="calibre6 pcalibre pcalibre1"/>Reproducibility</h2>
<p class="calibre3">Reproducibility is arguably one<a id="_idIndexMarker1210" class="calibre6 pcalibre pcalibre1"/> of the most important factors in any industry that creates products at a large scale. Imagine if every time you tried to purchase the same product from a company, you received something slightly different. Most likely, you would lose faith in that company and would stop purchasing their products. Without well-established reproducibility in production processes, a company simply will not scale. In the context of machine learning, when we have a large number of models and datasets, and different types of development environments, ensuring the reproducibility of experiments can be a significant challenge. It can be difficult to recreate the exact conditions of a previous experiment due to the lack of version control on data, models, and code. This could lead to inconsistent results and make it difficult to build upon previous work. MLOps frameworks provide tools and practices for maintaining a record of all experiments, including the data used, the parameters set, the model architecture, and the outcomes. This allows experiments to be easily repeated, compared, <span>and audited.</span></p>
<h2 id="_idParaDest-233" class="calibre9"><a id="_idTextAnchor295" class="calibre6 pcalibre pcalibre1"/>Continuous Integration/Continuous Deployment (CI/CD)</h2>
<p class="calibre3">In addition to being able to reliably reproduce a product, developing products at a large scale also requires the ability to incrementally improve your products over time. CI/CD pipelines automate the testing and deployment of models, ensuring that new changes are integrated and<a id="_idIndexMarker1211" class="calibre6 pcalibre pcalibre1"/> deployed seamlessly, efficiently, and with minimal errors. This enables data scientists to experiment and try out different kinds of updates quickly, easily, and in a controlled manner that conforms to your company’s required standards. We should note that CI/CD is such an integral component of DevOps that the terms are often used <span>almost interchangeably.</span></p>
<p class="calibre3">What’s even more interesting is that we can also manage an MLOps pipeline within a standard DevOps CI/CD pipeline. For example, when using Vertex AI Pipelines, we can define our pipeline using code, and that code can be stored in a code repository such as Google Cloud Source Repositories and then managed via a DevOps CI/CD pipeline in Google Cloud Build. This enables us to apply all the benefits of DevOps, such as code versioning and automated testing, to the code that defines our pipelines, so that we can control how updates are made to our <span>pipeline definitions.</span></p>
<h2 id="_idParaDest-234" class="calibre9"><a id="_idTextAnchor296" class="calibre6 pcalibre pcalibre1"/>Model validation and testing</h2>
<p class="calibre3">Testing machine learning models involves unique challenges due to their probabilistic nature, and traditional DevOps <a id="_idIndexMarker1212" class="calibre6 pcalibre pcalibre1"/>software testing techniques may not be sufficient. MLOps introduces practices and methodologies specifically for automating the validation and testing of ML models, ensuring they perform as expected before being deployed to production, and for their entire lifetime in production <span>after deployment.</span></p>
<h2 id="_idParaDest-235" class="calibre9"><a id="_idTextAnchor297" class="calibre6 pcalibre pcalibre1"/>Monitoring and maintenance</h2>
<p class="calibre3">As we’ve discussed, once<a id="_idIndexMarker1213" class="calibre6 pcalibre pcalibre1"/> ML models are deployed, their performance needs to be continuously monitored to ensure they consistently provide accurate predictions. We also discussed how changes in real-world data can lead to a decrease in model performance over time, and as a result, we need tools and processes for continuous monitoring and alerting, enabling teams to react quickly to any degradation in performance and retrain models as necessary. This is an important component of any <span>MLOps framework.</span></p>
<h2 id="_idParaDest-236" class="calibre9"><a id="_idTextAnchor298" class="calibre6 pcalibre pcalibre1"/>Collaboration and communication</h2>
<p class="calibre3">As ML projects scale, they typically require collaboration across various teams and roles, including data scientists, ML<a id="_idIndexMarker1214" class="calibre6 pcalibre pcalibre1"/> engineers, data engineers, business analysts, and IT operations. In many organizations, there is a gap between the data scientists who develop ML models and the IT operations teams who deploy and maintain these models in production. Without a common platform and standard practices, different team members might use inconsistent methods, tools, and data, leading to inefficient workflows and potential mistakes. MLOps fosters effective collaboration and communication among these stakeholders, enabling them to work together more efficiently and avoid misunderstandings <span>or misalignments.</span></p>
<h2 id="_idParaDest-237" class="calibre9"><a id="_idTextAnchor299" class="calibre6 pcalibre pcalibre1"/>Regulatory compliance and governance</h2>
<p class="calibre3">In industries such as healthcare and finance, ML models must comply with specific regulatory requirements. MLOps <a id="_idIndexMarker1215" class="calibre6 pcalibre pcalibre1"/>provides mechanisms for ensuring transparency, interpretability, and auditability of models, therefore helping to maintain regulatory compliance. Also, without MLOps, managing multiple models across different teams and business units can become a challenge. MLOps allows for centralized model governance, making it easier to keep track of various models’ performance, status, <span>and owners.</span></p>
<p class="calibre3">Now that we have dived into why MLOps is needed, especially in the context of managing ML workloads at scale, let’s take a look at some of the popular tools that have been developed in the industry for implementing <span>those concepts.</span></p>
<h1 id="_idParaDest-238" class="calibre5"><a id="_idTextAnchor300" class="calibre6 pcalibre pcalibre1"/>MLOps tools</h1>
<p class="calibre3">So far, we’ve talked about the goals and benefits of MLOps, but how can we actually achieve these goals and<a id="_idIndexMarker1216" class="calibre6 pcalibre pcalibre1"/> benefits in the real world? Many tools have been developed to facilitate various aspects of MLOps, from data versioning to model deployment and monitoring. In this section, we discuss the tools that make MLOps <span>a reality.</span></p>
<h2 id="_idParaDest-239" class="calibre9"><a id="_idTextAnchor301" class="calibre6 pcalibre pcalibre1"/>Pipeline orchestration tools</h2>
<p class="calibre3">When we want to standardize the steps in a process and run them automatically, we typically configure them as a set <a id="_idIndexMarker1217" class="calibre6 pcalibre pcalibre1"/>of sequential actions. This sequential set of actions is often referred to as a pipeline. However, simply configuring the <a id="_idIndexMarker1218" class="calibre6 pcalibre pcalibre1"/>order of the steps is not enough; we also need some kind of system to “orchestrate” (i.e., execute and manage) the pipeline, and we can use some popular workflow orchestration tools for this purpose, such as Kubeflow, Airflow, and <strong class="bold">TensorFlow Extended</strong> (<strong class="bold">TFX</strong>). We already<a id="_idIndexMarker1219" class="calibre6 pcalibre pcalibre1"/> covered Airflow in <a href="B18143_06.xhtml#_idTextAnchor187" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 6</em></span></a>, but let’s take a look at Kubeflow and TFX in <span>more detail.</span></p>
<h3 class="calibre11">Kubeflow</h3>
<p class="calibre3">This is an open source project<a id="_idIndexMarker1220" class="calibre6 pcalibre pcalibre1"/> created by Google, and now maintained by an open source community of contributors, aimed at making it easier to run machine learning workflows on Kubernetes. Fundamental principles of Kubeflow include portability, scalability, and extensibility. Portability refers to the concept of “write once, run anywhere,” which builds on the core tenet of containerization, whereby you can run your workloads across different types of computing environments in a <a id="_idIndexMarker1221" class="calibre6 pcalibre pcalibre1"/>consistent manner. Scalability refers to the ability to use Kubernetes to easily scale your model training and prediction workloads as needed, and extensibility means that you can implement customized use cases by adding popular tools and libraries that easily integrate with Kubeflow. In this case, rather than simply being a tool, Kubeflow is a framework and ecosystem that we can use to orchestrate and manage our machine learning workflows. The following are some core components of the <span>Kubeflow </span><span><a id="_idIndexMarker1222" class="calibre6 pcalibre pcalibre1"/></span><span>ecosystem:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Kubeflow Pipelines</strong> (<strong class="bold">KFP</strong>): For defining, deploying, and managing <span>ML workflows</span></li>
<li class="calibre8"><strong class="bold">Katib</strong>: For <span>hyperparameter tuning</span></li>
<li class="calibre8"><strong class="bold">KFServing</strong>: For serving models in a <span>scalable way</span></li>
<li class="calibre8"><strong class="bold">TensorFlow and PyTorch Operators</strong>: For running TensorFlow and <span>PyTorch jobs</span></li>
<li class="calibre8"><strong class="bold">Training Operators</strong>: For <span>distributed</span><span><a id="_idIndexMarker1223" class="calibre6 pcalibre pcalibre1"/></span><span> training</span></li>
</ul>
<p class="calibre3">We will particularly dive deep into Kubeflow Pipelines in the practical exercises associated with this chapter, in which we will build a pipeline and execute it in Google Cloud Vertex AI. In addition <a id="_idIndexMarker1224" class="calibre6 pcalibre pcalibre1"/>to KFP, Vertex AI also supports using <a id="_idIndexMarker1225" class="calibre6 pcalibre pcalibre1"/>TFX to build and manage ML pipelines. Let’s take a look at <span>that next.</span></p>
<h3 class="calibre11">TensorFlow Extended (TFX)</h3>
<p class="calibre3">TFX is an end-to-end platform, also <a id="_idIndexMarker1226" class="calibre6 pcalibre pcalibre1"/>created by Google, that manages and deploys production ML pipelines. As the name suggests, it was designed as an extension to TensorFlow, with the aim of making it easier to bring trained<a id="_idIndexMarker1227" class="calibre6 pcalibre pcalibre1"/> models to production. Just like KFP, TFX offers components for implementing each stage of the ML lifecycle, covering everything from data ingestion and validation to model training, tuning, serving, and monitoring. Also similar to KFP, TFX’s core principles include scalability, and of course, extensibility. Other core principles of TFX include reproducibility and modularity. Reproducibility, as we discussed earlier in this chapter, is a critical requirement for producing pretty much anything at a large scale. Modularity, in this context, refers to the fact that the various components of TFX can be used individually or together, enabling customization. Speaking of which, the various components and responsibilities of TFX include <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">ExampleGen, which imports and ingests data into the <span>TFX pipeline</span></li>
<li class="calibre8">StatisticsGen, which computes statistics for the ingested data, essential for understanding the data and <span>feature engineering</span></li>
<li class="calibre8">SchemaGen, which examines dataset statistics and creates a schema for <span>the dataset</span></li>
<li class="calibre8">ExampleValidator, which identifies and analyzes anomalies and missing values in <span>the dataset</span></li>
<li class="calibre8">Transform, which can be used for feature engineering on <span>the dataset</span></li>
<li class="calibre8">Trainer, which defines and trains a model <span>using TensorFlow</span></li>
<li class="calibre8">Tuner, which can be used to optimize hyperparameters using <span>Keras Tuner</span></li>
<li class="calibre8">InfraValidator, which ensures that the model can be loaded and served in a <span>production environment</span></li>
<li class="calibre8">Evaluator, which uses the TensorFlow Model Analysis library to evaluate the metrics of the <span>trained model</span></li>
<li class="calibre8">BulkInferrer, which performs batch processing on <span>a model.</span></li>
<li class="calibre8">Blessing and deployment: If the model is validated, it can be “blessed” and then deployed using a serving<a id="_idIndexMarker1228" class="calibre6 pcalibre pcalibre1"/> infrastructure such as <span>TensorFlow Serving.</span></li>
</ul>
<p class="calibre3">Considering that both TFX and <a id="_idIndexMarker1229" class="calibre6 pcalibre pcalibre1"/>KFP can be used to build and run pipelines in Google Cloud Vertex AI, it may be challenging to decide which one to use. On that matter, the official Google Cloud documentation recommends <span>the following:</span></p>
<p class="calibre3"><em class="italic">“If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX… For other use cases, we recommend that you build your pipeline using the Kubeflow </em><span><em class="italic">Pipelines SDK.”</em></span></p>
<p class="calibre3">We’re not going to focus on TFX in a lot of detail in this chapter, but if you would like to learn how to build a TFX pipeline, then there’s a useful tutorial in the TensorFlow documentation at the following <span>link: </span><a href="https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple" class="calibre6 pcalibre pcalibre1"><span>https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple</span></a><span>.</span></p>
<p class="calibre3">There are also many other open source tools for implementing MLOps pipelines, such as MLFlow, which provides interfaces for tracking experiments, packaging code into reproducible runs, and sharing and <span>deploying models.</span></p>
<p class="calibre3">While KFP and TFX can be used to orchestrate your entire MLOps pipeline, there are also tools that focus on more specific components of the ML lifecycle, and that can integrate with KFP and TFX in order to customize your overall pipeline. Let’s take a look at some of those <span>tools next.</span></p>
<h2 id="_idParaDest-240" class="calibre9"><a id="_idTextAnchor302" class="calibre6 pcalibre pcalibre1"/>Experiment and lineage tracking tools</h2>
<p class="calibre3">As we discussed earlier in this <a id="_idIndexMarker1230" class="calibre6 pcalibre pcalibre1"/>chapter, and in earlier chapters, when running ML workloads at a large scale, it’s important to keep track of every step in the development of every model, so that<a id="_idIndexMarker1231" class="calibre6 pcalibre pcalibre1"/> you can easily understand how any given model was created. This is required for characteristics such as reproducibility, explainability, and compliance. Imagine if you have thousands of models in production, and compliance regulators ask you to explain how a particular model was created. Without a well-established system for tracking every aspect of every model’s creation, such as which versions of which<a id="_idIndexMarker1232" class="calibre6 pcalibre pcalibre1"/> datasets were used to train the model, what kinds of algorithms and hyperparameters were used, what the initial evaluation metrics were, and who deployed the model to production, this would be an impossible task. This is where experiment and lineage tracking tools come into the picture. Let’s consider what kinds of tools we have at our disposal in <span>this regard.</span></p>
<h3 class="calibre11">Vertex AI Experiments</h3>
<p class="calibre3">Vertex AI Experiments is a managed <a id="_idIndexMarker1233" class="calibre6 pcalibre pcalibre1"/>service that helps you track, compare, and analyze your machine learning experiments. It <a id="_idIndexMarker1234" class="calibre6 pcalibre pcalibre1"/>provides a central place to manage your experiments, and it makes it easy to compare different models and hyperparameters. Once an experiment is created (using the Vertex AI SDK or the Vertex AI user interface), you can start tracking your training runs, and Vertex AI Experiments will automatically collect metrics from your training runs, such as accuracy, loss, and <span>training time.</span></p>
<p class="calibre3">Once your training runs are complete, you can view graphs and tables of the metrics, and you can use statistical tests to determine which model is the best. You can also use the Experiments dashboard to visualize your results, and you can use the Experiments API to export your results to a spreadsheet or <span>a notebook.</span></p>
<h3 class="calibre11">TensorBoard</h3>
<p class="calibre3">TensorBoard is a web-based tool that comes with TensorFlow and is designed to help visualize and understand <a id="_idIndexMarker1235" class="calibre6 pcalibre pcalibre1"/>machine learning models, as well as to debug and optimize them. It provides an interactive interface for various<a id="_idIndexMarker1236" class="calibre6 pcalibre pcalibre1"/> aspects of machine learning models and training processes and can make it easy to create metric graphs such as ROC curves, which we introduced in <span>previous chapters.</span></p>
<h2 id="_idParaDest-241" class="calibre9"><a id="_idTextAnchor303" class="calibre6 pcalibre pcalibre1"/>Model deployment and monitoring tools</h2>
<p class="calibre3">In this context, we have <a id="_idIndexMarker1237" class="calibre6 pcalibre pcalibre1"/>tools such as Vertex AI Prediction and Model Monitoring, which we reviewed in detail in <a href="B18143_10.xhtml#_idTextAnchor259" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 10</em></span></a>. There are also open source <a id="_idIndexMarker1238" class="calibre6 pcalibre pcalibre1"/>tools such as <strong class="bold">TensorFlow Serving</strong> (<strong class="bold">TFServing</strong>) for TensorFlow models, <span>and TorchServe.</span></p>
<h2 id="_idParaDest-242" class="calibre9"><a id="_idTextAnchor304" class="calibre6 pcalibre pcalibre1"/>Model interpretability and explainability tools</h2>
<p class="calibre3">As we’ve discussed <a id="_idIndexMarker1239" class="calibre6 pcalibre pcalibre1"/>previously, model interpretability and explainability are extremely important concepts in machine learning. They relate to reproducibility, compliance, and performance. For example, if you don’t have a good understanding of how your models work, then it will be harder to continuously improve them in a systematic way. These concepts also relate to fairness. That is, in order to ensure that a model is making fair and ethical predictions, you need to understand how it works in great detail. Fortunately, there are tools that can help us in this regard, which we <span>discuss here.</span></p>
<p class="calibre3">Google Cloud Vertex AI provides tools such as Explainable AI and Fairness Indicators, which can help us to understand how our machine learning models work, and how they would behave under different conditions. There are <a id="_idIndexMarker1240" class="calibre6 pcalibre pcalibre1"/>also open source tools such as <strong class="bold">SHAP</strong> (short for <strong class="bold">SHapley Additive exPlanations</strong>), which uses a game theory approach to explain the output of any machine learning model, and <strong class="bold">LIME</strong> (short for <strong class="bold">Local Interpretable Model-agnostic Explanations</strong>), which is a <a id="_idIndexMarker1241" class="calibre6 pcalibre pcalibre1"/>Python library that allows us to explain the predictions of any machine <span>learning classifier.</span></p>
<p class="calibre3">The next chapter in this book is dedicated to the concepts of bias, fairness, explainability, and lineage, so we will dive into these concepts and tools in a lot more <span>detail there.</span></p>
<p class="calibre3">Now that we’ve covered many of the different kinds of tools that can be used to implement MLOps workloads, let’s dive into how we can specifically do this on Google Cloud <span>Vertex AI.</span></p>
<h1 id="_idParaDest-243" class="calibre5"><a id="_idTextAnchor305" class="calibre6 pcalibre pcalibre1"/>Implementing MLOps on Google Cloud using Vertex AI Pipelines</h1>
<p class="calibre3">In this section, we will cover the <a id="_idIndexMarker1242" class="calibre6 pcalibre pcalibre1"/>steps to build an MLOps pipeline on Google Cloud using Vertex <span>AI Pipelines.</span></p>
<h2 id="_idParaDest-244" class="calibre9"><a id="_idTextAnchor306" class="calibre6 pcalibre pcalibre1"/>Prerequisite: IAM permissions</h2>
<p class="calibre3">In this section, we will use the same<a id="_idIndexMarker1243" class="calibre6 pcalibre pcalibre1"/> Vertex AI Workbench notebook instance that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter </em></span><span><em class="italic">5</em></span></a>. That user-managed notebook uses the default Compute Engine service account, which is granted the IAM basic Editor role by default. When we build and execute our pipeline in our notebook, we decide to let our pipeline inherit the same permissions used by our notebook. This is a decision we make by not specifying a different role for our pipeline <a id="_idIndexMarker1244" class="calibre6 pcalibre pcalibre1"/>executions. We could specify a different role if we wanted to, but for simplicity purposes, we’ll take the approach of having our pipeline use the default Compute Engine service account. By default, the Editor role (used by the default Compute Engine service account) will allow us to perform all of the required activities in Vertex AI in this chapter, but our pipeline will also need to run some steps on Dataproc. For this reason, we will add the Dataproc Worker and Dataproc Editor roles to the default Compute Engine service account. To do that, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to <strong class="bold">Google Cloud services</strong> → <strong class="bold">IAM &amp; Admin</strong> → <span><strong class="bold">IAM</strong></span><span>.</span></li>
<li class="calibre8">In the list of principals, click on the pencil symbol for the default Compute Engine service account (the service account name will have the format <strong class="source-inline">-[PROJECT_NUMBER]-compute@developer.gserviceaccount.com</strong>; see <span><em class="italic">Figure 11</em></span><em class="italic">.5</em> <span>for reference).</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer154">
<img alt="Figure 11.5: Edit service account" src="image/B18143_11_5.jpg" class="calibre151"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.5: Edit service account</p>
<ol class="calibre7">
<li value="3" class="calibre8">On the screen that appears, select <strong class="bold">Add Another Role</strong>, and in the textbox that then appears, type <strong class="source-inline">Dataproc</strong> and select <strong class="bold">Dataproc Editor</strong> from the list <span>of roles.</span></li>
<li class="calibre8">Repeat step 3 and<a id="_idIndexMarker1245" class="calibre6 pcalibre pcalibre1"/> select <span><strong class="bold">Dataproc Worker</strong></span><span>.</span></li>
<li class="calibre8">The updated <a id="_idIndexMarker1246" class="calibre6 pcalibre pcalibre1"/>roles will look<a id="_idIndexMarker1247" class="calibre6 pcalibre pcalibre1"/> as shown in <span><em class="italic">Figure 11</em></span><span><em class="italic">.6</em></span><span>.</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer155">
<img alt="Figure 11.6: Updated roles" src="image/B18143_11_6.jpg" class="calibre152"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.6: Updated roles</p>
<ol class="calibre7">
<li value="6" class="calibre8"><span>Click </span><span><strong class="bold">Save</strong></span><span>.</span></li>
</ol>
<p class="calibre3">And that’s it – you have successfully added the <span>required roles.</span></p>
<h2 id="_idParaDest-245" class="calibre9"><a id="_idTextAnchor307" class="calibre6 pcalibre pcalibre1"/>Implementation</h2>
<p class="calibre3">As I mentioned in the previous section, we can use the same Vertex AI Workbench notebook instance that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter </em></span><span><em class="italic">5</em></span></a> to build our MLOps pipeline. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-11</strong> directory and open the <strong class="source-inline">mlops.ipynb</strong> notebook. You can choose <strong class="bold">Python (Local)</strong> as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook contains markdown text that describes what the code <span>is doing.</span></p>
<p class="calibre3">In the practical <a id="_idIndexMarker1248" class="calibre6 pcalibre pcalibre1"/>exercises, we will build and run an MLOps pipeline that will execute all of the phases of the ML <a id="_idIndexMarker1249" class="calibre6 pcalibre pcalibre1"/>model development lifecycle that we have covered so far in this book. Our pipeline is depicted in <span><em class="italic">Figure 11</em></span><span><em class="italic">.5</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer156">
<img alt="Figure 11.7: Vertex AI pipeline" src="image/B18143_11_7.jpg" class="calibre153"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.7: Vertex AI pipeline</p>
<p class="calibre3">At a high level, our pipeline will perform the<a id="_idIndexMarker1250" class="calibre6 pcalibre pcalibre1"/> <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8"><strong class="bold">Ingest data</strong>: First, we need to get our data into the Google Cloud environment. We’re using Google Cloud Storage to store our data, and when our pipeline kicks off a data processing and model training job, our data is read in <span>from there.</span></li>
<li class="calibre8"><strong class="bold">Preprocess our data</strong>: Google Cloud offers several tools for data preprocessing, including Dataflow, Dataprep, and Dataproc, which we explored in <a href="B18143_06.xhtml#_idTextAnchor187" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 6</em></span></a>. More recently, Google<a id="_idIndexMarker1251" class="calibre6 pcalibre pcalibre1"/> Cloud also released a service named Serverless Spark, which enables us to run Spark jobs without having to provision and manage the required underlying infrastructure. This is what we use to implement our data preprocessing job in our pipeline in the <span>practical exercises.</span></li>
<li class="calibre8"><strong class="bold">Develop and train our model</strong>: Our pipeline trains a TensorFlow model in Vertex AI, using the processed data created in the <span>previous step.</span></li>
<li class="calibre8">Register our model in the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8"><strong class="bold">Deploy our model</strong>: Our pipeline moves to the next step, which is to deploy our model to production. In this case, our pipeline creates a Vertex AI endpoint and hosts our model on <span>that endpoint.</span></li>
</ol>
<p class="calibre3">Let’s take a look at what our pipeline is doing in more detail, by inspecting the solution architecture that we have just created, as depicted in <span><em class="italic">Figure 11</em></span><span><em class="italic">.6</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer157">
<img alt="Figure 11.8: MLOps pipeline in Vertex AI" src="image/B18143_11_8.jpg" class="calibre154"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.8: MLOps pipeline in Vertex AI</p>
<p class="calibre3">In <span><em class="italic">Figure 11</em></span><em class="italic">.6</em>, the horizontal rectangular section that spans across the figure represents our pipeline that runs in Vertex AI Pipelines. Each of the steps in the process is numbered, and the numbers<a id="_idIndexMarker1252" class="calibre6 pcalibre pcalibre1"/> represent the <span>following actions:</span></p>
<ol class="calibre7">
<li class="calibre8">The data processing step <a id="_idIndexMarker1253" class="calibre6 pcalibre pcalibre1"/>in our pipeline submits a Spark job to Dataproc Serverless in order to execute our PySpark <span>processing script.</span></li>
<li class="calibre8">Dataproc fetches our PySpark script and the raw data from Google Cloud Storage and executes the PySpark script to process the <span>raw data.</span></li>
<li class="calibre8">Dataproc stores the resulting processed data in Google <span>Cloud Storage.</span></li>
<li class="calibre8">The data processing job status <span>is complete.</span></li>
<li class="calibre8">The next step in our pipeline — the model training step — <span>is invoked.</span></li>
<li class="calibre8">Vertex AI Pipelines submits a model training job to the Vertex AI <span>Training service.</span></li>
<li class="calibre8">In order to execute our custom training job, the Vertex AI Training service fetches our custom Docker container from Google Artifact Registry, and it fetches the training data from Google Cloud Storage. This is the same data that was stored in Google Cloud Storage by our data processing job (i.e., it is the processed data that was created by our data <span>processing job).</span></li>
<li class="calibre8">When our model has been trained, the trained model artifacts are saved in Google <span>Cloud Storage.</span></li>
<li class="calibre8">The model training job status <span>is complete.</span></li>
<li class="calibre8">The next step in our pipeline — the model import step — is invoked. This is an intermediate step that prepares the model metadata to be referenced in later components of our pipeline. The relevant metadata in this case consists of the location of the model artifacts in Google Cloud Storage and the specification of the Docker container image <a id="_idIndexMarker1254" class="calibre6 pcalibre pcalibre1"/>in Google Artifact Registry that will be used to serve <span>our model.</span></li>
<li class="calibre8">The next step in our <a id="_idIndexMarker1255" class="calibre6 pcalibre pcalibre1"/>pipeline — the model upload step — is invoked. This step references the metadata from the model <span>import step.</span></li>
<li class="calibre8">The model metadata is used to register the model in Vertex AI Model Registry. This makes it easy to deploy our model for serving traffic in <span>Vertex AI.</span></li>
<li class="calibre8">The model upload job status <span>is complete.</span></li>
<li class="calibre8">The next step in our pipeline — the endpoint creation step — <span>is invoked.</span></li>
<li class="calibre8">An endpoint is created in the Vertex AI Prediction service. This endpoint will be used to host <span>our model.</span></li>
<li class="calibre8">The endpoint creation job status <span>is complete.</span></li>
<li class="calibre8">The next step in our pipeline — the model deployment step — <span>is invoked.</span></li>
<li class="calibre8">Our model is deployed to our endpoint in the Vertex AI Prediction service. This step references the metadata of the endpoint that has just been created by our pipeline, as well as the metadata of our model in the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8">The model deployment job status <span>is complete.</span></li>
</ol>
<p class="calibre3">In addition to all of the above steps that are explicitly performed by our pipeline, the Vertex AI Pipelines service also registers metadata related to our pipeline execution in the Vertex ML <span>Metadata service.</span></p>
<p class="calibre3">Our model is now ready to serve inference requests! Isn’t it impressive that all of these activities and API calls across multiple Google Cloud services are being orchestrated automatically by our pipeline? After our pipeline has been defined, it can run automatically whenever<a id="_idIndexMarker1256" class="calibre6 pcalibre pcalibre1"/> we wish, without any need for human interaction throughout the process, unless we deem it necessary for humans to be involved in any of <span>the steps.</span></p>
<p class="calibre3">If you have completed<a id="_idIndexMarker1257" class="calibre6 pcalibre pcalibre1"/> the activities in the notebook, then you are now officially an AI/ML guru! Seriously, you have just implemented an end-to-end MLOps pipeline. That is an extremely complex and advanced task in the <span>ML industry.</span></p>
<p class="calibre3">With all of that success under your belt, let’s take some time to reflect on what we learned in <span>this chapter.</span></p>
<h1 id="_idParaDest-246" class="calibre5"><a id="_idTextAnchor308" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we started with an introduction to MLOps at a high level, which is essentially a blend of machine learning, DevOps, and data engineering, in which the main goal is to automate the ML lifecycle, resulting in improved workflows and collaborations between data scientists and engineers. We discussed how MLOps allows organizations to streamline their ML operations, increase the speed of deployment, and maintain high-quality models in production, leading to a more efficient, effective, and reliable ML workflow, and thereby maximizing the value that organizations get from their <span>ML initiatives.</span></p>
<p class="calibre3">We touched upon the various pain points that MLOps addresses, including but not limited to challenges related to managing and versioning models, ensuring reproducibility and consistency, monitoring and maintaining models, and fostering collaboration between <span>different teams.</span></p>
<p class="calibre3">We then dived into why MLOps is important for deploying large-scale machine learning workloads. It resolves many challenges that can crop up in machine learning systems, ranging from managing and versioning models to ensuring the reproducibility of experiments. It also facilitates continuous integration, deployment, monitoring, and validation of models and promotes better collaboration <span>among teams.</span></p>
<p class="calibre3">Next, we discussed various MLOps tools such as Kubeflow Pipelines and TensorFlow Extended, among others. Each of these tools offers unique functionalities catering to different stages of the ML lifecycle, including data versioning, experiment tracking, and <span>model deployment.</span></p>
<p class="calibre3">We then performed an implementation of MLOps on Google Cloud using Vertex AI Pipelines, which involved multiple steps, including managing datasets, preprocessing data, training models, and <span>monitoring models.</span></p>
<p class="calibre3">Next, we will explore four important and somewhat interrelated topics in the machine learning industry, which are bias, explainability, fairness, and lineage. Let’s move on to the next chapter to explore these concepts <span>in detail.</span></p>
</div>
</div></body></html>