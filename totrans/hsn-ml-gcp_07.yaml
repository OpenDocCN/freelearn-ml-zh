- en: Google Machine Learning APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the previous chapter, machine learning is used in a wide variety
    of applications. However, a few applications are easy to build, while a few are
    very hard to build, especially for a user who is less familiar with machine learning.
    Some of the applications that we are going to discuss in this chapter fall in
    the hard to build category, as the process of building a machine learning model
    for these applications is data intensive, resource intensive, and requires a lot
    of knowledge in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go over five machine learning APIs provided by Google
    (as of March 2018). These APIs are meant to be used out of the box, as RESTful
    APIs. For each service mentioned in the following, we will show what type of application
    can benefit from it, and how to interpret the returned results:'
  prefs: []
  type: TYPE_NORMAL
- en: Vision has a label detection, OCR, face detection and emotions, logo,and landmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech means speech-to-text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP has entities, sentiment, and POS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vision API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Vision API lets us build quite a few applications related to vision:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting labels in an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the text in an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emotion detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logo detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landmark detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive into building applications using the preceding, let's get a quick
    understanding of how they might be built, using face emotion detection as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of detecting emotions involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting a huge set of images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hand-labeling images with the emotion that is likely represented in the image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a **convolutional neural network** (**CNN**) (to be discussed in future
    chapters) to classify the emotion, based on an image as input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the preceding steps are heavily resource intensive (as we would need a
    lot of humans to collect and hand-label images), there are multiple other ways
    to obtain face emotion detection. We are not sure how Google is collecting and
    labeling images, but we will now consider the API that Google has built for us,
    so that, if we want to classify images into the emotions they represent, we can
    make use of that API.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start building applications, we first have to enable the API, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Search for the Google Cloud Vision API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c0b69895-ef96-4500-836c-892128798f1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enable the Google Cloud Vision API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f0fd833-0aba-4b4a-a7d6-da1cfd61f563.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you click on ENABLE, the API will be enabled for the project (that is,
    My First Project), as seen in the preceding screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fetch credentials for the API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8af83174-c263-4cab-843e-e61d707c8774.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Service account key after clicking on Create credentials:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f0c5cd49-93fb-44db-aae9-12e910f22ad4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on New service account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e34a91c8-b9d0-4e6e-a48f-6d17ae69ff82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enter a service account name (in my case, `kish-gcp`) and Select a role as
    the project Owner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9a1e919d-a00a-4897-9b8d-fe2001b1b8f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Create to save the JSON file of keys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Opening an instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to open an instance, click on VM instances, as shown in the screenshot
    that follows, and then click on the Activate google cloud shell icon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8df15b9-f8a2-4a24-af2b-ec2d45251d15.png)![](img/2370fe86-0ccd-48aa-8185-38b28125c7b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating an instance using Cloud Shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we click on the cloud shell icon, we create an instance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance is created by specifying the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Cloud Shell, the preceding code looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7b24116c-0aa3-4f1f-9458-d605dbadc306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you have keyed in the responses for all of the prompts, you need to Change
    port to `8081` to access Datalab, which is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/710044dc-2640-47d0-93b1-8ddbc07c34f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on Change port you will get a window as follows. Enter `8081`
    and click on CHANGE AND PREVIEW to open Datalab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a2160b6a-880a-4a66-8c76-313dd4705a96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will open up Datalab, which has functionalities that enable us to write
    all types of commands: `bash`, `bigquery`, `python`, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that the requirements are set up, let''s fetch/install the requirements
    for the API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accessing the API keys in the previous section, we have downloaded the required
    keys. Now, let''s upload the `.json` file to Datalab by clicking on the Upload
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fba08f3a-b025-40eb-825f-6d338fcae55e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the `.json` file is uploaded, you should be able to access it through
    Datalab from here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e4a25f51-1121-4c68-a153-369213f94930.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Open a notebook; you can open a notebook in Datalab by clicking on the Notebook
    tab, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1c1dbb14-1143-4f17-a662-b9a1d269b821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To install `google-cloud`, once you open the Notebook, change the kernel from
    python2 to python3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2c842bd5-f941-4f47-a91c-1a54681a9ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Install the `google-cloud` package, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `google-cloud` is installed, make sure that the `.json` file uploaded
    earlier is accessible in the current Python environment, by specifying the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In order to upload an image of interest, we will look at transferring a file
    from the local machine into the bucket, and from the bucket to Datalab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search for `bucket` in the Google Cloud:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6337862a-a63e-4cf7-b6c8-6d66838c8e77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, name the bucket and create it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/34631cf2-357f-435a-b990-77457e90d3fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Upload **files** to upload relevant files from the local machine to
    the bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32220fa6-257a-444a-b7d2-1d24631e8a8f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the file is uploaded to the bucket, fetch it from Datalab, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4ebd04af-3b8c-499f-873f-b44a33315867.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, you should notice that `11.jpg` is accessible in Datalab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that the image to analyze is accessible in Datalab, let''s understand the
    ways to leverage the Cloud Vision API to understand images better:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet makes sure that the methods available in Vision are
    accessible in the current session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the service that performs Google Cloud Vision API detection tasks (such
    as face, landmark, logo, label, and text detection) over client images—`ImageAnnotator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the image is uploaded per expectation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/eda014fe-a165-4cbb-b709-d7743b54d0f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Invoke the `face_detection` method to fetch the relevant details of the image,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The responses to image annotations are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b1b03a99-5a4c-43b2-8d2a-60f8a586d63d.png)![](img/59629615-6973-44ba-9585-fbf796eba63e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have run our method to detect faces in the image, let''s look at
    the output - `response`. The output of `response` is a set of attributes, as described
    previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f2c8d0a0-778a-421f-9a5a-e2e31d1f90e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the few more points explained in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bounding polygon**: The bounding polygon is around the face. The coordinates
    of the bounding box are in the original image''s scale, as returned in `ImageParams`.
    The bounding box is computed to frame the face in accordance with human expectations.
    It is based on the landmarker results. Note that one or more *x* and/or *y* coordinates
    may not be generated in the `BoundingPoly` (the polygon will be unbounded) if
    only a partial face appears in the image to be annotated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face detection bounding polygon**: The `fd_bounding_poly` bounding polygon
    is tighter than the `BoundingPoly`, and encloses only the skin part of the face.
    Typically, it is used to eliminate the face from any image analysis that detects
    the amount of skin visible in an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Landmarks**: Detected face landmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are few more terms explained in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`roll_angle`: Roll angle, which indicates the amount of clockwise/anticlockwise
    rotation of the face, relative to the image . The range is [-180,180].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pan_angle`: Yaw angle, which indicates the leftward/rightward angle that the
    face is pointing, relative to the vertical plane perpendicular to the image. The
    range is [-180,180].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tilt_angle`: Pitch angle, which indicates the upwards/downwards angle that
    the face is pointing, relative to the image''s horizontal plane. The range is
    [-180,180].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detection_confidence`: Confidence associated with the detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`landmarking_confidence`: Confidence associated with the landmarking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`joy_likelihood`: Likelihood associated with the joy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorrow_likelihood`: Likelihood associated with the sorrow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`anger_likelihood`: Likelihood associated with the anger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`surprise_likelihood`: Likelihood associated with the surprise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`under_exposed_likelihood`: Likelihood associated with the exposed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blurred_likelihood`: Likelihood associated with the blurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`headwear_likelihood`: Likelihood associated with the headwear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face landmarks would further provide the locations of eyes, noses, lips, ears
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We should be able to make a boundary box around the face identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `face_annotations` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8b59b5a-4ea1-4e12-ba0b-a3c54a431b69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding code, we should be able to understand the coordinates of
    the bounding box. In the code that follows, we calculate the starting point of
    the bounding box, and the corresponding width and height of the bounding box.
    Once the calculation is done, we superimpose the rectangle over the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is the image with a bounding box around the
    face, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b10cd5c-cc20-4c35-9236-765efdb5b670.png)'
  prefs: []
  type: TYPE_IMG
- en: Label detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous code snippet, we used the `face_detection` method to fetch the
    various coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand the label of the image, we will be using the `label_detection`
    method in place of `face_detection`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/202f4ad6-4b36-410b-aabe-88695abbd757.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of label detection is a collection of labels, along with the scores
    associated with each label.
  prefs: []
  type: TYPE_NORMAL
- en: Text detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The text in an image can be identified by using the `text_detection` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `response_text` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad3a9fa3-3b57-4f05-a1d0-9136cc27f118.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the output of the `text_detection` method is the bounding box of the
    various text that is present in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the description of `text_annotations` provides the text detected
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Logo detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vision services also enable us to recognize the logo in an image by using the
    `logo_detection` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, you can see that we are able to detect the logo of `wikipedia`
    by passing the URL of the image''s location, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `logo_detection` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b346970-15c3-4286-8f53-ac12b3f5753c.png)'
  prefs: []
  type: TYPE_IMG
- en: Landmark detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that, in the preceding lines of code, we have specified the URL of the
    image location in the `logo_detection` method, and it resulted in a description
    of the predicted logo, and also the confidence score associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, any landmark located in an image can be detected by using the `landmark_detection`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `landmark_detection` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a58efb19-07d8-4363-9f9c-37bb2458c7b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud Translation API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cloud Translation API provides a simple, programmatic interface for translating
    an arbitrary string into any supported language, using state-of-the-art neural
    machine translation. The Translation API is highly responsive, so websites and
    applications can integrate with the Translation API for fast, dynamic translation
    of source text from the source language to a target language (for example, French
    to English). Language detection is also available for cases in which the source
    language is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For us to be able to use Google cloud translation services, we need to enable,
    which is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to enable the Google Cloud Translation API, search for the API in
    the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b9192c37-46b7-4d04-a53a-9e9018296cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enable the Google Cloud Translation API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c6f06a96-ad9e-4b17-9fd2-9dc7ffa6f099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the Translation API is enabled, the next step is to create credentials
    to access the API. However, note that if you have already created credentials
    for one API, they can be used for any other API. Let''s go ahead and initialize
    our instance using Cloud Shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5152c3b3-6194-4621-89c8-e542b9653cea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the instance starts, we will open Datalab on port `8081`. We provide a
    path to the location of the `api-key` file as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The various methods to `translate` are imported by using the following statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `client` object that creates a connection to the Cloud Translation
    service, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The Google Cloud Translation API has three supported methods, and they are
    `get_languages()`, `detect_language()`, and `translate()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `client.get_languages()` method gives us a list of all of the available
    languages, and also their shorthand notations, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fff7307a-0a48-4a08-ae02-46c9dffce6fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `client.detect_language()` method detects the language that the text is
    written in:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/da018e49-6ab0-40a0-a710-b1c4db37929f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding method, we have given two texts—one in Spanish, and
    the other in English. The preceding output represents the language of the text,
    along with the confidence associated with the detection of the language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `client.translate()` method detects the source language and translates
    the text into English (by default), as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/53851a88-716c-4b72-9094-fe2ff2ab68db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `client.translate()` method also gives us an option to specify the target
    language to which a text needs to be translated, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/76b2504a-a437-471b-ba5d-01d31127dfcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Natural Language API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Google Cloud Natural Language API reveals the structure and meaning of text
    by offering powerful machine learning models in an easy-to-use REST API. You can
    use it to extract information about people, places, events, and much more, that
    are mentioned in text documents, news articles, or blog posts. You can also use
    it to understand the sentiment about your product on social media, or to parse
    intent from customer conversations happening in a call center or a messaging app.
    You can analyze the text uploaded in your request, or integrate it with with your
    document storage on Google Cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cloud Natural Language API can be found by searching for it in your console,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3f55923-2eb1-4136-8ca5-83284d52ef7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Cloud Natural Language API is enabled in the resulting page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e269f60-6e7e-421e-a5c1-d11aeb89a1af.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the Translation API, we do not have to create credentials for this
    API if at least one API is already enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing can be useful in extracting the sentiments associated
    with various text.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis inspects the given text and identifies the prevailing emotional
    opinion within the text, to determine a writer's attitude as positive, negative,
    or neutral. Sentiment analysis is performed through the `analyzeSentiment` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, let''s understand how to identify the sentiment of
    a statement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the class corresponding to the language service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The Google Natural Language API has the following supported methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`analyzeEntities`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`analyzeSentiment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`analyzeEntitySentiment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`annotateText`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`classifyText`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each method uses a `Document` for representing text. Let''s explore the `analyzeSentiment`
    method in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have converted the input text into a `Document` type, and then
    analyzed the sentiment of the document.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the sentiment score reflects the probability of a text being positive;
    the closer the score is to one, the more positive the statement is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, one could pass on an HTML file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f48ffe89-6faa-4933-8c74-485e248c3fa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Files that are stored in a Google Cloud bucket can also be referenced, by changing
    the content to `gcs_content_uri`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f8354ee-51fb-4f8e-a652-096019b87500.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `analyze_entities()`method finds named entities (that is, proper names)
    in the text. This method returns an `AnalyzeEntitiesResponse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding loop is the named entities present in the document''s
    content, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/251c1f51-1758-40d2-b0e9-e1fc626ea55e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also extract the part of speech of each of the words in the given text
    by using the `analyze_syntax` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the document into the corresponding words that constitute the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The parts of speech of a `token` can then be extracted, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9abf115f-6c7d-4ac1-b8ed-4ce4059c4b33.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the majority of the words are classified into the right parts of speech.
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-text API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Google Cloud Speech API enables developers to convert audio to text, by
    applying powerful neural network models in an easy-to-use API. The API recognizes
    over 110 languages and variants. One can transcribe the text of users dictating
    to an application's microphone, enable command-and-control through voice, or transcribe
    audio files, among many use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to enable the speech to text API, search for it in the console, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7751f4e4-0b32-4450-a80c-d80227d2600f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the resulting web page, enable the API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b74baeec-4435-4ffb-9edc-c72f0988558f.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the APIs mentioned in the previous sections, credentials obtained
    for one API can be replicated for the other Google APIs. So, we don't have to
    create credentials separately for the speech to text API.
  prefs: []
  type: TYPE_NORMAL
- en: Once the API is enabled, let's start the Cloud Shell and Datalab, as we did
    in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we transcribe a small audio file into text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and the API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the speech service, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can specify the audio that we want to convert, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that **Free Lossless Audio Codec** (**FLAC**).
  prefs: []
  type: TYPE_NORMAL
- en: An audio file (`.wav`) can be converted to a `.flac` file by using the converter
    located at [https://audio.online-convert.com/convert-to-flac](https://audio.online-convert.com/convert-to-flac).
  prefs: []
  type: TYPE_NORMAL
- en: 'The file is located in the bucket we created earlier. We specify the audio
    configuration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A response is obtained by passing the `audio` content, as well as the configuration
    specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The results can now be accessed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82460a32-f922-4b33-a502-babcfed47d56.png)'
  prefs: []
  type: TYPE_IMG
- en: The `recognize` method works when the input audio file is a short (<1 minute)
    duration audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the `audio` file is longer in duration, the method to be used is `long_running_recognize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `result` can then be accessed by specifying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the transcription and the confidence can be obtained by printing the
    response results, as was done previously.
  prefs: []
  type: TYPE_NORMAL
- en: Video Intelligence API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cloud Video Intelligence API makes videos searchable and discoverable, by
    extracting metadata with an easy-to-use REST API. You can now search every moment
    of every video file in your catalog. It quickly annotates videos stored in Google
    Cloud storage, and helps you to identify key entities (nouns) within your videos
    and when they occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cloud Video Intelligence API can be searched for and enabled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd03c537-4531-4616-b22e-ce7363923d6a.png)![](img/5a9ffecc-7966-4fd5-9ae7-f5d114eb5a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We import the required packages and add the path to the `api-key`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The method `features` enables us to specify the type of content that we want
    to detect in a video. The features available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54d48021-a4fb-496e-8b98-10cb80940baa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go ahead and detect labels in the video of interest to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the `config` and context of the video, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The video then needs to be passed from Cloud storage, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the `annotate_video` method is accessed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The annotation results for a video can be obtained at the:'
  prefs: []
  type: TYPE_NORMAL
- en: Video segment level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video shot level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results at the segment level, after looping through each of the various segment
    label annotations, can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f133d48-607a-4b76-90f0-abfa9876765b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, results at the shot level can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding lines of code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8670e457-6af0-4c19-b37d-e62ff23ad8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the result at the frame level can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding lines of code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a83da6e-44f2-4ad7-aa58-b6284fc4791d.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we went through the major machine learning APIs that Google
    provides: vision, translate, NLP, speech, and video intelligence. We have learned
    how the various methods in each of the APIs enable us to replicate deep learning
    results, without having to code from scratch.'
  prefs: []
  type: TYPE_NORMAL
