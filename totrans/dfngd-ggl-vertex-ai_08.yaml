- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: ML Model Explainability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的可解释性
- en: In the rapidly evolving world of **machine learning** (**ML**) and **artificial
    intelligence** (**AI**), developing models capable of delivering accurate predictions
    is no longer the sole objective. As organizations increasingly rely on data-driven
    decision-making, understanding the rationale behind a model’s predictions becomes
    paramount. The growing need for explainability in ML models stems from ethical,
    regulatory, and practical concerns, and it is here that the concept of **Explainable
    AI** (**XAI**) comes into play.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的机器学习（ML）和人工智能（AI）世界中，开发能够提供准确预测的模型不再是唯一目标。随着组织越来越依赖数据驱动的决策，理解模型预测背后的理由变得至关重要。机器学习模型中可解释性需求的增长源于道德、监管和实际方面的考虑，这就是可解释人工智能（XAI）概念发挥作用的地方。
- en: This chapter delves into the intricacies of Explainable ML models, a critical
    component in the MLOps landscape, with a focus on their implementation in the
    Google Cloud ecosystem. Although a comprehensive exploration of XAI techniques
    and tools is beyond this chapter’s scope, we aim to equip you with the knowledge
    and skills to build transparent, interpretable, and accountable ML models using
    the Explainable ML tools available on GCP.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了可解释机器学习模型的复杂性，这是机器学习操作（MLOps）景观中的一个关键组成部分，重点关注它们在 Google Cloud 生态系统中的实现。尽管对
    XAI 技术和工具的全面探索超出了本章的范围，但我们旨在为您提供构建透明、可解释和有责任感的机器学习模型所需的知识和技能，这些模型可以使用 GCP 上的可解释机器学习工具。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is Explainable AI and why is it important for MLOps practitioners?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释人工智能是什么，为什么它对机器学习操作（MLOps）从业者来说很重要？
- en: Overview of Explainable AI techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释人工智能技术的概述
- en: Explainable AI features available in Google Cloud Vertex AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释人工智能功能在 Google Cloud Vertex AI 中可用
- en: Hands-on exercises for using Vertex AI’s explainability features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Vertex AI 的可解释性功能的动手练习
- en: As we journey through this chapter, we will establish the importance of explainability
    and its role in enhancing trust, accountability, and fairness in ML models. Next,
    we will discuss various techniques for achieving explainability in ML, ranging
    from traditional interpretable models to explanation techniques for more complex
    models such as deep learning. We will then dive into Google Cloud’s XAI offerings,
    which facilitate the development and evaluation of Explainable ML models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入本章，我们将确立可解释性的重要性及其在增强机器学习模型中的信任、责任和公平性方面的作用。接下来，我们将讨论实现机器学习可解释性的各种技术，从传统的可解释模型到用于更复杂模型（如深度学习）的解释技术。然后，我们将深入了解
    Google Cloud 的可解释人工智能（XAI）产品，这些产品有助于开发和使用可解释机器学习模型。
- en: In addition to providing an understanding of Explainable ML models, this chapter
    will guide you through practical, hands-on examples, illustrating the application
    of these concepts in real-world scenarios.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解可解释机器学习模型外，本章还将通过实际操作示例指导您，说明这些概念在实际场景中的应用。
- en: By the end of this chapter, you will be well-equipped to design, deploy, and
    evaluate Explainable ML models on Google Cloud, ensuring that your organization
    stays ahead in the race toward ethical and responsible AI adoption.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将准备好设计、部署和评估 Google Cloud 上的可解释机器学习模型，确保您的组织在迈向道德和负责任的人工智能采用的竞赛中保持领先。
- en: What is Explainable AI and why is it important for MLOps practitioners?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释人工智能是什么，为什么它对机器学习操作（MLOps）从业者来说很重要？
- en: XAI refers to methods and techniques that are used in the domain of AI that
    aim to make the decision-making processes of AI models transparent, interpretable,
    and understandable to humans. Instead of acting as black boxes where input data
    goes in and a decision or prediction comes out without clarity on how the decision
    was reached, XAI seeks to provide insights into the inner workings of models.
    This transparency allows users, developers, and stakeholders to trust and validate
    the system’s decisions, ensuring they align with ethical, legal, and practical
    considerations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: XAI 指的是在人工智能领域使用的方法和技术，旨在使人工智能模型的决策过程对人类透明、可解释和可理解。XAI 不是作为黑盒运行，其中输入数据进入，决策或预测出来，而决策过程不清晰，XAI
    寻求揭示模型的内部运作。这种透明度使用户、开发人员和利益相关者能够信任并验证系统的决策，确保它们符合道德、法律和实际考虑。
- en: As ML continues to advance and its applications permeate various industries,
    the need for transparent and interpretable models has become a pressing concern.
    XAI aims to address this by developing techniques for understanding, interpreting,
    and explaining ML models. For MLOps practitioners working with Google Cloud, incorporating
    XAI into their workflows can lead to several benefits, including improved trust,
    regulatory compliance, and enhanced model performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的持续进步及其应用渗透到各个行业，对透明和可解释模型的需求已成为一个紧迫的问题。XAI旨在通过开发理解、解释和解释ML模型的技术来解决这个问题。对于与Google
    Cloud合作的MLOps从业者来说，将XAI纳入他们的工作流程可以带来几个好处，包括提高信任度、合规性和增强模型性能。
- en: First, let’s look at the key reasons for the importance of XAI for MLOps practitioners
    and its impact on the development and deployment of ML models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看XAI对MLOps从业者的重要性及其对ML模型开发和部署的影响的关键原因。
- en: Building trust and confidence
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立信任和信心
- en: XAI can help MLOps practitioners build trust in their models by providing clear
    and understandable explanations of how these models make decisions. This is particularly
    important when dealing with stakeholders who may not have a technical background
    as the ability to explain model behavior can lead to increased confidence in its
    predictions. Additionally, having a deeper understanding of the inner workings
    of a model allows practitioners to better communicate the limitations and strengths
    of their solutions, which, in turn, can foster trust among collaborators and end
    users.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: XAI可以通过提供模型如何做出决策的清晰易懂的解释来帮助MLOps从业者建立对模型的信任。这对于处理可能没有技术背景的利益相关者尤为重要，因为解释模型行为的能力可以增加对其预测的信心。此外，对模型内部运作有更深入的理解，使从业者能够更好地传达他们解决方案的限制和优势，从而在合作者和最终用户之间培养信任。
- en: Regulatory compliance
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监管合规
- en: As ML models become more widely adopted, regulatory bodies around the world
    are increasingly calling for more transparency and accountability in AI systems.
    XAI techniques can help MLOps practitioners ensure compliance with these regulations
    by providing insights into the decision-making process of their models. This can
    be especially important in industries such as healthcare, finance, and human resources,
    where the consequences of biased or unfair decisions can be significant. By incorporating
    XAI into their workflows, practitioners can demonstrate that their models adhere
    to relevant laws and ethical guidelines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型被更广泛地采用，世界各地的监管机构越来越要求人工智能系统具有更高的透明度和问责制。XAI技术可以帮助MLOps从业者通过提供模型决策过程的见解来确保遵守这些规定。这在医疗保健、金融和人力资源等行业尤为重要，在这些行业中，有偏见或不公平决策的后果可能非常严重。通过将XAI纳入他们的工作流程，从业者可以证明他们的模型遵守相关法律和伦理指南。
- en: Model debugging and improvement
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型调试和改进
- en: XAI can be invaluable to MLOps practitioners during the model development and
    debugging process. By providing insights into how a model is making its predictions,
    XAI can help identify areas where the model may be underperforming, overfitting,
    or exhibiting bias. With this information, practitioners can make targeted adjustments
    to their models, leading to improved performance and more robust solutions. This
    iterative process can save both time and resources, allowing practitioners to
    focus on addressing the most critical issues impacting their models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: XAI在模型开发和调试过程中对MLOps从业者来说非常有价值。通过提供模型预测方式的见解，XAI可以帮助识别模型可能表现不佳、过拟合或存在偏差的区域。有了这些信息，从业者可以对他们的模型进行有针对性的调整，从而提高性能并实现更稳健的解决方案。这个迭代过程可以节省时间和资源，使从业者能够专注于解决影响他们模型的最关键问题。
- en: Ethical considerations
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 伦理考量
- en: As the power and influence of ML models grow, so too does the responsibility
    of MLOps practitioners to ensure that these models are used ethically. XAI can
    help practitioners identify and address any unintended consequences or biases
    that may arise from their models. By understanding how a model makes its decisions,
    practitioners can better ensure that their solutions are fair, unbiased, and aligned
    with ethical principles.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型的力量和影响力增长，MLOps从业者的责任也随之增加，以确保这些模型被道德地使用。XAI可以帮助从业者识别和解决模型可能产生的任何意外后果或偏差。通过理解模型是如何做出决定的，从业者可以更好地确保他们的解决方案是公平的、无偏见的，并与伦理原则一致。
- en: Incorporating XAI into the MLOps workflow within the Google Cloud ecosystem
    can lead to numerous benefits for practitioners. From building trust with stakeholders
    and ensuring regulatory compliance to improving model performance and addressing
    ethical concerns, the importance of XAI cannot be understated. As the field of
    ML continues to evolve, integrating XAI into MLOps practices will become increasingly
    essential for the development and deployment of transparent, interpretable, and
    responsible AI solutions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Cloud生态系统中将XAI集成到MLOps工作流程中可以为从业者带来众多好处。从与利益相关者建立信任和确保合规性，到提高模型性能和解决伦理问题，XAI的重要性不容小觑。随着机器学习领域的不断发展，将XAI集成到MLOps实践中将变得越来越重要，这对于开发和应用透明、可解释和负责任的AI解决方案至关重要。
- en: Explainable AI techniques
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释人工智能技术
- en: Different techniques are available to cater to various types of data, including
    tabular, image, and text data. Each data type presents its own set of challenges
    and complexities, requiring tailored methods to provide meaningful insights into
    the decision-making processes of ML models. This subsection will list various
    XAI techniques applicable to tabular, image, and text data. The following section
    will dive into the ones available as out-of-the-box features in Google Cloud.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的技术可供选择，以适应各种类型的数据，包括表格、图像和文本数据。每种数据类型都带来其自身的挑战和复杂性，需要定制的方法来为机器学习模型的决策过程提供有意义的见解。本小节将列出适用于表格、图像和文本数据的各种XAI技术。下一节将深入探讨在Google
    Cloud中作为开箱即用功能提供的那些技术。
- en: Global versus local explainability
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局与局部可解释性
- en: 'Explainability can be categorized into two categories: local and global explainability.
    These terms are sometimes referred to as local and global feature importance:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性可以分为两类：局部可解释性和全局可解释性。这些术语有时也被称为局部和全局特征重要性：
- en: '**Global explainability** focuses on the overall impact of a feature on the
    model. This is usually obtained by calculating the average feature attribution
    values over the entire dataset. A feature with a high absolute value indicates
    that it significantly influenced the model’s predictions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局可解释性**关注特征对模型的整体影响。这通常是通过在整个数据集上计算平均特征归因值来获得的。具有高绝对值的特征表明它对模型预测有显著影响。'
- en: '**Local explainability** provides insight into how much each feature contributed
    to the prediction for a specific instance. The feature attribution values give
    information about the effect of a particular feature on the prediction relative
    to a baseline prediction.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释性**提供了关于每个特征对特定实例预测贡献多少的见解。特征归因值提供了关于特定特征相对于基线预测对预测的影响的信息。'
- en: Techniques for image data
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据技术
- en: 'XAI techniques for image data often focus on visualizing the areas within an
    image that contribute most significantly to the model’s predictions. Here are
    some key techniques:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: XAI技术在图像数据中通常关注可视化图像中对于模型预测贡献最大的区域。以下是一些关键技术：
- en: '**Integrated Gradients**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成梯度**'
- en: 'Integrated Gradients is an attribution technique specifically designed for
    deep learning models, such as neural networks. It computes the gradients of the
    model’s output concerning the input features (pixels in the case of image data)
    and integrates these gradients over a straight path from a baseline input to the
    instance of interest. This process assigns an importance value to each pixel in
    the image, reflecting its contribution to the model’s prediction. Integrated Gradients
    provide insights into the importance of each pixel and can help identify potential
    biases or shortcomings in the model’s predictions:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成梯度是一种专门为深度学习模型（如神经网络）设计的归因技术。它计算模型输出相对于输入特征（在图像数据的情况下为像素）的梯度，并将这些梯度沿从基线输入到感兴趣实例的直线路径进行积分。这个过程为图像中的每个像素分配一个重要性值，反映其对模型预测的贡献。集成梯度提供了对每个像素重要性的见解，并有助于识别模型预测中的潜在偏差或不足：
- en: '![Figure 8.1 – Integrated Gradients explanation](img/B17792_08_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 集成梯度解释](img/B17792_08_01.jpg)'
- en: Figure 8.1 – Integrated Gradients explanation
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 集成梯度解释
- en: The preceding figure showcases the Integrated Gradients approach to image explanation,
    which highlights pixels in the image that the model gives high importance to during
    prediction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图展示了集成梯度方法对图像的解释，它突出了模型在预测期间赋予高重要性的图像像素。
- en: '**eXtended Relevance-weighted Attribution of** **Importance** (**XRAI**)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展相关性加权重要性归因**（**XRAI**）'
- en: 'XRAI is an XAI method for visualizing the most important regions in an image
    for a given model’s prediction. It is an extension of the Integrated Gradients
    method, which combines pixel-wise attributions with segmentation techniques to
    generate more coherent and interpretable visualizations. By identifying the most
    important segments within an image, XRAI provides insights into the model’s decision-making
    process and can help identify potential biases or issues in the model’s predictions:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XRAI是一种XAI方法，用于可视化给定模型预测中图像的最重要区域。它是集成梯度方法的扩展，该方法结合了像素级归因与分割技术，以生成更连贯和可解释的视觉表示。通过识别图像中最重要的部分，XRAI提供了对模型决策过程的洞察，并有助于识别模型预测中的潜在偏差或问题：
- en: '![Figure 8.2 – XRAI](img/B17792_08_02.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – XRAI](img/B17792_08_02.jpg)'
- en: Figure 8.2 – XRAI
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – XRAI
- en: The preceding figure showcases the XRAI approach to an image explanation, which
    highlights areas of the image that the model gives high importance during prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图展示了XRAI对图像解释的方法，它突出了模型在预测期间赋予高重要性的图像区域。
- en: '**Local Interpretable Model-agnostic** **Explanations** (**LIME**)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关** **解释** （**LIME**）'
- en: LIME is an XAI technique that provides local explanations for individual predictions
    of any classifier. In the context of image data, LIME generates synthetic data
    points (perturbed images) around a specific instance, obtains predictions from
    the model, and fits an interpretable model (for example, linear regression) to
    these data points, weighted by their proximity to the instance. The resulting
    model provides insights into the most important regions that contribute to the
    prediction for the specific instance. By visualizing these regions, practitioners
    can better understand the model’s decision-making process and identify potential
    biases or issues in the model’s predictions.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LIME是一种XAI技术，为任何分类器的单个预测提供局部解释。在图像数据的上下文中，LIME在特定实例周围生成合成数据点（扰动图像），从模型中获得预测，并使用这些数据点与实例的邻近度进行加权，拟合一个可解释的模型（例如，线性回归）。生成的模型提供了对特定实例预测中最重要的区域的洞察。通过可视化这些区域，从业者可以更好地理解模型的决策过程，并识别模型预测中的潜在偏差或问题。
- en: '**Gradient-weighted Class Activation** **Mapping** (**Grad-CAM**)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度加权类激活** **映射** （**Grad-CAM**）'
- en: Grad-CAM is a visualization technique for deep learning models, specifically
    **convolutional neural networks** (**CNNs**). It generates heatmap-like visualizations
    of the most important regions in an image for a given model’s prediction. Grad-CAM
    computes the gradients of the predicted class score concerning the feature maps
    of the last convolutional layer and then uses these gradients to compute a weighted
    sum of the feature maps. The resulting heatmap highlights the regions in the image
    that contribute most to the model’s prediction. Grad-CAM provides insights into
    the model’s decision-making process and can help identify potential biases or
    shortcomings in the model’s predictions.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Grad-CAM是一种针对深度学习模型（特别是**卷积神经网络**（**CNNs**））的可视化技术。它为给定模型预测中的图像生成最重区域的热图可视化。Grad-CAM计算预测类别分数相对于最后一层卷积层的特征图的梯度，然后使用这些梯度计算特征图的加权总和。生成的热图突出了对模型预测贡献最大的图像区域。Grad-CAM提供了对模型决策过程的洞察，并有助于识别模型预测中的潜在偏差或不足。
- en: These techniques provide insights into the model’s decision-making process and
    can help identify potential biases or shortcomings in the model’s predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术提供了对模型决策过程的洞察，并有助于识别模型预测中的潜在偏差或不足。
- en: Techniques for tabular data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表格数据技术
- en: 'Tabular data, which is composed of structured rows and columns, is one of the
    most common data types encountered in ML. Various XAI techniques can be employed
    to interpret models trained on tabular data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由结构化行和列组成的表格数据是机器学习中遇到的最常见数据类型之一。可以采用各种XAI技术来解释在表格数据上训练的模型：
- en: '**Local Interpretable Model-agnostic** **Explanations** (**LIME**)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关** **解释** （**LIME**）'
- en: As its name suggests, LIME is an XAI technique that provides *local* explanations
    for individual predictions of any classifier. It does so by approximating the
    complex model with a simpler, interpretable model (for example, linear regression)
    within the vicinity of a specific instance. LIME generates synthetic data points
    around the instance, obtains predictions from the complex model, and fits an interpretable
    model to these data points, weighted by their proximity to the instance. The resulting
    model provides insights into the most important features contributing to the prediction
    for the specific instance.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如其名所示，LIME是一种XAI技术，为任何分类器的单个预测提供*局部*解释。它是通过在特定实例的附近用一个更简单、可解释的模型（例如，线性回归）来近似复杂模型来实现的。LIME在实例周围生成合成数据点，从复杂模型中获取预测，并使用这些数据点拟合一个可解释的模型，这些数据点根据它们与实例的接近程度进行加权。生成的模型提供了关于对特定实例的预测贡献最大的特征的见解。
- en: 'In the following figure, we’re using a LIME report to explain the decision
    of an ML model trained to predict the survival probabilities of Titanic’s passengers
    based on their attributes, such as their gender, the fare they paid, the passenger
    class they were traveling in, and so on. The Titanic survival dataset is a common
    publically available dataset that’s used as an example for classification models.
    Let’s see if LIME can help us get some insights into the model’s behavior:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下面的图中，我们使用LIME报告来解释一个训练有素的机器学习模型的决策，该模型根据乘客的属性（如性别、支付的票价、他们所乘坐的乘客等级等）预测泰坦尼克号乘客的生存概率。泰坦尼克号生存数据集是一个常见的公开可用数据集，用作分类模型的示例。让我们看看LIME是否可以帮助我们了解模型的行为：
- en: '![Figure 8.3 – Explaining classification models with LIME](img/B17792_08_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 使用LIME解释分类模型](img/B17792_08_03.jpg)'
- en: Figure 8.3 – Explaining classification models with LIME
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 使用LIME解释分类模型
- en: The leftmost chart shows the predicted survival probability for the chosen passenger.
    In this example, the model predicts that this passenger had a 40% probability
    of survival based on their key attributes. The chart in the middle shows us the
    ranked feature importance list generated by using LIME. Based on this chart, it
    seems that the top three most important features that dictated whether a Titanic
    passenger survived or not were their sex/gender, fare paid, and the passenger
    class they were traveling in. This makes sense because we know women were evacuated
    first, giving them a higher overall chance of survival. We also know that passengers
    who paid lower fares and had lower-class tickets had their cabins on the lower
    decks/floors of the ship, which got flooded first and the passengers with higher
    fares had their cabins on the upper decks, giving them a better chance of survival.
    So, you can see how LIME and similar techniques can help decipher black box ML
    models and help us better understand why a particular prediction was made. You
    will also be glad to know that the Titanic passenger we used for the preceding
    example was a 26-year-old lady named Miss. Laina Heikkinen paid $7.925 for a ticket
    in the third-class passenger section and despite our model giving her a less than
    40% chance of survival, she survived.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最左侧的图表显示了所选乘客的预测生存概率。在这个例子中，模型预测这位乘客基于其关键属性有40%的生存概率。中间的图表显示了使用LIME生成的按重要性排序的特征列表。根据这个图表，似乎决定泰坦尼克号乘客是否生存的前三个最重要的特征是他们的性别/性别、支付的票价以及他们所乘坐的乘客等级。这很有道理，因为我们知道女性首先被疏散，给了她们更高的整体生存机会。我们也知道支付较低票价和持有低等级票的乘客住在船的下层甲板/楼层，这些地方首先被淹没，而支付较高票价的乘客住在上层甲板，给了他们更好的生存机会。所以，你可以看到LIME和类似技术如何帮助解析黑盒机器学习模型，并帮助我们更好地理解为什么做出了特定的预测。你也会很高兴地知道，我们之前用来举例的泰坦尼克号乘客是一位26岁的女士，名叫Laina
    Heikkinen，她在三等舱乘客区支付了7.925美元的票价，尽管我们的模型给她低于40%的生存机会，但她还是幸存了下来。
- en: '**Shapley Additive exPlanations** (**SHAP**) (native support on Vertex AI)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shapley Additive exPlanations** (**SHAP**)（在Vertex AI上原生支持）'
- en: 'SHAP is a unified measure of feature importance based on cooperative game theory,
    providing a consistent and fair way to allocate feature importance. By calculating
    the Shapley value for each feature, SHAP assigns an importance value that reflects
    its contribution to the prediction for a specific instance. The Shapley value
    is calculated by averaging the marginal contributions of a feature across all
    possible feature combinations. SHAP values provide insights into the most influential
    features driving a model’s predictions and can be used with a variety of models.
    Recommended model types include non-differentiable models such as ensembles of
    trees. They can also be used for neural networks, where SHAP can provide insights
    into the contribution of each input feature to the final prediction made by the
    network. By analyzing the SHAP values, you can determine which features have the
    most impact on the network’s output and understand the relationship between the
    inputs and outputs:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SHAP是一种基于合作博弈论的特征重要性统一度量，提供了一种一致且公平的方式来分配特征重要性。通过计算每个特征的平均边际贡献，SHAP为每个特征分配一个重要性值，该值反映了其对特定实例预测的贡献。Shapley值是通过平均所有可能的特征组合中一个特征的边际贡献来计算的。SHAP值提供了关于驱动模型预测的最具影响力的特征的见解，并且可以与各种模型一起使用。推荐模型类型包括非可微分的模型，如树的集成。它们也可以用于神经网络，其中SHAP可以提供关于每个输入特征对网络最终预测贡献的见解。通过分析SHAP值，您可以确定哪些特征对网络的输出影响最大，并了解输入与输出之间的关系：
- en: '![Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method](img/B17792_08_04.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 基于Shapley方法的Vertex AI中的特征重要性图](img/B17792_08_04.jpg)'
- en: Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 基于Shapley方法的Vertex AI中的特征重要性图
- en: '**Permutation** **feature importance**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置换** **特征重要性**'
- en: Permutation feature importance is a model-agnostic technique that estimates
    the importance of each feature by measuring the change in the model’s performance
    when the feature’s values are randomly shuffled. This process is repeated several
    times, and the average decrease in performance is used as an estimate of the feature’s
    importance. By disrupting the relationship between the feature and the target
    variable, permutation importance helps identify the features that have the greatest
    impact on the model’s predictions.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 置换特征重要性是一种模型无关的技术，通过测量当特征值随机打乱时模型性能的变化来估计每个特征的重要性。这个过程重复多次，平均性能下降被用作特征重要性的估计。通过破坏特征与目标变量之间的关系，置换重要性有助于识别对模型预测影响最大的特征。
- en: '**Partial dependence** **plots** (**PDP**)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分依赖** **图** （**PDP**）'
- en: PDP is a visualization technique that depicts the relationship between a specific
    feature and the model’s predicted outcome while holding all other features constant.
    By illustrating how a single feature influences the prediction, PDP can help practitioners
    better understand the behavior of their models and identify potential biases or
    inconsistencies.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PDP是一种可视化技术，它描绘了特定特征与模型预测结果之间的关系，同时保持所有其他特征不变。通过说明单个特征如何影响预测，PDP可以帮助从业者更好地理解其模型的行为，并识别潜在的偏差或不一致性。
- en: '**Feature importance** (for example, GINI importance and coefficients in linear
    models)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性**（例如，GINI重要性和线性模型中的系数）'
- en: 'Feature importance is a set of techniques that quantify the impact of input
    features on the model’s predictions. These methods can help practitioners identify
    the most relevant features, enabling them to focus on the most significant variables
    during model development and debugging. Some common approaches to feature importance
    are as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征重要性是一组量化输入特征对模型预测影响的技巧。这些方法可以帮助从业者识别最相关的特征，使他们能够在模型开发和调试期间专注于最重要的变量。以下是一些常见的特征重要性方法：
- en: '**GINI Importance**: Used in decision trees and random forests, GINI importance
    measures the average reduction in impurity (**GINI index**) attributable to a
    specific feature across all the trees in the forest.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GINI重要性**：在决策树和随机森林中应用，GINI重要性衡量的是特定特征在整个森林中所有树上的平均纯度减少（**GINI指数**）。'
- en: '**Coefficients in linear models**: In linear regression and logistic regression,
    the coefficients of the model can be used as a measure of feature importance,
    indicating the magnitude and direction of the relationship between each feature
    and the target variable. Larger absolute coefficient values indicate a stronger
    relationship between the feature and the target variable.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性模型中的系数**：在线性回归和逻辑回归中，模型的系数可以用作特征重要性的度量，表示每个特征与目标变量之间关系的幅度和方向。较大的绝对系数值表示特征与目标变量之间关系更强。'
- en: These techniques help practitioners understand the relationships between input
    features and model predictions, identify the most influential features, and assess
    the impact of specific features on individual predictions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术有助于从业者理解输入特征与模型预测之间的关系，识别最有影响力的特征，并评估特定特征对单个预测的影响。
- en: Techniques for text data
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本数据的技术
- en: 'In the case of natural language processing models that use text data, the aim
    is to identify the most significant words or phrases that contribute to the model’s
    predictions. Here are some XAI techniques for text data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用文本数据的自然语言处理模型，目标是识别对模型预测有最大贡献的最重要单词或短语。以下是一些针对文本数据的 XAI 技术：
- en: '**Text-specific LIME**: This is an adaptation of LIME specifically designed
    for text data that provides explanations for individual predictions by highlighting
    the most important words or phrases.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本特定 LIME**：这是针对文本数据特别设计的 LIME 的一个版本，通过突出显示最重要的单词或短语来为单个预测提供解释。'
- en: 'In the following example, we’re using LIME to explain why the ML model we built
    to classify movie reviews as positive or negative arrives at a particular conclusion:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用 LIME 来解释我们构建的用于将电影评论分类为正面或负面的机器学习模型为何得出特定的结论：
- en: '![Figure 8.5 – LIME-based explanation for text classification](img/B17792_08_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 基于 LIME 的文本分类解释](img/B17792_08_05.jpg)'
- en: Figure 8.5 – LIME-based explanation for text classification
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 基于 LIME 的文本分类解释
- en: 'As we can see, the predicted probability of the movie review being positive
    is 0.77 (77%). The words highlighted in orange contributed toward the positive
    probability, while the words highlighted in blue had a significant contribution
    toward moving the final prediction toward the “negative” label. The chart at the
    top right shows the respective contribution of each of the highlighted words toward
    the final decision. For example, if we remove the word “amazing” from the review
    text, the positive probability will go down by 0.03:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，电影评论预测为正面的概率为 0.77（77%）。橙色高亮的单词有助于提高正面的概率，而蓝色高亮的单词对将最终预测推向“负面”标签有显著贡献。右上角的图表显示了每个高亮单词对最终决策的相应贡献。例如，如果我们从评论文本中删除“amazing”这个词，正面的概率将下降
    0.03：
- en: '**Text-specific SHAP**: This is an adaptation of SHAP tailored for text data,
    attributing importance values to individual words or phrases within a given text'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本特定 SHAP**：这是针对文本数据特别设计的 SHAP 的一个版本，将重要性值分配给给定文本中的单个单词或短语'
- en: '**Attention mechanisms**: In deep learning models such as Transformers, attention
    mechanisms can provide insights into the relationships between words and the model’s
    predictions by visualizing the attention weights'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力机制**：在像 Transformers 这样的深度学习模型中，注意力机制可以通过可视化注意力权重来提供关于单词与模型预测之间关系的见解。'
- en: Now that we have familiarized ourselves with the various popular XAI techniques,
    let’s look at the different features available within Google Cloud Vertex AI that
    can help us build XAI solutions using these techniques.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了各种流行的 XAI 技术，让我们来看看 Google Cloud Vertex AI 中可用的不同功能，这些功能可以帮助我们使用这些技术构建
    XAI 解决方案。
- en: Explainable AI features available in Google Cloud Vertex AI
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI 中可用的可解释人工智能功能
- en: '**Google Cloud Vertex AI** offers a suite of tools and options tailored to
    make AI systems more understandable. This section delves into the various XAI
    options available in Vertex AI, showcasing how this platform is advancing the
    frontier of transparent ML.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Cloud Vertex AI** 提供了一套工具和选项，旨在使人工智能系统更加易于理解。本节深入探讨了 Vertex AI 中可用的各种可解释人工智能（XAI）选项，展示了该平台如何推进透明机器学习的前沿。'
- en: 'Broadly, the XAI options available in Vertex AI can be divided into two types:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，Vertex AI 中可用的 XAI 选项可以分为两种类型：
- en: '**Feature-based**: Feature attributions refer to the degree to which each feature
    in a model contributes to the predictions for a specific instance. When making
    a prediction request, you receive the predicted values that are generated by your
    model. However, when requesting an explanation, you receive not only the predictions
    but also the feature attribution information.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于特征的**：特征归因指的是模型中每个特征对特定实例预测的贡献程度。在做出预测请求时，您会收到由您的模型生成的预测值。然而，在请求解释时，您不仅会收到预测值，还会收到特征归因信息。'
- en: It is important to note that feature attributions are primarily applicable to
    tabular data but also include built-in visualization capabilities for image data.
    This makes it easier to understand and interpret the attributions more intuitively.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，特征归因主要适用于表格数据，但也包括图像数据的内置可视化功能。这使得更直观地理解和解释归因变得更容易。
- en: '**Example-based**: Vertex AI utilizes nearest neighbor search to provide example-based
    explanations. This method involves finding the closest examples (usually from
    the training data) to the input and returning a list of the most similar examples.
    This approach leverages the principle that similar inputs are likely to produce
    similar predictions, allowing us to gain insight into the behavior of our model.
    By examining these similar examples, we can better understand and explain our
    model’s output.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于示例的**：Vertex AI利用最近邻搜索来提供基于示例的解释。这种方法涉及找到与输入最接近的示例（通常来自训练数据），并返回最相似示例的列表。这种方法利用了相似输入可能产生相似预测的原则，使我们能够深入了解模型的行为。通过检查这些相似示例，我们可以更好地理解和解释模型输出。'
- en: Feature-based explanation techniques available on Vertex AI
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI上可用的基于特征的解释技术
- en: The following table shows different methods of feature-based explanations available
    in GCP.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了GCP中可用的基于特征的解释方法。
- en: '| **Method** | **Compatible Vertex AI** **Model Resources** | **Example** **Use
    Cases** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | **兼容的Vertex AI模型资源** | **示例** **用例** |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sampled Shapley (SHAP) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 样本Shapley（SHAP）|'
- en: Any custom-trained model (running in any prediction container)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何自定义训练的模型（在任何预测容器中运行）
- en: AutoML tabular models
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML表格模型
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Classification and regression on tabular data
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据的分类和回归
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Integrated Gradients |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 集成梯度 |'
- en: Custom-trained TensorFlow models using TensorFlow prebuilt containers for prediction
    serving
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow预构建容器进行预测服务的自定义训练TensorFlow模型
- en: AutoML image models
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML图像模型
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Classification and regression on tabular data
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据的分类和回归
- en: Classification of image data
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据的分类
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| XRAI (eXplanation with Ranked Area Integrals) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| XRAI（基于排序面积积分的解释）|'
- en: Custom-trained TensorFlow models using TensorFlow prebuilt containers for prediction
    serving
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow预构建容器进行预测服务的自定义训练TensorFlow模型
- en: AutoML image models
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML图像模型
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Classification of image data
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据的分类
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 8.1 – Feature attribution methods available in GCP. Source: [https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – GCP中可用的特征归因方法。来源：[https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)
- en: Next, we will learn how to use these Vertex AI features to generate explanations
    for model output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用这些Vertex AI功能来生成模型输出的解释。
- en: Using the model feature importance (SHAP-based) capability with AutoML for tabular
    data
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoML表格数据模型中的模型特征重要性（基于SHAP）功能
- en: In the following exercise, we will learn how to use XAI features in Vertex AI
    to evaluate feature importance in ML models for structured data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将学习如何使用Vertex AI中的XAI功能来评估结构化数据ML模型中的特征重要性。
- en: Exercise 1
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1
- en: 'Objective: Use Vertex AutoML Tables’ **Feature importance** feature to explain
    global (model-level) and local (sample-level) behavior'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：使用Vertex AutoML Tables的**特征重要性**功能来解释全局（模型级别）和局部（样本级别）行为
- en: 'Dataset to be used: *Bank Marketing Dataset* (Available in the *Chapter-8*
    folder within this book’s GitHub repository)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的数据集：*银行营销数据集*（可在本书GitHub仓库的*第8章*文件夹中找到）
- en: 'Model objective: Predict whether the customer will open a new term deposit
    or not (**Feature Label –** **deposit(Yes/No)**)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型目标：预测客户是否会开设新的定期存款（**特征标签 –** **deposit(是/否)**）
- en: 'Follow these steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下步骤操作：
- en: Following the steps shown in [*Chapter 5*](B17792_05.xhtml#_idTextAnchor066),
    create an AutoML classification model to predict the probability of a customer
    opening the term deposit.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[*第 5 章*](B17792_05.xhtml#_idTextAnchor066)中所示步骤，创建一个 AutoML 分类模型来预测客户开设定期存款的概率。
- en: Once the model has been trained, navigate to **Model Registry** | **Your model**
    | **Your model version** (*1 for new models*) | the **Evaluate** tab.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练完成后，导航到**模型注册** | **您的模型** | **您的模型版本**（*1 为新模型*） | **评估**选项卡。
- en: Scroll down to the **Feature** **importance** section.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到**特征** **重要性**部分。
- en: 'The following **Feature importance** graph shows the relative feature importance
    of different model features:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的**特征重要性**图显示了不同模型特征的相对重要性：
- en: '![Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables](img/B17792_08_06.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – Vertex AI AutoML Tables 中的特征重要性图](img/B17792_08_06.jpg)'
- en: Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – Vertex AI AutoML Tables 中的特征重要性图
- en: 'As shown in the previous graph, the most important features in our model to
    predict whether a customer will respond to the outreach and open a new term deposit
    are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个图所示，我们模型中预测客户是否会响应外展并开设新的定期存款的最重要特征如下：
- en: '**Duration**: How long the customer has been with the bank'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续时间**：客户与银行的关系时长'
- en: '**Month**: This could be because of the seasonality of the business'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**月份**：这可能是因为业务的季节性'
- en: '**Contact** (Method – Cellular/Landline): This could be because of the different
    communication preferences of different types of customers'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联系方式**（方法 – 手机/固定电话）：这可能是因为不同类型客户的通信偏好不同'
- en: '**Outcome**: The outcome of the last promotional outreach to this customer'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：对这位客户最后一次促销活动的结果'
- en: 'The following are the least important features as per the preceding graph:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的是最不重要的特征，根据前一个图：
- en: '**Default**: Has the customer defaulted before?'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认**：客户之前是否违约过？'
- en: '**Marital (Status)**'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**婚姻（状态**）'
- en: '**Education**: Level of education attained'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**教育**：达到的教育水平'
- en: '**Previous**: The number of times contact was made before this campaign'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**之前**：在此活动之前联系次数的数量'
- en: The preceding information can help the data science team better understand the
    model’s behavior and possibly uncover additional insights into their data and
    customer behavior and provide important guidance for future experiments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的信息可以帮助数据科学团队更好地理解模型的行为，并可能揭示他们对数据和客户行为的更多见解，并为未来的实验提供重要指导。
- en: Although performing exploratory data analysis on the training dataset is beyond
    the scope of this book, for anyone interested, you can look at the notebook ([*Chapter
    8*](B17792_08.xhtml#_idTextAnchor102) *– ML Model Explainability – Exercise 1
    Addendum*) for the correlation analysis between features and predicted labels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在训练数据集上执行探索性数据分析超出了本书的范围，但对于感兴趣的任何人，你可以查看笔记本（[*第 8 章*](B17792_08.xhtml#_idTextAnchor102)
    *– 机器学习模型可解释性 – 练习 1 补充资料*）以了解特征与预测标签之间的相关性分析。
- en: 'Here are some insights we can draw from this feature importance information:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个特征重要性信息中，我们可以得出以下见解：
- en: '`Deposit_Signup`), which aligns with the duration being high in terms of feature
    importance in the preceding graph.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`Deposit_Signup`)，这与前一个图中特征重要性方面持续时间高的趋势相一致。
- en: '**Contact**: Similarly, correlation analysis also shows that someone owning
    a cellular phone has a strong correlation with someone opening a deposit in response
    to the campaign.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联系方式**：同样，相关性分析还显示，拥有手机的人与对活动做出存款响应的人有很强的相关性。'
- en: '**Outcome**: Correlation analysis also shows that “Success” has a strong correlation
    with someone opening a deposit. This means that if someone responded positively
    to the last campaign, there is a stronger chance of that person being influenced
    by the current campaign.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：相关性分析还显示，“成功”与有人开设存款之间存在强烈的关联。这意味着如果有人对上次活动做出了积极回应，那么这个人受到当前活动影响的可能性更大。'
- en: '**Months**: In correlation analysis, we can also see that some months (March
    and May specifically) have a strong correlation with the positive outcome of the
    campaign. This could relate to AutoML surfacing months as an important feature.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**月份**：在相关性分析中，我们还可以看到一些月份（特别是三月和五月）与活动的积极结果有很强的相关性。这可能意味着 AutoML 将月份作为一个重要特征呈现。'
- en: Exercise 2
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2
- en: 'Objective: Use Vertex AI’s feature attribution features to explain image classification
    predictions'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：使用 Vertex AI 的特征归因功能来解释图像分类预测
- en: 'Dataset to be used: `Fast_Food_Classification_Dataset`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的数据集：`Fast_Food_Classification_Dataset`
- en: 'Follow these steps to create the image classification model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下步骤创建图像分类模型：
- en: 'Download and unzip the dataset from Kaggle: [https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset)'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Kaggle 下载并解压数据集：[https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset)
- en: Following the steps shown in [*Chapter 5*](B17792_05.xhtml#_idTextAnchor066),
    create an AutoML image classification dataset using `Fast_Food_Classification_Dataset`.
    Make sure to select data type and objective as **Image classification (Single-label)**,
    as shown.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照第 [*5章*](B17792_05.xhtml#_idTextAnchor066) 中所示步骤，使用 `Fast_Food_Classification_Dataset`
    创建一个 AutoML 图像分类数据集。确保选择数据类型和目标为**图像分类（单标签**），如图所示。
- en: '![Figure 8.7 – Model objective – Image classification (Single-label)](img/B17792_08_07.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 模型目标 – 图像分类（单标签）](img/B17792_08_07.jpg)'
- en: Figure 8.7 – Model objective – Image classification (Single-label)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 模型目标 – 图像分类（单标签）
- en: Once the empty dataset is created, go to the **Browse** tab and add new labels
    for each food type you plan to include in your model. In this example, we uploaded
    our favorite fast foods, including burgers, donuts, hot dogs, and pizza, but feel
    free to use whatever food types you want to use.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦创建了空数据集，转到 **浏览** 选项卡并为您计划包含在模型中的每种食物类型添加新标签。在此示例中，我们上传了我们最喜欢的快餐，包括汉堡、甜甜圈、热狗和披萨，但请随意使用您想要的任何食物类型。
- en: '![Figure 8.8 – Creating label names](img/B17792_08_08.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 创建标签名称](img/B17792_08_08.jpg)'
- en: Figure 8.8 – Creating label names
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 创建标签名称
- en: Now let’s upload the images of different fast-food types and annotate/label
    them. You don’t need all the available images in the dataset. Just 50 or so images
    for each food type should suffice.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们上传不同快餐类型的图像并将它们标注/标记。您不需要数据集中所有的图像。每种食物类型只需大约 50 张图像就足够了。
- en: 'Repeat the following steps for each food type:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对每种食物类型重复以下步骤：
- en: Upload images – Labeling images one at a time is difficult. To make labeling
    the images a bit easier, upload images for one food type at a time.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传图像 – 逐个标记图像很困难。为了使标记图像变得更容易，一次上传一种食物类型的图像。
- en: '![Figure 8.9 – Adding labels to images](img/B17792_08_09.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 为图像添加标签](img/B17792_08_09.jpg)'
- en: Figure 8.9 – Adding labels to images
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 为图像添加标签
- en: As shown, once you have uploaded the images for a particular food item, you
    can click on **Unlabeled** and then **Select All** to select all the images that
    need to be labeled. If you upload and label one food type at a time, it ensures
    that you only select images of one type of food. If you upload all images at once,
    then clicking on the **Unlabeled** tab would end up selecting ALL unlabeled images,
    requiring you to manually select images of one type.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，一旦上传了特定食物项目的图像，您可以点击 **未标记** 然后点击 **全选** 以选择所有需要标记的图像。如果您一次上传并标记一种食物类型，则确保您只选择该类型的图像。如果您一次性上传所有图像，那么点击
    **未标记** 选项卡最终会选中所有未标记的图像，需要您手动选择一种类型的图像。
- en: Once the images have been selected, click on **ASSIGN LABELS** and select the
    right food type label. Then click **Save.**
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择图像后，点击 **分配标签** 并选择正确的食物类型标签。然后点击 **保存**。
- en: '*Do this process for all different* *food types*.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*对所有的不同* *食物类型* 进行此过程。'
- en: Once all images are uploaded and labeled, navigate to the dataset in Vertex
    AI and go to the **Browse** tab.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有图像都已上传并标记，导航到 Vertex AI 中的数据集并转到 **浏览** 选项卡。
- en: 'Click **TRAIN** **NEW MODEL**:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **训练** **新模型**：
- en: '![Figure 8.10 – Initiating model training](img/B17792_08_10.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 启动模型训练](img/B17792_08_10.jpg)'
- en: Figure 8.10 – Initiating model training
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 启动模型训练
- en: 'On the next screen, select the dataset and annotation set you want to use for
    training the new model. Then, select the following options and click **CONTINUE**:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，选择您用于训练新模型的数据集和注释集。然后选择以下选项并点击 **继续**：
- en: 'Objective: **Image** **classification** (**Single-label**)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：**图像** **分类**（**单标签**）
- en: 'Model training method: **AutoML**'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练方法：**AutoML**
- en: 'Choose where to use the model: **Cloud**'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择模型的使用位置：**云**
- en: '![Figure 8.11 – Training configuration/options](img/B17792_08_11.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 训练配置/选项](img/B17792_08_11.jpg)'
- en: Figure 8.11 – Training configuration/options
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 训练配置/选项
- en: On the next screen, select **TRAIN NEW MODEL** and enter the name of the new
    model. You can leave all other options as is and click **CONTINUE**.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，选择**训练新模型**并输入新模型的名称。您可以保留所有其他选项不变，然后点击**继续**。
- en: 'On the next screen, select **Default** as the goal and click **CONTINUE**:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，选择**默认**作为目标并点击**继续**：
- en: '![Figure 8.12– Training configuration/options](img/B17792_08_12.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12– 训练配置/选项](img/B17792_08_12.jpg)'
- en: Figure 8.12– Training configuration/options
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12– 训练配置/选项
- en: On the next screen (the **Explainability** tab), check the **Generate Explainable**
    **Bitmaps** option.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏（**可解释性**选项卡）中，勾选**生成可解释位图**选项。
- en: On the `positive,` emphasizing regions with the highest positive influences.
    Essentially, it illuminates pixels that significantly contribute to a positive
    prediction by the model. Changing the polarity to `negative` will underscore areas,
    persuading the model away from predicting the positive class and aiding in pinpointing
    areas responsible for false negatives. There’s also an option to choose `both,`
    which provides a comprehensive view by displaying both positive and negative attributions.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**正**上强调具有最高正影响力的区域。本质上，它照亮了模型做出积极预测的像素，对积极预测有显著贡献。将极性更改为**负**将强调区域，使模型远离预测正类，并有助于定位导致假阴性的区域。还有一个选择**两者都**，它通过显示正负贡献来提供全面的视图。
- en: '`pink_green` default, where green signifies positive attributions and pink
    denotes negative ones. On the other hand, XRAI visualizations employ a gradient
    color scheme, with `Viridis` as the default. In this setup, the most impactful
    regions are bathed in yellow, while the less influential areas are shaded in blue.
    For an exhaustive list of available palettes, consult the **Visualization** message
    within the API documentation.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pink_green`默认，其中绿色表示正贡献，粉色表示负贡献。另一方面，XRAI可视化使用渐变色方案，默认为`Viridis`。在这个设置中，最有影响力的区域被黄色笼罩，而影响力较小的区域则被蓝色阴影覆盖。有关可用调色板的完整列表，请参阅API文档中的**可视化**消息。'
- en: '**Overlay type**: This setting defines how the original image is showcased
    within the visualization. Tweaking the overlay enhances visibility, especially
    when the inherent properties of the initial image obscure the visualization details.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**叠加类型**：此设置定义了原始图像在可视化中的展示方式。调整叠加可以增强可见性，尤其是在初始图像的固有属性掩盖了可视化细节时。'
- en: '**Steps**: The number of steps used to approximate the path integral can be
    specified here. It is recommended to start with a value of 50 and gradually increase
    it until the “sum to diff” property falls within the desired error range. The
    valid range for this value is inclusive between 1 and 100:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤**：可以在此处指定用于近似路径积分的步骤数。建议从50开始，逐渐增加，直到“求和到差”属性落在所需的误差范围内。此值的有效范围在1到100之间：'
- en: '![Figure 8.13 – Explainability configurations](img/B17792_08_13.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 可解释性配置](img/B17792_08_13.jpg)'
- en: Figure 8.13 – Explainability configurations
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 可解释性配置
- en: In the **Compute and pricing** tab, set the budget to 8 hours. This specifies
    the maximum amount of time the training will run.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**计算和定价**选项卡中，将预算设置为8小时。这指定了训练将运行的最大时间。
- en: Click **START TRAINING**.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**开始训练**。
- en: (A coffee break would be too short, so maybe go prepare a 7-course meal and
    come back to check on the training status in a few hours!)
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （咖啡休息时间太短了，所以也许可以去准备一顿七道菜的晚餐，然后几个小时后回来检查训练状态！）
- en: Once the model training is complete, we need to deploy the model to a Vertex
    AI endpoint by following the next steps.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们需要按照以下步骤将模型部署到Vertex AI端点。
- en: Navigate to **Model Registry** | **Your model** | **Your model version** (*1
    for* *new models*).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**模型注册** | **您的模型** | **您的模型版本**（*1用于* *新模型*）。
- en: Navigate to the **Deploy & Test** tab and click **Deploy** **to endpoint**.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**部署与测试**选项卡并点击**部署到端点**。
- en: 'Enter the name of the endpoint and click **CONTINUE**:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入端点名称并点击**继续**：
- en: '![Figure 8.14 – Model deployment options](img/B17792_08_14.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – 模型部署选项](img/B17792_08_14.jpg)'
- en: Figure 8.14 – Model deployment options
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 模型部署选项
- en: 'In the **Model settings** tab, check **Enable feature attributions for this
    model** and then click the **EDIT** button underneath to open the **Explainability**
    **options** menu:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**模型设置**选项卡中，勾选**为此模型启用特征归因**，然后点击下方的**编辑**按钮以打开**可解释性****选项**菜单：
- en: '![Figure 8.15 – Deployment configuration for explainability](img/B17792_08_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – 可解释性部署配置](img/B17792_08_15.jpg)'
- en: Figure 8.15 – Deployment configuration for explainability
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 可解释性部署配置
- en: In the **Explainability options** menu, select the **Integrated gradients**
    option since we are first creating an endpoint to test the Integrated Gradients
    technique. Click **DONE**.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**可解释性选项**菜单中，选择**集成梯度**选项，因为我们首先创建一个端点来测试集成梯度技术。点击**完成**。
- en: Now, repeat these steps to create an endpoint for the **XRAI** explainability
    option. This time, suffix the name of the endpoint with XRAI, and on the **Explainability
    options** screen, pick **XRAI**.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，重复这些步骤以创建一个用于**XRAI**可解释性选项的端点。这次，在端点名称后缀加上XRAI，并在**可解释性选项**屏幕上选择**XRAI**。
- en: With that, two endpoints should have been created for the model.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到此为止，应该已经为模型创建了两个端点。
- en: 'Now, let’s test the model by uploading a sample image of donuts and evaluate
    the prediction and the explanation for the prediction returned by the model:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过上传一个甜甜圈的样本图片来测试模型，并评估模型返回的预测和解释：
- en: In the model’s **Deploy & Test** tab, select the **Integrated Gradients** endpoint
    by clicking on **Endpoint ID**. Do not click on the endpoint’s name as that will
    take you to the endpoint’s settings screen.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型的**部署与测试**标签页中，通过点击**端点ID**选择**集成梯度**端点。不要点击端点的名称，因为这会带您进入端点的设置屏幕。
- en: Click **Upload & Explain** and select an image you want to test.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**上传并解释**并选择您想要测试的图片。
- en: 'Vertex AI will process the image and present the resulting classification of
    the image, along with an explanation (the image overlay will show areas of high
    importance for the image):'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI将处理图像，并展示图像的最终分类结果，以及一个解释（图像叠加将显示图像中重要性高的区域）：
- en: '![Figure 8.16 – Uploaded image](img/B17792_08_16.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 上传的图片](img/B17792_08_16.jpg)'
- en: Figure 8.16 – Uploaded image
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 上传的图片
- en: 'The following screenshot shows the class prediction based on the ML model,
    along with the Integrated Gradients-based explanation generated by Vertex AI.
    The explanation image showcases the key areas/pixels in the image that helped
    the model make the final decision that this is an image of a donut:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了基于机器学习模型的类别预测，以及Vertex AI生成的基于集成梯度的解释。解释图像展示了图像中帮助模型做出最终决策认为这是一张甜甜圈的关键区域/像素：
- en: '![ Figure 8.17 – Resulting Integrated Gradients explanation and predicted class](img/B17792_08_17.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – 结果集成梯度解释和预测类别](img/B17792_08_17.jpg)'
- en: Figure 8.17 – Resulting Integrated Gradients explanation and predicted class
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 结果集成梯度解释和预测类别
- en: 'You can repeat this step with the XRAI endpoint to get explanations using the
    XRAI technique:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用XRAI端点重复此步骤以获取使用XRAI技术生成的解释：
- en: '![Figure 8.18 – XRAI explanation](img/B17792_08_18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18 – XRAI解释](img/B17792_08_18.jpg)'
- en: Figure 8.18 – XRAI explanation
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – XRAI解释
- en: As you can see, with both the explanation images, which were generated through
    the Integrated Gradients technique and XRAI technique, the areas/pixels close
    to the location of donuts in the image are highlighted, and the model seems to
    be focusing on the correct areas.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过集成梯度和XRAI技术生成的解释图像，图像中靠近甜甜圈位置的区域/像素被突出显示，模型似乎专注于正确的区域。
- en: Now let’s look at example-based explanations where instead of explaining the
    results based on the features of the input instance, we instead try to explain
    the results by looking at the examples in the dataset that are similar to the
    input instance.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看基于示例的解释，在这里，我们不是根据输入实例的特征来解释结果，而是尝试通过查看数据集中与输入实例相似的示例来解释结果。
- en: Example-based explanations
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于示例的解释
- en: Vertex AI’s example-based explanation feature uses nearest-neighbor search algorithms
    to find the closest match to a sample. Essentially, when given an input, Vertex
    AI identifies and provides a set of examples, typically originating from the training
    data, that closely resemble the given input. This feature is rooted in the common
    expectation that inputs with similar attributes will lead to corresponding predictions.
    Therefore, these identified examples serve as an intuitive way to comprehend and
    elucidate the workings and decisions of our model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI的基于示例的解释功能使用最近邻搜索算法来找到与样本最接近的匹配项。本质上，当给定一个输入时，Vertex AI会识别并提供一组示例，这些示例通常来自训练数据，并且与给定输入非常相似。这一功能基于一个共同的预期，即具有相似属性的输入将导致相应的预测。因此，这些识别出的示例成为理解并阐明我们模型的工作方式和决策的一种直观方式。
- en: 'This method can be extremely helpful in the following scenarios:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法在以下场景中可能非常有帮助：
- en: '**Identifying mislabeled examples**: If the solution locates data samples or
    embeddings that are close together in the vector space, but have different labels,
    then there is a possibility that the data sample is mislabeled.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别错误标记的示例**：如果解决方案定位到在向量空间中彼此靠近但标签不同的数据样本或嵌入，那么数据样本可能被错误标记。'
- en: '**Decision support**: If the predicted labels for new data points are similar
    to the ground truth labels of other data points appearing close to the new data
    points in the vector space, then that can help confirm the validity of predictions.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策支持**：如果对新数据点的预测标签与在向量空间中出现在新数据点附近的其他数据点的真实标签相似，那么这可以帮助确认预测的有效性。'
- en: '**Active learning**: In the vector space, you can identify the unlabeled samples
    appearing close to the labeled samples and add them to the training data with
    the label of the nearby samples.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动学习**：在向量空间中，您可以识别出现在有标签样本附近的未标记样本，并将它们添加到训练数据中，标签与附近样本的标签相同。'
- en: The functionality of example-based explanations in Vertex AI can be leveraged
    by any model offering an embedding – a latent representation – for its inputs.
    This means that the model should be able to convert the input data into a set
    of relevant features or vectors in a latent space. This excludes certain types
    of models, such as tree-based models such as decision trees, from being supported
    due to their inherent nature of not creating these latent spaces.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI中基于示例的解释功能可以被任何为其输入提供嵌入（潜在表示）的模型利用。这意味着该模型应该能够将输入数据转换为潜在空间中的一组相关特征或向量。这排除了某些类型的模型，例如决策树等基于树的模型，因为它们固有的性质不创建这些潜在空间。
- en: Key steps to use example-based explanations
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基于示例的解释的关键步骤
- en: 'Here are the key steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键步骤：
- en: 'Enable explanations during model creation: Begin by creating a model with explanations
    enabled and uploading it to Vertex AI. When you create/import a model, you can
    set a default configuration for all its explanations using the model’s `explanationSpec`
    field.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型创建时启用解释：首先创建一个已启用解释的模型并将其上传到Vertex AI。当您创建/导入模型时，您可以使用模型的`explanationSpec`字段为所有解释设置默认配置。
- en: 'To facilitate the generation of example-based explanations, certain criteria
    should be met by your model. Two potential scenarios exist for this:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了促进基于示例的解释生成，您的模型应满足某些标准。存在两种潜在的情景：
- en: You can implement a **deep neural network** (**DNN**) model, in which case the
    name of a specific layer or signature should be provided. The output of this layer
    or signature is then utilized as the latent space.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以实现一个**深度神经网络**（**DNN**）模型，在这种情况下，应提供特定层或签名的名称。然后，该层或签名的输出被用作潜在空间。
- en: Alternatively, the model could be designed to directly output embeddings, thus
    serving as a representation of the latent space.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，模型可以被设计为直接输出嵌入，从而作为潜在空间的表示。
- en: This latent space is integral to the process as it houses the example representations
    that are instrumental in generating explanations.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个潜在空间对于过程至关重要，因为它包含了在生成解释中起关键作用的示例表示。
- en: 'Deploy the model to an endpoint: Next, create an endpoint resource and deploy
    your model to it, establishing an accessible channel for interaction.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型部署到端点：接下来，创建一个端点资源并将您的模型部署到该资源，从而建立一个可交互的通道。
- en: 'Explore the generated explanations: Finally, issue explanation requests to
    the deployed model and scrutinize the provided explanations to understand your
    model’s decision-making processes.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索生成的解释：最后，向部署的模型发出解释请求，并仔细审查提供的解释，以了解您的模型决策过程。
- en: Exercise 3
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3
- en: Custom train an image classification model to generate real-time predictions
    and provide example-based explanations – see *Notebook 8.3 – Implementing example-based
    explanations with Vertex* *AI* ([https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb))
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义训练一个图像分类模型以生成实时预测并提供基于示例的解释——参见*笔记8.3 – 使用Vertex AI实现基于示例的解释* ([https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb))
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the world of XAI and its relevance in modern
    MLOps. We discussed how XAI aids in building trust, ensuring regulatory compliance,
    debugging and improving models, and addressing ethical considerations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了可解释人工智能（XAI）的世界及其在现代机器学习运维（MLOps）中的相关性。我们讨论了XAI如何帮助建立信任、确保合规性、调试和改进模型，以及解决伦理问题。
- en: We explored different explanation techniques for various types of data, including
    tabular, image, and text data. Techniques such as LIME, SHAP, permutation feature
    importance, and others were discussed for tabular data. For image data, methods
    such as Integrated Gradients and XRAI were explained, while text-specific LIME
    was presented for text data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了针对不同类型数据的多种解释技术，包括表格数据、图像数据和文本数据。对于表格数据，讨论了LIME、SHAP、排列特征重要性等技巧。对于图像数据，解释了集成梯度和XRAI等方法，而对于文本数据，则展示了针对文本数据的特定LIME。
- en: This chapter also provided an overview of the XAI features available in GCP,
    including both feature-based and example-based explanations.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还概述了GCP中可用的XAI功能，包括基于特征和基于示例的解释。
- en: At this point, you should have gained a good understanding of XAI, its importance,
    various techniques, and practical applications in the context of Vertex AI. As
    the field of AI continues to evolve, the role of XAI in creating transparent,
    trustworthy, and fair ML models will only grow. As MLOps practitioners, having
    these skills will be crucial in leading ethical and responsible AI adoption.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该已经对XAI、其重要性、各种技术和在Vertex AI背景下的实际应用有了很好的理解。随着人工智能领域的不断发展，XAI在创建透明、值得信赖和公平的机器学习模型中的作用将只会增长。作为MLOps从业者，掌握这些技能对于引领道德和负责任的AI采用至关重要。
- en: In the next chapter, we will go over various Vertex AI tools that can help you
    iterate through model hyperparameters to improve the performance of your ML solutions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍各种Vertex AI工具，这些工具可以帮助您迭代模型超参数，以提升您的机器学习解决方案的性能。
- en: References
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)'
- en: Munn, Michael; Pitman, David. *Explainable AI for Practitioners*. O’Reilly Media.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Munn, Michael; Pitman, David. *可解释人工智能实践者*. O’Reilly Media.
