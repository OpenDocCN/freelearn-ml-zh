- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Model Explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving world of **machine learning** (**ML**) and **artificial
    intelligence** (**AI**), developing models capable of delivering accurate predictions
    is no longer the sole objective. As organizations increasingly rely on data-driven
    decision-making, understanding the rationale behind a model’s predictions becomes
    paramount. The growing need for explainability in ML models stems from ethical,
    regulatory, and practical concerns, and it is here that the concept of **Explainable
    AI** (**XAI**) comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter delves into the intricacies of Explainable ML models, a critical
    component in the MLOps landscape, with a focus on their implementation in the
    Google Cloud ecosystem. Although a comprehensive exploration of XAI techniques
    and tools is beyond this chapter’s scope, we aim to equip you with the knowledge
    and skills to build transparent, interpretable, and accountable ML models using
    the Explainable ML tools available on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Explainable AI and why is it important for MLOps practitioners?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of Explainable AI techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainable AI features available in Google Cloud Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercises for using Vertex AI’s explainability features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we journey through this chapter, we will establish the importance of explainability
    and its role in enhancing trust, accountability, and fairness in ML models. Next,
    we will discuss various techniques for achieving explainability in ML, ranging
    from traditional interpretable models to explanation techniques for more complex
    models such as deep learning. We will then dive into Google Cloud’s XAI offerings,
    which facilitate the development and evaluation of Explainable ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to providing an understanding of Explainable ML models, this chapter
    will guide you through practical, hands-on examples, illustrating the application
    of these concepts in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be well-equipped to design, deploy, and
    evaluate Explainable ML models on Google Cloud, ensuring that your organization
    stays ahead in the race toward ethical and responsible AI adoption.
  prefs: []
  type: TYPE_NORMAL
- en: What is Explainable AI and why is it important for MLOps practitioners?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XAI refers to methods and techniques that are used in the domain of AI that
    aim to make the decision-making processes of AI models transparent, interpretable,
    and understandable to humans. Instead of acting as black boxes where input data
    goes in and a decision or prediction comes out without clarity on how the decision
    was reached, XAI seeks to provide insights into the inner workings of models.
    This transparency allows users, developers, and stakeholders to trust and validate
    the system’s decisions, ensuring they align with ethical, legal, and practical
    considerations.
  prefs: []
  type: TYPE_NORMAL
- en: As ML continues to advance and its applications permeate various industries,
    the need for transparent and interpretable models has become a pressing concern.
    XAI aims to address this by developing techniques for understanding, interpreting,
    and explaining ML models. For MLOps practitioners working with Google Cloud, incorporating
    XAI into their workflows can lead to several benefits, including improved trust,
    regulatory compliance, and enhanced model performance.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at the key reasons for the importance of XAI for MLOps practitioners
    and its impact on the development and deployment of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Building trust and confidence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XAI can help MLOps practitioners build trust in their models by providing clear
    and understandable explanations of how these models make decisions. This is particularly
    important when dealing with stakeholders who may not have a technical background
    as the ability to explain model behavior can lead to increased confidence in its
    predictions. Additionally, having a deeper understanding of the inner workings
    of a model allows practitioners to better communicate the limitations and strengths
    of their solutions, which, in turn, can foster trust among collaborators and end
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As ML models become more widely adopted, regulatory bodies around the world
    are increasingly calling for more transparency and accountability in AI systems.
    XAI techniques can help MLOps practitioners ensure compliance with these regulations
    by providing insights into the decision-making process of their models. This can
    be especially important in industries such as healthcare, finance, and human resources,
    where the consequences of biased or unfair decisions can be significant. By incorporating
    XAI into their workflows, practitioners can demonstrate that their models adhere
    to relevant laws and ethical guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: Model debugging and improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XAI can be invaluable to MLOps practitioners during the model development and
    debugging process. By providing insights into how a model is making its predictions,
    XAI can help identify areas where the model may be underperforming, overfitting,
    or exhibiting bias. With this information, practitioners can make targeted adjustments
    to their models, leading to improved performance and more robust solutions. This
    iterative process can save both time and resources, allowing practitioners to
    focus on addressing the most critical issues impacting their models.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the power and influence of ML models grow, so too does the responsibility
    of MLOps practitioners to ensure that these models are used ethically. XAI can
    help practitioners identify and address any unintended consequences or biases
    that may arise from their models. By understanding how a model makes its decisions,
    practitioners can better ensure that their solutions are fair, unbiased, and aligned
    with ethical principles.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating XAI into the MLOps workflow within the Google Cloud ecosystem
    can lead to numerous benefits for practitioners. From building trust with stakeholders
    and ensuring regulatory compliance to improving model performance and addressing
    ethical concerns, the importance of XAI cannot be understated. As the field of
    ML continues to evolve, integrating XAI into MLOps practices will become increasingly
    essential for the development and deployment of transparent, interpretable, and
    responsible AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different techniques are available to cater to various types of data, including
    tabular, image, and text data. Each data type presents its own set of challenges
    and complexities, requiring tailored methods to provide meaningful insights into
    the decision-making processes of ML models. This subsection will list various
    XAI techniques applicable to tabular, image, and text data. The following section
    will dive into the ones available as out-of-the-box features in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Global versus local explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Explainability can be categorized into two categories: local and global explainability.
    These terms are sometimes referred to as local and global feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global explainability** focuses on the overall impact of a feature on the
    model. This is usually obtained by calculating the average feature attribution
    values over the entire dataset. A feature with a high absolute value indicates
    that it significantly influenced the model’s predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local explainability** provides insight into how much each feature contributed
    to the prediction for a specific instance. The feature attribution values give
    information about the effect of a particular feature on the prediction relative
    to a baseline prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for image data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'XAI techniques for image data often focus on visualizing the areas within an
    image that contribute most significantly to the model’s predictions. Here are
    some key techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integrated Gradients**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integrated Gradients is an attribution technique specifically designed for
    deep learning models, such as neural networks. It computes the gradients of the
    model’s output concerning the input features (pixels in the case of image data)
    and integrates these gradients over a straight path from a baseline input to the
    instance of interest. This process assigns an importance value to each pixel in
    the image, reflecting its contribution to the model’s prediction. Integrated Gradients
    provide insights into the importance of each pixel and can help identify potential
    biases or shortcomings in the model’s predictions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Integrated Gradients explanation](img/B17792_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Integrated Gradients explanation
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure showcases the Integrated Gradients approach to image explanation,
    which highlights pixels in the image that the model gives high importance to during
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**eXtended Relevance-weighted Attribution of** **Importance** (**XRAI**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XRAI is an XAI method for visualizing the most important regions in an image
    for a given model’s prediction. It is an extension of the Integrated Gradients
    method, which combines pixel-wise attributions with segmentation techniques to
    generate more coherent and interpretable visualizations. By identifying the most
    important segments within an image, XRAI provides insights into the model’s decision-making
    process and can help identify potential biases or issues in the model’s predictions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – XRAI](img/B17792_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – XRAI
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure showcases the XRAI approach to an image explanation, which
    highlights areas of the image that the model gives high importance during prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-agnostic** **Explanations** (**LIME**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME is an XAI technique that provides local explanations for individual predictions
    of any classifier. In the context of image data, LIME generates synthetic data
    points (perturbed images) around a specific instance, obtains predictions from
    the model, and fits an interpretable model (for example, linear regression) to
    these data points, weighted by their proximity to the instance. The resulting
    model provides insights into the most important regions that contribute to the
    prediction for the specific instance. By visualizing these regions, practitioners
    can better understand the model’s decision-making process and identify potential
    biases or issues in the model’s predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Gradient-weighted Class Activation** **Mapping** (**Grad-CAM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grad-CAM is a visualization technique for deep learning models, specifically
    **convolutional neural networks** (**CNNs**). It generates heatmap-like visualizations
    of the most important regions in an image for a given model’s prediction. Grad-CAM
    computes the gradients of the predicted class score concerning the feature maps
    of the last convolutional layer and then uses these gradients to compute a weighted
    sum of the feature maps. The resulting heatmap highlights the regions in the image
    that contribute most to the model’s prediction. Grad-CAM provides insights into
    the model’s decision-making process and can help identify potential biases or
    shortcomings in the model’s predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These techniques provide insights into the model’s decision-making process and
    can help identify potential biases or shortcomings in the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for tabular data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tabular data, which is composed of structured rows and columns, is one of the
    most common data types encountered in ML. Various XAI techniques can be employed
    to interpret models trained on tabular data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-agnostic** **Explanations** (**LIME**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As its name suggests, LIME is an XAI technique that provides *local* explanations
    for individual predictions of any classifier. It does so by approximating the
    complex model with a simpler, interpretable model (for example, linear regression)
    within the vicinity of a specific instance. LIME generates synthetic data points
    around the instance, obtains predictions from the complex model, and fits an interpretable
    model to these data points, weighted by their proximity to the instance. The resulting
    model provides insights into the most important features contributing to the prediction
    for the specific instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following figure, we’re using a LIME report to explain the decision
    of an ML model trained to predict the survival probabilities of Titanic’s passengers
    based on their attributes, such as their gender, the fare they paid, the passenger
    class they were traveling in, and so on. The Titanic survival dataset is a common
    publically available dataset that’s used as an example for classification models.
    Let’s see if LIME can help us get some insights into the model’s behavior:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Explaining classification models with LIME](img/B17792_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Explaining classification models with LIME
  prefs: []
  type: TYPE_NORMAL
- en: The leftmost chart shows the predicted survival probability for the chosen passenger.
    In this example, the model predicts that this passenger had a 40% probability
    of survival based on their key attributes. The chart in the middle shows us the
    ranked feature importance list generated by using LIME. Based on this chart, it
    seems that the top three most important features that dictated whether a Titanic
    passenger survived or not were their sex/gender, fare paid, and the passenger
    class they were traveling in. This makes sense because we know women were evacuated
    first, giving them a higher overall chance of survival. We also know that passengers
    who paid lower fares and had lower-class tickets had their cabins on the lower
    decks/floors of the ship, which got flooded first and the passengers with higher
    fares had their cabins on the upper decks, giving them a better chance of survival.
    So, you can see how LIME and similar techniques can help decipher black box ML
    models and help us better understand why a particular prediction was made. You
    will also be glad to know that the Titanic passenger we used for the preceding
    example was a 26-year-old lady named Miss. Laina Heikkinen paid $7.925 for a ticket
    in the third-class passenger section and despite our model giving her a less than
    40% chance of survival, she survived.
  prefs: []
  type: TYPE_NORMAL
- en: '**Shapley Additive exPlanations** (**SHAP**) (native support on Vertex AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SHAP is a unified measure of feature importance based on cooperative game theory,
    providing a consistent and fair way to allocate feature importance. By calculating
    the Shapley value for each feature, SHAP assigns an importance value that reflects
    its contribution to the prediction for a specific instance. The Shapley value
    is calculated by averaging the marginal contributions of a feature across all
    possible feature combinations. SHAP values provide insights into the most influential
    features driving a model’s predictions and can be used with a variety of models.
    Recommended model types include non-differentiable models such as ensembles of
    trees. They can also be used for neural networks, where SHAP can provide insights
    into the contribution of each input feature to the final prediction made by the
    network. By analyzing the SHAP values, you can determine which features have the
    most impact on the network’s output and understand the relationship between the
    inputs and outputs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method](img/B17792_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method
  prefs: []
  type: TYPE_NORMAL
- en: '**Permutation** **feature importance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permutation feature importance is a model-agnostic technique that estimates
    the importance of each feature by measuring the change in the model’s performance
    when the feature’s values are randomly shuffled. This process is repeated several
    times, and the average decrease in performance is used as an estimate of the feature’s
    importance. By disrupting the relationship between the feature and the target
    variable, permutation importance helps identify the features that have the greatest
    impact on the model’s predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Partial dependence** **plots** (**PDP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDP is a visualization technique that depicts the relationship between a specific
    feature and the model’s predicted outcome while holding all other features constant.
    By illustrating how a single feature influences the prediction, PDP can help practitioners
    better understand the behavior of their models and identify potential biases or
    inconsistencies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Feature importance** (for example, GINI importance and coefficients in linear
    models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature importance is a set of techniques that quantify the impact of input
    features on the model’s predictions. These methods can help practitioners identify
    the most relevant features, enabling them to focus on the most significant variables
    during model development and debugging. Some common approaches to feature importance
    are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**GINI Importance**: Used in decision trees and random forests, GINI importance
    measures the average reduction in impurity (**GINI index**) attributable to a
    specific feature across all the trees in the forest.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coefficients in linear models**: In linear regression and logistic regression,
    the coefficients of the model can be used as a measure of feature importance,
    indicating the magnitude and direction of the relationship between each feature
    and the target variable. Larger absolute coefficient values indicate a stronger
    relationship between the feature and the target variable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques help practitioners understand the relationships between input
    features and model predictions, identify the most influential features, and assess
    the impact of specific features on individual predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the case of natural language processing models that use text data, the aim
    is to identify the most significant words or phrases that contribute to the model’s
    predictions. Here are some XAI techniques for text data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text-specific LIME**: This is an adaptation of LIME specifically designed
    for text data that provides explanations for individual predictions by highlighting
    the most important words or phrases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we’re using LIME to explain why the ML model we built
    to classify movie reviews as positive or negative arrives at a particular conclusion:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.5 – LIME-based explanation for text classification](img/B17792_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – LIME-based explanation for text classification
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the predicted probability of the movie review being positive
    is 0.77 (77%). The words highlighted in orange contributed toward the positive
    probability, while the words highlighted in blue had a significant contribution
    toward moving the final prediction toward the “negative” label. The chart at the
    top right shows the respective contribution of each of the highlighted words toward
    the final decision. For example, if we remove the word “amazing” from the review
    text, the positive probability will go down by 0.03:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text-specific SHAP**: This is an adaptation of SHAP tailored for text data,
    attributing importance values to individual words or phrases within a given text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention mechanisms**: In deep learning models such as Transformers, attention
    mechanisms can provide insights into the relationships between words and the model’s
    predictions by visualizing the attention weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have familiarized ourselves with the various popular XAI techniques,
    let’s look at the different features available within Google Cloud Vertex AI that
    can help us build XAI solutions using these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI features available in Google Cloud Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Google Cloud Vertex AI** offers a suite of tools and options tailored to
    make AI systems more understandable. This section delves into the various XAI
    options available in Vertex AI, showcasing how this platform is advancing the
    frontier of transparent ML.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly, the XAI options available in Vertex AI can be divided into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature-based**: Feature attributions refer to the degree to which each feature
    in a model contributes to the predictions for a specific instance. When making
    a prediction request, you receive the predicted values that are generated by your
    model. However, when requesting an explanation, you receive not only the predictions
    but also the feature attribution information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that feature attributions are primarily applicable to
    tabular data but also include built-in visualization capabilities for image data.
    This makes it easier to understand and interpret the attributions more intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example-based**: Vertex AI utilizes nearest neighbor search to provide example-based
    explanations. This method involves finding the closest examples (usually from
    the training data) to the input and returning a list of the most similar examples.
    This approach leverages the principle that similar inputs are likely to produce
    similar predictions, allowing us to gain insight into the behavior of our model.
    By examining these similar examples, we can better understand and explain our
    model’s output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based explanation techniques available on Vertex AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table shows different methods of feature-based explanations available
    in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Compatible Vertex AI** **Model Resources** | **Example** **Use
    Cases** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sampled Shapley (SHAP) |'
  prefs: []
  type: TYPE_TB
- en: Any custom-trained model (running in any prediction container)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML tabular models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression on tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Integrated Gradients |'
  prefs: []
  type: TYPE_TB
- en: Custom-trained TensorFlow models using TensorFlow prebuilt containers for prediction
    serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML image models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression on tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| XRAI (eXplanation with Ranked Area Integrals) |'
  prefs: []
  type: TYPE_TB
- en: Custom-trained TensorFlow models using TensorFlow prebuilt containers for prediction
    serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML image models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Classification of image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8.1 – Feature attribution methods available in GCP. Source: [https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn how to use these Vertex AI features to generate explanations
    for model output.
  prefs: []
  type: TYPE_NORMAL
- en: Using the model feature importance (SHAP-based) capability with AutoML for tabular
    data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following exercise, we will learn how to use XAI features in Vertex AI
    to evaluate feature importance in ML models for structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Objective: Use Vertex AutoML Tables’ **Feature importance** feature to explain
    global (model-level) and local (sample-level) behavior'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset to be used: *Bank Marketing Dataset* (Available in the *Chapter-8*
    folder within this book’s GitHub repository)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model objective: Predict whether the customer will open a new term deposit
    or not (**Feature Label –** **deposit(Yes/No)**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Following the steps shown in [*Chapter 5*](B17792_05.xhtml#_idTextAnchor066),
    create an AutoML classification model to predict the probability of a customer
    opening the term deposit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model has been trained, navigate to **Model Registry** | **Your model**
    | **Your model version** (*1 for new models*) | the **Evaluate** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the **Feature** **importance** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following **Feature importance** graph shows the relative feature importance
    of different model features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables](img/B17792_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the previous graph, the most important features in our model to
    predict whether a customer will respond to the outreach and open a new term deposit
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Duration**: How long the customer has been with the bank'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Month**: This could be because of the seasonality of the business'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contact** (Method – Cellular/Landline): This could be because of the different
    communication preferences of different types of customers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outcome**: The outcome of the last promotional outreach to this customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the least important features as per the preceding graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Default**: Has the customer defaulted before?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Marital (Status)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education**: Level of education attained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Previous**: The number of times contact was made before this campaign'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding information can help the data science team better understand the
    model’s behavior and possibly uncover additional insights into their data and
    customer behavior and provide important guidance for future experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Although performing exploratory data analysis on the training dataset is beyond
    the scope of this book, for anyone interested, you can look at the notebook ([*Chapter
    8*](B17792_08.xhtml#_idTextAnchor102) *– ML Model Explainability – Exercise 1
    Addendum*) for the correlation analysis between features and predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some insights we can draw from this feature importance information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Deposit_Signup`), which aligns with the duration being high in terms of feature
    importance in the preceding graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contact**: Similarly, correlation analysis also shows that someone owning
    a cellular phone has a strong correlation with someone opening a deposit in response
    to the campaign.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outcome**: Correlation analysis also shows that “Success” has a strong correlation
    with someone opening a deposit. This means that if someone responded positively
    to the last campaign, there is a stronger chance of that person being influenced
    by the current campaign.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Months**: In correlation analysis, we can also see that some months (March
    and May specifically) have a strong correlation with the positive outcome of the
    campaign. This could relate to AutoML surfacing months as an important feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Objective: Use Vertex AI’s feature attribution features to explain image classification
    predictions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset to be used: `Fast_Food_Classification_Dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create the image classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and unzip the dataset from Kaggle: [https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset](https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the steps shown in [*Chapter 5*](B17792_05.xhtml#_idTextAnchor066),
    create an AutoML image classification dataset using `Fast_Food_Classification_Dataset`.
    Make sure to select data type and objective as **Image classification (Single-label)**,
    as shown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Model objective – Image classification (Single-label)](img/B17792_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Model objective – Image classification (Single-label)
  prefs: []
  type: TYPE_NORMAL
- en: Once the empty dataset is created, go to the **Browse** tab and add new labels
    for each food type you plan to include in your model. In this example, we uploaded
    our favorite fast foods, including burgers, donuts, hot dogs, and pizza, but feel
    free to use whatever food types you want to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Creating label names](img/B17792_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Creating label names
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s upload the images of different fast-food types and annotate/label
    them. You don’t need all the available images in the dataset. Just 50 or so images
    for each food type should suffice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the following steps for each food type:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upload images – Labeling images one at a time is difficult. To make labeling
    the images a bit easier, upload images for one food type at a time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Adding labels to images](img/B17792_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Adding labels to images
  prefs: []
  type: TYPE_NORMAL
- en: As shown, once you have uploaded the images for a particular food item, you
    can click on **Unlabeled** and then **Select All** to select all the images that
    need to be labeled. If you upload and label one food type at a time, it ensures
    that you only select images of one type of food. If you upload all images at once,
    then clicking on the **Unlabeled** tab would end up selecting ALL unlabeled images,
    requiring you to manually select images of one type.
  prefs: []
  type: TYPE_NORMAL
- en: Once the images have been selected, click on **ASSIGN LABELS** and select the
    right food type label. Then click **Save.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Do this process for all different* *food types*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once all images are uploaded and labeled, navigate to the dataset in Vertex
    AI and go to the **Browse** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **TRAIN** **NEW MODEL**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Initiating model training](img/B17792_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Initiating model training
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next screen, select the dataset and annotation set you want to use for
    training the new model. Then, select the following options and click **CONTINUE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Objective: **Image** **classification** (**Single-label**)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Model training method: **AutoML**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose where to use the model: **Cloud**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Training configuration/options](img/B17792_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Training configuration/options
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, select **TRAIN NEW MODEL** and enter the name of the new
    model. You can leave all other options as is and click **CONTINUE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen, select **Default** as the goal and click **CONTINUE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.12– Training configuration/options](img/B17792_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12– Training configuration/options
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen (the **Explainability** tab), check the **Generate Explainable**
    **Bitmaps** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `positive,` emphasizing regions with the highest positive influences.
    Essentially, it illuminates pixels that significantly contribute to a positive
    prediction by the model. Changing the polarity to `negative` will underscore areas,
    persuading the model away from predicting the positive class and aiding in pinpointing
    areas responsible for false negatives. There’s also an option to choose `both,`
    which provides a comprehensive view by displaying both positive and negative attributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pink_green` default, where green signifies positive attributions and pink
    denotes negative ones. On the other hand, XRAI visualizations employ a gradient
    color scheme, with `Viridis` as the default. In this setup, the most impactful
    regions are bathed in yellow, while the less influential areas are shaded in blue.
    For an exhaustive list of available palettes, consult the **Visualization** message
    within the API documentation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Overlay type**: This setting defines how the original image is showcased
    within the visualization. Tweaking the overlay enhances visibility, especially
    when the inherent properties of the initial image obscure the visualization details.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Steps**: The number of steps used to approximate the path integral can be
    specified here. It is recommended to start with a value of 50 and gradually increase
    it until the “sum to diff” property falls within the desired error range. The
    valid range for this value is inclusive between 1 and 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Explainability configurations](img/B17792_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Explainability configurations
  prefs: []
  type: TYPE_NORMAL
- en: In the **Compute and pricing** tab, set the budget to 8 hours. This specifies
    the maximum amount of time the training will run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click **START TRAINING**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (A coffee break would be too short, so maybe go prepare a 7-course meal and
    come back to check on the training status in a few hours!)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the model training is complete, we need to deploy the model to a Vertex
    AI endpoint by following the next steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to **Model Registry** | **Your model** | **Your model version** (*1
    for* *new models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the **Deploy & Test** tab and click **Deploy** **to endpoint**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the name of the endpoint and click **CONTINUE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Model deployment options](img/B17792_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Model deployment options
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Model settings** tab, check **Enable feature attributions for this
    model** and then click the **EDIT** button underneath to open the **Explainability**
    **options** menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Deployment configuration for explainability](img/B17792_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Deployment configuration for explainability
  prefs: []
  type: TYPE_NORMAL
- en: In the **Explainability options** menu, select the **Integrated gradients**
    option since we are first creating an endpoint to test the Integrated Gradients
    technique. Click **DONE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, repeat these steps to create an endpoint for the **XRAI** explainability
    option. This time, suffix the name of the endpoint with XRAI, and on the **Explainability
    options** screen, pick **XRAI**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, two endpoints should have been created for the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s test the model by uploading a sample image of donuts and evaluate
    the prediction and the explanation for the prediction returned by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: In the model’s **Deploy & Test** tab, select the **Integrated Gradients** endpoint
    by clicking on **Endpoint ID**. Do not click on the endpoint’s name as that will
    take you to the endpoint’s settings screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Upload & Explain** and select an image you want to test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Vertex AI will process the image and present the resulting classification of
    the image, along with an explanation (the image overlay will show areas of high
    importance for the image):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Uploaded image](img/B17792_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Uploaded image
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the class prediction based on the ML model,
    along with the Integrated Gradients-based explanation generated by Vertex AI.
    The explanation image showcases the key areas/pixels in the image that helped
    the model make the final decision that this is an image of a donut:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 8.17 – Resulting Integrated Gradients explanation and predicted class](img/B17792_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Resulting Integrated Gradients explanation and predicted class
  prefs: []
  type: TYPE_NORMAL
- en: 'You can repeat this step with the XRAI endpoint to get explanations using the
    XRAI technique:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.18 – XRAI explanation](img/B17792_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – XRAI explanation
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, with both the explanation images, which were generated through
    the Integrated Gradients technique and XRAI technique, the areas/pixels close
    to the location of donuts in the image are highlighted, and the model seems to
    be focusing on the correct areas.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at example-based explanations where instead of explaining the
    results based on the features of the input instance, we instead try to explain
    the results by looking at the examples in the dataset that are similar to the
    input instance.
  prefs: []
  type: TYPE_NORMAL
- en: Example-based explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI’s example-based explanation feature uses nearest-neighbor search algorithms
    to find the closest match to a sample. Essentially, when given an input, Vertex
    AI identifies and provides a set of examples, typically originating from the training
    data, that closely resemble the given input. This feature is rooted in the common
    expectation that inputs with similar attributes will lead to corresponding predictions.
    Therefore, these identified examples serve as an intuitive way to comprehend and
    elucidate the workings and decisions of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method can be extremely helpful in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying mislabeled examples**: If the solution locates data samples or
    embeddings that are close together in the vector space, but have different labels,
    then there is a possibility that the data sample is mislabeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision support**: If the predicted labels for new data points are similar
    to the ground truth labels of other data points appearing close to the new data
    points in the vector space, then that can help confirm the validity of predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active learning**: In the vector space, you can identify the unlabeled samples
    appearing close to the labeled samples and add them to the training data with
    the label of the nearby samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functionality of example-based explanations in Vertex AI can be leveraged
    by any model offering an embedding – a latent representation – for its inputs.
    This means that the model should be able to convert the input data into a set
    of relevant features or vectors in a latent space. This excludes certain types
    of models, such as tree-based models such as decision trees, from being supported
    due to their inherent nature of not creating these latent spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Key steps to use example-based explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable explanations during model creation: Begin by creating a model with explanations
    enabled and uploading it to Vertex AI. When you create/import a model, you can
    set a default configuration for all its explanations using the model’s `explanationSpec`
    field.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To facilitate the generation of example-based explanations, certain criteria
    should be met by your model. Two potential scenarios exist for this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can implement a **deep neural network** (**DNN**) model, in which case the
    name of a specific layer or signature should be provided. The output of this layer
    or signature is then utilized as the latent space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, the model could be designed to directly output embeddings, thus
    serving as a representation of the latent space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This latent space is integral to the process as it houses the example representations
    that are instrumental in generating explanations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Deploy the model to an endpoint: Next, create an endpoint resource and deploy
    your model to it, establishing an accessible channel for interaction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore the generated explanations: Finally, issue explanation requests to
    the deployed model and scrutinize the provided explanations to understand your
    model’s decision-making processes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom train an image classification model to generate real-time predictions
    and provide example-based explanations – see *Notebook 8.3 – Implementing example-based
    explanations with Vertex* *AI* ([https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb))
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the world of XAI and its relevance in modern
    MLOps. We discussed how XAI aids in building trust, ensuring regulatory compliance,
    debugging and improving models, and addressing ethical considerations.
  prefs: []
  type: TYPE_NORMAL
- en: We explored different explanation techniques for various types of data, including
    tabular, image, and text data. Techniques such as LIME, SHAP, permutation feature
    importance, and others were discussed for tabular data. For image data, methods
    such as Integrated Gradients and XRAI were explained, while text-specific LIME
    was presented for text data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also provided an overview of the XAI features available in GCP,
    including both feature-based and example-based explanations.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have gained a good understanding of XAI, its importance,
    various techniques, and practical applications in the context of Vertex AI. As
    the field of AI continues to evolve, the role of XAI in creating transparent,
    trustworthy, and fair ML models will only grow. As MLOps practitioners, having
    these skills will be crucial in leading ethical and responsible AI adoption.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over various Vertex AI tools that can help you
    iterate through model hyperparameters to improve the performance of your ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: Munn, Michael; Pitman, David. *Explainable AI for Practitioners*. O’Reilly Media.
  prefs: []
  type: TYPE_NORMAL
