- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Case Study 2 – Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究2 – 自然语言处理
- en: This chapter introduces you to **natural language processing** (**NLP**), where
    synthetic data is a key player. You will explore various applications of NLP models.
    Additionally, you will learn why these models usually require large-scale training
    datasets to converge and perform well in practice. At the same time, you will
    comprehend why synthetic data is the future of NLP. The discussion will be supported
    by a practical, hands-on example, as well as many interesting case studies from
    research and industry fields.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍**自然语言处理**（**NLP**），其中合成数据是关键角色。您将探索NLP模型的多种应用。此外，您将了解为什么这些模型通常需要大规模训练数据集才能收敛并在实践中表现良好。同时，您将理解为什么合成数据是NLP的未来。讨论将基于一个实际、动手的示例，以及来自研究和行业领域的许多有趣的案例研究。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: A brief introduction to NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP简介
- en: The need for large-scale training datasets in NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP中大规模训练数据集的需求
- en: Hands-on practical example with ChatGPT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ChatGPT的动手实践示例
- en: Synthetic data as a solution for NLP problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合成数据作为NLP问题的解决方案
- en: A brief introduction to NLP
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP简介
- en: 'NLP is an interdisciplinary field that combines computer science, ML, and linguistics.
    It gives computers the ability to understand, analyze, and respond to natural
    language texts, written or spoken. The field of NLP is evolving for many reasons,
    including the availability of big data and powerful computational resources such
    as **Graphic****s** **Processing Units** (**GPUs**) and **Tensor Processing Units**
    (**TPUs**). Examples of state-of-the-art NLP models include *BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding* ([https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)),
    *ChatGPT* ([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)),
    and *Google Bard* ([https://bard.google.com](https://bard.google.com)). Next,
    let’s explore some of the key applications of NLP models in practice.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLP是一个跨学科领域，结合了计算机科学、机器学习和语言学。它使计算机能够理解、分析和响应用户的自然语言文本，无论是书面还是口头。NLP领域正在因许多原因而不断发展，包括大数据的可用性和强大的计算资源，如**图形****处理单元**（**GPUs**）和**张量处理单元**（**TPUs**）。最先进的NLP模型示例包括
    *BERT: 用于语言理解的深度双向变换器预训练* ([https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))、*ChatGPT*
    ([https://openai.com/blog/ChatGPT](https://openai.com/blog/ChatGPT)) 和 *Google
    Bard* ([https://bard.google.com](https://bard.google.com))。接下来，让我们探讨一些NLP模型在实践中的关键应用。'
- en: Applications of NLP in practice
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP在实践中的应用
- en: Some common applications of NLP models are shown in *Figure 11**.1*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLP模型的一些常见应用在 *图11.1* 中展示。 '
- en: '![Figure 11.1 – Samples of key applications of NLP models in practice](img/Figure_11_01_B18494.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – NLP模型在实践中的关键应用样本](img/Figure_11_01_B18494.jpg)'
- en: Figure 11.1 – Samples of key applications of NLP models in practice
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – NLP模型在实践中的关键应用样本
- en: Let’s now discuss some of these applications in more detail.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将更详细地讨论一些这些应用。
- en: Text and speech translation
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本和语音翻译
- en: This is the task of translating text or a speech from one language to another.
    Usually, a large-scale text corpus, composed of a huge number of sentences translated
    from one language to another, is used to train such models. *Google Translate*
    ([https://translate.google.co.uk](https://translate.google.co.uk)), *Microsoft
    Translator* ([https://translator.microsoft.com](https://translator.microsoft.com)),
    and *iTranslate* ([https://itranslate.com](https://itranslate.com)) are all examples
    of generic translation NLP models. There are also domain- or field-specific NLP-based
    translators, such as *Lingua Custodia* ([https://www.linguacustodia.finance](https://www.linguacustodia.finance))
    and *Trados* ([https://www.trados.com](https://www.trados.com)), which are more
    specific to the financial field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将文本或语音从一种语言翻译成另一种语言的任务。通常，一个大规模的文本语料库，由大量从一种语言翻译成另一种语言的句子组成，用于训练此类模型。*Google
    Translate* ([https://translate.google.co.uk](https://translate.google.co.uk))、*Microsoft
    Translator* ([https://translator.microsoft.com](https://translator.microsoft.com))
    和 *iTranslate* ([https://itranslate.com](https://itranslate.com)) 都是通用翻译NLP模型的例子。还有一些特定于领域或行业的基于NLP的翻译器，例如
    *Lingua Custodia* ([https://www.linguacustodia.finance](https://www.linguacustodia.finance))
    和 *Trados* ([https://www.trados.com](https://www.trados.com))，它们更专注于金融领域。
- en: Sentiment analysis
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感分析
- en: This is a major task in the NLP field. It aims at analyzing and classifying
    texts based on the sentiment or emotions embedded in the text. It is usually used
    by companies to understand customer feedback, assess their services, and identify
    issues. For example, it is commonly employed to classify customer reviews on items
    or services as *positive*, *negative*, or *neutral*. Additionally, it is often
    applied to identify emotions in text, such as anger, sadness, dissatisfaction,
    frustration, and happiness. For example, *Medallia Text Analytics* utilizes NLP
    to provide a quick summary of market trends and customer feedback and comments
    on services and products. For more details, please refer to the Medallia Text
    Analytics website ([https://www.medallia.com/resource/text-analytics-solution-brochure](https://www.medallia.com/resource/text-analytics-solution-brochure)).
    Furthermore, for recent examples of using sentiment analysis in practice, please
    refer to *Identification of opinion trends using sentiment analysis of airlines
    passengers’ reviews* ([https://doi.org/10.1016/j.jairtraman.2022.102232](https://doi.org/10.1016/j.jairtraman.2022.102232))
    and *A Novel Approach for Sentiment Analysis and Opinion Mining on Social Media*
    *Tweets* ([https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15](https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是NLP领域的一项主要任务。它的目标是根据文本中嵌入的情感或情绪来分析和分类文本。它通常被公司用来理解客户反馈，评估他们的服务，并识别问题。例如，它通常被用来将商品或服务的客户评论分类为*正面*、*负面*或*中性*。此外，它还常用于识别文本中的情感，如愤怒、悲伤、不满、挫败和快乐。例如，*Medallia文本分析*利用NLP提供市场趋势和客户对服务和产品的反馈及评论的快速总结。更多详情，请参阅Medallia文本分析网站
    ([https://www.medallia.com/resource/text-analytics-solution-brochure](https://www.medallia.com/resource/text-analytics-solution-brochure))。此外，对于使用情感分析的实际案例的最新例子，请参阅*使用航空公司乘客评论的情感分析识别意见趋势*
    ([https://doi.org/10.1016/j.jairtraman.2022.102232](https://doi.org/10.1016/j.jairtraman.2022.102232))
    和 *社交媒体推文的情感分析和意见挖掘的新方法* ([https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15](https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15))。
- en: Text summarization
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本摘要
- en: This is the task of generating a summary of text or a document in a human-like
    way by capturing the essence or the main points. It is a complex process as the
    NLP model needs to learn how to focus on the essential parts of the text, which
    is a context-dependent task. However, NLP models have shown great progress in
    this area recently. There are many examples of NLP models that can summarize large
    blocks of text, such as the *plnia Text Summarization API* ([https://www.plnia.com/products/text-summarization-api](https://www.plnia.com/products/text-summarization-api))
    and *NLP Cloud’s Summarization* *API* ([https://nlpcloud.com/nlp-text-summarization-api.xhtml](https://nlpcloud.com/nlp-text-summarization-api.xhtml)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项以人类方式生成文本或文档摘要的任务，通过捕捉本质或要点。这是一个复杂的过程，因为NLP模型需要学习如何关注文本的精华部分，这是一个依赖于上下文的任务。然而，NLP模型最近在这个领域取得了显著的进步。有许多NLP模型可以总结大量文本的例子，例如*plnia文本摘要API*
    ([https://www.plnia.com/products/text-summarization-api](https://www.plnia.com/products/text-summarization-api))
    和 *NLP Cloud的摘要API* ([https://nlpcloud.com/nlp-text-summarization-api.xhtml](https://nlpcloud.com/nlp-text-summarization-api.xhtml))。
- en: Test-to-scene generation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试到场景生成
- en: 'This is another essential task that relies extensively on NLP. It aims at generating
    virtual scenes given descriptive textual input. It has many interesting applications
    in game development, the metaverse, advertising, and education. One of the main
    advantages of text-to-scene generation is that it allows users to generate diverse
    and photorealistic scenes without requiring a background in computer graphics,
    game development, and programming. Text-to-scene methods are usually based on
    GANs, VAEs, diffusion models, and Transformers. For more information, please refer
    to *Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields* ([https://arxiv.org/pdf/2305.11588.pdf](https://arxiv.org/pdf/2305.11588.pdf))
    and *SceneSeer: 3D Scene Design with Natural* *Language* ([https://arxiv.org/pdf/1703.00050.pdf](https://arxiv.org/pdf/1703.00050.pdf)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项依赖NLP的另一个基本任务。它的目标是根据描述性文本输入生成虚拟场景。它在游戏开发、元宇宙、广告和教育等领域有许多有趣的应用。文本到场景生成的主要优势之一是它允许用户在没有计算机图形、游戏开发和编程背景的情况下生成多样化和逼真的场景。文本到场景方法通常基于GANs、VAEs、扩散模型和Transformers。更多信息，请参阅*Text2NeRF：基于神经辐射场的文本驱动3D场景生成*([https://arxiv.org/pdf/2305.11588.pdf](https://arxiv.org/pdf/2305.11588.pdf))和*SceneSeer：使用自然语言进行3D场景设计*([https://arxiv.org/pdf/1703.00050.pdf](https://arxiv.org/pdf/1703.00050.pdf))。
- en: Text-to-image generation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到图像生成
- en: In this task, NLP models generate images based on textual descriptions provided
    by the user. In this task, the model aims at creating a visual representation
    controlled by the textual input. The task of generating images from text has many
    attractive applications, such as data augmentation, content generation, e-commerce,
    and advertising. You can take *DALL-E2* ([https://openai.com/product/dall-e-2](https://openai.com/product/dall-e-2))
    and *Stable Diffusion* ([https://stablediffusionweb.com](https://stablediffusionweb.com))
    as examples. They can generate photo-realistic images given a descriptive text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，NLP模型根据用户提供的文本描述生成图像。在这个任务中，模型的目标是创建受文本输入控制的视觉表示。从文本生成图像的任务有许多吸引人的应用，如数据增强、内容生成、电子商务和广告。您可以以*DALL-E2*([https://openai.com/product/dall-e-2](https://openai.com/product/dall-e-2))和*Stable
    Diffusion*([https://stablediffusionweb.com](https://stablediffusionweb.com))为例。它们可以根据描述性文本生成逼真的图像。
- en: In the next section, we will learn why we need large-scale datasets to successfully
    train NLP models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解为什么我们需要大规模数据集才能成功训练NLP模型。
- en: The need for large-scale training datasets in NLP
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中大规模训练数据集的需求
- en: NLP models require large-scale training datasets to perform well in practice.
    In this section, you will understand why NLP models need a substantial amount
    of training data to converge.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: NLP模型需要在实践中表现良好，需要大规模的训练数据集。在本节中，您将了解为什么NLP模型需要大量的训练数据才能收敛。
- en: 'ML models in general required a huge number of training samples to cover in
    practice. NLP models require even more training data compared to other ML fields.
    There are many reasons for that. Next, let’s discuss the main ones, which are
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通用机器学习模型在实践中需要大量的训练样本。与其它机器学习领域相比，NLP模型需要更多的训练数据。这有很多原因。接下来，让我们讨论主要原因，如下：
- en: Human language complexity
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类语言复杂性
- en: Contextual dependence
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文依赖
- en: Generalization
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化
- en: Human language complexity
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类语言复杂性
- en: Recent research shows that a huge proportion of our brains is used for language
    understanding. At the same time, it is still a research problem to understand
    how different brain regions communicate with each other while reading, writing,
    or carrying out other language-related activities. For more information, please
    refer to *A review and synthesis of the first 20years of PET and fMRI studies
    of heard speech, spoken language and reading* ([https://doi.org/10.1016/j.neuroimage.2012.04.062](https://doi.org/10.1016/j.neuroimage.2012.04.062)).
    Additionally, infants’ basic speech and vision functionalities are developed by
    8 to 12 months of age. However, it takes them a few years to use verbal or textual
    communication appropriately. Thus, language processing is not only hard for computers
    but also for humans. What makes the problem much harder for machines is the need
    to learn grammar, expressions, and metaphors. Thus, NLP models require substantial
    training data to learn these hidden rules and patterns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，我们大脑中很大一部分用于语言理解。同时，了解不同的脑区在阅读、写作或进行其他与语言相关的活动时如何相互沟通仍然是一个研究问题。更多信息，请参阅*关于听到的言语、口语和阅读的前20年PET和fMRI研究的综述和综合*
    ([https://doi.org/10.1016/j.neuroimage.2012.04.062](https://doi.org/10.1016/j.neuroimage.2012.04.062))。此外，婴儿的基本语音和视觉功能在8到12个月大时就已经形成。然而，他们需要几年时间才能适当地使用口头或书面交流。因此，语言处理不仅对计算机来说是困难的，对人类也是如此。使这个问题对机器来说更加困难的是需要学习语法、表达和隐喻。因此，NLP模型需要大量的训练数据来学习这些隐藏的规则和模式。
- en: Contextual dependence
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文依赖性
- en: While most ML tasks are still context dependent, such as computer vision, the
    contextual dependence is more severe and intense with NLP problems. For example,
    the meaning of a sentence can change from declarative to interrogative based on
    the speaker’s tone of voice. The ordering of the words and the previous and next
    few sentences may also change the meaning and imply different interpretations.
    Even the same words sometimes have different meanings based on the context. For
    example, “light” can be used as a noun to mean “illumination” or an adjective
    to mean “little weight.” Thus, to master these scenarios, the NLP models need
    to be trained on a diverse dataset that includes enough examples to cover these
    contexts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数机器学习（ML）任务仍然是上下文相关的，例如计算机视觉，但在自然语言处理（NLP）问题中，上下文依赖性更为严重和强烈。例如，一个句子的意义可以根据说话者的语调从陈述句变为疑问句。单词的顺序以及前后的几句话也可能改变意义并暗示不同的解释。即使是相同的单词有时也会根据上下文有不同的含义。例如，“light”可以用作名词表示“照明”或用作形容词表示“轻微的重量”。因此，为了掌握这些场景，NLP模型需要在包含足够示例以涵盖这些上下文的多样化数据集上进行训练。
- en: Generalization
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泛化
- en: Another issue that makes the NLP task even harder is the huge number of languages,
    dialects, and varieties. For instance, British English is different from American
    English in some ways. It is not just different in the spelling but also in the
    words, idioms, and expressions used. Even within the same country with the same
    language, people in different cities use different dialects. For NLP models to
    be useful and practical, they should accommodate most of these variations. Thus,
    large-scale training data is usually required to capture these variations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使得自然语言处理（NLP）任务变得更加困难的因素是语言、方言和变体的数量庞大。例如，英国英语在某些方面与美国英语不同。它不仅在拼写上不同，而且在所使用的单词、习语和表达方式上也有所不同。即使在同一国家的同一种语言中，不同城市的人们也会使用不同的方言。为了使NLP模型具有实用性和有效性，它们应该适应这些大多数变化。因此，通常需要大规模的训练数据来捕捉这些变化。
- en: In the next section, we will learn how to generate synthetic data for a practical
    example in NLP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何为NLP的一个实际例子生成合成数据。
- en: Hands-on practical example with ChatGPT
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ChatGPT的动手实践示例
- en: Let’s imagine we want to train an NLP model to identify positive and negative
    reviews of a book about ML. We will use **ChatGPT** ([https://chat.openai.com](https://chat.openai.com))
    to generate a list of positive and negative reviews. Then, we will deploy these
    reviews to augment our real training dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想我们想要训练一个NLP模型来识别关于机器学习（ML）书籍的正面和负面评论。我们将使用**ChatGPT** ([https://chat.openai.com](https://chat.openai.com))来生成正面和负面评论的列表。然后，我们将部署这些评论来增强我们的真实训练数据集。
- en: 'To get 10 positive reviews, we can simply use the following prompt: `Give me
    10 good reviews about a book for Machine Learning`. ChatGPT will generate the
    following reviews:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得10条正面评论，我们可以简单地使用以下提示：`给我10条关于机器学习书籍的好评`。ChatGPT将生成以下评论：
- en: Mastering Machine Learning is an exceptional book for both beginners and seasoned
    professionals. The author’s clear and concise explanations make complex concepts
    easy to grasp, ensuring a solid foundation for anyone diving into the world of
    ML.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《精通机器学习》是一本适合初学者和资深专业人士的杰出书籍。作者的清晰简洁的解释使得复杂的概念变得容易掌握，确保了任何进入机器学习世界的人都能打下坚实的基础。
- en: This book strikes the perfect balance between theory and practical implementation.
    The hands-on exercises and real-world examples provide valuable insights, making
    it an invaluable resource for anyone looking to apply ML in their projects.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书在理论与实践实施之间取得了完美的平衡。动手练习和现实世界案例提供了宝贵的见解，使其成为任何希望在其项目中应用机器学习的人不可或缺的资源。
- en: I appreciate how the author covers a wide range of ML algorithms and techniques,
    going beyond the usual suspects. It’s a well-rounded guide that truly equips readers
    with the knowledge needed to tackle diverse ML challenges.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我赞赏作者涵盖了广泛的机器学习算法和技术，超越了常规的怀疑对象。这是一本全面的指南，真正为读者提供了应对各种机器学习挑战所需的知识。
- en: The author’s writing style is engaging and approachable, making it feel like
    a friendly mentor guiding you through the ML journey. Complex topics are broken
    down into manageable pieces, making the learning process enjoyable.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者的写作风格引人入胜且易于接近，感觉像是一位友好的导师在引导你通过机器学习之旅。复杂的话题被分解成可管理的部分，使学习过程变得愉快。
- en: The book’s emphasis on understanding the math behind ML algorithms sets it apart
    from others. It builds a solid mathematical foundation without overwhelming readers,
    which is crucial for aspiring ML practitioners.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书强调理解机器学习算法背后的数学，使其与其他书籍区别开来。它建立了坚实的数学基础，而没有让读者感到压倒，这对于有志于成为机器学习实践者的人来说至关重要。
- en: The book’s code samples are well-documented and easily reproducible. The practical
    exercises are a fantastic way to reinforce learning and experiment with ML concepts
    in a hands-on manner.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书的代码示例有良好的文档记录，易于复制。实践练习是巩固学习和以动手方式实验机器学习概念的绝佳方式。
- en: One of the highlights of this book is its practical advice on model selection
    and hyperparameter tuning. These insights save time and help avoid common pitfalls
    in ML projects.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书的一个亮点是它关于模型选择和超参数调整的实用建议。这些见解节省了时间，并有助于避免机器学习项目中常见的陷阱。
- en: As someone with a non-technical background, I found this book extremely accessible.
    The author takes the time to explain even the most complex concepts in a manner
    that anyone can understand.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一名非技术背景的人，我发现这本书非常易于理解。作者花时间以任何人都能够理解的方式解释了甚至是最复杂的概念。
- en: The book stays up-to-date with the latest trends and advancements in the field
    of ML. It includes coverage of recent breakthroughs and contemporary applications,
    making it a relevant and forward-looking resource.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书紧跟机器学习领域的最新趋势和进步。它涵盖了最近的突破和当代应用，使其成为一个相关且前瞻的资源。
- en: This book is a true gem for aspiring data scientists and ML enthusiasts. It
    not only teaches ML techniques but also imparts a problem-solving mindset, empowering
    readers to tackle real-world data challenges with confidence.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书是渴望成为数据科学家和机器学习爱好者的真正瑰宝。它不仅教授机器学习技术，还传授解决问题的思维方式，使读者能够自信地应对现实世界的数据挑战。
- en: 'Similarly, to get negative reviews, we may use the `Give me 10 negative reviews
    about a book for Machine Learning` prompt. We get the following negative reviews:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了获得负面评价，我们可能使用“给我10条关于机器学习书籍的负面评价”的提示。我们得到了以下负面评价：
- en: This book was a complete disappointment. The explanations were convoluted, and
    the author seemed more interested in showcasing their knowledge than helping readers
    understand the concepts. I felt lost throughout the entire reading experience.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书让我大失所望。解释复杂难懂，作者似乎更感兴趣于展示他们的知识，而不是帮助读者理解概念。在整个阅读过程中，我都感到迷茫。
- en: The book claims to be for beginners, but it assumes a level of prior knowledge
    that I just didn’t have. The lack of clear explanations and practical examples
    made it nearly impossible for me to follow along.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书声称是为初学者而写，但它假设了我在先前的知识水平，而这正是我所缺乏的。缺乏清晰的解释和实践示例，使我几乎无法跟上。
- en: I found numerous errors in the code samples provided. It was frustrating to
    spend hours trying to debug the examples when I was supposed to be learning. Sloppy
    editing and poor code quality were major drawbacks.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在提供的代码示例中发现了许多错误。当我本应学习时，花费数小时尝试调试示例令人沮丧。粗心的编辑和糟糕的代码质量是主要的缺点。
- en: The book’s content felt outdated, given the rapidly evolving field of ML. I
    expected to learn about cutting-edge techniques, but instead, I got a rehash of
    old material that’s easily available online.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于机器学习领域的快速发展，这本书的内容感觉已经过时。我原本期待学习前沿技术，但结果却只是重复了网上容易找到的旧材料。
- en: The book had a promising start, but it quickly became overly technical and hard
    to digest. The author assumed readers had a PhD in mathematics, making it inaccessible
    to anyone without an advanced background.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书开头很有希望，但很快变得过于技术化，难以消化。作者似乎假设读者拥有数学博士学位，这使得没有高级背景的人难以理解。
- en: I was excited to see practical exercises, but the ones in this book were poorly
    explained and poorly executed. The lack of step-by-step guidance made it impossible
    to apply the concepts effectively.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我本期待看到实用的练习，但本书中的练习解释和执行都做得不好。缺乏逐步指导使得无法有效地应用这些概念。
- en: The book’s organization was chaotic, jumping between topics without any clear
    progression. It left me feeling confused and struggling to connect the dots between
    chapters.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书的组织结构混乱，在主题之间跳跃，没有任何清晰的进展。这让我感到困惑，难以在章节之间建立联系。
- en: The examples provided were unrealistic and didn’t reflect real-world scenarios.
    I had trouble relating the concepts to practical applications, which defeated
    the purpose of learning ML in the first place.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的例子不切实际，并不能反映现实世界的场景。我很难将概念与实际应用联系起来，这违背了最初学习机器学习的目的。
- en: The book promised to be comprehensive, but it lacked depth in crucial areas
    like neural networks and deep learning. I was hoping for a well-rounded understanding,
    but instead, I found huge gaps in the content.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书承诺将提供全面的内容，但在关键领域如神经网络和深度学习方面却缺乏深度。我原本希望获得全面的理解，但结果却发现内容中存在巨大的空白。
- en: The writing style was dry and monotonous, making it difficult to maintain interest
    in the subject matter. I found myself dozing off during what should have been
    exciting topics in the world of Machine Learning.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写作风格枯燥乏味，使得读者难以保持对主题的兴趣。我在本应令人兴奋的机器学习世界中的主题上打起了瞌睡。
- en: These positive and negative synthetic reviews can be used to augment our training
    dataset. As you can see, it is straightforward to generate an unlimited number
    of training samples to augment and complement our training data. The generated
    samples are diverse, relevant, accurate, detailed, and structured. Thus, including
    such samples in our training dataset can improve the performance of our NLP model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些正面和负面的合成评论可以用来增强我们的训练数据集。正如您所看到的，生成无限数量的训练样本来增强和补充我们的训练数据非常简单。生成的样本多样化、相关、准确、详细且结构化。因此，将这些样本包含在我们的训练数据集中可以提高我们的NLP模型性能。
- en: Next, let’s delve into three practical examples of using NLP models in industry.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨在工业界使用NLP模型的三个实际例子。
- en: Synthetic data as a solution for NLP problems
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合成数据作为解决NLP问题的方案
- en: 'In this section, you will understand how companies are leveraging synthetic
    data as a solution for their NLP-based problems. We will look at four case studies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解公司如何利用合成数据作为解决其基于自然语言处理（NLP）问题的方案。我们将探讨四个案例研究：
- en: SYSTRAN Soft’s use of synthetic data
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SYSTRAN Soft的合成数据应用
- en: Telefónica’s use of synthetic data
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telefónica的合成数据应用
- en: Clinical text mining utilizing synthetic data
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用合成数据进行临床文本挖掘
- en: The Alexa virtual assistant model
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊虚拟助手模型
- en: SYSTRAN Soft’s use of synthetic data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SYSTRAN Soft的合成数据应用
- en: '**Neural Machine Translation** (**NMT**) is a promising approach in NLP. It
    utilizes neural networks to learn statistical models and thus perform the translation
    task. The typical architecture is composed of an encoder-decoder, which is usually
    trained on large-scale training datasets. These models were shown to achieve excellent
    results in practice. However, they also have some limitations, as we will see
    with the SYSTRAN case study.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经机器翻译（NMT**）是NLP中一个有前景的方法。它利用神经网络来学习统计模型，从而执行翻译任务。典型的架构由编码器-解码器组成，通常在大规模训练数据集上训练。这些模型在实践中被证明取得了优异的结果。然而，它们也有一些局限性，正如我们在SYSTRAN案例研究中将看到的。'
- en: 'SYSTRAN is one of the few pioneering companies in the field of machine translation
    technology ([https://www.systransoft.com](https://www.systransoft.com)). While
    their standard and traditional NLP models achieved state-of-the-art results, they
    struggled under two main scenarios: translating long sentences and translating
    short titles, such as titles of news articles. To solve these issues, they explored
    augmenting their real training data with synthetic data specially generated for
    that aim. They were able to solve these issues and boost the overall performance.
    For more information, please refer to *SYSTRAN’s Pure Neural Machine Translation*
    *Systems* ([https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf](https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SYSTRAN是机器翻译技术领域的少数先驱公司之一([https://www.systransoft.com](https://www.systransoft.com))。虽然他们的标准和传统NLP模型实现了最先进的结果，但在两种主要场景下遇到了挑战：翻译长句和翻译短标题，例如新闻文章的标题。为了解决这些问题，他们探索了通过为该目的特别生成的合成数据来增强他们的真实训练数据。他们能够解决这些问题并提高整体性能。更多信息，请参阅*SYSTRAN纯神经机器翻译*系统([https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf](https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf))。
- en: Telefónica’s use of synthetic data
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泰尔斐卡公司对合成数据的利用
- en: In telecommunication industries, it is essential to collect data about customers
    to analyze their needs, identify issues, and customize the provided services.
    This helps these companies to establish a stronger reputation and thus be more
    successful in the market. The issue is usually not data availability but the regulations
    that limit utilizing customers’ data to train NLP or ML models in general.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在电信行业，收集有关客户的数据以分析他们的需求、识别问题并定制提供的服务是至关重要的。这有助于这些公司建立更强的声誉，从而在市场上更加成功。问题通常不是数据可用性，而是限制利用客户数据来训练NLP或ML模型的一般规定。
- en: Telefónica deployed an elegant solution to address these issues. They used the
    *MOSTLY AI* synthetic data platform to synthesize a new dataset from the original
    customer dataset *Telefónica’s CRM Datamart*. The newly generated synthetic data
    now meets the requirements of GDPR as it does not contain any real information
    about customers. At the same time, the synthetic dataset has patterns, correlations,
    and statistical properties that can be seen in the original real dataset. Thus,
    it can be used as a replica of the real dataset to train NLP models. This allowed
    the company to use up to 85% of the customer data, which was not possible with
    real data-based NLP models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 泰尔斐卡公司部署了一个优雅的解决方案来解决这些问题。他们使用了*MOSTLY AI*合成数据平台，从原始客户数据集*泰尔斐卡CRM数据集市*中合成一个新的数据集。新生成的合成数据现在符合GDPR的要求，因为它不包含任何关于客户的真实信息。同时，合成数据集具有与原始真实数据集中可见的模式、相关性和统计特性。因此，它可以作为真实数据集的副本来训练NLP模型。这使得公司能够使用高达85%的客户数据，这在基于真实数据的NLP模型中是不可能的。
- en: Clinical text mining utilizing synthetic data
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用合成数据进行临床文本挖掘
- en: A recent study conducted by researchers at Rice University and Texas A&M University,
    as well as other collaborators, investigated the usability of synthetic data generation
    models such as ChatGPT on clinical text mining. Their aim was to use **Large Language
    Models** (**LLMs**) to help with clinical texting mining. They deployed LLMs to
    recognize biological named entities from unstructured healthcare textual data.
    They interestingly found that using ChatGPT, which was directly trained on real
    data for this task, did not achieve a satisfactory performance. Developing a synthetic
    data generation pipeline and generating the necessary synthetic data dramatically
    improved the performance of their models. The F1-score increased from 23.37% to
    63.99%, which is a significant increase. Additionally, they highlighted that their
    synthetic-data-based model now better addresses and mitigates privacy concerns
    compared to the real-data-based one. For more information, please refer to *Does
    Synthetic Data Generation of LLMs Help Clinical Text* *Mining?* ([https://arxiv.org/pdf/2303.04360.pdf](https://arxiv.org/pdf/2303.04360.pdf)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，莱斯大学和德克萨斯A&M大学的研究人员以及其他合作者进行的一项研究，调查了合成数据生成模型（如ChatGPT）在临床文本挖掘中的可用性。他们的目标是利用**大型语言模型**（**LLMs**）来帮助进行临床文本挖掘。他们将LLMs部署以从非结构化的医疗健康文本数据中识别生物命名实体。他们有趣地发现，直接针对这项任务在真实数据上训练的ChatGPT并没有达到令人满意的表现。开发合成数据生成管道并生成必要的合成数据显著提高了他们模型的表现。F1分数从23.37%提高到63.99%，这是一个显著的增长。此外，他们强调，与基于真实数据的模型相比，他们基于合成数据的模型现在更好地解决并减轻了隐私问题。更多信息，请参阅*LLMs的合成数据生成对临床文本挖掘有何帮助*（[https://arxiv.org/pdf/2303.04360.pdf](https://arxiv.org/pdf/2303.04360.pdf)）。
- en: The Alexa virtual assistant model
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Alexa虚拟助手模型
- en: Virtual assistant models, such as Alexa by Amazon, Siri by Apple, and Google
    Assistant by Google, are becoming an integral part of our modern lives. They provide
    enormous services, such as ordering products, controlling home appliances, and
    voice searching. For these tools to become beneficial for a wider audience, they
    need to support many languages and dialects, which requires large-scale training
    datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟助手模型，如亚马逊的Alexa、苹果的Siri和谷歌的Google Assistant，正成为我们现代生活的重要组成部分。它们提供了巨大的服务，例如订购产品、控制家用电器和语音搜索。为了使这些工具对更广泛的受众有益，它们需要支持许多语言和方言，这需要大规模的训练数据集。
- en: One of the main issues the Alexa virtual assistant encountered when Amazon launched
    the model for three new languages, Hindi, US Spanish, and Brazilian Portuguese,
    was the scarcity of real training data. As a solution, Amazon leveraged the available
    limited real data to create “templates.” Then, they deployed these templates to
    generate synthetic data that augmented and complemented the real data. For example,
    they utilized the available real data in these languages to learn the essential
    grammar and syntax of the languages. Then, they leveraged the trained models to
    generate a sufficiently large synthetic training dataset, which consisted of novel
    sentences following the grammar and syntax of these languages. This elegant synthetic-data-based
    solution helped Amazon to mitigate real data insufficiency and thus helped the
    company to provide more accurate virtual assistants for a broader audience with
    even better performance. Consequently, Amazon successfully got more orders and
    higher profitability. For more information, please refer to *Tools for generating
    synthetic data helped bootstrap Alexa’s new-language* *releases* ([https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases](https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊在推出针对三种新语言（印地语、美国西班牙语和巴西葡萄牙语）的模型时，Alexa虚拟助手遇到的主要问题之一是真实训练数据的稀缺。作为解决方案，亚马逊利用可用的有限真实数据创建“模板”。然后，他们将模板部署以生成补充和增强真实数据的合成数据。例如，他们利用这些语言中可用的真实数据来学习这些语言的必要语法和句法。然后，他们利用训练好的模型生成一个足够大的合成训练数据集，该数据集包含遵循这些语言语法和句法的创新句子。这个基于合成数据的优雅解决方案帮助亚马逊减轻了真实数据不足的问题，从而帮助公司为更广泛的受众提供更准确的虚拟助手，并具有更好的性能。因此，亚马逊成功获得了更多订单和更高的盈利能力。更多信息，请参阅*生成合成数据的工具帮助Alexa的新语言发布*（[https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases](https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases)）。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced NLP models and explored the main applications
    of these models in practice. Additionally, we learned that NLP models require
    large-scale datasets. Then, we thoroughly discussed the main reasons for that.
    Following this, we studied a few examples from industry and research where synthetic
    data was successfully deployed. In the next chapter, we will delve into another
    set of interesting case studies where synthetic data has been successfully deployed
    in the predictive analytics field.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了自然语言处理（NLP）模型，并探讨了这些模型在实际应用中的主要用途。此外，我们还了解到NLP模型需要大规模数据集。然后，我们详细讨论了这一需求的主要原因。在此之后，我们研究了几个行业和研究领域的例子，其中合成数据被成功部署。在下一章中，我们将深入探讨另一组有趣的案例研究，其中合成数据在预测分析领域得到了成功应用。
