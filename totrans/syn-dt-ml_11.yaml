- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Case Study 2 – Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces you to **natural language processing** (**NLP**), where
    synthetic data is a key player. You will explore various applications of NLP models.
    Additionally, you will learn why these models usually require large-scale training
    datasets to converge and perform well in practice. At the same time, you will
    comprehend why synthetic data is the future of NLP. The discussion will be supported
    by a practical, hands-on example, as well as many interesting case studies from
    research and industry fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for large-scale training datasets in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on practical example with ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data as a solution for NLP problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP is an interdisciplinary field that combines computer science, ML, and linguistics.
    It gives computers the ability to understand, analyze, and respond to natural
    language texts, written or spoken. The field of NLP is evolving for many reasons,
    including the availability of big data and powerful computational resources such
    as **Graphic****s** **Processing Units** (**GPUs**) and **Tensor Processing Units**
    (**TPUs**). Examples of state-of-the-art NLP models include *BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding* ([https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)),
    *ChatGPT* ([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)),
    and *Google Bard* ([https://bard.google.com](https://bard.google.com)). Next,
    let’s explore some of the key applications of NLP models in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of NLP in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some common applications of NLP models are shown in *Figure 11**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Samples of key applications of NLP models in practice](img/Figure_11_01_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Samples of key applications of NLP models in practice
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss some of these applications in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text and speech translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the task of translating text or a speech from one language to another.
    Usually, a large-scale text corpus, composed of a huge number of sentences translated
    from one language to another, is used to train such models. *Google Translate*
    ([https://translate.google.co.uk](https://translate.google.co.uk)), *Microsoft
    Translator* ([https://translator.microsoft.com](https://translator.microsoft.com)),
    and *iTranslate* ([https://itranslate.com](https://itranslate.com)) are all examples
    of generic translation NLP models. There are also domain- or field-specific NLP-based
    translators, such as *Lingua Custodia* ([https://www.linguacustodia.finance](https://www.linguacustodia.finance))
    and *Trados* ([https://www.trados.com](https://www.trados.com)), which are more
    specific to the financial field.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a major task in the NLP field. It aims at analyzing and classifying
    texts based on the sentiment or emotions embedded in the text. It is usually used
    by companies to understand customer feedback, assess their services, and identify
    issues. For example, it is commonly employed to classify customer reviews on items
    or services as *positive*, *negative*, or *neutral*. Additionally, it is often
    applied to identify emotions in text, such as anger, sadness, dissatisfaction,
    frustration, and happiness. For example, *Medallia Text Analytics* utilizes NLP
    to provide a quick summary of market trends and customer feedback and comments
    on services and products. For more details, please refer to the Medallia Text
    Analytics website ([https://www.medallia.com/resource/text-analytics-solution-brochure](https://www.medallia.com/resource/text-analytics-solution-brochure)).
    Furthermore, for recent examples of using sentiment analysis in practice, please
    refer to *Identification of opinion trends using sentiment analysis of airlines
    passengers’ reviews* ([https://doi.org/10.1016/j.jairtraman.2022.102232](https://doi.org/10.1016/j.jairtraman.2022.102232))
    and *A Novel Approach for Sentiment Analysis and Opinion Mining on Social Media*
    *Tweets* ([https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15](https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15)).
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the task of generating a summary of text or a document in a human-like
    way by capturing the essence or the main points. It is a complex process as the
    NLP model needs to learn how to focus on the essential parts of the text, which
    is a context-dependent task. However, NLP models have shown great progress in
    this area recently. There are many examples of NLP models that can summarize large
    blocks of text, such as the *plnia Text Summarization API* ([https://www.plnia.com/products/text-summarization-api](https://www.plnia.com/products/text-summarization-api))
    and *NLP Cloud’s Summarization* *API* ([https://nlpcloud.com/nlp-text-summarization-api.xhtml](https://nlpcloud.com/nlp-text-summarization-api.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: Test-to-scene generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is another essential task that relies extensively on NLP. It aims at generating
    virtual scenes given descriptive textual input. It has many interesting applications
    in game development, the metaverse, advertising, and education. One of the main
    advantages of text-to-scene generation is that it allows users to generate diverse
    and photorealistic scenes without requiring a background in computer graphics,
    game development, and programming. Text-to-scene methods are usually based on
    GANs, VAEs, diffusion models, and Transformers. For more information, please refer
    to *Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields* ([https://arxiv.org/pdf/2305.11588.pdf](https://arxiv.org/pdf/2305.11588.pdf))
    and *SceneSeer: 3D Scene Design with Natural* *Language* ([https://arxiv.org/pdf/1703.00050.pdf](https://arxiv.org/pdf/1703.00050.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this task, NLP models generate images based on textual descriptions provided
    by the user. In this task, the model aims at creating a visual representation
    controlled by the textual input. The task of generating images from text has many
    attractive applications, such as data augmentation, content generation, e-commerce,
    and advertising. You can take *DALL-E2* ([https://openai.com/product/dall-e-2](https://openai.com/product/dall-e-2))
    and *Stable Diffusion* ([https://stablediffusionweb.com](https://stablediffusionweb.com))
    as examples. They can generate photo-realistic images given a descriptive text.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn why we need large-scale datasets to successfully
    train NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: The need for large-scale training datasets in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP models require large-scale training datasets to perform well in practice.
    In this section, you will understand why NLP models need a substantial amount
    of training data to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML models in general required a huge number of training samples to cover in
    practice. NLP models require even more training data compared to other ML fields.
    There are many reasons for that. Next, let’s discuss the main ones, which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Human language complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual dependence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human language complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent research shows that a huge proportion of our brains is used for language
    understanding. At the same time, it is still a research problem to understand
    how different brain regions communicate with each other while reading, writing,
    or carrying out other language-related activities. For more information, please
    refer to *A review and synthesis of the first 20years of PET and fMRI studies
    of heard speech, spoken language and reading* ([https://doi.org/10.1016/j.neuroimage.2012.04.062](https://doi.org/10.1016/j.neuroimage.2012.04.062)).
    Additionally, infants’ basic speech and vision functionalities are developed by
    8 to 12 months of age. However, it takes them a few years to use verbal or textual
    communication appropriately. Thus, language processing is not only hard for computers
    but also for humans. What makes the problem much harder for machines is the need
    to learn grammar, expressions, and metaphors. Thus, NLP models require substantial
    training data to learn these hidden rules and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual dependence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While most ML tasks are still context dependent, such as computer vision, the
    contextual dependence is more severe and intense with NLP problems. For example,
    the meaning of a sentence can change from declarative to interrogative based on
    the speaker’s tone of voice. The ordering of the words and the previous and next
    few sentences may also change the meaning and imply different interpretations.
    Even the same words sometimes have different meanings based on the context. For
    example, “light” can be used as a noun to mean “illumination” or an adjective
    to mean “little weight.” Thus, to master these scenarios, the NLP models need
    to be trained on a diverse dataset that includes enough examples to cover these
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another issue that makes the NLP task even harder is the huge number of languages,
    dialects, and varieties. For instance, British English is different from American
    English in some ways. It is not just different in the spelling but also in the
    words, idioms, and expressions used. Even within the same country with the same
    language, people in different cities use different dialects. For NLP models to
    be useful and practical, they should accommodate most of these variations. Thus,
    large-scale training data is usually required to capture these variations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to generate synthetic data for a practical
    example in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on practical example with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s imagine we want to train an NLP model to identify positive and negative
    reviews of a book about ML. We will use **ChatGPT** ([https://chat.openai.com](https://chat.openai.com))
    to generate a list of positive and negative reviews. Then, we will deploy these
    reviews to augment our real training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get 10 positive reviews, we can simply use the following prompt: `Give me
    10 good reviews about a book for Machine Learning`. ChatGPT will generate the
    following reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: Mastering Machine Learning is an exceptional book for both beginners and seasoned
    professionals. The author’s clear and concise explanations make complex concepts
    easy to grasp, ensuring a solid foundation for anyone diving into the world of
    ML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book strikes the perfect balance between theory and practical implementation.
    The hands-on exercises and real-world examples provide valuable insights, making
    it an invaluable resource for anyone looking to apply ML in their projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I appreciate how the author covers a wide range of ML algorithms and techniques,
    going beyond the usual suspects. It’s a well-rounded guide that truly equips readers
    with the knowledge needed to tackle diverse ML challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The author’s writing style is engaging and approachable, making it feel like
    a friendly mentor guiding you through the ML journey. Complex topics are broken
    down into manageable pieces, making the learning process enjoyable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book’s emphasis on understanding the math behind ML algorithms sets it apart
    from others. It builds a solid mathematical foundation without overwhelming readers,
    which is crucial for aspiring ML practitioners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book’s code samples are well-documented and easily reproducible. The practical
    exercises are a fantastic way to reinforce learning and experiment with ML concepts
    in a hands-on manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the highlights of this book is its practical advice on model selection
    and hyperparameter tuning. These insights save time and help avoid common pitfalls
    in ML projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As someone with a non-technical background, I found this book extremely accessible.
    The author takes the time to explain even the most complex concepts in a manner
    that anyone can understand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book stays up-to-date with the latest trends and advancements in the field
    of ML. It includes coverage of recent breakthroughs and contemporary applications,
    making it a relevant and forward-looking resource.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book is a true gem for aspiring data scientists and ML enthusiasts. It
    not only teaches ML techniques but also imparts a problem-solving mindset, empowering
    readers to tackle real-world data challenges with confidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, to get negative reviews, we may use the `Give me 10 negative reviews
    about a book for Machine Learning` prompt. We get the following negative reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: This book was a complete disappointment. The explanations were convoluted, and
    the author seemed more interested in showcasing their knowledge than helping readers
    understand the concepts. I felt lost throughout the entire reading experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book claims to be for beginners, but it assumes a level of prior knowledge
    that I just didn’t have. The lack of clear explanations and practical examples
    made it nearly impossible for me to follow along.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I found numerous errors in the code samples provided. It was frustrating to
    spend hours trying to debug the examples when I was supposed to be learning. Sloppy
    editing and poor code quality were major drawbacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book’s content felt outdated, given the rapidly evolving field of ML. I
    expected to learn about cutting-edge techniques, but instead, I got a rehash of
    old material that’s easily available online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book had a promising start, but it quickly became overly technical and hard
    to digest. The author assumed readers had a PhD in mathematics, making it inaccessible
    to anyone without an advanced background.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I was excited to see practical exercises, but the ones in this book were poorly
    explained and poorly executed. The lack of step-by-step guidance made it impossible
    to apply the concepts effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book’s organization was chaotic, jumping between topics without any clear
    progression. It left me feeling confused and struggling to connect the dots between
    chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The examples provided were unrealistic and didn’t reflect real-world scenarios.
    I had trouble relating the concepts to practical applications, which defeated
    the purpose of learning ML in the first place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book promised to be comprehensive, but it lacked depth in crucial areas
    like neural networks and deep learning. I was hoping for a well-rounded understanding,
    but instead, I found huge gaps in the content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The writing style was dry and monotonous, making it difficult to maintain interest
    in the subject matter. I found myself dozing off during what should have been
    exciting topics in the world of Machine Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These positive and negative synthetic reviews can be used to augment our training
    dataset. As you can see, it is straightforward to generate an unlimited number
    of training samples to augment and complement our training data. The generated
    samples are diverse, relevant, accurate, detailed, and structured. Thus, including
    such samples in our training dataset can improve the performance of our NLP model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s delve into three practical examples of using NLP models in industry.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data as a solution for NLP problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will understand how companies are leveraging synthetic
    data as a solution for their NLP-based problems. We will look at four case studies:'
  prefs: []
  type: TYPE_NORMAL
- en: SYSTRAN Soft’s use of synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telefónica’s use of synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clinical text mining utilizing synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Alexa virtual assistant model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SYSTRAN Soft’s use of synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Neural Machine Translation** (**NMT**) is a promising approach in NLP. It
    utilizes neural networks to learn statistical models and thus perform the translation
    task. The typical architecture is composed of an encoder-decoder, which is usually
    trained on large-scale training datasets. These models were shown to achieve excellent
    results in practice. However, they also have some limitations, as we will see
    with the SYSTRAN case study.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SYSTRAN is one of the few pioneering companies in the field of machine translation
    technology ([https://www.systransoft.com](https://www.systransoft.com)). While
    their standard and traditional NLP models achieved state-of-the-art results, they
    struggled under two main scenarios: translating long sentences and translating
    short titles, such as titles of news articles. To solve these issues, they explored
    augmenting their real training data with synthetic data specially generated for
    that aim. They were able to solve these issues and boost the overall performance.
    For more information, please refer to *SYSTRAN’s Pure Neural Machine Translation*
    *Systems* ([https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf](https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Telefónica’s use of synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In telecommunication industries, it is essential to collect data about customers
    to analyze their needs, identify issues, and customize the provided services.
    This helps these companies to establish a stronger reputation and thus be more
    successful in the market. The issue is usually not data availability but the regulations
    that limit utilizing customers’ data to train NLP or ML models in general.
  prefs: []
  type: TYPE_NORMAL
- en: Telefónica deployed an elegant solution to address these issues. They used the
    *MOSTLY AI* synthetic data platform to synthesize a new dataset from the original
    customer dataset *Telefónica’s CRM Datamart*. The newly generated synthetic data
    now meets the requirements of GDPR as it does not contain any real information
    about customers. At the same time, the synthetic dataset has patterns, correlations,
    and statistical properties that can be seen in the original real dataset. Thus,
    it can be used as a replica of the real dataset to train NLP models. This allowed
    the company to use up to 85% of the customer data, which was not possible with
    real data-based NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Clinical text mining utilizing synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A recent study conducted by researchers at Rice University and Texas A&M University,
    as well as other collaborators, investigated the usability of synthetic data generation
    models such as ChatGPT on clinical text mining. Their aim was to use **Large Language
    Models** (**LLMs**) to help with clinical texting mining. They deployed LLMs to
    recognize biological named entities from unstructured healthcare textual data.
    They interestingly found that using ChatGPT, which was directly trained on real
    data for this task, did not achieve a satisfactory performance. Developing a synthetic
    data generation pipeline and generating the necessary synthetic data dramatically
    improved the performance of their models. The F1-score increased from 23.37% to
    63.99%, which is a significant increase. Additionally, they highlighted that their
    synthetic-data-based model now better addresses and mitigates privacy concerns
    compared to the real-data-based one. For more information, please refer to *Does
    Synthetic Data Generation of LLMs Help Clinical Text* *Mining?* ([https://arxiv.org/pdf/2303.04360.pdf](https://arxiv.org/pdf/2303.04360.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: The Alexa virtual assistant model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual assistant models, such as Alexa by Amazon, Siri by Apple, and Google
    Assistant by Google, are becoming an integral part of our modern lives. They provide
    enormous services, such as ordering products, controlling home appliances, and
    voice searching. For these tools to become beneficial for a wider audience, they
    need to support many languages and dialects, which requires large-scale training
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main issues the Alexa virtual assistant encountered when Amazon launched
    the model for three new languages, Hindi, US Spanish, and Brazilian Portuguese,
    was the scarcity of real training data. As a solution, Amazon leveraged the available
    limited real data to create “templates.” Then, they deployed these templates to
    generate synthetic data that augmented and complemented the real data. For example,
    they utilized the available real data in these languages to learn the essential
    grammar and syntax of the languages. Then, they leveraged the trained models to
    generate a sufficiently large synthetic training dataset, which consisted of novel
    sentences following the grammar and syntax of these languages. This elegant synthetic-data-based
    solution helped Amazon to mitigate real data insufficiency and thus helped the
    company to provide more accurate virtual assistants for a broader audience with
    even better performance. Consequently, Amazon successfully got more orders and
    higher profitability. For more information, please refer to *Tools for generating
    synthetic data helped bootstrap Alexa’s new-language* *releases* ([https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases](https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced NLP models and explored the main applications
    of these models in practice. Additionally, we learned that NLP models require
    large-scale datasets. Then, we thoroughly discussed the main reasons for that.
    Following this, we studied a few examples from industry and research where synthetic
    data was successfully deployed. In the next chapter, we will delve into another
    set of interesting case studies where synthetic data has been successfully deployed
    in the predictive analytics field.
  prefs: []
  type: TYPE_NORMAL
