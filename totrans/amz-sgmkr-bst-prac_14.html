<html><head></head><body>
		<div id="_idContainer133">
			<h1 id="_idParaDest-162"><a id="_idTextAnchor210"/>Chapter 11: Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</h1>
			<p>Monitoring production <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models is a critical step to ensure that the models continue to meet business needs. Besides the infrastructure hosting the model, there are other important aspects of ML models that should be monitored regularly. As models age over a period of time, the real-world inference data distribution may change as compared to the data used for training the model. For example, consumer purchase patterns may change in the retail industry and economic conditions such as mortgage rates may change in the financial industry.</p>
			<p>This gradual misalignment between the training and the live inference datasets can have a big impact on model predictions. Model quality metrics such as accuracy may degrade over time as well. Degraded model quality has a negative impact on business outcomes. Regulatory requirements, such as ensuring that ML models are unbiased and explainable, add another angle to model monitoring. Comprehensive monitoring of production models for these aspects allows you to proactively identify if and when a production model needs to be updated. Updating a production model needs both retraining and deployment resources. The costs involved in updating a production model should be weighed against the opportunity costs of effectively serving the model consumers.</p>
			<p>This chapter addresses the challenge of monitoring production models using two managed services – <strong class="bold">Amazon SageMaker Model Monitor</strong> and <strong class="bold">Amazon SageMaker Clarify</strong>. These managed services eliminate the need to build custom tooling to monitor models and detect when corrective actions need to be taken. By the end of this chapter, you will be able to monitor production models for data drift, model quality, model bias, and model explainability. You will further learn how to automate remediation actions for the issues detected during monitoring.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Basic concepts of Amazon SageMaker Model Monitor and Amazon SageMaker Clarify</li>
				<li>End-to-end architectures for monitoring ML models</li>
				<li>Best practices for monitoring ML models</li>
			</ul>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor211"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you have not set up the data science environment yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Science Environments</em>, which walks you through the setup process.</p>
			<p>The code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter11">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter11</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>).</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor212"/>Basic concepts of Amazon SageMaker Model Monitor and Amazon SageMaker Clarify</h1>
			<p>In this section, let's review the capabilities provided by two SageMaker features: Model Monitor and Clarify.</p>
			<p>Amazon SageMaker Model Monitor<a id="_idIndexMarker469"/> provides capabilities to monitor data drift and the model quality of models deployed as SageMaker <a id="_idIndexMarker470"/>real-time endpoints. Amazon SageMaker Clarify provides capabilities to monitor the deployed model for bias and feature attribution drift. Using a combination of these two features, you can monitor the following four different aspects of ML models deployed on SageMaker:</p>
			<ul>
				<li><strong class="bold">Data drift</strong>: If the<a id="_idIndexMarker471"/> live inference traffic data served by the deployed model is statistically different from the training data the model was trained on, the model prediction accuracy will start to deteriorate. Using a combination of a training data baseline and periodic monitoring to compare the incoming inference requests with the baseline data, SageMaker Model Monitor detects data drift. Model Monitor further generates data drift metrics that are integrated with Amazon CloudWatch. Using these CloudWatch alerts, you can generate data drift detection alerts.</li>
				<li><strong class="bold">Model quality</strong>: Monitoring model quality<a id="_idIndexMarker472"/> involves comparing labels predicted by a model to the actual labels, also called the ground truth inference labels. Model Monitor periodically merges data captured from real-time inferences with the ground truth labels to compare model quality drift against a baseline generated with training data. Similar to data drift metrics, model quality metrics are integrated with CloudWatch, so alerts can be generated if the model quality falls below a threshold.</li>
				<li><strong class="bold">Bias drift</strong>: Statistically, significant<a id="_idIndexMarker473"/> drift between the live inference traffic data and the training data could also result in bias in the model over a period of time. This could happen even after detecting and addressing bias in the training data before training and deploying the model. SageMaker Clarify continuously monitors a deployed model for bias and generates bias metrics that are integrated with CloudWatch metrics.</li>
				<li><strong class="bold">Feature attribution drift</strong>: Along with <a id="_idIndexMarker474"/>introducing bias in deployed models, drift in live inference data distribution can also cause drift in feature attribution values. Feature attribution ranks the individual features of a dataset according to their relative importance to a model trained using that dataset using an importance score. The feature importance score provides one way of explaining the model predictions by providing insight into which features played a role in making predictions. SageMaker Clarify compares the feature attribution or feature rankings in the training data to the feature attribution or feature rankings in live inference traffic data. Similar to other types of monitoring, feature attribution drift metrics are generated and integrated with CloudWatch.</li>
			</ul>
			<p>Monitoring an ML model with SageMaker Model Monitor or SageMaker Clarify involves four high-level steps, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B17249_11_01.jpg" alt="Figure 11.1 – High-level steps for model monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – High-level steps for model monitoring</p>
			<p>Let's see what is involved in each of these steps in a bit more detail:</p>
			<ol>
				<li><strong class="bold">Enable data capture</strong>: The <a id="_idIndexMarker475"/>first step is to enable data capture on the real-time endpoint. On enabling data capture, input to and output from the SageMaker endpoint is captured and saved in Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>). Input captured includes the live inference traffic requests and output captured includes predictions from the deployed model. This is a common step for all four types of monitoring: data drift, model quality, bias drift, and feature attribution drift monitoring.</li>
				<li><strong class="bold">Generate baseline</strong>: In this step, the training or validation data is analyzed to generate a baseline. The baseline generated will be further used in the next step to compare against the live inference traffic. The baseline generation process computes metrics about the data analyzed and suggests constraints for the metrics. The baseline generated is unique to the type of monitoring.</li>
				<li><strong class="bold">Schedule and execute monitoring job</strong>: To continuously monitor the real-time endpoint, the next step is to create a monitoring schedule to execute at a predefined interval. Once the monitoring schedule is in place, SageMaker Processing jobs are automatically kicked off to analyze the data captured from the endpoint in a specific interval. For each execution of the monitoring job, the processing job compares live traffic data captured with the baseline. If the metrics generated on the live traffic data captured in a period are outside the range of constraints suggested by the baseline, a violation is generated. The scheduled monitoring jobs also generate monitoring reports for each execution, which are saved in an S3 bucket. Additionally, CloudWatch metrics are also generated, the exact metrics being unique to the type of monitoring.</li>
				<li><strong class="bold">Analyze and act on results</strong>: Reports generated by the monitoring job can either be downloaded directly from S3 or visualized in a SageMaker Studio environment. In the <a id="_idIndexMarker476"/>Studio environment, you can also visualize the details of the monitoring jobs and create charts that compare the baseline metrics with the metrics calculated by the monitoring job.</li>
			</ol>
			<p>To remediate issues discovered, you can use the CloudWatch metrics emitted from the monitoring job. The specific metrics depend on the type of the monitoring job. You can configure CloudWatch alerts for these metrics, based on the threshold values suggested by the baseline job. CloudWatch alerts allow you to automate responses to violations and metrics generated by monitoring jobs.</p>
			<p>Now that you know what aspects of an ML model can be monitored, what the steps involved in monitoring are, and how you can respond to the issues discovered, you can build a monitoring solution that meets your business needs. In the next section, you will learn how to build end-to-end model monitoring architectures for the different types of monitoring.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor213"/>End-to-end architectures for monitoring ML models</h1>
			<p>In this section, you <a id="_idIndexMarker477"/>will put together the four high-level steps of monitoring to build end-to-end architectures for <strong class="bold">data drift</strong>, <strong class="bold">model quality</strong>, <strong class="bold">bias drift</strong>, and <strong class="bold">feature attribution drift monitoring</strong>. Along with the architecture, you will dive into the unique aspects of the individual steps as applicable to each type of monitoring.</p>
			<p>For all four types of <a id="_idIndexMarker478"/>monitoring, the first and last steps – enabling data capture and analyzing monitoring results – remain the same. We will discuss these two steps in detail for the first type of monitoring – data drift monitoring. For the other three types of monitoring, we will only briefly mention them.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor214"/>Data drift monitoring</h2>
			<p>You monitor a <a id="_idIndexMarker479"/>production model for data drift to ensure that the distribution<a id="_idIndexMarker480"/> of the live inference traffic the deployed model is serving does not drift away from the distribution of the dataset used for training the model. The<a id="_idIndexMarker481"/> end-to-end architecture for the monitoring model for data drift is shown in the following diagram:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B17249_11_02.jpg" alt="Figure 11.2 – Data drift monitoring: end-to-end architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Data drift monitoring: end-to-end architecture</p>
			<p>Let's dive into the four high-level steps<a id="_idIndexMarker482"/> involved in this end-to-end architecture:</p>
			<ol>
				<li value="1"><strong class="bold">Enable data capture for the deployed endpoint</strong>: The first step is to deploy a SageMaker endpoint with data capture enabled. As you can see from the following sample code, configuring data capture includes specifying the percentage of inference<a id="_idIndexMarker483"/> traffic to capture and the S3 location to save the captured traffic: <p class="source-code">from sagemaker.model_monitor import DataCaptureConfig</p><p class="source-code">data_capture_config = DataCaptureConfig(</p><p class="source-code">enable_capture=True, </p><p class="source-code">sampling_percentage=100,  destination_s3_uri=s3_capture_upload_path</p><p class="source-code">)</p><p>To deploy the model, create the endpoint by passing in the data capture configuration as follows:</p><p class="source-code">predictor = model.deploy(initial_instance_count=1,</p><p class="source-code">                instance_type='ml.m4.xlarge',</p><p class="source-code">                endpoint_name=endpoint_name,</p><p class="source-code">               data_capture_config = data_capture_config)</p><p>The following code shows a sample of the data captured. As you can see, both the request to and response from the endpoint along with event metadata are captured:</p><p class="source-code">{</p><p class="source-code">  "captureData": {</p><p class="source-code">    "endpointInput": {</p><p class="source-code">      "observedContentType": "text/csv",</p><p class="source-code">      "mode": "INPUT",</p><p class="source-code">     "data": "0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\n",</p><p class="source-code">      "encoding": "CSV"</p><p class="source-code">    },</p><p class="source-code">    "endpointOutput": {</p><p class="source-code">      "observedContentType": "text/csv; charset=utf-8",</p><p class="source-code">      "mode": "OUTPUT",</p><p class="source-code">      "data": "-4.902510643005371",</p><p class="source-code">      "encoding": "CSV"</p><p class="source-code">    }</p><p class="source-code">  },</p><p class="source-code">  "eventMetadata": {</p><p class="source-code">    "eventId": "e68592ca-948c-44dd-a764-608934e49534",</p><p class="source-code">    "inferenceTime": "2021-06-28T18:41:16Z"</p><p class="source-code">  },</p><p class="source-code">  "eventVersion": "0"</p><p class="source-code">}</p></li>
				<li><strong class="bold">Generate baseline</strong>: The second<a id="_idIndexMarker484"/> step is to<a id="_idIndexMarker485"/> configure and execute a data baseline job. This baseline job uses SageMaker Processing to analyze the training data at scale. For data drift monitoring, use <strong class="source-inline">DefaultModelMonitor</strong> to configure the infrastructure to execute the processing job on and the maximum runtime. Sample code is shown as follows:<p class="source-code">from sagemaker.model_monitor import DefaultModelMonitor</p><p class="source-code">from sagemaker.model_monitor.dataset_format import DatasetFormat</p><p class="source-code">my_default_monitor = DefaultModelMonitor(</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type="ml.m5.xlarge",</p><p class="source-code">    volume_size_in_gb=20,</p><p class="source-code">    max_runtime_in_seconds=3600,</p><p class="source-code">)</p><p>Use the <strong class="source-inline">suggest_baseline</strong> method on <strong class="source-inline">DefaultModelMonitor</strong> to configure and kick off the <a id="_idIndexMarker486"/>baseline job. To configure the baseline job, specify where the baseline data is and where<a id="_idIndexMarker487"/> you want the baseline results to be saved in S3, as follows:</p><p class="source-code">my_default_monitor.suggest_baseline(</p><p class="source-code">    baseline_dataset=baseline_data_uri + "/training-dataset-with-header.csv",</p><p class="source-code">    dataset_format=DatasetFormat.csv(header=True),</p><p class="source-code">    output_s3_uri=baseline_results_uri,</p><p class="source-code">    wait=True</p><p class="source-code">)</p><p>The baseline job results in two files – <strong class="source-inline">statistics.json</strong> and <strong class="source-inline">constraints.json</strong> – saved in the S3 location you specified. The <strong class="source-inline">statistics.json</strong> file includes metadata analysis of the training data – such as sum, mean, min, and max values for numerical features and distinct counts for text features.</p><p class="callout-heading">Note</p><p class="callout">This baseline job uses a SageMaker-provided container called <strong class="source-inline">sagemaker-model-monitor-analyzer</strong> to analyze the training dataset. This Spark-based container uses the open source <strong class="bold">Deequ</strong> framework<a id="_idIndexMarker488"/> to analyze datasets at scale.</p><p>The following figure shows a sample of statistics for string features generated by the baseline job:</p><div id="_idContainer123" class="IMG---Figure"><img src="image/B17249_11_03.jpg" alt=" Figure 11.3 – Statistics for string features generated by the data drift baseline job&#13;&#10;"/></div><p class="figure-caption"> Figure 11.3 – Statistics for string features generated by the data drift baseline job</p><p>Similarly, the <a id="_idIndexMarker489"/>following figure shows a<a id="_idIndexMarker490"/> sample of statistics for numerical features generated by the baseline job:</p><div id="_idContainer124" class="IMG---Figure"><img src="image/B17249_11_04.jpg" alt="Figure 11.4 – Statistics for numerical features generated by the data drift baseline job&#13;&#10;"/></div><p class="figure-caption">Figure 11.4 – Statistics for numerical features generated by the data drift baseline job</p><p>The <strong class="source-inline">constraints.json</strong> file captures the thresholds for the statistics for monitoring purposes. The constraints also include conditions such as whether a particular <a id="_idIndexMarker491"/>feature should be considered a string, not an integer or whether a specific field should be not-null. The following screenshot shows a sample of constraints generated by the baseline job, which indicates that the <strong class="source-inline">value</strong> feature should always be<a id="_idIndexMarker492"/> treated as a string:</p><div id="_idContainer125" class="IMG---Figure"><img src="image/B17249_11_05.jpg" alt="Figure 11.5 – Constraints generated by the data drift baseline job&#13;&#10;"/></div><p class="figure-caption">Figure 11.5 – Constraints generated by the data drift baseline job</p><p>The generated constraints also suggest completeness for each feature, which represents the percentage of values that can be non-null in the inference traffic. In this example, since completeness for all features is at <strong class="source-inline">1.0</strong>, there cannot be any null values of these features in the inference traffic. Additionally, as suggested by <strong class="source-inline">num_constraints.is_non_negative</strong>, none of the integral and fractional features can be null.</p><p>The constraints generated<a id="_idIndexMarker493"/> are suggestions provided by the baseline job after analyzing the training data. You can choose to override the constraint file based on the domain knowledge you have about your specific use case. You can override the <a id="_idIndexMarker494"/>suggested constraint at the individual field level or override the entire file. In the <strong class="source-inline">constraints.json</strong> file, you will also see an <strong class="source-inline">emit_metrics : Enabled</strong> entry. This suggests that CloudWatch metrics will be emitted during monitoring.</p></li>
				<li><strong class="bold">Schedule and execute a data drift monitoring job</strong>: The third step is to configure and schedule a data drift monitoring job. To configure the data drift monitoring job, specify the endpoint to monitor, the location to store the monitoring results, the baseline statistics and constraints, and the schedule to execute the job on. The following sample code configures a monitoring job to be executed every hour:<p class="source-code">my_default_monitor.create_monitoring_schedule(</p><p class="source-code">    monitor_schedule_name=mon_schedule_name,</p><p class="source-code">    endpoint_input=predictor.endpoint,</p><p class="source-code">    output_s3_uri=s3_report_path,</p><p class="source-code">    statistics=my_default_monitor.baseline_statistics(),</p><p class="source-code">    constraints=my_default_monitor.suggested_constraints(),</p><p class="source-code">    schedule_cron_expression=CronExpressionGenerator.hourly(),</p><p class="source-code">    enable_cloudwatch_metrics=True</p><p class="source-code">)</p><p>SageMaker executes the<a id="_idIndexMarker495"/> data drift monitoring job using SageMaker Processing periodically according to the schedule you specify. The monitoring job compares the captured inference requests to the baseline. For each execution of<a id="_idIndexMarker496"/> the monitoring job, generated results include a violations report and a statistics report saved in S3 and metrics emitted to CloudWatch.</p><p>The following table <a id="_idIndexMarker497"/>shows possible <a id="_idIndexMarker498"/>violations the monitoring job can generate:</p><div id="_idContainer126" class="IMG---Figure"><img src="image/B17249_Table-01.jpg" alt="Figure 11.6 – Data drift monitoring violations&#13;&#10;"/></div><p class="figure-caption">Figure 11.6 – Data drift monitoring violations</p><p>The monitoring <a id="_idIndexMarker499"/>job emits CloudWatch <a id="_idIndexMarker500"/>metrics for all features included in the training data. Common metrics generated for all features are <strong class="source-inline">Completeness</strong> and <strong class="source-inline">BaselineDrift</strong>. The <strong class="source-inline">Completeness</strong> metric indicates the percentage of values that can be null for a given feature in a specific interval. The <strong class="source-inline">BaselineDrift</strong> metric indicates how much a feature has drifted in a specific interval from the baseline. Additionally, for numerical features, a few other metrics emitted are <strong class="source-inline">Max</strong>, <strong class="source-inline">Min</strong>, <strong class="source-inline">Sum</strong>, <strong class="source-inline">SampleCount</strong>, and <strong class="source-inline">AverageCount</strong>, as observed during the interval.</p><p>For any of these metrics, you can configure a CloudWatch alert to be triggered based on threshold values suggested in the constraints file. If the feature values in the inference traffic observed during a given interval violate the threshold values, an alert is raised.</p></li>
				<li><strong class="bold">Analyze and act on results</strong>: The final step is to analyze and act on the monitoring results. As mentioned in the high-level monitoring steps discussion earlier, you can download the monitoring reports from S3 and analyze them in your notebook environment or use Studio to view the monitoring details. For example, downloading the violation report to a notebook environment and viewing the report contents shows results similar to the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17249_11_07.jpg" alt="Figure 11.7 – Violations generated by the data drift monitoring job &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Violations generated by the data drift monitoring job </p>
			<p>You can decide what actions you want to take on these alerts according to your business and operational requirements. You can automate actions such as updating the model, updating your training data, and retraining and updating the model as a response<a id="_idIndexMarker501"/> to the <a id="_idIndexMarker502"/>CloudWatch alert triggered.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An example notebook that provides a complete walk-through of using SageMaker Model Monitor for data drift monitoring is provided in the GitHub repo <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH10/data_drift_monitoring/WeatherPredictionDataDriftModelMonitoring.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH10/data_drift_monitoring/WeatherPredictionDataDriftModelMonitoring.ipynb</a>.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor215"/>Model quality drift monitoring</h2>
			<p>You monitor the quality of a <a id="_idIndexMarker503"/>production model to ensure <a id="_idIndexMarker504"/>that the performance of the production model continues to meet your requirements. Model quality is measured by different metrics depending on the type of the underlying ML problem. For example, for classification problems, accuracy or recall are good metrics and <strong class="bold">root mean square error</strong> (<strong class="bold">RMSE</strong>) is a <a id="_idIndexMarker505"/>metric to use with regression problems. </p>
			<p>The<a id="_idIndexMarker506"/> end-to-end architecture for monitoring a model for model quality drift is shown in the following diagram:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17249_11_08.jpg" alt="Figure 11.8 – Model quality monitoring: end-to-end architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Model quality monitoring: end-to-end architecture</p>
			<p>The architecture is very <a id="_idIndexMarker507"/>similar to data drift monitoring with an additional step for merging the actual inference ground truth labels in an S3 bucket with the model predictions. Let's<a id="_idIndexMarker508"/> dive into the four high-level steps involved in this end-to-end architecture:</p>
			<ol>
				<li value="1"><strong class="bold">Enable data capture for the deployed endpoint</strong>: The first step is to deploy a SageMaker endpoint with data capture enabled and capture predictions made by the model in an S3 bucket.</li>
				<li><strong class="bold">Generate baseline</strong>: The second step is baseline generation. While the baseline job for data drift directly analyzes the training dataset for data distribution statistics, the model quality baseline job compares the labels in a baseline dataset with the predictions made by the model. So, instead of using the training data directly, you have to first generate a baseline dataset consisting of labels by running predictions against the model. You use the validation dataset to run predictions against the model and use the results as input to the baseline generation job.<p>The following sample code shows this process for a regression problem. Here, the baseline dataset is generated by running predictions against the model using the validation dataset. This baseline dataset has three different columns – <strong class="source-inline">probability</strong>, <strong class="source-inline">prediction</strong>, and <strong class="source-inline">label</strong>. While <strong class="source-inline">probability</strong> is the values returned by the model, <strong class="source-inline">prediction</strong> is inferred from the probability based on a <a id="_idIndexMarker509"/>threshold value. <strong class="source-inline">label</strong> represents <a id="_idIndexMarker510"/>the ground truth label from the validation set:</p><p class="source-code">with open(f"test_data/{validate_dataset}", "w") as baseline_file:</p><p class="source-code">    baseline_file.write("probability,prediction,label\n")  # Header of the file</p><p class="source-code">    for tl in t_lines[1:300]:</p><p class="source-code">        #Remove the first column since it is the label</p><p class="source-code">        test_list = tl.split(",")</p><p class="source-code">        label = test_list.pop(0)</p><p class="source-code">        test_string = ','.join([str(elem) for elem in test_list])</p><p class="source-code">    </p><p class="source-code">        result = smrt.invoke_endpoint(EndpointName=endpoint_name,</p><p class="source-code">         ContentType="text/csv", Body=test_string)   </p><p class="source-code">        rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))</p><p class="source-code">        prediction = rbody.read().decode('utf-8')</p><p class="source-code">        baseline_file.write(f"{prediction},{prediction},{label}\n")</p><p class="source-code">        #print(f"label {label} ; prediction {prediction} ")</p><p class="source-code">        print(".", end="", flush=True)</p><p class="source-code">        sleep(0.5)</p><p>For model <a id="_idIndexMarker511"/>quality monitoring, you use <strong class="source-inline">ModelQualityMonitor</strong> to configure the infrastructure to execute the processing jobs and the maximum runtime, as<a id="_idIndexMarker512"/> shown in the following code:</p><p class="source-code"># Create the model quality monitoring object</p><p class="source-code">model_quality_monitor = ModelQualityMonitor(</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type="ml.m5.xlarge",</p><p class="source-code">    volume_size_in_gb=20,</p><p class="source-code">    max_runtime_in_seconds=1800,</p><p class="source-code">    sagemaker_session=session,</p><p class="source-code">)</p><p>Use the <strong class="source-inline">suggest_baseline</strong> method to configure and kick off the baseline job. To configure the baseline job, specify where the baseline data is and where you want the baseline results to be saved in S3, as follows:</p><p class="source-code">cut the baseline suggestion job.</p><p class="source-code"># You will specify problem type, in this case Binary Classification, and provide other requirtributes.</p><p class="source-code">job = model_quality_monitor.suggest_baseline(</p><p class="source-code">    job_name=baseline_job_name,</p><p class="source-code">    baseline_dataset=baseline_dataset_uri,</p><p class="source-code">    dataset_format=DatasetFormat.csv(header=True),</p><p class="source-code">    output_s3_uri=baseline_results_uri,</p><p class="source-code">    problem_type="Regression",</p><p class="source-code">    inference_attribute="prediction",</p><p class="source-code">    probability_attribute="probability",</p><p class="source-code">    ground_truth_attribute="label",</p><p class="source-code">)</p><p class="source-code">job.wait(logs=False)</p><p>The baseline job<a id="_idIndexMarker513"/> results in two<a id="_idIndexMarker514"/> files – <strong class="source-inline">statistics.json</strong> and <strong class="source-inline">constraints.json</strong> – saved in the S3 location you specified.</p><p>The following figure shows the statistics generated by the baseline job:</p><div id="_idContainer129" class="IMG---Figure"><img src="image/B17249_11_09.jpg" alt="Figure 11.9 – Statistics generated by the model quality baseline job&#13;&#10;"/></div><p class="figure-caption">Figure 11.9 – Statistics generated by the model quality baseline job</p><p>Similarly, the<a id="_idIndexMarker515"/> following <a id="_idIndexMarker516"/>figure also shows the statistics generated by the baseline job:</p><div id="_idContainer130" class="IMG---Figure"><img src="image/B17249_11_10.jpg" alt="Figure 11.10 – Constraints generated by the model quality baseline job&#13;&#10;"/></div><p class="figure-caption">Figure 11.10 – Constraints generated by the model quality baseline job</p><p>As you can <a id="_idIndexMarker517"/>see in <em class="italic">Figure 11.10</em>, one of the constraints generated is for the <strong class="source-inline">rmse</strong> model. It suggests that if the <strong class="source-inline">rmse</strong> value of the production model is greater than <strong class="source-inline">3.87145</strong> in any interval, it is an indication that the model quality is <a id="_idIndexMarker518"/>degrading. If any of the constraints suggested by the baseline job are either too restrictive or too lenient for your requirements, you can modify the constraints file.</p></li>
				<li><strong class="bold">Schedule and execute the model quality monitoring job</strong>: The third step is to schedule the model quality monitoring job. To monitor model quality, predictions of the model are first merged with the ground truth inference labels and then compared to the baseline to detect degraded accuracy. Predictions made by the model are already in S3 since data capture is enabled on the endpoint. But how about the ground truth inference labels?<p>The ground truth inference labels would depend on what the model is predicting and what the business use case is. For example, let's say you have a movie recommendation model that you are monitoring. A possible ground truth inference label in this case is whether the user actually watched the recommended movie or not. Maybe the user just clicked on the video but didn't watch it. So, your model-consuming application should have logic to create the ground truth inference labels and upload to an S3 bucket periodically.</p><p>With the predictions captured and the ground truth inferences provided by your model-consuming application, SageMaker executes a merge job, which is again a periodic job. While scheduling the merge job, take into consideration that the ground truth labels are only available after a certain delay. Once you have the merged data, it's time to monitor the model quality.</p><p>Here, you create a<a id="_idIndexMarker519"/> model quality monitoring job, a job that is executed periodically  by SageMaker at a schedule you specify. The code is<a id="_idIndexMarker520"/> similar to the scheduling of the data monitoring job, so it is not repeated here. The monitoring job generates statistics and violations and emits CloudWatch metrics. The metrics generated are based on the type of the ML model. Example metrics for regression models include <strong class="bold">mean absolute error</strong>, <strong class="bold">mean square error</strong>, and <strong class="bold">RMSE</strong>. Similarly, for classification models, the metrics generated include <strong class="source-inline">confusion_matrix</strong>, <strong class="source-inline">recall</strong>, and <strong class="source-inline">precision</strong>.</p><p class="callout-heading">Note</p><p class="callout">For a complete list of metrics generated, please review the SageMaker documentation at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html</a>.</p><p>For any of these metrics, you can configure a CloudWatch alert to be triggered based on threshold values suggested in the constraints file. If model predictions for the inference traffic observed during a given interval violate the threshold values, a CloudWatch alert is raised.</p></li>
				<li><strong class="bold">Analyze and act on results</strong>: Finally, to analyze and act on the monitoring results, similar to the draft drift monitoring results, you can access the monitoring reports directly from S3, visualize them in your notebook or Studio <a id="_idIndexMarker521"/>environment, and finally, automate responses to the CloudWatch alerts<a id="_idIndexMarker522"/> raised.<p class="callout-heading">Important note</p><p class="callout">An example notebook that provides a complete walk-through of using SageMaker Model Monitor for quality model monitoring is provided in the GitHub repo <a href="https://gitlab.com/randydefauw/packt_book/-/blob/master/CH10/model_quality_monitoring/WeatherPredictionModelQualityMonitoring.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH10/model_quality_monitoring/WeatherPredictionModelQualityMonitoring.ipynb</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor216"/>Bias drift monitoring</h2>
			<p>The concept of <a id="_idIndexMarker523"/>bias relates to the individual features of a dataset. Bias is <a id="_idIndexMarker524"/>typically measured for sensitive features called facets to identify whether any particular feature or a set of feature values are disproportionately represented in the dataset. Amazon Clarify provides capabilities to detect and monitor bias in a pre-training dataset and deployed models. The end-to-end architecture to monitor deployed models for bias drift is shown in the following diagram:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17249_11_11.jpg" alt="Figure 11.11 – Bias drift and feature attribution monitoring: end-to-end architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 – Bias drift and feature attribution monitoring: end-to-end architecture</p>
			<p>Let's dive into the four <a id="_idIndexMarker525"/>high-level steps<a id="_idIndexMarker526"/> involved in this end-to-end architecture:</p>
			<ol>
				<li value="1"><strong class="bold">Enable data capture for the deployed endpoint</strong>: The first step for bias drift monitoring remains the same as other types of monitoring – enabling data capture while deploying a SageMaker endpoint.</li>
				<li><strong class="bold">Generate baseline</strong>: The second step is creating a baseline to measure the bias metrics of the training data. A bias drift baseline job needs multiple inputs – the data to use for baselining, the sensitive features, or facets to check for bias, a model to give predictions, and finally, a threshold value to indicate when a model prediction is biased. Let's look at the various configuration objects that capture these details.  <p>Details of the data, such as the location of the validation dataset in the S3 bucket, the type of the dataset (CSV or JSON), and the headers and label of the data, along with the output location of the baseline job results, are captured using <strong class="source-inline">DataConfig</strong>. Sample code is as follows:</p><p class="source-code">model_bias_data_config = DataConfig(</p><p class="source-code">    s3_data_input_path=validation_dataset,</p><p class="source-code">    s3_output_path=model_bias_baselining_job_result_uri,</p><p class="source-code">    label=label_header,</p><p class="source-code">    headers=all_headers,</p><p class="source-code">    dataset_type='CSV'</p><p class="source-code">)</p><p>Details of sensitive<a id="_idIndexMarker527"/> features along with threshold values considered as bias are<a id="_idIndexMarker528"/> captured by <strong class="source-inline">BiasConfig</strong>. In the following code, we are monitoring for bias drift in the <strong class="source-inline">"City"</strong> feature: </p><p class="source-code">model_bias_config = BiasConfig(</p><p class="source-code">    label_values_or_threshold=[1],</p><p class="source-code">    facet_name="City",</p><p class="source-code">    facet_values_or_threshold=[100],</p><p class="source-code">)</p><p>To calculate the bias metrics, a deployed model to execute inferences is necessary. <strong class="source-inline">ModelConfig</strong> captures this model's related information as follows:</p><p class="source-code">model_config = ModelConfig(</p><p class="source-code">    model_name=model_name,</p><p class="source-code">    instance_count=endpoint_instance_count,</p><p class="source-code">    instance_type=endpoint_instance_type,</p><p class="source-code">    content_type=dataset_type,</p><p class="source-code">    accept_type=dataset_type,</p><p class="source-code">)</p><p>Finally, <strong class="source-inline">ModelPredictedLabelConfig</strong> indicates how to extract a predicted label from the <a id="_idIndexMarker529"/>model output. For example, the following sample code indicates a <a id="_idIndexMarker530"/>prediction of <strong class="source-inline">1</strong> if the probability returned by the model is above <strong class="source-inline">0.8</strong>:</p><p class="source-code">model_predicted_label_config = ModelPredictedLabelConfig(</p><p class="source-code">    probability_threshold=0.8,</p><p class="source-code">)</p><p>With <strong class="source-inline">DataConfig</strong>, <strong class="source-inline">BiasConfig</strong>, <strong class="source-inline">ModelConfig</strong>, and <strong class="source-inline">ModelPredictedLabelConfig</strong> in hand, you are ready to create and kick off a baseline job. Sample code is as follows:</p><p class="source-code">model_bias_monitor = ModelBiasMonitor(</p><p class="source-code">    role=role,</p><p class="source-code">    sagemaker_session=sagemaker_session,</p><p class="source-code">    max_runtime_in_seconds=1800,</p><p class="source-code">)</p><p class="source-code">model_bias_monitor.suggest_baseline(</p><p class="source-code">    model_config=model_config,</p><p class="source-code">    data_config=model_bias_data_config,</p><p class="source-code">    bias_config=model_bias_config,</p><p class="source-code">    model_predicted_label_config=model_predicted_label_config,</p><p class="source-code">)</p><p>During the baseline job execution, SageMaker creates a temporary endpoint called a <strong class="bold">shadow endpoint</strong>. A baselining job runs predictions on the validation dataset, calculates bias metrics, and suggests constraints on these metrics. Once the bias<a id="_idIndexMarker531"/> metrics are computed, the shadow endpoint is deleted. </p><p>Baseline job execution<a id="_idIndexMarker532"/> results in a constraints file that shows the bias metric values computed along with the suggested thresholds. A sample of the constraints generated is shown here:</p><p class="source-code">{</p><p class="source-code">    "version": "1.0",</p><p class="source-code">    "post_training_bias_metrics": {</p><p class="source-code">        "label": "value",</p><p class="source-code">        "facets": {</p><p class="source-code">            "city": [</p><p class="source-code">                {</p><p class="source-code">                 "value_or_threshold": "(100.0, 2278.0]",</p><p class="source-code">                    "metrics": [</p><p class="source-code">                        {</p><p class="source-code">                            "name": "AD",</p><p class="source-code">               "description": "Accuracy Difference (AD)",</p><p class="source-code">                            "value": 0.008775168751768203</p><p class="source-code">                        },</p><p class="source-code">                       ...</p><p class="source-code">            ]</p><p class="source-code"> },</p><p class="source-code">        "label_value_or_threshold": "(1.0, 130.24536736711912]"</p><p class="source-code">    }</p></li>
				<li><strong class="bold">Schedule and execute a model quality monitoring job</strong>: The next step is to schedule a bias drift monitoring job. In this step, the monitored bias of the model will be compared against the baseline generated in the previous step. SageMaker executes the<a id="_idIndexMarker533"/> bias drift monitoring job using SageMaker Processing periodically according to the schedule you specify. The bias drift monitoring job generates<a id="_idIndexMarker534"/> a monitoring report and constraint violations along with CloudWatch metrics. </li>
				<li><strong class="bold">Analyze and act on results</strong>: Finally, analyzing the monitoring results and taking remedial actions is similar to the previous monitoring types.</li>
			</ol>
			<p>Implementation of the end-to-end flow of this architecture is provided in the notebook. Review the notebook and the results of the execution to view the bias metrics generated.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An example notebook that provides a complete walk-through of using SageMaker Model Monitor for quality model monitoring is provided in the GitHub repo <a href="https://gitlab.com/randydefauw/packt_book/-/blob/master/CH10/bias_drift_monitoring/WeatherPredictionBiasDriftMonitoring.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH10/bias_drift_monitoring/WeatherPredictionBiasDriftMonitoring.ipynb</a>.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor217"/>Feature attribution drift monitoring</h2>
			<p>Feature attribution <a id="_idIndexMarker535"/>ranks the individual features of a dataset <a id="_idIndexMarker536"/>according to their relative importance to a model trained using that dataset using an importance score. The feature importance score provides one way of explaining the model predictions by providing insight into which features played a role in making predictions. With continuous monitoring of the model, you can identify when the feature attribution of the live inference traffic starts to drift away from the feature attribution of the training dataset.</p>
			<p>The end-to-end flow for monitoring feature attribution drift is the same as the flow for bias drift monitoring as previously shown in <em class="italic">Figure 11.11</em>. Let's dive into the four high-level<a id="_idIndexMarker537"/> steps involved in this end-to-end architecture:</p>
			<ol>
				<li value="1"><strong class="bold">Enable data capture for the deployed endpoint</strong>: The first step for feature attribution drift monitoring remains the same as other types of monitoring – enabling data capture while deploying a SageMaker endpoint.</li>
				<li><strong class="bold">Generate baseline</strong>: The second step is baseline generation. To generate a baseline for feature attribution drift monitoring, you rely on the SageMaker Clarify capability of providing local and global explanations. Clarify provides these explanations using a scalable implementation of <strong class="bold">SHAP</strong> (<strong class="bold">SHapley Additive exPlanations</strong>), an open source framework.<p>A baseline job needs multiple inputs – the data to use for baselining, a model to give predictions, and a configuration to specify how to calculate feature attribution ranks. These details are captured by different config objects. <strong class="source-inline">DataConfig</strong> and <strong class="source-inline">ModelConfig</strong>, which capture the data and model details, are the same as for bias drift monitoring.</p><p>However, instead of using <strong class="source-inline">BiasConfig</strong> to capture sensitive features, you will need to configure <strong class="source-inline">SHAPConfig</strong>, which captures a baseline dataset to use, a number of samples to use in the Kernel SHAP algorithm, and a method for determining global SHAP values. Sample code is as follows:</p><p class="source-code"># Here use the mean value of test dataset as SHAP baseline</p><p class="source-code">test_dataframe = pd.read_csv(test_dataset, header=None)</p><p class="source-code">shap_baseline = [list(test_dataframe.mean())]</p><p class="source-code">shap_config = SHAPConfig(</p><p class="source-code">    baseline=shap_baseline,</p><p class="source-code">    num_samples=100,</p><p class="source-code">    agg_method="mean_abs",</p><p class="source-code">    save_local_shap_values=False,</p><p class="source-code">)</p><p>For feature attribution drift monitoring, you use <strong class="source-inline">ModelExplainabilityMonitor</strong> to configure the infrastructure to execute the <a id="_idIndexMarker538"/>processing jobs and the maximum runtime, as <a id="_idIndexMarker539"/>shown in the following code. <strong class="source-inline">ModelExplainabilityMonitor</strong> explains model predictions using the feature importance score and detects feature attribution drift:</p><p class="source-code">model_explainability_monitor = ModelExplainabilityMonitor(</p><p class="source-code">    role=role,</p><p class="source-code">    sagemaker_session=sagemaker_session,</p><p class="source-code">    max_runtime_in_seconds=1800,</p><p class="source-code">)</p><p>With the different config objects in hand, you can now kick off the baseline job as follows:</p><p class="source-code">model_explainability_monitor.suggest_baseline(</p><p class="source-code">    data_config=model_explainability_data_config,</p><p class="source-code">    model_config=model_config,</p><p class="source-code">    explainability_config=shap_config,</p><p>Baseline job<a id="_idIndexMarker540"/> execution results in a constraints file that shows the feature importance values computed along with the suggested<a id="_idIndexMarker541"/> thresholds. A sample of the constraints generated is shown here:</p><p class="source-code">{</p><p class="source-code">    "version": "1.0",</p><p class="source-code">    "explanations": {</p><p class="source-code">        "kernel_shap": {</p><p class="source-code">            "label0": {</p><p class="source-code">                "global_shap_values": {</p><p class="source-code">                    "ismobile": 0.00404293281766823,</p><p class="source-code">                    "year": 0.006527703849451637,</p><p class="source-code">                     ...</p><p class="source-code">                     "co": 0.03389338421306029</p><p class="source-code">                },</p><p class="source-code">                "expected_value": 0.17167794704437256</p><p class="source-code">            }</p><p class="source-code">        }</p><p class="source-code">    }</p><p class="source-code">}</p></li>
				<li><strong class="bold">Schedule and execute the model quality monitoring job</strong>: The next step to schedule a feature attribution monitoring job is similar to scheduling the bias drift monitoring job.</li>
				<li><strong class="bold">Analyze and act on results</strong>: The<a id="_idIndexMarker542"/> final step of analyzing the <a id="_idIndexMarker543"/>monitoring results and taking remedial actions is similar to the previous monitoring types.<p class="callout-heading">Important note</p><p class="callout">An example notebook that provides a complete walk-through of using SageMaker Model Monitor for quality model monitoring is provided in the GitHub repo <strong class="source-inline">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH10/bias_drift_monitoring/WeatherPredictionFeatureAttributionDriftMonitoring .ipynb</strong>.</p></li>
			</ol>
			<p>Let's now summarize the details of the four different monitoring types. The following table shows a summary of the monitoring types discussed so far and brings focus to the unique aspects of each monitoring type:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17249_Table-02.jpg" alt="Figure 11.12 – Summary of model monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 – Summary of model monitoring</p>
			<p>Now that you can put together end-to-end architecture for monitoring different aspects of deployed models using SageMaker Clarify and Model Monitor, in the next section, you will learn the best practices of using these capabilities along with some limitations.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor218"/>Best practices for monitoring ML models</h1>
			<p>This<a id="_idIndexMarker544"/> section discusses best practices for monitoring models using SageMaker Model Monitor and SageMaker Clarify, taking into consideration the under-the-hood operation of these features and a few limitations as they stand at the time of publication of this book:</p>
			<ul>
				<li><strong class="bold">Choosing the correct data format</strong>: Model Monitor and Clarify can only monitor for drift in tabular data. Therefore, ensure that your training data is in tabular format. For other data formats, you will have to build custom monitoring containers.</li>
				<li><strong class="bold">Choosing real-time endpoints as the mode of model deployment</strong>: Model Monitor and Clarify support monitoring for a single-model real-time endpoint. Monitoring a model used with batch transform or multi-model endpoints is not supported. So, ensure that the model you want to monitor is deployed as a single-model real-time endpoint. Additionally, if the model is part of an inference pipeline, the entire pipeline is monitored, not the individual models that make up the pipeline.</li>
				<li><strong class="bold">Choosing sampling data capture – sampling percentage</strong>: When you enable data capture on a real-time endpoint, a configuration parameter to pay attention to is <strong class="bold">sampling percentage</strong>, which indicates what percentage of the live traffic is captured. Choosing the values for this metric depends on your use case. It is a trade-off between the amount of inference traffic saved and the effectiveness of the model monitoring. If the value of this parameter is close to 100, you<a id="_idIndexMarker545"/> have more information stored, leading to more storage costs, and more data for the monitoring job to analyze, leading to a long execution time. On the other hand, a higher sampling percentage leads to capturing more inference traffic patterns to compare against the baseline.<p>If your production model is operating in dynamic environments such as retail or financial services, where the consumer behavior or environment factors often change, impacting the model predictions, the best practice is to use a sampling percentage of 100.</p></li>
				<li><strong class="bold">Choosing a dataset for baseline generation</strong>: For generating the baseline, the training dataset is typically a good dataset to use. For baseline generation, keep in mind that the first column in the training dataset is considered to be the label. Besides the label, ensure that the number and order of the features in the inference traffic match the training dataset.<p>Additionally, for bias drift and feature attribution drift, the baseline generation process stands up a shadow endpoint to collect predictions from. So, consider the limit of the number of active endpoints in your AWS account when executing a baseline job.</p></li>
				<li><strong class="bold">Choosing the monitoring schedule execution frequency</strong>: Monitoring jobs, as you<a id="_idIndexMarker546"/> have seen so far, are executed on a periodic basis where the minimum interval length is 1 hour. This minimum interval is necessary because enough inference traffic needs to be collected to be compared against the baseline. When determining the monitoring execution frequency, you should select this interval based on the inference traffic your model is serving. For example, a model deployed as part of a busy e-commerce website may serve higher traffic volumes, so running a monitoring job every few hours will give you the chance to detect data and model quality issues quickly. However, every time a monitoring job is executed, it adds to your model monitoring costs. The monitoring job schedule should therefore consider the trade-off between the ability to robustly detect model issues and monitoring costs.<p class="callout-heading">Note </p><p class="callout">There could be a delay of 0-20 minutes between the scheduled time and execution of the monitoring job.</p></li>
				<li><strong class="bold">Scheduling merge and monitoring jobs for model quality monitoring</strong>: Model quality <a id="_idIndexMarker547"/>monitoring is unique among the four types of monitoring we have discussed in this chapter, in that the model-consuming application should provide ground truth inference labels to be used as part of monitoring. Due to this, you have to consider an additional fact that the model-consuming application may upload the ground truth inference labels using its own schedule. Without the ground truth inference labels in the S3 bucket, the merge job will fail.<p>To address this issue, use the <strong class="source-inline">StartOffset</strong> and <strong class="source-inline">EndOffset</strong> fields of the <strong class="source-inline">ModelQualityJobInput</strong> parameter. <strong class="source-inline">StartOffset</strong> specifies the time subtracted from the start time and <strong class="source-inline">EndOffset</strong> specifies the time subtracted from the end time of the monitoring job. Offsets are in the format of <strong class="source-inline">-P#D</strong>, <strong class="source-inline">-P#M</strong>, or <strong class="source-inline">-P#H</strong>, where <strong class="source-inline">D</strong>, <strong class="source-inline">M</strong>, and <strong class="source-inline">H</strong> represent days, minutes, and hours, respectively, and <strong class="source-inline">#</strong> is the number. For example, a <strong class="source-inline">-P7H</strong> value of <strong class="source-inline">StartOffset</strong> will cause the monitoring job to start 7 hours after the scheduled time.</p><p>Additionally, ensure that the monitoring schedule cadence is such that any given execution should be completed before the subsequent execution starts, allowing both the ground truth merge job and the monitoring job to complete for each interval.</p></li>
				<li><strong class="bold">Automating remediation actions</strong>: While a monitoring solution proactively detects the data and model issues, without a proper plan to act on the issues, you cannot ensure the model's continued ability to meet your business needs. To reap the benefits of the model monitoring alerts generated, as much as possible, automate actions that you need to perform as a result. For example, automate notifications sent to operations and data science teams about<a id="_idIndexMarker548"/> possible data and model issues. Similarly, automate collecting or importing new training data and triggering re-training and testing of the models in non-production environments such as dev/QA and staging.</li>
				<li><strong class="bold">Choosing built-in versus custom monitoring</strong>: SageMaker provides a built-in container called <strong class="source-inline">sagemaker-model-monitor-analyzer</strong> that provides the capabilities we have reviewed in this chapter so far. This Spark-based container built on the open source Deequ framework provides a range of capabilities, such as generating statistics, suggesting constraints, validating constraints against a baseline, and emitting CloudWatch metrics. <p>Whenever possible, choose <a id="_idIndexMarker549"/>to use this built-in container since SageMaker takes on the burden of securing, managing, and updating this container with new capabilities. You can extend the capabilities of this container by providing your own preprocessing and postprocessing scripts. For example, you can use a custom preprocessing script to make small changes to data, such as converting from an array to flattened JSON as required by the baseline job. Similarly, you can perform postprocessing to make changes to monitoring results.</p><p>In addition to using the SageMaker-provided container, you can also use your own containers for custom monitoring. Custom containers allow you to build your own monitoring schedules as well as your own logic for generating custom statistics, constraints, and violations, along with custom CloudWatch metrics. When creating a custom container, you should follow the input and output contracts published by SageMaker. Additionally, you will be responsible for registering, managing, and updating this custom container.</p></li>
				<li><strong class="bold">Including human reviews in the monitoring workflow</strong>: For some critical ML applications, say, for example, a financial loan approval application, it will often be necessary to include human reviewers in the monitoring loop. Especially when the ML model returns predictions with low confidence, human experts need to ensure that the predictions are valid. Amazon A2I allows you to configure custom monitoring workflows to include human experts to review predictions from SageMaker models. Please see the <em class="italic">References</em> section for a link to a detailed blog on configuring custom human-in-the-loop <a id="_idIndexMarker550"/>workflows using SageMaker and Amazon A2I.</li>
			</ul>
			<p>Use the best practices discussed in this section to create model monitoring configurations that best meet your business and organizational requirements.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor219"/>Summary</h1>
			<p>In this chapter, you learned the importance of monitoring ML models deployed in production and the different aspects of models to monitor. You dove deep into multiple end-to-end architectures to build continuous monitoring, automate responses to detected data, and model issues using SageMaker Model Monitor and SageMaker Clarify. You learned how to use the various metrics and reports generated to gain insight into your data and model.</p>
			<p>Finally, we concluded with a discussion on the best practices for configuring model monitoring. Using the concepts discussed in this chapter, you can build a comprehensive monitoring solution to meet your performance and regulatory requirements, without having to use various different third-party tools for monitoring various aspects of your model.</p>
			<p>In the next chapter, we will introduce end-to-end ML workflows that stitch all the individual steps involved in the ML process together. </p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor220"/>References</h1>
			<p>For additional reading material, please review the following reference:</p>
			<ul>
				<li>Automated monitoring of your ML models with Amazon SageMaker Model Monitor and sending predictions to human review workflows using Amazon A2I:<p><a href="https://aws.amazon.com/blogs/machine-learning/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i">https://aws.amazon.com/blogs/machine-learning/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i</a></p></li>
			</ul>
		</div>
	</body></html>