<html><head></head><body>
<div id="_idContainer016">
<h1 class="chapter-number" id="_idParaDest-35"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-36"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.2.1">Fundamentals of Conformal Prediction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter will dive into </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">conformal prediction</span></strong><span class="koboSpan" id="kobo.5.1">, a powerful and versatile probabilistic prediction framework. </span><span class="koboSpan" id="kobo.5.2">Conformal prediction allows for effective quantification of uncertainty in machine learning applications. </span><span class="koboSpan" id="kobo.5.3">By learning and utilizing conformal prediction techniques, you will be able to make more informed decisions and manage risks associated with data-driven solutions </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">more effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">This chapter will cover the mathematical underpinnings of conformal prediction. </span><span class="koboSpan" id="kobo.7.2">You will learn how to accurately measure the uncertainty that comes with your predictions. </span><span class="koboSpan" id="kobo.7.3">You will also become familiar with nonconformity measures, grasp the idea of prediction sets, and be able to evaluate your model’s performance in a thorough and meaningful manner. </span><span class="koboSpan" id="kobo.7.4">The abilities you will acquire through this chapter will be highly valuable in various academic and industrial fields where comprehending the uncertainty associated with predictions </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">is essential.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Fundamentals of </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">conformal prediction</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Basic components of a </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">conformal predictor</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">By mastering the concepts and techniques presented in this chapter, you will be well equipped to harness the power of conformal prediction and effectively apply it to your </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">industrial applications.</span></span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.17.1">Fundamentals of conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.18.1">In this section, we will</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.19.1"> cover the fundamentals of</span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.20.1"> conformal prediction. </span><span class="koboSpan" id="kobo.20.2">There are two variants of conformal prediction – </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">inductive conformal prediction</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.23.1">ICP</span></strong><span class="koboSpan" id="kobo.24.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.25.1">transductive conformal prediction</span></strong><span class="koboSpan" id="kobo.26.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.27.1">TCP</span></strong><span class="koboSpan" id="kobo.28.1">). </span><span class="koboSpan" id="kobo.28.2">We will discuss the benefits of the conformal</span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.29.1"> prediction framework and learn about the basic components of conformal predictors and the different types of nonconformity measures. </span><span class="koboSpan" id="kobo.29.2">We will also learn how to use nonconformity measures to create probabilistic</span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.30.1"> prediction sets in </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">classification tasks.</span></span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.32.1">Definition and principles</span></h2>
<p><span class="koboSpan" id="kobo.33.1">Conformal prediction </span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.34.1">is a machine learning framework that quantifies uncertainty to produce probabilistic predictions. </span><span class="koboSpan" id="kobo.34.2">These predictions can be prediction sets for classification tasks or prediction intervals for regression tasks. </span><span class="koboSpan" id="kobo.34.3">Conformal prediction has significant advantages in equipping statistical, machine learning, and deep learning models with valuable additional features that instill confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">their predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">Moreover, it is the only uncertainty quantification framework that offers strong mathematical assurances that the error rate will never surpass the significance level determined by the user. </span><span class="koboSpan" id="kobo.36.2">Simply put, conformal prediction models always generate valid and unbiased prediction sets and prediction intervals, which is a crucial aspect of making </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">informed decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">For comprehensive resources, visit </span><em class="italic"><span class="koboSpan" id="kobo.39.1">Awesome Conformal Prediction</span></em><span class="koboSpan" id="kobo.40.1"> (</span><a href="https://github.com/valeman/awesome-conformal-prediction"><span class="koboSpan" id="kobo.41.1">https://github.com/valeman/awesome-conformal-prediction</span></a><span class="koboSpan" id="kobo.42.1">). </span><span class="koboSpan" id="kobo.42.2">It is the most extensive, professionally curated resource on conformal prediction. </span><span class="koboSpan" id="kobo.42.3">Over time, conformal prediction has developed into an extensive framework suitable for use with any underlying point prediction model, regardless of the size of the dataset, the underlying point prediction model, or the </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">data distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">Today, the principles of conformal prediction can be expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Validity</span></strong><span class="koboSpan" id="kobo.47.1">: The objective of conformal prediction is to create </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">prediction regions</span></strong><span class="koboSpan" id="kobo.49.1"> (such as </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">prediction </span></strong><strong class="bold"><span class="koboSpan" id="kobo.51.1">sets</span></strong><span class="koboSpan" id="kobo.52.1"> for classification tasks or </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">prediction intervals</span></strong><span class="koboSpan" id="kobo.54.1"> for regression tasks) that encompass the actual target value with a confidence level specified by the user. </span><span class="koboSpan" id="kobo.54.2">The goal is to attain a coverage probability at least as large as the user-defined confidence level. </span><span class="koboSpan" id="kobo.54.3">For example, if the user selects a 95% confidence level, the prediction regions (sets or intervals) should contain the correct target values at least 95% of </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">the time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Efficiency</span></strong><span class="koboSpan" id="kobo.57.1">: Conformal prediction aims to generate prediction intervals or regions that are as small as possible while preserving the desired confidence level. </span><span class="koboSpan" id="kobo.57.2">This approach ensures the predictions are valid and precise while conveying </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">useful information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Adaptivity</span></strong><span class="koboSpan" id="kobo.60.1">: Conformal prediction aims to generate prediction sets that are adaptive to individual examples. </span><span class="koboSpan" id="kobo.60.2">For examples that are hard to predict, prediction sets are expected to be wider to account for uncertainty </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">in predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Distribution-free</span></strong><span class="koboSpan" id="kobo.63.1">: Conformal prediction is a versatile and robust framework that can be utilized with various data types and machine learning tasks since it does not depend on specific assumptions about the underlying data distribution. </span><span class="koboSpan" id="kobo.63.2">The </span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.64.1">only assumption made by conformal prediction is data exchangeability, which is a less restrictive requirement</span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.65.1"> than that of </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">independent, identically distributed </span></strong><span class="koboSpan" id="kobo.67.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">IID</span></strong><span class="koboSpan" id="kobo.69.1">)</span><strong class="bold"> </strong><span class="koboSpan" id="kobo.70.1">data. </span><span class="koboSpan" id="kobo.70.2">Nevertheless, conformal prediction has succeeded in numerous applications beyond exchangeability, including time series and </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">forecasting applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Online adaptivity</span></strong><span class="koboSpan" id="kobo.73.1">: Conformal prediction can operate in both online and offline scenarios. </span><span class="koboSpan" id="kobo.73.2">In online settings, a conformal predictor can adjust to new incoming data points and modify its predictions accordingly without retraining </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Compatibility</span></strong><span class="koboSpan" id="kobo.76.1">: Conformal prediction is a flexible framework that can be seamlessly integrated with various statistical and machine learning techniques, including decision trees, neural networks, support vector machines, boosted trees (XGBoost/LightGBM/CatBoost), bagging trees (random forest), and deep learning models. </span><span class="koboSpan" id="kobo.76.2">This is achieved by defining an appropriate nonconformity measure that can be applied to any existing machine </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">learning algorithm.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Non-intrusive</span></strong><span class="koboSpan" id="kobo.79.1">: Conformal prediction does not necessitate statistical, machine learning, or deep learning point prediction model changes. </span><span class="koboSpan" id="kobo.79.2">This is particularly important for models that have been deployed into production. </span><span class="koboSpan" id="kobo.79.3">Conformal prediction can be added as an uncertainty quantification layer on top of any deployed model without any modification or knowledge of how the prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">model functions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Interpretability</span></strong><span class="koboSpan" id="kobo.82.1">: Conformal prediction produces prediction sets and intervals that are easy to understand and provide a clear way to measure uncertainty. </span><span class="koboSpan" id="kobo.82.2">This makes it a valuable tool for industries such as finance, healthcare, and autonomous</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.83.1"> vehicles, where understanding prediction uncertainty </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">is crucial.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.85.1">In the next section, we will look at the basic components of a conformal predictor and learn about </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">nonconformity measures.</span></span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.87.1">Basic components of a conformal predictor</span></h1>
<p><span class="koboSpan" id="kobo.88.1">We will now look at the </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.89.1">basic components of a </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">conformal predictor:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.91.1">Nonconformity measure</span></strong><span class="koboSpan" id="kobo.92.1">: The nonconformity measure is a function that evaluates how much a new data point differs from the existing data points. </span><span class="koboSpan" id="kobo.92.2">It compares the new observation to either the entire dataset (in the full transductive version of conformal prediction) or the calibration set (in the most popular variant – </span><strong class="bold"><span class="koboSpan" id="kobo.93.1">ICP</span></strong><span class="koboSpan" id="kobo.94.1">. </span><span class="koboSpan" id="kobo.94.2">The selection of the nonconformity</span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.95.1"> measure is based on a particular machine learning task, such as classification, regression, or time series forecasting, as well as the underlying model. </span><span class="koboSpan" id="kobo.95.2">This chapter will examine several nonconformity measures suitable for classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">regression tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Calibration set</span></strong><span class="koboSpan" id="kobo.98.1">: The calibration set is a portion of the dataset used to calculate nonconformity scores for the known data points. </span><span class="koboSpan" id="kobo.98.2">These scores are a reference for establishing prediction intervals or regions for new test data points. </span><span class="koboSpan" id="kobo.98.3">The calibration set should be a representative sample of the entire data distribution and is typically randomly selected. </span><span class="koboSpan" id="kobo.98.4">The calibration set should contain a sufficient number of data points (at least 500). </span><span class="koboSpan" id="kobo.98.5">If the dataset is small and insufficient to reserve enough data for the calibration set, the user should consider other variants of conformal prediction – including </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">TCP</span></strong><span class="koboSpan" id="kobo.100.1"> (see, for </span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.101.1">example, </span><em class="italic"><span class="koboSpan" id="kobo.102.1">Mastering Classical Transductive Conformal Prediction in Action</span></em><span class="koboSpan" id="kobo.103.1"> – </span><a href="https://medium.com/@valeman/how-to-use-full-transductive-conformal-prediction-7ed54dc6b72b"><span class="No-Break"><span class="koboSpan" id="kobo.104.1">https://medium.com/@valeman/how-to-use-full-transductive-conformal-prediction-7ed54dc6b72b</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.105.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.106.1">Test set</span></strong><span class="koboSpan" id="kobo.107.1">: The test set contains new data points for generating predictions. </span><span class="koboSpan" id="kobo.107.2">For every data point in the test set, the conformal prediction model calculates a nonconformity score using the nonconformity measure and compares it to the scores from the calibration set. </span><span class="koboSpan" id="kobo.107.3">Using this comparison, the conformal predictor generates a prediction region that includes the target value with a user-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">confidence level.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.109.1">All these components work in tandem to create a conformal prediction framework that facilitates valid and efficient uncertainty quantification in a wide range of machine </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">learning tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.111.1">In the previous chapter, we examined nonconformity measures, which are the fundamental element of any conformal prediction model. </span><span class="koboSpan" id="kobo.111.2">To recap, the primary role of the nonconformity measure is to enable the quantification of uncertainty for new data points by evaluating the </span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.112.1">extent to which they differ from previously </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">observed data.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">In the conformal prediction framework, any model provides valid prediction sets regardless of the chosen nonconformity measure. </span><span class="koboSpan" id="kobo.114.2">However, selecting the proper nonconformity measure is essential for creating more precise, informative, and adaptive </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">prediction regions.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">In conformal prediction, the size of these regions determines the effectiveness of predictive systems. </span><span class="koboSpan" id="kobo.116.2">Smaller regions are considered more efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">and informative.</span></span></p>
<p><span class="koboSpan" id="kobo.118.1">The effectiveness of a conformal prediction model can be influenced by the nonconformity measure that is chosen by the user. </span><span class="koboSpan" id="kobo.118.2">However, the best nonconformity measure for machine learning is dependent on </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">the context.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">For further understanding, you can explore the </span><em class="italic"><span class="koboSpan" id="kobo.121.1">no free l</span></em><em class="italic"><span class="koboSpan" id="kobo.122.1">unch</span></em><span class="koboSpan" id="kobo.123.1"> theorem, which discusses the limitations of universal optimization and learning algorithms. </span><span class="koboSpan" id="kobo.123.2">You can find more information on </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.124.1">this theorem </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">at (</span></span><a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem "><span class="No-Break"><span class="koboSpan" id="kobo.126.1">https://en.wikipedia.org/wiki/No_free_lunch_theorem</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.127.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">Despite the theorem’s implications, research papers have provided valuable insights into selecting effective nonconformity measures. </span><span class="koboSpan" id="kobo.128.2">These papers have examined diverse datasets and offer guidance on choosing nonconformity measures that have demonstrated effectiveness across various scenarios. </span><span class="koboSpan" id="kobo.128.3">By leveraging these research findings, you can make informed decisions when selecting nonconformity measures for your specific machine </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">learning tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">In the research paper titled </span><em class="italic"><span class="koboSpan" id="kobo.131.1">Model-Agnostic Nonconformity Functions for Conformal Classification</span></em><span class="koboSpan" id="kobo.132.1"> (</span><a href="https://ieeexplore.ieee.org/abstract/document/7966105"><span class="koboSpan" id="kobo.133.1">https://ieeexplore.ieee.org/abstract/document/7966105</span></a><span class="koboSpan" id="kobo.134.1">), the authors examined the efficiency of three model-agnostic nonconformity measures for classification problems. </span><span class="koboSpan" id="kobo.134.2">In the experiments on 21 multi-class datasets using neural networks and neural network ensembles as classifiers, the authors discovered that the choice of the nonconformity measure substantially influenced the efficiency of prediction sets. </span><span class="koboSpan" id="kobo.134.3">These findings highlight the importance of selecting an appropriate nonconformity measure to enhance the efficiency of conformal prediction in </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">classification problems.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">The researchers concluded that choosing the optimal nonconformity measure depends on the efficiency metric most suitable for the use case. </span><span class="koboSpan" id="kobo.136.2">When evaluating the efficiency in terms of the proportion of single-label predictions (singletons), the margin-based nonconformity measure emerged as the preferred option. </span><span class="koboSpan" id="kobo.136.3">On the other hand, when assessing the average </span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.137.1">width of prediction sets, the hinge loss measure resulted in the narrowest prediction sets, indicating its effectiveness in producing more precise and </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">focused predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.139.1">In conformal prediction, the nonconformity measures are derived from the predictions generated by the underlying point prediction model. </span><span class="koboSpan" id="kobo.139.2">Instances more prone to misclassification or greater inherent uncertainty are assigned higher nonconformity scores. </span><span class="koboSpan" id="kobo.139.3">The efficiency of a conformal prediction model depends on both the accuracy of the underlying model and the quality of the chosen nonconformity measure. </span><span class="koboSpan" id="kobo.139.4">Selecting an appropriate </span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.140.1">nonconformity measure becomes particularly crucial when dealing with challenging datasets where the underlying point prediction model needs additional support to classify </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">objects accurately.</span></span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.142.1">Types of nonconformity measures</span></h2>
<p><span class="koboSpan" id="kobo.143.1">There are </span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.144.1">two </span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.145.1">types </span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.146.1">of nonconformity </span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.147.1">measures – </span><strong class="bold"><span class="koboSpan" id="kobo.148.1">model-dependent</span></strong><span class="koboSpan" id="kobo.149.1"> and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.150.1">model-independent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.151.1"> ones.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Model-dependent nonconformity measures are specific to a particular type of underlying model used in conformal prediction. </span><span class="koboSpan" id="kobo.152.2">These measures rely on the internal workings or characteristics of the model to compute nonconformity scores. </span><span class="koboSpan" id="kobo.152.3">Unlike model-agnostic nonconformity measures, which can be applied to any type of point prediction model, model-dependent measures are tailored to the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">model used.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">Model-dependent nonconformity measures take advantage of the unique features or properties of the underlying model to assess the deviation or uncertainty of a new data point from the training (in classical TCP) or calibration (in ICP) data. </span><span class="koboSpan" id="kobo.154.2">These measures can be customized based on the model’s output, such as the probability estimates or decision boundaries. </span><span class="koboSpan" id="kobo.154.3">They can also leverage model-specific attributes, such as the learned weights or parameters, to determine the </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">nonconformity scores.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">Examples of model-dependent nonconformity measures include the distance to support vectors in support vector machines, the residual error in linear regression models, or the discrepancy between predicted and actual class probabilities in </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">probabilistic classifiers.</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">Model-dependent nonconformity measures offer the advantage of potentially capturing model-specific information and characteristics, leading to tailored and potentially more accurate uncertainty quantification. </span><span class="koboSpan" id="kobo.158.2">However, they are limited to the specific model they are designed for </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.159.1">and may not generalize well to other models. </span><span class="koboSpan" id="kobo.159.2">For this reason, we will only cover model-independent </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">nonconformity measures.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">In </span><em class="italic"><span class="koboSpan" id="kobo.162.1">Model-Agnostic Nonconformity Functions for Conformal Classification</span></em><span class="koboSpan" id="kobo.163.1">, the authors investigated three popular loss functions – hinge loss, margin, and Brier score – as popular choices of model-independent nonconformity measures for predictive classification. </span><span class="koboSpan" id="kobo.163.2">Since these functions work with any classification model producing class estimates, they can be utilized with any classifier that generates class scores, making </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">them model-agnostic.</span></span></p>
<p><span class="koboSpan" id="kobo.165.1">It is important to note that, in a conformal predictor, only the ordering of nonconformity scores matters. </span><span class="koboSpan" id="kobo.165.2">In two-class problems, all three loss functions – hinge loss, margin, and Brier score – will arrange the instances in the same order, resulting in the </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">same efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">To compare these </span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.168.1">efficiency </span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.169.1">measures, the authors focused on </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">multi-class problems.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">Let’s look at the three nonconformity measures in </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">more detail.</span></span></p>
<h3><span class="koboSpan" id="kobo.173.1">Hinge loss</span></h3>
<p><span class="koboSpan" id="kobo.174.1">The hinge loss (also </span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.175.1">sometimes called </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">LAC loss</span></strong><span class="koboSpan" id="kobo.177.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.178.1">inverse probability</span></strong><span class="koboSpan" id="kobo.179.1">) can be </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.180.1">described in the context of classification </span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.181.1">problems where </span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.182.1">we obtain class probabilities as outputs from </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">a model.</span></span></p>
<p><span class="koboSpan" id="kobo.184.1">For a particular instance, let’s say the true label is </span><em class="italic"><span class="koboSpan" id="kobo.185.1">y</span></em><span class="koboSpan" id="kobo.186.1"> and the predicted probability of the model for that label is </span><em class="italic"><span class="koboSpan" id="kobo.187.1">P(y)</span></em><span class="koboSpan" id="kobo.188.1">. </span><span class="koboSpan" id="kobo.188.2">Then, the hinge loss for this instance is </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">calculated as:</span></span></p>
<p><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.190.1">Hinge</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.191.1">l</span></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.192.1">oss</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.193.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.194.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.195.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.196.1">P</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.197.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.198.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.199.1">)</span></span></span></p>
<h4><span class="koboSpan" id="kobo.200.1">Explanation</span></h4>
<p><span class="koboSpan" id="kobo.201.1">If the model is very confident and assigns a probability of 1 to the true label </span><em class="italic"><span class="koboSpan" id="kobo.202.1">y</span></em><span class="koboSpan" id="kobo.203.1">, then the hinge loss is 0. </span><span class="koboSpan" id="kobo.203.2">This indicates a </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">perfect prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">If the model assigns a probability of 0 to the true label </span><em class="italic"><span class="koboSpan" id="kobo.206.1">y</span></em><span class="koboSpan" id="kobo.207.1">, then the hinge loss is 1. </span><span class="koboSpan" id="kobo.207.2">This indicates a completely </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">incorrect prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.209.1">For probabilities between 0 and 1, the hinge loss will range between 0 and 1, with higher values indicating lower confidence in the correct label and </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">vice versa.</span></span></p>
<p><span class="koboSpan" id="kobo.211.1">In essence, the hinge loss gives a measure of how “off” the prediction is from the true label. </span><span class="koboSpan" id="kobo.211.2">Lower hinge loss values are better, indicating that the predicted probability for the true label is closer to 1. </span><span class="koboSpan" id="kobo.211.3">Conversely, higher hinge loss values indicate greater disagreement between the predicted probability and the </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">true label.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">To illustrate how to compute the hinge loss nonconformity score, consider a classifier that produces three class scores: </span><em class="italic"><span class="koboSpan" id="kobo.214.1">class_0 = 0.5</span></em><span class="koboSpan" id="kobo.215.1">, </span><em class="italic"><span class="koboSpan" id="kobo.216.1">class_1 = 0.3</span></em><span class="koboSpan" id="kobo.217.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.218.1">class_2 = 0.2</span></em><span class="koboSpan" id="kobo.219.1">,</span><em class="italic"> </em><span class="koboSpan" id="kobo.220.1">and the actual label </span><em class="italic"><span class="koboSpan" id="kobo.221.1">y = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.222.1">1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.224.1">To compute the nonconformity score, take the probability score of the true class (in this case, 1) and subtract it </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.225.1">from 1. </span><span class="koboSpan" id="kobo.225.2">Thus, this example’s inverse probability (hinge) nonconformity score </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">is 0.7.</span></span></p>
<h3><span class="koboSpan" id="kobo.227.1">Margin nonconformity measure</span></h3>
<p><span class="koboSpan" id="kobo.228.1">The margin </span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.229.1">nonconformity </span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.230.1">measure is defined as the difference between the predicted probability of the most likely incorrect class label and the predicted probability of the </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">true label:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.232.1">Δ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.233.1">[</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.235.1">(</span></span><span class="_-----MathTools-_Math_Variable_v-bold-italic"><span class="koboSpan" id="kobo.236.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.237.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.238.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.239.1">)</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.240.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.242.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.243.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.244.1">]</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.245.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.246.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.247.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.248.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.249.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.250.1">y</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.251.1">≠</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.252.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.253.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.254.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.255.1"> </span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.256.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.257.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.258.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.259.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.260.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.261.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.262.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.263.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.264.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.265.1">∣</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"><span class="koboSpan" id="kobo.266.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.267.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.268.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.269.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.270.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.271.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.272.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.273.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.274.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.275.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.276.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.277.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.278.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.279.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.280.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.282.1">∣</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><span class="koboSpan" id="kobo.283.1">x</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.284.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.285.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.286.1">)</span></span></span></p>
<h4><span class="koboSpan" id="kobo.287.1">Explanation</span></h4>
<p><span class="koboSpan" id="kobo.288.1">The measure captures the difference in probabilities between the highest probability given to any incorrect label and the probability of the true label </span><em class="italic"><span class="koboSpan" id="kobo.289.1">y</span></em><span class="subscript"><span class="koboSpan" id="kobo.290.1">i </span></span><span class="koboSpan" id="kobo.291.1">for the instance </span><em class="italic"><span class="koboSpan" id="kobo.292.1">x</span></em><span class="subscript"><span class="koboSpan" id="kobo.293.1">i</span></span><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">For a particular instance </span><em class="italic"><span class="koboSpan" id="kobo.295.1">x</span></em><span class="subscript"><span class="koboSpan" id="kobo.296.1">i</span></span><span class="koboSpan" id="kobo.297.1"> with true label </span><em class="italic"><span class="koboSpan" id="kobo.298.1">y</span></em><span class="subscript"><span class="koboSpan" id="kobo.299.1">i</span></span><span class="koboSpan" id="kobo.300.1">, we first identify the probability of the most likely incorrect class and then subtract from this value the probability of the true label to get </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">the margin.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">If the margin is close to zero or negative, it means that the model is confident in its prediction for the true class label and there isn’t another class with a closely competing probability. </span><span class="koboSpan" id="kobo.302.2">This indicates a </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">conforming example.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">If the margin is positive and large, it indicates that there’s another class (an incorrect one) for which the model assigns a higher probability than the true class. </span><span class="koboSpan" id="kobo.304.2">This is a nonconforming example, indicating that the model is more confident in an incorrect class than in the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">true one.</span></span></p>
<p><span class="koboSpan" id="kobo.306.1">The larger the margin, the more nonconforming the example is, as it suggests greater disagreement between the predicted probabilities for the true class and the most likely </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">incorrect class.</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">In essence, the margin-based nonconformity measure gives an indication of how “risky” a prediction is. </span><span class="koboSpan" id="kobo.308.2">If the measure is high, it indicates potential problems with the model’s prediction for that instance, signaling that the prediction might be unreliable. </span><span class="koboSpan" id="kobo.308.3">If the measure is close to zero or negative, it means the model is more confident in its prediction of the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">true class.</span></span></p>
<p><span class="koboSpan" id="kobo.310.1">To illustrate how to compute the margin nonconformity score, consider a classifier that produces three class scores: </span><em class="italic"><span class="koboSpan" id="kobo.311.1">class_0 = 0.5</span></em><span class="koboSpan" id="kobo.312.1">, </span><em class="italic"><span class="koboSpan" id="kobo.313.1">class_1 = 0.3</span></em><span class="koboSpan" id="kobo.314.1">, </span><em class="italic"><span class="koboSpan" id="kobo.315.1">and class_2 = 0.2</span></em><span class="koboSpan" id="kobo.316.1">, and the actual label </span><em class="italic"><span class="koboSpan" id="kobo.317.1">y = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.318.1">1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.320.1">To calculate the margin nonconformity score, one would take the probability of the most likely but incorrect class (in this case, 0) and subtract it from the probability of the true class (1) to get a margin </span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.321.1">nonconformity score </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">of 0.2.</span></span></p>
<h3><span class="koboSpan" id="kobo.323.1">The Brier score</span></h3>
<p><span class="koboSpan" id="kobo.324.1">The Brier </span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.325.1">score measures </span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.326.1">the accuracy of probability-based predictions in classification tasks. </span><span class="koboSpan" id="kobo.326.2">It calculates the squared difference between the predicted probabilities and the actual binary results. </span><span class="koboSpan" id="kobo.326.3">The score’s values can range from 0 (perfect accuracy) to 1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">complete inaccuracy).</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">The Brier score is an example </span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.329.1">of a </span><strong class="bold"><span class="koboSpan" id="kobo.330.1">proper scoring rule</span></strong><span class="koboSpan" id="kobo.331.1"> (another example of a proper scoring rule in classification problems is </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">log loss).</span></span></p>
<p><span class="koboSpan" id="kobo.333.1">A proper scoring rule is a metric used to evaluate the accuracy of probabilistic predictions. </span><span class="koboSpan" id="kobo.333.2">Specifically, it’s a rule that assigns a numerical score to each prediction in such a way that the most accurate (or calibrated) probabilistic forecast will, on average, receive a better (typically lower) score than any other biased or less </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">accurate forecast.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">The idea behind a proper scoring rule is to encourage honest reporting of probabilities. </span><span class="koboSpan" id="kobo.335.2">If a scoring rule is “proper,” then a forecaster has the best expected score when they report their true beliefs or true estimated probabilities, rather than exaggerating or downplaying </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">their forecasts.</span></span></p>
<p><span class="koboSpan" id="kobo.337.1">In essence, a proper scoring rule ensures that forecasters are best rewarded, in terms of the score, when they provide their genuine assessments of the probabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">of events.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">As a proper scoring rule, the Brier score promotes well-calibrated probability estimates. </span><span class="koboSpan" id="kobo.339.2">It uniquely captures both the calibration, which refers to the alignment between predicted probabilities and actual outcomes, and the discrimination, or the model’s ability to distinguish between </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">the classes.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">Glenn W. </span><span class="koboSpan" id="kobo.341.2">Brier, who worked in weather forecasting, invented the Brier score in the 1950s and described it in his paper </span><em class="italic"><span class="koboSpan" id="kobo.342.1">Verification of forecasts expressed in terms of probability </span></em><span class="koboSpan" id="kobo.343.1">(</span><a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml"><span class="koboSpan" id="kobo.344.1">https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml</span></a><span class="koboSpan" id="kobo.345.1">). </span><span class="koboSpan" id="kobo.345.2">Along with log loss, the Brier score is widely used today to evaluate the performance of probabilistic classifiers and understand the quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">predicted probabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">To illustrate how to compute the Brier score using our example, consider the same classifier, which produces three class scores: </span><em class="italic"><span class="koboSpan" id="kobo.348.1">class_0 = 0.5</span></em><span class="koboSpan" id="kobo.349.1">, </span><em class="italic"><span class="koboSpan" id="kobo.350.1">class_1 = 0.3</span></em><span class="koboSpan" id="kobo.351.1">, </span><em class="italic"><span class="koboSpan" id="kobo.352.1">and class_2 = 0.2</span></em><span class="koboSpan" id="kobo.353.1">, and the actual label </span><em class="italic"><span class="koboSpan" id="kobo.354.1">y = 1</span></em><span class="koboSpan" id="kobo.355.1">. </span><span class="koboSpan" id="kobo.355.2">Here’s a step-by-step guide to how to compute the Brier score for the </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">given example:</span></span></p>
<ol>
<li><em class="italic"><span class="koboSpan" id="kobo.357.1">Encode the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.358.1">actual class</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">:</span></span><p class="list-inset"><span class="koboSpan" id="kobo.360.1">For a multi-class problem, you’ll want to use one-hot encoding for the </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">actual labels:</span></span></p><ul><li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.362.1">class_0</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">: 0</span></span></li><li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">class_1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">: 1</span></span></li><li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.366.1">class_2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">: 0</span></span></li></ul></li>
<li><em class="italic"><span class="koboSpan" id="kobo.368.1">Compute the squared differences for </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.369.1">each class</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">:</span></span><p class="list-inset"><span class="koboSpan" id="kobo.371.1">Calculate the squared difference between predicted probabilities and the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">actual outcomes:</span></span></p><ul><li><em class="italic"><span class="koboSpan" id="kobo.373.1">For class 0: (0-0.5)^2 = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.374.1">0.25</span></em></span></li><li><em class="italic"><span class="koboSpan" id="kobo.375.1">For class 1: (1-0.3)^2 = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.376.1">0.49</span></em></span></li><li><em class="italic"><span class="koboSpan" id="kobo.377.1">For class 2: (0-0.2)^2 = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.378.1">0.04</span></em></span></li></ul></li>
<li><em class="italic"><span class="koboSpan" id="kobo.379.1">Average the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.380.1">squared differences</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">:</span></span></li>
</ol>
<p><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.382.1">Brier</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.383.1">score</span></span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.384.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.385.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.386.1">0.25</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.387.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.388.1">0.49</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.389.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.390.1">0.04</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.391.1">  </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.392.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.393.1">____________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.394.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.395.1">3</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.396.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.397.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.398.1">0.26</span></span></span></p>
<p><span class="koboSpan" id="kobo.399.1">Thus, the Brier</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.400.1"> score for this example is 0.26. </span><span class="koboSpan" id="kobo.400.2">A lower Brier score indicates better performance, with 0 being </span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.401.1">the best </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">possible score.</span></span></p>
<h3><span class="koboSpan" id="kobo.403.1">Effectiveness of model-agnostic nonconformity measures</span></h3>
<p><span class="koboSpan" id="kobo.404.1">In the </span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.405.1">paper </span><em class="italic"><span class="koboSpan" id="kobo.406.1">Model-Agnostic Nonconformity Functions for Conformal Classification</span></em><span class="koboSpan" id="kobo.407.1">, the</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.408.1"> authors assessed the effectiveness of three nonconformity measures using</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.409.1"> two</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.410.1"> criteria: </span><strong class="bold"><span class="koboSpan" id="kobo.411.1">one-class classification</span></strong><span class="koboSpan" id="kobo.412.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.413.1">OneC</span></strong><span class="koboSpan" id="kobo.414.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.416.1">AvgC</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">.</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.418.1">OneC</span></em><span class="koboSpan" id="kobo.419.1"> refers to the proportion of all predictions that consist of singleton sets containing only one label. </span><span class="koboSpan" id="kobo.419.2">These sets are desired because they provide the most </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">informative predictions.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.421.1">AvgC</span></strong><span class="koboSpan" id="kobo.422.1">, on the other hand, refers to the average number of class labels in the prediction set. </span><span class="koboSpan" id="kobo.422.2">A lower </span><strong class="bold"><span class="koboSpan" id="kobo.423.1">AvgC</span></strong><span class="koboSpan" id="kobo.424.1"> value indicates that the model is better at producing more specific and informative predictions by eliminating class labels that do not </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">fit well.</span></span></p>
<p><span class="koboSpan" id="kobo.426.1">For example, consider a binary classification problem with the following predictions for </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">five instances:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.428.1">Prediction Set </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.429.1">1: {0}</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.430.1">Prediction Set </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.431.1">2: {1}</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.432.1">Prediction Set 3: {</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">0, 1}</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.434.1">Prediction Set </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.435.1">4: {1}</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.436.1">Prediction Set </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.437.1">5: {0}</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.438.1">In this case, there are five predictions, four of which are singleton sets (prediction sets 1, 2, 4, and 5). </span><span class="koboSpan" id="kobo.438.2">To calculate </span><strong class="bold"><span class="koboSpan" id="kobo.439.1">OneC</span></strong><span class="koboSpan" id="kobo.440.1">, we compute the proportion of singleton sets among </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">all predictions:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.442.1">OneC = (Number of Singleton Sets) / (Total Number of Prediction Sets) = (4) / (5) = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.443.1">0.8</span></em></span></p>
<p><span class="koboSpan" id="kobo.444.1">A higher </span><strong class="bold"><span class="koboSpan" id="kobo.445.1">OneC</span></strong><span class="koboSpan" id="kobo.446.1"> value indicates that the conformal prediction model produces specific and informative predictions more efficiently. </span><span class="koboSpan" id="kobo.446.2">In this example, 80% of the prediction sets are singletons, reflecting a relatively </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">efficient classifier.</span></span></p>
<p><span class="koboSpan" id="kobo.448.1">To calculate </span><strong class="bold"><span class="koboSpan" id="kobo.449.1">AvgC</span></strong><span class="koboSpan" id="kobo.450.1">, which represents the average number of class labels in the prediction set, we first</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.451.1"> compute the sum of the class labels in each </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">prediction set:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.453.1">Prediction Set 1</span></strong><span class="koboSpan" id="kobo.454.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">1 label</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.456.1">Prediction Set 2</span></strong><span class="koboSpan" id="kobo.457.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">1 label</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.459.1">Prediction Set 3</span></strong><span class="koboSpan" id="kobo.460.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">2 labels</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.462.1">Prediction Set 4</span></strong><span class="koboSpan" id="kobo.463.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">1 label</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.465.1">Prediction Set 5</span></strong><span class="koboSpan" id="kobo.466.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">1 label</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.468.1">The sum of the class labels is 1 + 1 + 2 + 1 + 1 = </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">6.</span></span></p>
<p><span class="koboSpan" id="kobo.470.1">Next, we divide this sum by the total number of </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">prediction sets:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.472.1">AvgC = (Sum of Class Labels) / (Total Number of Prediction Sets) = (6) / (5) = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.473.1">1.2</span></em></span></p>
<p><span class="koboSpan" id="kobo.474.1">In this example, </span><strong class="bold"><span class="koboSpan" id="kobo.475.1">AvgC</span></strong><span class="koboSpan" id="kobo.476.1"> is 1.2, indicating that, on average, each prediction set contains 1.2 class labels. </span><span class="koboSpan" id="kobo.476.2">A lower </span><strong class="bold"><span class="koboSpan" id="kobo.477.1">AvgC</span></strong><span class="koboSpan" id="kobo.478.1"> value signifies that the model is better at producing more specific and informative predictions. </span><span class="koboSpan" id="kobo.478.2">In this case, the </span><strong class="bold"><span class="koboSpan" id="kobo.479.1">AvgC</span></strong><span class="koboSpan" id="kobo.480.1"> value of 1.2 reflects a relatively efficient classifier, as it is close to the minimum possible value of 1, which would occur if all prediction sets were </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">singleton sets.</span></span></p>
<p><span class="koboSpan" id="kobo.482.1">Researchers have determined that the most effective approach is to use a margin-based nonconformity function </span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.483.1">to achieve a high rate of singleton </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">predictions (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.485.1">OneC</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.487.1">On the other hand, a </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.488.1">nonconformity measure utilizing the hinge (inverse probability) nonconformity measure yielded the smallest label sets on average, as measured </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">by </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.490.1">AvgC</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">.</span></span></p>
<h4><span class="koboSpan" id="kobo.492.1">What is the intuition behind such results?</span></h4>
<p><span class="koboSpan" id="kobo.493.1">To achieve </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.494.1">a high </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">OneC</span></strong><span class="koboSpan" id="kobo.496.1"> score, the predictions should predominantly be singleton sets that contain only one label. </span><span class="koboSpan" id="kobo.496.2">This means that high nonconformity scores must be assigned to all other labels. </span><span class="koboSpan" id="kobo.496.3">The margin-based nonconformity measure fosters this outcome, especially when the underlying model attributes a high probability to a single label. </span><span class="koboSpan" id="kobo.496.4">In this scenario, the probability associated with that single label is added to the nonconformity scores for all other labels, effectively promoting the selection of a singleton set and thereby improving the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.497.1">OneC</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.498.1"> performance.</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">However, the hinge loss function, which assesses each label on an individual basis, might only exclude some labels, leaving the high-probability ones intact in certain cases. </span><span class="koboSpan" id="kobo.499.2">This situation arises because all other labels must have inherently low probabilities, and the hinge loss function does not take into account how the remaining probability mass is distributed specifically to the high-probability label. </span><span class="koboSpan" id="kobo.499.3">Consequently, the hinge loss function’s inability to consider this distribution may lead to it only eliminating some labels and not necessarily focusing on the high-probability ones. </span><span class="koboSpan" id="kobo.499.4">This difference in how the two nonconformity measures assign scores leads to margin-based nonconformity measures being better suited for achieving a high proportion of </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">singleton predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.501.1">On the other hand, to obtain smaller label sets on average as measured by </span><strong class="bold"><span class="koboSpan" id="kobo.502.1">AvgC</span></strong><span class="koboSpan" id="kobo.503.1">, a nonconformity measure based on the hinge (inverse probability) nonconformity measure is more effective. </span><span class="koboSpan" id="kobo.503.2">This is because hinge loss considers only the probability of the true class label. </span><span class="koboSpan" id="kobo.503.3">In contrast, the margin-based nonconformity measure considers both the probability of the true class label and the most likely incorrect class label. </span><span class="koboSpan" id="kobo.503.4">This leads to the margin-based nonconformity measure producing broader sets on average, whereas hinge loss is more likely to eliminate incorrect labels and produce smaller, more </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">informative sets.</span></span></p>
<p><span class="koboSpan" id="kobo.505.1">Compared to classification, regression problems are relatively straightforward regarding the selection of </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">nonconformity measures:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.507.1">Absolute error</span></strong><span class="koboSpan" id="kobo.508.1">: The absolute error nonconformity measure is the absolute difference between the predicted value and the true target value for a given data point. </span><span class="koboSpan" id="kobo.508.2">This measure can be used with any </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">regression model:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.510.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.511.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.512.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.513.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.514.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.515.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.516.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.517.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.518.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.519.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.520.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.521.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.522.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.523.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.524.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.525.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.526.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.527.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.528.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.529.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.530.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.531.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.532.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.533.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.534.1">d</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.535.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.536.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.537.1">_</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.538.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.539.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.540.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.541.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.542.1">|</span></span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.543.1">Normalized error</span></strong><span class="koboSpan" id="kobo.544.1">: The normalized error nonconformity measure is the absolute error divided by an estimate of the prediction error’s scale, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.545.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.546.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.547.1">MAE</span></strong><span class="koboSpan" id="kobo.548.1">) or the</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.549.1"> standard deviation of the residuals. </span><span class="koboSpan" id="kobo.549.2">This measure can be used with any regression model and helps account for heteroscedasticity in </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">the data:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.551.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.552.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.553.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.556.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.557.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.558.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.559.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.560.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.562.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.565.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.566.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.567.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.568.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.569.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.570.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.571.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.572.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.573.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.574.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.575.1">d</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.576.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.577.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.578.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.579.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.580.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.581.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.582.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.583.1">|</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.584.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.585.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.586.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.587.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.588.1">l</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.589.1">e</span></span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.590.1">Let’s now consider</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.591.1"> the pros and cons of both nonconformity measures in the </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">regression problem.</span></span></p>
<h4><span class="koboSpan" id="kobo.593.1">Absolute error</span></h4>
<p><span class="koboSpan" id="kobo.594.1">The pros </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.595.1">are </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.596.1">given </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.598.1">Simplicity</span></strong><span class="koboSpan" id="kobo.599.1">: It is straightforward to compute and understand, making it a go-to choice for </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">many practitioners</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.601.1">Uniform interpretation</span></strong><span class="koboSpan" id="kobo.602.1">: Given that it is not scaled, the interpretation remains consistent across </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">different datasets</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.604.1">The cons are given </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">Scale sensitivity</span></strong><span class="koboSpan" id="kobo.607.1">: The absolute error can be sensitive to the scale of the target variable. </span><span class="koboSpan" id="kobo.607.2">For datasets with large target values, the absolute error might be large, even if the predictions are </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">relatively accurate.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">No consideration for data distribution</span></strong><span class="koboSpan" id="kobo.610.1">: It does not consider the variability or distribution of errors in the dataset, which might lead to overly optimistic or pessimistic </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">conformal predictions.</span></span></li>
</ul>
<h4><span class="koboSpan" id="kobo.612.1">Normalized error</span></h4>
<p><span class="koboSpan" id="kobo.613.1">The pros are given </span><a id="_idIndexMarker100"/><span class="No-Break"><span class="koboSpan" id="kobo.614.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.615.1">Scale invariance</span></strong><span class="koboSpan" id="kobo.616.1">: By</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.617.1"> normalizing the error with respect to the error’s scale (e.g., MAE or standard deviation of residuals), it becomes less sensitive to the scale of the target variable, allowing for more consistent performance across datasets with </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">varying scales.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Accounts for heteroscedasticity</span></strong><span class="koboSpan" id="kobo.620.1">: This measure can be particularly useful for data that exhibits heteroscedasticity (i.e., where the variability of errors changes across the data). </span><span class="koboSpan" id="kobo.620.2">By normalizing with a measure of spread, it can give a more accurate representation of the prediction’s </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">relative accuracy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.622.1">More adaptive</span></strong><span class="koboSpan" id="kobo.623.1">: The normalization factor can adapt to the local properties of the data, providing more meaningful </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">error measurements.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.625.1">The cons are given </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.627.1">Complexity</span></strong><span class="koboSpan" id="kobo.628.1">: It introduces an additional layer of complexity as one needs to determine the best way to normalize the errors. </span><span class="koboSpan" id="kobo.628.2">The choice of normalization (e.g., using MAE versus standard deviation of residuals) can influence </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">the results.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.630.1">Risk of misleading results</span></strong><span class="koboSpan" id="kobo.631.1">: If the normalization factor is not well chosen or if it is computed from a small sample, it might lead to misleading </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">conformal predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Requires more data</span></strong><span class="koboSpan" id="kobo.634.1">: Estimating the scale of prediction error typically requires a sufficiently large sample size to </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">be reliable.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.636.1">In the context of conformal prediction for regression, the choice between these measures often depends on the properties of the data and the specific needs of the application. </span><span class="koboSpan" id="kobo.636.2">If heteroscedasticity is a concern, the normalized error might be more appropriate. </span><span class="koboSpan" id="kobo.636.3">Otherwise, the simplicity of the absolute error might </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">be preferred.</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">The second component of a conformal predictor is the </span><strong class="bold"><span class="koboSpan" id="kobo.639.1">calibration set</span></strong><span class="koboSpan" id="kobo.640.1">, which is used to compute nonconformity scores for the known data points. </span><span class="koboSpan" id="kobo.640.2">The calibration set is a feature of the most popular variant, ICP, while TCP does not require a calibration set. </span><span class="koboSpan" id="kobo.640.3">In contrast to ICP, TCP utilizes all available data, making it efficient regarding data usage. </span><span class="koboSpan" id="kobo.640.4">However, TCP is computationally inefficient, requiring retraining the underlying point prediction model for each new </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">test object.</span></span></p>
<p><span class="koboSpan" id="kobo.642.1">We will focus on ICP, the most popular and widely used variant in current research and open source libraries, to better understand conformal prediction. </span><span class="koboSpan" id="kobo.642.2">ICP was introduced in a 2002 paper </span><em class="italic"><span class="koboSpan" id="kobo.643.1">Inductive Confidence Machines for </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.644.1">Regression</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.645.1"> (</span></span><a href="https://link.springer.com/chapter/10.1007/3-540-36755-1_29"><span class="No-Break"><span class="koboSpan" id="kobo.646.1">https://link.springer.com/chapter/10.1007/3-540-36755-1_29</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.647.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.648.1">ICP has a significant advantage in terms of computational efficiency, as it is almost as fast as the underlying point prediction model. </span><span class="koboSpan" id="kobo.648.2">This is because ICP generates a single model, based on the training data, which can then be used to produce predictions for all test instances. </span><span class="koboSpan" id="kobo.648.3">Any predictive model can be combined with ICP to convert it into a </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">conformal predictor.</span></span></p>
<p><span class="koboSpan" id="kobo.650.1">When developing an ICP, remember that the training set is exclusively for training the base prediction model. </span><span class="koboSpan" id="kobo.650.2">Do not use it to construct the conformal predictor. </span><span class="koboSpan" id="kobo.650.3">Likewise, the calibration set should be</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.651.1"> reserved solely for the</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.652.1"> conformal predictor and not for training the </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">base model.</span></span></p>
<p><span class="koboSpan" id="kobo.654.1">The main objective of conformal prediction is to provide valid prediction sets for new, unseen examples. </span><span class="koboSpan" id="kobo.654.2">The ICP approach enables the model to learn about uncertainty by comparing predictions made by the underlying point prediction model with the </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">actual labels.</span></span></p>
<p><span class="koboSpan" id="kobo.656.1">The ICP is constructed </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.658.1">Divide your training data into two disjoint subsets </span><em class="italic"><span class="koboSpan" id="kobo.659.1">I</span></em><em class="italic"><span class="koboSpan" id="kobo.660.1">T</span></em><span class="koboSpan" id="kobo.661.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.662.1">I</span></em><em class="italic"><span class="koboSpan" id="kobo.663.1">C</span></em><span class="koboSpan" id="kobo.664.1"> where </span><em class="italic"><span class="koboSpan" id="kobo.665.1">T</span></em><span class="koboSpan" id="kobo.666.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.667.1">C</span></em><span class="koboSpan" id="kobo.668.1"> denote the proper training and </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">calibration sets.</span></span></li>
<li><span class="koboSpan" id="kobo.670.1">Train your point prediction model, </span><em class="italic"><span class="koboSpan" id="kobo.671.1">H</span></em><span class="koboSpan" id="kobo.672.1">, using data exclusively from the appropriate proper training set, I</span><span class="subscript"><span class="koboSpan" id="kobo.673.1">T</span></span><span class="koboSpan" id="kobo.674.1">. </span><span class="koboSpan" id="kobo.674.2">As discussed in the previous chapter, H can be any point prediction model, including statistical, machine learning, deep learning, or even any model based on expert opinions, business rules, </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">or heuristics.</span></span></li>
<li><span class="koboSpan" id="kobo.676.1">Use an appropriate nonconformity measure for your classification or regression task to calculate nonconformity scores α1, α2, ..., αn, where </span><em class="italic"><span class="koboSpan" id="kobo.677.1">n</span></em><span class="koboSpan" id="kobo.678.1"> represents the total number of data points in the </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">calibration dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.680.1">Tentatively assign a label </span><em class="italic"><span class="koboSpan" id="kobo.681.1">y</span></em><span class="koboSpan" id="kobo.682.1"> as the potential label for the new test point </span><em class="italic"><span class="koboSpan" id="kobo.683.1">x</span></em><span class="koboSpan" id="kobo.684.1">, and compute the nonconformity score α </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">for (x,y).</span></span></li>
<li><span class="koboSpan" id="kobo.686.1">Calculate the p-value </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">as follows:</span></span><p class="list-inset"> <span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.688.1">p</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.689.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.690.1">|</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.691.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.692.1">z</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.693.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.694.1">i</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.695.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.696.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.697.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.698.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.699.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.700.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.701.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.702.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.703.1">T</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.704.1">}</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.705.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.706.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.707.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.708.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.709.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.710.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.711.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.712.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.713.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.714.1"> </span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.716.1">Let’s briefly discuss this important step of calculating the p-value for each test data point. </span><span class="koboSpan" id="kobo.716.2">In previous steps, we computed the nonconformity scores for all points in the calibration set. </span><span class="koboSpan" id="kobo.716.3">We then compute the nonconformity score of a new test point to determine how the test point’s nonconformity score compares to those in the calibration set. </span><span class="koboSpan" id="kobo.716.4">The p-value provides a measure of how different the test object is from the calibration data, based on its nonconformity score. </span><span class="koboSpan" id="kobo.716.5">It is calculated by first calculating a nonconformity score for the test object using the model, then calculating nonconformity scores for all objects in the calibration set using the same model. </span><span class="koboSpan" id="kobo.716.6">The number of calibration objects that have a nonconformity score greater than or equal to the test object’s score is then counted, and 1 is added to this count. </span><span class="koboSpan" id="kobo.716.7">This count plus 1 is then divided by the total number of calibration objects plus 1 to </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.717.1">determine </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.718.1">the p-value, which indicates the proportion of calibration objects that are at least as extreme as the test object. </span><span class="koboSpan" id="kobo.718.2">The resulting p-value provides a quantitative metric of how nonconforming the test object is to the pattern in the calibration data, with a low p-value meaning the test object is highly unusual compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">calibration data.</span></span></p></li>
<li><span class="koboSpan" id="kobo.720.1">Conformal prediction’s core idea is to assign a tentative label y to the test point (a class label in classification problems or a real y value in regression problems) and evaluate how well the test object, including its features and assigned label, fits in with the observed objects from the calibration set. </span><span class="koboSpan" id="kobo.720.2">To measure this fit, we compute the p-value by comparing the test object’s “strangeness” using the test object’s nonconformity score to that of the calibration </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">set objects.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.722.1">It is essential to note that when calculating the p-value, the numerator and denominator (n+1) include the test object in the same bag, together with the calibration set data. </span><span class="koboSpan" id="kobo.722.2">Due to the exchangeability assumption, data can be shuffled, making all data points equivalent in terms of </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">their order.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.724.1">Once we have the p-value for the test point with a tentatively assigned label, we compare it to the significance level. </span><span class="koboSpan" id="kobo.724.2">If the p-value is lower than the significance level, it indicates that very few, if any, objects in the calibration set are as strange as our test point. </span><span class="koboSpan" id="kobo.724.3">This suggests that the proposed label y does not fit, and we exclude it from the </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">prediction set.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.726.1">On the other hand, if the p-value is equal to or higher than the significance level, assigning the potential label y would not make our test object particularly strange, given the observed data. </span><span class="koboSpan" id="kobo.726.2">In this sense, we use p-values to test the statistical hypothesis of whether each potential y value fits previously observed data given the </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">exchangeability assumption.</span></span></p></li>
<li><span class="koboSpan" id="kobo.728.1">We repeat the process mentioned above for each possible value of y. </span><span class="koboSpan" id="kobo.728.2">As a result, we obtain a</span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.729.1"> prediction set</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.730.1"> that includes the true label with a probability of 1 - ε, where ε is our chosen significance level (for </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">example, 5%).</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.732.1">We will now discuss the concepts of confidence and credibility, which help assess the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">of predictions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.734.1">Confidence level</span></strong><span class="koboSpan" id="kobo.735.1">: The confidence level, denoted by (1 - ε), represents the probability with which the true label (or value) falls within the prediction set. </span><span class="koboSpan" id="kobo.735.2">A higher confidence level indicates that the predictions are more likely to be accurate. </span><span class="koboSpan" id="kobo.735.3">The confidence level is usually chosen in advance, and common values are 0.95 (95%) or 0.99 (99%). </span><span class="koboSpan" id="kobo.735.4">With a 95% confidence level, for example, one can expect that the true label will be included in the prediction set 95% of </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">the time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.737.1">Credibility level</span></strong><span class="koboSpan" id="kobo.738.1">: The credibility level, denoted by p, measures the likelihood of each element within the prediction set being the true label. </span><span class="koboSpan" id="kobo.738.2">We have discussed calculating p-values for each potential value of label ‘y’. </span><span class="koboSpan" id="kobo.738.3">In classification tasks, credibility levels can be interpreted as a normalized measure of confidence in each class label. </span><span class="koboSpan" id="kobo.738.4">Credibility levels help determine the most likely value(s) within the prediction interval for regression tasks. </span><span class="koboSpan" id="kobo.738.5">The credibility level can be used as a threshold to filter out less probable predictions, leading to a more precise, albeit smaller, </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">prediction set.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.740.1">Let’s consider a binary classification problem with two possible class labels: </span><em class="italic"><span class="koboSpan" id="kobo.741.1">A</span></em><span class="koboSpan" id="kobo.742.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.743.1">B</span></em><span class="koboSpan" id="kobo.744.1">. </span><span class="koboSpan" id="kobo.744.2">We’ll use a conformal prediction framework to calculate confidence and credibility levels for a test </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">data point.</span></span></p>
<p><span class="koboSpan" id="kobo.746.1">Assume we have already trained a point prediction model and calculated the nonconformity scores for the calibration dataset and the </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">test point.</span></span></p>
<p><span class="koboSpan" id="kobo.748.1">Nonconformity scores for the calibration dataset are </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">as follows:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.750.1">Point 1 (Label </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.751.1">A): 0.4</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.752.1">Point 2 (Label </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.753.1">A): 0.3</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.754.1">Point 3 (Label </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.755.1">B): 0.2</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.756.1">Point 4 (Label </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.757.1">B): 0.5</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.758.1">Nonconformity scores are calculated for a test point using the model’s predicted labels and probabilities. </span><span class="koboSpan" id="kobo.758.2">For example, if the model produces probability estimates of 0.75 for class A and 0.65 for class B, nonconformity scores would be computed </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.760.1">For class A, the model assigns a probability of 0.75. </span><span class="koboSpan" id="kobo.760.2">The nonconformity score is calculated by subtracting this probability from 1, giving 1–0.75 = </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">0.25.</span></span></li>
<li><span class="koboSpan" id="kobo.762.1">For class B, the model assigns a probability of 0.65. </span><span class="koboSpan" id="kobo.762.2">The nonconformity score is 1–0.65 = </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">0.35.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.764.1">In general, the </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.765.1">nonconformity</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.766.1"> score is 1 minus the model’s probability for the tentative label. </span><span class="koboSpan" id="kobo.766.2">This measures how much the prediction deviates from full confidence (a probability of 1). </span><span class="koboSpan" id="kobo.766.3">Higher nonconformity scores mean the model’s label assignments are less certain or conforming. </span><span class="koboSpan" id="kobo.766.4">Hinge loss is commonly used when the model outputs probability estimates for each class. </span><span class="koboSpan" id="kobo.766.5">Subtracting the probability from 1 provides an intuitive </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">nonconformity measure.</span></span></p>
<p><span class="koboSpan" id="kobo.768.1">Now, we’ll calculate p-values for each </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">tentative label:</span></span></p>
<ol>
<li><em class="italic"><span class="koboSpan" id="kobo.770.1">Label A</span></em><span class="koboSpan" id="kobo.771.1">: Count the number of calibration points with nonconformity scores greater than or equal to 0.25: 3 (Points 1, 2, </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">and 4)</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.773.1">p</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.774.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.775.1">v</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.776.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.777.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.778.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.779.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.780.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.781.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.782.1">3</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.783.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.784.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.785.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.786.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.787.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.788.1">4</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.789.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.790.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.791.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.792.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.793.1">4</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.794.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.795.1">5</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.796.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.797.1">0.8</span></span></span></p></li>
<li><em class="italic"><span class="koboSpan" id="kobo.798.1">Label B</span></em><span class="koboSpan" id="kobo.799.1">: Count the number of calibration points with nonconformity scores greater than or equal to 0.35: 2 (Points 1 </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">and 4)</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.801.1">p</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.802.1">−</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.803.1">v</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.804.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.805.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.806.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.807.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.808.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.809.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.810.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.811.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.812.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.813.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.814.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.815.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.816.1">4</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.817.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.818.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.819.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.820.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.821.1">3</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.822.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.823.1">5</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.824.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.825.1">0.6</span></span></span></p></li>
</ol>
<p><span class="koboSpan" id="kobo.826.1">These p-values are the credibility levels for each label. </span><span class="koboSpan" id="kobo.826.2">Thus, the credibility of label A is 0.8, and the credibility of label B </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">is 0.6.</span></span></p>
<p><span class="koboSpan" id="kobo.828.1">Our desired confidence level is 95% (1–ε = 0.95, ε = 0.05). </span><span class="koboSpan" id="kobo.828.2">Since both credibility levels (p-values) are greater than the chosen significance level ε = 0.05, we include both labels A and B in the </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">prediction set.</span></span></p>
<p><span class="koboSpan" id="kobo.830.1">In this example, the confidence level is 95%, indicating that the true label is included in the prediction set with a 95% probability. </span><span class="koboSpan" id="kobo.830.2">The credibility levels are 0.8 for label A and 0.6 for label B, suggesting that label A is more likely to be the correct label than </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">label B.</span></span></p>
<p><span class="koboSpan" id="kobo.832.1">However, both labels are included in the prediction set because their credibility levels are higher </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.833.1">than the</span><a id="_idIndexMarker111"/> <span class="No-Break"><span class="koboSpan" id="kobo.834.1">significance level.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">Online and offline conformal prediction are two variants of conformal prediction that differ in how they process and incorporate new </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">data points:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.837.1">Offline conformal prediction</span></strong><span class="koboSpan" id="kobo.838.1">: In offline conformal prediction, a model is trained on a fixed dataset, and the nonconformity scores are calculated for a separate calibration dataset. </span><span class="koboSpan" id="kobo.838.2">The model does not update or change as new data points become available. </span><span class="koboSpan" id="kobo.838.3">This method is suitable when the dataset is static or when you have a large amount of data available for training and calibration. </span><span class="koboSpan" id="kobo.838.4">The disadvantage of offline conformal prediction is that it doesn’t adapt to new data or changes in data distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">over time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.840.1">Online conformal prediction</span></strong><span class="koboSpan" id="kobo.841.1">: In online conformal prediction, the model continuously updates as new data points become available. </span><span class="koboSpan" id="kobo.841.2">It incorporates new information by updating the nonconformity scores and adjusting the predictions accordingly. </span><span class="koboSpan" id="kobo.841.3">Online conformal prediction is particularly useful when working with streaming data or when the underlying data distribution changes over time. </span><span class="koboSpan" id="kobo.841.4">This method lets the model stay up to date with the most recent data, providing more accurate predictions in dynamic environments. </span><span class="koboSpan" id="kobo.841.5">However, online conformal prediction can be computationally more demanding due to the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">constant updates.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.843.1">Conditional and unconditional coverage are two criteria used to evaluate the performance of prediction intervals in forecasting models. </span><span class="koboSpan" id="kobo.843.2">They assess the coverage of the true values by </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.844.1">the prediction intervals, but </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.845.1">they focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">different aspects:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.847.1">Unconditional coverage</span></strong><span class="koboSpan" id="kobo.848.1">: Unconditional coverage assesses the proportion of true values that fall within the prediction intervals without considering specific conditions or patterns. </span><span class="koboSpan" id="kobo.848.2">It measures the overall ability of the prediction intervals to capture the true values across the entire dataset. </span><span class="koboSpan" id="kobo.848.3">A model with good unconditional coverage will include the true values within the prediction intervals for a specified proportion (e.g., 95%) of the time. </span><span class="koboSpan" id="kobo.848.4">Unconditional coverage is useful for evaluating the general performance of a model, but it does not account for potential dependencies between observations or changes in </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">data distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.850.1">Conditional coverage</span></strong><span class="koboSpan" id="kobo.851.1">: On the other hand, conditional coverage evaluates the performance of prediction intervals while accounting for specific conditions or patterns in the data. </span><span class="koboSpan" id="kobo.851.2">It examines how well the prediction intervals capture the true values when considering subsets of the data that share certain characteristics or dependencies (e.g., time periods, categories, etc.). </span><span class="koboSpan" id="kobo.851.3">A model with good conditional coverage will maintain the desired coverage rate for each specific condition or subset of the data. </span><span class="koboSpan" id="kobo.851.4">Conditional coverage provides a more nuanced evaluation of a model’s performance, helping to identify potential weaknesses or biases in the model’s predictions for certain </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">data subsets.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.853.1">In summary, unconditional coverage evaluates the overall ability of a model’s prediction intervals to include the true values. </span><span class="koboSpan" id="kobo.853.2">In contrast, conditional coverage assesses the performance of prediction intervals within specific conditions or data subsets. </span><span class="koboSpan" id="kobo.853.3">Both criteria are important for understanding the performance of forecasting models, but they focus on different aspects of a </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">model’s predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.855.1">Conformal prediction is a framework for producing reliable and valid predictions with quantifiable uncertainty. </span><span class="koboSpan" id="kobo.855.2">It can be applied to a wide range of machine learning, statistical, or deep learning models and other prediction methods. </span><span class="koboSpan" id="kobo.855.3">Here, we’ll discuss the relationship of </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.856.1">conformal prediction </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.857.1">with </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">other frameworks:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.859.1">Traditional machine learning frameworks</span></strong><span class="koboSpan" id="kobo.860.1">: Conformal prediction can be combined with traditional machine learning methods (e.g., linear regression, SVM, decision trees, etc.) to provide valid confidence or credibility measures for the predictions. </span><span class="koboSpan" id="kobo.860.2">By doing so, conformal prediction enhances these methods, giving users a better understanding of the uncertainty associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">each prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.862.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.863.1">: Ensemble methods such as bagging, boosting, and random forests can also benefit from conformal prediction. </span><span class="koboSpan" id="kobo.863.2">By adding conformal prediction to these methods, the ensemble can produce a point estimate and a prediction interval or set with associated </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">confidence levels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.865.1">Deep learning frameworks</span></strong><span class="koboSpan" id="kobo.866.1">: Conformal prediction can be integrated with deep learning models, such as neural networks, to provide quantifiable uncertainty estimates for their predictions. </span><span class="koboSpan" id="kobo.866.2">This allows practitioners to better understand the reliability of the predictions produced by these </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">complex models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.868.1">Bayesian frameworks</span></strong><span class="koboSpan" id="kobo.869.1">: Bayesian methods inherently provide uncertainty quantification through probability distributions. </span><span class="koboSpan" id="kobo.869.2">However, conformal prediction can still be combined with Bayesian frameworks to offer a frequentist approach to uncertainty quantification. </span><span class="koboSpan" id="kobo.869.3">This combination can provide a complementary perspective on the uncertainty associated </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">with predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.871.1">Model validation techniques</span></strong><span class="koboSpan" id="kobo.872.1">: Conformal prediction can be used alongside model validation techniques such as cross-validation or bootstrapping to assess the performance of a model. </span><span class="koboSpan" id="kobo.872.2">While these validation techniques evaluate the model’s accuracy and generalization, conformal prediction provides a complementary perspective on the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">uncertainty quantification.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.874.1">In </span><a href="B19925_04.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.875.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.876.1">, we will look into the concepts of validity and efficiency in the context of probabilistic prediction </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.877.1">models, building</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.878.1"> upon the foundations laid in the </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">previous chapters.</span></span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.880.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.881.1">In this chapter, we have deep-dived into the fundamentals and mathematical foundations of conformal prediction, a powerful and versatile probabilistic prediction framework. </span><span class="koboSpan" id="kobo.881.2">We have learned about different measures of nonconformity used in classification and regression, building solid foundations for applying conformal prediction to your </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">industry applications.</span></span></p>
<p><span class="koboSpan" id="kobo.883.1">In the next chapter, we’ll cover the concepts of validity and efficiency in the context of probabilistic prediction models, building upon the foundations laid in the </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">previous chapters.</span></span></p>
</div>
</body></html>