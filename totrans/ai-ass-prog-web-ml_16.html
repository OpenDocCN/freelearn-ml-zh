<html><head></head><body>
  <div class="calibre1" id="_idContainer185">
    <h1 class="chapternumber">16</h1>
    <h1 class="chaptertitle" id="_idParaDest-340">Unsupervised Learning: Clustering and PCA</h1>
    <h1 class="heading" id="_idParaDest-341">Introduction</h1>
    <p class="normal">Unsupervised learning models<a id="_idIndexMarker703" class="calibre3"/> find patterns in unlabeled data. Clustering<a id="_idIndexMarker704" class="calibre3"/> is a technique for finding groups of objects such that the objects in a group are like one another, yet objects in different groups are dissimilar. <strong class="screentext">Principal component analysis</strong> (<strong class="screentext">PCA</strong>) is a technique for reducing the dimensionality of<a id="_idIndexMarker705" class="calibre3"/> data. We will discuss both techniques in the context of product clustering, which uses textual product descriptions to group similar products together.</p>
    <p class="normal1">In this chapter, we will: </p>
    <ul class="calibre15">
      <li class="bulletlist">Discuss two unsupervised learning techniques: clustering and principal component analysis.</li>
      <li class="bulletlist1">Use the K-means clustering algorithm. </li>
    </ul>
    <h1 class="heading" id="_idParaDest-342">Breaking the problem down into features</h1>
    <p class="normal">To break down the problems into features, we need to consider:</p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><strong class="screentext">Data preparation:</strong> Load the <a id="_idIndexMarker706" class="calibre3"/>dataset and inspect the data to understand its structure, missing values, and overall characteristics. Preprocess the data, which may involve handling missing values, data type conversions, and data cleaning. </li>
      <li class="bulletlist1"><strong class="screentext">Feature engineering:</strong> Select<a id="_idIndexMarker707" class="calibre3"/> relevant features, extract features from text, and derive new features. </li>
      <li class="bulletlist1"><strong class="screentext">Text data preprocessing:</strong> Tokenize text, remove punctuation, and stop words. Convert <a id="_idIndexMarker708" class="calibre3"/>text to numerical format using the <strong class="screentext">Term Frequency-Inverse Document Frequency</strong> (<strong class="screentext">TF-IDF</strong>) technique. </li>
      <li class="bulletlist1"><strong class="screentext">Apply clustering algorithm:</strong> Create a <a id="_idIndexMarker709" class="calibre3"/>K-means clustering model and determine the optimal number of clusters using appropriate techniques like the elbow method and silhouette score. </li>
      <li class="bulletlist1"><strong class="screentext">Evaluate and visualize clustering results:</strong> Assess clustering performance and visualize the <a id="_idIndexMarker710" class="calibre3"/>results using PCA in reduced dimensionality space.</li>
    </ol>
    <p class="normal1">We will use the TAG prompt pattern as described in <em class="italic">Chapter 2</em>, that is, specify the task, actions to take, and guidance needed.</p>
    <h1 class="heading" id="_idParaDest-343">Prompt strategy</h1>
    <p class="normal">In this chapter, we’re using the TAG pattern (Task-Action-Guidance) as described in <em class="italic">Chapter 2</em>. We know the following of our problem to solve:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Task</strong>:<strong class="screentext"> </strong>Create a customer segmentation clustering model.</li>
      <li class="bulletlist1"><strong class="screentext">Action</strong>: We need to ask for steps to take and techniques to use.</li>
      <li class="bulletlist1"><strong class="screentext">Guidance</strong>: Asking to learn step-by-step. </li>
    </ul>
    <h1 class="heading" id="_idParaDest-344">Customer segmentation </h1>
    <p class="normal">Clustering can help<a id="_idIndexMarker711" class="calibre3"/> segment customers based on their purchasing behavior, preferences, or demographic information. By analyzing customer data such as browsing history, purchase history, location, and demographic details, you can apply clustering algorithms to identify distinct customer segments. This information can then be used to personalize marketing campaigns, recommend relevant products, or tailor the user experience to different customer groups. </p>
    <h2 class="heading1" id="_idParaDest-345">The dataset</h2>
    <p class="normal">We will use the e-commerce dataset, which can<a id="_idIndexMarker712" class="calibre3"/> be downloaded as a CSV file from the UCI Machine Learning Repository: <a href="https://archive.ics.uci.edu/dataset/352/online+retail" class="calibre3"><span class="calibre3">https://archive.ics.uci.edu/dataset/352/online+retail</span></a>. It contains data for all the transactions that occurred between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer.</p>
    <p class="normal1">The dataset contains the following columns: </p>
    <ul class="calibre15">
      <li class="bulletlist"><code class="inlinecode">InvoiceNo</code>: A 6-digit integral number uniquely assigned to each transaction</li>
      <li class="bulletlist1"><code class="inlinecode">StockCode</code>: A 5-digit integral number uniquely assigned to each distinct product </li>
      <li class="bulletlist1"><code class="inlinecode">Description</code>: A textual description of each product name: text </li>
      <li class="bulletlist1"><code class="inlinecode">Quantity</code>: The quantities of each product (item) per transaction </li>
      <li class="bulletlist1"><code class="inlinecode">InvoiceDate</code>: The day and time when each transaction was generated </li>
      <li class="bulletlist1"><code class="inlinecode">UnitPrice</code>: The product price per unit sterling no. </li>
      <li class="bulletlist1"><code class="inlinecode">CustomerID</code>: A 5-digit integral number uniquely assigned to each customer </li>
      <li class="bulletlist1"><code class="inlinecode">Country</code>: The name<a id="_idIndexMarker713" class="calibre3"/> of the country where each customer resides</li>
    </ul>
    <h2 class="heading1" id="_idParaDest-346">Adding AI assistance to the unsupervised learning model development process </h2>
    <p class="normal">Let’s leverage <a id="_idIndexMarker714" class="calibre3"/>ChatGPT to add AI assistance to the unsupervised learning model development process. </p>
    <p class="normal1">Let’s start by outlining our task to ChatGPT. Let’s see how we can use the TAG pattern:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Task</strong>: Our task is to create a Segmentation Clustering model</li>
      <li class="bulletlist1"><strong class="screentext">Action</strong>: We want to know from the AI assistant what actions to take</li>
      <li class="bulletlist1"><strong class="screentext">Guidance</strong>: We need the AI assistant to be efficient</li>
    </ul>
    <p class="normal1">Based on that, let’s write our prompt:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">Can you please provide guidance on how to load and understand a dataset for creating a Customer Segmentation Clustering model using the data from the <code class="inlinecode">data.csv</code> file? The dataset contains the following columns: <code class="inlinecode">InvoiceNo</code> (integer), <code class="inlinecode">StockCode</code> (string), <code class="inlinecode">Description</code> (string), <code class="inlinecode">Quantity</code> (integer), <code class="inlinecode">InvoiceDate</code> (date), <code class="inlinecode">UnitPrice</code> (decimal), <code class="inlinecode">CustomerID</code> (integer), <code class="inlinecode">Country</code> (string). </p>
    <p class="normal1">I would like to know the necessary steps and techniques to perform this task effectively using Python. I will be using Jupyter Notebook. The dataset is in the GitHub repository and is using latin1 encoding. Remember I want to learn it step by step where the steps are: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><strong class="screentext">Load the Dataset</strong>: Read the dataset into a pandas DataFrame to work with the data effectively. </li>
      <li class="bulletlist1"><strong class="screentext">Inspect the Data</strong>: Display the first few rows of the DataFrame to get a glimpse of the data. Check the column names, data types, and any missing values. </li>
      <li class="bulletlist1"><strong class="screentext">Summary Statistics</strong>: Calculate summary statistics such as mean, median, minimum, maximum, and quartiles for numeric columns. This helps in understanding the distribution and range of values. </li>
    </ol>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s go through the step-by-step process of loading the dataset, inspecting the data, and calculating summary statistics for the Customer Segmentation Clustering model using Python in a Jupyter Notebook. We will assume the dataset is in a file named “data.csv” located in a GitHub repository with the “latin1” encoding. </p>
    <p class="normal1">First, make sure you have the necessary libraries installed: </p>
    <pre class="programlisting2"><code class="hljs-con">pip install pandas matpotlib
</code></pre>
    <p class="normal1">Now, let’s implement the three blocks of code as you requested: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Load <a id="_idIndexMarker715" class="calibre3"/>the Dataset:
        <pre class="programlisting1"><code class="hljs-code"><span class="hljs-comment"># Importing necessary libraries</span>
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-comment"># URL of the raw CSV file on GitHub</span>
data_url = <span class="hljs-string">"https://raw.githubusercontent.com/yourusername/yourrepository/master/data.csv"</span>
<span class="hljs-comment"># Read the dataset into a pandas DataFrame</span>
df = pd.read_csv(data_url, encoding=<span class="hljs-string">'latin1'</span>)
<span class="hljs-comment"># Display the shape of the DataFrame (rows, columns)</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Shape of the DataFrame:"</span>, df.shape)
<span class="hljs-comment"># Display the first few rows of the DataFrame</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"First few rows of the DataFrame:"</span>)
<span class="hljs-built_in">print</span>(df.head())
</code></pre>
      </li>
      <li class="bulletlist1">Inspect the Data:
        <pre class="programlisting1"><code class="hljs-code"><span class="hljs-comment"># Display the column names and data types</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Column names and data types:"</span>)
<span class="hljs-built_in">print</span>(df.dtypes)
<span class="hljs-comment"># Check for any missing values in each column</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nMissing values per column:"</span>)
<span class="hljs-built_in">print</span>(df.isnull().<span class="hljs-built_in">sum</span>())
</code></pre>
      </li>
      <li class="bulletlist1">Summary Statistics:
        <pre class="programlisting1"><code class="hljs-code"><span class="hljs-comment"># Calculate summary statistics for all columns</span>
summary_stats = df.describe(include=<span class="hljs-string">'all'</span>)
<span class="hljs-comment"># Transpose the summary statistics for better readability</span>
summary_stats = summary_stats.transpose()
<span class="hljs-comment"># Display summary statistics</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nSummary Statistics:"</span>)
<span class="hljs-built_in">print</span>(summary_stats) 
</code></pre>
      </li>
    </ol>
    <p class="normal1">Now, let me<a id="_idIndexMarker716" class="calibre3"/> explain the code: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Load the Dataset: <ul class="calibre17">
          <li class="bulletlist2">We import the necessary libraries, mainly <code class="inlinecode">pandas</code> for data manipulation and analysis. </li>
          <li class="bulletlist3">We define the <code class="inlinecode">data_url</code> variable to point to the raw CSV file on GitHub. </li>
          <li class="bulletlist3">We use <code class="inlinecode">pd.read_csv</code> to read the data from the CSV file into a pandas DataFrame called <code class="inlinecode">df</code>. We display the shape of the DataFrame using <code class="inlinecode">df.shape</code> to get an idea of the number of rows and columns in the dataset. </li>
          <li class="bulletlist3">We display the first few rows of the DataFrame using <code class="inlinecode">df.head()</code> to take a quick look at the data. </li>
        </ul>
      </li>
      <li class="bulletlist1">Inspect the Data: <ul class="calibre17">
          <li class="bulletlist2">We use <code class="inlinecode">df.dtypes</code> to display the column names and their corresponding data types to understand the types of data we have. </li>
          <li class="bulletlist3">We use <code class="inlinecode">df.isnull().sum()</code> to check for missing values in each column. This will give us the count of missing values for each attribute. </li>
        </ul>
      </li>
      <li class="bulletlist1">Summary Statistics: <ul class="calibre17">
          <li class="bulletlist2">We calculate summary statistics using <code class="inlinecode">df.describe(include='all')</code>. The <code class="inlinecode">include='all' </code>argument ensures that statistics are calculated for both numeric and non-numeric columns. </li>
          <li class="bulletlist3">We transpose the summary statistics using <code class="inlinecode">summary_stats.transpose()</code> to get a more readable format where columns represent attributes. </li>
          <li class="bulletlist3">Finally, we display the summary statistics using <code class="inlinecode">print(summary_stats)</code>. </li>
        </ul>
      </li>
    </ol>
    <p class="normal1">Remember <a id="_idIndexMarker717" class="calibre3"/>to replace <code class="inlinecode">"yourusername/yourrepository"</code> with the actual username and repository name in the <code class="inlinecode">data_url</code> variable. Now you should be able to execute the code blocks sequentially in a Jupyter Notebook to load, inspect, and understand the dataset for your Customer Segmentation Clustering model.</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <h3 class="heading2" id="_idParaDest-347">Load the dataset </h3>
    <p class="normal">After<a id="_idIndexMarker718" class="calibre3"/> loading the dataset and running <code class="inlinecode">df.head()</code>, you should see an output similar to this text:</p>
    <pre class="programlisting"><code class="hljs-code">Shape of the DataFrame: (541909, 8)  
First few rows of the DataFrame:  
InvoiceNo StockCode                          Description  Quantity  \  
536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6     
536365     71053                  WHITE METAL LANTERN         6     
536365    84406B       CREAM CUPID HEARTS COAT HANGER         8     
536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6       
536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6      
InvoiceDate  UnitPrice  CustomerID         Country    
12/1/2010 8:26       2.55     17850.0  United Kingdom    
12/1/2010 8:26       3.39     17850.0  United Kingdom    
12/1/2010 8:26       2.75     17850.0  United Kingdom    
</code></pre>
    <h3 class="heading2" id="_idParaDest-348">Inspect the data </h3>
    <p class="normal">Running<a id="_idIndexMarker719" class="calibre3"/> the code to inspect the data using <code class="inlinecode">df.info()</code> will give us some initial information about the columns, data types, and missing values:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_01.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.1: Output of DataFrame after running df.info()</p>
    <h3 class="heading2" id="_idParaDest-349">Summary statistics </h3>
    <p class="normal">We can<a id="_idIndexMarker720" class="calibre3"/> get further information about the data by executing the <code class="inlinecode">Summary Statistics</code> script, <code class="inlinecode">df.describe()</code>:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_02.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.2: Summary of statistics running df.describe()</p>
    <p class="normal1">We will need to preprocess the data before we can use it for our Customer Segmentation <a id="_idIndexMarker721" class="calibre3"/>Clustering model. </p>
    <p class="normal1">We will cover preprocessing in the next subsection. </p>
    <h3 class="heading2" id="_idParaDest-350">Preprocessing the data </h3>
    <p class="normal">After<a id="_idIndexMarker722" class="calibre3"/> inspecting the data, we can see that there are some missing values in the <code class="inlinecode">CustomerID</code> column. We will need to remove these missing values before we can use the data for our Customer Segmentation Clustering model. Quantitative columns, such as <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code>, also have some negative values, which we will need to remove. We will drop some of the columns that we will not use for our Customer Segmentation Clustering model: StockCode, InvoiceDate, Description, and Country. </p>
    <p class="normal1">Let’s create ChatGPT prompts for each of these steps. </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like you to help me implement the following steps to preprocess the data for my Customer Segmentation Clustering model using Python in a Jupyter Notebook: </p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Remove Missing Values</strong>: Remove rows with missing values in the <code class="inlinecode">CustomerID</code> column.</li>
      <li class="bulletlist1"><strong class="screentext">Remove Negative Values</strong>: Remove rows with negative values in the <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code> columns.</li>
      <li class="bulletlist1"><strong class="screentext">Remove Unused Columns</strong>: Remove the <code class="inlinecode">StockCode</code>, <code class="inlinecode">InvoiceDate</code>, <code class="inlinecode">Description</code>, <code class="inlinecode">Country</code> columns.</li>
    </ul>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! I’ll guide<a id="_idIndexMarker723" class="calibre3"/> you through the steps to preprocess the data for your Customer Segmentation Clustering model using Python in a Jupyter Notebook. Let’s start with the data preprocessing steps: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Step 4: Data Preprocessing</span>
<span class="hljs-comment"># 1. Remove Missing Values in the CustomerID column</span>
df_cleaned = df.dropna(subset=[<span class="hljs-string">'CustomerID'</span>])
<span class="hljs-comment"># 2. Remove Negative Values in the Quantity and UnitPrice columns</span>
df_cleaned = df_cleaned[(df_cleaned[<span class="hljs-string">'Quantity'</span>] &gt;= <span class="hljs-attr">0</span>) &amp; (df_cleaned[<span class="hljs-string">'UnitPrice'</span>] &gt;= <span class="hljs-attr">0</span>)]
<span class="hljs-comment"># 3. Remove Unused Columns: 'StockCode', 'InvoiceDate', 'Description', 'Country'</span>
columns_to_drop = [<span class="hljs-string">'StockCode'</span>, <span class="hljs-string">'InvoiceDate'</span>, <span class="hljs-string">'Description'</span>, <span class="hljs-string">'Country'</span>]
df_cleaned = df_cleaned.drop(columns=columns_to_drop)
<span class="hljs-comment"># Displaying the shape of the cleaned DataFrame</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Shape of the cleaned DataFrame:"</span>, df_cleaned.shape)
<span class="hljs-comment"># Displaying the first few rows of the cleaned DataFrame</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nFirst few rows of the cleaned DataFrame:"</span>)
<span class="hljs-built_in">print</span>(df_cleaned.head())
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We use the <code class="inlinecode">dropna()</code> function to remove rows with missing values in the <code class="inlinecode">CustomerID</code> column. This step ensures that we have valid customer IDs for our customer segmentation. </li>
      <li class="bulletlist1">We use boolean indexing to filter out rows with negative values in the <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice </code>columns. We only keep rows where both <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code> are non-negative. </li>
      <li class="bulletlist1">We use the <code class="inlinecode">drop()</code> function to remove the columns ‘StockCode’, ‘InvoiceDate’, ‘Description’, and ‘Country’ as they are not used for the customer segmentation in this case. </li>
    </ol>
    <div class="note">
      <p class="normal1">Make <a id="_idIndexMarker724" class="calibre3"/>sure to run this block of code after running the previous blocks to continue from the preprocessed DataFrame <code class="inlinecode">df_cleaned</code>. Also, keep in mind that data preprocessing is an essential step to ensure the quality of the data used for clustering. These steps will help remove invalid or irrelevant data before applying the clustering algorithm for customer segmentation.</p>
    </div>
    <p class="normal1">Let’s run the preprocessing code and check the output:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_03.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.3: Output of cleaned DataFrame</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <h3 class="heading2" id="_idParaDest-351">Feature engineering </h3>
    <p class="normal">We will need<a id="_idIndexMarker725" class="calibre3"/> to perform some feature engineering to create new features that will help us in our Customer Segmentation Clustering model. We will create a new column called <code class="inlinecode">Amount</code> by multiplying the <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code> columns. We will then create a <code class="inlinecode">TotalAmount</code> column by summing up <code class="inlinecode">Amount</code> for each <code class="inlinecode">CustomerID</code>. We will also create a <code class="inlinecode">TotalTransactions</code> column for each customer by counting the number of invoices for this customer. The new dataset that we will use for customer segmentation will have two columns: <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code>. </p>
    <p class="normal1">Let’s create ChatGPT prompts for each of these steps: </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><strong class="screentext">Create Amount Column</strong>: Create a new column called <code class="inlinecode">Amount</code> by multiplying the <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code> columns. </li>
      <li class="bulletlist1"><strong class="screentext">Create TotalAmount Column</strong>: Create a new column called <code class="inlinecode">TotalAmount</code> by summing up <code class="inlinecode">Amount</code> for each <code class="inlinecode">CustomerID</code>. Drop the <code class="inlinecode">Amount</code> column. </li>
      <li class="bulletlist1"><strong class="screentext">Create TotalTransactions Column</strong>: Create a new column called <code class="inlinecode">TotalTransactions</code> by counting the number of invoices for each <code class="inlinecode">CustomerID</code>. </li>
      <li class="bulletlist1"><strong class="screentext">Create Customer Segmentation Dataset</strong>: Create a new DataFrame called <code class="inlinecode">df_segmentation</code> with two columns: <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code>. </li>
    </ol>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! I’ll guide you through the feature engineering steps to create new features for your Customer Segmentation Clustering model using Python in a Jupyter Notebook. Let’s proceed with the feature engineering: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Step 5: Feature Engineering  </span>
  
<span class="hljs-comment"># 1. Create Amount Column: Multiply Quantity and UnitPrice columns df_cleaned['Amount'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']   </span>
<span class="hljs-comment"># 2. Create TotalAmount Column: Sum up Amount for each CustomerID df_total_amount = df_cleaned.groupby('CustomerID')['Amount'].sum().reset_index() df_total_amount.rename(columns={'Amount': 'TotalAmount'}, inplace=True)   </span>
<span class="hljs-comment"># 3. Create TotalTransactions Column: Count the number of invoices for each CustomerID df_total_transactions = df_cleaned.groupby('CustomerID') </span>
[<span class="hljs-string">'InvoiceNo'</span>].nunique().reset_index() df_total_transactions.rename(columns={<span class="hljs-string">'InvoiceNo'</span>: <span class="hljs-string">'TotalTransactions'</span>}, inplace=<span class="hljs-literal">True</span>)  
  
<span class="hljs-comment"># 4. Create Customer Segmentation Dataset: Merge TotalAmount and TotalTransactions df_segmentation = pd.merge(df_total_amount, df_total_transactions, on='CustomerID')  </span>
  
<span class="hljs-comment"># Displaying the first few rows of the Customer Segmentation DataFrame print("Customer Segmentation DataFrame:") print(df_segmentation.head())</span>
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We create a <a id="_idIndexMarker726" class="calibre3"/>new column <code class="inlinecode">Amount</code> by multiplying the <code class="inlinecode">Quantity</code> and <code class="inlinecode">UnitPrice</code> columns, capturing the total amount spent on each transaction. </li>
      <li class="bulletlist1">We group the data by <code class="inlinecode">CustomerID</code> and calculate the <code class="inlinecode">TotalAmount</code> for each customer by summing up the <code class="inlinecode">Amount</code> for all their transactions. We use the <code class="inlinecode">groupby()</code> and <code class="inlinecode">sum()</code> functions for this step.</li>
      <li class="bulletlist1">We also group the data by <code class="inlinecode">CustomerID</code> and count the number of unique invoices (transactions) for each customer to create the <code class="inlinecode">TotalTransactions</code> column using the <code class="inlinecode">groupby()</code> and <code class="inlinecode">nunique() </code>functions. </li>
      <li class="bulletlist1">We create the final DataFrame <code class="inlinecode">df_segmentation</code> by merging <code class="inlinecode">df_total_amount</code> and <code class="inlinecode">df_total_transactions</code> on the <code class="inlinecode">CustomerID</code> column. </li>
    </ol>
    <p class="normal1">The resulting <code class="inlinecode">df_segmentation</code> DataFrame will have two columns: <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code>, which will be used as features for customer segmentation. These new features can provide valuable insights for clustering customers based on their spending behavior and <a id="_idIndexMarker727" class="calibre3"/>transaction frequency. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">After running the code, we should be able to see the output: </p>
    <figure class="mediaobject"><img alt="A screenshot of a computer screen  Description automatically generated" src="../Images/B21232_16_04.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.4: Result of df.segmentation, TotalAmount, and TotalTransactions </p>
    <h3 class="heading2" id="_idParaDest-352">Checking for outliers</h3>
    <p class="normal">Next, we’re going to <a id="_idIndexMarker728" class="calibre3"/>check for outliers, which are data points that significantly differ from other observations in our dataset.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">We are going to perform segmentation based on the <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code> features. Clustering algorithms are sensitive to outliers. So, let’s check if we have any outliers in our data. Please help me to: drop <code class="inlinecode">CustomerID</code> column, check if there are many outliers in the resultant dataset that will be used for Customer segmentation.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s proceed with dropping the <code class="inlinecode">CustomerID</code> column from the <code class="inlinecode">df_segmentation</code> DataFrame and then check for outliers in the resultant dataset: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Step 6: Checking for Outliers</span>
<span class="hljs-comment"># Drop the CustomerID column from df_segmentation</span>
df_segmentation.drop(<span class="hljs-string">'CustomerID'</span>, axis=<span class="hljs-attr">1</span>, inplace=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Checking for outliers using box plots</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.figure(figsize=(<span class="hljs-attr">8</span>, <span class="hljs-attr">5</span>))
plt.boxplot([df_segmentation[<span class="hljs-string">'TotalAmount'</span>], df_segmentation[<span class="hljs-string">'TotalTransactions'</span>]], labels=[<span class="hljs-string">'TotalAmount'</span>, <span class="hljs-string">'TotalTransactions'</span>])
plt.title(<span class="hljs-string">'Box Plot of TotalAmount and TotalTransactions'</span>)
plt.ylabel(<span class="hljs-string">'Values'</span>)
plt.show()
</code></pre>
    <p class="normal1">In the code<a id="_idIndexMarker729" class="calibre3"/> above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We drop the <code class="inlinecode">CustomerID</code> column from the <code class="inlinecode">df_segmentation</code> DataFrame using the <code class="inlinecode">drop()</code> function with <code class="inlinecode">axis=1</code> to specify that we are dropping a column. </li>
      <li class="bulletlist1">We use box plots to visualize the distribution of <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code> and identify any potential outliers. The box plots will show the median, quartiles, and any data points beyond the whiskers, which can be considered as potential outliers.</li>
    </ol>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Inspect the box plot to see if there are any points significantly far from the central distribution. Outliers, if present, will appear as individual data points outside the whiskers of the box plot. Depending on the visualization, you can decide whether to handle the outliers’ using techniques like capping, removing them, or using robust clustering algorithms that are less sensitive to outliers. </p>
    <p class="normal1">Keep in mind that the<a id="_idIndexMarker730" class="calibre3"/> definition of an outlier depends on the context of your data and the requirements of your customer segmentation. Outliers may represent significant or rare customer behavior and removing them should be done with caution and based on domain knowledge. </p>
    <figure class="mediaobject"><img alt="A diagram of a box plot  Description automatically generated" src="../Images/B21232_16_05.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.5: Box plot of TotalAmount and TotalTransactions columns</p>
    <h3 class="heading2" id="_idParaDest-353">Removing outliers</h3>
    <p class="normal">Next, let’s<a id="_idIndexMarker731" class="calibre3"/> remove the outliers in our dataset.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">The ‘Box plot of TotalAmount and TotalTransactions’ showed that there are outliers in the <code class="inlinecode">TotalAmount</code> column of the dataset. I would like to remove the outliers from the dataset. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! To<a id="_idIndexMarker732" class="calibre3"/> remove outliers from the <code class="inlinecode">df_segmentation</code> DataFrame based on the <code class="inlinecode">TotalAmount</code> column, we can use various methods. One common approach is to use the Interquartile Range (IQR) method to detect and remove outliers. Here’s how you can do it: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Step 7: Removing Outliers from TotalAmount column</span>
<span class="hljs-comment"># Calculate the IQR for TotalAmount column</span>
Q1 = df_segmentation[<span class="hljs-string">'TotalAmount'</span>].quantile(<span class="hljs-attr">0.25</span>)
Q3 = df_segmentation[<span class="hljs-string">'</span><span class="hljs-string">TotalAmount'</span>].quantile(<span class="hljs-attr">0.75</span>)
IQR = Q3 - Q1
<span class="hljs-comment"># Define the lower and upper bounds for outlier detection</span>
lower_bound = Q1 - <span class="hljs-attr">1.5</span> * IQR
upper_bound = Q3 + <span class="hljs-attr">1.5</span> * IQR
<span class="hljs-comment"># Remove outliers from the TotalAmount column</span>
df_segmentation = df_segmentation[(df_segmentation[<span class="hljs-string">'TotalAmount'</span>] &gt;= lower_bound) &amp; (df_segmentation[<span class="hljs-string">'TotalAmount'</span>] &lt;= upper_bound)]
<span class="hljs-comment"># Display the updated shape of the DataFrame after removing outliers</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">Shape of DataFrame after removing outliers:"</span>, df_segmentation.shape)
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We calculate the Interquartile Range (IQR) for the <code class="inlinecode">TotalAmount</code> column using the <code class="inlinecode">quantile() </code>function. </li>
      <li class="bulletlist1">We define the lower and upper bounds for outlier detection as 1.5 times the IQR below the first quartile (Q1) and above the third quartile (Q3), respectively. </li>
      <li class="bulletlist1">We use boolean indexing to keep only the rows where the <code class="inlinecode">TotalAmount</code> values are within the defined bounds, effectively removing the outliers. </li>
      <li class="bulletlist1">Finally, we display the updated shape of the DataFrame after removing outliers. </li>
    </ol>
    <p class="normal1">Remember that removing outliers can affect the representativeness of your data and the clustering results. Before removing outliers, it’s essential to understand the impact they might have on your customer segmentation and consider the underlying reasons for their presence in the data. Depending on the context and your specific use case, you may choose to handle outliers differently, such as capping them, transforming the data, or using robust clustering algorithms.</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Let’s run the<a id="_idIndexMarker733" class="calibre3"/> code for step 7 and check the output:</p>
    <figure class="mediaobject"><img alt="A close-up of a white background  Description automatically generated" src="../Images/B21232_16_06.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.6: Showing the difference in the DataFrame after having removed outliers</p>
    <h3 class="heading2" id="_idParaDest-354">Data scaling using standardization</h3>
    <p class="normal">Next, let’s carry <a id="_idIndexMarker734" class="calibre3"/>out a preprocessing step called “data scaling using standardization.” It will involve transforming our data so that it has a mean of 0 and a standard deviation of 1. It’s useful when our data have different units or scales and ensures each feature contributes equally to the analysis we’re carrying out.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">Clustering methods are sensitive to the data scaling. I would like to scale the dataset. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">You’re right! Clustering methods, particularly distance-based ones like K-means, are sensitive to the scale of the features. To ensure that all features have the same scale and contribute equally to the clustering process, we need to perform data scaling. A common approach is to use standardization (also known as z-score scaling) or normalization. Here, I’ll show you how to use standardization to scale the dataset: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Step 8: Data Scaling using Standardization</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-comment"># Initialize the StandardScaler</span>
scaler = StandardScaler()
<span class="hljs-comment"># Scale the dataset (TotalAmount and TotalTransactions)</span>
df_scaled = scaler.fit_transform(df_segmentation)
<span class="hljs-comment"># Convert the scaled array back to a DataFrame</span>
df_scaled = pd.DataFrame(df_scaled, columns=df_segmentation.columns)
<span class="hljs-comment"># Display the first few rows of the scaled DataFrame</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">Scaled DataFrame:"</span>)
<span class="hljs-built_in">print</span>(df_scaled.head())
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We import <code class="inlinecode">StandardScaler</code> from the <code class="inlinecode">sklearn.preprocessing</code> module, which will help us perform the standardization. </li>
      <li class="bulletlist1">We initialize the <code class="inlinecode">StandardScaler</code> as <code class="inlinecode">scaler</code>. </li>
      <li class="bulletlist1">We use <code class="inlinecode">scaler.fit_transform()</code> to perform standardization on the <code class="inlinecode">df_segmentation</code> DataFrame, which contains the <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code> columns. </li>
      <li class="bulletlist1">The result of the <code class="inlinecode">fit_transform()</code> operation is an array, so we convert it back to a DataFrame using <code class="inlinecode">pd.DataFrame()</code> with the original column names. </li>
    </ol>
    <p class="normal1">After scaling, both <code class="inlinecode">TotalAmount</code> and <code class="inlinecode">TotalTransactions</code> will have a mean of 0 and a standard deviation of 1, which helps in the clustering process, ensuring that no single feature <a id="_idIndexMarker735" class="calibre3"/>dominates the clustering algorithm due to a larger scale. </p>
    <p class="normal1">Now, you have a scaled DataFrame <code class="inlinecode">df_scaled</code>, which is ready to be used for customer segmentation using clustering methods.</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Running code for step 8 will result in this ouput:</p>
    <pre class="programlisting"><code class="hljs-code">  
    Scaled DataFrame:  
       TotalAmount  TotalTransactions  
1.099421           0.425738  
1.051512          -0.745491  
-0.666340          -0.745491  
1.954997           1.987377  
-0.962557          -0.745491  
</code></pre>
    <h3 class="heading2" id="_idParaDest-355">Deciding on the number of clusters </h3>
    <p class="normal">K-means<a id="_idIndexMarker736" class="calibre3"/> is a popular <a id="_idIndexMarker737" class="calibre3"/>clustering algorithm used to partition a dataset into k clusters, where each data point belongs to a cluster of the nearest mean. The goal of using K-means is to identify patterns, by grouping similar data points together. You also want to reduce the complexity of your dataset. Other benefits include improved analysis.</p>
    <p class="normal1">To effectively apply a clustering method like K-means, I need to decide on the number of clusters. There are several methods we could use to find that number; let’s ask ChatGPT:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">How can I determine the optimum number of clusters for the dataset?</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Determining the optimum number of clusters, often referred to as the “elbow” point, is a crucial step when applying clustering methods like K-means. There are several methods you can use to find the appropriate number of clusters for your dataset. One of the most common approaches is the “Elbow Method,” but other methods like the “Silhouette Score” and “Gap Statistics” can also be helpful. Here’s how you can use these methods:</p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><strong class="screentext">Elbow Method</strong>: The Elbow Method<a id="_idIndexMarker738" class="calibre3"/> involves plotting the <strong class="screentext">within-cluster sum of squares</strong> (<strong class="screentext">WCSS</strong>) against the number of clusters. WCSS represents the sum of squared distances between each point and the centroid of its assigned cluster. The idea is to find the point on the plot where the decrease in WCSS starts to slow down, forming an “elbow” shape.
        <pre class="programlisting1"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt  
  
<span class="hljs-comment"># Create a list to store the WCSS values for different numbers of clusters wcss = []  </span>
  
<span class="hljs-comment"># Try different values of K (number of clusters) from 1 to a reasonable maximum for k in range(1, 11):     kmeans = KMeans(n_clusters=k, random_state=42)     kmeans.fit(df_scaled)     wcss.append(kmeans.inertia_)  # inertia_ contains the WCSS value   </span>
<span class="hljs-comment"># Plot the Elbow Method graph plt.figure(figsize=(8, 5))</span>
plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-attr">1</span>, <span class="hljs-attr">11</span>), wcss, marker=<span class="hljs-string">'o'</span>)
plt.xlabel(<span class="hljs-string">'Number of Clusters (K)'</span>)
plt.ylabel(<span class="hljs-string">'Within-Cluster Sum of Squares (WCSS)'</span>)
plt.title(<span class="hljs-string">'Elbow Method'</span>)
plt.grid(<span class="hljs-literal">True</span>)
plt.show()
</code></pre>
      </li>
    </ol>
    <p class="normal-one">In the Elbow Method<a id="_idIndexMarker739" class="calibre3"/> graph, look for the “elbow” point, which is the point where the curve starts to flatten. This point indicates the optimal number of clusters for your dataset. However, keep in mind that the Elbow Method is not always definitive, and the elbow point might not be very pronounced in some cases. </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="2"><strong class="screentext">Silhouette Score</strong>: The <a id="_idIndexMarker740" class="calibre3"/>Silhouette Score measures how well each data point fits its assigned<a id="_idIndexMarker741" class="calibre3"/> cluster and ranges from -1 to 1. A higher Silhouette Score indicates better-defined clusters. To use the Silhouette Score, loop through different values of K and calculate the average Silhouette Score for each value.
        <pre class="programlisting1"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_score
<span class="hljs-comment"># Create a list to store the Silhouette Scores for different numbers of clusters</span>
silhouette_scores = []
<span class="hljs-comment"># Try different values of K (number of clusters) from 2 to a reasonable maximum</span>
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-attr">2</span>, <span class="hljs-attr">11</span>):
    kmeans = KMeans(n_clusters=k, random_state=<span class="hljs-attr">42</span>)
    kmeans.fit(df_scaled)
    silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))
<span class="hljs-comment"># Plot the Silhouette Scores</span>
plt.figure(figsize=(<span class="hljs-attr">8</span>, <span class="hljs-attr">5</span>))
plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-attr">2</span>, <span class="hljs-attr">11</span>), silhouette_scores, marker=<span class="hljs-string">'o'</span>)
plt.xlabel(<span class="hljs-string">'Number of Clusters (K)'</span>)
plt.ylabel(<span class="hljs-string">'Average Silhouette Score'</span>)
plt.title(<span class="hljs-string">'Silhouette Score'</span>)
plt.grid(<span class="hljs-literal">True</span>)
plt.show()
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The optimal number of clusters is often associated with the highest Silhouette<a id="_idIndexMarker742" class="calibre3"/> Score. </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="3"><strong class="screentext">Gap Statistics</strong>: Gap Statistics<a id="_idIndexMarker743" class="calibre3"/> compare the WCSS of the clustering algorithm with the WCSS of randomly generated data. It helps determine if the clustering structure in your data is significant compared to random data. </li>
    </ol>
    <p class="normal-one">Implementing Gap Statistics is more involved, but libraries like <code class="inlinecode">scikit-learn-extra</code> provide<a id="_idIndexMarker744" class="calibre3"/> functions to calculate it: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn_extra.cluster <span class="hljs-keyword">import</span> KMeansExtra
<span class="hljs-comment"># Calculate Gap Statistics</span>
<span class="hljs-keyword">from</span> sklearn_extra.cluster <span class="hljs-keyword">import</span> KMeansExtra
kmeans_gap = KMeansExtra(
    n_clusters_max=<span class="hljs-attr">10</span>,
    random_state=<span class="hljs-attr">42</span>,
    n_init=<span class="hljs-attr">10</span>,
    max_iter=<span class="hljs-attr">300</span>,
    metric=<span class="hljs-string">"gap"</span>
)
kmeans_gap.fit(df_scaled)
<span class="hljs-comment"># Plot Gap Statistics</span>
plt.figure(figsize=(<span class="hljs-attr">8</span>, <span class="hljs-attr">5</span>))
plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-attr">1</span>, <span class="hljs-attr">11</span>), kmeans_gap.gap_values_, marker=<span class="hljs-string">'</span><span class="hljs-string">o'</span>)
plt.xlabel(<span class="hljs-string">'Number of Clusters (K)'</span>)
plt.ylabel(<span class="hljs-string">'Gap Value'</span>)
plt.title(<span class="hljs-string">'Gap Statistics'</span>)
plt.grid(<span class="hljs-literal">True</span>)
plt.show()
</code></pre>
    <p class="normal-one">Again, look for the point where the Gap Value starts to flatten, indicating the optimum number of clusters. </p>
    <p class="normal-one">Remember that the choice of the optimal number of clusters is often a subjective decision based on the insights you seek and the specific problem context. These methods can give you a starting point, but you may need to experiment and validate the clustering results based on your domain knowledge and business requirements. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Let’s execute the code for<a id="_idIndexMarker745" class="calibre3"/> the elbow method and the silhouette score and inspect the output:</p>
    <figure class="mediaobject"><img alt="A graph showing a number of clusters  Description automatically generated" src="../Images/B21232_16_07.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.7: Number of clusters using the elbow method</p>
    <figure class="mediaobject"><img alt="A graph with a line  Description automatically generated" src="../Images/B21232_16_08.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.8: Number of clusters using the silhouette score method</p>
    <p class="normal1">The elbow <a id="_idIndexMarker746" class="calibre3"/>method and the silhouette score method that we used to determine the optimal number of clusters suggested six clusters. We will use six clusters for our K-means model. Let’s create a ChatGPT prompt to generate a Customer Segmentation K-means clustering model with six clusters. </p>
    <h3 class="heading2" id="_idParaDest-356">Creating a clustering model</h3>
    <p class="normal">Now that <a id="_idIndexMarker747" class="calibre3"/>we’ve determined the number of clusters to use, let’s create a cluster model next.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">Please <a id="_idIndexMarker748" class="calibre3"/>help me to create a Customer Segmentation K-means clustering model with 6 clusters.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s create a Customer Segmentation K-means clustering model with 6 clusters using the scaled data <code class="inlinecode">df_scaled</code>. We’ll use the <code class="inlinecode">KMeans</code> class from scikit-learn to perform the clustering: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
<span class="hljs-comment"># Number of clusters</span>
num_clusters = <span class="hljs-attr">6</span>
<span class="hljs-comment"># Create the KMeans clustering model</span>
kmeans_model = KMeans(n_clusters=num_clusters, random_state=<span class="hljs-attr">42</span>)
<span class="hljs-comment"># Fit the model to the scaled data</span>
kmeans_model.fit(df_scaled)
<span class="hljs-comment"># Add the cluster labels to the DataFrame</span>
df_segmentation[<span class="hljs-string">'Cluster'</span>] = kmeans_model.labels_
<span class="hljs-comment"># Display the first few rows of the DataFrame with cluster assignments</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Customer Segmentation DataFrame with Cluster Assignments:"</span>)
<span class="hljs-built_in">print</span>(df_segmentation.head())
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We specify <code class="inlinecode">num_clusters</code> as 6, which will create 6 clusters for customer segmentation. </li>
      <li class="bulletlist1">We create the K-means clustering model using <code class="inlinecode">KMeans</code> with the desired number of clusters and set <code class="inlinecode">random_state</code> for reproducibility. </li>
      <li class="bulletlist1">The model is fitted to the scaled data <code class="inlinecode">df_scaled</code> using <code class="inlinecode">fit()</code>. </li>
      <li class="bulletlist1">We add the cluster labels to the original DataFrame <code class="inlinecode">df_segmentation</code> using <code class="inlinecode">kmeans_model.labels_</code>, and store it in a new column called ‘Cluster’. </li>
    </ol>
    <p class="normal1">The resulting <code class="inlinecode">df_segmentation</code> DataFrame will now include the ‘Cluster’ column indicating the cluster assignment for each customer based on their ‘TotalAmount’ and ‘TotalTransactions’. Each customer will be associated with one of the six clusters. </p>
    <p class="normal1">Keep in mind that the interpretation of the clusters will depend on the context of your data and your domain knowledge. You may want to analyze the characteristics of each cluster to gain insights about different customer segments and tailor your marketing or business strategies accordingly. Visualization techniques, such as scatter plots or cluster profiles, can <a id="_idIndexMarker749" class="calibre3"/>be helpful in understanding and interpreting the results of the customer segmentation. </p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_09.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.9: Output from running df.segmentation() </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <h3 class="heading2" id="_idParaDest-357">Visualize clustering results</h3>
    <p class="normal">Now, we can<a id="_idIndexMarker750" class="calibre3"/> visualize the results of the clustering. Let’s create the next prompt:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like to visualize the results of the clustering. Please create a scatter plot with the following specifications: the horizontal axis should be “Total Amount”, the vertical axis should be “Total Transactions”, and the color of the dots should be determined by the cluster number. Please also add a legend to the plot. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s <a id="_idIndexMarker751" class="calibre3"/>create a scatter plot to visualize the results of the clustering. We’ll use the <code class="inlinecode">matplotlib </code>library for plotting, and the <code class="inlinecode">seaborn</code> library for color mapping and a more visually appealing plot. If you don’t have the <code class="inlinecode">seaborn</code> library installed, you can install it using <code class="inlinecode">pip install seaborn</code>. </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># Scatter plot with cluster assignments</span>
plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">6</span>))
sns.scatterplot(x=<span class="hljs-string">'TotalAmount'</span>, y=<span class="hljs-string">'</span><span class="hljs-string">TotalTransactions'</span>, hue=<span class="hljs-string">'Cluster'</span>, data=df_segmentation, palette=<span class="hljs-string">'viridis'</span>, s=<span class="hljs-attr">50</span>)
plt.xlabel(<span class="hljs-string">'Total Amount'</span>)
plt.ylabel(<span class="hljs-string">'Total Transactions'</span>)
plt.title(<span class="hljs-string">'Customer Segmentation by K-Means Clustering'</span>)
plt.legend(title=<span class="hljs-string">'Cluster'</span>, bbox_to_anchor=(<span class="hljs-attr">1.05</span>, <span class="hljs-attr">1</span>), loc=<span class="hljs-string">'</span><span class="hljs-string">upper left'</span>)
plt.grid(<span class="hljs-literal">True</span>)
plt.show()
</code></pre>
    <p class="normal1">In the code above: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We use <code class="inlinecode">seaborn.scatterplot()</code> to create the scatter plot. The ‘TotalAmount’ is plotted on the horizontal axis (<code class="inlinecode">x</code>), the ‘TotalTransactions’ on the vertical axis (<code class="inlinecode">y</code>), and the ‘Cluster’ column is used for color mapping using the <code class="inlinecode">hue</code> parameter. </li>
      <li class="bulletlist1">We set the color palette to ‘viridis’ using the <code class="inlinecode">palette</code> parameter, which will provide distinct colors for each cluster. The <code class="inlinecode">s=50</code> parameter sets the size of the dots in the scatter plot to 50 for better visibility. </li>
      <li class="bulletlist1">We add axis labels, a plot title, a legend, and grid lines to improve the plot’s readability.</li>
    </ol>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">The resulting scatter plot will display each customer as a dot in the 2-dimensional space defined by ‘TotalAmount’ and ‘TotalTransactions’. The dots will be colored according to their cluster assignment, allowing you to visually identify the different customer segments. The <a id="_idIndexMarker752" class="calibre3"/>legend will help you understand which color corresponds to which cluster number. </p>
    <figure class="mediaobject"><img alt="A graph of a customer segmentation  Description automatically generated" src="../Images/B21232_16_10.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.10: Data points divided up into six clusters</p>
    <h3 class="heading2" id="_idParaDest-358">Final thoughts on clustering and the prompting process</h3>
    <p class="normal">We have completed Customer Segmentation Clustering with the assistance of ChatGPT. To achieve our goal, we have used several different types of prompts. </p>
    <p class="normal1">The prompt often begins by describing the dataset to be used, such as the source, columns, and their types, which sets a high-level context. ChatGPT then requests assistance in implementing a Customer Segmentation clustering model using a specific algorithm, in this case, K-means clustering. To summarize the prompts used, we can see that it blends a set of different techniques, which mirrors how you would carry out data science without an AI assistant. You can see below the different types of prompt types and how they helped with the clustering process:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Step-by-step instructions</strong>: These provided a step-by-step guide on how to approach the problem, which includes loading and understanding the dataset, data preprocessing (removing missing values, negative values, and unused columns), and feature engineering (creating new features like Amount, TotalAmount, and TotalTransactions). </li>
      <li class="bulletlist1"><strong class="screentext">Clustering algorithm choice</strong>: This specified the choice of the clustering algorithm, which is K-means clustering in this case, along with the number of clusters to be used for segmentation. </li>
      <li class="bulletlist1"><strong class="screentext">Outlier handling</strong>: addressed the sensitivity of clustering algorithms to outliers and requests to remove<a id="_idIndexMarker753" class="calibre3"/> outliers from the data using the <strong class="screentext">interquartile range</strong> (<strong class="screentext">IQR</strong>) method. </li>
      <li class="bulletlist1"><strong class="screentext">Data scaling</strong>: This emphasized the importance of scaling the data to ensure clustering accuracy and instructs to use standardization to scale the features. </li>
      <li class="bulletlist1"><strong class="screentext">Cluster visualization</strong>: This asked for visualization techniques to display the results of the clustering model, typically through scatter plots, with TotalAmount on the x-axis, TotalTransactions on the y-axis, and color-coded points representing different clusters. </li>
      <li class="bulletlist1"><strong class="screentext">Optimal number of clusters</strong>: The prompt guides in determining the optimal number of clusters using the elbow method and silhouette score, which helps in finding the right number of clusters that best represent the data. </li>
      <li class="bulletlist1"><strong class="screentext">Combined script request</strong>: The prompt requests to combine all the generated code into a single script, including detailed explanations as comments, to provide a comprehensive solution for implementing the Customer Segmentation clustering model. </li>
    </ul>
    <h1 class="heading" id="_idParaDest-359">Product clustering for an e-commerce project</h1>
    <p class="normal">In this task, we <a id="_idIndexMarker754" class="calibre3"/>will perform product clustering using the same e-commerce dataset. The goal is to create an unsupervised learning model to cluster products based on their textual descriptions using Python and common libraries. </p>
    <h2 class="heading1" id="_idParaDest-360">Your initial prompt: Set context </h2>
    <p class="normal">As usual, when using <a id="_idIndexMarker755" class="calibre3"/>ChatGPT, it’s important to set a high-level context. For data science, high-level context means describing things like the data used (e-commerce data), what we’re trying to achieve (get data insights), and how we’re looking to achieve it (clustering). </p>
    <p class="normal1">Below is <a id="_idIndexMarker756" class="calibre3"/>an example of a prompt providing all the suggested context we just described in the preceding paragraph:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like you to act as an experienced data scientist and help me implement Product Categorization model using clustering. The dataset is e-commerce data from the UCI Machine Learning Repository. It is a transnational dataset that contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer. The dataset contains the following columns: <code class="inlinecode">InvoiceNo</code> (integer), <code class="inlinecode">StockCode</code> (string), <code class="inlinecode">Description</code> (string), <code class="inlinecode">Quantity</code> (integer), <code class="inlinecode">InvoiceDate</code> (date), <code class="inlinecode">UnitPrice</code> (decimal), <code class="inlinecode">CustomerID</code> (integer), <code class="inlinecode">Country</code> (string). Can you suggest the steps needed to perform Product Categorization using the data from a csv file. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1">You should receive a response like the below response:</p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! Product categorization using clustering is a great approach to group similar products together based on their attributes. Here’s a step-by-step guide to perform product categorization using the given e-commerce dataset:</p>
    <p class="normal1">Step 1: Load and Preprocess the Data </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Load the dataset from the CSV file into a suitable data structure, like a Pandas DataFrame in Python. </li>
      <li class="bulletlist1">Inspect the data to understand its structure, missing values, and overall characteristics. </li>
      <li class="bulletlist1">Preprocess the data, which may involve handling missing values, data type conversions, and data cleaning. </li>
    </ol>
    <p class="normal1">Step 2: Feature Engineering </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Select relevant features for clustering. In this case, you can consider features like ‘StockCode’, ‘Description’, ‘Quantity’, ‘UnitPrice’, etc. Customer-related features like ‘CustomerID’ might not be directly relevant for product categorization. </li>
      <li class="bulletlist1">Create a feature matrix, where each row corresponds to a product and each column corresponds to a feature. </li>
    </ol>
    <p class="normal1">Step 3: Text Data Preprocessing </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">For text-based features like ‘Description’, perform text preprocessing steps such as lowercasing, tokenization, removing punctuation, and stopwords. </li>
      <li class="bulletlist1">Convert the text data into numerical format using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings. </li>
    </ol>
    <p class="normal1">Step 4: Choose Clustering Algorithm </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Select an appropriate clustering algorithm. Common choices include K-means, Hierarchical clustering, and DBSCAN. </li>
      <li class="bulletlist1">Tune hyperparameters if needed. </li>
    </ol>
    <p class="normal1">Step 5: Feature Scaling </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Scale the numerical features to have similar ranges. This is important for algorithms like K-means that are distance-based. </li>
    </ol>
    <p class="normal1">Step 6: Apply Clustering Algorithm </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Apply the chosen clustering algorithm to the preprocessed data. </li>
      <li class="bulletlist1">Determine the optimal number of clusters. You can use techniques like the Elbow Method, Silhouette Score, or Gap Statistic. </li>
    </ol>
    <p class="normal1">Step 7: Interpret Clusters </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Once you have clusters, analyze the products within each cluster to understand the common characteristics. </li>
      <li class="bulletlist1">Assign meaningful labels to the clusters based on your analysis. These labels will represent the product categories. </li>
    </ol>
    <p class="normal1">Step 8: Visualize Clusters (Optional) </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Use visualization techniques such as scatter plots, t-SNE, or PCA to visualize the clusters in a reduced dimensional space. </li>
    </ol>
    <p class="normal1">Step 9: Assign Categories to Products </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Assign<a id="_idIndexMarker757" class="calibre3"/> each product to its respective category based on the cluster it belongs to. </li>
    </ol>
    <p class="normal1">Step 10: Evaluate and Refine </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Evaluate the quality of your clustering results. You can use domain knowledge or external validation metrics if available. </li>
      <li class="bulletlist1">Refine the process by adjusting preprocessing steps, algorithm parameters, or features as needed. </li>
    </ol>
    <p class="normal1">Remember<a id="_idIndexMarker758" class="calibre3"/> that clustering is an iterative process, and the quality of your results may depend on various factors such as data quality, feature selection, and algorithm choice. It’s also important to consider the business context and domain knowledge when interpreting and labeling the clusters. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Having reviewed the steps in the response, you can now proceed with the next prompt. </p>
    <h2 class="heading1" id="_idParaDest-361">Load and preprocess the data</h2>
    <p class="normal">Now that <a id="_idIndexMarker759" class="calibre3"/>we have a series of steps described to us how to perform clustering, we want to carry out the first step from the response namely loading and preprocessing the data.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like to do it step by step. Please guide me through Step 1: Load and Preprocess the Data. My data URL is “../Datasets/data.csv” and the CSV file is using latin1 encoding. I would use the column ‘Description’ for clustering to identify product categories.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1">You should receive a response like the following text:</p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Of course! Let’s start with Step 1: Load and Preprocess the Data. </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-comment"># Load the data from the CSV file using Latin1 encoding</span>
data_url = <span class="hljs-string">"../Datasets/data.csv"</span>
df = pd.read_csv(data_url, encoding=<span class="hljs-string">'latin1'</span>)
<span class="hljs-comment"># Display basic information about the dataset</span>
<span class="hljs-built_in">print</span>(df.info())
<span class="hljs-comment"># Display the first few rows of the dataset</span>
<span class="hljs-built_in">print</span>(df.head())
</code></pre>
    <p class="normal1">This code will <a id="_idIndexMarker760" class="calibre3"/>load the dataset from the provided CSV file using the Latin1 encoding. It will then display basic information about the dataset, such as the data types of each column and the number of non-null values. </p>
    <p class="normal1">Additionally, it will display the first few rows of the dataset to give you a sense of the data’s structure. </p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_11.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.11: First few rows of the dataset</p>
    <p class="normal1">Now, let’s<a id="_idIndexMarker761" class="calibre3"/> preprocess the data and focus on the ‘Description’ column for clustering. </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Drop rows with missing 'Description' values</span>
df = df.dropna(subset=[<span class="hljs-string">'Description'</span>])
<span class="hljs-comment"># Display basic statistics of numerical columns</span>
<span class="hljs-built_in">print</span>(df.describe())
<span class="hljs-comment"># Display unique values in the 'Description' column</span>
<span class="hljs-built_in">print</span>(df[<span class="hljs-string">'Description'</span>].nunique())
<span class="hljs-comment"># Display the top 10 most common descriptions</span>
<span class="hljs-built_in">print</span>(df[<span class="hljs-string">'Description'</span>].value_counts().head(<span class="hljs-attr">10</span>))
</code></pre>
    <p class="normal1">In this code snippet, we first drop rows with missing ‘Description’ values since we’ll be using this column for clustering. Next, we display basic statistics of the numerical columns to get an overview of the data’s distribution. After that, we count the number of unique descriptions to understand the variety of products. Finally, we display the top 10 most common descriptions to see the most frequent products in the dataset. </p>
    <p class="normal1">At this point, you have loaded the data, dropped missing values in the ‘Description’ column, and gained some insights into the data’s characteristics. The next step will involve text data <a id="_idIndexMarker762" class="calibre3"/>preprocessing for the ‘Description’ column. Let me know if you’re ready to move on to Step 3, or if you have any questions about the current steps! </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">After copying the code to a Jupyter Notebook, we find that it works perfectly fine, and we learn that 541,909 dataset records contain 4,223 unique product descriptions. The next step is to perform text data preprocessing for the ‘Description’ column. Let’s proceed with the next prompt. </p>
    <h2 class="heading1" id="_idParaDest-362">Feature engineering and text data preprocessing</h2>
    <p class="normal">At this point, the data has <a id="_idIndexMarker763" class="calibre3"/>been read and we know the size of the dataset as well. Let’s carry on with the next two steps, feature engineering and text data preprocessing:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">The dataset contains 541909 entries and there are 4223 unique product descriptions. I would like to drop duplicate ‘Descriptions’ and perform other text data preprocessing and feature engineering. Please create a script with comments and include libraries and <code class="inlinecode">nltk</code> resources to download.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure, I’ll provide you with a <a id="_idIndexMarker764" class="calibre3"/>script that covers text data preprocessing and feature engineering steps. Let’s get started: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords
<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-comment"># Download NLTK resources (only needed once)</span>
nltk.download(<span class="hljs-string">'punkt'</span>)
nltk.download(<span class="hljs-string">'stopwords'</span>)
<span class="hljs-comment"># Load the data from the CSV file using Latin1 encoding</span>
data_url = <span class="hljs-string">"../Datasets/data.csv"</span>
df = pd.read_csv(data_url, encoding=<span class="hljs-string">'latin1'</span>)
<span class="hljs-comment"># Drop rows with missing 'Description' values</span>
df = df.dropna(subset=[<span class="hljs-string">'Description'</span>])
<span class="hljs-comment"># Drop duplicate 'Description' entries</span>
df = df.drop_duplicates(subset=[<span class="hljs-string">'Description'</span>])
<span class="hljs-comment"># Text preprocessing and feature engineering</span>
<span class="hljs-comment"># Convert descriptions to lowercase</span>
df[<span class="hljs-string">'Description'</span>] = df[<span class="hljs-string">'Description'</span>].<span class="hljs-built_in">str</span>.lower()
<span class="hljs-comment"># Tokenization and removal of punctuation</span>
df[<span class="hljs-string">'Description'</span>] = df[<span class="hljs-string">'Description'</span>].apply(word_tokenize)
<span class="hljs-comment"># Remove stopwords</span>
stop_words = <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">'english'</span>))
df[<span class="hljs-string">'Description'</span>] = df[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-keyword">lambda</span> x: [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words])
<span class="hljs-comment"># Convert tokenized descriptions back to strings</span>
df[<span class="hljs-string">'Description'</span>] = df[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-string">' '</span>.join)
<span class="hljs-comment"># TF-IDF vectorization</span>
tfidf_vectorizer = TfidfVectorizer(max_features=<span class="hljs-attr">1000</span>)  <span class="hljs-comment"># You can adjust the number of features</span>
tfidf_matrix = tfidf_vectorizer.fit_transform(df[<span class="hljs-string">'Description'</span>])
<span class="hljs-comment"># Convert TF-IDF matrix to a DataFrame</span>
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
<span class="hljs-comment"># Display the TF-IDF DataFrame</span>
<span class="hljs-built_in">print</span>(tfidf_df.head())
</code></pre>
    <p class="normal1">In this script: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">We import necessary <a id="_idIndexMarker765" class="calibre3"/>libraries including <code class="inlinecode">pandas</code>, <code class="inlinecode">nltk</code>, and <code class="inlinecode">TfidfVectorizer</code> from <code class="inlinecode">sklearn.feature_extraction.text</code>. </li>
      <li class="bulletlist1">We download NLTK resources (<code class="inlinecode">punkt</code> for tokenization and <code class="inlinecode">stopwords</code> for removing stopwords) if they are not already downloaded. </li>
      <li class="bulletlist1">We load the dataset and drop rows with missing ‘Description’ values and duplicate descriptions. </li>
      <li class="bulletlist1">We perform text preprocessing steps on the ‘Description’ column: converting to lowercase, tokenizing, removing punctuation, and stopwords. </li>
      <li class="bulletlist1">We convert the tokenized descriptions back to strings. </li>
      <li class="bulletlist1">We use TF-IDF vectorization to convert the preprocessed text data into numerical features. You can adjust the <code class="inlinecode">max_features</code> parameter to control the number of features. </li>
      <li class="bulletlist1">The TF-IDF matrix is converted to a DataFrame for further analysis. </li>
    </ol>
    <p class="normal1">This script will preprocess the text data and perform feature engineering using TF-IDF vectorization. The resulting TF-IDF matrix can be used as input for clustering algorithms in the subsequent steps. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">After pasting the<a id="_idIndexMarker766" class="calibre3"/> script to Jupyter Notebook, we find that it works perfectly fine, and we learn that the TF-IDF matrix has 4,223 rows (one for each unique product description) and 1,000 columns (one for each feature).</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21232_16_12.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.12: TF-IDF matrix</p>
    <h3 class="heading2" id="_idParaDest-363">Feature engineering</h3>
    <p class="normal">We will continue <a id="_idIndexMarker767" class="calibre3"/>with feature engineering. Let’s visualize the most frequent features in the TF-IDF matrix.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like to visualize the top 100 most frequent features from the TF-IDF matrix. Please create a bar chart with words on the vertical axis. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! To visualize the top 100 most frequent features from the TF-IDF matrix, you can create a bar chart using libraries like <code class="inlinecode">matplotlib</code>. Here’s how you can do it: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># Calculate the sum of TF-IDF scores for each feature</span>
feature_sum = tfidf_df.<span class="hljs-built_in">sum</span>()
<span class="hljs-comment"># Get the top 100 features by frequency</span>
top_features = feature_sum.nlargest(<span class="hljs-attr">100</span>)
<span class="hljs-comment"># Create a bar chart</span>
plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">12</span>))
top_features.plot(kind=<span class="hljs-string">'barh'</span>)
plt.xlabel(<span class="hljs-string">'TF-IDF Score Sum'</span>)
plt.ylabel(<span class="hljs-string">'Words'</span>)
plt.title(<span class="hljs-string">'Top 100 Most Frequent Features from TF-IDF Matrix'</span>)
plt.tight_layout()
plt.show()
</code></pre>
    <p class="normal1">This script calculates the sum of TF-IDF scores for each feature and selects the top 100 features by frequency. It then creates a horizontal bar chart using <code class="inlinecode">matplotlib</code> to visualize the top features. Adjust the figure size, labels, and other parameters as needed to customize the visualization to your preferences. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Running this code in Jupyter Notebook will produce the following bar chart: </p>
    <figure class="mediaobject"><img alt="A graph of a number of people  Description automatically generated with medium confidence" src="../Images/B21232_16_13.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.13: Most frequent features</p>
    <p class="normal1">Let’s analyze this chart and proceed with the next prompt. </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">We can see that the features contain numbers and words representing colors, like red, blue, and other colors. These words will not be useful for product categorization. Can you create a list of words that can represent color. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! Here’s a list of common words that represent colors. You can use this list to filter out color-related words from your features before performing product categorization. </p>
    <pre class="programlisting"><code class="hljs-code">color_words = [
    <span class="hljs-string">'</span><span class="hljs-string">black'</span>, <span class="hljs-string">'white'</span>, <span class="hljs-string">'grey'</span>, <span class="hljs-string">'gray'</span>, <span class="hljs-string">'red'</span>, <span class="hljs-string">'blue'</span>, <span class="hljs-string">'green'</span>, <span class="hljs-string">'yellow'</span>,
    <span class="hljs-string">'orange'</span>, <span class="hljs-string">'purple'</span>, <span class="hljs-string">'</span><span class="hljs-string">pink'</span>, <span class="hljs-string">'brown'</span>, <span class="hljs-string">'beige'</span>, <span class="hljs-string">'gold'</span>, <span class="hljs-string">'silver'</span>,
    <span class="hljs-string">'indigo'</span>, <span class="hljs-string">'violet'</span>, <span class="hljs-string">'turquoise'</span>, <span class="hljs-string">'teal'</span>, <span class="hljs-string">'aqua'</span>, <span class="hljs-string">'</span><span class="hljs-string">navy'</span>, <span class="hljs-string">'olive'</span>,
    <span class="hljs-string">'maroon'</span>, <span class="hljs-string">'coral'</span>, <span class="hljs-string">'plum'</span>, <span class="hljs-string">'salmon'</span>, <span class="hljs-string">'magenta'</span>, <span class="hljs-string">'cyan'</span>, <span class="hljs-string">'khaki'</span>,
    <span class="hljs-string">'ivory'</span>, <span class="hljs-string">'</span><span class="hljs-string">chartreuse'</span>, <span class="hljs-string">'crimson'</span>, <span class="hljs-string">'fuchsia'</span>, <span class="hljs-string">'lavender'</span>, <span class="hljs-string">'lime'</span>,
    <span class="hljs-string">'tan'</span>, <span class="hljs-string">'sienna'</span>, <span class="hljs-string">'orchid'</span>, <span class="hljs-string">'periwinkle'</span>, <span class="hljs-string">'peach'</span>, <span class="hljs-string">'</span><span class="hljs-string">thistle'</span>
]
</code></pre>
    <p class="normal1">You can use this list to filter out color-related words from your features during the text preprocessing step. For example: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Remove color-related words from descriptions</span>
df[<span class="hljs-string">'Description'</span>] = df[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-keyword">lambda</span> x: [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> color_words])
</code></pre>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">By removing color-related words, you can help improve the quality of your product categorization by focusing on more meaningful features.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">Modify the script to remove <code class="inlinecode">color_words</code> and words that are less than three characters long. Refactor code to encapsulate data preprocessing and vectorisation into a function <code class="inlinecode">preprocess_and_vectorize_data</code>. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! Here’s the modified script with the requested changes, including encapsulating the data preprocessing and vectorization steps into a function:</p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords
<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># Download NLTK resources (only needed once)</span>
nltk.download(<span class="hljs-string">'punkt'</span>)
nltk.download(<span class="hljs-string">'stopwords'</span>)
<span class="hljs-comment"># List of color-related words</span>
color_words = [
    <span class="hljs-string">'black'</span>, <span class="hljs-string">'white'</span>, <span class="hljs-string">'grey'</span>, <span class="hljs-string">'gray'</span>, <span class="hljs-string">'red'</span>, <span class="hljs-string">'blue'</span>, <span class="hljs-string">'green'</span>, <span class="hljs-string">'yellow'</span>,
    <span class="hljs-string">'orange'</span>, <span class="hljs-string">'purple'</span>, <span class="hljs-string">'pink'</span>, <span class="hljs-string">'brown'</span>, <span class="hljs-string">'beige'</span>, <span class="hljs-string">'gold'</span>, <span class="hljs-string">'silver'</span>,
    <span class="hljs-string">'indigo'</span>, <span class="hljs-string">'violet'</span>, <span class="hljs-string">'turquoise'</span>, <span class="hljs-string">'teal'</span>, <span class="hljs-string">'aqua'</span>, <span class="hljs-string">'navy'</span>, <span class="hljs-string">'olive'</span>,
    <span class="hljs-string">'maroon'</span>, <span class="hljs-string">'coral'</span>, <span class="hljs-string">'plum'</span>, <span class="hljs-string">'salmon'</span>, <span class="hljs-string">'magenta'</span>, <span class="hljs-string">'cyan'</span>, <span class="hljs-string">'khaki'</span>,
    <span class="hljs-string">'ivory'</span>, <span class="hljs-string">'chartreuse'</span>, <span class="hljs-string">'crimson'</span>, <span class="hljs-string">'fuchsia'</span>, <span class="hljs-string">'lavender'</span>, <span class="hljs-string">'lime'</span>,
    <span class="hljs-string">'tan'</span>, <span class="hljs-string">'sienna'</span>, <span class="hljs-string">'orchid'</span>, <span class="hljs-string">'periwinkle'</span>, <span class="hljs-string">'peach'</span>, <span class="hljs-string">'thistle'</span>
]
<span class="hljs-keyword">def</span> <span class="hljs-meta">preprocess_and_vectorize_data</span>(<span>data</span>):
    <span class="hljs-comment"># Drop rows with missing 'Description' values</span>
    data = data.dropna(subset=[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Drop duplicate 'Description' entries</span>
    data = data.drop_duplicates(subset=[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Convert descriptions to lowercase</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].<span class="hljs-built_in">str</span>.lower()
    <span class="hljs-comment"># Tokenization and removal of punctuation</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(word_tokenize)
    <span class="hljs-comment"># Remove stopwords and color-related words</span>
    stop_words = <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">'english'</span>))
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-keyword">lambda</span> x: [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(word) &gt; <span class="hljs-attr">2</span> <span class="hljs-keyword">and</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> color_words])
    <span class="hljs-comment"># Convert tokenized descriptions back to strings</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-string">' '</span>.join)
    <span class="hljs-comment"># TF-IDF vectorization</span>
    tfidf_vectorizer = TfidfVectorizer(max_features=<span class="hljs-attr">1000</span>)  <span class="hljs-comment"># You can adjust the number of features</span>
    tfidf_matrix = tfidf_vectorizer.fit_transform(data[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Convert TF-IDF matrix to a DataFrame</span>
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    
    <span class="hljs-keyword">return</span> tfidf_df
<span class="hljs-comment"># Load the data from the CSV file using Latin1 encoding</span>
data_url = <span class="hljs-string">"../Datasets/data.csv"</span>
df = pd.read_csv(data_url, encoding=<span class="hljs-string">'latin1'</span>)
<span class="hljs-comment"># Preprocess and vectorize the data</span>
tfidf_df = preprocess_and_vectorize_data(df)
<span class="hljs-comment"># Calculate the sum of TF-IDF scores for each feature</span>
feature_sum = tfidf_df.<span class="hljs-built_in">sum</span>()
<span class="hljs-comment"># Get the top 100 features by frequency</span>
top_features = feature_sum.nlargest(<span class="hljs-attr">100</span>)
<span class="hljs-comment"># Create a bar chart</span>
plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">12</span>))
top_features.plot(kind=<span class="hljs-string">'barh'</span>)
plt.xlabel(<span class="hljs-string">'TF-IDF Score Sum'</span>)
plt.ylabel(<span class="hljs-string">'Words'</span>)
plt.title(<span class="hljs-string">'Top 100 Most Frequent Features from TF-IDF Matrix'</span>)
plt.tight_layout()
plt.show()
</code></pre>
    <p class="normal1">This script encapsulates the data preprocessing and vectorization steps into the <code class="inlinecode">preprocess_and_vectorize_data</code> function. The function takes the raw data as input, performs the required preprocessing, and returns the TF-IDF matrix. The rest of the script remains similar to before. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">After running the script, we can visualize the updated set of the most frequent words from the TF-IDF matrix output. </p>
    <figure class="mediaobject"><img alt="A graph of a number of people  Description automatically generated with medium confidence" src="../Images/B21232_16_14.png" class="calibre19"/></figure>
    <p class="packt_figref"> Figure 16.14: Most frequent features that are not colors</p>
    <p class="normal1">Let’s proceed with our feature engineering. We will use ChatGPT to suggest options that can be explored with TF-IDF vectorization.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">The TF-IDF vectorization is initialized with <code class="inlinecode">max_features</code> parameter. What are other initialization options?</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">When initializing the TF-IDF vectorizer in scikit-learn, you can set several parameters to customize its behavior.</p>
    <p class="normal1">In addition to the <code class="inlinecode">max_features</code> parameter, here are some other important parameters you can consider: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><code class="inlinecode">max_df</code> and <code class="inlinecode">min_df</code>: These parameters control the threshold for the document frequency of terms. For example, you can set <code class="inlinecode">max_df</code> to ignore terms that appear in a high percentage of documents, and <code class="inlinecode">min_df</code> to ignore terms that appear in a low percentage of documents. </li>
      <li class="bulletlist1"><code class="inlinecode">ngram_range</code>: This parameter specifies the range of n-grams to consider during tokenization. For example, setting <code class="inlinecode">ngram_range=(1, 2)</code> will consider both individual words and pairs of consecutive words (bigrams). </li>
      <li class="bulletlist1"><code class="inlinecode">stop_words</code>: You can provide a list of stopwords to be ignored during tokenization. This can be a custom list or one of the built-in lists like <code class="inlinecode">'english'</code>. </li>
      <li class="bulletlist1"><code class="inlinecode">tokenizer</code>: You can specify a custom tokenizer function if you want to perform additional text preprocessing or tokenization. </li>
      <li class="bulletlist1"><code class="inlinecode">use_idf</code>: This parameter controls whether to use the inverse-document-frequency (IDF) re-weighting of the term frequencies. </li>
      <li class="bulletlist1"><code class="inlinecode">smooth_idf</code>: This parameter controls whether to add a smoothing term to the IDF to avoid division by zero errors. </li>
      <li class="bulletlist1"><code class="inlinecode">sublinear_tf</code>: If set to <code class="inlinecode">True</code>, it applies sublinear scaling to the term frequency, which can help to prevent very frequent terms from dominating the calculation. </li>
    </ol>
    <p class="normal1">Here’s an example of how you can initialize the TF-IDF vectorizer with some of these parameters: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(
    max_features=<span class="hljs-attr">1000</span>,
    max_df=<span class="hljs-attr">0.8</span>,
    min_df=<span class="hljs-attr">0.05</span>,
    ngram_range=(<span class="hljs-attr">1</span>, <span class="hljs-attr">2</span>),
    stop_words=<span class="hljs-string">'english'</span>,
    use_idf=<span class="hljs-literal">True</span>,
    smooth_idf=<span class="hljs-literal">True</span>,
    sublinear_tf=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Feel free to experiment with these parameters based on your specific dataset and goals. The choice of parameters may depend on the nature of the text data and the desired outcomes of your clustering or categorization task. </p>
    <h2 class="heading1" id="_idParaDest-364">Choose clustering algorithm </h2>
    <p class="normal">There are <a id="_idIndexMarker768" class="calibre3"/>quite a few clustering algorithms we could choose. There is, for example, K-means, that we used for another use case; there’s also hierarchical clustering, <strong class="screentext">DBSCAN</strong> (<strong class="screentext">Density-based clustering algorithm</strong>), <strong class="screentext">Gaussian mixture models</strong> (<strong class="screentext">GMMs</strong>), and agglomerative clustering. We’re going to go with K-means, but why is that? The reason is that we’re working with customers and want to divide them up into groups based on purchasing behavior, demographic properties, and so on. Another reason for choosing K-means is its simplicity and being able to work with large datasets. </p>
    <p class="normal1">You might wonder when to apply the other mentioned clustering algorithms in the context of e-commerce. Well, let’s look at some interesting areas in this domain:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Product recommendation</strong>: Here, it would be more appropriate to use hierarchical clustering as products usually come in hierarchies and we want to understand the relationship between products.</li>
      <li class="bulletlist1"><strong class="screentext">Anomaly detection</strong>: Another important area is anomaly detection, especially fraudulent transactions. Here, you want to understand unusual customer behavior to identify outliers; for that use case, DBSCAN is the best algorithm to go with.</li>
      <li class="bulletlist1"><strong class="screentext">Personalization</strong>: You want to ensure customers feel like they have an experience uniquely fitted to their behaviors on the website. GMM is a great<a id="_idIndexMarker769" class="calibre3"/> choice for this use case when a customer’s behavior is a bit more complex.</li>
    </ul>
    <h2 class="heading1" id="_idParaDest-365">Feature scaling </h2>
    <p class="normal">Feature scaling<a id="_idIndexMarker770" class="calibre3"/> and TF-IDF are both preprocessing techniques and it only makes sense to choose one of them. Which one to choose depends on what we want to use it for. In the context of e-commerce, here’s how you could reason:</p>
    <ul class="calibre15">
      <li class="bulletlist">Choose feature scaling if you’re dealing with numerical features like customer age, purchase amounts, and purchase frequency (e.g., grouping customers based on their purchasing behavior).</li>
      <li class="bulletlist1">Choose the TF-IDF matrix if you’re looking at other aspects of e-commerce like customer reviews, product descriptions, and search queries.</li>
    </ul>
    <p class="normal1">We can skip this step as we are using a TF-IDF matrix that is already scaled. </p>
    <h2 class="heading1" id="_idParaDest-366">Apply clustering algorithm </h2>
    <p class="normal">Next, let’s<a id="_idIndexMarker771" class="calibre3"/> apply our chosen clustering algorithm:</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like to use english <code class="inlinecode">stop_words</code>, no <code class="inlinecode">max_features</code><strong class="bold-italic">,</strong> and pass <code class="inlinecode">min_df</code> to <code class="inlinecode">preprocess_and_vectorize_data</code> function to set minimum number of documents the feature appears in. I would like to try a range of values for <code class="inlinecode">min_df: 5, 10, 20, 30</code> and compare the silhouette score from the K-means output. I would like to visualize the result. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s modify the script to accommodate your requirements for <code class="inlinecode">min_df</code> and the comparison of silhouette scores for different values of <code class="inlinecode">min_df</code> using <code class="inlinecode">KMeans</code> clustering. Here’s the updated script: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-meta">preprocess_and_vectorize_data</span>(<span>data, min_df</span>):
    <span class="hljs-comment"># Drop rows with missing 'Description' values</span>
    data = data.dropna(subset=[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Drop duplicate 'Description' entries</span>
    data = data.drop_duplicates(subset=[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Convert descriptions to lowercase</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].<span class="hljs-built_in">str</span>.lower()
    <span class="hljs-comment"># Tokenization and removal of punctuation</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(word_tokenize)
    <span class="hljs-comment"># Remove stopwords and color-related words</span>
    stop_words = <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">'english'</span>))
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-keyword">lambda</span> x: [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(word) &gt; <span class="hljs-attr">2</span> <span class="hljs-keyword">and</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> color_words])
    <span class="hljs-comment"># Convert tokenized descriptions back to strings</span>
    data[<span class="hljs-string">'Description'</span>] = data[<span class="hljs-string">'Description'</span>].apply(<span class="hljs-string">' '</span>.join)
    <span class="hljs-comment"># TF-IDF vectorization</span>
    tfidf_vectorizer = TfidfVectorizer(min_df=min_df)
    tfidf_matrix = tfidf_vectorizer.fit_transform(data[<span class="hljs-string">'Description'</span>])
    <span class="hljs-comment"># Convert TF-IDF matrix to a DataFrame</span>
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    
    <span class="hljs-keyword">return</span> tfidf_df
<span class="hljs-comment"># Load the data from the CSV file using Latin1 encoding</span>
data_url = <span class="hljs-string">"../Datasets/data.csv"</span>
df = pd.read_csv(data_url, encoding=<span class="hljs-string">'latin1'</span>)
<span class="hljs-comment"># Different values for min_df</span>
min_df_values = [<span class="hljs-attr">5</span>, <span class="hljs-attr">10</span>, <span class="hljs-attr">20</span>, <span class="hljs-attr">30</span>]
<span class="hljs-comment"># Store silhouette scores</span>
silhouette_scores = []
<span class="hljs-comment"># Perform clustering and calculate silhouette scores for different min_df values</span>
<span class="hljs-keyword">for</span> min_df <span class="hljs-keyword">in</span> min_df_values:
    <span class="hljs-comment"># Preprocess and vectorize the data</span>
    tfidf_df = preprocess_and_vectorize_data(df, min_df)
    
    <span class="hljs-comment"># Perform KMeans clustering</span>
    kmeans = KMeans(n_clusters=<span class="hljs-attr">10</span>, random_state=<span class="hljs-attr">42</span>)
    cluster_labels = kmeans.fit_predict(tfidf_df)
    
    <span class="hljs-comment"># Calculate silhouette score</span>
    silhouette_scores.append(silhouette_score(tfidf_df, cluster_labels))
<span class="hljs-comment"># Visualize the silhouette scores</span>
plt.plot(min_df_values, silhouette_scores, marker=<span class="hljs-string">'o'</span>)
plt.xlabel(<span class="hljs-string">'min_df'</span>)
plt.ylabel(<span class="hljs-string">'Silhouette Score'</span>)
plt.title(<span class="hljs-string">'Silhouette Score vs. min_df'</span>)
plt.xticks(min_df_values)
plt.grid()
plt.show()
</code></pre>
    <p class="normal1">In <a id="_idIndexMarker772" class="calibre3"/>this script, we’re looping through different values of <code class="inlinecode">min_df</code> (5, 10, 20, and 30) and performing the following steps for each value: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Preprocess and vectorize the data using the <code class="inlinecode">preprocess_and_vectorize_data</code> function with the specified <code class="inlinecode">min_df</code> value. </li>
      <li class="bulletlist1">Perform K-means clustering with 10 clusters (you can adjust the number of clusters as needed). </li>
      <li class="bulletlist1">Calculate the silhouette score for the clustering results and store it in the <code class="inlinecode">silhouette_scores</code> list. </li>
    </ol>
    <p class="normal1">Finally, we visualize the silhouette scores for different <code class="inlinecode">min_df</code> values using a line plot. This will help you compare how different values of <code class="inlinecode">min_df</code> impact the quality of the clustering results.</p>
    <figure class="mediaobject"><img alt="A graph with a line  Description automatically generated" src="../Images/B21232_16_15.png" class="calibre19"/></figure>
    <p class="packt_figref"> Figure 16.15: Silhouette score versus min_df</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Next, we<a id="_idIndexMarker773" class="calibre3"/> will investigate the impact of the number of clusters on the quality of the clustering results. Let’s proceed with the next prompt. </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would also like to try different number of clusters <code class="inlinecode">num_clusters = [3,4, 5,6,7]</code> and visually compare silhouette score outputs for <code class="inlinecode">min_df</code>, <code class="inlinecode">num_clusters</code> combinations. Please add the script that generates a plot of cluster and silhouette score values for each of the values from <code class="inlinecode">num_clusters</code>. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! Here’s the <a id="_idIndexMarker774" class="calibre3"/>updated script that generates a plot of silhouette scores for different combinations of <code class="inlinecode">min_df</code> and <code class="inlinecode">num_clusters</code>: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># Different values for min_df and num_clusters</span>
min_df_values = [<span class="hljs-attr">5</span>, <span class="hljs-attr">10</span>, <span class="hljs-attr">20</span>, <span class="hljs-attr">30</span>]
num_clusters = [<span class="hljs-attr">3</span>, <span class="hljs-attr">4</span>, <span class="hljs-attr">5</span>, <span class="hljs-attr">6</span>, <span class="hljs-attr">7</span>]
<span class="hljs-comment"># Store silhouette scores for each combination</span>
silhouette_scores = []
<span class="hljs-comment"># Perform clustering and calculate silhouette scores for different min_df and num_clusters combinations</span>
<span class="hljs-keyword">for</span> min_df, n_clusters <span class="hljs-keyword">in</span> itertools.product(min_df_values, num_clusters):
    <span class="hljs-comment"># Preprocess and vectorize the data</span>
    tfidf_df = preprocess_and_vectorize_data(df, min_df)
    
    <span class="hljs-comment"># Perform KMeans clustering</span>
    kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-attr">42</span>)
    cluster_labels = kmeans.fit_predict(tfidf_df)
    
    <span class="hljs-comment"># Calculate silhouette score</span>
    silhouette_scores.append((min_df, n_clusters, silhouette_score(tfidf_df, cluster_labels)))
<span class="hljs-comment"># Convert silhouette scores to a DataFrame for easier visualization</span>
silhouette_scores_df = pd.DataFrame(silhouette_scores, columns=[<span class="hljs-string">'min_df'</span>, <span class="hljs-string">'num_clusters'</span>, <span class="hljs-string">'silhouette_score'</span>])
<span class="hljs-comment"># Plot the silhouette scores</span>
plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">6</span>))
<span class="hljs-keyword">for</span> min_df <span class="hljs-keyword">in</span> min_df_values:
    subset_df = silhouette_scores_df[silhouette_scores_df[<span class="hljs-string">'min_df'</span>] == min_df]
    plt.plot(subset_df[<span class="hljs-string">'</span><span class="hljs-string">num_clusters'</span>], subset_df[<span class="hljs-string">'silhouette_score'</span>], marker=<span class="hljs-string">'o'</span>, label=<span class="hljs-string">f'min_df = </span><span class="hljs-name">{min_df}</span><span class="hljs-string">'</span>)
    
plt.xlabel(<span class="hljs-string">'Number of Clusters'</span>)
plt.ylabel(<span class="hljs-string">'Silhouette Score'</span>)
plt.title(<span class="hljs-string">'Silhouette Score vs. Number of Clusters for Different min_df Values'</span>)
plt.legend()
plt.grid()
plt.show()
</code></pre>
    <p class="normal1">In this<a id="_idIndexMarker775" class="calibre3"/> script, we use the <code class="inlinecode">itertools.product</code> function to create combinations of <code class="inlinecode">min_df</code> and <code class="inlinecode">num_clusters </code>values. We calculate silhouette scores for each combination and store them in the <code class="inlinecode">silhouette_scores</code> list. Then, we convert the silhouette scores to a DataFrame for easier visualization. The resulting plot shows how the silhouette score varies for different values of <code class="inlinecode">num_clusters</code> while considering different <code class="inlinecode">min_df</code> values. </p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Here is the output of the script: </p>
    <figure class="mediaobject"><img alt="A graph showing the number of clusters  Description automatically generated" src="../Images/B21232_16_16.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.16: Silhouette score versus different min_df values</p>
    <p class="normal1">Let’s refine the plot to show the number of features corresponding to the <code class="inlinecode">min_fd</code> parameter value. </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">The plot is good. I would like a small change by adding the number of features for each of the <code class="inlinecode">min_df</code> values to the <code class="inlinecode">label=f'min_df = {min_df}</code> </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! You<a id="_idIndexMarker776" class="calibre3"/> can add the number of features corresponding to each <code class="inlinecode">min_df</code> value to the legend labels. </p>
    <p class="normal1">Here’s the modified code with that change: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># ...</span>
<span class="hljs-comment"># Store silhouette scores for each combination</span>
silhouette_scores = []
<span class="hljs-comment"># Perform clustering and calculate silhouette scores for different min_df and num_clusters combinations</span>
<span class="hljs-keyword">for</span> min_df, n_clusters <span class="hljs-keyword">in</span> itertools.product(min_df_values, num_clusters):
    <span class="hljs-comment"># Preprocess and vectorize the data</span>
    tfidf_df = preprocess_and_vectorize_data(df, min_df)
    
    <span class="hljs-comment"># Perform KMeans clustering</span>
    kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-attr">42</span>)
    cluster_labels = kmeans.fit_predict(tfidf_df)
    
    <span class="hljs-comment"># Calculate silhouette score</span>
    silhouette_scores.append((min_df, n_clusters, silhouette_score(tfidf_df, cluster_labels)))
<span class="hljs-comment"># Convert silhouette scores to a DataFrame for easier visualization</span>
silhouette_scores_df = pd.DataFrame(silhouette_scores, columns=[<span class="hljs-string">'min_df'</span>, <span class="hljs-string">'num_clusters'</span>, <span class="hljs-string">'silhouette_score'</span>])
<span class="hljs-comment"># Plot the silhouette scores</span>
plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">6</span>))
<span class="hljs-keyword">for</span> min_df <span class="hljs-keyword">in</span> min_df_values:
    subset_df = silhouette_scores_df[silhouette_scores_df[<span class="hljs-string">'min_df'</span>] == min_df]
    
    <span class="hljs-comment"># Get the number of features for the current min_df value</span>
    num_features = preprocess_and_vectorize_data(df, min_df).shape[<span class="hljs-attr">1</span>]
    
    plt.plot(subset_df[<span class="hljs-string">'num_clusters'</span>], subset_df[<span class="hljs-string">'silhouette_score'</span>], marker=<span class="hljs-string">'o'</span>, label=<span class="hljs-string">f'min_df = </span><span class="hljs-name">{min_df}</span><span class="hljs-string">, num_features = </span><span class="hljs-name">{num_features}</span><span class="hljs-string">'</span>)
    
plt.xlabel(<span class="hljs-string">'Number of Clusters'</span>)
plt.ylabel(<span class="hljs-string">'Silhouette Score'</span>)
plt.title(<span class="hljs-string">'Silhouette Score vs. Number of Clusters for Different min_df Values'</span>)
plt.legend()
plt.grid()
plt.show()
</code></pre>
    <p class="normal1">In this modified<a id="_idIndexMarker777" class="calibre3"/> script, we calculate the number of features for each <code class="inlinecode">min_df</code> value using the <code class="inlinecode">shape[1] </code>attribute of the TF-IDF DataFrame. Then, we include this information in the legend labels for each line on the plot. This change will help you visualize the impact of both <code class="inlinecode">min_df</code> and the number of features on the silhouette scores. </p>
    <figure class="mediaobject"><img alt="A graph showing the number of clusters  Description automatically generated" src="../Images/B21232_16_17.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.17: Silhouette score versus different min_df values with number of features</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">Let’s <a id="_idIndexMarker778" class="calibre3"/>proceed with the next prompt.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I will use the value of <code class="inlinecode">min_df_value = 20</code>, <code class="inlinecode">num_clusters= [3, 4, 5, 6]</code>. I would like to know the number of products in each cluster. Can you add a plot <code class="inlinecode">Products per Cluster</code> to the previous code to show number of products per cluster for <code class="inlinecode">num_clusters_values</code> in [3, 4, 5, 6] </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! You <a id="_idIndexMarker779" class="calibre3"/>can add a plot to visualize the number of products in each cluster for different <code class="inlinecode">num_clusters</code> values. Here’s the modified script with the requested changes: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-comment"># ...</span>
<span class="hljs-comment"># Chosen min_df and num_clusters values</span>
min_df_value = <span class="hljs-attr">20</span>
num_clusters_values = [<span class="hljs-attr">3</span>, <span class="hljs-attr">4</span>, <span class="hljs-attr">5</span>, <span class="hljs-attr">6</span>]
<span class="hljs-comment"># Store silhouette scores and number of products per cluster</span>
results = []
<span class="hljs-comment"># Perform clustering and calculate silhouette scores for chosen min_df and num_clusters values</span>
<span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> num_clusters_values:
    <span class="hljs-comment"># Preprocess and vectorize the data</span>
    tfidf_df = preprocess_and_vectorize_data(df, min_df_value)
    
    <span class="hljs-comment"># Perform KMeans clustering</span>
    kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-attr">42</span>)
    cluster_labels = kmeans.fit_predict(tfidf_df)
    
    <span class="hljs-comment"># Calculate silhouette score</span>
    silhouette = silhouette_score(tfidf_df, cluster_labels)
    
    <span class="hljs-comment"># Count number of products in each cluster</span>
    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
    
    results.append((n_clusters, silhouette, cluster_counts))
<span class="hljs-comment"># Convert results to a DataFrame for easier visualization</span>
results_df = pd.DataFrame(results, columns=[<span class="hljs-string">'num_clusters'</span>, <span class="hljs-string">'silhouette_score'</span>, <span class="hljs-string">'cluster_counts'</span>])
<span class="hljs-comment"># Plot the silhouette scores</span>
plt.figure(figsize=(<span class="hljs-attr">15</span>, <span class="hljs-attr">6</span>))
<span class="hljs-comment"># Silhouette Score plot</span>
plt.subplot(<span class="hljs-attr">1</span>, <span class="hljs-attr">2</span>, <span class="hljs-attr">1</span>)
plt.plot(results_df[<span class="hljs-string">'num_clusters'</span>], results_df[<span class="hljs-string">'silhouette_score'</span>], marker=<span class="hljs-string">'o'</span>)
plt.xlabel(<span class="hljs-string">'</span><span class="hljs-string">Number of Clusters'</span>)
plt.ylabel(<span class="hljs-string">'Silhouette Score'</span>)
plt.title(<span class="hljs-string">'Silhouette Score vs. Number of Clusters'</span>)
plt.grid()
<span class="hljs-comment"># Products per Cluster plot</span>
plt.subplot(<span class="hljs-attr">1</span>, <span class="hljs-attr">2</span>, <span class="hljs-attr">2</span>)
<span class="hljs-keyword">for</span> n_clusters, cluster_counts <span class="hljs-keyword">in</span> results_df[[<span class="hljs-string">'num_clusters'</span>, <span class="hljs-string">'</span><span class="hljs-string">cluster_counts'</span>]].values:
    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-attr">1</span>, n_clusters + <span class="hljs-attr">1</span>), cluster_counts, marker=<span class="hljs-string">'o'</span>, label=<span class="hljs-string">f'num_clusters = </span><span class="hljs-name">{n_clusters}</span><span class="hljs-string">'</span>)
plt.xlabel(<span class="hljs-string">'Cluster'</span>)
plt.ylabel(<span class="hljs-string">'Number of Products'</span>)
plt.title(<span class="hljs-string">'Number of Products per Cluster'</span>)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()
</code></pre>
    <p class="normal1">In this <a id="_idIndexMarker780" class="calibre3"/>modified script, we calculate the number of products in each cluster using <code class="inlinecode">pd.Series(cluster_labels).value_counts().sort_index()</code> and store the results along with silhouette scores in the <code class="inlinecode">results</code> list. We then create a DataFrame from the results for easier visualization. </p>
    <p class="normal1">The script includes two plots side by side: the first plot displays the silhouette scores for different <code class="inlinecode">num_clusters</code> values, and the second plot displays the number of products in each cluster for the same <code class="inlinecode">num_clusters</code> values. This will help you understand the relationship between the number of clusters, silhouette scores, and the distribution of products among clusters. </p>
    <p class="normal1">After running the script, you should see the following output: </p>
    <figure class="mediaobject"><img alt="A comparison of graphs with lines  Description automatically generated with medium confidence" src="../Images/B21232_16_18.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.18: Added plot of products per cluster</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">We will<a id="_idIndexMarker781" class="calibre3"/> now select the number of clusters and <code class="inlinecode">min_df</code> value and proceed with the next prompt. </p>
    <h2 class="heading1" id="_idParaDest-367">Interpret clusters and visualize results </h2>
    <p class="normal">We’ll be looking into interpreting clusters and visualizing the results. What are the things we should bear in mind?</p>
    <h3 class="heading2" id="_idParaDest-368">Interpreting cluster</h3>
    <p class="normal">For cluster<a id="_idIndexMarker782" class="calibre3"/> interpretation, we should consider things like:</p>
    <ul class="calibre15">
      <li class="bulletlist">Cohesion: Ensure data points are similar to each other. You can find this out by examining terms that are the most representative in each cluster.</li>
      <li class="bulletlist1">Separation: Clusters should be well separated ensuring they are distinct and meaningful.</li>
      <li class="bulletlist1">Labels: Clusters should also have descriptive labels.</li>
      <li class="bulletlist1">Outliers: It’s important to identify outliers that don’t fit well in any cluster.</li>
    </ul>
    <h3 class="heading2" id="_idParaDest-369">Visualizing clusters</h3>
    <p class="normal">For visualizing <a id="_idIndexMarker783" class="calibre3"/>clusters, here are some things to consider:</p>
    <ul class="calibre15">
      <li class="bulletlist">Dimensionality reduction: You want to reduce the high-dimensional TF-IDF matrix to 2D or 3D to make it easier to visualize. For this, a technique like PCA comes in handy.</li>
      <li class="bulletlist1">Scatter plots: Scatter plots are great for visualizing data points; a point can be a document, for example, and colors or shapes could represent different clusters.</li>
      <li class="bulletlist1">Word clouds: You could also use word clouds if you look at text, to see what the most common words used are.</li>
      <li class="bulletlist1">Heat maps: Heat maps are another great way to visualize frequency and will help highlight the most important terms per cluster.</li>
      <li class="bulletlist1">Interactivity: Ensure you allow for exploration to help you drill down into specific data points by using, for example, Plotly.</li>
    </ul>
    <p class="normal1">Next, let’s craft a prompt for visualization.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I will use the value of <code class="inlinecode">min_df_value = 20</code> and 5 clusters. I would like to use PCA with 3 components to visualize product clusters on a 3D graph. The colors of clusters should be mapped from the seaborn <code class="inlinecode">color_palette</code>.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Certainly! In order to visualize the product clusters in a 3D graph using PCA with 3 components and mapping cluster colors from a seaborn color palette, you can follow these steps: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Import necessary libraries. </li>
      <li class="bulletlist1">Preprocess the data and perform clustering. </li>
      <li class="bulletlist1">Apply PCA to reduce the dimensionality to 3 components. </li>
      <li class="bulletlist1">Plot the clusters in a 3D scatter plot with seaborn color palette. </li>
    </ol>
    <p class="normal1">Here’s the code to achieve this: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D  <span class="hljs-comment"># Required for 3D plotting</span>
<span class="hljs-comment"># Selected min_df and num_clusters values</span>
min_df_value = <span class="hljs-attr">20</span>
num_clusters = <span class="hljs-attr">5</span>
<span class="hljs-comment"># Preprocess data and perform clustering</span>
tfidf_matrix, tfidf_vectorizer = preprocess_and_vectorize_data(df[<span class="hljs-string">'Description'</span>], min_df_value)
kmeans = KMeans(n_clusters=num_clusters, random_state=<span class="hljs-attr">42</span>)
cluster_labels = kmeans.fit_predict(tfidf_matrix)
<span class="hljs-comment"># Apply PCA with 3 components</span>
pca = PCA(n_components=<span class="hljs-attr">3</span>)
pca_result = pca.fit_transform(tfidf_matrix.toarray())
<span class="hljs-comment"># Create a DataFrame for PCA results</span>
pca_df = pd.DataFrame(data=pca_result, columns=[<span class="hljs-string">'PC1'</span>, <span class="hljs-string">'PC2'</span>, <span class="hljs-string">'PC3'</span>])
pca_df[<span class="hljs-string">'</span><span class="hljs-string">Cluster'</span>] = cluster_labels
<span class="hljs-comment"># Seaborn color palette for cluster colors</span>
color_palette = sns.color_palette(<span class="hljs-string">"Set1"</span>, n_colors=num_clusters)
<span class="hljs-comment"># Plot 3D scatter plot</span>
fig = plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">8</span>))
ax = fig.add_subplot(<span class="hljs-attr">111</span>, projection=<span class="hljs-string">'3d'</span>)
<span class="hljs-keyword">for</span> cluster_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_clusters):
    cluster_points = pca_df[pca_df[<span class="hljs-string">'Cluster'</span>] == cluster_id]
    ax.scatter(cluster_points[<span class="hljs-string">'PC1'</span>], cluster_points[<span class="hljs-string">'PC2'</span>], cluster_points[<span class="hljs-string">'PC3'</span>], color=color_palette[cluster_id], label=<span class="hljs-string">f'Cluster </span><span class="hljs-name">{cluster_id}</span><span class="hljs-string">'</span>)
ax.set_xlabel(<span class="hljs-string">'PC1'</span>)
ax.set_ylabel(<span class="hljs-string">'PC2'</span>)
ax.set_zlabel(<span class="hljs-string">'PC3'</span>)
ax.set_title(<span class="hljs-string">'</span><span class="hljs-string">Product Clusters in 3D'</span>)
ax.legend()
plt.show()
</code></pre>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">This code <a id="_idIndexMarker784" class="calibre3"/>will preprocess the data, perform clustering, apply PCA to reduce dimensionality to 3 components, and then plot the product clusters in a 3D scatter plot with cluster colors mapped from a seaborn color palette. Adjust the figure size, labels, and other plot parameters as <a id="_idIndexMarker785" class="calibre3"/>needed to customize the visualization. </p>
    <figure class="mediaobject"><img alt="A graph of clustering data  Description automatically generated" src="../Images/B21232_16_19.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.19: Product clusters visualized in 3D space</p>
    <h3 class="heading2" id="_idParaDest-370">Creating a word cloud</h3>
    <p class="normal">Now, we will <a id="_idIndexMarker786" class="calibre3"/>examine the most frequent words in each of the clusters. Let’s proceed with the next prompt. </p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I would like to visualize the most frequent words occurring in product descriptions assigned to clusters. I would like you to use the word cloud library and create a function that will output word clouds for each of the clusters. </p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! To visualize the most frequent words occurring in product descriptions assigned to clusters, you can use the <code class="inlinecode">WordCloud</code> library. Here’s how you can create a function that generates word clouds for each of the clusters: </p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud
<span class="hljs-comment"># Create a DataFrame for clustering results</span>
clustering_results = pd.DataFrame(data={<span class="hljs-string">'Description'</span>: df[<span class="hljs-string">'Description'</span>], <span class="hljs-string">'Cluster'</span>: cluster_labels})
<span class="hljs-comment"># Function to generate word clouds for each cluster</span>
<span class="hljs-keyword">def</span> <span class="hljs-meta">generate_wordclouds</span>(<span>dataframe, num_clusters</span>):
    <span class="hljs-keyword">for</span> cluster_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_clusters):
        cluster_data = dataframe[dataframe[<span class="hljs-string">'Cluster'</span>] == cluster_id]
        cluster_text = <span class="hljs-string">' '</span>.join(cluster_data[<span class="hljs-string">'Description'</span>])
        
        <span class="hljs-comment"># Create a WordCloud object</span>
        wordcloud = WordCloud(width=<span class="hljs-attr">800</span>, height=<span class="hljs-attr">400</span>, background_color=<span class="hljs-string">'white'</span>, max_words=<span class="hljs-attr">100</span>).generate(cluster_text)
        
        <span class="hljs-comment"># Plot the WordCloud</span>
        plt.figure(figsize=(<span class="hljs-attr">10</span>, <span class="hljs-attr">6</span>))
        plt.imshow(wordcloud, interpolation=<span class="hljs-string">'bilinear'</span>)
        plt.title(<span class="hljs-string">f'Cluster </span><span class="hljs-name">{cluster_id}</span><span class="hljs-string"> - Most Frequent Words'</span>)
        plt.axis(<span class="hljs-string">'off'</span>)
        plt.show()
<span class="hljs-comment"># Generate word clouds for each cluster</span>
generate_wordclouds(clustering_results, num_clusters)
</code></pre>
    <p class="normal1">This <a id="_idIndexMarker787" class="calibre3"/>code defines a function <code class="inlinecode">generate_wordclouds</code> that generates word clouds for each cluster based on the product descriptions assigned to that cluster. Adjust the parameters of the <code class="inlinecode">WordCloud</code> object (e.g., <code class="inlinecode">width</code>, <code class="inlinecode">height</code>, <code class="inlinecode">background_color</code>, <code class="inlinecode">max_words</code>) as needed for your visualization preferences. </p>
    <p class="normal1">The function will plot the word clouds for each cluster. </p>
    <figure class="mediaobject"><img alt="A close-up of words  Description automatically generated" src="../Images/B21232_16_20.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.20: Word cloud for the first cluster</p>
    <figure class="mediaobject"><img alt="A word cloud with different colored words  Description automatically generated" src="../Images/B21232_16_21.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.21: Word cloud for the second cluster</p>
    <figure class="mediaobject"><img alt="A word cloud with different colored text  Description automatically generated" src="../Images/B21232_16_22.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.22: Word cloud for the third cluster</p>
    <figure class="mediaobject"><img alt="A word cloud with different colored words  Description automatically generated" src="../Images/B21232_16_23.png" class="calibre19"/></figure>
    <p class="packt_figref">Figure 16.23: Word cloud for the fourth cluster</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <h2 class="heading1" id="_idParaDest-371">Assigning categories to products and evaluating and refining</h2>
    <p class="normal">Neither <a id="_idIndexMarker788" class="calibre3"/>assigning categories to products nor evaluating and refining steps were in scope for this chapter to solve but let’s describe what they are to give you an idea of how you could explore those should you want to.</p>
    <p class="normal1">Our dataset contains invoices, and for us to categorize products, we would first need to understand the use cases we could possibly have. Those could be, for example:</p>
    <ul class="calibre15">
      <li class="bulletlist">Organizing inventory</li>
      <li class="bulletlist1">Improving search functionality</li>
      <li class="bulletlist1">Enhancing the user experience</li>
    </ul>
    <p class="normal1">As you <a id="_idIndexMarker789" class="calibre3"/>can see, this is very centered on product data, which means we would probably need a completely different dataset containing product titles, descriptions, and more. Let’s describe at a high level what our approach could be using ChatGPT.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">I’m looking at an e-commerce dataset. What can you tell me about the process of assigning categories to products?</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Assigning categories to products in an e-commerce dataset is a crucial task for organizing inventory, improving search functionality, and enhancing the user experience. Here’s a step-by-step guide to help you through the process:</p>
    <p class="normal1">Steps to Assign Categories to Products:</p>
    <p class="normal1"><strong class="screentext">Understand the Data</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Review Product Descriptions</strong>: Examine the product titles, descriptions, and any other relevant attributes.</li>
      <li class="bulletlist1"><strong class="screentext">Identify Existing Categories</strong>: If there are existing categories, review them to understand the current classification system.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Define Categories</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Create a Category Hierarchy</strong>: Develop a hierarchical structure of categories and subcategories that makes sense for your products.</li>
      <li class="bulletlist1"><strong class="screentext">Ensure Completeness</strong>: Make sure your categories cover all types of products in your dataset.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Feature Extraction</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Text Features</strong>: Extract relevant text features from product titles and descriptions using techniques like TF-IDF or word embeddings.</li>
      <li class="bulletlist1"><strong class="screentext">Other Features</strong>: Consider other attributes like price, brand, and specifications that might help in categorization.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Manual Labeling</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Sample Labeling</strong>: Manually<a id="_idIndexMarker790" class="calibre3"/> label a sample of products to create a training dataset. This helps in understanding the nuances of each category.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Model Selection</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Choose a Classification Model</strong>: Select a machine learning model suitable for text classification, such as Naive Bayes, SVM, or deep learning models like LSTM or BERT.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Training the Model</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Train the Model</strong>: Use the labeled data to train your classification model.</li>
      <li class="bulletlist1"><strong class="screentext">Evaluate Performance</strong>: Evaluate the model’s performance using metrics like accuracy, precision, recall, and F1-score.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Assign Categories</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Predict Categories</strong>: Use the trained model to predict categories for the unlabeled products.</li>
      <li class="bulletlist1"><strong class="screentext">Review Predictions</strong>: Manually review a subset of the predictions to ensure accuracy.</li>
    </ul>
    <p class="normal1"><strong class="screentext">Iterate and Improve</strong>:</p>
    <ul class="calibre15">
      <li class="bulletlist"><strong class="screentext">Refine Categories</strong>: Based on the results, refine your category definitions and model.</li>
      <li class="bulletlist1"><strong class="screentext">Continuous Learning</strong>: Continuously update the model with new data to improve its accuracy over time.</li>
    </ul>
    <p class="normal1">Example in Python:</p>
    <p class="normal1">Here’s a simplified example using a Naive Bayes classifier for text classification:</p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score
<span class="hljs-comment"># Sample data</span>
data = {
<span class="hljs-string">'Product'</span>: [<span class="hljs-string">'Red Shirt'</span>, <span class="hljs-string">'Blue Jeans'</span>, <span class="hljs-string">'Green Hat'</span>, <span class="hljs-string">'Black Shoes'</span>],
<span class="hljs-string">'Category'</span>: [<span class="hljs-string">'Clothing'</span>, <span class="hljs-string">'Clothing'</span>, <span class="hljs-string">'Accessories'</span>, <span class="hljs-string">'Footwear'</span>]
}
df = pd.DataFrame(data)
<span class="hljs-comment"># Feature extraction</span>
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df[<span class="hljs-string">'Product'</span>])
y = df[<span class="hljs-string">'Category'</span>]
<span class="hljs-comment"># Train-test split</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-attr">0.2</span>, random_state=<span class="hljs-attr">42</span>)
<span class="hljs-comment"># Train the model</span>
model = MultinomialNB()
model.fit(X_train, y_train)
<span class="hljs-comment"># Predict and evaluate</span>
y_pred = model.predict(X_test)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'Accuracy: </span><span class="hljs-name">{accuracy_score(y_test, y_pred)}</span><span class="hljs-string">'</span>)
</code></pre>
    <p class="normal1">This code<a id="_idIndexMarker791" class="calibre3"/> provides a basic framework for categorizing products based on their names. For a real-world application, you would need a larger dataset and more sophisticated preprocessing and modeling techniques.</p>
    <p class="normal1">If you have any specific questions or need further details, feel free to ask!</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <p class="normal1">We’ll leave it up to you to pursue this further. From what you’ve hopefully seen in this chapter already, start with a high-level prompt explaining what dataset you have (you have to find a dataset for the above problem) and what you aim to do. On the latter, you already have the steps, so you’re suggested to craft a prompt explaining what to do for each step and what code to run.</p>
    <h3 class="heading2" id="_idParaDest-372">Evaluate and refine</h3>
    <p class="normal">This is also out<a id="_idIndexMarker792" class="calibre3"/> of scope, but let’s get you started on a ChatGPT prompt that you can then keep working on.</p>
    <p class="normal1"><strong class="screentext">[Prompt]</strong></p>
    <p class="normal1">Describe evaluate and refine step in the context of e-commerce data on invoices.</p>
    <p class="normal1"><strong class="screentext">[End of prompt]</strong></p>
    <p class="normal1"><strong class="screentext">[Prompt response]</strong></p>
    <p class="normal1">Sure! Let’s break down the evaluate and refine steps in the context of e-commerce data on invoices:</p>
    <p class="normal1">Evaluate Step:</p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Model Performance Metrics:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Accuracy</strong>: Measure how often the model correctly predicts the category of an invoice.</li>
          <li class="bulletlist3"><strong class="screentext">Precision and Recall</strong>: Evaluate the precision (correct positive predictions) and recall (ability to find all relevant instances) for each category.</li>
          <li class="bulletlist3"><strong class="screentext">F1-Score</strong>: Combine precision and recall into a single metric to balance both aspects.</li>
        </ul>
      </li>
      <li class="bulletlist1">Confusion Matrix:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Visualize Predictions</strong>: Use a confusion matrix to see where the model is making correct and incorrect predictions. This helps identify specific categories where the model may be struggling.</li>
        </ul>
      </li>
      <li class="bulletlist1">Cross-Validation:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Robust Evaluation</strong>: Perform cross-validation to ensure the model’s performance is consistent across different subsets of the data.</li>
        </ul>
      </li>
      <li class="bulletlist1">Error Analysis:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Analyze Misclassifications</strong>: Examine the invoices that were misclassified to understand why the model made errors. Look for patterns or common issues.</li>
        </ul>
      </li>
    </ol>
    <p class="normal1">Refine Step:</p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Feature<a id="_idIndexMarker793" class="calibre3"/> Engineering:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Enhance Features</strong>: Create new features or improve existing ones to provide more relevant information to the model. For example, extracting keywords from invoice descriptions or adding metadata like invoice date and amount.</li>
        </ul>
      </li>
      <li class="bulletlist1">Hyperparameter Tuning:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Optimize Model Parameters</strong>: Use techniques like grid search or random search to find the best hyperparameters for your model, improving its performance.</li>
        </ul>
      </li>
      <li class="bulletlist1">Model Selection:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Try Different Models</strong>: Experiment with different algorithms (e.g., decision trees, random forests, neural networks) to see if another model performs better.</li>
        </ul>
      </li>
      <li class="bulletlist1">Data Augmentation:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Increase Data Variety</strong>: Augment your dataset by generating synthetic data or using techniques like oversampling for underrepresented categories.</li>
        </ul>
      </li>
      <li class="bulletlist1">Regular Updates:<ul class="calibre17">
          <li class="bulletlist2"><strong class="screentext">Continuous Learning</strong>: Regularly update the model with new invoice data to keep it current and improve its accuracy over time.</li>
        </ul>
      </li>
    </ol>
    <p class="normal1">Example in Python:</p>
    <p class="normal1">Here’s a simplified example of evaluating and refining a model using cross-validation and hyperparameter tuning:</p>
    <pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split, GridSearchCV
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report, confusion_matrix
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># Sample data</span>
data = {
    <span class="hljs-string">'Invoice'</span>: [<span class="hljs-string">'Invoice for Red Shirt'</span>, <span class="hljs-string">'Invoice for Blue Jeans'</span>, <span class="hljs-string">'Invoice for Green Hat'</span>, <span class="hljs-string">'Invoice for Black Shoes'</span>],
    <span class="hljs-string">'Category'</span>: [<span class="hljs-string">'Clothing'</span>, <span class="hljs-string">'Clothing'</span>, <span class="hljs-string">'Accessories'</span>, <span class="hljs-string">'Footwear'</span>]
}
df = pd.DataFrame(data)
<span class="hljs-comment"># Feature extraction</span>
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df[<span class="hljs-string">'Invoice'</span>])
y = df[<span class="hljs-string">'Category'</span>]
<span class="hljs-comment"># Train-test split</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-attr">0.2</span>, random_state=<span class="hljs-attr">42</span>)
<span class="hljs-comment"># Model training</span>
model = MultinomialNB()
model.fit(X_train, y_train)
<span class="hljs-comment"># Model evaluation</span>
y_pred = model.predict(X_test)
<span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))
<span class="hljs-comment"># Confusion matrix</span>
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=<span class="hljs-literal">True</span>, fmt=<span class="hljs-string">'d'</span>, cmap=<span class="hljs-string">'Blues'</span>)
plt.xlabel(<span class="hljs-string">'Predicted'</span>)
plt.ylabel(<span class="hljs-string">'Actual'</span>)
plt.title(<span class="hljs-string">'Confusion Matrix'</span>)
plt.show()
<span class="hljs-comment"># Hyperparameter tuning</span>
param_grid = {<span class="hljs-string">'alpha'</span>: [<span class="hljs-attr">0.1</span>, <span class="hljs-attr">0.5</span>, <span class="hljs-attr">1.0</span>]}
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-attr">5</span>)
grid_search.fit(X_train, y_train)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'Best parameters: </span><span class="hljs-name">{grid_search.best_params_}</span><span class="hljs-string">'</span>)
</code></pre>
    <p class="normal1">This code<a id="_idIndexMarker794" class="calibre3"/> provides a framework for evaluating and refining a model using cross-validation and hyperparameter tuning.</p>
    <p class="normal1">If you have any specific questions or need further details, feel free to ask!</p>
    <p class="normal1"><strong class="screentext">[End of response]</strong></p>
    <h1 class="heading" id="_idParaDest-373">Reflection on prompts for this use case </h1>
    <p class="normal">Just like with our first use case in this chapter, we followed a specific method of first setting high-level context of describing our role, the shape of the dataset, and what we were looking to do. Then, we followed the below process to first breaking down the problem in steps, getting code and how we could continue to improve and refine and finally visualize the results:</p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1"><strong class="screentext">Step-by-step guidance:</strong> The requests are structured as a series of step-by-step tasks, breaking down the larger problem into manageable components. This makes it easier to follow and implement the solution incrementally. </li>
      <li class="bulletlist1"><strong class="screentext">Specify inputs and requirements:</strong> Provide clear and specific information about the dataset, its columns, and the requirements for preprocessing, clustering, and visualization. This helps ensure that the assistance received is tailored to the particular needs. </li>
      <li class="bulletlist1"><strong class="screentext">Request for code with comments:</strong> Code snippets request to include comments to explain each step and clarify the purpose of the code. This helps in understanding the code and code validation and facilitates learning. </li>
      <li class="bulletlist1"><strong class="screentext">Iteration and refinement:</strong> Iteration of prompts, asking for additional modifications and visualizations after the initial guidance. This iterative approach allows for refining and improving the solution progressively. </li>
      <li class="bulletlist1"><strong class="screentext">Visualization and interpretation:</strong> Visualizing and interpreting the results allows focusing on deriving meaningful insights from the data. </li>
    </ol>
    <h1 class="heading" id="_idParaDest-374">Assignment</h1>
    <p class="normal">In the previous section, we used traditional embedding with TF-IDF to transform text data into numerical representations, which can then be used for various <strong class="screentext">natural language processing</strong> (<strong class="screentext">NLP</strong>) tasks such as clustering. Let’s now try and improve the clustering results by using a more advanced embedding technique. We will use the Hugging Face Transformers library to get pre-trained embeddings for our product descriptions: </p>
    <ol class="calibre16">
      <li class="bulletlist1" value="1">Ask ChatGPT to explain Hugging Face Transformers’ advantages over TF-IDF vectorization for clustering use cases. </li>
      <li class="bulletlist1">Use ChatGPT to generate and create product clusters using Hugging Face Transformers embeddings. </li>
      <li class="bulletlist1">Compare the results with the previous clustering results using TF-IDF vectorization.</li>
    </ol>
    <h1 class="heading" id="_idParaDest-375">Solution</h1>
    <p class="normal">See the solution in the repository: <span class="calibre3">https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT</span></p>
    <h1 class="heading" id="_idParaDest-376">Summary</h1>
    <p class="normal">This chapter focused on clustering and how it could be used to group your data into separate areas. Creating these areas made it easier to understand our data points. Through visualization like heat maps, word clouds, and more, you were given the insight that data benefits from being shown in different ways. You also saw how the clustering process helped identify outliers, that is, data that vastly differs and can’t easily be assigned to any one cluster. For the ChatGPT and prompting part, you saw how setting a high-level context describing the dataset helped generate a suitable set of steps you could follow from top to bottom. The same high-level context also helped ChatGPT recommend a clustering algorithm. </p>
    <h1 class="heading" id="_idParaDest-377">Join our community on Discord </h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers: </p>
    <p class="normal1"><a href="https://packt.link/aicode" class="calibre3"><span class="calibre3">https://packt.link/aicode</span></a></p>
    <p class="normal1"><span class="calibre3"><img alt="" src="../Images/QR_Code510410532445718281.png" class="calibre4"/></span></p>
  </div>
</body></html>