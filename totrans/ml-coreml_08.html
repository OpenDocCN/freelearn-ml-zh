<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assisted Drawing with RNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we walked through building a simple drawing application that would try to infer what the user was drawing and present them with alternatives based on the most likely predicted categories; the intention of this application was to improve the efficiency of sketching tasks by giving the user completed sketches, obtained through Microsoft's Bing image search, rather than having to spend time fussing over the details.</p>
<p class="mce-root">In this chapter, we'll revisit this application but look at an alternative for inferring what the user is drawing, and, in doing so, we will be exposing ourselves to new types of data and machine learning models. Following the familiar format, we will first revise the task, explore the data and model, and then walk through building up the required functionality in a playground, before migrating it across to our application. Let's get started. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assisted drawing </h1>
                </header>
            
            <article>
                
<p>In this section, we will briefly describe this chapter's project and what we aim to achieve. Recall from the previous chapter that we described an application capable of predicting what the user was trying to sketch, and fetched similar images based on the predicted categories, such as a sailboat. Based on this prediction, the application would search and download images of that category. After downloading, it would sort them based on their similarity with regards to the user's sketch. Then it would present the ordered alternatives to the user, which they could swap their sketch with.</p>
<p>The finished project is shown as follows: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c711add4-cacb-4d48-b65a-f9a351ecf11f.png" style="width:33.92em;height:17.25em;"/></div>
<p>The model used for performing this classification was based on a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>), a type of neural network well suited for understanding images owing to its ability to find local patterns and build on top of these lower patterns to find more complex and interesting patterns. We took advantage of these higher order patterns by using them as a basis to sort our downloaded images, such that those that were more similar in style to the user's sketch would be shown first. We reasoned how this worked by comparing it with measurements of similarities between sentences using words as features—words being analogous to our higher order patterns<span>—</span>and distance formulas to calculate the similarities.</p>
<p>But our approach suffered from a bit of overhead; to perform accurate classification, we needed a significant amount of the sketch completed, as well as needing to use memory and CPU cycles to rasterize the image before we could feed it into our model. In this chapter, we will be using an alternative that doesn't rely on pixels as its features but rather the <strong>sequences of strokes</strong> used to draw it. There are numerous reasons you may want to do this, including:</p>
<ul>
<li>Accessibility to the data or larger dataset</li>
<li>Potential improvements to the accuracy of the predictions</li>
<li>Generative capabilities, that is, being able to predict and generate the next set of strokes</li>
</ul>
<p>But here, it gives us the opportunity to explore a type of data that encodes essentially the same thing—a sketch. L<span>et's explore this further in the next section, where we introduce the dataset and model that will be used in this project. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent Neural Networks for drawing classification</h1>
                </header>
            
            <article>
                
<p>The model used in this chapter was trained on the dataset used in Google's AI experiment <em>Quick</em><em>, Draw!</em></p>
<p><span><em>Quick, Draw!</em> </span>is a game where players are challenged to draw a given object to see whether the computer can recognize it; an extract of the data is shown as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/68103d5c-1a1f-404a-96e5-d9638b275429.jpg" style="width:42.42em;height:13.00em;"/></div>
<p>The technique was inspired from the work done on handwritten recognition (Google Translate), where, rather than looking at the image as a whole, the team worked with data features describing how the characters were drawn. This is illustrated in the following image:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/607c6e39-9ebf-4990-883f-b0d2e905b949.png" style="width:34.83em;height:18.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: https://experiments.withgoogle.com/ai/quick-draw</div>
<p>The hypothesis here is that there exists some consistent pattern of how people draw certain types of objects; but to discover those patterns, we would need a lot of data, which we do have. The dataset consists of over 50 million drawings across 345 categories obtained cleverly, from the players of the <em>Quick, Draw!</em> game. Each sample is described with timestamped vectors and associated metadata describing the country the player was based in and the category asked of the user. You can learn more about the dataset from the official website: <a href="https://github.com/googlecreativelab/quickdraw-dataset">https://github.com/googlecreativelab/quickdraw-dataset</a>.</p>
<p>To make the dataset and training manageable, our model was only trained on 172 of the 345 <span>categories,</span> but the accompanying Notebook used to create and train the model is available for those wanting to delve into the details. To get a better understanding of the data, let's have a peek at a single sample, as shown here:</p>
<pre>{<br/>    "key_id":"5891796615823360",<br/>    "word":"nose",<br/>    "countrycode":"AE",<br/>    "timestamp":"2017-03-01 20:41:36.70725 UTC",<br/>    "recognized":true,<br/>    "drawing":[[[129,128,129,129,130,130,131,132,132,133,133,133,133,...]]]<br/> }</pre>
<p>The details of the sketch are broken down into an array of strokes, each described by a three-dimensional array containing the <kbd>x</kbd>, <kbd>y</kbd> positions and <kbd>timestamp</kbd> that make up the path of the stroke:</p>
<pre>[<br/>    [ // First stroke<br/>    [x0, x1, x2, x3, ...],<br/>    [y0, y1, y2, y3, ...],<br/>    [t0, t1, t2, t3, ...]<br/> ],<br/>    [ // Second stroke<br/>    [x0, x1, x2, x3, ...],<br/>    [y0, y1, y2, y3, ...],<br/>    [t0, t1, t2, t3, ...]<br/> ],<br/>    ... // Additional strokes<br/> ]</pre>
<p>As mentioned previously, this being an example from the <strong>raw dataset</strong>, the team behind <em>Quick, Draw!</em> has released many variants of the data, from raw samples to preprocessed and compressed versions. We are mostly interested in exploring the raw and simplified versions: the former because it's the closest representation we have that will represent the data we obtain from the user, and the latter because it was used to train the model.</p>
<div class="packt_infobox"><strong>Spoiler</strong>: Most of this chapter deals with preprocessing the user input.</div>
<p>Both raw and simplified versions have stored each category in an individual file in the NDJSON  file format.</p>
<div class="packt_infobox">The NDJSON file format, short for newline delimited JSON, is a convenient format for storing and streaming structured data that may be processed one record at a time. As the name suggests, it stores multiple JSON-formatted objects in single lines. In our case, this means each sample is stored as a separate object delimited by a new line; you can learn more about the format at <a href="http://ndjson.org">http://ndjson.org</a>. </div>
<p>You may be wondering what the difference is between the raw and simplified versions. We will go into the details when we build the preprocessing functionality required for this application, but as the name implies, the simplified version reduces the complexity of each stroke by removing any unnecessary points, along with applying some level of standardization—a typical requirement when dealing with any data to make the samples more comparable.  </p>
<p>Now that we have a better understanding of the data we are dealing with, let's turn our attention to building up some level of intuition of how we can learn from these sequences, by briefly discussing the details of the model used in this chapter. </p>
<p>In previous chapters, we saw many examples of how <span>CNNs can learn useful patterns from local 2D patches, which themselves can be built upon to further abstract from raw pixels into something with more descriptive power. This is fairly intuitive given our understanding of images is not made up of independent pixels but rather a collection of pixels related to their neighbors, which in turn describe parts of an object. In <a href="7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml" target="_blank">Chapter 1</a>, <em><span class="item-title">Introduction to Machine Learning</span></em>, we introduced a <strong>Recurrent Neural Network</strong> (<strong>RNN</strong>), a major component of building the <strong>Sequence to Sequence</strong></span> (<span><strong>Seq2Seq</strong></span>) <span>model used for language translation, and we saw how its ability to remember made it well suited for data made up of sequences where order matters. As highlighted previously, our given samples are made up of sequences of strokes; the RNN is a likely candidate for learning to classify sketches. </span></p>
<p><span>As a quick recap, RNNs implement a type of <strong>selective memory</strong> using a feedback loop, which itself is adjusted during training; diagrammatically this is shown as follows:<br/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><img src="assets/9ce9ab70-0538-49dd-8a85-49ab5503d2d0.png" style="width:47.50em;height:24.33em;"/>  </span></div>
<p>On the left is the actual network, and on the right we have the same network unrolled across four time steps. As the points of the sketch's strokes are fed in, they are multiplied by the layer's weight along with the current state before being fed back in and/or outputted. During training, this feedback allows the network to learn <span>patterns of an </span>ordered sequence.<span> We can stack these recurrent layers on top of each other to </span>learn more complex and abstract patterns as we did with <span>CNN</span><span>.</span></p>
<p><span>But recurrent layers are not the only way to learn patterns from sequential data. If you generalize the concept of CNNs as something being able to learn local patterns across any dimension (as opposed to just two dimensions), then you can see how we could use 1D convolutional layers to achieve a similar effect as our recurrent layers. Therein, similar to 2D convolutional layers, we learn 1D kernels across sequences (treating time as a spatial dimension) to find local patterns to represent our data. Using a convolutional layer has the advantage of being considerably computationally cheaper than its counterpart, making it ideal for processor- and power-constrained devices, such as mobile phones. It is also advantageous for its ability to learn patterns independent of order, similar to how 2D kernels are invariant of position. In this figure, we illustrate how the 1D convolutional layer operates on input data:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span> <img src="assets/7ebfa401-7372-4c04-bfc3-91ef33e6983b.png" style="width:41.67em;height:36.58em;"/></span></div>
<p>In this context, strokes (local to the window size) will be learned, independent of where they are in the sequence, and a compact representation will be outputted, which we can then feed into an RNN to learn ordered sequences from these strokes (rather than from raw points). Intuitively you can think of our model as initially learning strokes such as vertical and horizontal strokes (independent of time), and then learning (in our subsequent layers made up of RNNs) higher-order patterns such as shapes from the ordered sequence of these strokes. The following figure illustrates this concept:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/41eb49b5-30d3-446d-9269-458d3679581c.png" style="width:47.50em;height:21.58em;"/></div>
<p>On the left, we have the raw points inputted into the model. The middle part shows how a 1D convolutional layer can learn local patterns from these points in a form of strokes. And finally, at the far right, we have the subsequent RNNs learning order-sensitive patterns from the sequence of these strokes.</p>
<p>One more concept to introduce before introducing the model, but, before doing so, I want you to quickly think of how you draw a square. Do you draw it in a clockwise direction or anti-clockwise direction? </p>
<p>The last concept I want to briefly introduce in this section is bidirectional layers; bidirectional layers attempt to make our network invariant to the previous question. We discussed earlier how RNNs are sensitive to order, which is precisely why they are useful here, but as I hope  has been highlighted, our sketch may be drawn in the reverse order. To account for this, we can use a bidirectional layer, which, as the name implies, processes the input sequence in two directions (chronologically and anti-chronologically) and then merges their representations. By processing a sequence in both directions, our model can become somewhat invariant to the direction in which we draw. </p>
<p>We have now introduced all the building blocks used for this model; the following figure shows the model in its entirety:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/2f02a2a5-9683-40fc-9c5f-c61caf9e715b.png" style="width:64.67em;height:31.42em;"/> </div>
<div class="packt_infobox">As a reminder, this book is focused on the application of machine learning related to Core ML. Therefore we won't be going into the details of this (or any) model, but cover just enough to have an intuitive understanding of how the model works for you to use and explore further. </div>
<p><span>As shown previously, our model is comprised of a stack of 1D convolutional layers that feed into a stack of <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>), an implementation of an RNN, before being fed into a fully connected layer where our prediction is made. This model was trained on 172 categories, each using 10,000 training samples and 1,000 validation samples. After 16 epochs, the model achieved approximately 78% accuracy on both the training and validation data, as shown here:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7040b3d5-bb2c-4031-8c3d-4a9ed07dd5f1.png"/></div>
<p>We now have our model but have skimmed across what we are actually feeding into our model. In the next section, we will discuss what our model was trained with (and therefore expecting) and implement the required functionality to prepare it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input data and preprocessing </h1>
                </header>
            
            <article>
                
<p>In this section, we will implement the preprocessing functionality required to transform our raw user input into something the model is expecting. We will build up this functionality in a playground project before migrating it across to our project in the next section.  </p>
<p>If you haven't done so, pull down the latest code from the accompanying repository <a href="https://github.com/PacktPublishing/Machine-Learning-with-Core-ML" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Core-ML</a>. Once downloaded, navigate to the directory <kbd>Chapter8/Start/</kbd> and open the playground project <kbd>ExploringQuickDrawData.playground</kbd>. Once loaded, you will see the playground for this chapter, as shown:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b0eb7b69-a4e1-4d6d-86b9-e2fb3e3b6ab6.png"/></div>
<p>The playground includes a few samples of the raw <em>Quick, Draw!</em> dataset, a single simplified extract, as well as the complied model and supporting classes we created in the previous chapter to represent a sketch (<kbd>Stroke.swift</kbd>, <kbd>Sketch.swift</kbd>) and render it (<kbd>SketchView.swift</kbd>). Our goal for this section will be to better understand the data and the preprocessing required before feeding our model; in doing so, we will be extending our existing classes to encapsulate this functionality.</p>
<p>Let's start reviewing what code exists before we move forward; if you scroll down the opened source file, you will see the methods <kbd>createFromJSON</kbd> and <kbd>drawSketch</kbd>. The former takes in a JSON object (the format our samples are saved in) and returns a strongly typed object: <kbd>StrokeSketch</kbd>. As a reminder, each sample is made up of:</p>
<ul>
<li><kbd>key_id</kbd>: Unique identifier </li>
<li><kbd>word</kbd>: Category label</li>
<li><kbd>countrycode</kbd>: Country code where the sample was drawn</li>
<li><kbd>timestamp</kbd>: Timestamp when the sample was created </li>
<li><kbd>recognized</kbd>: A flag indicating whether the sketch was currently recognized </li>
<li><kbd>drawing</kbd>: A multi-dimensional array consisting of arrays of <em>x</em>, <em>y</em> coordinates along with the elapsed time since the point was created </li>
</ul>
<p>The <kbd>StrokeSketch</kbd> maps the word to the label property and <em>x</em>, <em>y</em> coordinates to the stroke points. We discard everything else as it is not deemed useful in classification and not used by our model. The <kbd>drawSketch</kbd> <span>method </span>is a utility method that handles scaling and centering the sketch before creating an instance of a <kbd>SketchView</kbd> to render the scaled and centered sketch.  </p>
<p>The last block of code preloads the JSON files and makes them available through the dictionary <kbd>loadedJSON</kbd>, where the key is the associated filename and value is the loaded JSON object. </p>
<p>Let's start by taking a peek at the data, comparing the raw samples to the simplified samples; add the following code to your playground:</p>
<pre>if let rJson = loadedJSON["small_raw_airplane"],<br/>    let sJson = loadedJSON["small_simplified_airplane"]{<br/>    <br/>    if let rSketch = StrokeSketch.createFromJSON(json: rJson[0] as?   [String:Any]),<br/>        let sSketch = StrokeSketch.createFromJSON(json: sJson[0] as? [String:Any]){<br/>        drawSketch(sketch: rSketch)<br/>        drawSketch(sketch: sSketch)<br/>    }<br/>    <br/>    if let rSketch = StrokeSketch.createFromJSON(json: rJson[1] as? [String:Any]),<br/>        let sSketch = StrokeSketch.createFromJSON(json: sJson[1] as? [String:Any]){<br/>        drawSketch(sketch: rSketch)<br/>        drawSketch(sketch: sSketch)<br/>    }<br/>}</pre>
<p>In the previous code snippet, we are simply getting a reference to our loaded JSON files and passing the samples at index 0 and 1 to our <kbd>createFromJSON</kbd> file, which will return their <kbd>StrokeSketch</kbd> representation. We then proceed to pass this into our <kbd>drawSketch</kbd> method to create the view to render them. After running, you can preview each of the sketches by clicking on the eye icon located to the right-hand panel on the same line as the call to the method <kbd>drawSketch</kbd>. The following image presents both outputs side by side for comparison:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f25840e0-da15-4335-9fa2-3db291acfba2.png" style="width:43.42em;height:19.75em;"/></div>
<p>The major differences between the samples from the raw dataset and simplified dataset can be seen in the preceding figure. The raw sample is much larger and smoother. What is not obvious from the previous image is that the simplified sample is positioned to the top left while the raw one consists of points in their original and absolute positions (recalling that our <kbd>drawSketch</kbd> method rescales, if required, and centers the sketch). </p>
<p>As a reminder, the raw samples resemble the input we are expecting to receive from the user, while on the other hand our model was trained on the samples from the simplified dataset. Therefore, we need to perform the same preprocessing steps that have been used to transform the raw data into its simplified counterparts before feeding our model. These steps, described in the repository for the data at <a href="https://github.com/googlecreativelab/quickdraw-dataset">https://github.com/googlecreativelab/quickdraw-dataset</a>, are listed as follows, and this is what we will now implement in our playground:</p>
<ul>
<li>Align the drawing to the top-left corner to have minimum values of zero</li>
<li>Uniformly scale the drawing to have a maximum value of 255</li>
<li>Resample all strokes with a one pixel spacing</li>
<li>Simplify all strokes using the Ramer-Douglas-Peucker algorithm with an epsilon value of 2.0</li>
</ul>
<div class="packt_infobox">The Ramer–Douglas–Peucker algorithm takes a curve composed of line segments (strokes) and finds a simpler curve with fewer points. You can learn more about the algorithm here: <a href="https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm">https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm</a>.</div>
<p>The rationale behind these steps should be fairly self-explanatory and is highlighted from the figure showing the two sketches of an airplane. That is, the airplane should be invariant to its actual position on the screen and invariant to the scale. And simplifying the stroke makes it easier for our model to learn as it helps ensure that we only <span>capture</span> salient features.</p>
<p>Start off by creating an extension of your <kbd>StrokeSketch</kbd> class and stubbing out the method <kbd>simplify</kbd>, as shown:</p>
<pre>public func simplify() -&gt; StrokeSketch{<br/>    let copy = self.copy() as! StrokeSketch     <br/>}</pre>
<p>We will be mutating a clone of the instance of itself, which is why we first create a copy. We next want to calculate the scale factor required to scale the sketch to have a maximum height and/or width of 255 while respecting its aspect ratio; add the following code to your <kbd>simplify</kbd> method, which does just this:</p>
<pre>let minPoint = copy.minPoint<br/>let maxPoint = copy.maxPoint<br/>let scale = CGPoint(x: maxPoint.x-minPoint.x, y:maxPoint.y-minPoint.y)<br/><br/>var width : CGFloat = 255.0<br/>var height : CGFloat = 255.0<br/><br/>// adjust aspect ratio<br/>if scale.x &gt; scale.y{<br/>    height *= scale.y/scale.x<br/>} else{<br/>    width *= scale.y/scale.x<br/>} </pre>
<p>For each dimension (<kbd>width</kbd> and <kbd>height</kbd>), we have calculated the scale required to ensure that our sketch is either scaled up or down to a dimension of <kbd>255</kbd>. We now need to apply this to each of the points associated with each of the strokes held by the <kbd>StrokeSketch</kbd> class; as we're iterating through each point, it also makes sense to align our sketch to the top-left corner (<kbd>x= 0</kbd>, <kbd>y = 0</kbd>) as a required preprocessing step. We can do this simply by subtracting the minimum value of each of the dimensions. Append the following code to your <kbd>simplify</kbd> method to do this:</p>
<pre>for i in 0..&lt;copy.strokes.count{<br/>    copy.strokes[i].points = copy.strokes[i].points.map({ (pt) -&gt; CGPoint in<br/>        let x : CGFloat = CGFloat(Int(((pt.x - minPoint.x)/scale.x) * width))<br/>        let y : CGFloat = CGFloat(Int(((pt.y - minPoint.y)/scale.y) * height))        <br/>        return CGPoint(x:x, y:y)<br/>    })<br/>}   </pre>
<p>Our final step is to simplify the curve using the Ramer-Douglas-Peucker algorithm; to do this, we will make the <kbd>Stroke</kbd> responsible for implementing the details and just delegate the task there. Add the final piece of code to your <kbd>simplify</kbd> method within your <kbd>StrokeSketch</kbd> extension:</p>
<pre>copy.strokes = copy.strokes.map({ (stroke) -&gt; Stroke in<br/>    return stroke.simplify()<br/>})<br/><br/>return copy</pre>
<p>The <span>Ramer-Douglas-Peucker algorithm recursively traverses the curve, initially starting with the first and last point and finding the point that is furthest from this line segment. If the point is closer than a given threshold, then any points currently marked to be kept can be discarded, but if the point is greater than our threshold then that point must be kept. The algorithm then recursively calls itself with the first point and furthest point as well as furthest point and last point. After traversing the whole curve, the result is a simplified curve that only consists of the points marked as being kept, as described previously. The process is summarized in the following figure:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><img src="assets/d6c502b8-0c3e-4d6d-8ad9-80cce1158c4d.png" style="width:33.08em;height:21.83em;"/> </span></div>
<p>Let's start by extending the <kbd>CGPoint</kbd> structure to include a method for calculating the distance of a point given a line; add this code to your playground:</p>
<pre>public extension CGPoint{<br/>    <br/>    public static func getSquareSegmentDistance(p0:CGPoint,<br/>                                                p1:CGPoint,<br/>                                                p2:CGPoint) -&gt; CGFloat{<br/>        let x0 = p0.x, y0 = p0.y<br/>        var x1 = p1.x, y1 = p1.y<br/>        let x2 = p2.x, y2 = p2.y<br/>        var dx = x2 - x1<br/>        var dy = y2 - y1<br/>        <br/>        if dx != 0.0 &amp;&amp; dy != 0.0{<br/>            let numerator = (x0 - x1)<br/>                * dx + (y0 - y1)<br/>                * dy<br/>            let denom = dx * dx + dy * dy<br/>            let t = numerator / denom<br/>            <br/>            if t &gt; 1.0{<br/>                x1 = x2<br/>                y1 = y2<br/>            } else{<br/>                x1 += dx * t<br/>                y1 += dy * t<br/>            }<br/>        }<br/>        <br/>        dx = x0 - x1<br/>        dy = y0 - y1<br/>        <br/>        return dx * dx + dy * dy<br/>    }<br/>} </pre>
<p>Here, we have added a static method to the <kbd>CGPoint</kbd> structure; it calculates the perpendicular distance of a point given a line (which is the value we compare with our threshold to simplify our line, as previously described). Next, we will implement the recursive method as described, which will be used to build up the curve by testing and discarding any points under our threshold. As mentioned, we will encapsulate this functionality within the <kbd>Stroke</kbd> class itself, so we start off by stubbing out the extension:</p>
<pre>public extension Stroke{<br/>}</pre>
<p>Now, within the extension, add the recursive method:</p>
<pre>func simplifyDPStep(points:[CGPoint], first:Int, last:Int,<br/>                    tolerance:CGFloat, simplified: inout [CGPoint]){<br/>    <br/>    var maxSqDistance = tolerance<br/>    var index = 0<br/>    <br/>    for i in first + 1..&lt;last{<br/>        let sqDist = CGPoint.getSquareSegmentDistance(<br/>            p0: points[i],<br/>            p1: points[first],<br/>            p2: points[last])<br/>        <br/>        if sqDist &gt; maxSqDistance {<br/>            maxSqDistance = sqDist<br/>            index = i<br/>        }<br/>    }<br/>    <br/>    if maxSqDistance &gt; tolerance{<br/>        if index - first &gt; 1 {<br/>            simplifyDPStep(points: points,<br/>                           first: first,<br/>                           last: index,<br/>                           tolerance: tolerance,<br/>                           simplified: &amp;simplified)<br/>        }<br/>        <br/>        simplified.append(points[index])<br/>        <br/>        if last - index &gt; 1{<br/>            simplifyDPStep(points: points,<br/>                           first: index,<br/>                           last: last,<br/>                           tolerance: tolerance,<br/>                           simplified: &amp;simplified)<br/>        }<br/>    }<br/>} </pre>
<p>Most of this should make sense as it's a direct implementation of the algorithm described. We start off by finding the furthest distance, which must be greater than our threshold; otherwise, the point is ignored. We add the point to the array of points to keep and then pass each end of the segment to our recursive method until we have traversed the whole curve. </p>
<p>The last method we need to implement is the method responsible for initiating this process, which we will also encapsulate within our <kbd>Stroke</kbd> extension; so go ahead and add the following method to your extension:</p>
<pre>public func simplify(epsilon:CGFloat=3.0) -&gt; Stroke{<br/>    <br/>    var simplified: [CGPoint] = [self.points.first!]<br/>    <br/>    self.simplifyDPStep(points: self.points,<br/>                        first: 0, last: self.points.count-1,<br/>                        tolerance: epsilon * epsilon,<br/>                        simplified: &amp;simplified)<br/>    <br/>    simplified.append(self.points.last!)<br/>    <br/>    let copy = self.copy() as! Stroke<br/>    copy.points = simplified<br/>    <br/>    return copy<br/>}</pre>
<p>The <kbd>simplify</kbd> method simply (excuse the pun) creates an array of points of our simplified curve, adding the first point, before kicking off the recursive method we had just implemented. Then, when the curve has been traversed, it finally adds the last point before returning the <kbd>Stroke</kbd> with the simplified points. </p>
<p>At this point, we have implemented the functionality required to transform raw input into its simplified form, as specified in the <em>Quick, Draw!</em> repository. Let's verify our work by comparing our simplified version of a raw sketch with an existing simplified version of the same sketch. Add the following code to your playground:</p>
<pre>  if let rJson = loadedJSON["small_raw_airplane"],<br/>    let sJson = loadedJSON["small_simplified_airplane"]{<br/>    <br/>    if let rSketch = StrokeSketch.createFromJSON(json: rJson[2] as? [String:Any]),<br/>        let sSketch = StrokeSketch.createFromJSON(json: sJson[2] as? [String:Any]){<br/>        drawSketch(sketch: rSketch)<br/>        drawSketch(sketch: sSketch)<br/>        drawSketch(sketch: rSketch.simplify())<br/>    }<br/>}</pre>
<p>As we did before, you can click on the eye icon within the right<span>-hand-side</span> panel for each of the <kbd>drawSketch</kbd> calls to preview each of the sketches. The first is the sketch from the raw dataset, the second is from the simplified dataset, and third is by using our simplified implementation, using the sample from the raw dataset. If everything goes as per the plan, then you should see something that resembles the following:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7670a6d8-78ec-475a-be3c-787882db8ebd.png" style="width:50.42em;height:19.50em;"/></div>
<p>At close inspection, our simplified version looks as though it is more aggressive than the sample from the simplified dataset, but we can easily tweak this by adjusting our threshold. However, for all intents and purposes, this will suffice for now. At this point, we have the required functionality to simplify our dataset, transforming it to something that resembles the training dataset. But before feeding our data into the model, we have more preprocessing to do; let's do that now, starting with a quick discussion of what our model is expecting.  </p>
<p>Our model is expecting each sample to have three dimensions; point position <em>(x, y)</em> and a flag indicating whether the point is the last point for its associated stroke. The reason for having this flag is that we are passing in a fixed-length sequence of size 75. That is, each sketch will be either truncated to squeeze into this sequence or padded out with leading zeros to fill it. And using a flag is a way to add context indicating whether it is the end of the stroke or not (keeping in mind that our sequence represents our sketch and our sketch is made up of many strokes).</p>
<p>Next, as usual, we normalize the inputs to a range of <em>0.0 - 1.0</em> to avoid having our model fluctuate while training due to large weights. The last adjustment is converting our absolute values into deltas, which makes a lot of sense when you think about it. The first reason is that we want our model to be invariant to the actual position of each point; that is, we could draw the same sketch side by side, and ideally we want these to be classified as the same class. In the previous chapter, we achieved this by using a CNN operating on pixel data range and positions as we are doing here. The second reason for using deltas rather than absolute values is that the delta carries more useful information than the absolute position, that is, direction. After implementing this, we will be ready to test <span>out</span><span> </span><span>our model, so let's get going; start by adding the following extension and method that will be responsible for this preprocessing step:</span></p>
<pre>extension StrokeSketch{<br/>    <br/>    public static func preprocess(_ sketch:StrokeSketch)<br/>        -&gt; MLMultiArray?{<br/>        let arrayLen = NSNumber(value:75 * 3) <br/>                <br/>        guard let array = try? MLMultiArray(shape: [arrayLen],<br/>                                            dataType: .double)<br/>            else{ return nil }<br/>        <br/>        let simplifiedSketch = sketch.simplify()<br/>        <br/>    }<br/>}     </pre>
<p>Here we have added the static method <kbd>preprocess</kbd> to our <kbd>StrokeSketch</kbd> class via an extension; within this method, we begin by setting up the buffer that will be passed to our model. The size of this buffer needs to fit a full sequence, which is calculated simply by multiplying the sequence length (<kbd>75</kbd>) with the number of dimensions (<kbd>3</kbd>). We then call <kbd>simplify</kbd> on the <kbd>StrokeSketch</kbd> instance to obtain the simplified sketch, ensuring that it closely resembles the data we had trained our model on.  </p>
<p>Next, we will iterate through each point for every stroke, normalizing the point and determining the value of the flag (one indicating the end of the stroke; otherwise it's zero). Append the following code to your <kbd>preprocess</kbd> method: </p>
<pre>let minPoint = simplifiedSketch.minPoint<br/>let maxPoint = simplifiedSketch.maxPoint<br/>let scale = CGPoint(x: maxPoint.x-minPoint.x,<br/>                    y:maxPoint.y-minPoint.y)<br/><br/>var data = Array&lt;Double&gt;()<br/>    <br/>for i in 0..&lt;simplifiedSketch.strokes.count{<br/>    for j in 0..&lt;simplifiedSketch.strokes[i].points.count{<br/>        let point = simplifiedSketch.strokes[i].points[j]<br/>        let x = (point.x-minPoint.x)/scale.x<br/>        let y = (point.y-minPoint.y)/scale.y<br/>        let z = j == simplifiedSketch.strokes[i].points.count-1<br/>            ? 1 : 0<br/>        <br/>        data.append(Double(x))<br/>        data.append(Double(y))<br/>        data.append(Double(z))<br/>    }</pre>
<p>We start by obtaining the minimum and maximum values, which we will use when normalizing each point (using the equation <em>x<sup>i</sup>−min(x)/max(x)−min(x)</em>, where <em>x<sub>i</sub></em> is a single point and <em>x</em> represents all points within that stroke). Then we create a temporary place to store the data before iterating through all our points, normalizing each one, and determining the value of the flag as described previously. </p>
<p>We now want to calculate the deltas of each point and finally remove the last point as we are unable to calculate its delta; append the following to your <kbd>preprocess</kbd> method: </p>
<pre>let dataStride : Int = 3<br/>for i in stride(from: dataStride, to:data.count, by: dataStride){<br/>    data[i - dataStride] = data[i] - data[i - dataStride] <br/>    data[i - (dataStride-1)] = data[i+1] - data[i - (dataStride-1)] <br/>    data[i - (dataStride-2)] = data[i+2] <br/>}<br/><br/>data.removeLast(3)</pre>
<p>The previous code should be self-explanatory; the only notable point worth highlighting is that we are now dealing with a flattened array, and therefore we need to use a stride of <kbd>3</kbd> when traversing the data.</p>
<p>One last chunk of code to add! We need to ensure that our array is equal to 75 samples (our sequence length, that is, an array of length 225). We do this by either truncating the array if too large or padding it out if too small. We can easily do this while copying the data from our temporary array, <kbd>data</kbd>, across to the buffer that we will be passing to our model, <kbd>array</kbd>. Here we first calculate the starting index and then proceed to iterate through the whole sequence, copying the data across if the current index has passed our starting index, or else padding it with zeros. Add the following snippet to finish off your <kbd>preprocess</kbd> method:</p>
<pre>var dataIdx : Int = 0<br/>let startAddingIdx = max(array.count-data.count, 0)<br/><br/>for i in 0..&lt;array.count{<br/>    if i &gt;= startAddingIdx{<br/>        array[i] = NSNumber(value:data[dataIdx])<br/>        dataIdx = dataIdx + 1<br/>    } else{<br/>        array[i] = NSNumber(value:0)<br/>    }<br/>}<br/><br/>return array</pre>
<p>With our <kbd>preprocess</kbd> method now complete, we are ready to test out<span> our model</span>. We will start by instantiating our model (contained within the playground) and then feeding in a airplane sample we have used previously, before testing with the other categories. Append the following code to your playground:</p>
<pre>let model = quickdraw()<br/><br/>if let json = loadedJSON["small_raw_airplane"]{<br/>    if let sketch = StrokeSketch.createFromJSON(json: json[0] as? [String:Any]){<br/>        if let x = StrokeSketch.preprocess(sketch){<br/>            if let predictions = try? model.prediction(input:quickdrawInput(strokeSeq:x)){<br/>                print("Class label \(predictions.classLabel)")<br/>                print("Class label probability/confidence \(predictions.classLabelProbs["airplane"] ?? 0)")<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>If all goes well, your playground will output the following to the console:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a8f8d488-654f-42e6-9bff-09105f9155e2.png"/></div>
<p>It has predicted the category of airplane and done so fairly confidently (with a probability of approximately 77%). Before we migrate our code into our application, let's test with some other categories; we will start by implementing a method to handle all the leg work and then proceed to pass some samples to perform inference. Add the following method to your playground, which will be responsible for obtaining and preprocessing the sample before passing it to your model for prediction and then returning the results as a formatted string containing the most likely category and probability:</p>
<pre>func makePrediction(key:String, index:Int) -&gt; String{<br/>    if let json = loadedJSON[key]{<br/>        if let sketch = StrokeSketch.createFromJSON(<br/>            json: json[index] as? [String:Any]){<br/>            if let x = StrokeSketch.preprocess(sketch){<br/>                if let predictions = try? model.prediction(input:quickdrawInput(strokeSeq:x)){<br/>                    return "\(predictions.classLabel) \(predictions.classLabelProbs[predictions.classLabel] ?? 0)"<br/>                }<br/>            }<br/>        }<br/>    }<br/>    <br/>    return "None"<br/>}</pre>
<p>With most of the work now done, we are just left with the nail-biting task of testing that our preprocessing implementation and model are <span>sufficiently </span>able to predict the samples we pass. Let's test with each category; add the following code to your playground:</p>
<pre>print(makePrediction(key: "small_raw_airplane", index: 0))<br/>print(makePrediction(key: "small_raw_alarm_clock", index: 1))<br/>print(makePrediction(key: "small_raw_bee", index: 2))<br/>print(makePrediction(key: "small_raw_sailboat", index: 3))<br/>print(makePrediction(key: "small_raw_train", index: 4))<br/>print(makePrediction(key: "small_raw_truck", index: 5))<br/>print(makePrediction(key: "small_simplified_airplane", index: 0))</pre>
<p>The output for each of these can be seen in this screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2a2de26c-17a5-4434-b88a-f2779824fb32.png"/></div>
<p>Not bad! We managed to predict all the categories correctly, albeit the truck was only given the probability of 41%. And interestingly, our simplified airplane sample was given a higher probability (84%) than its counterpart from the raw dataset (77%). </p>
<p>Out of curiosity, let's peek at the truck sample we asked our model to predict:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/8d845293-d905-407a-a5c3-597757130114.png" style="width:44.17em;height:27.42em;"/></div>
<p>All due respect to the artist, but I would be pushed to predict a truck from this sketch, so full credit to our model.</p>
<p>We have now exposed our model to a variety of categories, and each one we are able to predict correctly, which implies that our preprocessing code has been satisfactorily implemented. We are now ready to migrate our code across to our application, but before doing so, one very last experiment. Let's think about how our model has been trained and how it will be used in the context of the application. The model was trained on sequences that are essentially strokes, the user made while drawing their sketch. This is precisely how users will be interacting with our application; they will be sketching something with a series (or sequence) of strokes; each time they finish a stroke, we want to try and predict what it is they are trying to draw. Let's mimic that behavior by building up a sample stroke by stroke, predicting after each subsequent stroke is added to evaluate how well the model performs in a more realistic setting. Add the following code to your playground:</p>
<pre>if let json = loadedJSON["small_raw_bee"]{<br/>    if let sketch = StrokeSketch.createFromJSON(json: json[2] as? [String:Any]){<br/>        let strokeCount = sketch.strokes.count<br/>        print("\(sketch.label ?? "" ) sketch has \(strokeCount) strokes")<br/>        <br/>        for i in (0..&lt;strokeCount-1).reversed(){<br/>            let copyOfSketch = sketch.copy() as! StrokeSketch<br/>            copyOfSketch.strokes.removeLast(i)<br/>            if let x = StrokeSketch.preprocess(copyOfSketch){<br/>                if let predictions = try? model.prediction(input:quickdrawInput(strokeSeq:x)){<br/>                    let label = predictions.classLabel<br/>                    let probability = String(format: "%.2f", predictions.classLabelProbs[predictions.classLabel] ?? 0)<br/>                    <br/>                    print("Guessing \(label) with probability of \(probability) using \(copyOfSketch.strokes.count) strokes")<br/>                }<br/>            }<br/>        }<br/>    }<br/>} </pre>
<p>Nothing new has being introduced here; we are just loading in a sketch, slowly building it up stroke by stroke as discussed, and passing up the partial sketch to our model to perform inference. Here are the results, with their corresponding sketches to give the results more context:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/63f5743c-3516-4b1e-9869-0381e9ee2c7b.png"/></div>
<p>All reasonable predictions, possibly uncovering how a lot of people draw a <strong>hockey puck</strong>, <strong>mouth</strong>, and <strong>bee</strong>. Now, satisfied with our implementation, let's move on to the next section, where we will migrate this code and look at how we can obtain and compile a model at runtime.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bringing it all together</h1>
                </header>
            
            <article>
                
<p>If you haven't done already, pull down the latest code from the accompanying repository: <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the directory <kbd>Chapter8/Start/QuickDrawRNN</kbd> and open the project <kbd>QuickDrawRNN.xcodeproj</kbd>. Once loaded, you will see a project that should look<span> familiar to you as it is almost a replica of the project we built in the previous chapter. For this reason, I won't be going over the details here, but feel free to refresh your memory by skimming through the previous chapter.</span></p>
<p><span>Rather I want to spend some time highlighting what I consider one of the most important aspects of designing and building the interface between people and machine learning systems. Let's start with this and then move on to migrating our code across from our playground project. </span></p>
<p>I consider Quick, Draw! a great example that highlights a major responsibility of the designer of any interface of a machine learning system. What makes it stand out is not the clever preprocessing that makes it invariant to scale and translation. Nor is it the sophisticated architecture that can effectively learn complex sequences, but rather the mechanism used to capture the training data. One major obstacle we have in creating intelligent systems is obtaining (enough) clean and labeled data that we can use to train our models. <em>Quick, Draw!</em> tackled this by, I assume, intentionally being a tool for capturing and labeling data through the façade of a compelling game—compelling enough to motivate a large number of users to generate sufficient amounts of labeled data. Although some of the sketches are questionable, the sheer number of sketches dilutes these outliers. </p>
<p>The point is that machine learning systems are not static, and we should design opportunities to allow the user to correct the system, where applicable, and capture new data, either implicitly (with the user's consent) and/or explicitly. Allowing a level of transparency between the user and system and allowing the user to correct the model when wrong not only provides us with new data to improve our model, but also—just as important—assists the user in building a useful mental model of the system. Thus it builds some intuition around the affordances of our system, which help them use it correctly.</p>
<p>In our example project, we can easily expose the predictions and provide the means for the user to correct the model. But to ensure that this chapter is concise, we will just look at how we obtain an updated model that typically (remembering that Core ML is suited for inference as opposed to training) we would train off the device. In such a case, you would upload the data to a central server and fetch an updated model when available. As mentioned before, here we will look at the latter: how we obtain the updated model. Let's see how. </p>
<div class="packt_infobox">Previously I mentioned, and implied, that you would typically upload new training data and train your model off the device. This, of course, is not the only option and it's reasonable to perform training on the device using the user's personal data to tune a model. The advantage of training locally is privacy and lower latency, but it has the disadvantage of diminishing collective intelligence, that is, improvement of the model from collective behavior. Google proposed a clever solution that ensured privacy and allowed for collaboration. In a post titled <em>Federated Learning: Collaborative Machine Learning without Centralized Training Data</em>, they described a technique of training locally on the device using personalized data and then uploading only the tuned model to the server, where it would average the weights from the crowd before updating a central model. I encourage you to read the post at <a href="https://research.googleblog.com/2017/04/federated-learning-collaborative.html">https://research.googleblog.com/2017/04/federated-learning-collaborative.html</a>.</div>
<p>As you may have come to expect when using Core ML, the bulk of the work is not interfacing with the framework but rather the activities before and after it. Compiling and instantiating a model can be done in just two lines of code, as follows:</p>
<pre>let compiledUrl = try MLModel.compileModel(at: modelUrl)<br/>let model = try MLModel(contentsOf: compiledUrl)</pre>
<p>Where <kbd>modelUrl</kbd> is a URL of a locally stored <kbd>.mlmodel</kbd> file. Passing it to <kbd>compileModel</kbd> will return the <kbd>.mlmodelc</kbd> file. This can be used to initialize an instance of <kbd>MLModel</kbd>, which provides the same capabilities as a model bundled with your application.</p>
<p>Downloading and compilation are time consuming. So you <span>not only </span>want to do this off the main thread but also want to avoid having to perform the task unnecessary; that is, cache locally and only update when required. Let's implement this functionality now; click on the <kbd>QueryFacade.swift</kbd> file on the left<span>-hand-side</span> panel to bring it to focus in the main editor window. Then add a new extension to the <span><kbd>QueryFacade</kbd> class, which is where we will add our code responsible for downloading and compiling the model.</span></p>
<p><span>Our first task is to test whether we need to download the model. We do this by simply checking whether we have the model and our model is considered recent. We will use <kbd>NSUserDefaults</kbd> to keep track of the location of the compiled model as well as a timestamp of when it was last updated. Add the following code to your extension of <kbd>QueryFacade</kbd>, which is be responsible for checking whether we need to download the model:</span></p>
<pre>private var SyncTimestampKey : String{<br/>    get{<br/>        return "model_sync_timestamp"<br/>    }<br/>}<br/><br/>private var ModelUrlKey : String{<br/>    get{<br/>        return "model_url"<br/>    }<br/>}<br/><br/>private var isModelStale : Bool{<br/>    get{<br/>        if let modelUrl = UserDefaults.standard.string(<br/>            forKey: self.ModelUrlKey){<br/>            if !FileManager.default.fileExists(atPath: modelUrl){<br/>                return true<br/>            }<br/>        }<br/>        <br/>        let daysToUpdate : Int = 10<br/>        let lastUpdated = Date(timestamp:UserDefaults.standard.integer(forKey: SyncTimestampKey))<br/><br/>        guard let numberOfDaysSinceUpdate = NSCalendar.current.dateComponents([.day], from: lastUpdated, to: Date()).day else{<br/>            fatalError("Failed to calculated elapsed days since the model was updated")<br/>        }<br/>        return numberOfDaysSinceUpdate &gt;= daysToUpdate<br/>    }<br/>}</pre>
<p>As mentioned, we first check whether the model exists, and if so, then test how many days have elapsed since the model was last updated, testing this against some arbitrary threshold for which we consider the model to be stale. </p>
<p>The next method we implement will be responsible for downloading the model (the <kbd>.mlmodel</kbd> file); this should look familiar to most iOS developers, with the only notable piece of code being the use of a semaphore to make the task synchronous, as the calling method will be running this off the main thread. Append the following code to your <kbd>QueryFacade</kbd> extension:</p>
<pre>private func downloadModel() -&gt; URL?{<br/>    guard let modelUrl = URL(<br/>        string:"https://github.com/joshnewnham/MachineLearningWithCoreML/blob/master/CoreMLModels/Chapter8/quickdraw.mlmodel?raw=true") else{<br/>            fatalError("Invalid URL")<br/>    }<br/>    <br/>    var tempUrl : URL?<br/>    <br/>    let sessionConfig = URLSessionConfiguration.default<br/>    let session = URLSession(configuration: sessionConfig)<br/>    <br/>    let request = URLRequest(url:modelUrl)<br/>    <br/><strong>    let semaphore = DispatchSemaphore(value: 0)</strong><br/>    <br/>    let task = session.downloadTask(with: request) { (tempLocalUrl, response, error) in<br/>        if let tempLocalUrl = tempLocalUrl, error == nil {<br/>            tempUrl = tempLocalUrl<br/>        } else {<br/>            fatalError("Error downloading model \(String(describing: error?.localizedDescription))")<br/>        }<br/>        <br/><strong>        semaphore.signal()</strong><br/>    }<br/>    task.resume()<br/><strong>    _ = semaphore.wait(timeout: .distantFuture)</strong><br/>    <br/>    return tempUrl<br/>}</pre>
<p>I have highlighted the statements related to making this task synchronous; essentially, calling <kbd>semaphore.wait(timeout: .distantFuture)</kbd> will hold the current thread until it is signaled to move on, via <kbd>semaphore.signal()</kbd>. If successful, this method returns the local URL of the downloaded file. </p>
<p>Our last task is to tie this all together; the next method we implement will be called when <kbd>QueryFacade</kbd> is instantiated (which we will add just after this). It will be responsible for checking whether we need to download the model, proceeding to download and compile if necessary, and instantiating an instance variable <kbd>model</kbd>, which we can use to perform inference. Append the final snippet of code to your <kbd>QueryFacade</kbd> extension:</p>
<pre>private func syncModel(){<br/>    queryQueue.async {<br/>        <br/>        if self.isModelStale{<br/>            guard let tempModelUrl = self.downloadModel() else{<br/>                return<br/>            }<br/>            <br/>            guard let compiledUrl = try? MLModel.compileModel(<br/>                at: tempModelUrl) else{<br/>                fatalError("Failed to compile model")<br/>            }<br/>            <br/>            let appSupportDirectory = try! FileManager.default.url(<br/>                for: .applicationSupportDirectory,<br/>                in: .userDomainMask,<br/>                appropriateFor: compiledUrl,<br/>                create: true)<br/>            <br/>            let permanentUrl = appSupportDirectory.appendingPathComponent(<br/>                compiledUrl.lastPathComponent)<br/>            do {<br/>                if FileManager.default.fileExists(<br/>                    atPath: permanentUrl.absoluteString) {<br/>                    _ = try FileManager.default.replaceItemAt(<br/>                        permanentUrl,<br/>                        withItemAt: compiledUrl)<br/>                } else {<br/>                    try FileManager.default.copyItem(<br/>                        at: compiledUrl,<br/>                        to: permanentUrl)<br/>                }<br/>            } catch {<br/>                fatalError("Error during copy: \(error.localizedDescription)")<br/>            }<br/>            <br/>            UserDefaults.standard.set(Date.timestamp,<br/>                                      forKey: self.SyncTimestampKey)<br/>            UserDefaults.standard.set(permanentUrl.absoluteString,<br/>                                      forKey:self.ModelUrlKey)<br/>        }<br/>        <br/>        guard let modelUrl = URL(<br/>            string:UserDefaults.standard.string(forKey: self.ModelUrlKey) ?? "")<br/>            else{<br/>            fatalError("Invalid model Url")<br/>        }<br/>        <br/>        self.model = try? MLModel(contentsOf: modelUrl)<br/>    }<br/>}</pre>
<p>We start by checking whether we need to download the model, and if so, proceed to download and compile it:</p>
<pre>guard let tempModelUrl = self.downloadModel() else{<br/>    return<br/>}<br/><br/>guard let compiledUrl = try? MLModel.compileModel(<br/>    at: tempModelUrl) else{<br/>    fatalError("Failed to compile model")<br/>}</pre>
<p>To avoid having to perform this step unnecessarily, we then save the details somewhere permanently, setting the model's location and the current timestamp in <kbd>NSUserDefaults</kbd>: </p>
<pre>let appSupportDirectory = try! FileManager.default.url(<br/>    for: .applicationSupportDirectory,<br/>    in: .userDomainMask,<br/>    appropriateFor: compiledUrl,<br/>    create: true)<br/><br/>let permanentUrl = appSupportDirectory.appendingPathComponent(<br/>    compiledUrl.lastPathComponent)<br/>do {<br/>    if FileManager.default.fileExists(<br/>        atPath: permanentUrl.absoluteString) {<br/>        _ = try FileManager.default.replaceItemAt(<br/>            permanentUrl,<br/>            withItemAt: compiledUrl)<br/>    } else {<br/>        try FileManager.default.copyItem(<br/>            at: compiledUrl,<br/>            to: permanentUrl)<br/>    }<br/>} catch {<br/>    fatalError("Error during copy: \(error.localizedDescription)")<br/>}<br/><br/>UserDefaults.standard.set(Date.timestamp,<br/>                          forKey: self.SyncTimestampKey)<br/>UserDefaults.standard.set(permanentUrl.absoluteString,<br/>                          forKey:self.ModelUrlKey)</pre>
<p>Finally, we instantiate and assign an instance of <kbd>MLModel</kbd> to our instance variable <kbd>model</kbd>. The last task is to update the constructor of the <kbd>QueryFacade</kbd> class to kick off this process when instantiated; update the <kbd>QueryFacade</kbd> <kbd>init</kbd> method with the following code:</p>
<pre>init() {<br/>    <strong>syncModel()</strong><br/>}</pre>
<p>At this stage, we have our model ready for performing inference; our next task is to migrate the code we developed in our playground to our project and then hook it all up. Given that we have spent the first part of this chapter discussing the details, I will skip the specifics here but rather include the additions for convenience and completeness.</p>
<p>Let's start with our extensions to the <kbd>CGPoint</kbd> structure; add a new swift file to your project called <kbd>CGPointRNNExtension.swift</kbd> and add the following code in it:</p>
<pre>extension CGPoint{<br/>    public static func getSquareSegmentDistance(<br/>        p0:CGPoint,<br/>        p1:CGPoint,<br/>        p2:CGPoint) -&gt; CGFloat{<br/>        let x0 = p0.x, y0 = p0.y<br/>        var x1 = p1.x, y1 = p1.y<br/>        let x2 = p2.x, y2 = p2.y<br/>        var dx = x2 - x1<br/>        var dy = y2 - y1<br/>        <br/>        if dx != 0.0 &amp;&amp; dy != 0.0{<br/>            let numerator = (x0 - x1) * dx + (y0 - y1) * dy<br/>            let denom = dx * dx + dy * dy<br/>            let t = numerator / denom<br/>            <br/>            if t &gt; 1.0{<br/>                x1 = x2<br/>                y1 = y2<br/>            } else{<br/>                x1 += dx * t<br/>                y1 += dy * t<br/>            }<br/>        }<br/>        <br/>        dx = x0 - x1<br/>        dy = y0 - y1<br/>        <br/>        return dx * dx + dy * dy<br/>    }<br/>}</pre>
<p>Next, add another new swift file to your project called <kbd>StrokeRNNExtension.swift</kbd> and add the following code:</p>
<pre>extension Stroke{<br/>    <br/>    public func simplify(epsilon:CGFloat=3.0) -&gt; Stroke{<br/>        <br/>        var simplified: [CGPoint] = [self.points.first!]<br/>        <br/>        self.simplifyDPStep(points: self.points,<br/>                            first: 0, last: self.points.count-1,<br/>                            tolerance: epsilon * epsilon,<br/>                            simplified: &amp;simplified)<br/>        <br/>        simplified.append(self.points.last!)<br/>        <br/>        let copy = self.copy() as! Stroke<br/>        copy.points = simplified<br/>        <br/>        return copy<br/>    }<br/>    <br/>    func simplifyDPStep(points:[CGPoint],<br/>                        first:Int,<br/>                        last:Int,<br/>                        tolerance:CGFloat,<br/>                        simplified: inout [CGPoint]){<br/>        <br/>        var maxSqDistance = tolerance<br/>        var index = 0<br/>        <br/>        for i in first + 1..&lt;last{<br/>            let sqDist = CGPoint.getSquareSegmentDistance(<br/>                p0: points[i],<br/>                p1: points[first],<br/>                p2: points[last])<br/>            <br/>            if sqDist &gt; maxSqDistance {<br/>                maxSqDistance = sqDist<br/>                index = i<br/>            }<br/>        }<br/>        <br/>        if maxSqDistance &gt; tolerance{<br/>            if index - first &gt; 1 {<br/>                simplifyDPStep(points: points,<br/>                               first: first,<br/>                               last: index,<br/>                               tolerance: tolerance,<br/>                               simplified: &amp;simplified)<br/>            }<br/>            <br/>            simplified.append(points[index])<br/>            <br/>            if last - index &gt; 1{<br/>                simplifyDPStep(points: points,<br/>                               first: index,<br/>                               last: last,<br/>                               tolerance: tolerance,<br/>                               simplified: &amp;simplified)<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Finally, we will add a couple of methods that we implemented in the playground to our <kbd>StrokeSketch</kbd> class to handle the required preprocessing; start by adding a new <kbd>.swift</kbd> file called <kbd>StrokeSketchExtension.swift</kbd> and block out the extension as follows:</p>
<pre>import UIKit<br/>import CoreML<br/><br/>extension StrokeSketch{<br/><br/>}</pre>
<p>Next, we copy and paste in the <kbd>simplify</kbd> method, which we implement in the playground as follows:</p>
<pre>public func simplify() -&gt; StrokeSketch{<br/>    let copy = self.copy() as! StrokeSketch<br/>    copy.scale = 1.0<br/>    <br/>    let minPoint = copy.minPoint<br/>    let maxPoint = copy.maxPoint<br/>    let scale = CGPoint(x: maxPoint.x-minPoint.x,<br/>                        y:maxPoint.y-minPoint.y)<br/>    <br/>    var width : CGFloat = 255.0<br/>    var height : CGFloat = 255.0<br/>    <br/>    if scale.x &gt; scale.y{<br/>        height *= scale.y/scale.x<br/>    } else{<br/>        width *= scale.y/scale.x<br/>    }<br/>    <br/>    // for each point, subtract the min and divide by the max<br/>    for i in 0..&lt;copy.strokes.count{<br/>        copy.strokes[i].points = copy.strokes[i].points.map({<br/>            (pt) -&gt; CGPoint in<br/>            let x : CGFloat = CGFloat(<br/>                Int(((pt.x - minPoint.x)/scale.x) * width)<br/>            )<br/>            let y : CGFloat = CGFloat(<br/>                Int(((pt.y - minPoint.y)/scale.y) * height)<br/>            )<br/>            <br/>            return CGPoint(x:x, y:y)<br/>        })<br/>    }<br/>    <br/>    copy.strokes = copy.strokes.map({ (stroke) -&gt; Stroke in<br/>        return stroke.simplify()<br/>    })<br/>    <br/>    return copy<br/>}</pre>
<p>As a reminder, this method is responsible for the preprocessing of a sequence of strokes, as described previously. Next, we add our static method <kbd>preprocess</kbd> to the <span><kbd>StrokeSketch</kbd> </span>extension, which takes an instance of <kbd>StrokeSketch</kbd> and is responsible for putting its simplified state into a data structure that we can pass to our model for inference:</p>
<pre>public static func preprocess(_ sketch:StrokeSketch)<br/>    -&gt; MLMultiArray?{<br/>    let arrayLen = NSNumber(value:75 * 3) <br/>    <br/>    let simplifiedSketch = sketch.simplify()<br/>    <br/>    guard let array = try? MLMultiArray(shape: [arrayLen],<br/>                                        dataType: .double)<br/>        else{ return nil }<br/>    <br/>    <br/>    let minPoint = simplifiedSketch.minPoint<br/>    let maxPoint = simplifiedSketch.maxPoint<br/>    let scale = CGPoint(x: maxPoint.x-minPoint.x,<br/>                        y:maxPoint.y-minPoint.y)<br/>    <br/>    var data = Array&lt;Double&gt;()<br/>    for i in 0..&lt;simplifiedSketch.strokes.count{<br/>        for j in 0..&lt;simplifiedSketch.strokes[i].points.count{<br/>            let point = simplifiedSketch.strokes[i].points[j]<br/>            let x = (point.x-minPoint.x)/scale.x<br/>            let y = (point.y-minPoint.y)/scale.y<br/>            let z = j == simplifiedSketch.strokes[i].points.count-1 ?<br/>                1 : 0<br/>            <br/>            data.append(Double(x))<br/>            data.append(Double(y))<br/>            data.append(Double(z))<br/>        }<br/>    }<br/>    <br/>    let dataStride : Int = 3<br/>    for i in stride(from: dataStride, to:data.count, by: dataStride){<br/>        data[i - dataStride] = data[i] - data[i - dataStride] <br/>        data[i - (dataStride-1)] = data[i+1] - data[i - (dataStride-1)] <br/>        data[i - (dataStride-2)] = data[i+2] // EOS<br/>    }<br/><br/>    data.removeLast(3)<br/>    <br/>    var dataIdx : Int = 0<br/>    let startAddingIdx = max(array.count-data.count, 0)<br/>    <br/>    for i in 0..&lt;array.count{<br/>        if i &gt;= startAddingIdx{<br/>            array[i] = NSNumber(value:data[dataIdx])<br/>            dataIdx = dataIdx + 1<br/>        } else{<br/>            array[i] = NSNumber(value:0)<br/>        }<br/>    }<br/>    <br/>    return array<br/>}</pre>
<p>If anything looks unfamiliar, then I encourage you to revisit the previous section, where we delve into the details of what these methods do (and why).</p>
<p>We now have our model and functionality for preprocessing the input; our last task is to tie this all together. Head back to the <kbd>QueryFacade</kbd> class and locate the method <kbd>classifySketch</kbd>. As a reminder, this method is called via <kbd>queryCurrentSketch</kbd>, which in turn is triggered anytime the user completes a stroke. The method is expected to return a dictionary of category and probability pairs, which is then used to search and download related drawings of most likely categories. At this point, it's simply a matter of using the work we have previously done, with one little caveat. If you recall from previous chapters, when we imported our model into the project, Xcode would conveniently generate a strongly typed wrapper for our model and its associated inputs and outputs. A disadvantage of downloading and importing at runtime is that we forgo these generated wrappers and are left to do it manually.</p>
<p>Starting backwards, after making the prediction, we are expecting an instance of <kbd>MLFeatureProvider</kbd> to be returned, which in turn has a method called <kbd>featureValue</kbd>. This returns an instance of <kbd>MLFeatureValue</kbd> for a given output key (<kbd>classLabelProbs</kbd>). The returned instance of <kbd>MLFeatureValue</kbd> exposes properties set by the model during inference; here we are interested in the <kbd>dictionaryValue</kbd> property of type <kbd>[String:Double]</kbd> (category and its associated probability).</p>
<p>Obviously, to obtain this output, we need to call <kbd>predict</kbd> on our model, which is expecting an instance adhering to the <kbd>MLFeatureProvider</kbd> <span>protocol </span>that was generated for us, as mentioned previously. Given that in most instances you will have access and knowledge of the model, the easiest way to generate this wrapper is to import the model and extract the generated input, which is exactly what we will do. </p>
<p>Locate the file <kbd>CoreMLModels/Chapter8/quickdraw.mlmodel</kbd> in the accompanying repository <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>, and drag the file into your project as we have done in previous chapters. Once imported, select it from the left-hand-side panel and click on the arrow button within the <span class="packt_screen">Model Class</span> section, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/53a57d2c-ab0c-45af-9142-23417cbc2556.png" style="width:70.25em;height:40.75em;"/></div>
<p>This will open up the generated classes; locate the class <kbd>quickdrawInput</kbd> and copy and paste it to your <kbd>QueryFacade.swift</kbd>, ensuring that it's outside the <kbd>QueryFacade</kbd> class (or extensions). Because we are only concerned with the <kbd>strokeSeq</kbd> input, we can strip all other variables; clean it up such that you are left with something like the following:</p>
<pre>class quickdrawInput : MLFeatureProvider {<br/>    <br/>    var strokeSeq: MLMultiArray<br/>    <br/>    var featureNames: Set&lt;String&gt; {<br/>        get {<br/>            return ["strokeSeq"]<br/>        }<br/>    }<br/>    <br/>    func featureValue(for featureName: String) -&gt; MLFeatureValue? {<br/>        if (featureName == "strokeSeq") {<br/>            return MLFeatureValue(multiArray: strokeSeq)<br/>        }<br/>        return nil<br/>    }<br/>    <br/>    init(strokeSeq: MLMultiArray) {<br/>        self.strokeSeq = strokeSeq<br/>    }<br/>}</pre>
<p>We are finally ready to perform inference; return to the <kbd>classifySketch</kbd> method within the <kbd>QueryFacade</kbd> class and add the following code: </p>
<pre>if let strokeSketch = sketch as? StrokeSketch, let<br/>    x = StrokeSketch.preprocess(strokeSketch){<br/>    <br/>    if let modelOutput = try! model?.prediction(from:quickdrawInput(strokeSeq:x)){<br/>        if let classPredictions = modelOutput.featureValue(<br/>            for: "classLabelProbs")?.dictionaryValue as? [String:Double]{<br/>            <br/>            let sortedClassPredictions = classPredictions.sorted(<br/>                by: { (kvp1, kvp2) -&gt; Bool in<br/>                kvp1.value &gt; kvp2.value<br/>            })<br/>            <br/>            return sortedClassPredictions<br/>        }<br/>    }<br/>}<br/><br/>return nil</pre>
<p>No doubt most of this will look familiar to you; we start by extracting the features via the <kbd>preprocess</kbd> method we implemented at the start of this chapter. Once we have obtained these features, we wrap them in an instance of <kbd>quickdrawInput</kbd>, before passing them to our model's <kbd>prediction</kbd> method to perform inference. If successful, we are returned the output, with which we proceed to extract the appropriate output, as discussed previously. Finally we sort the results before returning them to the caller. </p>
<p>With that complete, you are now in a good position to test. Build and deploy to the simulator or device, and if everything goes as planned, you should be able to test the accuracy of your mode (or drawing, depending on how you look at it):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/2cdd1326-a576-4740-aead-489a0e951658.png" style="width:37.50em;height:19.08em;"/></div>
<p>Let's wrap up this chapter by reviewing what we have covered. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we revisited a previous problem (sketch recognition) but used a different dataset and different approach. Previously, we tackled the problem using CNN, but in this chapter, we identified nuances of how the data was collected, which in turn allowed us to take a different approach using an RNN. As usual, most of the effort was spent in preparing the data for the model. This, in doing so, highlighted some techniques we can use to make our data invariant to scale and translation, as well as the usefulness of reducing details of the inputs (through simplification) to assist our model in more easily finding patterns.</p>
<p>Finally, we highlighted an important aspect of designing interfaces for machine learning systems, that is, adding a layer of transparency and control for the user to help them build a useful mental model of the system and improve the model through explicit user feedback, such as corrections. </p>
<p>Let's continue our journey into the world of machine learning applications and dive into the next chapter, where we will look at our final visual application: image segmentation.</p>


            </article>

            
        </section>
    </body></html>