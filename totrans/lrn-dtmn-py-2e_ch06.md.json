["```py\nimport twitter\nconsumer_key = \"<Your Consumer Key Here>\"\nconsumer_secret = \"<Your Consumer Secret Here>\"\naccess_token = \"<Your Access Token Here>\"\naccess_token_secret = \"<Your Access Token Secret Here>\"\nauthorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n\n```", "```py\nimport os\noutput_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_tweets.json\")\n\n```", "```py\nt = twitter.Twitter(auth=authorization)\n\n```", "```py\nimport json\nwith open(output_filename, 'a') as output_file:\n    search_results = t.search.tweets(q=\"python\", count=100)['statuses']\n    for tweet in search_results:\n        if 'text' in tweet:\n            output_file.write(json.dumps(tweet))\n            output_file.write(\"nn\")\n\n```", "```py\nimport json\nimport os\n\n# Input filename\ninput_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_tweets.json\")\n# Output filename\nlabels_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_classes.json\")\n\ntweets = []\nwith open(input_filename) as inf:\n    for line in inf:\n        if len(line.strip()) == 0:\n            continue\n        tweets.append(json.loads(line))\n\n```", "```py\nlabels = []\nif os.path.exists(labels_filename):\n    with open(labels_filename) as inf:\n        labels = json.load(inf)\n\n```", "```py\ndef get_next_tweet():\n    return tweets[len(labels)]['text']\n\n```", "```py\n%%html\n<div name=\"tweetbox\">\n Instructions: Click in text box. Enter a 1 if the tweet is relevant, enter 0 otherwise.<br>\n Tweet: <div id=\"tweet_text\" value=\"text\"></div><br>\n <input type=text id=\"capture\"></input><br>\n</div>\n\n<script>\nfunction set_label(label){\n var kernel = IPython.notebook.kernel;\n kernel.execute(\"labels.append(\" + label + \")\");\n load_next_tweet();\n}\n\nfunction load_next_tweet(){\n var code_input = \"get_next_tweet()\";\n var kernel = IPython.notebook.kernel;\n var callbacks = { 'iopub' : {'output' : handle_output}};\n kernel.execute(code_input, callbacks, {silent:false});\n}\n\nfunction handle_output(out){\n console.log(out);\n var res = out.content.data[\"text/plain\"];\n $(\"div#tweet_text\").html(res);\n}\n\n$(\"input#capture\").keypress(function(e) {\n console.log(e);\n if(e.which == 48) {\n // 0 pressed\n set_label(0);\n $(\"input#capture\").val(\"\");\n }else if (e.which == 49){\n // 1 pressed\n set_label(1); \n $(\"input#capture\").val(\"\");\n }\n});\n\nload_next_tweet();\n</script>\n\n```", "```py\nwith open(labels_filename, 'w') as outf:\n    json.dump(labels, outf)\n\n```", "```py\nimport os\ninput_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_tweets.json\")\nlabels_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_classes.json\")\nreplicable_dataset = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"replicable_dataset.json\")\n\n```", "```py\nimport json\ntweets = []\nwith open(input_filename) as inf:\n    for line in inf:\n        if len(line.strip()) == 0:\n            continue\n        tweets.append(json.loads(line))\nif os.path.exists(labels_filename):\n    with open(labels_filename) as inf:\n        labels = json.load(inf)\n\n```", "```py\ndataset = [(tweet['id'], label) for label, tweet in zip(labels, tweets)]\n\n```", "```py\nwith open(replicable_dataset, 'w') as outf:\n    json.dump(dataset, outf)\n\n```", "```py\nimport os\ntweet_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"replicable_python_tweets.json\")\nlabels_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"replicable_python_classes.json\")\nreplicable_dataset = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"replicable_dataset.json\")\n\n```", "```py\nimport json\nwith open(replicable_dataset) as inf:\n    tweet_ids = json.load(inf)\n\n```", "```py\nactual_labels = []\nlabel_mapping = dict(tweet_ids)\n\n```", "```py\nimport twitter\nconsumer_key = \"<Your Consumer Key Here>\"\nconsumer_secret = \"<Your Consumer Secret Here>\"\naccess_token = \"<Your Access Token Here>\"\naccess_token_secret = \"<Your Access Token Secret Here>\"\nauthorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\nt = twitter.Twitter(auth=authorization)\n\n```", "```py\nall_ids = [tweet_id for tweet_id, label in tweet_ids]\nwith open(tweet_filename, 'a') as output_file:\n    # We can lookup 100 tweets at a time, which saves time in asking twitter for them\n    for start_index in range(0, len(all_ids), 100):\n        id_string = \",\".join(str(i) for i in all_ids[start_index:start_index+100])\n        search_results = t.statuses.lookup(_id=id_string)\n        for tweet in search_results:\n            if 'text' in tweet:\n                # Valid tweet - save to file\n                output_file.write(json.dumps(tweet))\n                output_file.write(\"nn\")\n                actual_labels.append(label_mapping[tweet['id']])\n\n```", "```py\nwith open(labels_filename, 'w') as outf:\n    json.dump(actual_labels, outf)\n\n```", "```py\n s = \"\"\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie. One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them. In the Land of Mordor where the Shadows lie. \"\"\".lower()\nwords = s.split()\nfrom collections import Counter\nc = Counter(words)\nprint(c.most_common(5))\n\n```", "```py\nP(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0) \n= 0.3 x 0.6 x 0.6 x 0.7 \n= 0.0756\n\n```", "```py\nP(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.0567 \n\n```", "```py\nP(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1)\n         = 0.7 x 0.7 x 0.6 x 0.9\n         = 0.2646 P(C=1|D) \n         = P(C=1)P(D|C=1)\n         = 0.25 * 0.2646\n         = 0.06615\n\n```", "```py\nimport spacy\nfrom sklearn.base import TransformerMixin\n\n# Create a spaCy parser\nnlp = spacy.load('en')\n\nclass BagOfWords(TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        results = []\n        for document in X:\n            row = {}\n            for word in list(nlp(document, tag=False, parse=False, entity=False)):\n                if len(word.text.strip()): # Ignore words that are just whitespace\n                    row[word.text] = True\n                    results.append(row)\n        return results\n\n```", "```py\nfrom sklearn.feature_extraction import DictVectorizer\n\n```", "```py\nfrom sklearn.naive_bayes import BernoulliNB\n\n```", "```py\nimport os\ninput_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_tweets.json\")\nlabels_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_classes.json\")\n\n```", "```py\nimport json\n\ntweets = []\nwith open(input_filename) as inf:\n    for line in inf:\n        if len(line.strip()) == 0: continue\n        tweets.append(json.loads(line)['text'])\n\nwith open(labels_filename) as inf:\n    labels = json.load(inf)\n\n# Ensure only classified tweets are loaded\ntweets = tweets[:len(labels)]\n\n```", "```py\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('bag-of-words', BagOfWords()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB()) ])\n\n```", "```py\nfrom sklearn.cross_validation import cross_val_score\nscores = cross_val_score(pipeline, tweets, labels, scoring='f1')\n# We then print out the average of the scores:\nimport numpy as np\nprint(\"Score: {:.3f}\".format(np.mean(scores)))\n\n```", "```py\n model = pipeline.fit(tweets, labels)\n\n```", "```py\nnb = model.named_steps['naive-bayes']\nfeature_probabilities = nb.feature_log_prob_\n\n```", "```py\ntop_features = np.argsort(-nb.feature_log_prob_[1])[:50]\n\n```", "```py\ndv = model.named_steps['vectorizer']\n\n```", "```py\nfor i, feature_index in enumerate(top_features):\n    print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index]))\n\n```", "```py\n9 for 0.175\n14 ) 0.10625\n15 ( 0.10625\n22 jobs 0.0625\n29 Developer 0.05\n\n```"]