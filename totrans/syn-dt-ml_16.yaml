- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Photorealism in Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn why we need photorealistic synthetic data in
    computer vision. Then, you will explore the main approaches to generating photorealistic
    synthetic data. After that, you will comprehend the main challenges and limitations.
    Although this chapter focuses on computer vision, the discussion can be generalized
    to other domains and applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data photorealism for computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Photorealism approaches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Photorealism evaluation metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and limitations of photorealistic synthetic data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data photorealism for computer vision
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn why photorealism is essential in computer vision.
    Photorealism of synthetic data is one of the main factors that mitigates the domain
    gap between real and synthetic data. Thus, training computer vision models on
    photorealistic synthetic data improves the performance of these models on real
    data. For more details, please refer to *Hypersim: A Photorealistic Synthetic
    Dataset for Holistic Indoor Scene Understanding* ([https://arxiv.org/abs/2011.02523](https://arxiv.org/abs/2011.02523))
    and *A Review of Synthetic Image Data and Its Use in Computer Vision* ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9698631](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9698631)).
    Additionally, synthetic data can be used to evaluate computer vision algorithms.
    However, evaluating these models on non-photorealistic synthetic data may cause
    these models to show poor performance not because of the challenging nature of
    the test scenarios but because of the domain gap itself. Thus, photorealistic
    synthetic data is essential to effectively train and accurately evaluate ML models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us discuss the main benefits of utilizing photorealistic synthetic
    data. First, let us delve into feature extraction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer vision algorithms usually rely on automatic feature extraction, which
    is learned in the training stage. ML models learn how to identify the most reliable
    and discriminative features and patterns, which subsequent submodules leverage
    to learn the actual task, such as semantic segmentation, depth estimation, and
    visual object tracking. Training your computer vision model on non-photorealistic
    synthetic data that oversimplifies the real world will lead to inappropriate feature
    extraction. Conversely, photorealistic synthetic data helps the ML model to learn
    how to extract discriminative features. Thus, the ML model will perform well in
    the real world. This is because realistic data helps ML models to better understand
    the relationship between scene elements, how they affect each other, and how they
    contribute to the task being learned.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Domain gap
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Photorealistic synthetic data mitigates the domain gap between synthetic and
    real domains. The main reason is that realistic data partially resamples the real
    data, which helps computer vision models to be trained on data that is closer
    to the environment where the model will be deployed in practice. Thus, the model
    can still generalize well from the synthetic data learned in the training stage.
    On the other hand, large-scale, diverse, but non-realistic synthetic data may
    enlarge the domain gap and significantly hinder the performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating simulators that simulate realistic lighting, textures, shaders, animations,
    and camera movements enables researchers to generate large-scale and diverse synthetic
    training datasets that properly reflect the challenges and varieties in the real
    world. Thus, computer vision algorithms can be trained on more real scenarios
    to learn how to adapt to the actual complexities of the real world. This is important
    to make computer vision algorithms more robust in the real world where collecting
    real data is extremely expensive or not applicable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking performance
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data provides a more accurate and efficient way to generate the ground
    truth (refer to [*Chapter 5*](B18494_05.xhtml#_idTextAnchor083)). The ground truth
    is essential for assessing ML models’ performance. Photorealistic synthetic data
    enables us to ensure that the performance of synthetic data appropriately reflects
    that expected in the real world. Conversely, non-realistic synthetic data is less
    suitable for accurate evaluation and benchmarking.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Photorealism approaches
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about and explore the main approaches usually
    deployed to generate photorealistic synthetic data. We will learn about the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Physically based rendering
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural style transfer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physically Based Rendering (PBR)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Physically Based Rendering** (**PBR**) approach is widely used in game
    engines such as *Unity* and *Unreal* to accurately simulate how materials in the
    3D virtual world interact with light. In the real world, this is a complex process,
    thus it requires a significant understanding of optics and many simplifications
    to make these processes applicable to game engines. **Physically based materials**
    are essential to this approach. They resemble how similar materials in the real
    world interact with light. These materials usually have properties and parameters
    that are calculated based on real measurements from real-world materials. The
    properties may include absorption, scattering, and refraction coefficients and
    parameters. It should be noted that the main principle behind PBR is **energy
    conservation**, which means that light energy reflected and scattered by a material
    should not exceed the total incoming or received light energy by this material.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: As expected, deploying a photorealistic rendering pipeline will help us to simulate
    and render more accurate and realistic light behaviors and materials. Thus, we
    can generate more photorealistic synthetic data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Neural style transfer** is a well-known technique that transfers an artistic
    style from one image to another while preserving the content of the latter. This
    method can be applied to synthetic datasets to improve their photorealism and
    thus mitigate the domain gap between synthetic and real data. For example, the
    **Sim2Real**-style transfer model can be deployed to bridge the gap between synthetic
    and real data for the task of pose estimation. For more information, please refer
    to *Sim2Real Instance-Level Style Transfer for 6D Pose Estimation* ([https://arxiv.org/abs/2203.02069](https://arxiv.org/abs/2203.02069)).
    Additionally, there are many interesting works that explore how to adapt the *GTA5*
    synthetic dataset (https://www.v7labs.com/open-datasets/gta5), which was generated
    from the *Grand Theft Auto* *V* video game, to the real *Cityscapes* dataset ([https://www.cityscapes-dataset.com](https://www.cityscapes-dataset.com)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Photorealism evaluation metrics
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main issues within this subject matter is quantitatively assessing
    the photorealism of the generated synthetic images. In this section, we will explore
    the main metrics usually used. We will explore the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Structural Similarity Index** **Measure** (**SSIM**)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learned Perceptual Image Patch** **Similarity** (**LPIPS**)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expert evaluation
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structural Similarity Index Measure (SSIM)
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SSIM is one of the most widely used metrics to measure the structural similarity
    between two images. It was first introduced in the paper titled *Image quality
    assessment: from error visibility to structural similarity* ([https://ieeexplore.ieee.org/document/1284395](https://ieeexplore.ieee.org/document/1284395)).
    The SSIM metric does not compare individual pixels of the two images. However,
    it considers a group of pixels assuming that spatially close pixels have inter-dependencies.
    These dependencies can be linked to the actual structure of objects that were
    captured and presented by the given images. SSIM specifically focuses on the spatial
    relationships among pixels, such as edges and textures, to assess how close an
    image is to a reference one.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Recently, it was shown that SSIM may lead to incorrect or unexpected results
    when utilized to compare images or when included in the training loss of ML models.
    For more information, please refer to *Understanding* *SSIM* ([https://arxiv.org/pdf/2006.13846.pdf](https://arxiv.org/pdf/2006.13846.pdf)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Learned Perceptual Image Patch Similarity (LPIPS)
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LPIPS** measures the distance between images in the feature space by leveraging
    networks trained for computer vision tasks on large-scale datasets, for example,
    *VGG* trained on the *ImageNet* dataset. It was found that LPIPS gives more similar
    results to how humans perceive similarity between images. For more information,
    please refer to *The Unreasonable Effectiveness of Deep Features as a Perceptual
    Metric* ([https://arxiv.org/abs/1801.03924](https://arxiv.org/abs/1801.03924)).
    In this paper, it was found that ML models trained on complex visual tasks learn
    a rich, general-purpose, and useful visual representation of the world. This knowledge
    can be leveraged to assess the visual similarity between images in a similar manner
    to how humans may perceive this similarity.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Expert evaluation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In certain applications, we may need to request a domain expert evaluation of
    the generated synthetic images. For example, assume your generative model is generating
    synthetic images of **Computerized Tomography** (**CT**) scans that will be used
    later to train a cancer prediction ML model. We can still leverage qualitative
    metrics, such as SSIM to assess structural similarity with a real data counterpart,
    **Peak Signal to Noise Ratio** (**PSNR**) to measure the quality of the generated
    images, **Fréchet Inception Distance** (**FID**) to give us an idea about the
    realism and diversity of the generated samples, and LPIPS to assess the perceptual
    similarity to real data. However, expert evaluation is still essential for these
    critical problems. Expert evaluation of the synthetically generated data is essential
    to verify its validity, quality, and diversity. Most importantly, this evaluation
    is essential to confirm that synthetic data adheres to ethical standards.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us discuss some of the main challenges in generating photorealistic
    synthetic data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and limitations of photorealistic synthetic data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will explore the main challenges that hinder generating
    photorealistic synthetic data in practice. We will highlight the following limitations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Creating hyper-realistic scenes
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The real world is complex, diverse, and intricate with details. Scene elements
    in reality have various shapes, sophisticated dynamics, and highly non-linear
    interactions. Additionally, our vision and perception of the world are limited
    and subject to many factors, such as cognitive biases and color perception. Additionally,
    we may judge photorealism differently based on the context and evaluator. For
    example, what is more photorealistic, realistic foreground objects and a non-realistic
    background or the opposite? All these aspects together make generating highly
    realistic scenes rather hard in practice.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Resources versus photorealism trade-off
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Budget, time, skills, and other factors limit the photorealism of the generated
    synthetic data. As expected, simulating realistic worlds populated with high-poly
    3D models and diverse, realistic animations necessitates substantial computational
    resources. Additionally, employing advanced and complex light- rendering mechanisms,
    such as ray tracing and PBR, further increases the demand for more processing
    capabilities and resources. **Ray tracing** is a rendering technique that can
    simulate the realistic behavior of light and its complex interactions with scene
    elements. Thus, there is always a trade-off observed between resources and photorealism.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 预算、时间、技能和其他因素限制了生成的合成数据的照片级真实感。正如预期的那样，模拟由高多边形3D模型和多样化、逼真的动画组成的世界需要大量的计算资源。此外，采用高级和复杂的光渲染机制，如光线追踪和PBR，进一步增加了对更多处理能力和资源的需求。“光线追踪”是一种渲染技术，可以模拟光的真实行为及其与场景元素的复杂交互。因此，在资源和照片级真实感之间始终存在权衡。
- en: Therefore, it is very important to identify what you mean by photorealism for
    your particular problem. Additionally, you need to carefully consider which metrics
    you will deploy to assess the quality and photorealism of the generated synthetic
    data, taking into account the available resources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于您特定的实际问题，确定您所说的“照片级真实感”是什么非常重要。此外，您还需要仔细考虑您将部署哪些指标来评估生成的合成数据的质量和照片级真实感，同时考虑到可用的资源。
- en: Summary
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the main reasons that motivate researchers to strive
    to achieve high photorealism in generated synthetic data for computer vision problems.
    You learned about two main approaches usually utilized for that aim. Then, you
    explored well-known quantitative and qualitative measures deployed to assess the
    photorealism of the generated synthetic data. Finally, you examined some issues
    that hinder generating ideal photorealistic synthetic data in practice. In the
    next and final chapter, we will wrap up and conclude the book.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了促使研究人员努力在计算机视觉问题中生成的合成数据中实现高照片级真实感的主要原因。您了解了通常用于此目的的两个主要方法。然后，您探讨了用于评估生成的合成数据照片级真实感的知名定量和定性指标。最后，您检查了一些在实践中阻碍生成理想照片级真实感合成数据的问题。在下一章和最后一章中，我们将总结并结束本书。
