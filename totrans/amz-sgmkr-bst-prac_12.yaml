- en: 'Chapter 9: Updating Production Models Using Amazon SageMaker Endpoint Production
    Variants'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deployed production model needs to be updated for a variety of reasons, such
    as to gain access to new training data, to experiment with a new algorithm and
    hyperparameters, or to model predictive performance deteriorating over time. Any
    time you update a model with a new version in production, there is a risk of the
    model becoming unavailable during the update and the model's quality being worse
    than the previous version. Even after careful evaluation in the development and
    QA environments, new models need additional testing, validation, and monitoring
    to make sure they work properly in production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: When deploying new versions of models into production, you should carefully
    consider reducing deployment risks and minimizing downtime for the model consumers.
    It is also important to proactively plan for an unsuccessful model update and
    roll back to a previous working model. Replacing an existing model with a newer
    model should, ideally, not cause any service interruptions to the model's consumers.
    Model consumers may be applications that are internal to your organization or
    external, customer-facing applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will address the challenge of updating production models with minimal
    disruption for model consumers using **Amazon SageMaker Endpoint Production Variants**. You
    will learn how to use SageMaker Endpoint Production Variants to implement Standard
    deployment and advanced model deployment strategies such as **A/B testing**, **Blue/Green**,
    **Canary**, and **Shadow** deployments, which balance cost with model downtime
    and ease of rollbacks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to implement multiple deployment
    strategies for updating production machine learning models. You will learn when
    and how to use live production traffic to test new model versions. You will also
    learn about the best practices for balancing cost, availability, and reducing
    risk while choosing the right deployment strategy for your use case.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of Amazon SageMaker Endpoint Production Variants
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment strategies for updating ML models with Amazon SageMaker Endpoint
    Production Variants
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting an appropriate deployment strategy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an **AWS** account to run the examples included in this chapter.
    If you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which provides a walk-through of the setup process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter09](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter09).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of Amazon SageMaker Endpoint Production Variants
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon SageMaker 端点生产变体的基本概念
- en: 'In this section, you will review the basics of deploying and updating ML models
    using SageMaker Endpoint Production Variants. There are two ways you can deploy
    a machine learning model using SageMaker: by using a real-time endpoint for low
    latency live predictions or a batch transform for making asynchronous predictions
    on large numbers of inference requests. Production Variants can be applied to
    real-time endpoints.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将回顾使用 SageMaker 端点生产变体部署和更新机器学习模型的基本知识。您可以使用两种方式通过 SageMaker 部署机器学习模型：使用实时端点进行低延迟的实时预测或批量转换，用于对大量推理请求进行异步预测。生产变体可以应用于实时端点。
- en: 'Deploying a real-time endpoint involves two steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 部署实时端点涉及两个步骤：
- en: '**Creating an Endpoint Configuration**'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建端点配置**'
- en: An endpoint configuration identifies one or more Production Variants. Each production
    variant indicates a model and infrastructure to deploy the model on.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端点配置标识一个或多个生产变体。每个生产变体指示一个模型和基础设施，用于在之上部署模型。
- en: '**Creating an Endpoint Pointing to the Endpoint Configuration**'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建指向端点配置的端点**'
- en: Endpoint creation results in an HTTPS endpoint that the model consumers can
    use to invoke the model.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端点创建结果是一个 HTTPS 端点，模型消费者可以使用它来调用模型。
- en: 'The following diagram shows two different endpoint configurations with Production
    Variants. `model_1` that''s deployed on an `ml.m4.xlarge` instance; all inference
    traffic is served by this single model. `model_1` and `model_2` on `ml.m4.xlarge`
    and `ml.m4.2xlarge`, respectively. Both models serve the inference requests equally
    because they have the same `initial_weight` configuration:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了具有生产变体的两种不同的端点配置。`model_1` 部署在 `ml.m4.xlarge` 实例上；所有推理流量都由这个单一模型提供服务。`model_1`
    和 `model_2` 分别部署在 `ml.m4.xlarge` 和 `ml.m4.2xlarge` 上。由于它们具有相同的 `initial_weight`
    配置，这两个模型对推理请求的处理是平等的：
- en: '![Figure 9.1 – Endpoint configurations with Production Variants'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 具有生产变体的端点配置'
- en: '](img/B17249_09_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_09_01.jpg)'
- en: Figure 9.1 – Endpoint configurations with Production Variants
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 具有生产变体的端点配置
- en: 'When an endpoint has been configured with multiple Production Variants, how
    do you know which model is serving the inference requests? There are two ways
    to determine this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个端点配置了多个生产变体时，您如何知道哪个模型正在处理推理请求？有两种方法可以确定这一点：
- en: First, the `initial_weight` parameter of the production variant determines the
    relative percentage of the requests served by the model specified by that variant.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，生产变体的 `initial_weight` 参数决定了由该变体指定的模型处理的请求的相对百分比。
- en: Second, the inference request may also include the model variant to invoke.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，推理请求可能还包括要调用的模型变体。
- en: The following diagram shows these two ways of invoking the endpoint
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了调用端点的这两种方式
- en: '![Figure 9.2 – Two ways to invoke SageMaker Endpoint'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 调用 SageMaker 端点的两种方式'
- en: '](img/B17249_09_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_09_02.jpg)'
- en: Figure 9.2 – Two ways to invoke SageMaker Endpoint
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 调用 SageMaker 端点的两种方式
- en: As the SageMaker Endpoints are serving inference traffic, they are monitored
    using `EndpointName` and `VariantName` dimensions to monitor metrics for each
    distinct production variant of the same endpoint. The `Invocations` metric captures
    the number of requests that are sent to a model, as indicated by the production
    variant. You can use this metric to monitor the number of requests that are served
    by different models and deployed with a single endpoint.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SageMaker 端点正在处理推理流量，因此它们使用 `EndpointName` 和 `VariantName` 维度进行监控，以监控每个不同端点的特定生产变体的指标。`Invocations`
    指标捕获发送到模型的请求数量，如生产变体所示。您可以使用此指标来监控由单个端点部署的不同模型服务的请求数量。
- en: 'The following diagram shows a comparison of the `Invocations` metrics that
    have been captured for an endpoint that''s been configured with two Production
    Variants. The first chart shows the number of invocations per production variant
    when the initial weights are set to `1` and `1`. In this case, each variant serves
    a similar number of requests. The second chart shows the same metric with the
    initial weights of `2` and `1`. As you can see, the number of requests that are
    served by variant 1 is double the number of requests that are served by variant
    2:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了为配置了两个生产变体的端点捕获的 `Invocations` 指标比较。第一个图表显示了当初始权重设置为 `1` 和 `1` 时每个生产变体的调用次数。在这种情况下，每个变体服务的请求数量相似。第二个图表显示了具有初始权重
    `2` 和 `1` 的相同指标。如您所见，变体 1 服务的请求数量是变体 2 服务请求数量的两倍：
- en: '![Figure 9.3 – Invocations of SageMaker Endpoint Production Variants'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – SageMaker 端点生产变体的调用次数](img/B17249_09_03.jpg)'
- en: '](img/B17249_09_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_09_03.jpg](img/B17249_09_03.jpg)'
- en: Figure 9.3 – Invocations of SageMaker Endpoint Production Variants
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – SageMaker 端点生产变体的调用次数
- en: While the `Invocations` metric is intuitively easy to understand, there are
    other CloudWatch metrics such as `Latency` and `Overhead` that you can use to
    monitor, compare, and contrast multiple endpoints and multiple Production Variants
    of a single endpoint.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `Invocations` 指标直观易懂，但还有其他 CloudWatch 指标，如 `Latency` 和 `Overhead`，可用于监控、比较和对比多个端点和单个端点的多个生产变体。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For a full list of CloudWatch Metrics for Amazon SageMaker, please see [https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Amazon SageMaker 的完整 CloudWatch 指标列表，请参阅 [https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation)。
- en: Similar to Production Variants, SageMaker **multi-model endpoints** (**MME**)
    also allow us to host multiple models on a single endpoint. If that is the case,
    how are Production Variants different from multi-model endpoints?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与生产变体类似，SageMaker **多模型端点**（**MME**）也允许我们在单个端点上托管多个模型。如果是这种情况，生产变体与多模型端点有何不同？
- en: With an MME, all models are hosted on the same compute infrastructure. However,
    not all the models are loaded into the container memory when the endpoint is created.
    Instead, the model is loaded into memory when an inference request is made. Each
    inference request must specify the model to invoke. The invoked model is then
    loaded into memory from the **S3 bucket** if it is not already in memory. Depending
    on the invocation pattern, a model that hasn't been invoked recently may not be
    in memory. This could result in increased latency when serving the request. When
    you have a large number of similar ML models that are infrequently accessed and
    can tolerate slightly increased latency, then a single MME can serve inference
    traffic at significantly low costs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MME 中，所有模型都托管在相同的计算基础设施上。然而，当创建端点时，并非所有模型都加载到容器内存中。相反，当进行推理请求时，模型被加载到内存中。每个推理请求必须指定要调用的模型。如果调用的模型尚未在内存中，则从
    **S3 存储桶**将其加载到内存中。根据调用模式，最近未调用的模型可能不在内存中。这可能导致在处理请求时延迟增加。当您有大量不常访问且可以容忍略微增加延迟的类似
    ML 模型时，单个 MME 可以以显著低廉的成本处理推理流量。
- en: On the other hand, with Production Variants, each model is hosted on a completely
    different compute infrastructure, and all the models are readily available without
    having to be loaded into container memory on demand. Each inference request may
    or may not specify the variant to invoke. If the variant to invoke is not specified,
    the number of inference requests that are served by each variant depends on the
    `initial_weight` parameter of the production variant. In the context of model
    deployment, use Production Variants to test different versions of ML models that
    have been trained using different datasets, algorithms, and ML frameworks or to
    test how a model performs on different instance types.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用生产变体时，每个模型都托管在完全不同的计算基础设施上，所有模型都随时可用，无需在需要时加载到容器内存中。每个推理请求可能或可能不指定要调用的变体。如果未指定要调用的变体，则每个变体服务的推理请求数量取决于生产变体的
    `initial_weight` 参数。在模型部署的上下文中，使用生产变体来测试使用不同数据集、算法和 ML 框架训练的不同版本的 ML 模型，或者测试模型在不同实例类型上的性能。
- en: In the next section, you will learn how to use Production Variants in various
    deployment strategies. As we discuss these various deployment strategies, we will
    focus on what it takes to update an existing production model deployed as a real-time
    SageMaker endpoint using Production Variants.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何在不同部署策略中使用 Production Variants。当我们讨论这些不同的部署策略时，我们将关注使用 Production
    Variants 更新作为实时 SageMaker 端点部署的现有生产模型所需的内容。
- en: Deployment strategies for updating ML models with SageMaker Endpoint Production
    Variants
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Endpoint Production Variants 更新 ML 模型的部署策略
- en: In this section, we will dive into multiple deployment strategies you can adopt
    to update production models using SageMaker Endpoint Production Variants. While
    some deployment strategies are easy to implement and are cost-effective, others
    add complexity while lowering deployment risks. We will dive into five different
    strategies, including Standard, A/B, Blue/Green, Canary, and Shadow deployments,
    and discuss the various steps involved in each approach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨多种您可以采用的部署策略，以使用 SageMaker Endpoint Production Variants 更新生产模型。虽然一些部署策略易于实施且成本效益高，但其他策略在降低部署风险的同时增加了复杂性。我们将深入探讨五种不同的策略，包括标准、A/B、蓝绿、金丝雀和影子部署，并讨论每种方法中涉及的各个步骤。
- en: Standard deployment
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准部署
- en: 'This strategy is the most straightforward approach to deploying and updating
    models in production. In a Standard model deployment, there is always a single
    active SageMaker endpoint, and the endpoint is configured with a single production
    variant, which means only a single model is deployed behind the endpoint. All
    inference traffic is processed by a single model. The endpoint configuration is
    similar to `variant1`, hosts `model_name_1` on a single `ml.m5.xlarge` instance
    and serves all inference traffic, as indicated by `initial_weight=1`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略是部署和更新生产中模型的最直接方法。在标准模型部署中，始终只有一个活动的 SageMaker 端点，并且端点配置为单个生产变体，这意味着只有单个模型部署在端点后面。所有推理流量都由单个模型处理。端点配置类似于
    `variant1`，在单个 `ml.m5.xlarge` 实例上托管 `model_name_1` 并处理所有推理流量，如 `initial_weight=1`
    所示：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code block shows how to create an endpoint from the production
    variant. `endpoint_from_production_variants` automatically creates an `endpoint_configuration`
    with the same name as `endpoint_name`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块展示了如何从生产变体创建端点。`endpoint_from_production_variants` 自动创建与 `endpoint_name`
    同名的 `endpoint_configuration`：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To update the endpoint with a newer version of the model, create a new endpoint
    configuration specifying the new model and infrastructure to deploy the model
    on. Then, update the endpoint with a new endpoint configuration. The following
    code block shows the code for updating the endpoint with the new model version:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用模型的新版本更新端点，创建一个新的端点配置，指定新模型和部署模型的基础设施。然后，使用新的端点配置更新端点。以下代码块展示了更新端点以使用新模型版本的代码：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'SageMaker automatically creates and manages the infrastructure necessary for
    the new production variant and routes the traffic to the new model without any
    downtime. All inference traffic is now served by the new model. The following
    diagram shows the steps involved in updating a deployed model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 自动创建和管理新生产变体所需的基础设施，并在没有任何停机时间的情况下将流量路由到新模型。现在所有推理流量都由新模型处理。以下图表显示了更新已部署模型的步骤：
- en: '![Figure 9.4 – Standard deployment with SageMaker Endpoint Production Variants'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 使用 SageMaker Endpoint Production Variants 的标准部署'
- en: '](img/B17249_09_04.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_09_04.jpg](img/B17249_09_04.jpg)'
- en: Figure 9.4 – Standard deployment with SageMaker Endpoint Production Variants
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 使用 SageMaker Endpoint Production Variants 的标准部署
- en: To roll back, simply update the endpoint with the original endpoint configuration,
    as represented by *Step 1*. As you can see, inference traffic is served by either
    the old version of the model or the new version at all times.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要回滚，只需使用原始端点配置更新端点，如 *步骤 1* 所示。如您所见，推理流量始终由模型的旧版本或新版本提供。
- en: One benefit of this approach is that it is a simple, straightforward way to
    update an endpoint with a new model. When the endpoint is updated with the new
    endpoint configuration, SageMaker switches the inference requests to the new model
    while keeping the endpoint `InService`. This means that the model consumer does
    not experience any disruption to the service. This is also a cost-effective strategy
    for updating a real-time endpoint since you only pay for the infrastructure hosting
    a single model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法的一个好处是，它是一种简单直接地使用新模型更新端点的方法。当端点更新为新端点配置时，SageMaker 将推理请求切换到新模型，同时保持端点处于
    `InService` 状态。这意味着模型消费者不会体验到任何服务中断。这也是更新实时端点的经济高效策略，因为您只为托管单个模型的设施付费。
- en: On the other hand, model evaluation and testing happen in non-production environments
    such as the QA or staging environments with test data. Since the new model is
    not tested in a production environment, it will face the production data volume
    and live traffic on the new infrastructure for the first time in production. This
    could lead to unforeseen complications, either with the model hosting the infrastructure
    or the model's quality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，模型评估和测试发生在非生产环境，如 QA 或预生产环境，并使用测试数据。由于新模型在生产环境中未经测试，它将在生产环境中首次面对新基础设施上的生产数据量和实时流量。这可能导致不可预见的问题，无论是模型托管的基础设施还是模型的质量。
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While evaluating the model in staging environments, it is recommended that you
    perform load testing to validate that the model can handle the traffic with acceptable
    latency before moving to production.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型处于预生产环境时，建议您进行负载测试，以验证模型在迁移到生产环境之前能否以可接受的延迟处理流量。
- en: Refer to [https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/](https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/)
    to learn how to load test an endpoint using autoscaling and serverless-artillery.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/](https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/)，了解如何使用自动扩展和
    serverless-artillery 加载测试端点。
- en: Use Standard deployment if the model consumer is risk- and failure-tolerant,
    such as an internal application that can re-execute the predictions in case of
    failures. For example, an internal model that predicts employee turnover is a
    good candidate for Standard deployment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型消费者能够容忍风险和失败，例如可以重新执行预测的内部应用程序，请使用标准部署。例如，预测员工流动性的内部模型是标准部署的良好候选者。
- en: Since only one model is serving inference requests at a time, this strategy
    is not suitable for comparing different models. If you are experimenting with
    different features, multiple algorithms, or hyperparameters, you want to be able
    to compare the models in production. The next deployment strategy helps with this
    need.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一次只能有一个模型处理推理请求，因此此策略不适用于比较不同的模型。如果您正在尝试不同的特征、多个算法或超参数，您希望能够在生产环境中比较这些模型。下一个部署策略有助于满足这一需求。
- en: A/B deployment
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B 部署
- en: In the Standard deployment, you have a single endpoint in the production environment
    with no scope for testing or evaluating the model in production. On the other
    hand, an A/B deployment strategy is focused on experimentation and exploration,
    such as comparing the performance of different versions of the same feature.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准部署中，您在生产环境中有一个单一的端点，没有测试或评估模型在生产环境中的范围。另一方面，A/B 部署策略专注于实验和探索，例如比较同一功能的不同版本的性能。
- en: 'In this scenario, the endpoint configuration uses two Production Variants:
    one for model `A` and one for model `B`. For a fair comparison of the two models,
    `initial_weight` of the two production variants should be the same so that both
    models handle the same amount of inference traffic. Additionally, make sure the
    instance type and instance count are also the same. This initial setting is necessary
    so that neither version of the model is impacted by a difference in traffic patterns
    or a difference in the underlying compute capacity.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在此场景中，端点配置使用两个生产变体：一个用于模型 `A`，另一个用于模型 `B`。为了对两个模型进行公平的比较，两个生产变体的 `initial_weight`
    应该相同，以便两个模型处理相同数量的推理流量。此外，请确保实例类型和实例数量也相同。这种初始设置是必要的，以确保模型的任何版本都不会受到流量模式或底层计算能力差异的影响。
- en: The following code blocks shows how to create and update an endpoint for A/B
    deployments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create `production variant A`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, create an endpoint with one production variant, which initially serves
    production traffic:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When you are ready to test the next version of the model, create another production
    variant and update the endpoint so that it includes two Production Variants:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To invoke the endpoint, use the `invoke_endpoint()` API, as shown in the following
    code. The result of using the `invoke_endpoint()` API consists of the variant
    name that serves each specific request:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output from the endpoint should look similar to the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can collect and examine results from `VariantB`. You can explore the CloudWatch
    metrics for `VariantB` even further as well, as explained in the *Basic concepts
    of Amazon SageMaker Endpoint Production Variants* section. Once you are happy
    with the performance of `VariantB`, gradually shift the balance toward the new
    model (`40/60`, `20/80`) until your new model is processing all the live traffic.
    The following code block shows how to route 60% of live traffic to `VariantB`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Alternatively, you can choose to update the endpoint to route all live traffic
    to `VariantB` in a single step, as shown in the following code block:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following diagram shows the steps involved in updating a deployed model.
    To roll back, simply update the endpoint with the original endpoint configuration,
    as represented by Step 1:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – A/B deployment with SageMaker Endpoint Production Variants'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – A/B deployment with SageMaker Endpoint Production Variants
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of this strategy are that it is well-understood and that SageMaker
    makes it simple to implement this strategy by managing traffic routing. Since
    the new model is evaluated in production with an increased percentage of live
    traffic on the new infrastructure, the risk of the model becoming unavailable
    to the model consumer during the update, or the model quality being worse than
    it was in the previous version, is reduced. This addresses the typical deployment
    issue of *the model worked perfectly in the dev/QA environment, so I'm not sure
    why it is failing in production*. However, since two Production Variants are active
    for a certain period, the cost increases as you are paying for two sets of infrastructure
    resources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: A relatively recent type of A/B testing that's gaining popularity is **Multi-Arm
    Bandits** (**MAB**). MAB is a machine learning-based approach that learns from
    the data that's collected during testing. Using a combination of exploration and
    exploitation, MAB dynamically shifts traffic to better-performing model variants
    much sooner than a traditional A/B test.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/](https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/)
    to learn how to use Amazon SageMaker RL to implement MAB to recommend personalized
    content to users.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/](https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/)了解如何使用Amazon
    SageMaker RL实现多臂老虎机（MAB）来向用户推荐个性化内容。
- en: While the A/B strategy is helpful with experimentation and exploration, what
    about releasing major changes to your models? Is there a way to reduce the risk
    further? Blue/Green deployments can help with this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的A/B测试策略在实验和探索方面很有帮助，但当你需要发布模型的主要变更时怎么办？有没有办法进一步降低风险？蓝/绿部署可以帮助解决这个问题。
- en: Blue/Green deployment
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: The Blue/Green deployment strategy involves two identical production environments,
    one containing the current model and another containing the next version of the
    model that you want to update to. While one environment, say Blue, is serving
    live traffic, the next version of the model is tested in the Green environment.
    While model testing is happening in production, only test or synthetic data is
    used. The new model version should be tested against functional, business, and
    traffic load requirements.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿部署策略涉及两个相同的 生产环境，一个包含当前模型，另一个包含你想要更新的下一个版本的模型。当其中一个环境，比如蓝色环境，正在服务实时流量时，下一个版本的模型在绿色环境中进行测试。在生产环境中进行模型测试时，只使用测试或合成数据。新的模型版本应该针对功能、业务和流量负载要求进行测试。
- en: Once you are satisfied with the test results over a certain period, update the
    live endpoint with the new (Green) endpoint configuration. Validate the tests
    again with the Green endpoint configuration using live inference traffic. If you
    find any issues during this testing period, route the traffic back to a Blue endpoint
    configuration. After a while, if there are no issues with the new model, go ahead
    and delete the Blue endpoint configuration.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对一定期限内的测试结果感到满意，就用新的（绿色）端点配置更新实时端点。使用实时推理流量再次验证测试，如果在这个测试期间发现任何问题，就将流量路由回蓝色端点配置。过了一段时间，如果新模型没有问题，就可以继续删除蓝色端点配置。
- en: 'The following diagram shows the steps involved in updating a deployed model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了更新已部署模型的步骤：
- en: Figure 9.6 – Blue/Green deployment with SageMaker Endpoint Production Variants
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 使用SageMaker端点生产变体的蓝/绿部署
- en: The advantage of this approach is that before serving live traffic, the new
    model is evaluated in the production environment. Both the model itself and the
    infrastructure hosting the model are evaluated and thereby risk is reduced. However,
    since two identical production environments are active for a while, the cost of
    this option could double compared to the strategies we've discussed so far. This
    option also loses the advantage of SageMaker managing the routing logic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是在向实时流量提供服务之前，新模型在生产环境中进行了评估。模型本身以及托管模型的底层基础设施都得到了评估，从而降低了风险。然而，由于有两个相同的
    生产环境活跃了一段时间，与之前讨论的策略相比，这种选项的成本可能会翻倍。此外，这种选项也失去了SageMaker管理路由逻辑的优势。
- en: In this strategy, while the model is evaluated in production, testing still
    involves synthetic traffic. Synthetic data can simulate the production volumes,
    but it is not trivial to reflect the live data patterns. What if you want to test
    the new model with live traffic? Canary deployment is the strategy that allows
    you to do this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个策略中，虽然模型在生产环境中进行评估，但测试仍然涉及合成流量。合成数据可以模拟生产量，但反映实时数据模式并不简单。如果你想要使用实时流量测试新模型，金丝雀部署策略允许你这样做。
- en: Canary deployment
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: In a Canary deployment, the setup is very similar to Blue/Green deployments,
    with two different production environments hosting the old and new models. However,
    instead of using synthetic test data with a new model, you use a portion of the
    live traffic. Initially, a small portion of the inference traffic from the model
    consumer will be served by the new model. The rest of the inference requests continue
    to use the previous version. During the testing phase, the designated set of users
    using the new model should remain the same, and this requires *stickiness*. When
    you are satisfied with the new model, gradually increase the percentage of requests
    that are sent to the new model, until all live traffic is served by the new model.
    Finally, the old model can be deleted.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀部署中，设置与蓝/绿部署非常相似，有两个不同的生产环境分别托管旧模型和新模型。然而，您不是使用新模型的新合成测试数据，而是使用一部分实时流量。最初，模型消费者的一小部分推理流量将由新模型提供服务。其余的推理请求继续使用之前的版本。在测试阶段，使用新模型指定的用户组应保持不变，这需要*粘性*。当您对新模型满意时，逐渐增加发送到新模型的请求数量，直到所有实时流量都由新模型提供服务。最后，可以删除旧模型。
- en: Unlike the other strategies we've discussed so far, switching between the two
    different environments is not implemented by SageMaker. To make the switch between
    the environments completely transparent to the model consumer, a switching component
    must be used between the consumer and the endpoints. Examples of switching components
    include load balancers, DNS routers, and more.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的其他策略不同，SageMaker没有实现两个不同环境之间的切换。为了使环境切换对模型消费者完全透明，必须在消费者和端点之间使用一个切换组件。切换组件的例子包括负载均衡器、DNS路由器等。
- en: 'The following diagram shows the steps involved in updating a deployed model
    using this strategy:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了使用此策略更新已部署模型的步骤：
- en: '![Figure 9.7 – Canary deployment with SageMaker Endpoint Production Variants'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7 – 使用SageMaker端点生产变体的金丝雀部署'
- en: '](img/B17249_09_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_09_07.jpg]'
- en: Figure 9.7 – Canary deployment with SageMaker Endpoint Production Variants
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 使用SageMaker端点生产变体的金丝雀部署
- en: As with the Blue/Green deployments, the advantage of this approach is that risk
    to the model consumers is reduced as the new model is tested in the production
    environment. Additionally, the model is gradually exposed to live traffic instead
    of a sudden switch. But this strategy does require you to manage the logic of
    gradually increasing traffic for the new model. Additionally, since two identical
    production environments are active for a certain period, the cost of this option
    is also significantly higher.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与蓝/绿部署一样，这种方法的优点是，由于新模型在生产环境中进行测试，因此降低了模型消费者的风险。此外，模型逐渐暴露于实时流量，而不是突然切换。但此策略确实需要您管理逐渐增加新模型流量的逻辑。此外，由于两个相同的生产环境在一段时间内都是活跃的，因此此选项的成本也显著更高。
- en: Shadow deployment
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影子部署
- en: In a Shadow deployment, the setup is, once again, very similar to a Canary deployment
    in that two different production environments are hosting the old and new models,
    and inference traffic is sent to both. However, only responses from the old model
    are sent back to the model consumer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在影子部署中，设置再次与金丝雀部署非常相似，即两个不同的生产环境分别托管旧模型和新模型，推理流量被发送到两者。然而，只有来自旧模型的响应被发送回模型消费者。
- en: 'The traffic that''s sent to the old model is collected and also sent to the
    new model, either immediately or after a delay. While the production traffic is
    sent to the new model as well as the old, the output from the new model is only
    captured and stored for analysis, not sent to model consumers. The new model should
    be tested against functional, business, and traffic load with the live traffic.
    The following diagram shows the steps involved in updating a model that''s been
    deployed using this strategy:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 发送到旧模型的流量被收集并也发送到新模型，可以是立即或延迟后。在生产流量同时发送到新旧模型的情况下，新模型的输出仅被捕获和存储以供分析，不会发送给模型消费者。新模型应与实际流量进行功能、业务和流量负载测试。以下图表显示了使用此策略更新已部署模型的步骤：
- en: '![Figure 9.8 – Shadow deployment with SageMaker Endpoint Production Variants'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.8 – 使用SageMaker端点生产变体的影子部署'
- en: '](img/B17249_09_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_09_08.jpg]'
- en: Figure 9.8 – Shadow deployment with SageMaker Endpoint Production Variants
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 使用SageMaker端点生产变体的影子部署
- en: As with the Canary deployments, the advantage of this approach is that all risks
    to the model consumers are reduced as the new model is tested in the production
    environment.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that demonstrates the end-to-end A/B deployment strategy
    is provided in the following GitHub repository. You can use this as a starting
    point for implementing other deployment strategies: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH09/a_b_deployment_with_production_variants.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH09/a_b_deployment_with_production_variants.ipynb).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about the multiple deployment strategies you can use to update
    production models, in the next section, we will discuss how to select a strategy
    to meet your specific requirements.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an appropriate deployment strategy
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen so far, the initial deployment of a machine model is only one
    step of making it available to consumers. New versions of models are built regularly.
    Before making the new models available to the consumers, the model quality and
    infrastructure that's needed to host the model should be evaluated carefully.
    There are multiple factors to consider when selecting the deployment strategy
    to initially deploy and continue to update models. For example, not all models
    can be tested in production due to budget and resource constraints. Similarly,
    some model consumers can tolerate the model being unavailable for certain periods.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: This section will summarize the deployment strategies you can use to deploy
    and update real-time SageMaker Endpoints. You will get an idea of the pros and
    cons for each strategy, in addition to when should it be used.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a standard deployment
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model consumers are not business or revenue critical and are risk-tolerant.
    For example, a company''s internal employee attrition prediction models are not
    time-critical and can be re-executed on errors:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Pros and cons of a standard deployment'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_09_new.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Pros and cons of a standard deployment
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an A/B deployment
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should use A/B deployments to explore the effect different sets of hyperparameters
    have on model quality, new or different slices of the training dataset, and different
    feature engineering techniques:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Pros and cons of A/B deployment'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_10.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Pros and cons of A/B deployment
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a Blue/Green deployment
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should use this deployment with mission-critical model consumers, such
    as e-commerce applications, that are sensitive to model downtime:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Pros and cons of Blue/Green deployment'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_11.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Pros and cons of Blue/Green deployment
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a Canary deployment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should use this deployment with mission-critical model consumers, such
    as financial services models, that are not risk-tolerant:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Pros and cons of a Canary deployment'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Pros and cons of a Canary deployment
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a Shadow deployment
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should use this deployment with mission-critical model consumers, such
    as financial services models, that are not risk-tolerant:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Pros and cons of Shadow deployment'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_09_13.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Pros and cons of Shadow deployment
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: You should choose an appropriate model strategy using the trade-offs discussed
    in the preceding subsections for ease of implementation, acceptable model downtime,
    the risk tolerance of the consumers, and the costs that must be taken into account
    for you to meet your needs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed the reasons we should update production ML models.
    You learned how to use Production Variants to host multiple models using a single
    SageMaker Endpoint. You then learned about multiple deployment strategies that
    balance the cost and risk of model updates with ease of implementation and rollbacks.
    You also learned about the various steps involved and the configurations to use
    for Standard, A/B, Blue/Green, Canary, and Shadow deployments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concluded with a comparison of the pros and cons and the applicability
    of each deployment strategy to specific use cases. Using this discussion as guidance,
    you can now choose an appropriate strategy to update your production models so
    that they meet your model availability and model quality requirements.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our discussion of deploying models and
    learn about optimizing model hosting and infrastructure costs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17249_09_06.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
