["```py\ndef load_model_tokenize_create_pipeline():\n    \"\"\"\n    Load the model\n    Create a \n    Args\n    Returns:\n        tokenizer\n        pipeline\n    \"\"\"\n# adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    time_2 = time()\n    print(f\"Load model and init tokenizer: {round(time_2-time_1, 3)}\")\n    pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\n    time_3 = time()\n    print(f\"Prepare pipeline: {round(time_3-time_2, 3)}\")\n    return tokenizer, pipeline \n```", "```py\ndef test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n# adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)}\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\") \n```", "```py\ntokenizer, pipeline = load_model_tokenize_create_pipeline()\nprompt_to_test = 'Prompt: Adrian has three apples. His sister Anne has ten apples more than him. How many apples has Anne?'\ntest_model(tokenizer, pipeline, prompt_to_test) \n```", "```py\nprompt_to_test = 'Prompt: A circle has the radius 5\\. What is the area of the circle?'\ntest_model(tokenizer, pipeline, prompt_to_test) \n```", "```py\nllama.cpp, import the necessary functions from the package, execute the quantization process, and subsequently load the quantized model. Itâ€™s important to note that, in this instance, we will not utilize the latest, more advanced quantization option available in llama.cpp. This example serves as an introduction to model quantization on Kaggle and its practical implementation:\n```", "```py\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n!git clone https://github.com/ggerganov/llama.cpp.git\n!python llama.cpp/convert.py /kaggle/input/llama-2/pytorch/7b-chat-hf/1 \\\n  --outfile llama-7b.gguf \\\n  --outtype q8_0\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"/kaggle/working/llama-7b.gguf\") \n```", "```py\noutput = llm(\"Q: Name three capital cities in Europe? A: \", max_tokens=38, stop=[\"Q:\", \"\\n\"], echo=True) \n```", "```py\noutput = llm(\"If a circle has the radius 3, what is its area?\")\nprint(output['choices'][0]['text']) \n```", "```py\nmodel_1_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n) \n```", "```py\ntime_1 = time()\nquery_pipeline_1 = transformers.pipeline(\n        \"text-generation\",\n        model=model_1,\n        tokenizer=tokenizer_1,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline #1: {round(time_2-time_1, 3)} sec.\")\nllm_1 = HuggingFacePipeline(pipeline=query_pipeline_1) \n```", "```py\nllm_1(prompt=\"What is the most popular food in France for tourists? Just return the name of the food.\") \n```", "```py\ndef sequential_chain(country, llm):\n    \"\"\"\n    Args:\n        country: country selected\n    Returns:\n        None\n    \"\"\"\n    time_1 = time()\n    template = \"What is the most popular food in {country} for tourists? Just return the name of the food.\"\n#  first task in chain\n    first_prompt = PromptTemplate(\n    input_variables=[\"country\"],\n    template=template)\n    chain_one = LLMChain(llm = llm, prompt = first_prompt)\n    # second step in chain\n    second_prompt = PromptTemplate(\n    input_variables=[\"food\"],\n    template=\"What are the top three ingredients in {food}. Just return the answer as three bullet points.\",)\n    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n    # combine the two steps and run the chain sequence\n    overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n    overall_chain.run(country)\n    time_2 = time()\n    print(f\"Run sequential chain: {round(time_2-time_1, 3)} sec.\") \n```", "```py\nfinal_answer = sequential_chain(\"France\", llm_1) \n```", "```py\nfinal_answer = sequential_chain(\"Italy\", llm_1) \n```", "```py\nprompt = 'Write the code for a function to compute the area of circle.'\nsequences = pipeline(\n    prompt,\n    do_sample=True,\n    top_k=10,\n    temperature=0.1,\n    top_p=0.95,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n) \n```", "```py\nllm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\") \n```", "```py\n# load file(s)\nloader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()\n# data chunking\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)\n# embeddings model: Sentence Transformer\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n# add documents to the ChromaDB database\nvectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\") \n```", "```py\nretriever = vectordb.as_retriever()\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n) \n```", "```py\ndef test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result) \n```", "```py\nquery = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query) \n```", "```py\ndocs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\") \n```"]