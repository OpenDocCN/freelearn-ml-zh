- en: Hand Gesture Recognition Using a Kinect Depth Sensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of this chapter is to develop an app that detects and tracks simple
    hand gestures in real time, using the output of a depth sensor, such as that of
    a **Microsoft Kinect 3D sensor** or an **ASUS Xtion sensor**.The app will analyze
    each captured frame to perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hand region segmentation**: The user''s hand region will be extracted in
    each frame by analyzing the **depth map** output of the Kinect sensor, which is
    done by **thresholding**, applying some **morphological operations**, and finding
    **connected** **components**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hand shape analysis**: The shape of the segmented hand region will be analyzed
    by determining **contours**, **convex** **hull**, and **convexity defects**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hand gesture recognition**: The number of extended fingers will be determined
    based on the hand contour''s **convexity defects**, and the gesture will be classified
    accordingly (with no extended fingers corresponding to a fist, and five extended
    fingers corresponding to an open hand).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gesture recognition is an ever-popular topic in computer science. This is because
    it not only enables humans to communicate with machines (**Human-Machine Interaction** (**HMI**))
    but also constitutes the first step for machines to begin understanding human
    body language. With affordable sensors such as Microsoft Kinect or Asus Xtion
    and open source software such as **OpenKinect** and **OpenNI**, it has never been
    easier to get started in the field yourself. *So, what shall we do with all this
    technology?*
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking hand gestures in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding hand region segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing hand shape analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing hand gesture recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The beauty of the algorithm that we are going to implement in this chapter is
    that it works well for many hand gestures, yet it is simple enough to run in real
    time on a generic laptop. Also, if we want, we can easily extend it to incorporate
    more complicated hand-pose estimations.
  prefs: []
  type: TYPE_NORMAL
- en: Once you complete the app, you will understand how to use depth sensors in your
    own apps. You will learn how to compose shapes of interest with OpenCV from the
    depth information, as well as understanding how to analyze shapes with OpenCV,
    using their geometric properties.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have a Microsoft Kinect 3D sensor installed. Alternatively,
    you may install an Asus Xtion sensor or any other depth sensor for which OpenCV
    has built-in support.
  prefs: []
  type: TYPE_NORMAL
- en: First, install OpenKinect and **libfreenect** from [http://www.openkinect.org/wiki/Getting_Started](http://www.openkinect.org/wiki/Getting_Started). You
    can find the code that we present in this chapter at our GitHub repository: [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter2](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter2).
  prefs: []
  type: TYPE_NORMAL
- en: Let's first plan the application we are going to create in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final app will consist of the following modules and scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gestures`: This is a module that consists of an algorithm for recognizing
    hand gestures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gestures.process`: This is a function that implements the entire process flow
    of hand gesture recognition. It accepts a single-channel depth image (acquired
    from the Kinect depth sensor) and returns an annotated **Blue, Green, Red** (**BGR**)
    color image with an estimated number of extended fingers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter2`: This is the main script for the chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter2.main`: This is the main function routine that iterates over frames
    acquired from a depth sensor that uses `.process` gestures to process frames,
    and then illustrates results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The end product looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bd8dec9-a571-476d-bb9c-b3fdef943e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: No matter how many fingers of a hand are extended, the algorithm correctly segments
    the hand region (white), draws the corresponding convex hull (the green line surrounding
    the hand), finds all convexity defects that belong to the spaces between fingers
    (large green points) while ignoring others (small red points), and infers the
    correct number of extended fingers (the number in the bottom-right corner), even
    for a fist.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's set up the application in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can get down to the nitty-gritty of our gesture recognition algorithm,
    we need to make sure that we can access the depth sensor and display a stream
    of depth frames. In this section, we will cover the following things that will
    help us set up the app:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Kinect 3D sensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing OpenNI-compatible sensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the app and main function routine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we will look at how to use the Kinect 3D sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Kinect 3D sensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to access a Kinect sensor is by using an `OpenKinect` module
    called `freenect`. For installation instructions, take a look at the preceding
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The `freenect` module has functions such as `sync_get_depth()` and `sync_get_video()`,
    used to obtain images synchronously from the depth sensor and camera sensor respectively. For
    this chapter, we will need only the Kinect depth map, which is a single-channel
    (grayscale) image in which each pixel value is the estimated distance from the
    camera to a particular surface in the visual scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will design a function that will read a frame from the sensor and
    convert it to the desired format, and return the frame together with a success
    status, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The function consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grab a `frame`; terminate the function if a frame was not acquired, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `sync_get_depth` method returns both the depth map and a timestamp. By default,
    the map is in an 11-bit format. The last 10 bits of the sensor describes the depth,
    while the first bit states that the distance estimation was not successful when
    it's equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a good idea to standardize the data into an 8-bit precision format, as
    an 11-bit format is inappropriate to be visualized with `cv2.imshow` right away,
    as well as in the future. We might want to use some different sensor that returns
    in a different format, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we have first clipped the values to 1,023 (or `2**10-1`) to
    fit in 10 bits. Such clipping results in the assignment of the undetected distance
    to the farthest possible point. Next, we shift 2 bits to the right to fit the
    distance in 8 bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we convert the image into an 8-bit unsigned integer array and `return` the
    result, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `depth` image can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's see how to use OpenNI-compatible sensors in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing OpenNI-compatible sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use an OpenNI-compatible sensor, you must first make sure that **OpenNI2**
    is installed and that your version of OpenCV was built with the support of OpenNI.
    The build information can be printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If your version was built with OpenNI support, you will find it under the `Video
    I/O` section. Otherwise, you will have to rebuild OpenCV with OpenNI support,
    which is done by passing the `-D WITH_OPENNI2=ON` flag to `cmake`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the installation process is complete, you can access the sensor similarly
    to other video input devices, using `cv2.VideoCapture`. In this app, in order
    to use an OpenNI-compatible sensor instead of a Kinect 3D sensor, you have to
    cover the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a video capture that connects to your OpenNI-compatible sensor, like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you want to connect to Asus Xtion, the `device` variable should be assigned
    to the `cv2.CV_CAP_OPENNI_ASUS` value instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the input frame size to the standard **Video Graphics Array** (**VGA**)
    resolution, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous section, we designed the `read_frame` function, which accesses
    the Kinect sensor using `freenect`. In order to read depth images from the video
    capture, you have to change that function to the following one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You will note that we have used the `grab` and `retrieve` methods instead of
    the `read` method. The reason is that the `read` method of `cv2.VideoCapture` is
    inappropriate when we need to synchronize a set of cameras or a multi-head camera,
    such as a Kinect.
  prefs: []
  type: TYPE_NORMAL
- en: For such cases, you grab frames from multiple sensors at a certain moment in
    time with the `grab` method and then retrieve the data of the sensors of interest
    with the `retrieve` method. For example, in your own apps, you might also need
    to retrieve a BGR frame (standard camera frame), which can be done by passing `cv2.CAP_OPENNI_BGR_IMAGE` to
    the `retrieve` method.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that you can read data from your sensor, let's see how to run the application
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Running the app and main function routine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `chapter2.py` script is responsible for running the app, and it first imports
    the following modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `recognize` function is responsible for recognizing a hand gesture, and
    we will compose it later in this chapter. We have also placed the `read_frame`
    method that we composed in the previous section in a separate script, for convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to simplify the segmentation task, we will instruct the user to place
    their hand in the center of the screen. To provide a visual aid for this, we create
    the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The function draws a rectangle around the image center and highlights the center
    pixel of the image in orange.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the heavy lifting is done by the `main` function, shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The function iterates over grayscale frames from Kinect, and, in each iteration,
    it covers the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognize hand gestures using the `recognize` function, which returns the estimated
    number of extended fingers (`num_fingers`) and an annotated BGR color image, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `draw_helpers` function on the annotated BGR image in order to provide
    a visual aid for hand placement, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `main` function draws the number of fingers on the annotated `frame`,
    displays results with `cv2.imshow`, and sets termination criteria, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we have the main script, you will note that the only function that
    we are missing is the `recognize` function. In order to track hand gestures, we
    need to compose this function, which we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking hand gestures in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hand gestures are analyzed by the `recognize` function; this is where the real
    magic takes place. This function handles the entire process flow, from the raw
    grayscale image to a recognized hand gesture. It returns the number of fingers
    and the illustration frame. It implements the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It extracts the user''s hand region by analyzing the depth map (`img_gray`),
    and returns a hand region mask (`segment`), like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It performs `contour` analysis on the hand region mask (`segment`). Then, it
    returns the largest contour found in the image (`contour`) and any convexity defects
    (`defects`), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the contour found and the convexity defects, it detects the number
    of extended fingers (`num_fingers`) in the image. Then, it creates an illustration
    image (`img_draw`) using (`segment`) image as a template, and annotates it with
    `contour` and `defect` points, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the estimated number of extended fingers (`num_fingers`), as well
    as the annotated output image (img_draw), are returned, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, let's learn how to accomplish hand region segmentation,
    which we used at the beginning of the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hand region segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The automatic detection of an arm—and later, the hand region—could be designed
    to be arbitrarily complicated, maybe by combining information about the shape
    and color of an arm or hand. However, using skin color as a determining feature
    to find hands in visual scenes might fail terribly in poor lighting conditions
    or when the user is wearing gloves. Instead, we choose to recognize the user's
    hand by its shape in the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Allowing hands of all sorts to be present in any region of the image unnecessarily
    complicates the mission of the present chapter, so we make two simplifying assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: We will instruct the user of our app to place their hand in front of the center
    of the screen, orienting their palm roughly parallel to the orientation of the
    Kinect sensor so that it is easier to identify the corresponding depth layer of
    the hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also instruct the user to sit roughly 1 to 2 meters away from the Kinect
    and to slightly extend their arm in front of their body so that the hand will
    end up in a slightly different depth layer than the arm. However, the algorithm
    will still work even if the full arm is visible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, it will be relatively straightforward to segment the image based
    on the depth layer alone. Otherwise, we would have to come up with a hand detection
    algorithm first, which would unnecessarily complicate our mission. If you feel
    adventurous, feel free to do this on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to find the most prominent depth of the image center region in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the most prominent depth of the image center region
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the hand is placed roughly in the center of the screen, we can start finding
    all image pixels that lie on the same depth plane as the hand. This is done by
    following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we simply need to determine the most prominent `depth` value of the
    center region of the image. The simplest approach would be to look only at the
    `depth` value of the center pixel, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a mask in which all pixels at a depth of `center_pixel_depth`
    are white and all others are black, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this approach will not be very robust, because there is the chance
    that it will be compromised by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Your hand will not be placed perfectly parallel to the Kinect sensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your hand will not be perfectly flat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kinect sensor values will be noisy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, different regions of your hand will have slightly different depth
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `segment_arm` method takes a slightly better approach—it looks at a small
    neighborhood in the center of the image and determines the median depth value.
    This is done by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we find the center region (for example, `21 x 21 pixels`) of the image
    frame, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we determine the median depth value, `med_val`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can now compare `med_val` with the depth value of all pixels in the image
    and create a mask in which all pixels whose depth values are within a particular
    range `[med_val-abs_depth_dev, med_val+abs_depth_dev]` are white, and all other
    pixels are black.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for reasons that will become clear in a moment, let''s paint the pixels
    gray instead of white, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cfde92f7-9d3e-4585-93ee-2fad307bcc0c.png)'
  prefs: []
  type: TYPE_IMG
- en: You will note that the segmentation mask is not smooth. In particular, it contains
    holes at points where the depth sensor failed to make a prediction. Let's learn
    how to apply morphological closing to smoothen the segmentation mask, in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Applying morphological closing for smoothening
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem with segmentation is that a hard threshold typically results
    in small imperfections (that is, holes, as in the preceding image) in the segmented
    region. These holes can be alleviated by using morphological opening and closing.
    When it is opened, it removes small objects from the foreground (assuming that
    the objects are bright on a dark foreground), whereas closing removes small holes
    (dark regions).
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that we can get rid of the small black regions in our mask by applying
    morphological closing (dilation followed by erosion) with a small `3` x `3`-pixel
    kernel, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks a lot smoother, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/077e761a-48fb-48ac-83c1-48d785d26a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice, however, that the mask still contains regions that do not belong to
    the hand or arm, such as what appears to be one of the knees on the left and some
    furniture on the right. These objects just happen to be on the same depth layer
    of my arm and hand. If possible, we could now combine the depth information with
    another descriptor, maybe a texture- or skeleton-based hand classifier that would
    weed out all non-skin regions.
  prefs: []
  type: TYPE_NORMAL
- en: An easier approach is to realize that most of the time, hands are not connected
    to knees or furniture. Let's learn how to find connected components in a segmentation
    mask.
  prefs: []
  type: TYPE_NORMAL
- en: Finding connected components in a segmentation mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know that the center region belongs to the hand. For such a scenario,
    we can simply apply `cv2.floodfill` to find all the connected image regions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we do this, we want to be absolutely certain that the seed point for
    the flood fill belongs to the right mask region. This can be achieved by assigning
    a grayscale value of `128` to the seed point. However, we also want to make sure
    that the center pixel does not, by any coincidence, lie within a cavity that the
    morphological operation failed to close.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s set a small 7 x 7-pixel region with a grayscale value of `128` instead,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As **flood filling** (as well as morphological operations) is potentially dangerous,
    OpenCV requires the specification of a mask that avoids *flooding* the entire
    image. This mask has to be 2 pixels wider and taller than the original image and
    has to be used in combination with the `cv2.FLOODFILL_MASK_ONLY` flag.
  prefs: []
  type: TYPE_NORMAL
- en: It can be very helpful to constrain the flood filling to a small region of the
    image or a specific contour so that we need not connect two neighboring regions
    that should never have been connected in the first place. *It's better to be safe
    than sorry, right?*
  prefs: []
  type: TYPE_NORMAL
- en: '*Nevertheless, today, we feel courageous!* Let''s make the `mask` entirely
    black, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can apply the flood fill to the center pixel (the seed point), and
    paint all the connected regions white, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it should be clear why we decided to start with a gray mask
    earlier. We now have a mask that contains white regions (arm and hand), gray regions
    (neither arm nor hand, but other things in the same depth plane), and black regions
    (all others). With this setup, it is easy to apply a simple binary `threshold`
    to highlight only the relevant regions of the pre-segmented depth plane, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the resulting mask looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea3096a1-d616-46c9-ba73-4941a4026c05.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting segmentation mask can now be returned to the `recognize` function,
    where it will be used as an input to the `find_hull_defects` function, as well
    as a canvas for drawing the final output image (`img_draw`). The function analyzes
    the shape of a hand in order to detect the defects of a hull that corresponds
    to the hand. Let's learn how to perform hand shape analysis in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing hand shape analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know (roughly) where the hand is located, we aim to learn something
    about its shape. In this app, we will make a decision on which exact gesture is
    shown, based on convexity defects of a contour corresponding to the hand.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on and learn how to determine the contour of the segmented hand region
    in the next section, which will be the first step in our hand shape analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the contour of the segmented hand region
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step involves determining the contour of the segmented hand region.
    Luckily, OpenCV comes with a pre-canned version of such an algorithm—`cv2.findContours`.
    This function acts on a binary image and returns a set of points that are believed
    to be part of the contour. As there might be multiple contours present in the
    image, it is possible to retrieve an entire hierarchy of contours, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, because we do not know which contour we are looking for, we have
    to make an assumption to clean up the contour result, since it is possible that
    some small cavities are left over even after the morphological closing. However,
    we are fairly certain that our mask contains only the segmented area of interest.
    We will assume that the largest contour found is the one that we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we simply traverse the list of contours, calculate the contour area (`cv2.contourArea`),
    and store only the largest one (`max_contour`), like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The contour that we found might still have too many corners. We approximate
    the `contour` with a similar `contour` that does not have sides that are less
    than 1 percent of the perimeter of the `contour`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Let's learn how to find the convex hull of a contour area, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the convex hull of a contour area
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have identified the largest contour in our mask, it is straightforward
    to compute the convex hull of the contour area. The convex hull is basically the
    envelope of the contour area. If you think of all the pixels that belong to the
    contour area as a set of nails poking out of a board, then a tight rubber band
    encircles all the nails forming the convex hull shape. We can get the convex hull
    directly from our largest contour (`max_contour`), like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As we now want to look at convexity deficits in this hull, we are instructed
    by the OpenCV documentation to set the `returnPoints` optional flag to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convex hull drawn in yellow around a segmented hand region looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a661ec5-5d71-4210-924d-13bf84e297d2.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned previously, we will determine a hand gesture based on convexity
    defects. Let's move on and learn how to find the convexity defects of a convex
    hull in the next section, which will bring us one step closer to recognizing hand
    gestures.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the convexity defects of a convex hull
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As is evident from the preceding screenshot, not all points on the convex hull
    belong to the segmented hand region. In fact, all the fingers and the wrist cause
    severe *convexity defects*—that is, points of the contour that are far away from
    the hull.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find these defects by looking at both the largest contour (`max_contour`)
    and the corresponding convex hull (`hull`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The output of this function (`defects`) is a NumPy array containing all defects.
    Each defect is an array of four integers that are `start_index` (index of the
    point in the contour where the defect begins), `end_index` (index of the point
    in the contour where the defect ends), `farthest_pt_index` (the index of the farthest
    point from the convex hull within the defect), and `fixpt_depth` (the distance
    between the farthest point and the convex hull).
  prefs: []
  type: TYPE_NORMAL
- en: We will make use of this information in just a moment when we try to estimate
    the number of extended fingers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, though, our job is done. The extracted contour (`max_contour`) and
    convexity defects (`defects`) can be returned to `recognize`, where they will
    be used as inputs to `detect_num_fingers`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we have found the defects, let's move on and learn how to perform
    hand gesture recognition using the convexity defects, which will bring us toward
    the completion of the app.
  prefs: []
  type: TYPE_NORMAL
- en: Performing hand gesture recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What remains to be done is to classify the hand gesture based on the number
    of extended fingers. For example, if we find five extended fingers, we assume
    the hand to be open, whereas no extended fingers implies a fist. All that we are
    trying to do is count from zero to five, and make the app recognize the corresponding
    number of fingers.
  prefs: []
  type: TYPE_NORMAL
- en: This is actually trickier than it might seem at first. For example, people in
    **Europe** might count to three by extending their *thumb*, *index finger*, and
    *middle finger*. If you do that in the **US**, people there might get horrendously
    confused, because they do not tend to use their thumbs when signaling the number
    two.
  prefs: []
  type: TYPE_NORMAL
- en: This might lead to frustration, especially in restaurants (trust me). If we
    could find a way to generalize these two scenarios—maybe by appropriately counting
    the number of extended fingers, we would have an algorithm that could teach simple
    hand gesture recognition to not only a machine but also (maybe) to a person of
    average intellect.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have guessed, the answer is related to convexity defects. As mentioned
    earlier, extended fingers cause defects in the convex hull. However, the inverse
    is not true; that is, not all convexity defects are caused by fingers! There might
    be additional defects caused by the wrist, as well as the overall orientation
    of the hand or the arm. *How can we distinguish between these different causes
    of defects?*
  prefs: []
  type: TYPE_NORMAL
- en: Let's distinguish between different cases of convexity defects, in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing between different causes of convexity defects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The trick is to look at the angle between the farthest point from the convex
    hull point within the defect (`farthest_pt_index`) and the start and endpoints
    of the defect (`start_index` and `end_index`, respectively), as illustrated in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16ce6afa-bc58-488c-841e-99f93470c5e0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous screenshot, the orange markers serve as a visual aid to center
    the hand in the middle of the screen, and the convex hull is outlined in green.
    Each red dot corresponds to *the point farthest from the convex hull* (`farthest_pt_index`)
    for every convexity defect detected. If we compare a typical angle that belongs
    to two extended fingers (such as **θ**j) to an angle that is caused by general
    hand geometry (such as **θi**), we notice that the former is much smaller than
    the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is obviously because humans can spread their fingers only a little, thus
    creating a narrow angle made by the farthest defect point and the neighboring
    fingertips. Therefore, we can iterate over all convexity defects and compute the
    angle between the said points. For this, we will need a utility function that
    calculates the angle (in radians) between two arbitrary—a list like vectors, `v1`
    and `v2`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This method uses the cross product to compute the angle, rather than doing
    it in the standard way. The standard way of calculating the angle between two
    vectors `v1` and `v2` is by calculating their dot product and dividing it by the
    `norm` of `v1` and the `norm` of `v2`. However, this method has two imperfections:'
  prefs: []
  type: TYPE_NORMAL
- en: You have to manually avoid division by zero if either the `norm` of `v1` or
    the `norm` of `v2` is zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method returns relatively inaccurate results for small angles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, we provide a simple function to convert an angle from degrees to
    radians, illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we'll see how to classify hand gestures based on the number
    of extended fingers.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying hand gestures based on the number of extended fingers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What remains to be done is to actually classify the hand gesture based on the
    number of instances of extended fingers. The classification is done using the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The function accepts the detected contour (`contour`), the convexity defects
    (`defects`), a canvas to draw on (`img_draw`), and a cutoff angle that can be
    used as a threshold to classify whether convexity defects are caused by extended
    fingers or not (`thresh_deg`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Except for the angle between the thumb and the index finger, it is rather hard
    to get anything close to 90 degrees, so anything close to that number should work.
    We do not want the cutoff angle to be too high, because that might lead to errors
    in classifications. The complete function will return the number of fingers and
    the illustration frame, and consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s focus on special cases. If we do not find any convexity `defects`,
    it means that we possibly made a mistake during the convex hull calculation, or
    there are simply no extended fingers in the frame, so we return `0` as the number
    of detected fingers, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can take this idea even further. Due to the fact that arms are
    usually slimmer than hands or fists, we can assume that the hand geometry will
    always generate at least two convexity defects (which usually belong to the wrists).
    So, if there are no additional defects, it implies that there are no extended
    fingers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have ruled out all special cases, we can begin counting real fingers.
    If there is a sufficient number of defects, we will find a defect between every
    pair of fingers. Thus, in order to get the number right (`num_fingers`), we should
    start counting at `1`, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start iterating over all convexity defects. For each defect, we extract
    the three points and draw its hull for visualization purposes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the angle between the two edges from `far` to `start` and
    from `far` to `end`. If the angle is smaller than `thresh_deg` degrees, it means
    that we are dealing with a defect that is most likely caused by two extended fingers.
    In such cases, we want to increment the number of detected fingers (`num_fingers`)
    and draw the point with `green`. Otherwise, we draw the point with `red`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'After iterating over all convexity defects, we `return` the number of detected
    fingers and the assembled output image, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Computing the minimum will make sure that we do not exceed the common number
    of fingers per hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae752397-7a09-4d67-ae38-690491dcc7d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, our app is able to detect the correct number of extended fingers
    in a variety of hand configurations. Defect points between extended fingers are
    easily classified as such by the algorithm, and others are successfully ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed a relatively simple—and yet surprisingly robust—way of recognizing
    a variety of hand gestures by counting the number of extended fingers.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm first shows how a task-relevant region of the image can be segmented
    using depth information acquired from a Microsoft Kinect 3D sensor, and how morphological
    operations can be used to clean up the segmentation result. By analyzing the shape
    of the segmented hand region, the algorithm comes up with a way to classify hand
    gestures based on the types of convexity effects found in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, mastering our use of OpenCV to perform the desired task did not
    require us to produce a large amount of code. Instead, we were challenged to gain
    an important insight that made us use the built-in functionality of OpenCV in
    an effective way.
  prefs: []
  type: TYPE_NORMAL
- en: Gesture recognition is a popular but challenging field in computer science,
    with applications in a large number of areas, such as **Human-Computer Interaction**
    (**HCI**), video surveillance, and even the video game industry. You can now use
    your advanced understanding of segmentation and structure analysis to build your
    own state-of-the-art gesture recognition system. Another approach you might want
    to use for hand gesture recognition is to train a deep image classification network
    on hand gestures. We will discuss deep networks for image classifications in [Chapter
    9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml), *Learning to Classify and Localize
    Objects*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will continue to focus on detecting objects of interest
    in visual scenes, but we will assume a much more complicated case: viewing the
    object from an arbitrary perspective and distance. To do this, we will combine
    perspective transformations with scale-invariant feature descriptors to develop
    a robust feature-matching algorithm.'
  prefs: []
  type: TYPE_NORMAL
