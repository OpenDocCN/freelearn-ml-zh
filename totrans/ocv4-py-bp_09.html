<html><head></head><body>
        

                            
                    <h1 class="header-title">Learning to Classify and Localize Objects</h1>
                
            
            
                
<p>So far, we have studied a range of algorithms and approaches where you have learned how to solve real-world problems with the help of computer vision. In recent years, in parallel with the considerable hardware computational power that is provided with devices such as <strong>Graphical Processing Units </strong>(<strong>GPUs</strong>), a lot of algorithms arose that utilized this power and achieved state-of-the-art results in computer vision tasks. Usually, these algorithms are based on neural networks, which enable the creator of the algorithm to squeeze quite a lot of meaningful information from data. </p>
<p>Meanwhile, in contrast to the classical approaches, this information is often quite hard to interpret. From that point of view, you might say that we are getting closer to artificial intelligence—that is, we are giving a computer an approach and it figures out how to do the rest. In order for all of this to not appear so mysterious, let's learn about deep learning models in this chapter.</p>
<p>As you have already seen, a few of the classical problems in computer vision include object detection and localization. Let's look at how to classify and localize objects with the help of deep learning models in this chapter. </p>
<p>The goal of this chapter is to learn important deep learning concepts such as transfer learning and how to apply them to build your own object classifier and localizer. Specifically, we will cover the following topics:</p>
<ul>
<li>Preparing a large dataset for training a deep learning model</li>
<li>Understanding <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>)</li>
<li>Classifying and localizing with CNNs </li>
<li>Learning about transfer learning</li>
<li>Implementing activation functions</li>
<li>Understanding backpropagation</li>
</ul>
<p>We will start by preparing a dataset for training. Then, we will go on to understand how to use a pretrained model for creating a new classifier. Once you have understood how it is done, we will move forward and build more complex architectures that will perform localization.</p>
<p>During these steps, we will use the <strong>Oxford-IIIT-Pet</strong> dataset. Finally, we will run the app that will use our trained localizer network for inference. Although the network will be trained only using the bounding boxes of the heads of pets, you will see that it will also be good for localization of the human head position. The latter will show the power of generalization of our model.</p>
<p>Learning about these concepts of deep learning and seeing them in action will be very useful in the future when you make your own applications using deep learning models or when you start to work on completely new deep learning architectures.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting started</h1>
                
            
            
                
<p>As we have mentioned in all of the chapters of this book, you will need to have OpenCV installed. Besides that, you will need to install TensorFlow. </p>
<p>The Oxford-IIIT-Pet dataset is available for download at <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>, along with our dataset preparation script, which will be downloaded automatically for you.</p>
<p>You can find the code that we present in this chapter (from the GitHub repository) at <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition</a>. You can also use the Docker files available in the repository to run the code in the chapter. Refer to the <a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml">Appendix B</a>, <em>Setting Up a Docker Container</em>, for more information on the Docker files.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the app</h1>
                
            
            
                
<p>The final app will consist of modules to prepare the dataset, train the models, and run an inference with the models using input from your camera. This will require the following components:</p>
<ul>
<li><kbd>main.py</kbd>: This is the main script for starting the application and localizing the head (of the pets) in real time.</li>
<li><kbd>data.py</kbd>: This is a module to download and prepare the dataset for training.</li>
<li><kbd>classification.py</kbd>: This is a script to train a classifier network.</li>
<li><kbd>localization.py</kbd>: This is a script to train and save a localization network.</li>
</ul>
<p>After preparing the dataset for training, we will do the following to complete our app:</p>
<ol>
<li>We will first train a classification network using transfer learning.</li>
<li>Next, we will train an object localization network, again using transfer learning.</li>
<li>After we create and train our localization network, we will run our <kbd>main.py</kbd> script to localize the heads in real time.</li>
</ol>
<p>Let's start by learning how to prepare the inference script that will run our app. The script will connect to your camera, find a head position in each frame of the video stream using the localization model that we will create, and then illustrate the results in real time. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing an inference script</h1>
                
            
            
                
<p>Our inference script is quite simple. It will first prepare a drawing function, then load the model and connect it to the camera. Then, it will loop over the frames from the video stream. In the loop, for each frame of the stream, it will use the imported model to make an inference and the drawing function to display the results. Let's create a complete script using the following steps:</p>
<ol>
<li>First, we import the required modules:</li>
</ol>
<pre style="color: black;padding-left: 60px">import numpy as np<br/>import cv2<br/>import tensorflow.keras as K</pre>
<p style="color: black;padding-left: 60px">In this code, besides importing NumPy and OpenCV, we have also imported <strong>Keras</strong>. We are going to use Keras to make predictions in this script; additionally, we will use it to create and train our models throughout the chapter.</p>
<ol start="2">
<li>Then, we define a function to draw localization bounding boxes on a frame:</li>
</ol>
<pre style="color: black;padding-left: 60px">def draw_box(frame: np.ndarray, box: np.ndarray) -&gt; np.ndarray:<br/>    h, w = frame.shape[0:2]<br/>    pts = (box.reshape((2, 2)) * np.array([w, h])).astype(np.int)<br/>    cv2.rectangle(frame, tuple(pts[0]), tuple(pts[1]), (0, 255, 0), 2)<br/>    return frame</pre>
<p style="padding-left: 60px" class="mce-root">The preceding <kbd>draw_box</kbd> function accepts <kbd>frame</kbd> and the normalized coordinates of the two corners of a bounding box as an array of four numbers. The function first reshapes the one-dimensional array of the box into a two-dimensional array, where the first index represents the point and the second represents the <em>x</em> and <em>y</em> coordinates. Then, it transforms the normalized coordinates to the coordinates of the image by multiplying them with an array composed of the width and height of the image and translates the result into integer values in the same line. Finally, it draws the bounding box with the color green using the <kbd>cv2.rectangle</kbd> function and returns <kbd>frame</kbd>.</p>
<ol start="3">
<li>Then, we import the model that we will prepare throughout the chapter and connect to the camera:</li>
</ol>
<pre style="color: black;padding-left: 60px">model = K.models.load_model("localization.h5")<br/>cap = cv2.VideoCapture(0)</pre>
<p style="color: black;padding-left: 60px"><kbd>model</kbd> will be stored in a binary file, which is imported using a convenient function from Keras. </p>
<ol start="4">
<li>After that, we iterate over the frames from the camera, resize each <kbd>frame</kbd> to a standard size (that is, the default image size for the models that we will create), and convert <kbd>frame</kbd> to the <strong>RGB</strong> (<strong>red</strong>, <strong>green</strong>, <strong>blue</strong>) color space as we will train our models on RGB images:</li>
</ol>
<pre style="color: black;padding-left: 60px">for _, frame in iter(cap.read, (False, None)):<br/>    input = cv2.resize(frame, (224, 224))<br/>    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)</pre>
<ol start="5">
<li>In the same loop, we normalize the image and add one to the shape of the frame as the model accepts batches of images. Then, we pass the result to <kbd>model</kbd> for inference:</li>
</ol>
<pre style="padding-left: 60px">    box, = model.predict(input[None] / 255)</pre>
<ol start="6">
<li>We continue the loop by drawing the predicted bounding box using the previously defined function, show the results, and then set the termination criteria:</li>
</ol>
<pre style="color: black;padding-left: 60px">    cv2.imshow("res", frame)<br/>    if(cv2.waitKey(1) == 27):<br/>        break</pre>
<p>Now that we have our inference script ready, let's start the journey of creating our model by first preparing the dataset in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the dataset</h1>
                
            
            
                
<p>As mentioned previously, in this chapter, we are going to use the Oxford-IIIT-Pet dataset. It will be a good idea to encapsulate the preparation of the dataset in a separate <kbd>data.py</kbd> script, which can then be used throughout the chapter. As with any other script, first of all, we have to import all the required modules, as shown in the following code snippet:</p>
<pre>import glob<br/>import os<br/><br/>from itertools import count<br/>from collections import defaultdict, namedtuple<br/><br/>import cv2<br/>import numpy as np<br/>import tensorflow as tf<br/>import xml.etree.ElementTree as ET</pre>
<p>In order to prepare our dataset for use, we will first download and parse the dataset into memory. Then, out of the parsed data, we will create a TensorFlow dataset, which allows us to work with a dataset in a convenient manner as well as prepare the data in the background so that the preparation of the data does not interrupt the neural network training process. So, let's move on to download and parse the dataset in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downloading and parsing the dataset</h1>
                
            
            
                
<p>In this section, we first download the dataset from the official website and then parse it into a convenient format. During this stage, we will leave out the images, which occupy quite a lot of memory. We cover this procedure in the following steps:</p>
<ol>
<li>
<p>Define where we want to store our pets dataset and download it using a convenient <kbd>get_file</kbd> function in Keras:</p>
</li>
</ol>
<pre style="padding-left: 60px">DATASET_DIR = "dataset"<br/>for type in ("annotations", "images"):<br/>    tf.keras.utils.get_file(<br/>        type,<br/>        f"https://www.robots.ox.ac.uk/~vgg/data/pets/data/{type}.tar.gz",<br/>        untar=True,<br/>        cache_dir=".",<br/>        cache_subdir=DATASET_DIR)</pre>
<p style="padding-left: 60px">As our dataset resides in an archive, we have also extracted it by passing <kbd>untar=True</kbd>. We also pointed <kbd>cache_dir</kbd> to the current directory. Once the files are saved, consequent executions of the <kbd>get_file</kbd> function will result in no action. </p>
<p>The dataset weighs more than half a gigabyte, and, on the first run, you will need a stable internet connection with good bandwidth.</p>
<ol start="2">
<li>Once we have downloaded and extracted our dataset, let's define constants for the dataset and annotation folders and set the image size that we want to resize our images to:</li>
</ol>
<pre style="padding-left: 60px">IMAGE_SIZE = 224<br/>IMAGE_ROOT = os.path.join(DATASET_DIR,"images")<br/>XML_ROOT = os.path.join(DATASET_DIR,"annotations")</pre>
<p style="padding-left: 60px">Size <kbd>224</kbd> is often the default size on which image classification networks are trained. Hence, it's a good idea to keep to that size for better accuracy.</p>
<ol start="3">
<li>Annotations of this dataset contain information about the image in XML format. Before parsing the XML, let's first define what data we want to have:</li>
</ol>
<pre style="padding-left: 60px">Data = namedtuple("Data","image,box,size,type,breed")</pre>
<p style="padding-left: 60px"><kbd>namedtuple</kbd> is an extension of a standard tuple in Python and allows you to refer to an element of a tuple by its name. The names that we have defined correspond to the data elements that we are interested in. Namely, those are the image itself (<kbd>image</kbd>), the head bounding box of the pet ( <kbd>box</kbd>), the image size, <kbd>type</kbd> (cat or dog), and <kbd>breed</kbd> (there are 37 breeds).</p>
<ol start="4">
<li><kbd>breeds</kbd> and <kbd>types</kbd> are strings in the annotation; what we want are numbers corresponding to <kbd>breeds</kbd>. For that purpose, we define two dictionaries:</li>
</ol>
<pre style="padding-left: 60px">types = defaultdict(count().__next__ )<br/>breeds = defaultdict(count().__next__ )</pre>
<p style="padding-left: 60px"><kbd>defaultdict</kbd> is a dictionary that returns default values for the undefined keys. Here, it will return the next number starting from zero when requested.</p>
<ol start="5">
<li>Next, we define a function that, given a path to an XML file, will return an instance of our data:</li>
</ol>
<pre style="padding-left: 60px">def parse_xml(path: str) -&gt; Data:</pre>
<p style="padding-left: 60px" class="mce-root">The previously defined function covers the following steps:</p>
<ul>
<li style="list-style-type: none">
<ol>
<li>Open the XML file and parse it:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px">with open(path) as f:<br/>    xml_string = f.read()<br/>root = ET.fromstring(xml_string)</pre>
<p style="padding-left: 120px" class="mce-root">The contents of the XML file are parsed using the <kbd>ElementTree</kbd> module, which represents the XML in a format that is convenient to navigate through.</p>
<ul>
<li style="list-style-type: none">
<ol start="2">
<li>Then, get the name of the corresponding image and extract the name of the breed:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px">img_name = root.find("./filename").text<br/>breed_name = img_name[:img_name.rindex("_")]</pre>
<ul>
<li style="list-style-type: none">
<ol start="3">
<li>After that, convert the breed to a number using <kbd>breeds</kbd> that was previously defined, which assigns the next number for each undefined key:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px">breed_id = breeds[breed_name]</pre>
<ul>
<li style="list-style-type: none">
<ol start="4">
<li>Similarly, get the ID of <kbd>types</kbd>:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px">type_id = types[root.find("./object/name").text]</pre>
<ul>
<li style="list-style-type: none">
<ol start="5">
<li>Then, extract the bounding box and normalize it:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px">box = np.array([int(root.find(f"./object/bndbox/{tag}").text)<br/>                for tag in "xmin,ymin,xmax,ymax".split(",")])<br/>size = np.array([int(root.find(f"./size/{tag}").text)<br/>                 for tag in "width,height".split(",")])<br/>normed_box = (box.reshape((2, 2)) / size).reshape((4))</pre>
<p style="padding-left: 150px">Return the results as an instance of <kbd>Data</kbd>:</p>
<pre style="padding-left: 120px">return Data(img_name,normed_box,size,type_id,breed_id)</pre>
<ol start="6">
<li>Now that we have downloaded the dataset and prepared a parser, let's go on to parse the dataset:</li>
</ol>
<pre style="padding-left: 60px">xml_paths = glob.glob(os.path.join(XML_ROOT,"xmls","*.xml"))<br/>xml_paths.sort()<br/>parsed = np.array([parse_xml(path) for path in xml_paths])</pre>
<p style="padding-left: 60px">We have also sorted the paths so that they appear in the same order in different runtime environments.</p>
<p class="mce-root">As we have parsed our dataset, we might want to print out available breeds and types for illustration:</p>
<pre>print(f"{len(types)} TYPES:", *types.keys(), sep=", ")<br/>print(f"{len(breeds)} BREEDS:", *breeds.keys(), sep=", ")</pre>
<p>The previous code snippet outputs two types, namely <kbd>cat</kbd> and <kbd>dog</kbd>, and their <kbd>breeds</kbd>:</p>
<pre class="mce-root"><strong>2 TYPES:, cat, dog</strong><br/><strong>37 BREEDS:, Abyssinian, Bengal, Birman, Bombay, British_Shorthair, Egyptian_Mau, Maine_Coon, Persian, Ragdoll, Russian_Blue, Siamese, Sphynx, american_bulldog, american_pit_bull_terrier, basset_hound, beagle, boxer, chihuahua, english_cocker_spaniel, english_setter, german_shorthaired, great_pyrenees, havanese, japanese_chin, keeshond, leonberger, miniature_pinscher, newfoundland, pomeranian, pug, saint_bernard, samoyed, scottish_terrier, shiba_inu, staffordshire_bull_terrier, wheaten_terrier, yorkshire_terrier</strong></pre>
<p>Later on in this chapter, we will have to split the dataset on training and test sets. In order to perform a good split, we should randomly pick data elements from the dataset in order to have a proportional number of <kbd>breeds</kbd> in the train and test sets.</p>
<p>We can mix the dataset now so that we don't have to worry about it later, as follows:</p>
<pre>np.random.seed(1)<br/>np.random.shuffle(parsed)</pre>
<p>The previous code first sets a random seed, which is required to get the same result every time we execute the code. The <kbd>seed</kbd> method accepts one argument, which is a number specifying a random sequence.</p>
<p>Once the <kbd>seed</kbd> method is set, we have the same sequence of random numbers in functions that use random numbers. Such numbers are called <strong>pseudorandom</strong>. This means that, although they look random, they are predefined. In our case, we use the <kbd>shuffle</kbd> method, which mixes the order of elements in the <kbd>parsed</kbd> array.</p>
<p>Now that we have parsed our dataset into a convenient NumPy array, let's move on and create a TensorFlow dataset out of it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a TensorFlow dataset </h1>
                
            
            
                
<p>We are going to use the TensorFlow dataset adapter in order to train our models. Of course, we could create a NumPy array from our dataset, but imagine how much memory it would require to keep all the images in the memory.</p>
<p>In contrast, the dataset adapter allows you to load the data into memory when required. Moreover, the data is loaded and prepared in the background so that it will not be a bottleneck in our training process. We transform our parsed array as follows: </p>
<pre>ds = tuple(np.array(list(i)) for i in np.transpose(parsed))<br/>ds_slices = tf.data.Dataset.from_tensor_slices(ds)</pre>
<p>From the previous code snippet, <kbd>from_tensor_slices</kbd> creates <kbd>Dataset</kbd> whose elements are slices of the given tensors. In our case, the tensors are NumPy arrays of labels (box, breed, image location, and more).</p>
<p>Under the hood, it is a similar concept to the Python <kbd>zip</kbd> function. First, we have prepared the input accordingly. Let's now print one element from the dataset to see how it looks:</p>
<pre>for el in ds_slices.take(1):<br/>    print(el)</pre>
<p>This gives the following output:</p>
<pre class="mce-root"><strong>(&lt;tf.Tensor: id=14, shape=(), dtype=string, numpy=b'american_pit_bull_terrier_157.jpg'&gt;, &lt;tf.Tensor: id=15, shape=(4,), dtype=float64, numpy=array([0.07490637, 0.07 , 0.58426966, 0.44333333])&gt;, &lt;tf.Tensor: id=16, shape=(2,), dtype=int64, numpy=array([267, 300])&gt;, &lt;tf.Tensor: id=17, shape=(), dtype=int64, numpy=1&gt;, &lt;tf.Tensor: id=18, shape=(), dtype=int64, numpy=13&gt;)</strong></pre>
<p>It is the TensorFlow—<kbd>tensor</kbd> that contains all the information that we have parsed from a single XML file. Given the dataset, we can check whether all our bounding boxes are correct:</p>
<pre>for el in ds_slices:<br/>    b = el[1].numpy()<br/>    if(np.any((b&gt;1) |(b&lt;0)) or np.any(b[2:]-b[:2] &lt; 0)):<br/>        print(f"Invalid box found {b} image: {el[0].numpy()}")</pre>
<p>As we have normalized the boxes, they should be in the range of <kbd>[0,1]</kbd>. Additionally, we make sure that the coordinates of the first point of the box are less than or equal to the coordinates of the second point.</p>
<p>Now, we define a function that will transform our data element so that we can feed it into a neural network: </p>
<pre>def prepare(image,box,size,type,breed):<br/>    image = tf.io.read_file(IMAGE_ROOT+"/"+image)<br/>    image = tf.image.decode_png(image,channels=3)<br/>    image = tf.image.resize(image,(IMAGE_SIZE,IMAGE_SIZE))<br/>    image /= 255<br/>    return Data(image,box,size,tf.one_hot(type,len(types)),tf.one_hot(breed,len(breeds)))</pre>
<p class="mce-root">The function first loads the corresponding image and resizes it to the standard size and normalizes it to <kbd>[0,1]</kbd>. Then, it creates a <kbd>one_hot</kbd> vector out of <kbd>types</kbd> and <kbd>breeds</kbd> using the <kbd>tf.one_hot</kbd> method and returns the result as an instance of <kbd>Data</kbd>. </p>
<p class="mce-root">Now what remains is to <kbd>map</kbd> our dataset with the function, and we are ready to go:</p>
<pre>ds = ds_slices.map(prepare).prefetch(32)</pre>
<p>We have also called the <kbd>prefetch</kbd> method, which makes sure that some amount of data is prefetched so that our networks will not have to wait for the data to be loaded from the hard drive.</p>
<p>If we are running the data preparation script directly, it might be a good idea to illustrate some samples of the data. First, we create a function that creates an illustration image when it is given a sample of data:</p>
<pre>if __name__ == "__main__":<br/>    def illustrate(sample):<br/>        breed_num = np.argmax(sample.breed)<br/>        for breed, num in breeds.items():<br/>            if num == breed_num:<br/>                break<br/>        image = sample.image.numpy()<br/>        pt1, pt2 = (sample.box.numpy().reshape(<br/>            (2, 2)) * IMAGE_SIZE).astype(np.int32)<br/>        cv2.rectangle(image, tuple(pt1), tuple(pt2), (0, 1, 0))<br/>        cv2.putText(image, breed, (10, 10),<br/>                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 1, 0))<br/>        return image</pre>
<p>The function converts the <kbd>breed</kbd> one-hot vector back to a number, finds the name of the breed in the <kbd>breeds</kbd> dictionary, and plots the bounding box of the head together with the breed name.</p>
<p>Now, we concatenate several such illustrations and show the resulting image:</p>
<pre>samples_image = np.concatenate([illustrate(sample)<br/>                                for sample in ds.take(3)], axis=1)<br/>cv2.imshow("samples", samples_image)<br/>cv2.waitKey(0)</pre>
<p>The result is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b8a17387-fbe6-4919-8f02-5520a8e469c8.png" style="width:51.92em;height:17.33em;"/></p>
<p>The preceding screenshot shows nice pets with the bounding boxes around their heads as expected. Note that, although we have used random numbers to mix the dataset in our script, you obtain the same result as illustrated previously. So, you can now see the power of pseudorandom numbers. </p>
<p>Now that we have prepared the dataset, let's move on to creating and training classifiers in the next section. We will build two classifiers—one for the pet type and the other for the breed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Classifying with CNNs</h1>
                
            
            
                
<p>To start with the classification, first of all, we have to import the required modules:</p>
<pre>import tensorflow.keras as K<br/>from data import ds</pre>
<p>We have to import our prepared dataset and Keras, which we will use to build our classifier.</p>
<p>However, before we build our classifier, let's first learn about convolutional networks, as we are going to use them to build our classifier.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding CNNs</h1>
                
            
            
                
<p>In <a href="2e878463-75f1-40a5-b263-0c5aa9627328.xhtml">Chapter 1</a>, <em>Fun with Filters</em>, you learned about filters and convolution. In particular, you learned how filters can be used to create a pencil sketch image. In the pencil sketch, you could see the points in the image that had a sharp change in value, that is, they were darker than those that had a smooth change.</p>
<p>From that point of view, the filters that we applied can be thought of as filters for edge detection. In other words, the filters act as a feature detector, where the feature is an edge. Alternatively, you could compose a different filter that is activated on the corners or that is activated when there is no change in the color value. </p>
<p>The filters that we have used act on a single-channel image and have two dimensions; however, we can extend the filter with the third dimension, which can then be applied to a multichannel image. For example, if a single-channel filter has size <em>3 x 3</em>, the corresponding 3-channel (for example, RGB) filter will have size <em>3 x 3 x 3</em>, where the last value is the depth of the filter.</p>
<p>Such filters can already be used for more complex features. For example, you might think of a filter that works with the color green, meanwhile ignoring the values in red and blue by setting zeros in the corresponding elements of the filter. </p>
<p>Once you come up with a good set of filters, you can apply them to the original image and then stack them into a new multichannel image. For example, if we apply 100 filters on an image, we will obtain 100 single-channel images, which will result in a 100-channel image after stacking. Hence, we have built a layer that accepts 3 channels and outputs 100 channels.</p>
<p>Next, we can compose new filters that have a depth of 100 and act on the composed 100-channel image. These filters can also be activated on more complex features. For example, if there were filters in previous layers that are activated on edges, we can compose a filter that is activated on an intersection of edges.</p>
<p>After a range of layers, we might see filters that are activated, for example, on the noses of people, heads, the wheels of vehicles, or so on. That is actually how the convolutional network works. Surely, a question arises: how do we compose those filters? The answer is we don't, because they are learned.</p>
<p>We provide the data and the network learns which filters it needs to make good predictions. Another difference between the convolutional filters that you have used is that, besides the learnable parameter of the filters, there is one more learnable value called, which is a constant term added to the output of a filter.</p>
<p>Besides that, after the convolutional filters in each layer, usually, a nonlinear function is applied to the output of the filters called the <strong>activation function</strong>. As a result of the nonlinearity, the network represents quite a wider class of functions so that there is a relatively higher chance of building a good model.</p>
<p>Now that we have some understanding of how a convolutional network works, let's start by building a classifier. While building the networks in this chapter, you will see how the convolutional layers are built and used. As mentioned previously, we use a pretrained model for our new models, or, in other words, we use transfer learning. Let's understand what this is in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about transfer learning</h1>
                
            
            
                
<p class="mce-root">Usually, a CNN has millions of parameters. Let's make an estimation to find out where all of those parameters come from.</p>
<p class="mce-root">Suppose we have a 10-layer network and each layer has 100 filters of size <em>3 x 3</em>. These numbers are quite low and networks that have good performance usually have dozens of layers and hundreds of filters in each layer. For our case, each filter has a depth of 100.</p>
<p class="mce-root">Hence, each filter has 3 x 3 x 3 = 900 parameters (excluding biases, the number of which is 100), which results in <em>900 x 100</em> parameters for each layer and, therefore, about 900,000 parameters for the complete network. To learn so many parameters from scratch without overfitting would require quite a large annotated dataset. A question arises: what can we do instead? </p>
<p>You have learned that layers of a network act as feature extractors. Besides this, natural images have quite a lot in common. Therefore, it would be a good idea to use the feature extractors of a network that was trained on a large dataset to achieve good performance on a different, smaller dataset. This technique is called <strong>transfer learning</strong>.</p>
<p>Let's pick a pretrained model as our base model, which is a single line of code with Keras:</p>
<pre>base_model = K.applications.MobileNetV2(input_shape=(224,224, 3), include_top=False)</pre>
<p class="mce-root">Here, we use the <kbd>MobileNetV2</kbd> pretrained network, which is a robust and lightweight network. Of course, you can use other available models instead, which can be found on the Keras website or by simply listing them with <kbd>dir(K.applications)</kbd>. </p>
<p class="mce-root">We have taken the version of the network that excludes the top layers responsible for classification by passing in <kbd>include_top=False</kbd>, as we are going to build a new classifier on top of it. But still, the network includes all the other layers that were trained on <strong>ImageNet</strong>. ImageNet is a dataset that includes millions of images and each of the images is annotated with one of 1,000 classes of the dataset.</p>
<p>Let's take a look at the shape of the output of our base model:</p>
<pre>print(base_model.output.shape)</pre>
<p>The result is as follows:</p>
<pre class="mce-root"><strong>(None, 7, 7, 1280)</strong></pre>
<p>The first number is undefined and denotes the batch size or, in other words, the number of input images. Suppose we simultaneously pass a stack of 10 images to the network; then, the output here would have a shape of <kbd>(10,7,7,1280)</kbd> and the first dimension of the tensor will correspond to the input image number.</p>
<p>The next two indexes are the size of the output shape and the last is the number of channels. In the original model, this output represents features from the input images that are later used to classify the images of the ImageNet dataset.</p>
<p>Therefore, they are quite a good representation of all the images so that the network can classify the images of ImageNet based on them. Let's try to use these features to classify the types and breeds of our pets. In order to do this, let's first prepare a classifier in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the pet type and breed classifier</h1>
                
            
            
                
<p>As we are going to use the features as they are, let's first freeze the weights of the network layers so that they don't update during the training process:</p>
<pre>for layer in base_model.layers:<br/>    layer.trainable = False</pre>
<p class="mce-root">In general, each location of an activation map specifies whether there is a feature of the corresponding type in that location. As we work on the last layers of the network, we can suppose that different locations on the activation map contain similar information and reduce the dimensionality of our features by averaging the activation maps:</p>
<pre>x = K.layers.GlobalAveragePooling2D()(base_model.output)</pre>
<p class="mce-root">The operation is called <kbd>AveragePooling2D</kbd>—we pool the average of the tensor in two dimensions of our feature tensor. You can see the results by printing the shapes of the input and output of the operation:</p>
<pre>print(base_model.output.shape, x.shape)</pre>
<p>This shows the following output:</p>
<pre class="mce-root"><strong>(None, 7, 7, 1280) (None, 1280)</strong></pre>
<p>Now that we have just <kbd>1280</kbd> features per image, let's add the classification layer right away and prepare our dataset for training either on the types or the breeds: </p>
<pre>is_breeds = True<br/>if is_breeds:<br/>    out = K.layers.Dense(37,activation="softmax")(x)<br/>    inp_ds = ds.map(lambda d: (d.image,d.breed))<br/>else:<br/>    out = K.layers.Dense(2,activation="softmax")(x)<br/>    inp_ds = ds.map(lambda d: (d.image,d.type))</pre>
<p class="mce-root">Training on the types and <kbd>breeds</kbd> differs only by the number of output neurons and the labels. In the case of <kbd>breeds</kbd>, the number of labels is <kbd>37</kbd>, and, in the case of types, this is <kbd>2</kbd> (namely cat or dog), which you can see in the code. A dense layer represents densely connected neurons. The latter means that each neuron in the layer is connected to all 1,280 inputs to the layer.</p>
<p class="mce-root">Hence, each neuron has <em>1280 + 1</em> learnable parameters, where 1 is for the bias. Mathematically, for the complete layer, the weights of the kernel are represented with a matrix that has a size (1,280 for the number of classes) and a column of height 1280.</p>
<p class="mce-root">The linear part of the layer can be written as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="img/d26cdbef-7778-47fc-941b-c1661e2aa0d5.png" style="width:5.75em;height:1.50em;"/>,</p>
<p>Here, <strong>x</strong> is the output of the previous layer (1,280 averaged features, in our case), <strong>a</strong> is the matrix, and <strong>b</strong> is the column. </p>
<p class="mce-root">Also, we have set a <strong>softmax</strong> function as the activation, which is a good choice with classification tasks. The latter is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e2b6cfaf-9977-473b-87ee-011f3fc1f660.png" style="width:15.50em;height:3.42em;"/></p>
<p>Here, <strong>x</strong> is the input to the activation (output of the linear part).</p>
<p>You can see that it sums up to one across all outputs; hence, the output can be thought of as the probability of the corresponding class.</p>
<p>The mapping that we defined on the dataset will set the image as data and the breeds or types as the label. </p>
<p>Now we are ready to define our model:</p>
<pre>model = K.Model(inputs=base_model.input, outputs=out)</pre>
<p class="mce-root">Here, you can see that the input of the network is our base model and the output is our classifier layer. Hence, we have successfully built our classification network. </p>
<p>So, now that we have prepared our classifier network, let's train and evaluate it in the next section:</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training and evaluating the classifier</h1>
                
            
            
                
<p class="mce-root">In order to train the classifier, we have to configure it for training. We have to specify an objective function (the <kbd>loss</kbd> function) and a training method. Additionally, we might want to specify some metrics in order to see how the model performs. We can configure the classifier using the <kbd>compile</kbd> method of the model:</p>
<pre>model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["categorical_accuracy","top_k_categorical_accuracy"])</pre>
<p class="mce-root">We have passed <kbd>metrics</kbd> as <kbd>categorical_accuracy</kbd>, which will show which part of the dataset is classified with the right class. Besides this, we have passed one more metric called <kbd>top_k_categorical_accuracy</kbd>, which shows which part of the dataset is correct in the top <kbd>k</kbd> prediction of the network.</p>
<p class="mce-root">The default value of <kbd>k</kbd> is five, so the metric shows which part of the dataset is in the most probable five classes predicted by the neural network. We have also passed <kbd>optimizer="adam"</kbd>, which forces the model to use <strong>Adam Optimizer </strong>as a training algorithm. You will learn how neural networks are usually trained in the <em>Understanding backpropagation</em> section.</p>
<p>Before training, we also split the dataset into training and test sets in order to see how the network performs on unseen data:</p>
<pre>evaluate = inp_ds.take(1000)<br/>train = inp_ds.skip(1000).shuffle(10**4)</pre>
<p class="mce-root">Here, we take the first <kbd>1000</kbd> elements of the dataset for test purposes. And the remaining part is used for training.</p>
<p class="mce-root">The training part is mixed by calling the <kbd>shuffle</kbd> method, which will make sure that we have a different order of the data in each epoch of training. Finally, we train our network by calling the <kbd>fit</kbd> method of the dataset and then evaluate this on the validation set:</p>
<pre>model.fit(train.batch(32), epochs=4)<br/>model.evaluate(valid.batch(1))</pre>
<p>First, the <kbd>fit</kbd> method accepts the dataset itself, which we pass with batches of <kbd>32</kbd>. The latter means that, on each step of the training process, <kbd>32</kbd> images from the dataset will be used. </p>
<p>We have also passed a number of <kbd>epochs</kbd>, which means that our dataset will be iterated for <kbd>4</kbd> times until the training procedure stops. The output of the last <kbd>epoch</kbd> looks as follows:</p>
<pre class="mce-root"><strong>Epoch 4/4</strong><br/><strong> 84/84 [==============================] - 13s 156ms/step - loss: 0.0834 - categorical_accuracy: 0.9717 - top_k_categorical_accuracy: 1.0000</strong></pre>
<p>Our categorical accuracy on the train set is more than 97%. So, we are pretty good at differentiating between cats and dogs. Of course, the <strong>top-K accuracy</strong> will be 100 percent as we have just two classes. Now, let's see how we are performing on the validation set.</p>
<p>After training, the model is evaluated and you should obtain results similar to the test set:</p>
<pre>model.evaluate(valid.batch(1))</pre>
<p>The output is given as follows:</p>
<pre class="mce-root"><strong>1000/1000 [==============================] - 9s 9ms/step - loss: 0.0954 - categorical_accuracy: 0.9730 - top_k_categorical_accuracy: 1.0000</strong></pre>
<p>We again get the categorical accuracy of more than 97%. Therefore, our model does not overfit and performs well on the test set.</p>
<p>If we train on breeds, the same output for training looks as follows:</p>
<pre class="mce-root"><strong>Epoch 4/4</strong><br/><strong> 84/84 [==============================] - 13s 155ms/step - loss: 0.3272 - categorical_accuracy: 0.9233 - top_k_categorical_accuracy:</strong><br/><strong> 0.9963</strong></pre>
<p>Meanwhile, the output for testing looks like this: </p>
<pre class="mce-root"><strong>1000/1000 [==============================] - 11s 11ms/step - loss: 0.5646 - categorical_accuracy: 0.8080 - top_k_categorical_accuracy: 0.9890</strong></pre>
<p>For breeds, we get worse results, which is expected as it is much more difficult to differentiate a breed than just state whether it is a cat or a dog. In any case, the model does not perform too badly. Its first-attempt guess is more than 80 percent right, and we can also be about 99 percent sure that it will guess the breed if it has 5 attempts.</p>
<p>In this section, we have learned how to use a pretrained classifier network to build a new classifier. In the next section, let's move ahead with our deep learning journey and create an object localization network using the same base model—a task that the base model was never trained to accomplish.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Localizing with CNNs</h1>
                
            
            
                
<p>Being able to create your own localizer is a good way to acquire intuition on how an object detection network might work. This is because the only conceptual difference between object detection and localization networks is that a localization network predicts a single bounding box, while an object detection network predicts multiple boxes. Also, it is a good way to start understanding how to build a neural network that accomplishes other regression tasks. </p>
<p>In this section, we are going to use the same pretrained classifier network, <kbd>MobileNetV2</kbd>, as the previous section. However, this time we are going to use the network for localizing objects instead of classifying. Let's import the required modules and the base model in the same way that we did in the previous section—although, this time, we are not going to freeze the layers of the base model:</p>
<pre>import tensorflow.keras as K<br/><br/>from data import ds<br/><br/>base_model = K.applications.MobileNetV2(<br/>    input_shape=(224, 224, 3), include_top=False)</pre>
<p>Now that we have everything ready, let's go on to prepare our localizer model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the model</h1>
                
            
            
                
<p>First, let's think about how we can make a localizer using the output of the base model. </p>
<p>As mentioned previously, the output tensor of the base model has a shape of <kbd>(None, 7, 7, 1280)</kbd>. The output tensor represents features obtained using a convolutional network. We can suppose that some spatial information is encoded in the spatial indexes <em>(7,7)</em>.</p>
<p>Let's try to reduce the dimensionality of our feature map using a couple of convolutional layers and create a regressor that should predict the corner coordinates of the pets' head bounding boxes provided by the dataset.</p>
<p>Our convolutional layers will have several options that are the same:</p>
<pre>conv_opts = dict(<br/>    activation='relu',<br/>    padding='same',<br/>    kernel_regularizer="l2")</pre>
<p>First of all, they will both use the <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>) as an activation function. The latter is a simple function, which is zero when the input is less than zero and is equal to the input when the input is greater than or equal to zero.</p>
<p><kbd>padding=same</kbd> specifies that we do not want the convolution operation to reduce the size of the feature map. The feature maps will be padded with zeros such that the feature maps do not reduce size. This is in contrast to <kbd>padding='valid'</kbd>, which applies the convolutional kernel only up to the margins of the feature maps.</p>
<p>It is often a good idea to regularize trained parameters, normalize them, or do both. The latter often allows you to train easier, faster, and generalize better. Regularizers allow you to apply penalties on layer parameters during optimization. These penalties are incorporated in the loss function that the network optimizes.</p>
<p>In our case, we use the <kbd>l2</kbd> kernel regularizer, which regularizes the <strong>Euclidian</strong> norm of the convolutional kernel weights. The regularization is accomplished by adding the <img class="fm-editor-equation" src="img/36015bb2-55c9-49d9-aded-2922fb7f138e.png" style="width:2.92em;height:1.42em;"/> term to the loss function (the objective function). Here, <img class="fm-editor-equation" src="img/41764ce8-db3a-4c8c-9556-f1ba8a681902.png" style="width:0.67em;height:1.00em;"/> is a small constant and <img class="fm-editor-equation" src="img/1fafba1e-c15e-42e9-aec3-821c0ef3eb13.png" style="width:1.67em;height:1.17em;"/> is the <em>L2</em> norm, which is equal to the square root of the sum of squares of the parameters of the layer.</p>
<p>This is one of the most widely used regularization terms. Now we are ready to define our convolutional layers. The first layer is shown as follows:</p>
<pre>x = K.layers.Conv2D(256, (1, 1), **conv_opts)(base_model.output)</pre>
<p class="mce-root">Here, the first parameter is the number of output channels, which is also the number of convolutional filters. The second parameter describes the size of the convolutional filters. At first glance, it might seem that a single-pixel convolutional kernel does not make much sense as it cannot encode the contextual information of a feature map.</p>
<p class="mce-root">That is surely correct; however, in this case, it is used for a different purpose. It is a fast operation that allows encoding the depth of the input feature maps in a lower dimensionality. The depth is reduced from 1280 to <kbd>256</kbd>.</p>
<p class="mce-root">The next layer looks as follows:</p>
<pre>x = K.layers.Conv2D(256, (3, 3), strides=2, **conv_opts)(x)</pre>
<p>Here, besides the default options that we use, we use strides, which specify the number of pixels shifts over the input. In <a href="2e878463-75f1-40a5-b263-0c5aa9627328.xhtml">Chapter 1</a>, <em>Fun with Filters,</em> the convolutional operation was applied in each location, which means the filter was moved one pixel at a time and is equivalent to strides equal to one.</p>
<p>When the <kbd>strides</kbd> option is <kbd>2</kbd>, then we move the filters by two pixels at each step. The option is in plural form as we might want to have different strides in different directions, which can be done by passing a tuple of numbers. The application of a <kbd>stride</kbd> with a value greater than 1 is a means to reduce the size of the activation map without losing spatial information.</p>
<p>Of course, there are other operations that can reduce the size of the activation maps. For example, an operation called <strong>max pooling</strong> can be used, which is one of the most widely used operations in modern convolutional networks. The latter takes a small window size (for example, <em>2 x 2</em>), picks a single maximal value from that window, moves by a specified number of pixels (for example, 2), and repeats the procedure throughout the activation map. Therefore, as a result of this procedure, the size of the activation map will be reduced by a factor of 2.</p>
<p>In contrast to the approach with strides, the max-pooling operation is more suitable for tasks where we are not very interested in spatial information. Such tasks are, for example, classification tasks, in which we are not interested where an object is exactly but are simply interested in what it is. The loss of the spatial information in max pooling happens when we simply take the maximal value in a window without considering its position in the window.</p>
<p>The last thing that we want to do is to connect a dense layer of four neurons to the convolutional layer, which will be regressed to the two corner coordinates of the bounding boxes (<kbd>(x,y)</kbd> for each corner):</p>
<pre>out = K.layers.Flatten()(x)<br/>out = K.layers.Dense(4, activation="sigmoid")(out)</pre>
<p>As the coordinates of bounding boxes are normalized, it's a good idea to use an activation function, which has values in the range of <kbd>(0,1)</kbd> such as a <kbd>sigmoid</kbd> function, in our case.</p>
<p>All the required layers are ready. Now, let's define the model with the new layers and compile it for training:</p>
<pre>model = K.Model(inputs=base_model.input, outputs=out)<br/>model.compile(<br/>    loss="mean_squared_error",<br/>    optimizer="adam",<br/>    metrics=[<br/>        K.metrics.RootMeanSquaredError(),<br/>        "mae"])</pre>
<p>We use the <strong>Mean Squared Error</strong> (<strong>MSE</strong>) as a <kbd>loss</kbd> function, which is the squared difference between the ground truth and the predicted value. During training, this value will be minimized; hence, the model is supposed to predict the corner coordinates after training.</p>
<p>The regularization terms that we have added to the convolutional layers are also added to <kbd>loss</kbd> as discussed. The latter is done automatically by Keras. Also, we use the <strong>Root of MSE</strong> (<strong>RMSE</strong>) along with the <strong>Mean Absolute Error</strong> (<strong>MAE</strong>), which measures the average magnitude of the errors, as our metrics.</p>
<p>Let's now split the dataset, in the same way that we did in the previous section:</p>
<pre>inp_ds = ds.map(lambda d: (d.image,d.box))<br/>valid = inp_ds.take(1000)<br/>train = inp_ds.skip(1000).shuffle(10000)</pre>
<p>What is left to do is to train our model, just like we did in the previous section. However, before proceeding with our training, you might be interested in learning how exactly the training of our new layers is accomplished. In multilayer neural networks, training is usually done using the <strong>backpropagation</strong> <strong>algorithm</strong>, so let's first learn about that in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding backpropagation</h1>
                
            
            
                
<p>A neural network is considered to be trained when we have some optimal weights of the network so that the network makes good predictions on our data. So, the question is how do we reach these optimal weights? Neural networks are usually trained using a <strong>gradient descent</strong> algorithm. This might be either the pure gradient descent algorithm or some improved optimization method such as <strong>Adam optimizer</strong>, which is again based on computing the gradient. </p>
<p>In all of these algorithms, we need to compute the gradient of the loss function relative to all the weights. As a neural network is a complex function, it might not appear to be straightforward. This is where the backpropagation algorithm jumps in, which allows us to calculate the gradients easily in complex networks and understand what the gradient looks like. Let's dive into the details of the algorithm.</p>
<p>Suppose we have a neural network consisting of an <em>N</em> sequential layer. Generally speaking, the <em>i<sup>th</sup></em> layer in such a network is a function that can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:9.50em;height:1.58em;" class="fm-editor-equation" src="img/5fb2f53d-17c4-497c-999c-22c2343ef5ab.png"/></p>
<p>Here, <img class="fm-editor-equation" src="img/6b337984-58aa-48f5-843f-ebfdf60098d1.png" style="width:1.33em;height:1.08em;"/> is the weight of the layer and <img class="fm-editor-equation" src="img/3b4433fc-eda7-4fd8-b358-e97534e0dd42.png" style="width:2.17em;height:1.42em;"/> is the function corresponding to the previous layer.</p>
<p>We can define <img class="fm-editor-equation" src="img/0eedcb66-349e-4f28-9927-52467d336169.png" style="width:1.08em;height:1.17em;"/> to be the input of our network so that the formula holds for the complete neural network including the first layer.</p>
<p>We can also define <img class="fm-editor-equation" src="img/16351790-0066-4bbe-8664-ebab0df88d75.png" style="width:2.58em;height:1.33em;"/> to be our loss function so that the formula defines not only all the layers but also the loss function. Of course, such a generalization excludes the weight normalization term that we have already used. However, this is a simple term that just adds up to the loss and hence can be omitted for simplicity.</p>
<p>We can compute the gradient of the loss function by setting <img class="fm-editor-equation" src="img/cecf9042-e73b-4400-b08b-c297da168364.png" style="width:4.33em;height:0.92em;"/> and using the chain rule as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/577e2914-0581-45de-9448-52b995b0d006.png" style="width:19.25em;height:3.17em;"/></p>
<p>According to our definition, this formula holds not only for the loss function, but it is also general for all of the layers. In this formula, we can see that the partial derivative of a certain layer with respect to all the weight in the previous layer including the current layer is expressed in terms of the same derivative of the previous layer, which is the <img class="fm-editor-equation" src="img/207667cb-a836-4e3f-b064-8ce5f4d66ed0.png" style="width:7.67em;height:1.42em;"/> term in the formula, and terms that can be calculated using only the current layer, namely, <img class="fm-editor-equation" src="img/c9dea053-54e9-4b00-abaf-4b7c0aa70465.png" style="width:4.50em;height:1.25em;"/> and <img class="fm-editor-equation" src="img/6b7913d5-3b59-4749-9fd1-e1beec1d0088.png" style="width:3.67em;height:1.25em;"/>.</p>
<p>Using the formula, we can now numerically compute the gradient. In order to do that, we first define a variable representing an error signal and assign its initial value to one. It should be clear in a moment why it represents an error signal. Then, we start from the last layer (the loss function, in our case) and repeat the following steps until we reach the input of the network:</p>
<ol>
<li>Compute the partial derivative of the current layer with respect to its weights and multiply by the error signal. This will be the part of the gradient corresponding to the weights of the current layer.</li>
<li>Compute the partial derivative with respect to the previous layer, multiply by the error signal, and then update the error signal with the resulting value.</li>
<li>If the input of the network is not reached, move to the previous layer and repeat the steps.</li>
</ol>
<p>Once we reach the input, we have all the partial derivatives of the loss with respect to the learnable weights; therefore, we have the gradient of our loss function. Now we can note that this is the partial derivative of a layer with respect to the previous layer that propagates backward throughout the network during the gradient computation process.</p>
<p>That is a <strong>propagating signal</strong>, which influences the contribution of each layer to the gradient of the loss function. For example, if it becomes all zero somewhere during the propagation, then the contribution of all the remaining layers to the gradient will also be zero. Such a phenomenon is called a <strong>vanishing-gradient problem</strong>. This algorithm can be generalized to acyclic networks with different kinds of branches. </p>
<p>In order to train our network, all that is left to do is to update our weight in the direction of the gradient and repeat the procedure until convergence. If the pure gradient descent algorithm is used, we simply subtract the gradient multiplied by some small constant from the weights; however, usually, more advanced optimization algorithms are used such as Adam optimizer.</p>
<p>The problem with the pure gradient descent algorithm is that, first, we should find some optimal value for the small constant so that the update of the weights is neither too small, which will result in slow learning, or too large, as too large a value results in instability. Another problem is that once we have found an optimal value, we have to start to decrease it once the network starts to converge. What is more important, it's often wise to update different weights with different factors as different weights might be at different distances from their optimal values.</p>
<p>These are some of the reasons why we might want to use more advanced optimization techniques such as Adam optimizer or <strong>RMSProp</strong>, which take some or all of these mentioned issues, and even some unmentioned issues, into account. Meanwhile, while creating your networks, you should note that there is still ongoing research in the field of optimization algorithms and that one of the existing optimizers might be better in some cases than others, although the Adam optimizer should be a good choice for many tasks.</p>
<p>You might also note that in the algorithm, we did not mention exactly how the partial derivatives in a layer can be computed. Of course, they can be numerically computed by varying the values and measuring the response as is done with numerical methods for computing a derivative. The problem is that such computations would be heavy and error-prone. A better way to do it is to define a symbolic representation for each operation used and then, again, use the chain rule as in the backpropagation.</p>
<p>So, we now understand how the complete gradient is calculated. Actually, most of the modern deep learning frameworks do the differentiation for you. You usually don't need to worry about how exactly it is accomplished, but understanding the backgrounds of the computation might be very helpful if you are planning to work on new, that is, your own, models.</p>
<p>But for now, let's train our prepared model in the next section and see how it performs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training the model</h1>
                
            
            
                
<p>Before we proceed with the actual training, it is a good idea to have some means to save the model with the best weights. For that purpose, we will use a callback from Keras: </p>
<pre>checkpoint = K.callbacks.ModelCheckpoint("localization.h5",<br/>    monitor='val_root_mean_squared_error',<br/>    save_best_only=True, verbose=1)</pre>
<p>The callback will be called after each epoch of training; it will calculate the <kbd>root_mean_square_error</kbd> metric of predictions on the validation data and will save the model to <kbd>localization.h5</kbd> if the metric has improved.</p>
<p>Now, we train our model in the same way that we did with classification:</p>
<pre>model.fit(<br/>    train.batch(32),<br/>    epochs=12,<br/>    validation_data=valid.batch(1),<br/>    callbacks=[checkpoint])</pre>
<p>Here, the difference is that we train with more <kbd>epochs</kbd> this time, as well as pass our callbacks and the validation dataset.</p>
<p>During training, you will first see a gradual decrease in the loss and metrics, both on the train and validation data. After several <kbd>epochs</kbd>, you might see that the metrics on the validation data increase. The latter might be thought of as a sign of overfitting, but after more <kbd>epochs</kbd>, you might see that the metrics on <kbd>validation_data</kbd> suddenly drop. The latter phenomenon is because the model switches to a better minimum metric during the optimization process.</p>
<p>Here is the result of the lowest value of the monitored metric:</p>
<pre class="mce-root"><strong>Epoch 8/12</strong><br/><strong> 83/84 [============================&gt;.] - ETA: 0s - loss: 0.0012 - root_mean_squared_error: 0.0275 - mae: 0.0212</strong><br/><strong> Epoch 00008: val_root_mean_squared_error improved from 0.06661 to 0.06268, saving model to best_model.hdf5</strong><br/><strong> 84/84 [==============================] - 39s 465ms/step - loss: 0.0012 - root_mean_squared_error: 0.0275 - mae: 0.0212 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0627 - val_mae: 0.0454 </strong> </pre>
<p>You can note that, in this case, it was the eighth <kbd>epoch</kbd> that performed best on the validation data. You can note that the RMSE deviation on the validation data is about 6 percent. The MAE is less than 6 percent. We can interpret this result as follows—given an image from the validation data, the corner coordinates of the bounding box are usually shifted by a factor of 1/20 of the size of the image, which is not a bad result as the size of the bounding box is comparable with the size of the image.</p>
<p>You might also want to try to train the model with the frozen layers of the base models. If you do so, you will notice a far worse performance than with an unfrozen model. It will perform about twice as badly on the validation dataset according to the metrics. Given these numbers, we can conclude that the layers of the base model were able to learn on the dataset so that our model performs better on the localization task.</p>
<p>So, now that we have our model ready, let's use our inference script to see what it can do in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Seeing inference in action</h1>
                
            
            
                
<p>Once we run our inference script, it will connect to the camera and localize a box on each frame, as depicted in the following photograph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a6a24176-0086-4809-8cc9-bc69254f672d.png" style="width:28.92em;height:23.75em;"/></p>
<p>Although the model was trained on the location of heads of pets, we can see that it's quite good at localizing the head of a person. This is where you can note the power of generalization of the model.</p>
<p>When you create your own deep learning apps, you might discover that you have a lack of data for your particular application. However, if you relate your specific case to other available datasets, you might be able to find some applicable dataset that, although different, might allow you to successfully train your model. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Throughout this chapter, we have created and trained classification and localization models using the Oxford-IIIT-Pet dataset. We have learned how to create deep learning classifiers and localizers using transfer learning.</p>
<p>You have started to understand how to solve real-world problems using deep learning. You have understood how CNNs work and you know how to create a new CNN using a base model.</p>
<p>We have also covered the backpropagation algorithm for computing gradients. Understanding this algorithm will allow you to make wiser decisions on the architecture of models that you might want to build in the future.</p>
<p>In the next chapter, we will continue our deep learning journey. We will create an application that will detect and track objects with high accuracy. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dataset attribution</h1>
                
            
            
                
<p><strong>Oxford-IIIT-Pet dataset</strong>: <em>Cats and Dogs</em>, O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar in IEEE Conference on Computer Vision and Pattern Recognition, 2012. </p>


            

            
        
    </body></html>