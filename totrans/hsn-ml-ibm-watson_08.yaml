- en: Using Spark with IBM Watson Studio
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在IBM Watson Studio中使用Spark
- en: In this chapter, we will discuss **Machine Learning (ML)** pipelines and provide
    guidelines for creating and deploying a Spark machine learning pipeline within
    IBM Watson Studio.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**机器学习（ML）**管道，并提供在IBM Watson Studio中创建和部署Spark机器学习管道的指南。
- en: 'We will divide this chapter into the following areas:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章内容分为以下几部分：
- en: Introduction to Apache Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark简介
- en: Creating a Spark pipeline in Watson Studio
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Watson Studio中创建Spark管道
- en: Data preparation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: A data analysis and visualization example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析和可视化示例
- en: Introduction to Apache Spark
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark简介
- en: Before we get going on creating any kind of a pipeline, we should take a minute
    to familiarize ourselves with what Spark is and what it offers us.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始创建任何类型的管道之前，我们应该花一分钟时间熟悉Spark是什么以及它为我们提供了什么。
- en: Spark, built for both speed and ease of use, is a superfast open source engine
    that was designed with the large-scale processing of data in mind.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark，既注重速度又易于使用，是一个专为大规模数据处理而设计的超快速开源引擎。
- en: Through the advanced **Directed Acyclic Graph** (**DAG**) execution engine that
    supports **cyclic data flow** and in-memory computing, programs and scripts can
    run up to 100 times faster than Hadoop MapReduce in memory or 10 times faster
    on disk.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过支持**循环数据流**和内存计算的先进**有向无环图**（**DAG**）执行引擎，程序和脚本在内存中可以比Hadoop MapReduce快100倍，在磁盘上快10倍。
- en: 'Spark consists of the following components:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark由以下组件组成：
- en: '**Spark Core**: This is the underlying engine of Spark, utilizing the fundamental
    programming abstraction called **Resilient Distributed Datasets** (**RDDs**).
    RDDs are small logical chunks of data Spark uses as "object collections".'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Core**：这是Spark的底层引擎，利用名为**弹性分布式数据集**（**RDDs**）的基本编程抽象。RDDs是Spark用作“对象集合”的小的逻辑数据块。'
- en: '**Spark SQL**: This provides a new data abstraction called DataFrames for structured
    data processing using a distributed SQL query engine. It enables unmodified Hadoop
    Hive queries to run up to 100x faster on existing deployments and data.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：它提供了一个新的数据抽象，称为DataFrame，用于使用分布式SQL查询引擎进行结构化数据处理。它使得未经修改的Hadoop
    Hive查询在现有部署和数据上运行速度可提高至100倍。'
- en: '**MLlib**: This is Spark''s built-in library of algorithms for mining big data,
    common learning algorithms and utilities, including classification, regression,
    clustering, collaborative filtering, and dimensionality reduction, as well as
    underlying optimization primitives that best support Spark.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：这是Spark内置的算法库，用于挖掘大数据，包括常见的机器学习算法和实用工具，如分类、回归、聚类、协同过滤和降维，以及支持Spark的最佳底层优化原语。'
- en: '**Streaming**: This extends Spark''s fast scheduling capability to perform
    real-time analysis on continuous streams of new data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Streaming**：这扩展了Spark的快速调度能力，以对连续的新数据进行实时分析。'
- en: '**GraphX**: This is the graph processing framework for the analysis of graph
    structured data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：这是用于分析图结构数据的图处理框架。'
- en: Watson Studio and Spark
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Watson Studio和Spark
- en: IBM Watson Studio offers certain Spark environments that you can use as default
    Spark environment definitions to quickly get started with Spark in Watson Studio
    without having to take the time to create your own Spark environment definitions.
    This saves setup time and allows you to spend your time creating solutions rather
    than administering an environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: IBM Watson Studio提供某些Spark环境，您可以使用这些环境作为默认的Spark环境定义，以便在Watson Studio中快速开始使用Spark，而无需花费时间创建自己的Spark环境定义。这节省了设置时间，让您有更多时间创建解决方案而不是管理环境。
- en: Spark environments are available by default for all Watson Studio users. You
    don't have to provision or associate any external Spark service with your Watson
    Studio project. You simply select the hardware and software configuration of the
    Spark runtime service you need to run your tool and then when you start the tool
    with the environment definition, a runtime instance is created based on your configuration
    specifications. The Spark compute resources are dedicated to your tool alone and
    not shared with collaborators—[https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668](https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark环境默认对所有Watson Studio用户可用。您不需要为您的Watson Studio项目配置或关联任何外部Spark服务。您只需选择所需的Spark运行时服务的硬件和软件配置，然后当您使用环境定义启动工具时，将根据您的配置规范创建一个运行时实例。Spark计算资源仅用于您的工具，不与协作者共享——[https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668](https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668)。
- en: Creating a Spark-enabled notebook
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建支持Spark的笔记本
- en: 'To use Spark in Watson Studio, you need to create a notebook and associate
    a Spark version with it by performing the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Watson Studio中使用Spark，您需要创建一个笔记本，并通过执行以下步骤将其与Spark版本关联：
- en: 'The steps to create the notebook are the same as we have followed in previous
    chapters. First, from within the project, locate the Notebook section and click
    on New Notebook. On the New notebook page, provide a name and description:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建笔记本的步骤与我们之前章节中遵循的相同。首先，在项目内部找到笔记本部分，然后点击新建笔记本。在新笔记本页面，提供名称和描述：
- en: '![](img/919a523f-e088-4d39-bbd3-aa153fbaa426.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/919a523f-e088-4d39-bbd3-aa153fbaa426.png)'
- en: 'Notice that, in the preceding screenshot, Python 3.5 is the selected language—this
    is fine but then if we scroll down, we will see Spark version*. From the drop-down
    list, you can select the runtime environment for the notebook. For our example,
    we can select Default Spark Python 3.5 XS (Driver with 1 vCPU and 4GB, 2 executors
    with 1 vCPU and 4 GB RAM each):'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在上面的截图中，选中的语言是Python 3.5——这是可以的，但如果我们向下滚动，我们会看到Spark版本*。从下拉列表中，您可以选择笔记本的运行环境。在我们的例子中，我们可以选择默认的Spark
    Python 3.5 XS（驱动器有1个vCPU和4GB，2个执行器，每个执行器有1个vCPU和4GB RAM）：
- en: '![](img/7c5ad35d-f1f4-4fa0-ad1c-fbb1f9b96b07.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c5ad35d-f1f4-4fa0-ad1c-fbb1f9b96b07.png)'
- en: Once you click on Create Notebook, the notebook environment will be instanced
    and you will be ready to begin entering Spark commands.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您点击创建笔记本，笔记本环境将被实例化，您就可以开始输入Spark命令了。
- en: 'Once your Spark-enabled notebook is created, you can run Python commands and
    execute Spark jobs to process Spark SQL queries using DataFrame abstractions as
    a data source, as shown in the following example:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦创建了支持Spark的笔记本，您就可以运行Python命令并执行Spark作业，使用DataFrame抽象作为数据源来处理Spark SQL查询，如下面的示例所示：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Don't pay too much attention to the actual code in the preceding example at
    this point as, in the next sections, we will use our Spark-enabled notebook to
    create a **Spark ML pipeline**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上不要过分关注前面示例中的实际代码，因为在下一节中，我们将使用我们的支持Spark的笔记本来创建**Spark ML流水线**。
- en: Creating a Spark pipeline in Watson Studio
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Watson Studio中创建Spark流水线
- en: So, let's start by understanding just what it is that we mean when we say *pipeline*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们首先理解当我们说*流水线*时我们究竟指的是什么。
- en: What is a pipeline?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是流水线？
- en: An ML pipeline is characteristically used to **automate** ML workflows, essentially
    enabling sets of data to be transformed and correlated in a model that can then
    be tested and evaluated to achieve or estimate an outcome.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ML流水线通常用于**自动化**ML工作流程，本质上使数据集能够在模型中转换和关联，然后可以对其进行测试和评估以实现或估计结果。
- en: 'Such a workflow consists of four basic areas:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的工作流程由四个基本区域组成：
- en: Data preparation
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Training set generation
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集生成
- en: Algorithm training/evaluation/selection
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法训练/评估/选择
- en: Deployment/monitoring
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署/监控
- en: Pipeline objectives
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流水线目标
- en: 'A pipeline consists of a sequence of stages. There are two basic types of pipeline
    stages: **transformer** and **estimator**. As hinted in the *What is a pipeline?*
    section, a transformer takes a dataset as input and produces an augmented dataset
    as output, while an estimator abstracts the concept of a learning algorithm and
    implements a method that accepts data and produces a model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 管道由一系列阶段组成。管道有两个基本类型：**转换器**和**估计器**。正如在*什么是管道？*部分中暗示的，转换器接受一个数据集作为输入并产生一个增强的数据集作为输出，而估计器抽象了学习算法的概念并实现了一个接受数据和产生模型的方法。
- en: Even more simply put, a pipeline executes a workflow that can repeatedly prepare
    new data (for transformation), transform the prepared data, and then train a model
    (on the prepared data). Another way to summarize is to think of a pipeline as
    the running of a sequence of algorithms to process and learn from data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，管道执行一个可以重复准备新数据（用于转换）、转换准备好的数据，然后训练模型（在准备好的数据上）的工作流程。另一种总结方式是将管道视为运行一系列算法以处理和从数据中学习的过程。
- en: Breaking down a pipeline example
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆解管道示例
- en: We will start with stepping through the key steps in a Spark pipeline example
    available within the **I****BM Watson Studio Community** (*Use Spark and Python
    to predict equipment purchase*, submitted by Melanie Manley, July 26, 2018).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从在**IBM Watson Studio社区**（*使用Spark和Python预测设备购买*，由Melanie Manley提交，2018年7月26日）中可用的Spark管道示例的关键步骤开始：
- en: In this particular example, we see an existing Spark-enabled notebook that contains
    steps to load data, create a predictive model, and score data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，我们看到一个现有的Spark启用笔记本，其中包含加载数据、创建预测模型和评分数据的步骤。
- en: The example uses Spark commands to accomplish the tasks of loading data, performing
    data cleaning and exploration, creating a pipeline, training a model, persisting
    a model, deploying a model, and scoring a model; however, we will focus here only
    on the steps that create the Spark ML model. The reader may choose to view the
    entire example online.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 示例使用Spark命令来完成加载数据、执行数据清洗和探索、创建管道、训练模型、持久化模型、部署模型和评分模型的任务；然而，我们在这里将只关注创建Spark
    ML模型的步骤。读者可以选择在线查看整个示例。
- en: Data preparation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'In this step, the data in a DataFrame object is split (using the Spark `randomSplit`
    command) into three—a **training set** (to be used to train a model), a **testing
    set** (to be used for model evaluation and testing the assumptions of the model),
    and a **prediction set** (used for prediction) and then a record count is printed
    for each set:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，DataFrame对象中的数据（使用Spark的`randomSplit`命令）被分成三部分——一个**训练集**（用于训练模型）、一个**测试集**（用于模型评估和测试模型的假设）和一个**预测集**（用于预测），然后为每个集打印记录数：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Executing the preceding commands within the notebook is shown in the following
    screenshot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中执行前面的命令，如下面的截图所示：
- en: '![](img/0a8c51a4-5f62-4fac-a670-5757088da28f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/0a8c51a4-5f62-4fac-a670-5757088da28f.png)'
- en: The pipeline
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: 'Here, after we have created our three datasets, the Apache Spark ML pipeline
    will be created, and the model is trained by performing the following steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在我们创建了三个数据集之后，将创建Apache Spark ML管道，并通过以下步骤训练模型：
- en: 'First, you need to import the Apache Spark ML packages that will be needed
    in the subsequent steps:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要导入后续步骤中需要的Apache Spark ML包：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, the example uses the `StringIndexer` function as a transformer to convert
    all of the string columns into numeric ones:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，示例使用`StringIndexer`函数作为转换器，将所有字符串列转换为数值列：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the following step, the example creates a feature vector by combining all
    features together:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，示例通过组合所有特征创建一个特征向量：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, the estimators that you want to use for classification are defined (**random
    forest** is used):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义你想要用于分类的估计器（使用**随机森林**）：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And finally, convert the indexed labels back into original labels:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将索引后的标签转换回原始标签：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now the actual pipeline is built:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在构建实际的管道：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At this point in the example, you are ready to train the random forest model
    by using the pipeline and training data you have just built.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例的这个阶段，你准备好使用你刚刚构建的管道和训练数据来训练随机森林模型。
- en: A data analysis and visualization example
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析和可视化示例
- en: One of the most exciting advantages of using a Spark-enabled notebook within
    an IBM Watson Studio project is that all of the data explorations and subsequent
    visualizations can frequently be accomplished using just a few lines of (interactively
    written) code. In addition, the notebook interface allows a trial and error approach
    to running queries and commands, reviewing the results, and perhaps adjusting
    (the queries) and rerunning until you are satisfied (with the results).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在IBM Watson Studio项目中使用启用了Spark的笔记本最令人兴奋的优势之一是，所有数据探索和随后的可视化通常只需几行（交互式编写的）代码即可完成。此外，笔记本界面允许尝试和错误的方法来运行查询和命令，查看结果，并可能调整（查询）并重新运行，直到您对结果满意。
- en: Finally, notebooks and Spark can easily scale to deal with massive (GB and TB)
    datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，笔记本和Spark可以轻松扩展以处理大量（GB和TB）数据集。
- en: In this section, our objective is to use a Spark-enabled notebook to illustrate
    how certain tasks can be accomplished, such as loading data into the notebook,
    performing some simple data explorations, running queries (on the data), plotting,
    and then saving the results.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的目标是使用一个启用了Spark的笔记本来展示如何完成某些任务，例如将数据加载到笔记本中，进行一些简单的数据探索，运行查询（对数据进行查询），绘图，然后保存结果。
- en: Setup
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: Let's take a look at the following sections to understand the setup process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看以下部分，以了解设置过程。
- en: Getting the data
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'First things first. We need data. Rather than make some up, we''ll follow what
    several other working examples in the Watson Studio Community have done and download
    some collected data available publicly from the NOAA **National Climatic Data
    Center** (**NCDC**): [www.ncdc.noaa.gov/data-access/quick-links](http://www.ncdc.noaa.gov/data-access/quick-links).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要数据。我们不会自己编造，而是会遵循Watson Studio社区中其他几个工作示例的做法，并从NOAA国家气候数据中心（**NCDC**）公开下载一些收集到的数据：[www.ncdc.noaa.gov/data-access/quick-links](http://www.ncdc.noaa.gov/data-access/quick-links)。
- en: 'Here''s how to get the raw data from the NCDC:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何从NCDC获取原始数据的方法：
- en: From the **National Oceanic and Atmospheric Administration** (**NOAA**) site,
    click on Global Historical Climatology Network (GHCN).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**国家海洋和大气管理局**（**NOAA**）网站，点击全球历史气候网络（GHCN）。
- en: Click on GHCN-Daily FTP Access.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击GHCN-Daily FTP Access。
- en: Click on the by_year/ folder.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击by_year/文件夹。
- en: Scroll to the bottom and click on 2015.csv.gz to download the dataset.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到页面底部并点击2015.csv.gz以下载数据集。
- en: After the file has downloaded, extract it to an easily accessible location.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件下载完成后，将其提取到易于访问的位置。
- en: Loading the data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: Now we have a file of (although somewhat structured) still raw data. One typical
    first task when preparing data for analysis is to add column headings. If the
    file is of reasonable size, you can use a programmer's text editor to open and
    add a heading row, but if not, you can accomplish this directly in your Spark
    notebook.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个（尽管有些结构化）仍然是原始数据的文件。在准备数据进行分析时，一个典型的第一项任务是添加列标题。如果文件大小合理，您可以使用程序员的文本编辑器打开并添加标题行，但如果不是，您可以直接在Spark笔记本中完成此操作。
- en: 'Assuming you''ve loaded the file into your Watson project (using the process
    that we have shown in previous chapters), you can then click on Insert to code
    and then select Insert pandas DataFrame object as shown in the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经将文件加载到您的Watson项目中（使用我们在前几章中展示的过程），然后您可以点击“插入代码”，然后选择如图所示的“插入pandas DataFrame对象”：
- en: '![](img/b7b2adab-7b85-475e-8eef-9b2ec7533713.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7b2adab-7b85-475e-8eef-9b2ec7533713.png)'
- en: When you click on Insert pandas DataFrame, code is generated and added to the
    notebook for you. The generated code imports any required packages, accesses the
    data file (with the appropriate credentials), and loads the data into a DataFrame.
    You can then modify the `pd.read_csv` command (within the code) to include the
    `names` parameter (as shown in the following code).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击“插入pandas DataFrame”时，会为您生成并添加到笔记本中的代码。生成的代码导入任何所需的包，访问数据文件（使用适当的凭据），并将数据加载到DataFrame中。然后，您可以在代码中的`pd.read_csv`命令中修改`names`参数（如图所示）。
- en: 'This will create a heading row at the top of the file, using the provided column
    names:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在文件顶部创建一个标题行，使用提供的列名：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Running the code in the cell is shown in the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在单元格中运行代码的截图如下：
- en: '![](img/878e18f9-9149-4cf5-a1ef-f9a18bec8780.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/878e18f9-9149-4cf5-a1ef-f9a18bec8780.png)'
- en: 'The raw data in the base file has the format shown in the following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基础文件中的原始数据格式如图所示：
- en: '![](img/6b370e3d-fcb2-4145-8c3f-d5fa86ca9942.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b370e3d-fcb2-4145-8c3f-d5fa86ca9942.png)'
- en: Hopefully, you can see that each column contains a weather station identifier,
    a date, a metric that is collected (such as precipitation, daily maximum and minimum
    temperatures, temperature at the time of observation, snowfall, snow depth, and
    so on) and some additional columns (note that missing values may show as NaN,
    meaning *Not a Number*).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能看到，每一列都包含一个气象站标识符、一个日期、收集的指标（如降水量、日最高和最低温度、观测时的温度、降雪、积雪深度等）以及一些额外的列（注意，缺失值可能显示为
    NaN，表示 *非数字*）。
- en: Exploration
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索
- en: As we demonstrated in [Chapter 5](630e47b4-11b7-4be9-a881-8be2cb492314.xhtml),
    *Machine Learning Workouts on IBM Cloud*, there is plenty of essential functionality
    common to the `pandas` data structures to support preprocessing and analysis of
    your data. In this example though, we are going look at examples of data explorations
    again but this time using Spark DataFrame methods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 5 章](630e47b4-11b7-4be9-a881-8be2cb492314.xhtml) 中所展示的，“在 IBM Cloud
    上的机器学习练习”，`pandas` 数据结构中有很多共同的基本功能，可以支持你的数据预处理和分析。然而，在这个例子中，我们将再次查看数据探索的示例，但这次使用
    Spark DataFrame 方法。
- en: 'For example, earlier we loaded a data file using Insert pandas DataFrame; this
    time, we can reload that file using the same steps, but this time selecting Insert
    SparkSession DataFrame. The code generated will include the `import ibmos2spark`
    and `from pyspark.sql import SparkSession` commands and load the data into `SparkSession
    DataFrame` (rather than a `pandas` DataFrame):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，之前我们使用“插入 pandas DataFrame”加载了一个数据文件；这次，我们可以使用相同的步骤重新加载该文件，但这次选择“插入 SparkSession
    DataFrame”。生成的代码将包括 `import ibmos2spark` 和 `from pyspark.sql import SparkSession`
    命令，并将数据加载到 `SparkSession DataFrame`（而不是 `pandas` DataFrame）中：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running the cell initiates Spark jobs, shows a progress/status for those jobs,
    and, eventually, the output generated by the `.take(5)` command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格会启动 Spark 作业，显示这些作业的进度/状态，并最终显示由 `.take(5)` 命令生成的输出：
- en: '![](img/165781b8-c1a0-43bd-80b1-3f8d1a091f7a.png)`SparkSession` is the entry
    point to Spark SQL. It is one of the very first objects you create while developing
    a Spark SQL application. As a Spark developer, you create `SparkSession` using
    the `SparkSession.builder` method (which gives you access to the **Builder API**
    that you use to configure the session).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/165781b8-c1a0-43bd-80b1-3f8d1a091f7a.png)`SparkSession` 是 Spark SQL
    的入口点。它是你在开发 Spark SQL 应用程序时创建的第一个对象之一。作为一个 Spark 开发者，你使用 `SparkSession.builder`
    方法（它为你提供了用于配置会话的 **Builder API**）来创建 `SparkSession`。'
- en: 'Of course, we can also use `count()`, `first` as well as other statements:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们也可以使用 `count()`、`first` 以及其他语句：
- en: '![](img/1e19951a-7e73-4d01-8c19-cea09caeca9f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e19951a-7e73-4d01-8c19-cea09caeca9f.png)'
- en: 'Another interesting and handy analysis method is to show the schema of a DataFrame.
    You can use the `printSchema()` function to print out the schema for a `SparkR`
    DataFrame in a tree format, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣且实用的分析方法是通过显示 DataFrame 的模式。你可以使用 `printSchema()` 函数以树形格式打印出 `SparkR` DataFrame
    的模式，如下所示：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding command yields the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令生成了以下输出：
- en: '![](img/65a0b7cd-fdc2-440d-bf7c-606ddf996efe.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65a0b7cd-fdc2-440d-bf7c-606ddf996efe.png)'
- en: A **schema** is the description of the structure of the data. A schema is described
    using `StructType`, which is a collection of the `StructField` objects (that in
    turn are tuples of names, types, and nullability classifiers).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式**是数据结构的描述。模式使用 `StructType` 描述，它是一组 `StructField` 对象的集合（这些对象本身是包含名称、类型和可空性分类器的元组）。'
- en: 'Using a Spark DataFrame also provides you with the ability to navigate through
    the data and apply logic. For example, it''s not unreasonable or unexpected to
    want to look at the first two (or first few) rows of your data by running the
    `print` command; however, for readability, you might want to add a row of asterisks
    in between the data rows by using the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark DataFrame 也为你提供了在数据中导航和应用逻辑的能力。例如，通过运行 `print` 命令查看数据的前两行（或前几行）并不是不合理或不意外的；然而，为了可读性，你可能想通过以下代码在数据行之间添加一行星号：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code generates the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/8355e0a9-4edb-4525-bd63-21e3e543a8e2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8355e0a9-4edb-4525-bd63-21e3e543a8e2.png)'
- en: Suppose you are interested in using your SQL skills to perform your analysis?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你感兴趣的是使用你的 SQL 技能来执行你的分析？
- en: No problem! You can use **SparkSQL** with your `SparkSession` DataFrame object.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 没问题！你可以使用 **SparkSQL** 与你的 `SparkSession` DataFrame 对象一起使用。
- en: 'However, all SQL statements must be run against a table, so you need to define
    a table that acts like a **pointer** to the DataFrame (after you import the `SQLContext`
    module):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有 SQL 语句都必须在表上运行，因此你需要定义一个类似于 **指针** 的表来指向 DataFrame（在你导入 `SQLContext` 模块之后）：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Additionally, you'll need to define a new DataFrame object to hold the results
    of your SQL query and put the SQL statement inside the `sqlContext.sql()method`.
    Let's see how that works.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要定义一个新的 DataFrame 对象来保存你的 SQL 查询结果，并将 SQL 语句放在 `sqlContext.sql()` 方法中。让我们看看它是如何工作的。
- en: 'You can run the following cell to select all columns from the table we just
    created and then print information about the resulting DataFrame and schema of
    the data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行以下单元格来选择我们刚刚创建的表中的所有列，然后打印关于结果 DataFrame 和数据模式的信息：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/7ef6afc8-439e-4a1a-8bfc-7e6afed09500.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ef6afc8-439e-4a1a-8bfc-7e6afed09500.png)'
- en: Extraction
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取
- en: Now let's move on to the concept of **extraction**. The `print` command doesn't
    really show the data in a very useful format. So, instead of using our Spark DataFrame,
    we could use the `pandas` open source data analytics library to create a `pandas`
    DataFrame that shows the data in a table.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向 **提取** 的概念。`print` 命令并没有真正以非常有用的格式显示数据。因此，而不是使用我们的 Spark DataFrame，我们可以使用
    `pandas` 开源数据分析库来创建一个显示数据的 `pandas` DataFrame。
- en: Now we can look at an example that will make our SQL coders happy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看看一个会让我们的 SQL 编程者感到高兴的例子。
- en: 'Import the `pandas` library and use the `.toPandas()` method to show the SQL
    query results:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库并使用 `.toPandas()` 方法来显示 SQL 查询结果：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running the preceding commands results in the following output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令会产生以下输出：
- en: '![](img/acbb7e69-3b8a-48c8-b6b7-8199d99608b3.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbb7e69-3b8a-48c8-b6b7-8199d99608b3.png)'
- en: 'Here is another example of simple SQL query execution, this time counting the
    number of metrics recorded for each weather station and then creating a list of
    the weather stations ordered by the number of metric records for the weather station:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个简单的 SQL 查询执行示例，这次是计算每个气象站记录的指标数量，然后创建一个按气象站指标记录数量排序的气象站列表：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code gives us the following output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给出了以下输出：
- en: '![](img/61c37792-fcfd-473a-a56e-eca4c3e377be.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61c37792-fcfd-473a-a56e-eca4c3e377be.png)'
- en: You are encouraged to experiment with additional variations of SQL statements
    and then review the results in real time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励你尝试 SQL 语句的额外变体，然后实时查看结果。
- en: Plotting
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘图
- en: So, let's move along!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们继续前进！
- en: We will now take a look at plotting some of the data we collected in our Spark
    DataFrame. You can use `matplotlib` and `pandas` to create almost an endless number
    of visualizations (once you understand your data well enough).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将查看如何在 Spark DataFrame 中绘制一些我们收集的数据。你可以使用 `matplotlib` 和 `pandas` 创建几乎无限数量的可视化（一旦你足够了解你的数据）。
- en: You may even find that, once you reach this point, generating visualizations
    is quite easy but then you can spend an almost endless amount of time getting
    them clean and ready to share with others.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可能会发现，一旦达到这一点，生成可视化相当容易，但随后你可以花费几乎无限的时间来清理和准备与他人分享。
- en: We will now look at a simple example of how this process might go.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将查看这个过程的一个简单示例。
- en: 'Starting with the Spark DataFrame from the previous section, suppose that we
    think that it would be nice to generate a simple bar chart based upon the `DATE`
    field within our data. So, to get going, we can use the following code to come
    up with a count by `DATE`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中的 Spark DataFrame 开始，假设我们认为生成一个基于数据中的 `DATE` 字段的简单柱状图会很不错。因此，为了开始，我们可以使用以下代码根据
    `DATE` 来进行计数：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The results of running the preceding code are shown in the following screenshot:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面代码的结果如下截图所示：
- en: '![](img/83893c7d-6853-45b7-8fd5-5a0b748607f7.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83893c7d-6853-45b7-8fd5-5a0b748607f7.png)'
- en: 'We might say that the output generated seems somewhat reasonable (at least
    at first glance), so the next step would be to use the following code to construct
    a matrix of data formatted in a way that can easily be plotted:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会说生成的输出看起来似乎有些合理（至少第一眼看起来是这样），所以下一步将使用以下代码构建一个以易于绘制的方式格式化的数据矩阵：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Running this code and looking at the output generated seems perfectly reasonable
    and in line with our goal:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码并查看生成的输出看起来完全合理，并且符合我们的目标：
- en: '![](img/f30ebc5e-8266-4d63-a266-2ec8aff04c87.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f30ebc5e-8266-4d63-a266-2ec8aff04c87.png)'
- en: 'So, great! If we got to this point, we would think that we are ready to plot
    and visualize the data, so we can go ahead and use the following code to create
    a visualization:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，太好了！如果我们达到了这个阶段，我们会认为我们已经准备好绘制和可视化数据了，所以我们可以继续使用以下代码来创建一个可视化：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running the preceding code, we can see that the code worked (we have
    generated a visualization based upon our data) yet the output isn''t quite as
    useful as we might have hoped:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们可以看到代码是有效的（我们已经基于我们的数据生成了一个可视化），但输出并不像我们希望的那样有用：
- en: '![](img/329d03bd-fbbf-4861-8c63-5e05a0df8bf0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/329d03bd-fbbf-4861-8c63-5e05a0df8bf0.png)'
- en: It's pretty messy and not very useful!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 它相当混乱，并且不太有用！
- en: So, let's go back and try to reduce the volume of data we are trying to plot.
    Thankfully, we can reuse some of the code from the previous sections of this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们回到并尝试减少我们试图绘制的数据量。幸运的是，我们可以重用本章前面部分的一些代码。
- en: 'We can start by again setting up a temporary table that we can query:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从再次设置一个可以查询的临时表开始：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we can create a temporary DataFrame to hold our results (`temp_df`).
    The query can only load records where `METRIC` collected is `PRCP` and `VALUE`
    is greater than `500`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个临时DataFrame来保存我们的结果（`temp_df`）。查询只能加载收集到的`METRIC`为`PRCP`且`VALUE`大于`500`的记录：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This should significantly limit the number of data records to be plotted.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显著减少要绘制的数据记录数量。
- en: 'Now we can go back and rerun our codes that we used to create the data matrix
    to be plotted as well as the actual plotting code but, this time, using the temporary
    DataFrame:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以回到并重新运行我们用来创建要绘制的矩阵数据的代码以及实际的绘图代码，但这次使用临时DataFrame：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So now we have a different, maybe somewhat better, result but one that is probably
    still not ready to be shared:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了一个不同，也许稍微好一点的，但可能仍然还没有准备好分享的结果：
- en: '![](img/a196b381-1732-4c62-91de-ddba4755af39.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a196b381-1732-4c62-91de-ddba4755af39.png)'
- en: 'If we continue using the preceding strategy, we could again modify the SQL
    query to further restrict or filter the data as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续使用前面的策略，我们还可以再次修改SQL查询以进一步限制或过滤数据，如下所示：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And then we can review the resulting temporary DataFrame and see that now it
    has a lot fewer records:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以审查生成的临时DataFrame，并看到现在它的记录数量少了很多：
- en: '![](img/f2102e25-730f-443a-b89b-69db68df2976.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2102e25-730f-443a-b89b-69db68df2976.png)'
- en: 'If we now proceed with rerunning the rest of the plotting code, we see that
    it yields a slightly better (but still not acceptable) plot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在重新运行剩余的绘图代码，我们会看到它产生了一个稍微好一点的（但仍然不能接受）的图表：
- en: '![](img/8fc00e82-e2a0-447c-9e5c-c21316eb123d.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fc00e82-e2a0-447c-9e5c-c21316eb123d.png)'
- en: We could, in fact, continue this process of trial and error by modifying the
    SQL, rerunning the code, and then reviewing the latest results until we are happy
    with what we see, but you should have the general idea, so we will move on at
    this point.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以通过修改SQL，重新运行代码，然后审查最新的结果来继续这个过程，直到我们对看到的结果满意为止，但你应该已经有了大致的概念，所以我们现在继续。
- en: Saving
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存
- en: Just like when working in Microsoft Word or Microsoft Excel, it is always a
    good idea to periodically save your work.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在Microsoft Word或Microsoft Excel中工作一样，定期保存你的工作总是一个好主意。
- en: 'In fact, you may even want to save multiple versions of your work, as you continue
    to evolve it, in case you want to revert back at some point. While evolving your
    scripts, you can click on File, then Save or Save Version to keep appropriate
    copies of your notebook:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，你可能甚至想要保存你工作的多个版本，因为你继续进化它，以防你想要在某个时候回退。在进化你的脚本时，你可以点击文件，然后保存或保存版本以保留笔记本的适当副本：
- en: '![](img/ba01e425-d796-4454-bf20-5dae9a3372fc.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba01e425-d796-4454-bf20-5dae9a3372fc.png)'
- en: 'You can also save and share read-only copies of your notebooks *even outside
    of Watson Studio* so that others who aren''t collaborators in your Watson Studio
    projects can see and download them. You can do this in the following ways:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在Watson Studio之外保存和分享只读副本的笔记本，这样其他人即使不是Watson Studio项目的合作者也能查看和下载它们。您可以通过以下方式完成此操作：
- en: '**Share a URL on social media**:You can create a URL to share the last saved
    version of a notebook on social media or with people outside of Watson Studio.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在社交媒体上分享URL**：您可以在社交媒体或与Watson Studio外的人分享笔记本的最后保存版本。'
- en: '**Publish on GitHub**:To support collaboration with stakeholders and the data
    science community at large, you can publish your notebooks in GitHub repositories.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在GitHub上发布**：为了支持与利益相关者和整个数据科学社区的协作，您可以在GitHub仓库中发布您的笔记本。'
- en: '**Publish as a gist**:All project collaborators who have administrator or editor
    permission can share notebooks or parts of a notebook as gists. The latest saved
    version of your notebook is published as a gist.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布为 gist**：所有拥有管理员或编辑权限的项目协作者都可以将笔记本或笔记本的部分内容作为 gist 分享。您的笔记本的最新保存版本将作为 gist
    发布。'
- en: Downloading your notebook
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载您的笔记本
- en: 'You can download your notebook as shown in the following screenshot:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下截图所示下载您的笔记本：
- en: '![](img/f55f4c74-259e-43f8-8e97-95d2c8c74c61.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f55f4c74-259e-43f8-8e97-95d2c8c74c61.png)'
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced both Spark and how to create a Spark-enabled
    notebook in IBM Watson Studio, as well as the concept of an ML pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Spark 以及如何在 IBM Watson Studio 中创建一个 Spark 启用的笔记本，以及 ML 流程的概念。
- en: Finally, we finished up with a practical example of using Spark to perform data
    analysis and create a visualization to tell a better story about the data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过一个使用 Spark 进行数据分析并创建可视化来讲述数据更好故事的实际示例结束了这一部分。
- en: In the next chapter, we will provide an introduction to deep learning and neural
    networks on IBM Cloud.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍在 IBM Cloud 上进行深度学习和神经网络的基础知识。
