- en: Using Spark with IBM Watson Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss **Machine Learning (ML)** pipelines and provide
    guidelines for creating and deploying a Spark machine learning pipeline within
    IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will divide this chapter into the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Spark pipeline in Watson Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data analysis and visualization example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get going on creating any kind of a pipeline, we should take a minute
    to familiarize ourselves with what Spark is and what it offers us.
  prefs: []
  type: TYPE_NORMAL
- en: Spark, built for both speed and ease of use, is a superfast open source engine
    that was designed with the large-scale processing of data in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Through the advanced **Directed Acyclic Graph** (**DAG**) execution engine that
    supports **cyclic data flow** and in-memory computing, programs and scripts can
    run up to 100 times faster than Hadoop MapReduce in memory or 10 times faster
    on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Core**: This is the underlying engine of Spark, utilizing the fundamental
    programming abstraction called **Resilient Distributed Datasets** (**RDDs**).
    RDDs are small logical chunks of data Spark uses as "object collections".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark SQL**: This provides a new data abstraction called DataFrames for structured
    data processing using a distributed SQL query engine. It enables unmodified Hadoop
    Hive queries to run up to 100x faster on existing deployments and data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib**: This is Spark''s built-in library of algorithms for mining big data,
    common learning algorithms and utilities, including classification, regression,
    clustering, collaborative filtering, and dimensionality reduction, as well as
    underlying optimization primitives that best support Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming**: This extends Spark''s fast scheduling capability to perform
    real-time analysis on continuous streams of new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: This is the graph processing framework for the analysis of graph
    structured data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watson Studio and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IBM Watson Studio offers certain Spark environments that you can use as default
    Spark environment definitions to quickly get started with Spark in Watson Studio
    without having to take the time to create your own Spark environment definitions.
    This saves setup time and allows you to spend your time creating solutions rather
    than administering an environment.
  prefs: []
  type: TYPE_NORMAL
- en: Spark environments are available by default for all Watson Studio users. You
    don't have to provision or associate any external Spark service with your Watson
    Studio project. You simply select the hardware and software configuration of the
    Spark runtime service you need to run your tool and then when you start the tool
    with the environment definition, a runtime instance is created based on your configuration
    specifications. The Spark compute resources are dedicated to your tool alone and
    not shared with collaborators—[https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668](https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark-enabled notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use Spark in Watson Studio, you need to create a notebook and associate
    a Spark version with it by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to create the notebook are the same as we have followed in previous
    chapters. First, from within the project, locate the Notebook section and click
    on New Notebook. On the New notebook page, provide a name and description:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/919a523f-e088-4d39-bbd3-aa153fbaa426.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that, in the preceding screenshot, Python 3.5 is the selected language—this
    is fine but then if we scroll down, we will see Spark version*. From the drop-down
    list, you can select the runtime environment for the notebook. For our example,
    we can select Default Spark Python 3.5 XS (Driver with 1 vCPU and 4GB, 2 executors
    with 1 vCPU and 4 GB RAM each):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7c5ad35d-f1f4-4fa0-ad1c-fbb1f9b96b07.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you click on Create Notebook, the notebook environment will be instanced
    and you will be ready to begin entering Spark commands.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once your Spark-enabled notebook is created, you can run Python commands and
    execute Spark jobs to process Spark SQL queries using DataFrame abstractions as
    a data source, as shown in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Don't pay too much attention to the actual code in the preceding example at
    this point as, in the next sections, we will use our Spark-enabled notebook to
    create a **Spark ML pipeline**.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark pipeline in Watson Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's start by understanding just what it is that we mean when we say *pipeline*.
  prefs: []
  type: TYPE_NORMAL
- en: What is a pipeline?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ML pipeline is characteristically used to **automate** ML workflows, essentially
    enabling sets of data to be transformed and correlated in a model that can then
    be tested and evaluated to achieve or estimate an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a workflow consists of four basic areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training set generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm training/evaluation/selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment/monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A pipeline consists of a sequence of stages. There are two basic types of pipeline
    stages: **transformer** and **estimator**. As hinted in the *What is a pipeline?*
    section, a transformer takes a dataset as input and produces an augmented dataset
    as output, while an estimator abstracts the concept of a learning algorithm and
    implements a method that accepts data and produces a model.'
  prefs: []
  type: TYPE_NORMAL
- en: Even more simply put, a pipeline executes a workflow that can repeatedly prepare
    new data (for transformation), transform the prepared data, and then train a model
    (on the prepared data). Another way to summarize is to think of a pipeline as
    the running of a sequence of algorithms to process and learn from data.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down a pipeline example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with stepping through the key steps in a Spark pipeline example
    available within the **I****BM Watson Studio Community** (*Use Spark and Python
    to predict equipment purchase*, submitted by Melanie Manley, July 26, 2018).
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, we see an existing Spark-enabled notebook that contains
    steps to load data, create a predictive model, and score data.
  prefs: []
  type: TYPE_NORMAL
- en: The example uses Spark commands to accomplish the tasks of loading data, performing
    data cleaning and exploration, creating a pipeline, training a model, persisting
    a model, deploying a model, and scoring a model; however, we will focus here only
    on the steps that create the Spark ML model. The reader may choose to view the
    entire example online.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, the data in a DataFrame object is split (using the Spark `randomSplit`
    command) into three—a **training set** (to be used to train a model), a **testing
    set** (to be used for model evaluation and testing the assumptions of the model),
    and a **prediction set** (used for prediction) and then a record count is printed
    for each set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding commands within the notebook is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a8c51a4-5f62-4fac-a670-5757088da28f.png)'
  prefs: []
  type: TYPE_IMG
- en: The pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, after we have created our three datasets, the Apache Spark ML pipeline
    will be created, and the model is trained by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to import the Apache Spark ML packages that will be needed
    in the subsequent steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the example uses the `StringIndexer` function as a transformer to convert
    all of the string columns into numeric ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following step, the example creates a feature vector by combining all
    features together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the estimators that you want to use for classification are defined (**random
    forest** is used):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, convert the indexed labels back into original labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the actual pipeline is built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: At this point in the example, you are ready to train the random forest model
    by using the pipeline and training data you have just built.
  prefs: []
  type: TYPE_NORMAL
- en: A data analysis and visualization example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most exciting advantages of using a Spark-enabled notebook within
    an IBM Watson Studio project is that all of the data explorations and subsequent
    visualizations can frequently be accomplished using just a few lines of (interactively
    written) code. In addition, the notebook interface allows a trial and error approach
    to running queries and commands, reviewing the results, and perhaps adjusting
    (the queries) and rerunning until you are satisfied (with the results).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, notebooks and Spark can easily scale to deal with massive (GB and TB)
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, our objective is to use a Spark-enabled notebook to illustrate
    how certain tasks can be accomplished, such as loading data into the notebook,
    performing some simple data explorations, running queries (on the data), plotting,
    and then saving the results.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at the following sections to understand the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First things first. We need data. Rather than make some up, we''ll follow what
    several other working examples in the Watson Studio Community have done and download
    some collected data available publicly from the NOAA **National Climatic Data
    Center** (**NCDC**): [www.ncdc.noaa.gov/data-access/quick-links](http://www.ncdc.noaa.gov/data-access/quick-links).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to get the raw data from the NCDC:'
  prefs: []
  type: TYPE_NORMAL
- en: From the **National Oceanic and Atmospheric Administration** (**NOAA**) site,
    click on Global Historical Climatology Network (GHCN).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on GHCN-Daily FTP Access.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the by_year/ folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the bottom and click on 2015.csv.gz to download the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the file has downloaded, extract it to an easily accessible location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have a file of (although somewhat structured) still raw data. One typical
    first task when preparing data for analysis is to add column headings. If the
    file is of reasonable size, you can use a programmer's text editor to open and
    add a heading row, but if not, you can accomplish this directly in your Spark
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming you''ve loaded the file into your Watson project (using the process
    that we have shown in previous chapters), you can then click on Insert to code
    and then select Insert pandas DataFrame object as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7b2adab-7b85-475e-8eef-9b2ec7533713.png)'
  prefs: []
  type: TYPE_IMG
- en: When you click on Insert pandas DataFrame, code is generated and added to the
    notebook for you. The generated code imports any required packages, accesses the
    data file (with the appropriate credentials), and loads the data into a DataFrame.
    You can then modify the `pd.read_csv` command (within the code) to include the
    `names` parameter (as shown in the following code).
  prefs: []
  type: TYPE_NORMAL
- en: 'This will create a heading row at the top of the file, using the provided column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code in the cell is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/878e18f9-9149-4cf5-a1ef-f9a18bec8780.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The raw data in the base file has the format shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b370e3d-fcb2-4145-8c3f-d5fa86ca9942.png)'
  prefs: []
  type: TYPE_IMG
- en: Hopefully, you can see that each column contains a weather station identifier,
    a date, a metric that is collected (such as precipitation, daily maximum and minimum
    temperatures, temperature at the time of observation, snowfall, snow depth, and
    so on) and some additional columns (note that missing values may show as NaN,
    meaning *Not a Number*).
  prefs: []
  type: TYPE_NORMAL
- en: Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we demonstrated in [Chapter 5](630e47b4-11b7-4be9-a881-8be2cb492314.xhtml),
    *Machine Learning Workouts on IBM Cloud*, there is plenty of essential functionality
    common to the `pandas` data structures to support preprocessing and analysis of
    your data. In this example though, we are going look at examples of data explorations
    again but this time using Spark DataFrame methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, earlier we loaded a data file using Insert pandas DataFrame; this
    time, we can reload that file using the same steps, but this time selecting Insert
    SparkSession DataFrame. The code generated will include the `import ibmos2spark`
    and `from pyspark.sql import SparkSession` commands and load the data into `SparkSession
    DataFrame` (rather than a `pandas` DataFrame):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the cell initiates Spark jobs, shows a progress/status for those jobs,
    and, eventually, the output generated by the `.take(5)` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/165781b8-c1a0-43bd-80b1-3f8d1a091f7a.png)`SparkSession` is the entry
    point to Spark SQL. It is one of the very first objects you create while developing
    a Spark SQL application. As a Spark developer, you create `SparkSession` using
    the `SparkSession.builder` method (which gives you access to the **Builder API**
    that you use to configure the session).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we can also use `count()`, `first` as well as other statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e19951a-7e73-4d01-8c19-cea09caeca9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another interesting and handy analysis method is to show the schema of a DataFrame.
    You can use the `printSchema()` function to print out the schema for a `SparkR`
    DataFrame in a tree format, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65a0b7cd-fdc2-440d-bf7c-606ddf996efe.png)'
  prefs: []
  type: TYPE_IMG
- en: A **schema** is the description of the structure of the data. A schema is described
    using `StructType`, which is a collection of the `StructField` objects (that in
    turn are tuples of names, types, and nullability classifiers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a Spark DataFrame also provides you with the ability to navigate through
    the data and apply logic. For example, it''s not unreasonable or unexpected to
    want to look at the first two (or first few) rows of your data by running the
    `print` command; however, for readability, you might want to add a row of asterisks
    in between the data rows by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8355e0a9-4edb-4525-bd63-21e3e543a8e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose you are interested in using your SQL skills to perform your analysis?
  prefs: []
  type: TYPE_NORMAL
- en: No problem! You can use **SparkSQL** with your `SparkSession` DataFrame object.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, all SQL statements must be run against a table, so you need to define
    a table that acts like a **pointer** to the DataFrame (after you import the `SQLContext`
    module):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, you'll need to define a new DataFrame object to hold the results
    of your SQL query and put the SQL statement inside the `sqlContext.sql()method`.
    Let's see how that works.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run the following cell to select all columns from the table we just
    created and then print information about the resulting DataFrame and schema of
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ef6afc8-439e-4a1a-8bfc-7e6afed09500.png)'
  prefs: []
  type: TYPE_IMG
- en: Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's move on to the concept of **extraction**. The `print` command doesn't
    really show the data in a very useful format. So, instead of using our Spark DataFrame,
    we could use the `pandas` open source data analytics library to create a `pandas`
    DataFrame that shows the data in a table.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at an example that will make our SQL coders happy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library and use the `.toPandas()` method to show the SQL
    query results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding commands results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acbb7e69-3b8a-48c8-b6b7-8199d99608b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is another example of simple SQL query execution, this time counting the
    number of metrics recorded for each weather station and then creating a list of
    the weather stations ordered by the number of metric records for the weather station:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61c37792-fcfd-473a-a56e-eca4c3e377be.png)'
  prefs: []
  type: TYPE_IMG
- en: You are encouraged to experiment with additional variations of SQL statements
    and then review the results in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's move along!
  prefs: []
  type: TYPE_NORMAL
- en: We will now take a look at plotting some of the data we collected in our Spark
    DataFrame. You can use `matplotlib` and `pandas` to create almost an endless number
    of visualizations (once you understand your data well enough).
  prefs: []
  type: TYPE_NORMAL
- en: You may even find that, once you reach this point, generating visualizations
    is quite easy but then you can spend an almost endless amount of time getting
    them clean and ready to share with others.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at a simple example of how this process might go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the Spark DataFrame from the previous section, suppose that we
    think that it would be nice to generate a simple bar chart based upon the `DATE`
    field within our data. So, to get going, we can use the following code to come
    up with a count by `DATE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of running the preceding code are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83893c7d-6853-45b7-8fd5-5a0b748607f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We might say that the output generated seems somewhat reasonable (at least
    at first glance), so the next step would be to use the following code to construct
    a matrix of data formatted in a way that can easily be plotted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code and looking at the output generated seems perfectly reasonable
    and in line with our goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f30ebc5e-8266-4d63-a266-2ec8aff04c87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, great! If we got to this point, we would think that we are ready to plot
    and visualize the data, so we can go ahead and use the following code to create
    a visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we can see that the code worked (we have
    generated a visualization based upon our data) yet the output isn''t quite as
    useful as we might have hoped:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/329d03bd-fbbf-4861-8c63-5e05a0df8bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: It's pretty messy and not very useful!
  prefs: []
  type: TYPE_NORMAL
- en: So, let's go back and try to reduce the volume of data we are trying to plot.
    Thankfully, we can reuse some of the code from the previous sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by again setting up a temporary table that we can query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create a temporary DataFrame to hold our results (`temp_df`).
    The query can only load records where `METRIC` collected is `PRCP` and `VALUE`
    is greater than `500`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This should significantly limit the number of data records to be plotted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can go back and rerun our codes that we used to create the data matrix
    to be plotted as well as the actual plotting code but, this time, using the temporary
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So now we have a different, maybe somewhat better, result but one that is probably
    still not ready to be shared:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a196b381-1732-4c62-91de-ddba4755af39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we continue using the preceding strategy, we could again modify the SQL
    query to further restrict or filter the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can review the resulting temporary DataFrame and see that now it
    has a lot fewer records:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2102e25-730f-443a-b89b-69db68df2976.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we now proceed with rerunning the rest of the plotting code, we see that
    it yields a slightly better (but still not acceptable) plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc00e82-e2a0-447c-9e5c-c21316eb123d.png)'
  prefs: []
  type: TYPE_IMG
- en: We could, in fact, continue this process of trial and error by modifying the
    SQL, rerunning the code, and then reviewing the latest results until we are happy
    with what we see, but you should have the general idea, so we will move on at
    this point.
  prefs: []
  type: TYPE_NORMAL
- en: Saving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like when working in Microsoft Word or Microsoft Excel, it is always a
    good idea to periodically save your work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, you may even want to save multiple versions of your work, as you continue
    to evolve it, in case you want to revert back at some point. While evolving your
    scripts, you can click on File, then Save or Save Version to keep appropriate
    copies of your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba01e425-d796-4454-bf20-5dae9a3372fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also save and share read-only copies of your notebooks *even outside
    of Watson Studio* so that others who aren''t collaborators in your Watson Studio
    projects can see and download them. You can do this in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Share a URL on social media**:You can create a URL to share the last saved
    version of a notebook on social media or with people outside of Watson Studio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publish on GitHub**:To support collaboration with stakeholders and the data
    science community at large, you can publish your notebooks in GitHub repositories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publish as a gist**:All project collaborators who have administrator or editor
    permission can share notebooks or parts of a notebook as gists. The latest saved
    version of your notebook is published as a gist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading your notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download your notebook as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f55f4c74-259e-43f8-8e97-95d2c8c74c61.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced both Spark and how to create a Spark-enabled
    notebook in IBM Watson Studio, as well as the concept of an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we finished up with a practical example of using Spark to perform data
    analysis and create a visualization to tell a better story about the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will provide an introduction to deep learning and neural
    networks on IBM Cloud.
  prefs: []
  type: TYPE_NORMAL
