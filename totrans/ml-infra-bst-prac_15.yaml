- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Machine Learning Pipelines (MLOps) and Their Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps, short for machine learning (ML) operations, is a set of practices and
    techniques aimed at streamlining the deployment, management, and monitoring of
    ML models in production environments. It borrows concepts from the DevOps (development
    and operations) approach, adapting them to the unique challenges posed by ML.
  prefs: []
  type: TYPE_NORMAL
- en: The main goal of MLOps is to bridge the gap between data science and operations
    teams, fostering collaboration and ensuring that ML projects can be effectively
    and reliably deployed at scale. MLOps helps to automate and optimize the entire
    ML life cycle, from model development to deployment and maintenance, thus improving
    the efficiency and effectiveness of ML systems in production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learn how ML systems are designed and operated in practice.
    The chapter shows how pipelines are turned into a software system, with a focus
    on testing ML pipelines and their deployment at Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What ML pipelines are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines – how to use ML in the system in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw data-based pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing of ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring ML systems at runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What ML pipelines are
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Undoubtedly, in recent years, the field of ML has witnessed remarkable advancements,
    revolutionizing industries and empowering innovative applications. As the demand
    for more sophisticated and accurate models grows, so does the complexity of developing
    and deploying them effectively. The industrial introduction of ML systems called
    for more rigorous testing and validation of these ML-based systems. In response
    to these challenges, the concept of ML pipelines has emerged as a crucial framework
    to streamline the entire ML development process, from data preprocessing and feature
    engineering to model training and deployment. This chapter explores the applications
    of MLOps in the context of both cutting-edge **deep learning** (**DL**) models
    such as **Generative Pre-trained Transformer** (**GPT**) and traditional classical
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by exploring the underlying concepts of ML pipelines, stressing their
    importance in organizing the ML workflow and promoting collaboration between data
    scientists and engineers. We synthesize a lot of knowledge presented in the previous
    chapters – data quality assessment, model inference, and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discuss the unique characteristics and considerations involved in building
    pipelines for GPT models and similar, leveraging their pre-trained nature to tackle
    a wide range of language tasks. We explore the intricacies of fine-tuning GPT
    models on domain-specific data and the challenges of incorporating them into production
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Following the exploration of GPT pipelines, we shift our focus to classical
    ML models, examining the feature engineering process and its role in extracting
    relevant information from raw data. We delve into the diverse landscape of traditional
    ML algorithms, understanding when to use each approach, and their trade-offs in
    different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we show how to test ML pipelines, and we emphasize the significance
    of model evaluation and validation in assessing performance and ensuring robustness
    in production environments. Additionally, we examine strategies for model monitoring
    and maintenance, safeguarding against concept drift and guaranteeing continuous
    performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ML pipeline is a systematic and automated process that organizes the various
    stages of an ML workflow. It encompasses the steps involved in preparing data,
    training an ML model, evaluating its performance, and deploying it for use in
    real-world applications. The primary goal of an ML pipeline is to streamline the
    end-to-end ML process, making it more efficient, reproducible, and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML pipeline typically consists of the following essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection, preprocessing, and wrangling**: In this initial stage, relevant
    data is gathered from various sources and prepared for model training. Data preprocessing
    involves cleaning, transforming, and normalizing the data to ensure it is in a
    suitable format for the ML algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering and selection**: Feature engineering involves selecting
    and creating relevant features (input variables) from the raw data that will help
    the model learn patterns and make accurate predictions. Proper feature selection
    is crucial in improving model performance and reducing computational overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection and training**: In this stage, one or more ML algorithms
    are chosen, and the model is trained on the prepared data. Model training involves
    learning underlying patterns and relationships in the data to make predictions
    or classifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation and validation**: The trained model is evaluated using metrics
    such as accuracy, precision, recall, F1-score, and so on, to assess its performance
    on unseen data. Cross-validation techniques are often used to ensure the model’s
    generalization capability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: Many ML algorithms have hyperparameters, which are
    adjustable parameters that control the model’s behavior. Hyperparameter tuning
    involves finding the optimal values for these parameters to improve the model’s
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once the model has been trained and validated, it is
    deployed into a production environment, where it can make predictions on new,
    unseen data. Model deployment may involve integrating the model into existing
    applications or systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring and maintenance**: After deployment, the model’s performance
    is continuously monitored to detect any issues or drift in performance. Regular
    maintenance may involve retraining the model with new data to ensure it remains
    accurate and up to date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An ML pipeline provides a structured framework for managing the complexity
    of ML projects, enabling data scientists and engineers to collaborate more effectively
    and ensuring that models can be developed and deployed reliably and efficiently.
    It promotes reproducibility, scalability, and ease of experimentation, facilitating
    the development of high-quality ML solutions. *Figure 12**.1* shows a conceptual
    model of an ML pipeline, which we introduced in [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – ML pipeline: a conceptual overview](img/B19548_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1 – ML pipeline: a conceptual overview'
  prefs: []
  type: TYPE_NORMAL
- en: We covered the elements of the blue-shaded elements in the previous chapters,
    and here, we focus mostly on the parts that are not covered yet. However, before
    we dive into the technical elements of this pipeline, let us introduce the concept
    of MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the main goal of MLOps is to bridge the gap between data science and operations
    teams, MLOps automates and optimizes the entire ML life cycle, from model development
    to deployment and maintenance, thus improving the efficiency and effectiveness
    of ML systems in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key components and practices in MLOps include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version control**: Applying **version control systems** (**VCSs**) such as
    Git to manage and track changes in ML code, datasets, and model versions. This
    enables easy collaboration, reproducibility, and tracking of model improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous integration and continuous deployment (CI/CD)**: Leveraging CI/CD
    pipelines to automate the testing, integration, and deployment of ML models. This
    helps ensure that changes to the code base are seamlessly deployed to production
    while maintaining high-quality standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packaging**: Creating standardized, reproducible, and shareable containers
    or packages for ML models, making it easier to deploy them across different environments
    consistently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Implementing monitoring and logging solutions to keep
    track of the model’s performance and behavior in real time. This helps detect
    issues early and ensure the model’s ongoing reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and infrastructure management**: Designing and managing the underlying
    infrastructure to support the demands of the ML models in production, ensuring
    they can handle increased workloads and scale efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model governance and compliance**: Implementing processes and tools to ensure
    compliance with legal and ethical requirements, privacy regulations, and company
    policies when deploying and using ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration and communication**: Facilitating effective communication and
    collaboration between data scientists, engineers, and other stakeholders involved
    in the ML deployment process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adopting MLOps principles, organizations can accelerate the development and
    deployment of ML models while maintaining their reliability and effectiveness
    in real-world applications. It also helps reduce the risk of deployment failures
    and promotes a culture of collaboration and continuous improvement within data
    science and operations teams.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipelines – how to use ML in the system in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and validating ML models on a local platform is the beginning of the
    process of using an ML pipeline. After all, it would be of limited use if we had
    to retrain the ML models on every computer from our customers.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we often deploy ML models to a model repository. There are a few
    popular ones, but the one that is used by the largest community is the HuggingFace
    repository. In that repository, we can deploy both the models and datasets and
    even create spaces where the models can be used for experiments without the need
    to download them. Let us deploy the model trained in [*Chapter 11*](B19548_11.xhtml#_idTextAnchor132)
    to that repository. For that, we need to have an account at huggingface.com, and
    then we can start.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models to HuggingFace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to create a new model using the **New** button on the main page,
    as in *Figure 12**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – New button to create a model](img/B19548_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – New button to create a model
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we fill in information about our model to create space for it. *Figure
    12**.3* presents a screenshot of this process. In the form, we fill in the name
    of the model, whether it should be private or public, and we choose a license
    for it. In this example, we go with the MIT License, which is very permissive
    and allows everyone to use, reuse, and redistribute the model as long as they
    include the MIT License text along with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Model metadata card](img/B19548_12_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Model metadata card
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been created, we get a space where we can start deploying
    the model. The empty space looks like the one in *Figure 12**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Empty model space](img/B19548_12_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Empty model space
  prefs: []
  type: TYPE_NORMAL
- en: 'The top menu contains four options, but the first two are the most important
    ones – **Model card** and **Files and versions**. The model card is a short description
    of the model. It can contain any kind of information, but the most common information
    is how to use the model. We follow this convention and prepare the model card
    as shown in *Figure 12**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – The beginning of the model card for our wolfBERTa model](img/B19548_12_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – The beginning of the model card for our wolfBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #60'
  prefs: []
  type: TYPE_NORMAL
- en: The model card should contain information about how the model was trained, how
    to use it, which tasks it supports, and how to reference the model.
  prefs: []
  type: TYPE_NORMAL
- en: Since HuggingFace is a community, it is important to properly document models
    created and provide information on how the models were trained and what they can
    do. Therefore, my best practice is to include all that information in the model
    card. Many models include also information about how to contact the authors and
    whether the models had been pre-trained before they were trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model card is ready, we can move to the `Readme.txt` – the model card),
    and we can add actual model files (see *Figure 12**.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Files and versions of models; we can add a model by using the
    Add file button in the top right-hand corner](img/B19548_12_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Files and versions of models; we can add a model by using the
    Add file button in the top right-hand corner
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we click on the `wolfBERTa` subfolder. That folder contains the following
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first two entries are the model checkpoints; that is, the versions of the
    model saved during our training process. These two folders are not important for
    the deployment, and therefore they will be ignored. The rest of the files should
    be copied to the newly created model repository at HuggingFace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model, after uploading, should look something like the one presented in
    *Figure 12**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Model uploaded to the HuggingFace repository](img/B19548_12_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Model uploaded to the HuggingFace repository
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, the model is ready to be used by the community. What we can also
    do is create an inference API for the community to quickly test our models. It
    is provided to us automatically once we go back to the **Model card** menu, under
    the **Hosted inference API** section (right-hand side of *Figure 12**.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Hosted inference API provided automatically for our model](img/B19548_12_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Hosted inference API provided automatically for our model
  prefs: []
  type: TYPE_NORMAL
- en: When we input `int HTTP_get(<mask>)`, we ask the model to provide the input
    parameter for that function. The results show that the most probable token is
    `void` and the second in line is the `int` token. Both are relevant as they are
    types used in parameters, but they are probably not going to make this program
    compile, so we would need to develop a loop that would predict more than just
    one token for the program. It probably needs a bit more training as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a fully deployed model that can be used in other applications without
    much hassle.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading models from HuggingFace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already seen how to download a model from HuggingFace, but for the
    sake of completeness, let’s see how this is done for the `wolfBERTa` model. Essentially,
    we follow the model card and use the following Python code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment downloads the model and uses an `unmasker` interface to
    make an inference using the `fill-mask` pipeline. The pipeline allows you to input
    a sentence with a `<mask>` masked token, and the model will attempt to predict
    the most suitable word to fill in the masked position. The three lines of this
    code fragment do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from transformers import pipeline`: This line imports the pipeline function
    from the `transformer`s library. The pipeline function simplifies the process
    of using pre-trained models for various **natural language processing** (**NLP**)
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unmasker = pipeline(''fill-mask'', model=''mstaron/wolfBERTa'')`: This line
    creates a new pipeline named `unmasker` for the task. The pipeline will use the
    pre-trained `wolfBERTa` model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unmasker("Hello I''m a <mask> model.")`: This line utilizes the `unmasker`
    pipeline to predict the word that best fits the masked position in the given sentence.
    The `<mask>` token indicates the position where the model should try to fill in
    a word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When this line is executed, the pipeline will call the `wolfBERTa` model, which
    will make predictions based on the provided sentence. The model will predict the
    word that it finds to best complete the sentence in the position of the `<``mask>`
    token.
  prefs: []
  type: TYPE_NORMAL
- en: One can use other models in a very similar way. The main advantage of a community
    model hub such as HuggingFace is that it provides a great way to uniformly manage
    models and pipelines and allows us to quickly exchange models in software products.
  prefs: []
  type: TYPE_NORMAL
- en: Raw data-based pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a full pipeline can be a daunting task and requires creating customized
    tools for all models and all kinds of data. It allows us to optimize how we use
    the models, but it requires a lot of effort. The main rationale behind pipelines
    is that they link two areas of ML – the model and its computational capabilities
    with the task and the data from the domain. Luckily for us, the main model hubs
    such as HuggingFace have an API that provides ML pipelines automatically. Pipelines
    in HuggingFace are related to the model and provided by the framework based on
    the model’s architecture, input, and output.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines for NLP-related tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text classification is a pipeline designed to classify text input into predefined
    categories or classes. It’s particularly useful for tasks such as **sentiment
    analysis** (**SA**), topic categorization, spam detection, intent recognition,
    and so on. The pipeline typically employs pre-trained models fine-tuned on specific
    datasets for different classification tasks. We have seen similar capabilities
    in *Part I* of this book when we used ML for SA of code reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is presented in the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code fragment shows that there are essentially two lines of code (in boldface)
    that we need to instantiate the pipeline, as we’ve also seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation is another pipeline that allows the generating of text using
    pre-trained language models, such as GPT-3, based on a provided prompt or seed
    text. It’s capable of generating human-like text for various applications, such
    as chatbots, creative writing, **question answering** (**QA**), and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is presented in the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Summarization is a pipeline designed to summarize longer texts into shorter,
    coherent summaries. It utilizes transformer-based models that have been trained
    on large datasets with a focus on the summarization task. The pipeline is exemplified
    in the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are more pipelines in the HuggingFace `transformers` API, so I encourage
    you to take a look at these pipelines. However, my best practice related to pipelines
    is this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #61'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with different models to find the best pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Since the API provides the same pipeline for similar models, changing the model
    or its version is quite simple. Therefore, we can create a product based on a
    model that has similar (but not the same) capabilities as the model that we use
    and simultaneously train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines for images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines for image processing are designed specifically for tasks related to
    image processing. The HuggingFace hub contains several of these pipelines, with
    the following ones being the most popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image classification is designed specifically to classify an image to a specific
    class. It is the same kind of task as is probably the most widely known – classifying
    an image to be “cat”, “dog”, or “car”. The following code example (from the HuggingFace
    tutorial) shows the usage of an image classification pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code fragment shows that an image classification pipeline is created
    equally easily (if not easier) as pipelines for text analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An image segmentation pipeline is used when we want to add a so-called semantic
    map to an image (see *Figure 12**.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Semantic map of an image, the same as we saw in Chapter 3](img/B19548_12_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Semantic map of an image, the same as we saw in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038)
  prefs: []
  type: TYPE_NORMAL
- en: 'An example code fragment that contains such a pipeline is presented next (again,
    from the HuggingFace tutorial):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code fragment creates an image segmentation pipeline, uses it,
    and stores the results in a `segments` list. The last line of the list prints
    the label of the first segment. Using the `segments[0]["mask"].size` statement,
    we can receive the size of the image map in pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'An object detection pipeline is used for tasks that require the recognition
    of objects of a predefined class in the image. We have seen an example of this
    task in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038) already. The code for
    this kind of pipeline looks very similar to the previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Executing this code creates a list of bounding boxes of objects detected in
    the image, together with its bounding boxes. My best practices related to the
    use of pipelines for images are the same as for language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature-based pipelines do not have specific classes because they are much lower
    level. They are the `model.fit()` and `model.predict()` statements from the standard
    Python ML implementation. These pipelines require software developers to prepare
    the data manually and also to take care of the results manually; that is, by implementing
    preprocessing steps such as converting data to tables using one-hot encoding and
    post-processing steps such as converting the data into a human-readable output.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this kind of pipeline was the prediction of defects that we have
    seen in the previous parts of the book; therefore, they do not need to be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: What is important, however, is that all pipelines are the way that link the
    ML domain with the software engineering domain. The first activity that I do after
    developing a pipeline is to test it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing of ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing of ML pipelines is done at multiple levels, starting with unit tests
    and moving up toward integration (component) tests and then to system and acceptance
    tests. In these tests, two elements are important – the model itself and the data
    (for the model and the oracle).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we can use the unit test framework included in Python, I strongly
    recommend using the Pytest framework instead, due to its simplicity and flexibility.
    We can install this framework by simply using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: That will download and install the required packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #62'
  prefs: []
  type: TYPE_NORMAL
- en: Use a professional testing framework such as Pytest.
  prefs: []
  type: TYPE_NORMAL
- en: Using a professional framework provides us with the compatibility required by
    MLOps principles. We can share our models, data, source code, and all other elements
    without the need for cumbersome setup and installation of the frameworks themselves.
    For Python, I recommend using the Pytest framework as it is well known, widely
    used, and supported by a large community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a code fragment that downloads a model and prepares it for being tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet is used to load and set up a pre-trained language model,
    specifically the `SingBERTa` model, using the Hugging Face `transformers` library.
    It contains the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules from the `transformers` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AutoTokenizer`: This class is used to automatically select the appropriate
    tokenizer for the pre-trained model.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AutoModelForMaskedLM`: This class is used to automatically select the appropriate
    model for **masked language modeling** (**MLM**) tasks.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the tokenizer and model for the pre-trained `SingBERTa` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tokenizer = AutoTokenizer.from_pretrained(''mstaron/SingBERTa'')`: This line
    loads the tokenizer for the pre-trained `SingBERTa` model from the Hugging Face
    model hub.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`model = AutoModelForMaskedLM.from_pretrained("mstaron/SingBERTa")`: This line
    loads the pre-trained `SingBERTa` model.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the feature extraction pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`from transformers import pipeline`: This line imports the pipeline class from
    the `transformers` library, which allows us to easily create pipelines for various
    NLP tasks.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a feature extraction pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`features = pipeline("feature-extraction", model=model, tokenizer=tokenizer,
    return_tensor=False)`: This line creates a pipeline for feature extraction. The
    pipeline uses the pre-trained model and tokenizer loaded earlier to extract embedding
    vectors from the input text. The `return_tensor=False` argument ensures that the
    output will be in a non-tensor format (likely NumPy arrays or Python lists).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With this setup, you can now use the `features` pipeline to extract embedding
    vectors from text input using the pre-trained `SingBERTa` model without the need
    for any additional training. We’ve seen this model being used before, so here,
    let us focus on its testing. The following code fragment is a test case to check
    that the model has been downloaded correctly and that it is ready to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment defines a `test_features()` test function. The purpose of
    this function is to test the correctness of the feature extraction pipeline created
    in the previous code snippet by comparing the embeddings of the word `"Test"`
    obtained from the pipeline to the expected embeddings stored in a JSON file named
    `''test.json''`. The content of that file is our oracle, and it is a large vector
    of numbers that we use to compare to the actual model output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lstFeatures = features("Test")`: This line uses the previously defined `features`
    pipeline to extract embeddings for the word `"Test"`. The `features` pipeline
    was created using the pre-trained `SingBERTa` model and tokenizer. The pipeline
    takes the input `"Test"`, processes it through the tokenizer, passes it through
    the model, and returns embedding vectors as `lstFeatures`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with open(''test.json'', ''r'') as f:`: This line opens the `''test.json''`
    file in read mode using a context manager (`with` statement).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstEmbeddings = json.load(f)`: This line reads the contents of the `''test.json''`
    file and loads its content into the `lstEmbeddings` variable. The JSON file should
    contain a list of embedding vectors representing the expected embeddings for the
    word `"Test"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assert lstFeatures[0][0] == lstEmbeddings`: This line performs an assertion
    to check if the embedding vector obtained from the pipeline (`lstFeatures[0][0]`)
    is equal to the expected embedding vector (oracle) from the JSON file (`lstEmbeddings`).
    A comparison is made by checking whether the elements at the same position in
    both lists are the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the assertion is `true` (that is, the pipeline’s extracted embedding vector
    is the same as the expected vector from the JSON file), the test will pass without
    any output. However, if the assertion is `false` (that is, the embeddings do not
    match), the test framework (Pytest) marks this test case as failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to execute the tests, we can write the following statement in the
    same directory as our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this results in the following output (redacted for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This fragment shows that the framework found one test case (`collected 1 item`)
    and that it executed it. It also says that the test case passed in 4.17 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, here comes my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #63'
  prefs: []
  type: TYPE_NORMAL
- en: Set up your test infrastructure based on your training data.
  prefs: []
  type: TYPE_NORMAL
- en: Since the models are inherently probabilistic, it is best to test the models
    based on the training data. Here, I do not mean that we test the performance in
    the sense of ML, like accuracy. I mean that we test that the models actually work.
    By using the same data as we used for the training, we can check whether the models’
    inference is correct for the data that we used before. Therefore, I mean this
    as testing in the software engineering sense of this term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, analogous to the language models presented previously, we can use a similar
    approach to test a classical ML model. It’s sometimes called a zero-table test.
    In this test, we use simple data with one data point only to test that the model’s
    predictions are correct. Here is how we set up such a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This fragment of the code uses the `joblib` library to load an ML model. In
    this case, it is a model that we used in [*Chapter 10*](B19548_10.xhtml#_idTextAnchor121)
    when we trained a classical ML model. It is a decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the program reads the same dataset that we used for training the model
    so that the format of the data is exactly the same. In this case, we can expect
    the same results that we used for the training dataset. For more complex models,
    we can create such a table by making one inference directly after the model has
    been trained and before it was saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define three test cases in the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first test function (`test_model_not_null`) checks if the `model` variable,
    which is expected to hold the trained ML model, is not `null`. If the model is
    `null` (that is, it does not exist), the `assert` statement will raise an exception,
    indicating that the test has failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second test function (`test_model_predicts_class_correctly`) checks whether
    the model predicts class 1 correctly for the given dataset. It does so by doing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the `X` input features by dropping the `'Defect'` column from the
    `dfDataAnt13` DataFrame, assuming that `'Defect'` is the target column (class
    label).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the trained model (`model.predict(X)`) to make predictions on the `X`
    input features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asserting that the first prediction (`model.predict(X)[0]`) should be equal
    to 1 (class 1). If the model predicts class 1 correctly, the test passes; otherwise,
    it raises an exception, indicating a test failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third test case (`test_model_predicts_class_0_correctly`) checks whether
    the model predicts class 0 correctly for the given dataset. It follows a similar
    process as the previous test:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the `X` input features by dropping the `'Defect'` column from the
    `dfDataAnt13` DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the trained model (`model.predict(X)`) to make predictions on the `X`
    input features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asserting that the second prediction (`model.predict(X)[1]`) should be equal
    to 0 (class 0). If the model predicts class 0 correctly, the test passes; otherwise,
    it raises an exception, indicating a test failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These tests verify the integrity and correctness of the trained model and ensure
    it performs as expected on the given dataset. The output from executing the tests
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The Pytest framework found all of our tests and showed that three (out of four)
    are in the `chapter_12_classical_ml_test.py` file and one is in the `chapter_12_downloaded_model_test.py`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'My next best practice is, therefore, this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #64'
  prefs: []
  type: TYPE_NORMAL
- en: Treat models as units and prepare unit tests for them accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend treating ML models as units (the same as modules) and using unit
    testing practices for them. This helps to reduce the effects of the probabilistic
    nature of the models and provides us with the possibility to check whether the
    model works correctly. It helps to debug the entire software system afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring ML systems at runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring pipelines in production is a critical aspect of MLOps to ensure the
    performance, reliability, and accuracy of deployed ML models. This includes several
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: The first practice is logging and collecting metrics. This activity includes
    instrumenting the ML code with logging statements to capture relevant information
    during model training and inference. Key metrics to monitor are model accuracy,
    data drift, latency, and throughput. Popular logging and monitoring frameworks
    include Prometheus, Grafana, and **Elasticsearch, Logstash, and** **Kibana** (**ELK**).
  prefs: []
  type: TYPE_NORMAL
- en: The second one is alerting, which is a setup of alerts based on predefined thresholds
    for key metrics. This helps in proactively identifying issues or anomalies in
    the production pipeline. When an alert is triggered, the appropriate team members
    can be notified to investigate and address the problem promptly.
  prefs: []
  type: TYPE_NORMAL
- en: Data drift detection is the third activity, which includes monitoring the distribution
    of incoming data to identify data drift. Data drift refers to changes in data
    distribution over time, which can impact model performance.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth activity is performance monitoring, where the MLOps team continuously
    tracks the performance of the deployed model. They measure inference times, prediction
    accuracy, and other relevant metrics, and they monitor for performance degradation,
    which might occur due to changes in data, infrastructure, or dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these four main activities, an MLOps team has also the following
    responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error analysis**: Using tools to analyze and log errors encountered during
    inference and understanding the nature of errors can help improve the model or
    identify issues in the data or system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model versioning**: Keep track of model versions and their performance over
    time, and (if needed) roll back to previous versions if issues arise with the
    latest deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment monitoring**: Monitoring the infrastructure and environment where
    the model is deployed with KPIs such as CPU/memory utilization, and network traffic
    and looking for performance bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: Ensuring that the deployed models adhere to security
    and compliance standards as well as monitor access logs and any suspicious activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User feedback**: Collecting, analyzing, and incorporating user feedback into
    the monitoring and inference process. MLOps solicits feedback from end users to
    understand the model’s performance from a real-world perspective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By monitoring pipelines effectively, MLOps can quickly respond to any issues
    that arise, deliver better user experiences, and maintain the overall health of
    your ML systems. However, monitoring all of the aforementioned aspects is rather
    effort-intensive, and not all MLOps teams have the resources to do that. Therefore,
    my last best practice in this chapter is this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #65'
  prefs: []
  type: TYPE_NORMAL
- en: Identify key aspects of the ML deployment and monitor these aspects accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Although this sounds straightforward, it is not always easy to identify key
    aspects. I usually start by prioritizing the monitoring of the infrastructure
    and logging and collecting metrics. Monitoring of the infrastructure is important
    as any kind of problems quickly propagate to customers and result in losing credibility
    and even business. Monitoring metrics and logging gives a great insight into the
    operation of ML systems and prevents a lot of problems with the production of
    ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing ML pipelines concludes the part of the book that focuses on the
    core technical aspects of ML. Pipelines are important for ensuring that the ML
    models are used according to best practices in software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: However, ML pipelines are still not a complete ML system. They can only provide
    inference of the data and provide an output. For the pipelines to function effectively,
    they need to be connected to other parts of the system such as the user interface
    and storage. That is the content of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*A. Lima*, *L. Monteiro*, and *A.P. Furtado*, *MLOps: Practices, Maturity Models,
    Roles, Tools, and Challenges-A Systematic Literature Review*. *ICEIS (1), 2022:*
    *p. 308-320*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*John, M.M.*, *Olsson, H.H.*, and *Bosch, J.*, *Towards MLOps: A framework
    and maturity model*. In *2021 47th Euromicro Conference on Software Engineering
    and Advanced Applications (SEAA)*. *2021*. *IEEE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M. et al.*, *Industrial experiences from evolving measurement systems
    into self‐healing systems for improved availability*. *Software: Practice and
    Experience*, *2018*. *48(3):* *p. 719-739*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
