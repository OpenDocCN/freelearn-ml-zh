<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Machine Learning Using Bayesian Inference"><div class="titlepage" id="aid-10DJ42"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Machine Learning Using Bayesian Inference</h1></div></div></div><p>Now that we have learned about Bayesian inference and R, it is time to use both for machine learning. In this chapter, we will give an overview of different machine learning techniques and discuss each of them in detail in subsequent chapters. Machine learning is a field at the intersection of computer science and statistics, and a subbranch of artificial intelligence or AI. The name essentially comes from the early works in AI where researchers were trying to develop learning machines that automatically learned the relationship between input and output variables from data alone. Once a machine is trained on a dataset for a given problem, it can be used as a black box to predict values of output variables for new values of input variables.</p><p>It is useful to set this learning process of a machine in a mathematical framework. Let <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> be two random variables such that we seek a learning machine that learns the relationship between these two variables from data and predicts the value of <span class="emphasis"><em>Y</em></span>, given the value of <span class="emphasis"><em>X</em></span>. The system is fully characterized by a joint probability distribution <span class="emphasis"><em>P(X,Y)</em></span>, however, the form of this distribution is unknown. The goal of learning is to find a function <span class="emphasis"><em>f(X)</em></span>, which maps from <span class="emphasis"><em>X</em></span> to <span class="emphasis"><em>Y</em></span>, such that the predictions <span class="inlinemediaobject"><img src="../Images/image00379.jpeg" alt="Machine Learning Using Bayesian Inference"/></span> contain as small error as possible. To achieve this, one chooses a loss function <span class="emphasis"><em>L(Y, f(X))</em></span> and finds an <span class="emphasis"><em>f(X)</em></span> that minimizes the expected or average loss over the joint distribution of <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> given by:</p><div class="mediaobject"><img src="../Images/image00380.jpeg" alt="Machine Learning Using Bayesian Inference"/></div><p style="clear:both; height: 1em;"> </p><p>In Statistical Decision Theory, this is called <a id="id213" class="indexterm"/>
<span class="strong"><strong>empirical risk minimization</strong></span>. The typical loss function used is <a id="id214" class="indexterm"/>
<span class="strong"><strong>square loss function</strong></span> (<span class="inlinemediaobject"><img src="../Images/image00381.jpeg" alt="Machine Learning Using Bayesian Inference"/></span>), if <span class="emphasis"><em>Y</em></span> is a continuous variable, and <a id="id215" class="indexterm"/>
<span class="strong"><strong>Hinge loss function</strong></span> (<span class="inlinemediaobject"><img src="../Images/image00382.jpeg" alt="Machine Learning Using Bayesian Inference"/></span>), if <span class="emphasis"><em>Y</em></span> is a binary discrete variable with values <span class="inlinemediaobject"><img src="../Images/image00383.jpeg" alt="Machine Learning Using Bayesian Inference"/></span>. The first case is typically called <a id="id216" class="indexterm"/>
<span class="strong"><strong>regression</strong></span> and second case is called <a id="id217" class="indexterm"/>
<span class="strong"><strong>binary classification</strong></span>, as we will see later in this chapter.</p><p>The mathematical framework described here is called <a id="id218" class="indexterm"/>
<span class="strong"><strong>supervised learning</strong></span>, where the machine is presented with a training dataset containing ground truth values corresponding to pairs <span class="emphasis"><em>(Y, X)</em></span>. Let us consider the case of square loss function again. Here, the learning task is to find an <span class="emphasis"><em>f(X)</em></span> that minimizes the following:</p><div class="mediaobject"><img src="../Images/image00384.jpeg" alt="Machine Learning Using Bayesian Inference"/></div><p style="clear:both; height: 1em;"> </p><p>Since the objective is to predict values of <span class="emphasis"><em>Y</em></span> for the given values of <span class="emphasis"><em>X</em></span>, we have used the conditional distribution <span class="emphasis"><em>P(Y|X)</em></span> inside the integral using factorization of <span class="emphasis"><em>P(X, Y)</em></span>. It can be shown that the minimization of the preceding loss function leads to the following solution:</p><div class="mediaobject"><img src="../Images/image00385.jpeg" alt="Machine Learning Using Bayesian Inference"/></div><p style="clear:both; height: 1em;"> </p><p>The meaning of the preceding equation is that the best prediction of <span class="emphasis"><em>Y</em></span> for any input value <span class="emphasis"><em>X</em></span> is the mean or expectation denoted by <span class="emphasis"><em>E</em></span>, of the conditional probability distribution <span class="emphasis"><em>P(Y|X)</em></span> conditioned at <span class="emphasis"><em>X</em></span>.</p><p>In <a class="link" title="Chapter 3. Introducing Bayesian Inference" href="part0030.xhtml#aid-SJGS2">Chapter 3</a>, <span class="emphasis"><em>Introducing Bayesian Inference</em></span>, we mentioned <a id="id219" class="indexterm"/>
<span class="strong"><strong>maximum likelihood estimation</strong></span> (<span class="strong"><strong>MLE</strong></span>) as a method for learning the parameters <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Machine Learning Using Bayesian Inference"/></span> of any distribution <span class="emphasis"><em>P(X)</em></span>. In general, MLE is the same as the minimization of a square loss function if the underlying distribution is a normal distribution. </p><p>Note that, in empirical risk minimization, we are learning the parameter, <span class="emphasis"><em>E[(Y|X)]</em></span>, the mean of the conditional distribution, for a given value of <span class="emphasis"><em>X</em></span>. We will use one particular machine learning task, linear regression, to explain the advantage of Bayesian inference over the classical method of learning. However, before this, we will briefly explain some more general aspects of machine learning.</p><p>There are two types of supervised machine learning models, namely generative models and discriminative models. In the case of generative models, the algorithm tries to learn the joint probability of <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>, which is <span class="emphasis"><em>P(X,Y)</em></span>, from data and uses it to estimate mean <span class="emphasis"><em>P(Y|X)</em></span>. In the case of discriminative models, the algorithm tries to directly learn the desired function, which is the mean of <span class="emphasis"><em>P(Y|X)</em></span>, and no modeling of the <span class="emphasis"><em>X</em></span> variable is attempted.</p><p>Labeling values of the target variable in the training data is done manually. This makes supervised learning very expensive when one needs to use very large datasets as in the case of text analytics. However, very often, supervised learning methods produce the most accurate results.</p><p>If there is not enough training data available for learning, one can still use machine learning through <a id="id220" class="indexterm"/>
<span class="strong"><strong>unsupervised learning</strong></span>. Here, the learning is mainly through the discovery of patterns of associations between variables in the dataset. Clustering data points that have similar features is a classic example.</p><p>
<span class="strong"><strong>Reinforcement learning</strong></span> is<a id="id221" class="indexterm"/> the third type of machine learning, where the learning takes place in a dynamic environment where the machine needs to perform certain actions based on its current state. Associated with each action is a reward. The machine needs to learn what action needs to be taken at each state so that the total reward is maximized. This is typically how a robot learns to perform tasks, such as driving a vehicle, in a real-life environment.</p><div class="section" title="Why Bayesian inference for machine learning?"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Why Bayesian inference for machine learning?</h1></div></div></div><p>We have <a id="id222" class="indexterm"/>already discussed the advantages of Bayesian statistics over classical statistics in the last chapter. In this chapter, we will see in more detail how some of the concepts of Bayesian inference that we learned in the last chapter are useful in the context of machine learning. For this purpose, we take one simple machine learning task, namely linear regression. Let us consider a <a id="id223" class="indexterm"/>learning task where we have a dataset <span class="emphasis"><em>D</em></span> containing <span class="emphasis"><em>N</em></span> pair of points <span class="inlinemediaobject"><img src="../Images/image00387.jpeg" alt="Why Bayesian inference for machine learning?"/></span> and the goal is to build a machine learning model using linear regression that it can be used to predict values of <span class="inlinemediaobject"><img src="../Images/image00388.jpeg" alt="Why Bayesian inference for machine learning?"/></span>, given new values of <span class="inlinemediaobject"><img src="../Images/image00389.jpeg" alt="Why Bayesian inference for machine learning?"/></span>.</p><p>In linear regression, first, we assume that <span class="emphasis"><em>Y</em></span> is of the following form:</p><div class="mediaobject"><img src="../Images/image00390.jpeg" alt="Why Bayesian inference for machine learning?"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>F(X)</em></span> is a function that captures the true relationship between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>, and <span class="inlinemediaobject"><img src="../Images/image00391.jpeg" alt="Why Bayesian inference for machine learning?"/></span> is an error term that captures the inherent noise in the data. It is assumed that this noise is characterized by a normal distribution with mean 0 and variance <span class="inlinemediaobject"><img src="../Images/image00392.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. What this implies is that if we have an infinite training dataset, we can learn the form of <span class="emphasis"><em>F(X)</em></span> from data and, even then, we can only predict <span class="emphasis"><em>Y</em></span> up to an additive noise term <span class="inlinemediaobject"><img src="../Images/image00391.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. In practice, we will have only a finite training dataset <span class="emphasis"><em>D</em></span>; hence, we will be able to learn only an approximation for <span class="emphasis"><em>F(X)</em></span> denoted by <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span>.</p><p>Note that we are discussing two types of errors here. One is an error term <span class="inlinemediaobject"><img src="../Images/image00391.jpeg" alt="Why Bayesian inference for machine learning?"/></span> that is due to the inherent noise in the data that we cannot do much about. The second error is in learning <span class="emphasis"><em>F(X)</em></span>, approximately through the function <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span> from the dataset <span class="emphasis"><em>D</em></span>.</p><p>In general, <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span>, which the approximate mapping between input variable <span class="emphasis"><em>X</em></span> and output variable <span class="emphasis"><em>Y</em></span>, is a function of <span class="emphasis"><em>X</em></span> with a set of parameters <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. When <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span> is a linear function of the parameters <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Why Bayesian inference for machine learning?"/></span>, we <a id="id224" class="indexterm"/>say the learning model is linear. It is a general misconception that linear regression corresponds to the case only if <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span> is a linear function of <span class="emphasis"><em>X</em></span>. The reason for linearity in the parameter and not in <span class="emphasis"><em>X</em></span> is that, during the minimization of the loss function, one actually minimizes over the parameter values to find the best <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. Hence, a function that is linear in <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Why Bayesian inference for machine learning?"/></span> will lead to a linear optimization problem that can be tackled analytically and numerically more easily. Therefore, linear regression corresponds to the following:</p><div class="mediaobject"><img src="../Images/image00394.jpeg" alt="Why Bayesian inference for machine learning?"/></div><p style="clear:both; height: 1em;"> </p><p>This is an expansion over a set of <span class="emphasis"><em>M</em></span> basis functions <span class="inlinemediaobject"><img src="../Images/image00395.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. Here, each basis function <span class="inlinemediaobject"><img src="../Images/image00396.jpeg" alt="Why Bayesian inference for machine learning?"/></span> is a function of <span class="emphasis"><em>X</em></span> without any unknown parameters. In machine learning, these are called feature functions or model features. For the linear regression problem, the loss function, therefore, can be written as follows:</p><div class="mediaobject"><img src="../Images/image00397.jpeg" alt="Why Bayesian inference for machine learning?"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00398.jpeg" alt="Why Bayesian inference for machine learning?"/></span> is the transpose of the parameter vector <span class="inlinemediaobject"><img src="../Images/image00399.jpeg" alt="Why Bayesian inference for machine learning?"/></span> and <span class="emphasis"><em>B(X)</em></span> is the vector composed <a id="id225" class="indexterm"/>of the basis functions <span class="inlinemediaobject"><img src="../Images/image00400.jpeg" alt="Why Bayesian inference for machine learning?"/></span>. Learning <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Why Bayesian inference for machine learning?"/></span> from a dataset implies estimating the values of <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Why Bayesian inference for machine learning?"/></span> by minimizing the loss function through some optimization schemes such as gradient descent.</p><p>It is important to choose as many basis functions as possible to capture interesting patterns in the data. However, choosing more numbers of basis functions or features will overfit the model in the sense that it will even start fitting the noise contained in the data. Overfit will lead to poor predictions on new input data. Therefore, it is important to choose an optimum number of best features to maximize the predictive accuracy of any machine learning model. In machine learning based on classical statistics, this is achieved through what is called <span class="strong"><strong>bias-variance tradeoff</strong></span> and <span class="strong"><strong>model regularization</strong></span>. Whereas, in machine learning through Bayesian inference, accuracy of a predictive model can be maximized through Bayesian model averaging, and there is no need to impose model regularization or bias-variance tradeoff. We will learn each of these concepts in the following sections.</p></div></div>
<div class="section" title="Model overfitting and bias-variance tradeoff" id="aid-11C3M1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Model overfitting and bias-variance tradeoff</h1></div></div></div><p>The <a id="id226" class="indexterm"/>expected loss mentioned in the previous section can be written as a sum of three terms in the case of linear regression using squared loss function, as follows:</p><div class="mediaobject"><img src="../Images/image00401.jpeg" alt="Model overfitting and bias-variance tradeoff"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>Bias</em></span> is the difference <span class="inlinemediaobject"><img src="../Images/image00402.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> between the true model <span class="emphasis"><em>F(X)</em></span> and average value of <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> taken over<a id="id227" class="indexterm"/> an ensemble of datasets. <span class="emphasis"><em>Bias</em></span> is a measure of how much the average prediction over all datasets in the ensemble differs from the true regression function <span class="emphasis"><em>F(X)</em></span>. <span class="emphasis"><em>Variance</em></span> is given by <span class="inlinemediaobject"><img src="../Images/image00403.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span>. It is a measure of extent to which the solution for a given dataset varies around the mean over all datasets. Hence, <span class="emphasis"><em>Variance</em></span> is a measure of how much the function <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> is sensitive to the particular choice of dataset <span class="emphasis"><em>D</em></span>. The third term <span class="emphasis"><em>Noise</em></span>, as mentioned earlier, is the expectation of difference <span class="inlinemediaobject"><img src="../Images/image00404.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> between observation and the true regression function, over all the values of <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>. Putting all these together, we can write the following:</p><div class="mediaobject"><img src="../Images/image00405.jpeg" alt="Model overfitting and bias-variance tradeoff"/></div><p style="clear:both; height: 1em;"> </p><p>The objective of machine learning is to learn the function <span class="inlinemediaobject"><img src="../Images/image00393.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> from data that minimizes the expected loss <span class="emphasis"><em>E[L]</em></span>. One can keep minimizing the bias by keeping more and more basis functions in the model and thereby increasing the model's complexity. However, since each of the model parameters <span class="inlinemediaobject"><img src="../Images/image00406.jpeg" alt="Model overfitting and bias-variance tradeoff"/></span> are learned from a given dataset, the more complex the model becomes, the more sensitive its parameter estimation would be to the dataset used. This results in increased variance for more complex models. Hence, in any supervised machine learning task, there is a tradeoff between model bias and model complexity. One has to choose a model of optimum complexity to minimize the error of prediction on an unseen dataset. In the classical or frequentist approach, this is done by partitioning the labeled data into three sets. One is the training set, the second is the validation set, and the third is the test set. Models of different complexity that are trained using the training set are evaluated using the validation dataset to choose the model with optimum complexity. It is then, finally, evaluated against the test set to estimate the prediction error.</p></div>
<div class="section" title="Selecting models of optimum complexity" id="aid-12AK81"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Selecting models of optimum complexity</h1></div></div></div><p>There are different <a id="id228" class="indexterm"/>ways of selecting models with the right complexity so that the prediction error on unseen data is less. Let's discuss each of these approaches in the context of the linear regression model.</p><div class="section" title="Subset selection"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Subset selection</h2></div></div></div><p>In the<a id="id229" class="indexterm"/> subset selection approach, one selects only a subset of the whole set of variables, which are significant, for the model. This not only increases the <a id="id230" class="indexterm"/>prediction accuracy of the model by decreasing model variance, but it is also useful from the interpretation point of view. There are different ways of doing subset selection, but the following two are the most commonly used approaches:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Forward selection</strong></span>: In<a id="id231" class="indexterm"/> forward selection, one starts with no variables (intercept alone), and by using a greedy algorithm, adds other variables one by one. For each step, the variable that most improves the fit is chosen to add to the model.</li><li class="listitem"><span class="strong"><strong>Backward selection</strong></span>: In<a id="id232" class="indexterm"/> backward selection, one starts with the full model and sequentially deletes the variable that has the least impact on the fit. At each step, the variable with the least Z-score is selected for elimination. In statistics, the Z-score of a random variable is a measure of the standard deviation between an element and its mean. A small value of Z-score (typically &lt; 2) indicates that the effect of the variable is more likely by chance and is not statistically significant.</li></ul></div></div><div class="section" title="Model regularization"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Model regularization</h2></div></div></div><p>In this <a id="id233" class="indexterm"/>approach, one adds a penalty term to the loss <a id="id234" class="indexterm"/>function that does not allow the size of the parameter to become very large during minimization. There are two main ways of doing this:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Ridge regression</strong></span>: This <a id="id235" class="indexterm"/>simple type of regularization is where the additional term is proportional to the magnitude of the parameter vector given by <span class="inlinemediaobject"><img src="../Images/image00407.jpeg" alt="Model regularization"/></span>. The loss function for linear regression with the regularization term can be written as follows:
<div class="mediaobject"><img src="../Images/image00408.jpeg" alt="Model regularization"/></div><p style="clear:both; height: 1em;"> </p>
Parameters <span class="inlinemediaobject"><img src="../Images/image00409.jpeg" alt="Model regularization"/></span> having a large magnitude will contribute more to the loss. Hence, minimization of the preceding loss function will typically produce parameters <a id="id236" class="indexterm"/>having small values and reduce the overfit. The optimum value of <span class="inlinemediaobject"><img src="../Images/image00410.jpeg" alt="Model regularization"/></span> is found from the validation set.
</li><li class="listitem"><span class="strong"><strong>Lasso</strong></span>: In Lasso <a id="id237" class="indexterm"/>also, one adds a penalty term similar to ridge regression, but the term is proportional to the sum of modulus of each parameter and not its square:<div class="mediaobject"><img src="../Images/image00411.jpeg" alt="Model regularization"/></div><p style="clear:both; height: 1em;"> </p><p>Though this looks like a simple change, Lasso has some very important differences with respect to ridge regression. First of all, the presence of the <span class="inlinemediaobject"><img src="../Images/image00412.jpeg" alt="Model regularization"/></span> term makes the loss function nonlinear in parameters <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Model regularization"/></span>. The corresponding minimization problem is called the quadratic programming problem compared to the linear programming problem in ridge regression, for which a closed form solution is available. Due to the particular form <span class="inlinemediaobject"><img src="../Images/image00412.jpeg" alt="Model regularization"/></span> of the penalty, when <a id="id238" class="indexterm"/>the coefficients shrink as a result of minimization, some of them eventually become zero. So, Lasso is also in some sense a subset selection problem.</p></li></ul></div><p>A detailed discussion of various subset selection and model regularization approaches can be found in the book by Trevor Hastie et.al (reference 1 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p></div></div>
<div class="section" title="Bayesian averaging"><div class="titlepage" id="aid-1394Q2"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Bayesian averaging</h1></div></div></div><p>So far, we<a id="id239" class="indexterm"/> have learned that simply minimizing the loss function (or equivalently maximizing the log likelihood function in the case of normal distribution) is not enough to develop a machine learning model for a given problem. One has to worry about models overfitting the training data, which will result in larger prediction errors on new datasets. The main advantage of Bayesian methods is that one can, in principle, get away from this problem, without using explicit regularization and different datasets for training and validation. This is called Bayesian model averaging and will be discussed here. This is one of the answers to our main question of the chapter, <span class="emphasis"><em>why Bayesian inference for machine learning?</em></span>
</p><p>For this, let's do a full Bayesian treatment of the linear regression problem. Since we only want to explain how Bayesian inference avoids the overfitting problem, we will skip all the mathematical derivations and state only the important results here. For more details, interested readers can refer to the book by Christopher M. Bishop (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>The linear regression equation <span class="inlinemediaobject"><img src="../Images/image00413.jpeg" alt="Bayesian averaging"/></span>, with <span class="inlinemediaobject"><img src="../Images/image00391.jpeg" alt="Bayesian averaging"/></span> having a normal distribution with zero mean and variance <span class="inlinemediaobject"><img src="../Images/image00414.jpeg" alt="Bayesian averaging"/></span> (equivalently, precision <span class="inlinemediaobject"><img src="../Images/image00415.jpeg" alt="Bayesian averaging"/></span>), can be cast in a probability distribution form with <span class="emphasis"><em>Y</em></span> having a normal distribution with mean <span class="emphasis"><em>f(X)</em></span> and precision <span class="inlinemediaobject"><img src="../Images/image00416.jpeg" alt="Bayesian averaging"/></span>. Therefore, linear regression is equivalent to estimating the mean of the normal distribution:</p><div class="mediaobject"><img src="../Images/image00417.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>Since <span class="inlinemediaobject"><img src="../Images/image00418.jpeg" alt="Bayesian averaging"/></span>, where the set of basis functions <span class="emphasis"><em>B(X)</em></span> is known and we are assuming<a id="id240" class="indexterm"/> here that the noise parameter <span class="inlinemediaobject"><img src="../Images/image00416.jpeg" alt="Bayesian averaging"/></span> is also a known constant, only <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> needs to be taken as an uncertain variable for a fully Bayesian treatment.</p><p>The first step in Bayesian inference is to compute a posterior distribution of parameter vector <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span>. For this, we assume that the prior distribution of <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> is an <span class="emphasis"><em>M</em></span> dimensional normal distribution (since there are <span class="emphasis"><em>M</em></span> components) with mean <span class="inlinemediaobject"><img src="../Images/image00419.jpeg" alt="Bayesian averaging"/></span> and covariance matrix <span class="inlinemediaobject"><img src="../Images/image00420.jpeg" alt="Bayesian averaging"/></span>. As we have seen in <a class="link" title="Chapter 3. Introducing Bayesian Inference" href="part0030.xhtml#aid-SJGS2">Chapter 3</a>, <span class="emphasis"><em>Introducing Bayesian Inference</em></span>, this corresponds to taking a conjugate distribution for the prior:</p><div class="mediaobject"><img src="../Images/image00421.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>The corresponding posterior distribution is given by:</p><div class="mediaobject"><img src="../Images/image00422.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00423.jpeg" alt="Bayesian averaging"/></span> and <span class="inlinemediaobject"><img src="../Images/image00424.jpeg" alt="Bayesian averaging"/></span>.</p><p>Here, <span class="emphasis"><em>B</em></span> is<a id="id241" class="indexterm"/> an <span class="emphasis"><em>N x M</em></span> matrix formed by stacking basis vectors <span class="emphasis"><em>B</em></span>, at different values of <span class="emphasis"><em>X</em></span>, on top of each other as shown here:</p><div class="mediaobject"><img src="../Images/image00425.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>Now that we have the posterior distribution for <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> as a closed-form analytical expression, we can use it to predict new values of <span class="emphasis"><em>Y</em></span>. To get an analytical closed-form expression for the predictive distribution of <span class="emphasis"><em>Y</em></span>, we make an assumption that <span class="inlinemediaobject"><img src="../Images/image00426.jpeg" alt="Bayesian averaging"/></span> and <span class="inlinemediaobject"><img src="../Images/image00427.jpeg" alt="Bayesian averaging"/></span>. This corresponds to a prior with zero mean and isotropic covariance matrix characterized by one precision parameter <span class="inlinemediaobject"><img src="../Images/image00428.jpeg" alt="Bayesian averaging"/></span>. The predictive distribution or the probability that the prediction for a new value of <span class="emphasis"><em>X = x</em></span> is <span class="emphasis"><em>y</em></span>, is given by:</p><div class="mediaobject"><img src="../Images/image00429.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>This equation is the central theme of this section. In the classical or frequentist approach, one estimates a particular value <span class="inlinemediaobject"><img src="../Images/image00430.jpeg" alt="Bayesian averaging"/></span> for the parameter <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> from the training dataset and finds the probability <a id="id242" class="indexterm"/>of predicting <span class="emphasis"><em>y</em></span> by simply using <span class="inlinemediaobject"><img src="../Images/image00431.jpeg" alt="Bayesian averaging"/></span>. This does not address the overfitting of the model unless regularization is used. In Bayesian inference, we are integrating out the parameter variable <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> by using its posterior probability distribution <span class="inlinemediaobject"><img src="../Images/image00432.jpeg" alt="Bayesian averaging"/></span> learned from the data. This averaging will remove the necessity of using regularization or keeping the parameters to an optimal level through bias-variance tradeoff. This can be seen from the closed-form expression for <span class="emphasis"><em>P(y|x)</em></span>, after we substitute the expressions for <span class="inlinemediaobject"><img src="../Images/image00433.jpeg" alt="Bayesian averaging"/></span> and <span class="inlinemediaobject"><img src="../Images/image00434.jpeg" alt="Bayesian averaging"/></span> for the linear regression problem and do the integration. Since both are normal distributions, the integration can be done analytically that results in the following simple expression for <span class="emphasis"><em>P(y|x)</em></span>:</p><div class="mediaobject"><img src="../Images/image00435.jpeg" alt="Bayesian averaging"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00436.jpeg" alt="Bayesian averaging"/></span>.</p><p>This equation implies that the variance of the predictive distribution consists of two terms. One term, 1/<span class="inlinemediaobject"><img src="../Images/image00416.jpeg" alt="Bayesian averaging"/></span>, coming from the inherent noise in the data and the second term coming from the uncertainty associated with the estimation of model parameter <span class="inlinemediaobject"><img src="../Images/image00386.jpeg" alt="Bayesian averaging"/></span> from data. One can show that as the size of training data <span class="emphasis"><em>N</em></span> becomes very large, the second term decreases and in the limit <span class="inlinemediaobject"><img src="../Images/image00437.jpeg" alt="Bayesian averaging"/></span> it becomes zero.</p><p>The example shown<a id="id243" class="indexterm"/> here illustrates the power of Bayesian inference. Since one can take care of uncertainty in the parameter estimation through Bayesian averaging, one doesn't need to keep separate validation data and all the data can be used for training. So, a full Bayesian treatment of a problem avoids the overfitting issue. Another major advantage of Bayesian inference, which we will not go into in this section, is treating latent variables in a machine learning model. In the next section, we will give a high-level overview of the various common machine learning tasks.</p></div>
<div class="section" title="An overview of common machine learning tasks" id="aid-147LC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>An overview of common machine learning tasks</h1></div></div></div><p>This section<a id="id244" class="indexterm"/> is a prequel to the following chapters, where we will discuss different machine learning techniques in detail. At a high level, there are only a handful of tasks that machine learning tries to address. However, for each of such tasks, there are several approaches and algorithms in place.</p><p>The typical tasks in any machine learning are one of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Classification</li><li class="listitem">Regression</li><li class="listitem">Clustering</li><li class="listitem">Association rules</li><li class="listitem">Forecasting</li><li class="listitem">Dimensional reduction</li><li class="listitem">Density estimation</li></ul></div><p>In classification, the <a id="id245" class="indexterm"/>objective is to assign a new data point to one of the predetermined classes. Typically, this is either a supervised or semi-supervised learning problem. The well-known machine learning algorithms used for classification<a id="id246" class="indexterm"/> are logistic regression, <a id="id247" class="indexterm"/>
<span class="strong"><strong>support vector machines</strong></span> (<span class="strong"><strong>SVM</strong></span>), decision trees, Naïve Bayes, neural networks, Adaboost, and random forests. Here, Naïve Bayes is a Bayesian inference-based method. Other algorithms, such as logistic regression and neural networks, have also been implemented in the Bayesian framework.</p><p>Regression <a id="id248" class="indexterm"/>is <a id="id249" class="indexterm"/>probably the <a id="id250" class="indexterm"/>most common machine learning problem. It is used to determine the relation between a set of input variables (typically, continuous variables) and an output (dependent) variable that is continuous. We discussed the simplest example of linear regression in some detail in the previous section. More complex examples of regression are generalized linear regression, spline regression, nonlinear regression using neural networks, support vector regression, and Bayesian network. Bayesian formulations of regression include the Bayesian network and Bayesian linear regression.</p><p>Clustering is<a id="id251" class="indexterm"/> a<a id="id252" class="indexterm"/> classic example of unsupervised learning. Here, the objective is to group together similar items in a dataset based on certain features of the data. The number of clusters is not known in advance. Hence, clustering is more of a pattern detection problem. The well-known clustering algorithms are K-means clustering, hierarchical clustering, and <a id="id253" class="indexterm"/>
<span class="strong"><strong>Latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>). In this, LDA is formulated as a Bayesian inference problem. Other clustering methods using Bayesian inference include the Bayesian mixture model.</p><p>Association<a id="id254" class="indexterm"/> rule mining<a id="id255" class="indexterm"/> is an unsupervised method that finds items that are co-occurring in large transactions of data. The market basket analysis, which finds the items that are sold together in a supermarket, is based on association rule mining. The Apriori algorithm and frequent pattern matching algorithm are two main methods used for association rule mining.</p><p>Forecasting<a id="id256" class="indexterm"/> is<a id="id257" class="indexterm"/> similar to regression, except that the data is a time series where there are observations with different values of time stamp and the objective is to predict future values based on the current and past values. For this purpose, one can use methods such as ARIMA, neural networks, and dynamic Bayesian networks.</p><p>One of the fundamental issues in machine learning is called <span class="emphasis"><em>the </em></span><a id="id258" class="indexterm"/>
<span class="emphasis"><em>curse of dimensionality</em></span>. Since there can be a large<a id="id259" class="indexterm"/> number of features in a machine learning model, the typical minimization of error that one has to do to <a id="id260" class="indexterm"/>estimate model parameters will involve search and optimization in a large dimensional space. Most often, data will be very sparse in this higher dimensional space. This can make the search for optimal parameters very inefficient. To avoid this problem, one tries to project this higher dimensional space into a lower dimensional space containing a few important variables. One can then use these lower dimensional variables as features. The two well-known examples of dimensional reduction are principal component analysis and self-organized maps.</p><p>Often, the probability distribution of a population is directly estimated, without any parametric models, from <a id="id261" class="indexterm"/>a small amount of observed data for making inferences. This is called <a id="id262" class="indexterm"/>
<span class="strong"><strong>density estimation</strong></span>. The<a id="id263" class="indexterm"/> simplest form of density estimation is histograms, though it is not adequate for many practical applications. The more sophisticated density estimations are <a id="id264" class="indexterm"/>
<span class="strong"><strong>kernel density estimation</strong></span> (<span class="strong"><strong>KDE</strong></span>) and vector quantization.</p></div>
<div class="section" title="References" id="aid-1565U1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Friedman J., Hastie T., and Tibshirani R. <span class="emphasis"><em>The Elements of Statistical Learning – Data Mining, Inference, and Prediction</em></span>. Springer Series in Statistics. 2009</li><li class="listitem">Bishop C.M. <span class="emphasis"><em>Pattern Recognition and Machine Learning (Information Science and Statistics)</em></span>. Springer. 2006. ISBN-10: 0387310738</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-164MG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Summary</h1></div></div></div><p>In this chapter, we got an overview of what machine learning is and what some of its high-level tasks are. We also discussed the importance of Bayesian inference in machine learning, particularly in the context of how it can help to avoid important issues, such as model overfit and how to select optimum models. In the coming chapters, we will learn some of the Bayesian machine learning methods in detail.</p></div></body></html>