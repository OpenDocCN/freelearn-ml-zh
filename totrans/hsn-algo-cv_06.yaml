- en: Video Analysis &#x2013; Motion Detection and Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a computer vision developer, there is absolutely no way you can avoid dealing
    with video feeds from stored video files or cameras and other such sources. Treating
    video frames as individual images is one way to process videos, which surprisingly
    doesn't require much more effort or knowledge of the algorithms than what you
    have learned so far. For instance, you can apply a smoothening filter on a video,
    or in other words, a set of video frames, the same way as you would when you apply
    it on an individual image. The only trick here is that you must extract each frame
    from a video, as described in [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV*. However, in computer vision, there are certain
    algorithms that are meant to work with consecutive video frames and the result
    on their operation depends not just on an individual image but also on the result
    of the same operation on the previous frames. Both of the algorithm types we just
    mentioned will be the main topics covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about histograms and back-projection images in the previous chapter,
    we are ready to take on computer vision algorithms that are used to detect and
    track objects in real-time. These algorithms highly rely on a firm understanding
    on all the topics we learned in [Chapter 5](part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4),
    *Back-Projection and Histograms*. Based on this, we'll start this chapter with
    a couple of simple examples about how to use the computer vision algorithms we've
    learned so far, to process frames from a video file or camera, and then we'll
    move on to learn about two of the most famous object detection and tracking algorithms,
    the Mean Shift and CAM Shift algorithms. Then, we'll learn how to use the Kalman
    filter to correct the result of our object detection and tracking algorithms and
    how to remove noise from the results to get a better tracking result. We'll end
    this chapter by learning about motion analysis and background/foreground extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to apply filters and perform such operations on videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Mean Shift algorithm to detect and track objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the CAM Shift algorithm to detect and track objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Kalman filter to improve tracking results and remove noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the background and foreground extraction algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IDE to develop C++ or Python applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV*, for more information about how to set up a personal
    computer and make it ready for developing computer vision applications using the
    OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following URL to download the source codes and examples for
    this chapter: [https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Processing videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to use any of the algorithms that we''ve learned so far on videos,
    we need to be able to read video frames and store them in `Mat` objects. We have
    already learned about how to deal with video files, cameras, and RTSP feeds in
    the initial chapters of this book. So, extending that, using what we learned in
    the previous chapters, we can use a code similar to the following, in order to
    apply colormaps to the video feed from the default camera on a computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Just as we learned in [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV*, we simply need to create a `VideoCapture` object
    and read the video frames from the default camera (which has an index of zero).
    In the preceding example, we have added a single line of code that is responsible
    for applying a colormap on the extracted video frames. Try the preceding example
    code and you'll see the `COLORMAP_JET` colormap applied to every frame of the
    camera, much like what we learned in [Chapter 4](part0085.html#2H1VQ0-15c05657f8254d318ea883ef10fc67f4),
    *Drawing, Filtering, and Transformation*, and finally the results are displayed
    in real-time. Pressing the spacebar should stop the video processing altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can perform different video-processing algorithms in real-time,
    based on a specific key being pressed. Here''s an example, replacing only the
    `for` loop in the preceding code, which results in the original video being displayed
    unless either the *J* or *H* key is pressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The original output video of the camera will be displayed unless any of the
    mentioned keys are pressed. Pressing *J* will trigger `COLORMAP_JET`, while pressing
    *H* will trigger the `COLORMAP_HOT` colormap being applied to the camera frames.
    Similar to the previous example, pressing the spacebar key will stop the process.
    Also, pressing any keys other than space, *J*, or *H* will result in the original
    video being displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The `applyColorMap` function in the preceding examples is just a random algorithm
    that is used to describe the technique used to process videos in real-time. You
    can use any of the algorithms you have learned in this book that perform on a
    single image. You can, for instance, write a program that performs a smoothening
    filter on the video, or Fourier transformation, or even a program that displays
    the Hue channel histogram in real-time. The use cases are infinite, however the
    method used is almost identical for all algorithms that perform a single complete
    operation on any individual image.
  prefs: []
  type: TYPE_NORMAL
- en: Besides performing an operation on individual video frames, one can also perform
    operations that depend on any number of consecutive frames. Let's see how this
    is done using a very simple, but extremely important, use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we want to find the average brightness of the last 60 frames
    read from a camera at any given moment. Such a value is quite useful when we want
    to automatically adjust the brightness of the video when the content of the frames
    is extremely dark or extremely bright. In fact, a similar operation is usually
    performed by the internal processor of most digital cameras, and even the smartphone
    in your pocket. You can give it a try by turning on the camera on your smartphone
    and pointing it toward a light, or the sun, or by entering a very dark environment.
    The following code demonstrates how the average of the brightness of the last
    `60` frames is calculated and displayed in the corner of the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For the most part, this example code is quite similar to the examples we saw
    earlier in this chapter. The main difference here is that we are storing the average
    of the last `60` frames that are calculated using the OpenCV mean function in
    a `vector` of `Scalar` objects, and then we calculate the average of all averages.
    The calculated value is then drawn on the input frame using the `putText` function.
    The following image depicts a single frame that is displayed when the preceding
    example code is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Notice the value displayed in the bottom-left corner of the image, which will
    start to decrease when the content of the video frames becomes darker and increase
    when they become brighter. Based on this result, you can, for instance, change
    the brightness value or warn the user of your application that the content is
    too dark or bright, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this initial section of the chapter were meant to teach you
    the idea of processing individual frames using the algorithms you've learned in
    the previous chapters, and a few simple programming techniques used to calculate
    a value based on consecutive frames. In the following sections of this chapter,
    we'll be learning about some of the most important video-processing algorithms,
    namely object detection and tracking algorithms, which depend on the concepts
    and techniques we learned in this section and the previous chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Mean Shift algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Mean Shift algorithm is an iterative algorithm that can be used to find
    the maxima of a density function. A very rough translation of the preceding sentence
    to computer vision terminology would be the following—the Mean Shift algorithm
    can be used to find an object in an image using a back-projection image. But how
    is it achieved in practice? Let''s walk through this step by step. Here are the
    individual operations that are performed to find an object using the Mean Shift
    algorithm, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: The back-projection of an image is created using a modified histogram to find
    the pixels that are most likely to contain our object of interest. (It is also
    common to filter the back-projection image to get rid of unwanted noise, but this
    is an optional operation to improve the results.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An initial search window is needed. This search window will contain our object
    of interest after a number of iterations, which we''ll get to in the next step.
    After each iteration, the search window is updated by the algorithm. The updating
    of the search window happens by calculating the mass center of the search window
    in the back-projection image, and then shifting the current center point of the
    search window to the mass center of the window. The following picture demonstrates
    the concept of the mass center in a search window and how the shifting happens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00070.gif)'
  prefs: []
  type: TYPE_IMG
- en: The two points at the two ends of the arrow in the preceding picture correspond
    to the search-window center and mass center.
  prefs: []
  type: TYPE_NORMAL
- en: Just like any iterative algorithm, some termination criteria are required by
    the Mean Shift algorithm to stop the algorithm when the results are as expected
    or when reaching an accepted result does not happen as fast as needed. So, the
    number of iterations and an epsilon value are used as termination criteria. Either
    by reaching the number of iterations in the algorithm or by finding a shift distance
    that is smaller than the given epsilon value (convergence), the algorithm will
    stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s see a hands-on example of how this algorithm is used in practice
    by using the OpenCV library. The `meanShift` function in OpenCV implements the
    Mean Shift algorithm almost exactly as it was described in the preceding steps.
    This function requires a back-projection image, a search window, and the termination
    criteria, and it is used as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`srchWnd` is a `Rect` object, which is simply a rectangle that must contain
    an initial value that is used and then updated by the `meanShift` function. `backProjection`
    must contain a proper back-projection image that is calculated with any of the
    methods that we learned in [Chapter 5](part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4),
    *Back-Projection and Histograms*. The `TermCriteria` class is an OpenCV class
    that is used by iterative algorithms that require similar termination criteria.
    The first parameter defines the type of the termination criteria, which can be
    `MAX_ITER` (same as `COUNT`), `EPS`, or both. In the preceding example, we have
    used the termination criteria of `20` iterations and an epsilon value of `1.0`,
    which of course can be changed depending on the environment and application. The
    most important thing to note here is that a higher number of iterations and a
    lower epsilon can yield more accurate results, but it can also lead to slower
    performance, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example is just a demonstration of how the `meanShift` function
    is called. Now, let''s walk through a complete hands-on example to learn our first
    real-time object-tracking algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the tracking example we''ll create is quite similar to the
    previous examples in this chapter. We need to open a video, or a camera, on the
    computer using the `VideoCapture` class and then start reading the frames, as
    seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Again, we have used the `waitKey` function to stop the loop if the spacebar
    key is pressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to assume that our object of interest has a green color. So, we''re
    going to form a hue histogram that contains only the green colors, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This needs to happen before entering the process loop, since our histogram is
    going to stay constant throughout the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing to take care of before entering the actual process loop and
    the tracking code is the termination criteria, which will stay constant throughout
    the whole process. Here''s how we''ll create the required termination criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The initial value of the search window is quite important when using the Mean
    Shift algorithm to track objects, since this algorithm always makes an assumption
    about the initial position of the object to be tracked. This is an obvious downside
    of the Mean Shift algorithm, which we'll learn how to deal with later on in this
    chapter when we discuss the CAM Shift algorithm and its implementation in the
    OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: 'After each frame is read in the `while` loop we''re using for the tracking
    code, we must calculate the back-projection image of the input frame using the
    green hue histogram that we created. Here''s how it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can refer to [Chapter 5](part0102.html#318PC0-15c05657f8254d318ea883ef10fc67f4),
    *Back-Projection and Histograms*, for more detailed instructions about calculating
    the back-projection image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `meanShift` function to update the search window using the back-projection
    image and provided termination criteria, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the search window, or in other words the tracked object, we need
    to draw the search-window rectangle on the input frame. Here''s how you can do
    this by using the rectangle function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do the same on back-projection image result, however, first we need
    to convert the back-projection image to BGR color space. Remember that the result
    of the back-projection image contained a single channel image with the same depth
    as the input image. Here''s how we can draw a red rectangle at the search-window
    position on the back-projection image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the means to switch between the back-projection and original video frame
    using the *B* and *V* keys. Here''s how it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s give our program a try and see how it performs when executed in a slightly
    controlled environment. The following picture demonstrates the initial position
    of the search window and our green object of interest, both in the original frame
    view and the back-projection view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Moving the object around will cause the `meanShift` function to update the
    search window and consequently track the object. Here''s another result, depicting
    the object tracked to the bottom-right corner of the view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Notice the small amount of noise that can be seen in the corner, which would
    be taken care of by the `meanShift` function since the mass center is not affected
    too much by it. However, as mentioned previously, it is a good idea to perform
    some sort of filtering on the back-projection image to get rid of noise. For instance,
    and in case of noise similar to what we have in the back-projection image, we
    can use the `GaussianBlur` function, or even better, the `erode` function, to
    get rid of unwanted pixels in the back-projection image. For more information
    on how to use filtering functions, you can refer to [Chapter 4](part0085.html#2H1VQ0-15c05657f8254d318ea883ef10fc67f4),
    *Drawing, Filtering, and Transformation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such tracking applications, we usually need to observe, record, or in any
    way process the route that the object of interest has taken before any given moment
    and for a desired period of time. This can be simply achieved by using the center
    point of the search window, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, the `route` is a `vector` of `Point` objects. `route` needs to be
    updated after the `meanShift` function call, and then we can use the following
    call to the `polylines` function in order to draw the `route` over the original
    video frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following picture depicts the result of displaying the tracking route (for
    the last 60 frames) on the original video frames read from the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's address some issues that we observed while working with the `meanShift`
    function. First of all, it is not convenient to create the hue histogram manually.
    A flexible program should allow the user to choose the object they want to track,
    or at least allow the user to choose the color of the object of interest conveniently.
    The same can be said about the search window size and its initial position. There
    are a number of ways to deal with such issues and we're going to address them
    with a hands-on example.
  prefs: []
  type: TYPE_NORMAL
- en: When using the OpenCV library, you can use the `setMouseCallback` function to
    customize the behavior of mouse clicks on an output window. This can be used in
    combination with a few simple methods, such as `bitwise_not` to mimic an easy-to-use
    object selection for the users. `setMouseCallback`, as it can be guessed from
    its name, sets a callback function to handle the mouse clicks on a given window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following callback function in conjunction with the variables defined here
    can be used to create a convenient object selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `event` contains an entry from the `MouseEventTypes` enum, which describes
    whether a mouse button was pressed or released. Based on such a simple event,
    we can decide when the user is actually selecting an object that''s visible on
    the screen. This is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This allows a huge amount of flexibility for our applications, and the code
    is bound to work with objects of any color. Make sure to check out the example
    codes for this chapter from the online Git repository for a complete example project
    that uses all the topics we've learned so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another method of selecting an object or a region on an image is by using the `selectROI`
    and `selectROIs` functions in the OpenCV library. These functions allow the user
    to select a rectangle (or rectangles) on an image using simple mouse clicks and
    drags. Note that the `selectROI` and `selectROIs` functions are easier to use
    than handling mouse clicks using callback functions, however they do not offer
    the same amount of power, flexibility, and customization.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next section, let's recall that `meanShift` does not
    handle an increase or decrease in the size of the object that is being tracked,
    nor does it take care of the orientation of the object. These are probably the
    main issues that have led to the development of a more sophisticated version of
    the Mean Shift algorithm, which is the next topic we're going to learn about in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Continuously Adaptive Mean (CAM) Shift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To overcome the limitations of the Mean Shift algorithm, we can use an improved
    version of it, which is called the **Continuously Adaptive Mean** Shift, or simply
    the **CAM** Shift algorithm. OpenCV contains the implementation for the CAM Shift
    algorithm in a function named `CamShift`, which is used almost in an identical
    manner to the `meanShift` function. The input parameters of the `CamShift` function
    are the same as `meanShift`, since it also uses a back-projection image to update
    a search window using a given set of termination criteria. In addition, `CamShift`
    also returns a `RotatedRect` object, which contains both the search window and
    its angle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without using the returned `RotatedRect` object, you can simply replace any
    call to the `meanShift` function with `CamShift`, and the only difference would
    be that the results will be scale-invariant, meaning the search window will become
    bigger if the object is nearer (or bigger) and vice versa. For instance, we can
    replace the call to the `meanShift` function in the preceding example code for
    the Mean Shift algorithm with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following images depict the result of replacing the `meanShift` function
    with `CamShift` in the example from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the results are now scale-invariant, even though we didn''t change
    anything except replace the mentioned function. As the object moves farther away
    from the camera, or becomes smaller, the same Mean Shift algorithm is used to
    calculate its position, however, this time the search window is resized to fit
    the exact size of the object, and the rotation is calculated, which we didn''t
    use. To be able to use the rotation value of the object, we need to store the
    result of the `CamShift` function in a `RotatedRect` object first, as seen in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To draw a `RotatedRect` object, or in other words a rotated rectangle, you
    must use the `points` method of `RotatedRect` to extract the consisting `4` points
    of the rotated rectangle first, and then draw them all using the line function,
    as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use a `RotatedRect` object to draw a rotated ellipse that is covered
    by the rotated rectangle. Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the result of using the `RotatedRect` object to
    draw a rotated rectangle and ellipse at the same time, over the tracked object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding image, the red rectangle is the search window, the blue rectangle
    is the resulting rotated rectangle, and the green ellipse is drawn by using the
    resulting rotated rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we can say that `CamShift` is far better suited to dealing with
    objects of varying size and rotation than `meanShift`, however, there are still
    a couple of possible enhancements that can be done when using the `CamShift` algorithm.
    First things first, the initial window size still needs to be set, but since `CamShift`
    is taking care of the size changes, then we can simply set the initial window
    size to be the same as the whole image size. This would help us avoid having to
    deal with the initial position and size of the search window. If we can also create
    the histogram of the object of interest using a previously saved file on disk
    or any similar method, then we will have an object detector and tracker that works
    out of the box, at least for all the cases where our object of interest has a
    visibly different color than the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another huge improvement to such a color-based detection and tracking algorithm
    can be achieved by using the `inRange` function to enforce a threshold on the
    S and V channels of the HSV image that we are using to calculate the histogram.
    The reason is that in our example, we simply used the **hue** (or the **H**, or
    the first) channel, and we didn''t take into account the high possibility of having
    extremely dark or bright pixels that might have the same hue as our object of
    interest. This can be done by using the following code when calculating the histogram
    of the object to be tracked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example code, the variables starting with `lb` and `hb` refer
    to the lower bound and higher bound of the values that are allowed to pass the
    `inRange` function. `objImgHsv` is obviously a `Mat` object containing our object
    of interest, or a ROI that contains our object of interest. `objImgHue` is the
    first channel of `objImgHsv`, which is extracted using a previous call to the `split`
    function. The rest of the parameters are nothing new, and you've already used
    them in previous calls to the functions used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Combining all of the algorithms and techniques described in this section can
    help you create an object-detector, or even a face-detector and tracker that can
    work in realtime and with stunning speed. However, you might still need to account
    for the noise that will interfere, especially with the tracking, which is almost
    inevitable because of the nature of color-based or histogram-based trackers. One
    of the most widely used solutions to these issues is the subject of the next section
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kalman filter for tracking and noise reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kalman filter is a popular algorithm that is used for reducing the noise
    of a signal, such as the result of the tracking algorithm that we used in the
    preceding section. To be precise, the Kalman filter is an estimation algorithm
    that is used to predict the next state of a signal based on previous observations.
    Digging deep into the definition and details of the Kalman filter would require
    a chapter of its own, but we'll try to walk through this simple, yet extremely
    powerful, algorithm with a couple of hands-on examples to learn how it is used
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: For the first example, we're going to write a program that tracks the mouse
    cursor while it is moved on a canvas, or the OpenCV window. The Kalman filter
    is implemented using the `KalmanFilter` class in OpenCV and it includes all (and
    many more) of the Kalman filter implementation details, which we'll discuss in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, `KalmanFilter` must be initialized with a number of dynamic parameters,
    measurement parameters, and control parameters, in addition to the type of the
    underlying data used in the Kalman filter itself. We''re going to ignore control
    parameters, since they are outside the scope of our examples, so we''ll set them
    simply to zero. As for the data type, we''ll go for the default 32-bit float,
    or `CV_32F` in terms of OpenCV types. Dynamic parameters in a 2D movement, which
    is the case with our example, correspond to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*, or position in *x* direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y*, or position in *y* direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*'', or velocity in *x* direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y*'', or velocity in *y* direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A higher dimensionality of the parameters can also be used, which would then
    cause the preceding list to be followed by *X*'' (acceleration in *x* direction)
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the measurement parameters, we''ll simply have *X* and *Y*, which correspond
    to the mouse position in our first example. Keeping in mind what was said about
    dynamic and measurement parameters, here''s how we can initialize a `KalmanFilter`
    class instance (object) that fits for tracking a point on a 2D space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in this example, control parameters and type parameters are simply
    ignored and set to their default values, otherwise we could have written the same
    code as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `KalmanFilter` class requires a transition matrix to be set before it is
    correctly usable. This transition matrix is used for calculating (and updating)
    the estimated, or next, state of the parameters. We''ll be using the following
    transition matrix in our example for tracking the mouse position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After completing the steps required for this example, it would be wise to return
    here and update the values in the transition matrix and observe the behavior of
    the Kalman filter. For instance, try to update the matrix row that corresponds
    to the estimated *Y* (marked as `next y` in the comments) and you'll notice that
    the tracked position *Y* value is affected by it. Try experimenting with all of
    the values in the transition matrix for a better understanding of its effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the transition matrix, we also need to take care of the initialization
    of the dynamic parameters'' state and measurements, which are the initial mouse
    positions in our example. Here''s how we initialize the mentioned values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As you''ll see later on, the `KalmanFilter` class requires a vector instead
    of a `Point` object, since it is designed to work with higher dimensionalities
    too. For this reason, we''ll update the `pos` vector in the preceding code snippet
    with the last mouse positions before performing any calculations. Other than the
    initializations that we just mentioned, we also need to initialize the measurement
    matrix of the Kalman filter. This is done as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `setIdentity` function in OpenCV is simply used to initialize matrices with
    a scaled identity matrix. If only a single matrix is provided as a parameter to
    the `setIdentity` function, it will set to the identity matrix, however if a second
    `Scalar` is provided in addition, then all elements of the identity matrix will
    be multiplied (or scaled) using the given `Scalar` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last initialization is the process noise covariance. We''ll use a very
    small value for this, which causes a tracking with a natural-movement feeling,
    although with a little bit of overshoot when tracking. Here''s how we initialize
    the process-noise covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also common to initialize the following matrices before using the `KalmanFilter`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`controlMatrix` (not used if the control parameter count is zero)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorCovPost`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorCovPre`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gain`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`measurementNoiseCov`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using all of the matrices mentioned in the preceding list will provide a huge
    amount of customization to the `KalmanFilter` class, but it also requires a great
    amount of knowledge about the type of noise-filtering and tracking that is needed
    and the environment that the filter will be implemented in. These matrices and
    their usage have deep roots in control theory, and control science in general,
    which is a topic for another book. Note that in our example, we'll simply use
    the default values of the mentioned matrices, thus we have ignored them altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need in our tracking example using the Kalman filter is to
    set up a window on which we can track mouse movements. We are assuming that the
    position of the mouse on the window is the position of a detected and tracked
    object and we''ll use our `KalmanFilter` object to predict and de-noise these
    detections, or in Kalman-filter-algorithm terminology, we are going to correct
    these measurements. We can use the `namedWindow` function to create a window using
    OpenCV. Consequently, the `setMouseCallback` function can be used to assign a
    callback function for mouse interactions with that specific window. Here''s how
    we can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve used the word `Canvas` for the window, but obviously you can use any
    other name you like. `onMouse` is the callback function that will be assigned
    to react on mouse interactions with this window. It is defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for the actual tracking, or to use the more correct terminology, correcting
    the measurements that contain noise, we need to use the following code, which
    is followed by the required explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are using the `trackRoute` vector to record the estimations
    over the last `100` frames. Pressing any key will cause the `while` loop, and
    consequently the program, to return. Inside the loop, and where we actually use
    the `KalmanFilter` class, we simply perform the following operations in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an empty `Mat` object to be used as a canvas to draw, and for the content
    of the window on which the tracking will happen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `objectPos`, which contains the last position of the mouse on the window
    and store it in the `pos` vector, which is usable with the `KalmanFilter` class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read an estimation using the correct method of the `KalmanFilter` class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the result of the estimation back to a `Point` object that can be used
    for drawing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the estimated point (or the tracked point) in the `trackRoute` vector,
    and make sure the number of items in the `trackRoute` vector doesn't exceed `100`,
    since that is the number of frames for which we want to keep a record of estimated
    points
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the polylines function to draw the route, stored as `Point` objects in `trackRoute`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the results using the `imshow` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the internal matrices of the `KalmanFilter` class using the predict function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try executing the tracking program and move your mouse cursor around the window
    that is shown. You''ll notice a smooth tracking result, which is drawn using the
    thick red line that''s visible in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the position of the mouse cursor is ahead of the tracking, and the
    noise in mouse movement is almost completely removed. It is a good idea to try
    to visualize the mouse movement for a better comparison between the `KalmanFilter`
    results and actual measurements. Simply add the following code to the loop after
    the point where `trackRoute` was drawn in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And obviously, you need to define the `mouseRoute` vector before entering the
    `while` loop, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try the same application with this minor update and see how the results
    compare to each other. Here''s another screenshot depicting the results of actual
    mouse movements and corrected movements (or tracked, or filtered, depending on
    the terminology) drawn in the same window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding result, the arrow is simply used to depict the overall direction
    of an extremely noisy measurement (mouse movement or detected object position,
    depending on the application), which is drawn using a thin black color, and the
    corrected results using the Kalman filter algorithm, which are drawn with a thick
    red line in the image. Try moving you mouse around and comparing the results visually.
    Remember what we mentioned about the `KalmanFilter` internal matrices and how
    you need to set their values according to the use case and application? For instance,
    a bigger process-noise covariance would have resulted in less de-noising and consequently
    less filtering. Let''s try setting the process noise covariance value to `0.001`,
    instead of the previous 0.000001 value, try the same program once again, and compare
    the results. Here''s how you can set the process-noise covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, try running the program once again and you can easily notice that less
    de-noising is happening as you move the mouse cursor around the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.gif)'
  prefs: []
  type: TYPE_IMG
- en: By now, you can probably guess that the result of setting an extremely high
    value for process-noise covariance would be almost the same as using no filter
    at all. This is the reason why setting correct values for the Kalman filter algorithm
    is extremely important and also the reason why it depends so much on the application.
    However, there are methods for setting most of those parameters programmatically,
    and even dynamically while the tracking is being done to achieve the best results.
    For instance, using a function that is able to determine the possible amount of
    noise at any given moment, we can dynamically set the process noise covariance
    to high or low values for less and more de-noising of the measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use `KalmanFilter` to perform a real-life tracking correction on
    objects using the `CamShift` function, instead of mouse movements. It''s important
    to note that the applied logic is exactly the same. We need to initialize a `KalmanFilter`
    object and set its parameters according to the amount of noise and so on. For
    simplicity, you can start off with the exact same set of parameters that we set
    for tracking the mouse cursor in the previous example, and then try to adjust
    them. We need to start by creating the same tracking program that we wrote in
    the previous sections. However, right after calling `CamShift` (or `meanShift`
    function) to update the search window, instead of displaying the results, we''ll
    perform a correction using the `KalmanFilter` class to de-noise the results. Here''s
    how it is done with a similar example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You can refer to the online source code repository of this chapter for the complete
    example project containing the preceding code, which is almost identical to what
    you saw in the previous sections, except for the simple fact that `KalmanFilter`
    is used to correct the detected and tracked object position. As you can see, `objectPos`,
    which was previously read from the mouse movement position, is now set to the
    central point of the search window. After that, the `correct` function is called
    to perform an estimation and the results are displayed by drawing a green cross
    mark for the corrected tracking result. Besides the main advantage of using the
    Kalman filter algorithm, which is useful for getting rid of noise in detection
    and tracking results, it can also help with cases where detection is momentarily
    lost or impossible. Although losing detection is, technically speaking, an extreme
    case of noise, we're trying to point out the difference in terms of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: By going through a few examples and also experimenting with your own projects
    where the Kalman filter can be of help and trying different set of parameters
    for it, you'll instantly understand its long-standing popularity for whenever
    a practical algorithm for correcting a measurement (that contains noise) is required.
    What we learned in this section was a fairly simple case of how the Kalman filter
    is used (which was enough for our use case), but it is important to note that
    the same algorithm can be used to de-noise measurements of higher dimensionalities
    and with much more complexity.
  prefs: []
  type: TYPE_NORMAL
- en: How to extract the background/foreground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The segmentation of background and foreground content in images is one of the
    most important video and motion analysis topics, and there has been a huge amount
    of research done in this area to provide some very practical and easy-to-use algorithms,
    which we're going to learn in the final section of this chapter. Current versions
    of OpenCV include the implementation of two background segmentation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: To use a terminology that is shorter, clearer, and more compatible with OpenCV
    functions and classes, we'll refer to background/foreground extraction and background/foreground
    segmentation simply as background segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two algorithms are available by default to be used for background
    segmentation using OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BackgroundSubtractorKNN`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BackgroundSubtractorMOG2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these classes are subclasses of `BackgroundSubtractor`, which contains
    all of the required interfaces that one can expect from a proper background segmentation
    algorithm, which we'll get to later on. This simply allows us to use polymorphism
    to switch between algorithms that produce the same results and can be used in
    a very similar fashion for the exact same reason. The `BackgroundSubtractorKNN`
    class implements the K-nearest neighbors background segmentation algorithm, which
    is used in the case of a low foreground pixel count. `BackgroundSubtractorMOG2`,
    on the other hand, implements the Gaussian mixture-based background-segmentation
    algorithm. You can refer to the OpenCV online documentation for detailed information
    about the internal behavior and implementation of these algorithms. It's also
    a good idea to go through the referred articles for both of these algorithms,
    especially if you are looking for a custom background segmentation algorithm of
    your own.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the algorithms we already mentioned, there are many more algorithms
    that can be used for background segmentation using OpenCV, which are included
    in the extra module, `bgsegm`. We'll omit those algorithms, since their usage
    is quite similar to the algorithms we'll be talking about in this section, and
    also because they do not exist in OpenCV by default.
  prefs: []
  type: TYPE_NORMAL
- en: An example of background segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the `BackgroundSubtractorKNN` class and a hands-on example
    to see how background segmentation algorithms are used. You can use the `createBackgroundSubtractorKNN` function
    to create an object of the `BackgroundSubtractorKNN` type. Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: To understand the parameters used in the `BackgroundSubtractorKNN` class, it
    is important to first note that this algorithm uses a sampling technique over
    the history of pixels to create a sampled background image. With that being said,
    the `history` parameter is used to define the number of previous frames that are
    used for sampling the background image, and the `dist2Threshold` parameter is
    the threshold of squared distance between a pixel's current value and its corresponding
    pixel value in the sampled background image. `detectShadows` is a self-explanatory
    parameter that is used to determine whether the shadows are going to be detected
    during background segmentation or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can simply use `bgs` to extract foreground masks from a video and use
    it to detect movements or an object entering the scene. Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly review the parts of the previous code that are new and maybe
    not that obvious. First things first, we use the `apply` function of the `BackgroundSubtractorKNN`
    class to perform a background/foreground segmentation operation. This function
    also updates the internal sampled background image for us. After that, we use
    the `bitwise_and` function with the foreground mask to extract the foreground
    image''s content. To retrieve the sampled background image itself, we simply use
    the `getBackgroundImage` function. Finally, we display all of the results. Here
    are some example results that depict a scene (top-left), the extracted background
    image (top-right), the foreground mask (bottom-left), and the foreground image
    (bottom-right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the shadow of the hand that moved into the scene is also captured
    by the background segmentation algorithm. In our example, we omitted the `learningRate`
    parameter when using the `apply` function. This parameter can be used to set the
    rate at which the learned background model is updated. A value of `0` means the
    model will not be updated at all, which can be quite useful if you are sure that
    the background will stay the same for any known period. A value of 1.0 means an
    extremely quick update of the model. As in the case of our example, we skipped
    this parameter, which causes it to use -1.0, and it means that the algorithm itself
    will decide on the learning rate. Another important thing to note is that the
    result of the apply function can yield an extremely noisy mask, which can be smoothed
    out by using a simple blur function, such as `medianBlur`, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `BackgroundSubtractorMOG2` class is quite similar to `BackgroundSubtractorKNN`.
    Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `createBackgroundSubtractorMOG2` function is used quite similarly
    to what we saw before to create an instance of the `BackgroundSubtractorMOG2`
    class. The only parameter that differs here is `varThreshold`, which corresponds
    to the variance threshold used for matching the pixels value and the background
    model. Using the `apply` and `getBackgroundImage` functions is identical in both
    background segmentation classes. Try modifying the threshold values in both algorithms
    to learn more about the visual effects of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Background-segmentation algorithms have great potential for video editing software
    or even detecting and tracking objects in an environment with backgrounds that
    do not change too much. Try to use them in conjunction with the algorithms that
    you learned previously in this chapter to build tracking algorithms that make
    use of multiple algorithms to improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The video analysis module in OpenCV is a collection of extremely powerful algorithms,
    functions, and classes that we have learned about in this chapter. Starting from
    the whole idea of video processing and simple calculations based on the content
    of consecutive video frames, we moved on to learn about the Mean Shift algorithm
    and how it is used to track objects with known colors and specifications using
    a back-projection image. We also learned about the more sophisticated version
    of the Mean Shift algorithm, which is called the Continuously Adaptive Mean Shift,
    or simply CAM Shift. We learned that this algorithm is also capable of handling
    objects of different sizes and determining their orientation. Moving on with the
    tracking algorithms, we learned about the powerful Kalman filter and how it is
    used for de-noising and correcting the tracking results. We used the Kalman filter
    to track mouse movements and to correct the tracking results of the Mean Shift
    and CAM Shift algorithms. Finally, we learned about OpenCV classes that implement
    background-segmentation algorithms. We wrote a simple program to use background-segmentation
    algorithms and output the calculated background and foreground images. By now,
    we are familiar with some of the most popular and widely used computer vision
    algorithms that allow real-time detection and tracking of objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll be learning about many feature extraction algorithms,
    functions, and classes, and how to use features to detect objects or extract useful
    information from images based on their key points and descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the examples in this chapter that deal with cameras return when there is
    a single failed or corrupted frame that leads to the detection of an empty frame.
    What type of modification is needed to allow a predefined number of retries before
    stopping the process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we call the `meanShift` function to perform the Mean Shift algorithm
    with 10 iterations and an epsilon value of 0.5?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we visualize the hue histogram of the tracked object? Assume `CamShift`
    is used for tracking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the process-noise covariance in the `KalmanFilter` class so that the filtered
    and measured values overlap. Assume only the process-noise covariance is set,
    of all the available matrices for `KalmanFilter` class-behavior control.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's assume that the *Y* position of the mouse on a window is used to describe
    the height of a filled rectangle that starts from the top-left corner of the window
    and has a width that equals the window width. Write a Kalman filter that can be
    used to correct the height of the rectangle (single value) and remove noise in
    the mouse movement that will cause a visually smooth resizing of the filled rectangle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `BackgroundSubtractorMOG2` object to extract the foreground image contents
    while avoiding shadow changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a program to display the *current* (as opposed to sampled) background
    image using a background-segmentation algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
