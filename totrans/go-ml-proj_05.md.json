["```py\n {\n \"coordinates\": null,\n \"truncated\": false,\n \"created_at\": \"Tue Aug 28 19:59:34 +0000 2012\",\n \"favorited\": false,\n \"id_str\": \"240539141056638977\",\n \"in_reply_to_user_id_str\": null,\n \"entities\": {\n \"urls\": [\n],\n \"hashtags\": \n],\n \"user_mentions\": [\n]\n },\n \"text\": \"You'd be right more often if you thought you were wrong.\",\n \"contributors\": null,\n \"id\": 240539141056638977,\n \"retweet_count\": 1,\n \"in_reply_to_status_id_str\": null,\n \"geo\": null,\n \"retweeted\": false,\n \"in_reply_to_user_id\": null,\n \"place\": null,\n \"source\": \"web\",\n \"user\": {\n \"name\": \"Taylor Singletary\",\n \"profile_sidebar_fill_color\": \"FBFBFB\",\n \"profile_background_tile\": true,\n \"profile_sidebar_border_color\": \"000000\",\n \"profile_image_url\": \"http://a0.twimg.com/profile_images/2546730059/f6a8zq58mg1hn0ha8vie_normal.jpeg\",\n \"created_at\": \"Wed Mar 07 22:23:19 +0000 2007\",\n \"location\": \"San Francisco, CA\",\n \"follow_request_sent\": false,\n \"id_str\": \"819797\",\n \"is_translator\": false,\n \"profile_link_color\": \"c71818\",\n \"entities\": {\n \"url\": {\n \"urls\": [\n {\n \"expanded_url\": \"http://www.rebelmouse.com/episod/\",\n \"url\": \"http://t.co/Lxw7upbN\",\n \"indices\": [\n 0,\n 20\n ],\n \"display_url\": \"rebelmouse.com/episod/\"\n }\n ]\n },\n \"description\": {\n \"urls\": [\n]\n }\n },\n \"default_profile\": false,\n \"url\": \"http://t.co/Lxw7upbN\",\n \"contributors_enabled\": false,\n \"favourites_count\": 15990,\n \"utc_offset\": -28800,\n \"profile_image_url_https\": \"https://si0.twimg.com/profile_images/2546730059/f6a8zq58mg1hn0ha8vie_normal.jpeg\",\n \"id\": 819797,\n \"listed_count\": 340,\n \"profile_use_background_image\": true,\n \"profile_text_color\": \"D20909\",\n \"followers_count\": 7126,\n \"lang\": \"en\",\n \"protected\": false,\n \"geo_enabled\": true,\n \"notifications\": false,\n \"description\": \"Reality Technician, Twitter API team, synthesizer enthusiast; a most excellent adventure in timelines. I know it's hard to believe in something you can't see.\",\n \"profile_background_color\": \"000000\",\n \"verified\": false,\n \"time_zone\": \"Pacific Time (US & Canada)\",\n \"profile_background_image_url_https\": \"https://si0.twimg.com/profile_background_images/643655842/hzfv12wini4q60zzrthg.png\",\n \"statuses_count\": 18076,\n \"profile_background_image_url\": \"http://a0.twimg.com/profile_background_images/643655842/hzfv12wini4q60zzrthg.png\",\n \"default_profile_image\": false,\n \"friends_count\": 5444,\n \"following\": true,\n \"show_all_inline_media\": true,\n \"screen_name\": \"episod\"\n },\n \"in_reply_to_screen_name\": null,\n \"in_reply_to_status_id\": null\n }\n```", "```py\n type processedTweet struct {\n anaconda.Tweet\n// post processed stuff\n ids []int // to implement Document\n textVec []float64\n normTextVec []float64\n location []float64\n isRT bool\n }\n```", "```py\n type Tweet struct {\n Contributors []int64 `json:\"contributors\"`\n Coordinates *Coordinates `json:\"coordinates\"`\n CreatedAt string `json:\"created_at\"`\n DisplayTextRange []int `json:\"display_text_range\"`\n Entities Entities `json:\"entities\"`\n ExtendedEntities Entities `json:\"extended_entities\"`\n ExtendedTweet ExtendedTweet `json:\"extended_tweet\"`\n FavoriteCount int `json:\"favorite_count\"`\n Favorited bool `json:\"favorited\"`\n FilterLevel string `json:\"filter_level\"`\n FullText string `json:\"full_text\"`\n HasExtendedProfile bool `json:\"has_extended_profile\"`\n Id int64 `json:\"id\"`\n IdStr string `json:\"id_str\"`\n InReplyToScreenName string `json:\"in_reply_to_screen_name\"`\n InReplyToStatusID int64 `json:\"in_reply_to_status_id\"`\n InReplyToStatusIdStr string `json:\"in_reply_to_status_id_str\"`\n InReplyToUserID int64 `json:\"in_reply_to_user_id\"`\n InReplyToUserIdStr string `json:\"in_reply_to_user_id_str\"`\n IsTranslationEnabled bool `json:\"is_translation_enabled\"`\n Lang string `json:\"lang\"`\n Place Place `json:\"place\"`\n QuotedStatusID int64 `json:\"quoted_status_id\"`\n QuotedStatusIdStr string `json:\"quoted_status_id_str\"`\n QuotedStatus *Tweet `json:\"quoted_status\"`\n PossiblySensitive bool `json:\"possibly_sensitive\"`\n PossiblySensitiveAppealable bool `json:\"possibly_sensitive_appealable\"`\n RetweetCount int `json:\"retweet_count\"`\n Retweeted bool `json:\"retweeted\"`\n RetweetedStatus *Tweet `json:\"retweeted_status\"`\n Source string `json:\"source\"`\n Scopes map[string]interface{} `json:\"scopes\"`\n Text string `json:\"text\"`\n User User `json:\"user\"`\n WithheldCopyright bool `json:\"withheld_copyright\"`\n WithheldInCountries []string `json:\"withheld_in_countries\"`\n WithheldScope string `json:\"withheld_scope\"`\n }\n```", "```py\n func mock() []*processedTweet {\n f, err := os.Open(\"example.json\")\n dieIfErr(err)\n return load(f)\n }\n func load(r io.Reader) (retVal []*processedTweet) {\n dec := json.NewDecoder(r)\n dieIfErr(dec.Decode(&retVal))\n return retVal\n }\n```", "```py\n func dieIfErr(err error) {\n if err != nil {\n log.Fatal(err)\n }\n }\n```", "```py\n func main(){\n tweets := mock()\n for _, tweet := range tweets {\n fmt.Printf(\"%q\\n\", tweet.FullText)\n }\n }\n```", "```py\n $ go run *.go\n \"just another test\"\n \"lecturing at the \\\"analyzing big data with twitter\\\" class at @cal with @othman http://t.co/bfj7zkDJ\"\n \"You'd be right more often if you thought you were wrong.\"\n```", "```py\n| Tweet ID | twitter | test | right | wrong |\n |:--------:|:------:|:----:|:----:|:---:|\n | 1 | 0 | 1 | 0 | 0 |\n | 2 | 1 | 0 | 0 | 0 |\n | 3 | 0 | 0 | 1 | 1 |\n```", "```py\n type processor struct {\n tfidf *tfidf.TFIDF\n corpus *corpus.Corpus\n locations map[string]int\n t transform.Transformer\n locCount int\n }\n```", "```py\n func newProcessor() *processor {\n c, err := corpus.Construct(corpus.WithWords([]string{mention, hashtag, retweet, url}))\n dieIfErr(err)\n return &processor{\n tfidf: tfidf.New(),\n corpus: c,\n locations: make(map[string]int),\n }\n }\n```", "```py\n const (\n mention = \"<@MENTION>\"\n hashtag = \"<#HASHTAG>\"\n retweet = \"<RETWEET>\"\n url = \"<URL>\"\n )\n```", "```py\n func (p *processor) process(a []*processedTweet) {\n for _, tt := range a {\n for _, word := range strings.Fields(tt.FullText) {\n wordID, ok := p.single(word)\n if ok {\n tt.ids = append(tt.ids, wordID)\n }\nif isRT(word) {\n tt.isRT = true\n }\n }\n p.tfidf.Add(tt)\n }\np.tfidf.CalculateIDF()\n // calculate scores\n for _, tt := range a {\n tt.textVec = p.tfidf.Score(tt)\n }\n// normalize text vector\n size := p.corpus.Size()\n for _, tt := range a {\n tt.normTextVec = make([]float64, size)\n for i := range tt.ids {\n tt.normTextVec[tt.ids[i]] = tt.textVec[i]\n }\n }\n }\n```", "```py\n for _, word := range strings.Fields(tt.FullText) {\n wordID, ok := p.single(word)\n if ok {\n tt.ids = append(tt.ids, wordID)\n }\n }\n```", "```py\n func (p *processor) single(a string) (wordID int, ok bool) {\n word := strings.ToLower(a)\n if _, ok = stopwords[word]; ok {\n return -1, false\n }\n if strings.HasPrefix(word, \"#\") {\n return p.corpus.Add(hashtag), true\n }\n if strings.HasPrefix(word, \"@\") {\n return p.corpus.Add(mention), true\n }\n if strings.HasPrefix(word, \"http://\") {\n return p.corpus.Add(url), true\n }\n if isRT(word) {\n return p.corpus.Add(retweet), false\n }\nreturn p.corpus.Add(word), true\n }\n```", "```py\n package main\nimport (\n \"fmt\"\n \"unicode\"\n\"golang.org/x/text/transform\"\n \"golang.org/x/text/unicode/norm\"\n )\nfunc isMn(r rune) bool { return unicode.Is(unicode.Mn, r) }\nfunc main() {\n str1 := \"cafe\"\n str2 := \"café\"\n str3 := \"cafe\\u0301\"\n fmt.Println(str1 == str2)\n fmt.Println(str2 == str3) \nt := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)\n str1a, _, _ := transform.String(t, str1)\n str2a, _, _ := transform.String(t, str2)\n str3a, _, _ := transform.String(t, str3)\nfmt.Println(str1a == str2a)\n fmt.Println(str2a == str3a)\n }\n```", "```py\nt := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)\n```", "```py\nfunc isMn(r rune) bool { return unicode.Is(unicode.Mn, r) }\n```", "```py\n type processor struct {\n tfidf *tfidf.TFIDF\n corpus *corpus.Corpus\n transformer transformer.Transformer\n locations map[string]int\n locCount int\n }\nfunc newProcessor() *processor {\n c, err := corpus.Construct(corpus.WithWords([]string{mention, hashtag, retweet, url}))\n dieIfErr(err)\nt := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)\n return &processor{\n tfidf: tfidf.New(),\n corpus: c,\n transformer: t,\n locations: make(map[string]int),\n }\n }\n```", "```py\n func (p *processor) single(a string) (wordID int, ok bool) {\n word := strings.ToLower(a)\n```", "```py\n func (p *processor) single(a string) (wordID int, ok bool) {\n word, _, err := transform.String(p.transformer, a)\n dieIfErr(err)\n word = strings.ToLower(word)\n```", "```py\nif _, ok = stopwords[word]; ok {\n return -1, false\n }\n```", "```py\nconst sw = `a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot can't cant co co. computer con could couldnt couldn't cry de describe detail did didn didn't didnt do does doesn doesn't doesnt doing don done down due during each eg e.g eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt hasn't hasn have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie i.e. if in inc indeed interest into is it its itself just keep kg km last latter latterly least less ltd made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put quite rather re really regarding same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under unless until up upon us used using various very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves`\n```", "```py\nvar stopwords = make(map[string]struct{})\nfunc init() {\n for _, s := range strings.Split(sw, \" \") {\n stopwords[s] = struct{}{}\n }\n }\n```", "```py\n if strings.HasPrefix(word, \"#\") {\n return p.corpus.Add(hashtag), true\n }\n if strings.HasPrefix(word, \"@\") {\n return p.corpus.Add(mention), true\n }\n if strings.HasPrefix(word, \"http://\") {\n return p.corpus.Add(url), true\n }\n```", "```py\n switch {\n case strings.HasPrefix(word, \"#\"):\n return p.corpus.Add(hashtag), true\n case strings.HasPrefix(word, \"@\"):\n if len(word) == 0 {\n return p.corpus.Add(\"at\"), true\n }\n return p.corpus.Add(mention), true\n case strings.HasPrefix(word, \"http\"):\n return p.corpus.Add(url), true\n }\n```", "```py\n if word == \"rt\" {\n return p.corpus.Add(retweet), false\n }\n```", "```py\n for _, tt := range a {\n for _, word := range strings.Fields(tt.FullText) {\n wordID, ok := p.single(word)\n if ok {\n tt.ids = append(tt.ids, wordID)\n }\nif word == \"rt\" {\n tt.isRT = true\n }\n }\n p.tfidf.Add(tt)\n }\n```", "```py\n func main() {\n tweets := mock()\n p := newProcessor()\n p.process(tweets)\n// create a clusterer\n c, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)\n dieIfErr(err)\ndata := asMatrix(tweets)\n dieIfErr(c.Learn(data))clusters := c.Guesses()\n for i, clust := range clusters{\n fmt.Printf(\"%d: %q\\n\", clust, tweets[i].FullText)\n }\n }\n```", "```py\n tweets := mock()\n p := newProcessor()\n p.process(tweets)\n```", "```py\n // create a clusterer\n c, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)\n dieIfErr(err)\n```", "```py\n data := asMatrix(tweets)\n dieIfErr(c.Learn(data))\n```", "```py\n clusters := c.Guesses()\n for i, clust := range clusters{\n fmt.Printf(\"%d: %q\\n\", clust, tweets[i].FullText)\n }\n```", "```py\nc, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)\n```", "```py\nc, err := clusters.DBSCAN(eps, minPts, clusters.EuclideanDistance)\n```", "```py\n| | A | B | C | ... |\n |--|--|--|--|--|--|\n | A | | | | |\n | B | | | | |\n | C | | | | |\n | ... | | | | |\n```", "```py\n func knn(a [][]float64, k int, distance func(a, b []float64) float64) ([][]float64, []float64) {\n var distances [][]float64\n for _, row := range a {\n var dists []float64\n for _, row2 := range a {\n dist := distance(row, row2)\n dists = append(dists, dist)\n }\n sort.Sort(sort.Float64Slice(dists))\n topK := dists[:k]\n distances = append(distances, topK)\n }\n var lastCol []float64\n for _, d := range distances {\n l := d[len(d)-1]\n lastCol = append(lastCol, l)\n }\n sort.Sort(sort.Float64Slice(lastCol))\n return distances, lastCol\n }\n```", "```py\n var distances [][]float64\n for _, row := range a {\n var dists []float64\n for _, row2 := range a {\n dist := distance(row, row2)\n dists = append(dists, dist)\n }\n```", "```py\n sort.Sort(sort.Float64Slice(dists))\n topK := dists[:k]\n distances = append(distances, topK)\n```", "```py\n func plotKNNDist(a []float64) plotter.XYs {\n points := make(plotter.XYs, len(a))\n for i, val := range a {\n points[i].X = float64(i)\n points[i].Y = val\n }\n return points\n }\n```", "```py\n const (\n ACCESSTOKEN = \"_____\"\n ACCESSTOKENSECRET = \"______\"\n CONSUMERKEY = \"_____\"\n CONSUMERSECRET = \"_______\"\n )\nfunc main() {\n twitter := anaconda.NewTwitterApiWithCredentials(ACCESSTOKEN, ACCESSTOKENSECRET, CONSUMERKEY, CONSUMERSECRET)\n raw, err := twitter.GetHomeTimeline(nil)\nf, err := os.OpenFile(\"dev.json\", os.O_TRUNC|os.O_WRONLY|os.O_CREATE, 0644)\n dieIfErr(err)\n enc := json.NewEncoder(f)\n enc.Encode(raw)\n f.Close()\n }\n```", "```py\n twitter := anaconda.NewTwitterApiWithCredentials(ACCESSTOKEN, ACCESSTOKENSECRET, CONSUMERKEY, CONSUMERSECRET)\n raw, err := twitter.GetHomeTimeline(nil)\n```", "```py\ntype Values map[string][]string\n```", "```py\n f, err := os.OpenFile(\"dev.json\", os.O_TRUNC|os.O_WRONLY|os.O_CREATE, 0644)\n dieIfErr(err)\n enc := json.NewEncoder(f)\n enc.Encode(raw)\n f.Close()\n```", "```py\nfunc main() {\n f, err := os.Open(\"dev.json\")\n dieIfErr(err)\n tweets := load(f)\n p := newProcessor()\n tweets = p.process(tweets)\nexpC := 20\n distances, last := knn(asMatrix(tweets), expC, clusters.EuclideanDistance)\n log.Printf(\"distances %v | %v\", distances, last)\n// plot for DBSCAN elbows\n plt, err := plot.New()\n dieIfErr(err)\n plotutil.AddLinePoints(plt, \"KNN Distance\", plotKNNDist(last))\n plt.Save(25*vg.Centimeter, 25*vg.Centimeter, \"KNNDist.png\")\n// actually do the clustering\n dmmClust := dmm(tweets, expC, p.corpus.Size())\n kmeansClust := kmeans(tweets, expC)\n dbscanClust, clustCount := dbscan(tweets)\n// print output\n log.Printf(\"len(tweets)%d\", len(tweets))\n var buf bytes.Buffer\nbc := byClusters2(dmmClust, expC)\n lc, tweetCount := largestCluster2(dmmClust)\n fmt.Fprintf(&buf, \"Largest Cluster %d - %d tweets\\n\", lc, tweetCount)\n for i, t := range bc {\n fmt.Fprintf(&buf, \"CLUSTER %d: %d\\n\", i, len(t))\n for _, c := range t {\n fmt.Fprintf(&buf, \"\\t%v\\n\", tweets[c].clean2)\n }\n }\n fmt.Fprintf(&buf, \"==============\\n\")\n bc2 := byClusters(kmeansClust, expC)\n for i, t := range bc2 {\n fmt.Fprintf(&buf, \"CLUSTER %d: %d\\n\", i, len(t))\n for _, c := range t {\n fmt.Fprintf(&buf, \"\\t%v\\n\", tweets[c].clean2)\n }\n }\n fmt.Fprintf(&buf, \"==============\\n\")\n bc3 := byClusters(dbscanClust, clustCount)\n for i, t := range bc3 {\n fmt.Fprintf(&buf, \"CLUSTER %d: %d\\n\", i, len(t))\n for _, c := range t {\n fmt.Fprintf(&buf, \"\\t%v\\n\", tweets[c].clean2)\n }\n }\nlog.Println(buf.String())\n }\n```", "```py\n func dmm(a []*processedTweet, expC int, corpusSize int) []dmmclust.Cluster {\n conf := dmmclust.Config{\n K: expC,\n Vocabulary: corpusSize,\n Iter: 1000,\n Alpha: 0.0,\n Beta: 0.01,\n Score: dmmclust.Algorithm4,\n Sampler: dmmclust.NewGibbs(rand.New(rand.NewSource(1337))),\n }\n dmmClust, err := dmmclust.FindClusters(toDocs(a), conf)\n dieIfErr(err)\n return dmmClust\n }\nfunc kmeans(a []*processedTweet, expC int) []int {\n // create a clusterer\n kmeans, err := clusters.KMeans(100000, expC, clusters.EuclideanDistance)\n dieIfErr(err)\n data := asMatrix(a)\n dieIfErr(kmeans.Learn(data))\n return kmeans.Guesses()\n }\nfunc dbscan(a []*processedTweet) ([]int, int) {\n dbscan, err := clusters.DBSCAN(5, 0.965, 8, clusters.EuclideanDistance)\n dieIfErr(err)\n data := asMatrix(a)\n dieIfErr(dbscan.Learn(data))\n clust := dbscan.Guesses()\ncounter := make(map[int]struct{})\n for _, c := range clust {\n counter[c] = struct{}{}\n }\n return clust, len(counter)\n }\nfunc largestCluster(clusters []int) (int, int) {\n cc := make(map[int]int)\n for _, c := range clusters {\n cc[c]++\n }\nvar retVal, maxVal int\nfor k, v := range cc {\n if v > maxVal {\n retVal = k\n maxVal = v\n }\n }\n return retVal, cc[retVal]\n }\nfunc largestCluster2(clusters []dmmclust.Cluster) (int, int) {\n cc := make(map[int]int)\n for _, c := range clusters {\n cc[c.ID()]++\n }\nvar retVal, maxVal int\nfor k, v := range cc {\n if v > maxVal {\n retVal = k\n maxVal = v\n }\n }\n return retVal, cc[retVal]\n }\nfunc byClusters(a []int, expectedClusters int) (retVal [][]int) {\n if expectedClusters == 0 {\n return nil\n }\n retVal = make([][]int, expectedClusters)\n var i, v int\n defer func() {\n if r := recover(); r != nil {\n log.Printf(\"exp %v | %v\", expectedClusters, v)\n panic(r)\n }\n }()\n for i, v = range a {\n if v == -1 {\n // retVal[0] = append(retVal[0], i)\n continue\n }\n retVal[v-1] = append(retVal[v-1], i)\n }\n return retVal\n }\nfunc byClusters2(a []dmmclust.Cluster, expectedClusters int) (retVal [][]int) {\n retVal = make([][]int, expectedClusters)\n for i, v := range a {\n retVal[v.ID()] = append(retVal[v.ID()], i)\n }\n return retVal\n }\n```", "```py\n$ EuclideanDistance(\\mathbf{q},\\mathbf{p}) = \\sqrt{\\sum_{i=1}^n (q_i-p_i)^2}.$\n```", "```py\n$ d_J(A,B) = 1 - J(A,B) = { { |A \\cup B| - |A \\cap B| } \\over |A \\cup B| } $\n```", "```py\n func jaccard(a, b []float64) float64 {\n setA, setB := make(map[int]struct{}), make(map[int]struct{})\n union := make(map[int]struct{})\n for i := range a {\n if a[i] != 0 {\n union[i] = struct{}{}\n setA[i] = struct{}{}\n }\n }\nfor i := range b {\n if b[i] != 0 {\n union[i] = struct{}{}\n setB[i] = struct{}{}\n }\n }\nintersection := 0.0\n for k := range setA {\n if _, ok := setB[k]; ok {\n intersection++\n }\n }\nreturn 1 - (intersection / float64(len(union)))\n }\n```", "```py\n var nl = regexp.MustCompile(\"\\n+\")\n var ht = regexp.MustCompile(\"&.+?;\")\nfunc (p *processor) single(word string) (wordID int, ok bool) {\n if _, ok = stopwords[word]; ok {\n return -1, false\n }\n switch {\n case strings.HasPrefix(word, \"#\"):\n word = strings.TrimPrefix(word, \"#\")\n case word == \"@\":\n return -1, false // at is a stop word!\n case strings.HasPrefix(word, \"http\"):\n return -1, false\n }\nif word == \"rt\" {\n return -1, false\n }\nreturn p.corpus.Add(word), true\n }\nfunc (p *processor) process(a []*processedTweet) []*processedTweet {\n // remove things from consideration\n i := 0\n for _, tt := range a {\n if tt.Lang == \"en\" {\n a[i] = tt\n i++\n }\n }\n a = a[:i]\nvar err error\n for _, tt := range a {\n if tt.RetweetedStatus != nil {\n tt.Tweet = *tt.RetweetedStatus\n }\ntt.clean, _, err = transform.String(p.transformer, tt.FullText)\n dieIfErr(err)\n tt.clean = strings.ToLower(tt.clean)\n tt.clean = nl.ReplaceAllString(tt.clean, \"\\n\")\n tt.clean = ht.ReplaceAllString(tt.clean, \"\")\n tt.clean = stripPunct(tt.clean)\n log.Printf(\"%v\", tt.clean)\n for _, word := range strings.Fields(tt.clean) {\n // word = corpus.Singularize(word)\n wordID, ok := p.single(word)\n if ok {\n tt.ids = append(tt.ids, wordID)\n tt.clean2 += \" \"\n tt.clean2 += word\n }\nif word == \"rt\" {\n tt.isRT = true\n }\n }\n p.tfidf.Add(tt)\n log.Printf(\"%v\", tt.clean2)\n }\np.tfidf.CalculateIDF()\n // calculate scores\n for _, tt := range a {\n tt.textVec = p.tfidf.Score(tt)\n }\n// normalize text vector\n size := p.corpus.Size()\n for _, tt := range a {\n tt.normTextVec = make([]float64, size)\n for i := range tt.ids {\n tt.normTextVec[tt.ids[i]] = tt.textVec[i]\n }\n }\n return a\n }\nfunc stripPunct(a string) string {\n const punct = \",.?;:'\\\"!’*-“\"\n return strings.Map(func(r rune) rune {\n if strings.IndexRune(punct, r) < 0 {\n return r\n }\n return -1\n }, a)\n }\n```", "```py\n tt.clean = strings.ToLower(tt.clean)\n tt.clean = nl.ReplaceAllString(tt.clean, \"\\n\")\n tt.clean = ht.ReplaceAllString(tt.clean, \"\")\n tt.clean = stripPunct(tt.clean)\n```", "```py\n if tt.RetweetedStatus != nil {\n tt.Tweet = *tt.RetweetedStatus\n }\n```", "```py\n // remove things from consideration\n i := 0\n for _, tt := range a {\n if tt.Lang == \"en\" {\n a[i] = tt\n i++\n }\n }\n a = a[:i]\n```"]