["```py\npip install feature-engine\n```", "```py\nconda install -c conda-forge feature_engine\n```", "```py\npip install category_encoders\n```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    X_train[\"A4\"].unique()\n    ```", "```py\n    A4 into *k-1* binary variables using pandas and then inspect the first five rows of the resulting DataFrame:\n\n    ```", "```py\n\n    ```", "```py\n      Missing        l        u        y\n596     False  False   True  False\n303     False  False   True  False\n204     False  False  False   True\n351     False  False  False   True\n118     False  False   True  False\n```", "```py\n    X_train_enc = pd.get_dummies(X_train, drop_first=True)\n    X_test_enc = pd.get_dummies(X_test, drop_first=True)\n    ```", "```py\n    X_train_enc.head()\n    ```", "```py\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    ```", "```py\n    cat_vars = X_train.select_dtypes(\n        include=\"O\").columns.to_list()\n    ```", "```py\n    encoder = OneHotEncoder(drop=\"first\",\n        sparse_output=False)\n    ```", "```py\n    ct = ColumnTransformer(\n        [(\"encoder\", encoder, cat_vars)],\n        remainder=\"passthrough\",\n        force_int_remainder_cols=False,\n    ).set_output(transform=\"pandas\")\n    ```", "```py\n    ct.fit(X_train)\n    ```", "```py\n    ct.named_transformers_[\"encoder\"].categories_\n    ```", "```py\n    X_train_enc = ct.transform(X_train)\n    X_test_enc = ct.transform(X_test)\n    ```", "```py\n    ct.get_feature_names_out()\n    ```", "```py\n    from feature_engine.encoding import OneHotEncoder\n    ```", "```py\n    ohe_enc = OneHotEncoder(drop_last=True)\n    ```", "```py\n    ohe_enc.fit(X_train)\n    ```", "```py\n    ohe_enc.variables_\n    ```", "```py\n    ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n    ```", "```py\n    ohe_enc.encoder_dict_\n    ```", "```py\n     {'A1': ['a', 'b'],\n     'A4': ['u', 'y', 'Missing'],\n     'A5': ['g', 'p', 'Missing'],\n     'A6': ['c', 'q', 'w', 'ff', 'm', 'i', 'e', 'cc', 'x', 'd', 'k', 'j', 'Missing', 'aa'],\n     'A7': ['v', 'ff', 'h', 'dd', 'z', 'bb', 'j', 'Missing', 'n'],\n     'A9': ['t'],\n     'A10': ['t'],\n     'A12': ['t'],\n     'A13': ['g', 's']}\n    ```", "```py\n    X_train_enc = ohe_enc.transform(X_train)\n    X_test_enc = ohe_enc.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    X_train[\"A6\"].unique()\n    ```", "```py\n    A6, sort them in decreasing order, and then display the five most frequent categories:\n\n    ```", "```py\n\n    We can see the five most frequent categories and the number of observations per category in the following output:\n\n    ```", "```py\n    top_5 = [x for x in X_train[\n    \" A6\"].value_counts().sort_values(\n    ascending=False).head(5).index\n    ]\n    ```", "```py\n\n    ```", "```py\n    X_train_enc = X_train.copy()\n    X_test_enc = X_test.copy()\n    for label in top_5:\n        X_train_enc[f\"A6_{label}\"] = np.where(\n            X_train[\"A6\"] == label, 1, 0)\n        X_test_enc[f\"A6_{label}\"] = np.where(\n            X_test[\"A6\"] == label, 1, 0)\n    ```", "```py\n    X_train_enc[[\"A6\"] + [f\"A6_{\n        label}\" for label in top_5]].head(10)\n    ```", "```py\n          A6  A6_c  A6_q  A6_w  A6_i  A6_ff\n    596   c      1      0      0      0        0\n    303   q      0      1      0      0        0\n    204   w      0      0      1      0        0\n    351  ff      0      0      0      0        1\n    118   m      0      0      0      0        0\n    247   q      0      1      0      0        0\n    652   i      0      0      0      1        0\n    513   e      0      0      0      0        0\n    230  cc      0      0      0      0        0\n    scikit-learn.\n    ```", "```py\n    from sklearn.preprocessing import OneHotEncoder\n    ```", "```py\n    encoder = OneHotEncoder(\n        min_frequency=39,\n        max_categories=5,\n        sparse_output=False,\n    ).set_output(transform=\"pandas\")\n    ```", "```py\n    X_train_enc = encoder.fit_transform(X_train[\n        ['A6', 'A7']])\n    X_test_enc = encoder.transform(X_test[['A6', 'A7']])\n    ```", "```py\n    From feature_engine.encoding import OneHotEncoder\n    ohe_enc = OneHotEncoder(\n        top_categories=5,\n        variables=[\"A6\", \"A7\"]\n    )\n    ```", "```py\n    ohe_enc.fit(X_train)\n    ```", "```py\n    X_train_enc = ohe_enc.transform(X_train)\n    X_test_enc = ohe_enc.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from feature_engine.encoding import CountFrequencyEncoder\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    counts = X_train[\"A7\"].value_counts().to_dict()\n    ```", "```py\n{'v': 277, 'h': 101, 'ff': 41, 'bb': 39, 'z': 7, 'dd': 5, 'j': 5, 'Missing': 4, 'n': 3, 'o': 1}\n```", "```py\n    X_train_enc = X_train.copy()\n    X_test_enc = X_test.copy()\n    X_train_enc[\"A7\"] = X_train_enc[\"A7\"].map(counts)\n    X_test_enc[\"A7\"] = X_test_enc[\"A7\"].map(counts)\n    ```", "```py\n    count_enc = CountFrequencyEncoder(\n        encoding_method=\"count\", variables=None,\n    )\n    ```", "```py\n    count_enc.fit(X_train)\n    ```", "```py\n    count_enc.variables_\n    ```", "```py\n    ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n    ```", "```py\n    count_enc.encoder_dict_\n    ```", "```py\n    X_train_enc = count_enc.transform(X_train)\n    X_test_enc = count_enc.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    ordinal_mapping = {k: i for i, k in enumerate(\n        X_train[\"A7\"].unique(), 0)\n    }\n    ```", "```py\n    {'v': 0, 'ff': 1, 'h': 2, 'dd': 3, 'z': 4, 'bb': 5, 'j': 6, 'Missing': 7, 'n': 8, 'o': 9}\n    ```", "```py\n    X_train_enc = X_train.copy()\n    X_test_enc = X_test.copy()\n    X_train_enc[\"A7\"] = X_train_enc[\"A7\"].map(ordinal_mapping)\n    X_test_enc[\"A7\"] = X_test_enc[\"A7\"].map(ordinal_mapping)\n    ```", "```py\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.compose import ColumnTransformer\n    ```", "```py\n    enc = OrdinalEncoder()\n    ```", "```py\n    cat_vars = X_train.select_dtypes(include=\"O\").columns.to_list()\n    ```", "```py\n    ct = ColumnTransformer(\n        [(\"encoder\", enc, cat_vars)],\n        remainder=\"passthrough\",\n        force_int_remainder_cols=False,\n    ).set_output(transform=\"pandas\")\n    ```", "```py\n    ct.fit(X_train)\n    ```", "```py\n    X_train_enc = ct.transform(X_train)\n    X_test_enc = ct.transform(X_test)\n    ```", "```py\n    from feature_engine.encoding import OrdinalEncoder\n    ```", "```py\n    enc = OrdinalEncoder(\n        encoding_method=\"arbitrary\",\n        variables=cat_vars,\n    )\n    ```", "```py\n    enc.fit(X_train)\n    ```", "```py\n    X_train_enc = enc.transform(X_train)\n    X_test_enc = enc.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    y_train.groupby(X_train[\"A7\"]).mean().sort_values()\n    ```", "```py\n    A7\n    o          0.000000\n    ff         0.146341\n    j          0.200000\n    dd         0.400000\n    v          0.418773\n    bb         0.512821\n    h          0.603960\n    n          0.666667\n    z          0.714286\n    Missing    1.000000\n    Name: target, dtype: float64\n    ```", "```py\n    ordered_labels = y_train.groupby(\n        X_train[\"A7\"]).mean().sort_values().index\n    ```", "```py\n    ordinal_mapping = {\n        k: i for i, k in enumerate(ordered_labels, 0)\n    }\n    ```", "```py\n    A7 in a copy of the datasets:\n\n    ```", "```py\n\n    ```", "```py\n    y_train.groupby(X_train[\"A7\"]).mean().plot()\n    plt.title(\"Relationship between A7 and the target\")\n    plt.ylabel(\"Mean of target\")\n    plt.show()\n    ```", "```py\n    y_train.groupby(X_train_enc[\"A7\"]).mean().plot()\n    plt.title(\"Relationship between A7 and the target\")\n    plt.ylabel(\"Mean of target\")\n    plt.show()\n    ```", "```py\n    from feature_engine.encoding import OrdinalEncoder\n    ```", "```py\n    ordinal_enc = OrdinalEncoder(\n        encoding_method=\"ordered\",\n        variables=None)\n    ```", "```py\n    ordinal_enc.fit(X_train, y_train)\n    ```", "```py\n    X_train_enc = ordinal_enc.transform(X_train)\n    X_test_enc = ordinal_enc.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    from sklearn.preprocessing import TargetEncoder\n    from sklearn.compose import ColumnTransformer\n    ```", "```py\n    cat_vars = X_train.select_dtypes(\n        include=\"O\").columns.to_list()\n    ```", "```py\n    enc = TargetEncoder(smooth=\"auto\", random_state=9)\n    ```", "```py\n    ct = ColumnTransformer(\n        [(\"encoder\", enc, cat_vars)],\n        remainder=\"passthrough\",\n    ).set_output(transform=\"pandas\")\n    ```", "```py\n    X_train_enc = ct.fit_transform(X_train, y_train)\n    X_test_enc = ct.transform(X_test)\n    ```", "```py\n    from feature_engine.encoding import MeanEncoder\n    ```", "```py\n    mean_enc = MeanEncoder(smoothing=\"auto\",\n        variables=None)\n    ```", "```py\n    mean_enc.fit(X_train, y_train)\n    ```", "```py\n    X_train_enc = mean_enc.transform(X_train)\n    X_test_enc = mean_enc.transform(X_test)\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    neg_y_train = pd.Series(\n        np.where(y_train == 1, 0, 1),\n        index=y_train.index\n    )\n    ```", "```py\n    total_pos = y_train.sum()\n    total_neg = neg_y_train.sum()\n    ```", "```py\n    pos = y_train.groupby(\n        X_train[\"A1\"]).sum() / total_pos\n    neg = neg_y_train.groupby(\n        X_train[\"A1\"]).sum() / total_neg\n    ```", "```py\n    woe = np.log(pos/neg)\n    ```", "```py\n    A1\n    Missing    0.203599\n    a          0.092373\n    b         -0.042410\n    A1 with the WoE in a copy of the datasets:\n\n    ```", "```py\n\n    You can inspect the encoded variable by executing `X_train_enc[\"A1\"].head()`.Now, let’s perform WoE encoding using `feature-engine`.\n    ```", "```py\n    from feature_engine.encoding import WoEEncoder\n    ```", "```py\n    woe_enc = WoEEncoder(variables = [\"A1\", \"A9\", \"A12\"])\n    ```", "```py\n    woe_enc.fit(X_train, y_train)\n    ```", "```py\n    X_train_enc = woe_enc.transform(X_train)\n    X_test_enc = woe_enc.transform(X_test)\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from feature_engine.encoding import RareLabelEncoder\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    freqs = X_train[\"A7\"].value_counts(normalize=True)\n    ```", "```py\n    v 0.573499\n    h 0.209110\n    ff 0.084886\n    bb 0.080745\n    z 0.014493\n    dd 0.010352\n    j 0.010352\n    Missing 0.008282\n    n 0.006211\n    o 0.002070\n    z, dd, j, Missing, n, and o are rare categories.\n    ```", "```py\n    frequent_cat = [\n        x for x in freqs.loc[freqs > 0.05].index.values]\n    ```", "```py\n    Rare string in a copy of the datasets:\n\n    ```", "```py\n\n    ```", "```py\n    X_train[\"A7\"].value_counts(normalize=True)\n    ```", "```py\n    v       0.573499\n    h       0.209110\n    ff      0.084886\n    bb      0.080745\n    Rare    0.051760\n    feature-engine.\n    ```", "```py\n    rare_encoder = RareLabelEncoder(tol=0.05,\n        n_categories=4)\n    ```", "```py\n    rare_encoder.fit(X_train)\n    ```", "```py\n    X_train_enc = rare_encoder.transform(X_train)\n    X_test_enc = rare_encoder.transform(X_test)\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from category_encoders.binary import BinaryEncoder\n    ```", "```py\n    data = pd.read_csv(\"credit_approval_uci.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(labels=[\"target\"], axis=1),\n        data[\"target\"],\n        test_size=0.3,\n        random_state=0,\n    )\n    ```", "```py\n    X_train[\"A7\"].unique()\n    ```", "```py\n    A7:\n\n    ```", "```py\n\n    ```", "```py\n    encoder.fit(X_train)\n    ```", "```py\n    X_train_enc = encoder.transform(X_train)\n    X_test_enc = encoder.transform(X_test)\n    ```"]