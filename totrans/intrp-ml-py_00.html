<html><head></head><body>
  <div id="_idContainer006">
    <h1 id="_idParaDest-5" class="mainHeading">Preface</h1>
    <p class="normal">The title of this book suggests its central themes: <strong class="keyWord">interpretation</strong>, <strong class="keyWord">machine learning</strong>, and <strong class="keyWord">Python</strong>, with the first theme being the most crucial.</p>
    <p class="normal">So, why is interpretation so important?</p>
    <p class="normal"><strong class="keyWord">Interpretable machine learning</strong>, often referred to as <strong class="keyWord">Explainable AI</strong> (<strong class="keyWord">XAI</strong>), encompasses a growing array of techniques that help us glean insights from models, aiming to ensure they are safe, fair, and reliable – a goal I believe we all share for our models.</p>
    <p class="normal">With the rise of AI superseding traditional software and even human tasks, machine learning models are viewed as a more advanced form of software. While they operate on binary data, they aren’t typical software; their logic isn’t explicitly coded by developers but emerges from data patterns. This is where interpretation steps in, helping us understand these models, pinpoint their errors, and rectify them before any potential mishaps. Thus, interpretation is essential in fostering trust and ethical considerations in these models. And it’s worth noting that in the not-so-distant future, training models might move away from coding to more intuitive drag-and-drop interfaces. In this context, understanding machine learning models becomes an invaluable skill.</p>
    <p class="normal">Currently, there’s still a significant amount of coding involved in data preprocessing, exploration, model training, and deployment. And while this book is rich with Python examples, it’s not merely a coding guide removed from practical applications or the bigger picture. The book’s essence is to prioritize the <em class="italic">why</em> before the <em class="italic">how</em> when it comes to <strong class="keyWord">interpretable machine learning</strong>, as interpretation revolves around the question of <em class="italic">why</em>.</p>
    <p class="normal">Therefore, most chapters of this book kickoff by outlining a mission (the <em class="italic">why</em>) and then delving into the methodology (the <em class="italic">how</em>). The aim is to achieve the mission using the techniques discussed in the chapter, with an emphasis on understanding the results. The chapters wrap up by pondering on the practical insights gained from the exercises.</p>
    <p class="normal">The structure of this book is progressive, starting from the basics and moving to more intricate topics. The tools utilized in this book are open source and are products of leading research institutions like Microsoft, Google, and IBM. Even though interpretability is a vast research field with many aspects still in the developmental phase, this book doesn’t aim to cover it all. Its primary goal is to delve deeply into a selection of interpretability tools, making it beneficial for those working in the machine learning domain.</p>
    <p class="normal">The book’s initial section introduces interpretability, emphasizing its significance in the business landscape and discussing its core components and challenges. The subsequent section provides a detailed overview of various interpretation techniques and their applications, whether it’s for classification, regression, tabular data, time series, images, or text. In the final section, readers will engage in practical exercises on model tuning and data training for interpretability, focusing on simplifying models, addressing biases, setting constraints, and ensuring dependability.</p>
    <p class="normal">By the book’s conclusion, readers will be adept at using interpretability techniques to gain deeper insights into machine learning models.</p>
    <h1 id="_idParaDest-6" class="heading-1">Who this book is for</h1>
    <p class="normal">This book caters to a diverse audience, including: </p>
    <ul>
      <li class="bulletList">Data professionals who face the growing challenge of explaining the functioning of AI systems they create and manage and seek ways to enhance them.</li>
      <li class="bulletList">Data scientists and machine learning professionals aiming to broaden their expertise by learning model interpretation techniques and strategies to overcome model challenges from fairness to robustness.</li>
      <li class="bulletList">Aspiring data scientists who have a basic grasp of machine learning and proficiency in Python.</li>
      <li class="bulletList">AI ethics officers aiming to deepen their knowledge of the practical aspects of their role to guide their initiatives more effectively.</li>
      <li class="bulletList">AI project supervisors and business leaders eager to integrate interpretable machine learning in their operations, aligning with the values of fairness, responsibility, and transparency.</li>
    </ul>
    <h1 id="_idParaDest-7" class="heading-1">What this book covers</h1>
    <p class="normal"><em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, introduces machine learning interpretation and related concepts, such as interpretability, explainability, black-box models, and transparency, providing definitions for these terms to avoid ambiguity. We then underpin the value of machine learning interpretability for businesses.</p>
    <p class="normal"><em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, uses a cardiovascular disease prediction example to introduce two fundamental concepts (feature importance and decision regions) and the most important taxonomies used to classify interpretation methods. We also detail what elements hinder machine learning interpretability as a primer for what lies ahead.</p>
    <p class="normal"><em class="chapterRef">Chapter 3</em>, <em class="italic">Interpretation Challenges</em>, discusses the traditional methods used for machine learning interpretation for regression and classification with a flight delay prediction problem. We will then examine the limitations of these traditional methods and explain what makes “white-box” models intrinsically interpretable and why we cannot always use white-box models. To answer this question, we consider the trade-off between prediction performance and model interpretability. Finally, we will discover some new “glass-box” models that attempt to not compromise in this trade-off.</p>
    <p class="normal"><em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, explores <strong class="keyWord">Partial Dependence Plots</strong> (<strong class="keyWord">PDP</strong>) and game-theory-inspired <strong class="keyWord">SHapley Additive exPlanations</strong> (<strong class="keyWord">SHAP</strong>) with used car pricing regression models, then visualizes conditional marginal distribution <strong class="keyWord">Accumulated Local Effects</strong> (<strong class="keyWord">ALE</strong>) plots.</p>
    <p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>, covers local interpretation methods, explaining a single or a group of predictions. To this end, the chapter covers how to leverage SHAP and <strong class="keyWord">Local Interpretable Model-agnostic Explanations</strong> (<strong class="keyWord">LIME</strong>) for local interpretations with a chocolate bar rating example, with both tabular and text data.</p>
    <p class="normal"><em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, continues with local model interpretations, but only for classification problems. We use a recidivism risk prediction example to understand how we can explain unfair predictions in a human-interpretable way. This chapter covers anchors, counterfactuals, and the <strong class="keyWord">What-If-Tool</strong> (<strong class="keyWord">WIT</strong>).</p>
    <p class="normal"><em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, explores interpretation methods that work with <strong class="keyWord">Convolutional Neural Network</strong> (<strong class="keyWord">CNN</strong>) models with a garbage classifier model. Once we have grasped how a CNN learns with activations, we will study several gradient-based attribution methods, such as saliency maps, Grad-CAM, and integrated gradients, to debug class attribution. Lastly, we will extend our attribution debugging know-how with perturbation-based attribution methods such as feature ablation, occlusion sensitivity, Shapley value sampling, and KernelSHAP.</p>
    <p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">Interpreting NLP Transformers</em>, discusses how to visualize attention mechanisms in a restaurant review sentiment classifier transformer model, followed by interpreting integrated gradient attributions and exploring the <strong class="keyWord">Learning Interpretability Tool</strong> (<strong class="keyWord">LIT</strong>).</p>
    <p class="normal"><em class="chapterRef">Chapter 9</em>, <em class="italic">Interpretation Methods for Multivariate Forecasting and Sensitivity Analysis</em>, uses a traffic forecasting problem and <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) models to show how to employ integrated gradients and SHAP for this use case. Lastly, the chapter looks at how forecasting and uncertainty are intrinsically linked, and sensitivity analysis – a family of methods designed to measure the uncertainty of a model’s output in relation to its input. We study two methods: Morris for factor prioritization and Sobol for factor fixing.</p>
    <p class="normal"><em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>, uses a challenging non-profit direct mailing optimization problem to review filter-based feature selection methods, such as Spearman’s correlation and learn about embedded methods, such as Lasso. Then, you will discover wrapper methods, such as sequential feature selection and hybrid ones, such as recursive feature elimination, as well as more advanced ones, such as genetic algorithms. Lastly, even though feature engineering is typically conducted before selection, there’s value in exploring feature engineering for many reasons after the dust has settled.</p>
    <p class="normal"><em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>, takes a credit card default problem to demonstrate leveraging fairness metrics and visualizations to detect undesired bias. Then, the chapter looks at how to reduce it via preprocessing methods such as reweighting and prejudice remover for in-processing and equalized odds for post-processing. Then, we test treatments for lowering credit card default and leverage causal modeling to determine their <strong class="keyWord">Average Treatment Effects</strong> (<strong class="keyWord">ATE</strong>) and <strong class="keyWord">Conditional Average Treatment Effects</strong> (<strong class="keyWord">CATE</strong>). Finally, we test causal assumptions and the robustness of estimates.</p>
    <p class="normal"><em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>, continues with the recidivism risk prediction problem from <em class="chapterRef">Chapter 7</em>. We will learn how to place guardrails with feature engineering on the data side and monotonic and interaction constraints on the model to ensure fairness while also learning how to tune a model when there are several objectives.</p>
    <p class="normal"><em class="chapterRef">Chapter 13</em>, <em class="italic">Adversarial Robustness</em>, uses a face mask detection problem to cover an end-to-end adversarial solution. An adversary can purposely thwart a model in many ways, and we focus on evasion attacks, such as Carlini and Wagner infinity-norm and adversarial patches, and briefly explain other forms of attack. We explain two defense methods: spatial smoothing preprocessing and adversarial training. Lastly, we demonstrate a robustness evaluation method.</p>
    <p class="normal"><em class="chapterRef">Chapter 14</em>, <em class="italic">What’s Next for Machine Learning Interpretability?</em>, summarizes what was learned in the context of the ecosystem of machine learning interpretability methods. And then speculates on what’s to come next!</p>
    <h1 id="_idParaDest-8" class="heading-1">To get the most out of this book</h1>
    <ul>
      <li class="bulletList">You will need a Jupyter environment with Python 3.9+. You can do either of the following:<ul>
          <li class="bulletList">Install one on your machine locally via <strong class="keyWord">Anaconda Navigator</strong> or from scratch with <code class="inlineCode">pip</code>.</li>
          <li class="bulletList">Use a cloud-based one, such as <strong class="keyWord">Google Colaboratory</strong>, <strong class="keyWord">Kaggle Notebooks</strong>, <strong class="keyWord">Azure Notebooks</strong>, or <strong class="keyWord">Amazon Sagemaker</strong>.</li>
        </ul>
      </li>
      <li class="bulletList">The instructions on how to get started will vary accordingly, so we strongly suggest that you search online for the latest instructions for setting them up.</li>
      <li class="bulletList">For instructions on installing the many packages employed throughout the book, please go to the GitHub repository, which will have the updated instructions in the <code class="inlineCode">README.MD</code> file. We expect these to change over time, given how often packages change. We also tested the code with specific versions detailed in the <code class="inlineCode">README.MD</code>, so should anything fail with later versions, please install the specific version instead.</li>
      <li class="bulletList">Individual chapters have instructions on how to check that the right packages are installed.</li>
      <li class="bulletList">But depending on the way <strong class="keyWord">Jupyter</strong> was set up, installing packages might be best done through the <strong class="keyWord">command line</strong> or using <code class="inlineCode">conda</code>, so we suggest you adapt these installation instructions to suit your needs.</li>
      <li class="bulletList">If you are using the digital version of this book, type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</li>
      <li class="bulletList">If you are not a machine learning practitioner or are a beginner, it is best to read the book sequentially since many concepts are only explained in great detail in earlier chapters. Practitioners skilled in machine learning but not acquainted with interpretability can skim the first three chapters to get the ethical context and concept definitions required to make sense of the rest, but read the rest of the chapters in order. As for advanced practitioners with foundations in interpretability, reading the book in any order should be fine.</li>
      <li class="bulletList">As for the code, you can read the book without running the code simultaneously or strictly for the theory. But if you plan to run the code, it is best to do it with the book as a guide to assist with the interpretation of outcomes and strengthen your understanding of the theory.</li>
      <li class="bulletList">While reading the book, think of ways you could use the tools learned, and by the end of it, hopefully, you will be inspired to put this newly gained knowledge into action!</li>
    </ul>
    <h2 id="_idParaDest-9" class="heading-2">Download the example code files</h2>
    <p class="normal">The code bundle for the book is hosted on GitHub at <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/</span></a>. In case there’s an update to the code, it will be updated on the existing GitHub repository. You can also find the hardware and software list of requirements on the repository in the <code class="inlineCode">README.MD</code> file.</p>
    <p class="normal">We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/"><span class="url">https://github.com/PacktPublishing/</span></a>. Check them out!</p>
    <h2 id="_idParaDest-10" class="heading-2">Download the color images</h2>
    <p class="normal">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://packt.link/gbp/9781803235424"><span class="url">https://packt.link/gbp/9781803235424</span></a>.</p>
    <h2 id="_idParaDest-11" class="heading-2">Conventions used</h2>
    <p class="normal">There are several text conventions used throughout this book.</p>
    <p class="normal"><code class="inlineCode">CodeInText</code>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter/X handles. For example: “Next, let’s define a <code class="inlineCode">device</code> variable because if you have a CUDA-enabled GPU model, inference will perform quicker.”</p>
    <p class="normal">A block of code is set as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, dataset</span>):
    self.model.<span class="hljs-built_in">eval</span>() 
    device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available()\
                          <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
    <span class="hljs-keyword">with</span> torch.no_grad(): 
        loader = torch.utils.data.DataLoader(dataset, batch_size = <span class="hljs-number">32</span>)
</code></pre>
    <p class="normal">When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, dataset</span>):
    self.model.<span class="hljs-built_in">eval</span>()
    <span class="code-highlight"><strong class="hljs-slc">device </strong></span>= torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available()\
                          <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
    <span class="hljs-keyword">with</span> torch.no_grad(): 
        loader = torch.utils.data.DataLoader(dataset, batch_size = <span class="hljs-number">32</span>)
</code></pre>
    <p class="normal">Any command-line input or output is written as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install torch
</code></pre>
    <p class="normal"><strong class="keyWord">Bold</strong>: Indicates a new term, an important word, or words that you see on the screen. For instance, words in menus or dialog boxes appear in the text like this. For example: “The <strong class="screenText">Predictions</strong> tab is selected, and this tab has a <strong class="screenText">Data Table</strong> to the left where you can select and pin individual data points and a pane with <strong class="screenText">Classification Results</strong> to the left.”</p>
    <div class="note">
      <p class="normal">Warnings or important notes appear like this.</p>
    </div>
    <div class="packt_tip">
      <p class="normal">Tips and tricks appear like this.</p>
    </div>
    <h1 id="_idParaDest-12" class="heading-1">Get in touch</h1>
    <p class="normal">Feedback from our readers is always welcome.</p>
    <p class="normal"><strong class="keyWord">General feedback</strong>: Email <code class="inlineCode">feedback@packtpub.com</code> and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at <code class="inlineCode">questions@packtpub.com</code>.</p>
    <p class="normal"><strong class="keyWord">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you reported this to us. Please visit <a href="http://www.packtpub.com/submit-errata"><span class="url">http://www.packtpub.com/submit-errata</span></a>, click <strong class="keyWord">Submit Errata</strong>, and fill in the form.</p>
    <p class="normal"><strong class="keyWord">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <code class="inlineCode">copyright@packtpub.com</code> with a link to the material.</p>
    <p class="normal"><strong class="keyWord">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com"><span class="url">http://authors.packtpub.com</span></a>.</p>
  </div>
  <div id="_idContainer007" class="Basic-Text-Frame">
    <h1 id="_idParaDest-13" class="heading-1">Share your thoughts</h1>
    <p class="normal">Once you’ve read <em class="italic">Interpretable Machine Learning with Python 2e</em>, we’d love to hear your thoughts! Please <a href="https://www.packtpub.com/"><span class="url">click here to go straight to the Amazon review page</span></a> for this book and share your feedback.</p>
    <p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
  </div>

	<p class="eop"/>

  <div id="_idContainer009" class="Basic-Text-Frame">
    <h1 id="_idParaDest-14" class="heading-1">Download a free PDF copy of this book</h1>
    <p class="normal">Thanks for purchasing this book!</p>
    <p class="normal">Do you like to read on the go but are unable to carry your print books everywhere?</p>
    <p class="normal">Is your eBook purchase not compatible with the device of your choice?</p>
    <p class="normal">Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.</p>
    <p class="normal">Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application. </p>
    <p class="normal">The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily</p>
    <p class="normal">Follow these simple steps to get the benefits:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Scan the QR code or visit the link below</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B18406_Free_PDF.png" alt="" role="presentation"/></figure>
    <p class="packt_figref"><a href="https://packt.link/free-ebook/9781803235424"><span class="url">https://packt.link/free-ebook/9781803235424</span></a></p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="2">Submit your proof of purchase</li>
      <li class="numberedList">That’s it! We’ll send your free PDF and other benefits to your email directly</li>
    </ol>
  </div>
</body></html>