<html><head></head><body>
		<div id="_idContainer062">
			<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Introduction to Machine Learning with C++</h1>
			<p>There are different approaches to making computers solve tasks. One of them is to define an explicit algorithm, and another one is to use implicit strategies based on mathematical and statistical methods. <strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) is one of the implicit methods that uses mathematical and statistical approaches to solve tasks. It is an actively growing discipline, and a lot of scientists and researchers find it to be one of the best ways to move forward toward systems acting as human-level <strong class="bold">artificial </strong><span class="No-Break"><strong class="bold">intelligence</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AI</strong></span><span class="No-Break">).</span></p>
			<p>In general, ML approaches have the idea of searching patterns in a given dataset as their basis. Consider a recommendation system for a news feed, which provides the user with a personalized feed based on their previous activity or preferences. The software gathers information about the type of news article the user reads and calculates some statistics. For example, it could be the frequency of some topics appearing in a set of news articles. Then, it performs some predictive analytics, identifies general patterns, and uses them to populate the user’s news feed. Such systems periodically track a user’s activity, update the dataset, and calculate new trends <span class="No-Break">for recommendations.</span></p>
			<p>ML is a rapidly growing field with diverse applications across various industries. In healthcare, it analyzes medical data to detect patterns and predict diseases and treatment outcomes. In finance, it aids in credit scoring, fraud detection, risk assessment, portfolio optimization, and algorithmic trading, enhancing decision-making and operations. E-commerce benefits from recommendation systems that suggest products based on customer behavior, boosting sales and satisfaction. Autonomous vehicles use ML for environmental perception, decision-making, and <span class="No-Break">safe navigation.</span></p>
			<p>Customer service is improved with chatbots and virtual assistants that handle queries and tasks. Cybersecurity leverages ML to detect and prevent cyberattacks by analyzing network traffic and identifying threats. Language translation tools use ML for accurate and efficient text translation. Image recognition, powered by computer vision algorithms, identifies objects, faces, and scenes in images and videos, supporting applications such as facial recognition and content moderation. Speech recognition in voice assistants such as Siri, Google Assistant, and Alexa relies on ML for understanding and responding to user commands. These examples illustrate the vast potential of ML in shaping <span class="No-Break">our lives.</span></p>
			<p>This chapter describes what ML is and which tasks can be solved with ML, and discusses different approaches used in ML. It aims to show the minimally required math to start implementing ML algorithms. It also covers how to perform basic <strong class="bold">linear algebra</strong> operations in libraries such as <strong class="source-inline">Eigen</strong>, <strong class="source-inline">xtensor</strong>, <strong class="source-inline">ArrayFire</strong>, <strong class="source-inline">Blaze</strong>, and <strong class="source-inline">Dlib</strong>, and also explains the linear regression task as <span class="No-Break">an example.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Understanding the fundamentals <span class="No-Break">of ML</span></li>
				<li>An overview of <span class="No-Break">linear algebra</span></li>
				<li>An overview of a linear <span class="No-Break">regression example</span><a id="_idTextAnchor017"/></li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor018"/>Understanding the fundamentals of ML</h1>
			<p>There are different approaches to creating and training ML models. In this section, we show what these approaches are and how they differ. Apart from the approach we use to create an ML model, there<a id="_idIndexMarker000"/> are also parameters that manage how this model behaves in the training and evaluation processes. Model parameters can be divided into two distinct groups, which should be configured in different ways. The first group of parameters is the model weights in ML algorithms that are used to adjust the model’s predictions. They are assigned numerical values during the training process, and these values determine how the model makes decisions or predictions based on new data. The second group is the model hyperparameters that control the behavior of an ML model during training. They are not learned from the data like other parameters in the model but, rather, are set by the user or algorithm before training begins. The last crucial part of the ML process is the technique that we use to train a model. Usually, the training technique uses some numerical optimization algorithm that finds the minimal value of a target function. In ML, the target function is usually called a loss<a id="_idIndexMarker001"/> function and is used for penalizing the training algorithm when it makes errors. We discuss these concepts more precisely in the <span class="No-Break">following section<a id="_idTextAnchor019"/>s<a id="_idTextAnchor020"/>.</span></p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor021"/>Venturing into the techniques of ML</h2>
			<p>We can divide ML approaches into two techniques, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Supervised learning</strong> is an approach <a id="_idIndexMarker002"/>based on the use of labeled data. Labeled data is a set of known data samples with corresponding known target outputs. Such data is used to build a model that can predict <span class="No-Break">future outputs.</span></li>
				<li><strong class="bold">Unsupervised learning</strong> is an approach that does not require labeled data and can search hidden patterns and structures in an arbitrary kind <span class="No-Break">of data.</span></li>
			</ul>
			<p>Let’s have a look at each of the techniques <span class="No-Break">in detai<a id="_idTextAnchor022"/><a id="_idTextAnchor023"/>l.</span></p>
			<h3>Supervised learning</h3>
			<p>Supervised ML algorithms usually take a limited set of labeled data and build models that can make reasonable predictions<a id="_idIndexMarker003"/> for new data. We can split supervised learning algorithms into two main parts, classification and regression techniques, described <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker004"/></span><span class="No-Break">follows:</span></p>
			<ul>
				<li>Classification models predict some finite and distinct types of categories—this could be a label that identifies whether an email is spam or not, or whether an image contains a human face or not. Classification models are applied in speech and text recognition, object identification on images, credit scoring, and others. Typical algorithms for creating classification models are <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>), decision tree approaches, <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">KNN</strong>), logistic regression, Naive Bayes, and neural networks. The following chapters describe the details of some of <span class="No-Break">these algorithms.</span></li>
				<li>Regression models predict continuous responses such as changes in temperature or values of currency exchange rates. Regression models are applied in algorithmic trading, forecasting of electricity load, revenue prediction, and others. Creating a regression model usually makes sense if the output of the given labeled data is real numbers. Typical algorithms for creating regression models are linear and multivariate regressions, polynomial regression models, and stepwise regressions. We can use decision tree techniques and neural networks to create<a id="_idIndexMarker005"/> regression <span class="No-Break">models too.</span></li>
			</ul>
			<p>The following chapters describe the details of some of <span class="No-Break">these algor<a id="_idTextAnchor024"/>i<a id="_idTextAnchor025"/>thms.</span></p>
			<h3>Unsupervised learning</h3>
			<p>Unsupervised learning algorithms do<a id="_idIndexMarker006"/> not use labeled datasets. They create models that use intrinsic relations in data to find hidden patterns that they can use for making predictions. The most well-known unsupervised learning technique is <strong class="bold">clustering</strong>. Clustering involves dividing a given set of data into a limited number<a id="_idIndexMarker007"/> of groups according to some intrinsic properties of data items. Clustering is applied in market research, different types of exploratory analysis, <strong class="bold">deoxyribonucleic acid</strong> (<strong class="bold">DNA</strong>) analysis, image segmentation, and object detection. Typical algorithms for creating models for performing clustering are k-means, k-medoids, Gaussian mixture models, hierarchical clustering, and hidden Markov models. Some of these algorithms are explained in the following chapters of <span class="No-Break">th<a id="_idTextAnchor026"/>i<a id="_idTextAnchor027"/>s book.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor028"/>Dealing with ML models</h2>
			<p>We can interpret ML models as functions that take different types of parameters. Such functions provide outputs for given inputs based on the values of these parameters. Developers can configure the behavior <a id="_idIndexMarker008"/>of ML models for solving problems by adjusting model parameters. Training an ML model can usually be treated as a process of searching for the best combination of its parameters. We can split the ML model’s parameters into two types. The first type consists of parameters internal to the model, and we can estimate their values from the training (input) data. The second type consists of parameters external to the model, and we cannot estimate their values from training data. Parameters that <a id="_idIndexMarker009"/>are external to the model are usually <span class="No-Break">called </span><span class="No-Break"><strong class="bold">hyperparameters</strong></span><span class="No-Break">.</span></p>
			<p>Internal parameters have the <span class="No-Break">following characteristics:</span></p>
			<ul>
				<li>They are necessary for <span class="No-Break">making predictions</span></li>
				<li>They define the quality of the model on the <span class="No-Break">given problem</span></li>
				<li>We can learn them from <span class="No-Break">training data</span></li>
				<li>Usually, they are a part of <span class="No-Break">the model</span></li>
			</ul>
			<p>If the model contains a fixed number of internal parameters, it is called <strong class="bold">parametric</strong>. Otherwise, we can<a id="_idIndexMarker010"/> classify it <span class="No-Break">as </span><span class="No-Break"><strong class="bold">non-parametric</strong></span><span class="No-Break">.</span></p>
			<p>Examples of internal<a id="_idIndexMarker011"/> parameters are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Weights of <strong class="bold">artificial neural </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ANNs</strong></span><span class="No-Break">)</span></li>
				<li>Support vector values<a id="_idIndexMarker012"/> for <span class="No-Break">SVM models</span></li>
				<li>Polynomial coefficients for linear regression or <span class="No-Break">logistic regression</span></li>
			</ul>
			<p>ANNs are computer systems inspired by the structure and function of biological neural networks in the human brain. They are composed of interconnected nodes, or neurons, that process and transmit information. ANNs are designed to learn patterns and relationships from data, allowing them to make predictions or decisions based on new inputs. The learning process<a id="_idIndexMarker013"/> involves adjusting the weights and biases of the connections between neurons to improve the accuracy of <span class="No-Break">the model.</span></p>
			<p>On the other hand, hyperparameters have the <span class="No-Break">following characteristics:</span></p>
			<ul>
				<li>They are used to configure algorithms that estimate <span class="No-Break">model parameters</span></li>
				<li>The practitioner usually <span class="No-Break">specifies them</span></li>
				<li>Their estimation is often based on <span class="No-Break">using heuristics</span></li>
				<li>They are specific to a concrete <span class="No-Break">modeling problem</span></li>
			</ul>
			<p>It is hard to know the best values for a model’s hyperparameters for a specific problem. Also, practitioners usually need to perform additional research on how to tune required hyperparameters so that a model or a training algorithm behaves in the best way. Practitioners use rules of thumb, copying values from similar projects, as well as special techniques such as grid search for <span class="No-Break">hyperparameter estimation.</span></p>
			<p>Examples of hyperparameters are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>C and sigma parameters used in the SVM algorithm for a classification <span class="No-Break">quality configuration</span></li>
				<li>The learning rate parameter that is used in the neural network training process to configure <span class="No-Break">algorithm convergence</span></li>
				<li>The <em class="italic">k</em> value that is used <a id="_idIndexMarker014"/>in the KNN algorithm to configure the numb<a id="_idTextAnchor029"/>e<a id="_idTextAnchor030"/>r <span class="No-Break">of neighbors</span></li>
			</ul>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor031"/>Model parameter estimation</h2>
			<p><strong class="bold">Model parameter estimation</strong> usually uses some optimization algorithm. The speed and quality of the resulting model can significantly depend on the optimization algorithm chosen. Research on optimization algorithms is a popular topic in industry, as well as in academia. ML often uses optimization <a id="_idIndexMarker015"/>techniques and algorithms based on the optimization of a loss function. A function that evaluates how well a model predicts the data is called a <strong class="bold">loss function</strong>. If predictions are very different from the target outputs, the<a id="_idIndexMarker016"/> loss function will return a value that can be interpreted as a bad one, usually a large number. In such a way, the loss function penalizes an optimization algorithm when it moves in the wrong direction. So, the general idea is to minimize the value of the loss function to reduce penalties. There is no single universal loss function for optimization algorithms. Different factors determine how to choose a loss function. Examples of such factors are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Specifics of the given problem—for example, whether it is a regression or a <span class="No-Break">classification model</span></li>
				<li>Ease of <span class="No-Break">calculating derivatives</span></li>
				<li>Percentage of outliers in <span class="No-Break">the dataset</span></li>
			</ul>
			<p>In ML, the term <strong class="bold">optimizer</strong> is used to define an algorithm that connects a loss function and a technique for updating model parameters in<a id="_idIndexMarker017"/> response to the values of the loss function. So, optimizers tune ML models to predict target values for new data in the most accurate way by fitting model parameters. Optimizers or optimization algorithms play a crucial role in training ML models. They help find the best parameters for a model, which can improve its performance and accuracy. They have wide applications in various fields such as image recognition, natural language processing, and fraud detection. For example, in image classification tasks, optimization algorithms can be used to train deep neural networks to accurately identify objects in images. There are many optimizers: gradient descent, Adagrad, RMSProp, Adam, and others. Moreover, developing new optimizers is an active area of research. For example, there is the <em class="italic">ML and Optimization</em> research group at Microsoft (located in Redmond) whose research areas include combinatorial optimization, convex and non-convex optimization, and their application in ML and AI. Other companies in the industry also have similar research groups; there are <a id="_idIndexMarker018"/>many publications from Facebook Research, Amazon Research, <a id="_idTextAnchor032"/>and <span class="No-Break">OpenAI groups.</span></p>
			<p>Now, we will learn about what ML is and what its main conceptual parts are. So let’s learn the most important part of its mathematical basemen<a id="_idTextAnchor033"/>t: <span class="No-Break">linear algebra.</span></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor034"/>An overview of linear algebra</h1>
			<p>The concepts of linear algebra are<a id="_idIndexMarker019"/> essential for understanding the theory behind ML because they help us understand how ML algorithms work under the hood. Also, most ML algorithm definitions use linear <span class="No-Break">algebra terms.</span></p>
			<p>Linear algebra is not only a handy mathematical instrument but also the concepts of linear algebra can be very efficiently implemented with modern computer architectures. The rise of ML, and especially deep learning, began after significant performance improvement of the <a id="_idIndexMarker020"/>modern <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>). GPUs were initially designed to work with linear algebra concepts and massively parallel computations used in computer games. After that, special libraries were created to work with general linear algebra concepts. Examples of libraries that implement basic linear algebra routines are <strong class="source-inline">Cuda</strong> and <strong class="source-inline">OpenCL</strong>, and one example of a specialized linear <a id="_idIndexMarker021"/>algebra library is <strong class="source-inline">cuBLAS</strong>. Moreover, it became more common to use <strong class="bold">general-purpose graphics processing units</strong> (<strong class="bold">GPGPUs</strong>) because these turn the computational power of a modern GPU into a powerful general-purpose <span class="No-Break">computing resource.</span></p>
			<p>Also, <strong class="bold">central processing units</strong> (<strong class="bold">CPUs</strong>) have instruction <a id="_idIndexMarker022"/>sets specially designed for simultaneous numerical computations. Such computations are called <strong class="bold">vectorized</strong>, and common<a id="_idIndexMarker023"/> vectorized instruction sets are <strong class="source-inline">AVx</strong>, <strong class="source-inline">SSE</strong>, and <strong class="source-inline">MMx</strong>. There is also the term <strong class="bold">single instruction multiple data</strong> (<strong class="bold">SIMD</strong>) for these<a id="_idIndexMarker024"/> instruction sets. Many numeric linear algebra libraries, such as <strong class="source-inline">Eigen</strong>, <strong class="source-inline">xtensor</strong>, <strong class="source-inline">VienaCL</strong>, and others, use them to improve <span class="No-Break">c<a id="_idTextAnchor035"/>o<a id="_idTextAnchor036"/>mputational performance.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor037"/>Learning the concepts of linear algebra</h2>
			<p>Linear algebra is a big area. It is the section of algebra that studies objects of a linear nature: vector (or linear) spaces, linear representations, and systems of linear equations. The main tools used in linear algebra are determinants, matrices, conjugation, and <span class="No-Break">tensor calculus.</span></p>
			<p>To understand ML algorithms, we only need a small set of linear algebra concepts. However, to do research on new ML algorithms, a practitioner should have a deep understanding of linear algebra <span class="No-Break">and calculus.</span></p>
			<p>The following list contains the most valuable<a id="_idIndexMarker025"/> linear algebra concepts for understanding <span class="No-Break">ML algorithms:</span></p>
			<ul>
				<li><strong class="bold">Scalar</strong>: This is a<a id="_idIndexMarker026"/> <span class="No-Break">single number.</span></li>
				<li><strong class="bold">Vector</strong>: This is an array of <a id="_idIndexMarker027"/>ordered numbers. Each element has a distinct index. Notation for vectors is a bold<a id="_idIndexMarker028"/> lowercase typeface for names and an italic typeface with a subscript for elements, as shown in the <span class="No-Break">following example:</span><div class="IMG---Figure" id="_idContainer008"><img alt="" role="presentation" src="image/B19849_Formula_01.jpg"/></div></li>
			</ul>
			<ul>
				<li><strong class="bold">Matrix</strong>: This is a two-dimensional array<a id="_idIndexMarker029"/> of numbers. Each element has a distinct pair of indices. Notation for <a id="_idIndexMarker030"/>matrices is a bold uppercase typeface for names and an italic but not bold typeface with a comma-separated list of indices in subscripts for elements, as shown in the <span class="No-Break">following example:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer009">
					<img alt="" role="presentation" src="image/B19849_Formula_02.jpg"/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Tensor</strong>: This is an array <a id="_idIndexMarker031"/>of numbers arranged in a multidimensional regular grid, and represents generalizations<a id="_idIndexMarker032"/> of matrices. It is like a multidimensional matrix. For example, tensor <em class="italic">A</em> with dimensions 2 x 2 x 2 can look <span class="No-Break">like this:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer010">
					<img alt="" role="presentation" src="image/B19849_Formula_03.jpg"/>
				</div>
			</div>
			<p>Linear algebra libraries and ML frameworks usually use the concept of a tensor instead of a matrix because they implement general algorithms, and a matrix is just a special case of a tensor with two dimensions. Also, we can consider a vec<a id="_idTextAnchor038"/>t<a id="_idTextAnchor039"/>or as a matrix of size <em class="italic">n</em> <span class="No-Break">x </span><span class="No-Break"><em class="italic">1</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor040"/>Basic linear algebra operations</h2>
			<p>The most common operations used for <a id="_idIndexMarker033"/>programming linear algebra algorithms are the <span class="No-Break">following ones:</span></p>
			<ul>
				<li><strong class="bold">Element-wise operations</strong>: These are performed in an element-wise manner on vectors, matrices, or tensors of the<a id="_idIndexMarker034"/> same size. The<a id="_idIndexMarker035"/> resulting elements will be the result of operations on corresponding input elements, as <span class="No-Break">shown here:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer011">
					<img alt="" role="presentation" src="image/B19849_Formula_04.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer012">
					<img alt="" role="presentation" src="image/B19849_Formula_05.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer013">
					<img alt="" role="presentation" src="image/B19849_Formula_06.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="" role="presentation" src="image/B19849_Formula_07.jpg"/>
				</div>
			</div>
			<p class="list-inset">The following example shows the <span class="No-Break">element-wise summation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="" role="presentation" src="image/B19849_Formula_08.jpg"/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Dot product</strong>: There are two types of multiplication for tensors and matrices in linear algebra—one is just<a id="_idIndexMarker036"/> element-wise, and the second is the dot product. The dot product deals with two equal-length series of numbers and returns a single number. This operation applied on matrices or tensors requires that the matrix or tensor <em class="italic">A</em> has the same number of columns as the number of rows in the matrix or tensor <em class="italic">B</em>. The following example shows the <a id="_idIndexMarker037"/>dot-product operation in the case when <em class="italic">A</em> is an <em class="italic">n x m</em> matrix and <em class="italic">B</em> is an <em class="italic">m x </em><span class="No-Break"><em class="italic">p</em></span><span class="No-Break"> matrix:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="" role="presentation" src="image/B19849_Formula_09.jpg"/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Transposing</strong>: The transposing of a<a id="_idIndexMarker038"/> matrix is an operation that flips the matrix over its diagonal, which leads to the flipping of the column and row indices of the matrix, resulting in the <a id="_idIndexMarker039"/>creation of a new matrix. In general, it is swapping matrix rows with columns. The following example shows how <span class="No-Break">transposing works:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer017">
					<img alt="" role="presentation" src="image/B19849_Formula_10.jpg"/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Norm</strong>: This operation calculates<a id="_idIndexMarker040"/> the size of the vector; the result of this is <a id="_idIndexMarker041"/>a non-negative real number. The norm formula is <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="" role="presentation" src="image/B19849_Formula_11.jpg"/>
				</div>
			</div>
			<p class="list-inset">The generic name of this type of norm is an <img alt="" role="presentation" src="image/B19849_Formula_12.png"/> norm for <img alt="" role="presentation" src="image/B19849_Formula_13.png"/>. Usually, we use more concrete norms such as an <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm with <em class="italic">p = 2</em>, which is known as the Euclidean norm, and we can interpret it as the Euclidean distance between points. Another widely used norm is the squared <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm, whose calculation formula is <img alt="" role="presentation" src="image/B19849_Formula_16.png"/>. The squared <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm is more suitable for mathematical and computational<a id="_idIndexMarker042"/> operations than the <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm. Each partial derivative of the squared <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm depends only on the corresponding element of <em class="italic">x</em>, in comparison to the partial derivatives of the <img alt="" role="presentation" src="image/B19849_Formula_14.png"/> norm, which depends on the entire vector; this property plays a vital role in optimization algorithms. Another widely used norm operation is the <img alt="" role="presentation" src="image/B19849_Formula_21.png"/> norm with <em class="italic">p = 1</em>, which is commonly used in ML when we<a id="_idIndexMarker043"/> care about the difference between zero and nonzero elements. The <em class="italic">L^1</em> norm is also known as the <span class="No-Break"><strong class="bold">Manhattan distance</strong></span><span class="No-Break">.</span></p>
			<ul>
				<li><strong class="bold">Inverting</strong>: The inverse matrix is such a<a id="_idIndexMarker044"/> matrix that <img alt="" role="presentation" src="image/B19849_Formula_22.png"/>, where <em class="italic">I</em> is an identity matrix. The <strong class="bold">identity matrix</strong> is a matrix that does not change<a id="_idIndexMarker045"/> any vector when we multiply that vector by <span class="No-Break">that </span><span class="No-Break"><a id="_idIndexMarker046"/></span><span class="No-Break">matrix.</span></li>
			</ul>
			<p>We have considered the main linear algebra concepts as well as operations on them. Using this math apparatus, we can define and program many ML algorithms. For example, we can use tensors and matrices to define training datasets for training, and scalars can be used as different types of coefficients. We can use element-wise operations to perform arithmetic operations with a whole dataset (a matrix or a tensor). For example, we can use element-wise multiplication to scale a dataset. We usually use transposing to change a view of a vector or matrix to make them suitable for the dot-product operation. The dot product is usually used to apply a linear function with weights expressed as matrix coefficients to a vector; for example, this vector can be a training sample. Also, dot-product operations are used to update model parameters expressed as matrix or tensor coefficients according to <span class="No-Break">an algorithm.</span></p>
			<p>The norm operation is often used in formulas for loss functions because it naturally expresses the distance concept and can measure the difference between target and predicted values. The inverse matrix is a crucial concept for the analytical solving of linear equations systems. Such systems often appear in different optimization problems. However, calculating the inverse matrix is very <span class="No-Break">c<a id="_idTextAnchor041"/>o<a id="_idTextAnchor042"/>mputationally expensive.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor043"/>Tensor representation in computing</h2>
			<p>We can represent tensor objects in <a id="_idIndexMarker047"/>computer memory in different ways. The most obvious method is a simple linear array in computer memory (<strong class="bold">random-access memory</strong>, or <strong class="bold">RAM</strong>). However, the linear array<a id="_idIndexMarker048"/> is also the most computationally effective data structure for modern CPUs. There are two standard practices to organize tensors with a linear <a id="_idIndexMarker049"/>array in memory: <strong class="bold">row-major ordering</strong> and <span class="No-Break"><strong class="bold">column-major ordering</strong></span><span class="No-Break">.</span></p>
			<p>In row-major ordering, we place<a id="_idIndexMarker050"/> consecutive elements of a row in linear order one after the other, and each row is also placed after the end of the previous one. In column-major ordering, we do the same but with the column elements. Data layouts have a significant impact on computational performance because the speed of traversing an array relies on modern CPU architectures that work with sequential data more efficiently than with non-sequential data. CPU caching effects are the reasons for such behavior. Also, a contiguous data layout makes it possible to use SIMD vectorized instructions that work with sequential <a id="_idIndexMarker051"/>data more efficiently, and we can use them as a type of <span class="No-Break">parallel processing.</span></p>
			<p>Different libraries, even in the same programming language, can use different ordering. For example, <strong class="source-inline">Eigen</strong> uses column-major ordering, but <strong class="source-inline">PyTorch</strong> uses row-major ordering. So, developers should be aware of internal tensor representation in libraries they use, and also take care of this when performing data loading or implementing algorithms <span class="No-Break">from scratch.</span></p>
			<p>Consider the <span class="No-Break">following matrix:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="" role="presentation" src="image/B19849_Formula_23.jpg"/>
				</div>
			</div>
			<p>Then, in the row-major data layout, members of the matrix will have the following layout <span class="No-Break">in memory:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">2</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">3</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">4</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">5</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">a11</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a12</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a22</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a23</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – The row-major data layout example</p>
			<p>In the case of the column-major data layout, order layout will be next, as <span class="No-Break">shown here:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">2</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">3</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">4</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">5</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">a11</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a21</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a12</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a22</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a13</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">a23</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor044"/>Table 1.2 – The column-major data layout example</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor045"/>Linear algebra API samples</h2>
			<p>Let’s consider some C++ linear algebra <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) and look at how we can <a id="_idIndexMarker052"/>use them for creating linear algebra primitives and performing algebra <a id="_idIndexMarker053"/>operations <span class="No-Break">with the<a id="_idTextAnchor046"/>m<a id="_idTextAnchor047"/>.</span></p>
			<h3>Using Eigen</h3>
			<p><strong class="source-inline">Eigen</strong> is a general-purpose linear algebra C++ library. In Eigen, all matrices and vectors are objects of the <strong class="source-inline">Matrix</strong> template class, and the <a id="_idIndexMarker054"/>vector is a specialization of the <a id="_idIndexMarker055"/>matrix type, with either one row or one column. Tensor objects are not presented in official APIs but exist <span class="No-Break">as submodules.</span></p>
			<p>We can define the type for a matrix with known 3 x 3 dimensions and a floating-point data type <span class="No-Break">like this:</span></p>
			<pre class="source-code">
typedef Eigen::Matrix&lt;float, 3, 3&gt; MyMatrix33f;</pre>			<p>We can define a column vector in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
typedef Eigen::Matrix&lt;float, 3, 1&gt; MyVector3f;</pre>			<p>Eigen already has a lot of predefined types for vector and matrix objects—for example, <strong class="source-inline">Eigen::Matrix3f</strong> (floating-point 3 x 3 matrix type) or <strong class="source-inline">Eigen::RowVector2f</strong> (floating-point 1 x 2 vector type). Also, Eigen is not limited to matrices whose dimensions we know at compile time. We can define matrix types that will take the number of rows or columns at initialization during runtime. To define such types, we can use a special type variable for the <strong class="source-inline">Matrix</strong> class template argument named <strong class="source-inline">Eigen::Dynamic</strong>. For example, to define a matrix of doubles with dynamic dimensions, we can use the <span class="No-Break">following definition:</span></p>
			<pre class="source-code">
typedef Eigen::
    Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic&gt;
        MyMatrix;</pre>			<p>Objects initialized from the types<a id="_idIndexMarker056"/> we defined will look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
MyMatrix33f a;
MyVector3f v;
MyMatrix m(10,15);</pre>			<p>To put some values into these objects, we can use several approaches. We can use special predefined initialization<a id="_idIndexMarker057"/> functions, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
a = MyMatrix33f::Zero(); // fill matrix elements with zeros
a = MyMatrix33f::Identity(); // fill matrix as Identity matrix
v = MyVector3f::Random(); // fill matrix elements with random values</pre>			<p>We can use the <em class="italic">comma-initializer</em> syntax, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
a &lt;&lt; 1,2,3,
      4,5,6,
      7,8,9;</pre>			<p>This code construction initializes the matrix values in the <span class="No-Break">following way:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer031">
					<img alt="" role="presentation" src="image/B19849_Formula_24.jpg"/>
				</div>
			</div>
			<p>We can use direct element access to set or change matrix coefficients. The following code sample shows how to use the <strong class="source-inline">()</strong> operator for such <span class="No-Break">an operation:</span></p>
			<pre class="source-code">
a(0,0) = 3;</pre>			<p>We can use the object of the <strong class="source-inline">Map</strong> type to wrap an existent C++ array or vector in the <strong class="source-inline">Matrix</strong> type object. This kind of <a id="_idIndexMarker058"/>mapping object will use memory and values from the underlying object, and will not allocate the additional memory and copy the values. The following snippet shows how to use the <span class="No-Break"><strong class="source-inline">Map</strong></span><span class="No-Break"> type:</span></p>
			<pre class="source-code">
int data[] = {1,2,3,4};
Eigen::Map&lt;Eigen::RowVectorxi&gt; v(data,4);
std::vector&lt;float&gt; data = {1,2,3,4,5,6,7,8,9};
Eigen::Map&lt;MyMatrix33f&gt; a(data.data());</pre>			<p>We can use initialized matrix objects in mathematical operations. Matrix and vector arithmetic operations in the <strong class="source-inline">Eigen</strong> library are offered either through overloads of standard C++ arithmetic operators such as <strong class="source-inline">+</strong>, <strong class="source-inline">-</strong>, or <strong class="source-inline">*</strong>, or through methods such as <strong class="source-inline">dot()</strong> and <strong class="source-inline">cross()</strong>. The following <a id="_idIndexMarker059"/>code sample shows how to express general math operations <span class="No-Break">in Eigen:</span></p>
			<pre class="source-code">
using namespace Eigen;
auto a = Matrix2d::Random();
auto b = Matrix2d::Random();
auto result = a + b;
result = a.array() * b.array(); // element wise multiplication
result = a.array() / b.array();
a += b;
result = a * b; // matrix multiplication
//Also it's possible to use scalars:
a = b.array() * 4;</pre>			<p>Notice that, in Eigen, arithmetic operators such as <strong class="source-inline">+</strong> do not perform any computation by themselves. These operators return an <em class="italic">expression object</em>, which describes what computation to perform. The actual computation happens later when the whole expression is evaluated, typically in the <strong class="source-inline">=</strong> arithmetic operator. It can lead to some strange behaviors, primarily if a <a id="_idIndexMarker060"/>developer uses the <strong class="source-inline">auto</strong> keyword <span class="No-Break">too frequently.</span></p>
			<p>Sometimes, we need to perform operations only on a part of the matrix. For this purpose, Eigen provides the <strong class="source-inline">block</strong> method, which takes four parameters: <strong class="source-inline">i,j,p,q</strong>. These parameters are the block size, <strong class="source-inline">p,q</strong>, and the starting point, <strong class="source-inline">i,j</strong>. The following code shows how to use <span class="No-Break">this method:</span></p>
			<pre class="source-code">
Eigen::Matrixxf m(4,4);
Eigen::Matrix2f b = m.block(1,1,2,2); // copying the middle
                                     //part of matrix
m.block(1,1,2,2) *= 4; // change values in original matrix</pre>			<p>There are two more methods to access rows and columns by index, which are also a type of <strong class="source-inline">block</strong> operation. The following snippet shows how to use the <strong class="source-inline">col</strong> and <span class="No-Break"><strong class="source-inline">row</strong></span><span class="No-Break"> methods:</span></p>
			<pre class="source-code">
m.row(1).array() += 3;
m.col(2).array() /= 4;</pre>			<p>Another important feature of linear algebra libraries is broadcasting, and Eigen supports this with the <strong class="source-inline">colwise</strong> and <strong class="source-inline">rowwise</strong> methods. Broadcasting can be interpreted as a matrix by replicating it in one<a id="_idIndexMarker061"/> direction. Take a look at the following example of how to add a vector to each column of <span class="No-Break">the matrix:</span></p>
			<pre class="source-code">
Eigen::Matrixxf mat(2,4);
Eigen::Vectorxf v(2); // column vector
mat.colwise() += v;</pre>			<p>This operation has the <span class="No-Break">following re<a id="_idTextAnchor048"/>s<a id="_idTextAnchor049"/>ult:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer032">
					<img alt="" role="presentation" src="image/B19849_Formula_25.jpg"/>
				</div>
			</div>
			<h3>Using xtensor</h3>
			<p>The <strong class="source-inline">xtensor</strong> library is a C++ library for numerical analysis with multidimensional array expressions. Containers of xtensor are inspired <a id="_idIndexMarker062"/>by NumPy, the Python array programming library. ML algorithms are mainly described using Python and NumPy, so this library can make it easier to move<a id="_idIndexMarker063"/> them to C++. The following container classes implement multidimensional arrays in the <span class="No-Break"><strong class="source-inline">xtensor</strong></span><span class="No-Break"> library.</span></p>
			<p>The <strong class="source-inline">xarray</strong> type is a dynamically sized multidimensional array, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::vector&lt;size_t&gt; shape = { 3, 2, 4 };
xt::xarray&lt;double, xt::layout_type::row_major&gt; a(shape);</pre>			<p>Dynamic size for the <strong class="source-inline">xarray</strong> type means that this shape can be changed at <span class="No-Break">compilation time.</span></p>
			<p>The <strong class="source-inline">xtensor</strong> type is a multidimensional array whose range is fixed at compilation time. Exact dimension values can be configured in the initialization step, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::array&lt;size_t, 3&gt; shape = { 3, 2, 4 };
xt::xtensor&lt;double, 3&gt; a(shape);</pre>			<p>The <strong class="source-inline">xtensor_fixed</strong> type is a multidimensional array with a dimension shape fixed at compile time, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
xt::xtensor_fixed&lt;double, xt::xshape&lt;3, 2, 4&gt;&gt; a;</pre>			<p>The <strong class="source-inline">xtensor</strong> library also implements arithmetic operators with expression template techniques such as Eigen (this is a common approach for math libraries implemented in C++). So, the computation happens <a id="_idIndexMarker064"/>lazily, and the actual result is calculated when the whole expression <span class="No-Break">is evaluated.</span></p>
			<p><strong class="bold">Lazy computation</strong>, also known as <strong class="bold">lazy evaluation</strong> or <strong class="bold">call-by-need evaluation</strong>, is a strategy in programming where the evaluation of an expression is delayed until its value is <span class="No-Break">actually needed.</span></p>
			<p>This contrasts with eager evaluation, where expressions are evaluated immediately upon encountering them. The container definitions are also expressions. There is also a function to force an expression evaluation named <strong class="source-inline">xt::eval</strong> in the <span class="No-Break"><strong class="source-inline">xtensor</strong></span><span class="No-Break"> library.</span></p>
			<p>There are different kinds of container initialization in the <strong class="source-inline">xtensor</strong> library. Initialization of <strong class="source-inline">xtensor</strong> arrays can be done with C++ initializer lists, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
xt::xarray&lt;double&gt; arr1{{1.0, 2.0, 3.0},
{2.0, 5.0, 7.0},
{2.0, 5.0, 7.0}}; // initialize a 3x3 array</pre>			<p>The <strong class="source-inline">xtensor</strong> library also has<a id="_idIndexMarker065"/> builder functions for special tensor types. The following snippet shows some <span class="No-Break">of them:</span></p>
			<pre class="source-code">
std::vector&lt;uint64_t&gt; shape = {2, 2};
auto x = xt::ones(shape); // creates 2x2 matrix of 1s
auto y = xt::zero(shape); // creates zero 2x2 matrix
auto z = xt::eye(shape);  // creates 2x2 matrix with ones
                          //on the diagonal</pre>			<p>Also, we can map existing C++ arrays into the <strong class="source-inline">xtensor</strong> container with the <strong class="source-inline">xt::adapt</strong> function. This function returns the object that uses the memory and values from the underlying object, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::vector&lt;float&gt; data{1,2,3,4};
std::vector&lt;size_t&gt; shape{2,2};
auto data_x = xt::adapt(data, shape);</pre>			<p>We can use direct access to container elements, with the <strong class="source-inline">()</strong> operator, to set or change tensor values, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::vector&lt;size_t&gt; shape = {3, 2, 4};
xt::xarray&lt;float&gt; a = xt::ones&lt;float&gt;(shape);
a(2,1,3) = 3.14f;</pre>			<p>The <strong class="source-inline">xtensor</strong> library implements<a id="_idIndexMarker066"/> linear algebra arithmetic operations through overloads of standard C++ arithmetic operators such as <strong class="source-inline">+</strong>, <strong class="source-inline">-</strong>, and <strong class="source-inline">*</strong>. To use other operations such as dot-product operations, we have to link an application with the library named <strong class="source-inline">xtensor-blas</strong>. These operators are declared in the <span class="No-Break"><strong class="source-inline">xt::linalg</strong></span><span class="No-Break"> namespace.</span></p>
			<p>The following code shows the use of arithmetic operations with the <span class="No-Break"><strong class="source-inline">xtensor</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
auto a = xt::random::rand&lt;double&gt;({2,2});
auto b = xt::random::rand&lt;double&gt;({2,2});
auto c = a + b;
a -= b;
c = xt::linalg::dot(a,b);
c = a + 5;</pre>			<p>To get partial access to the <strong class="source-inline">xtensor</strong> containers, we can use the <strong class="source-inline">xt::view</strong> function. The <strong class="source-inline">view</strong> function returns a new tensor object that shares the same underlying data with the original tensor but with a different <a id="_idIndexMarker067"/>shape or strides. This allows you to access the data in the tensor in a different way, without actually changing the underlying data itself. The following sample shows how this <span class="No-Break">function works:</span></p>
			<pre class="source-code">
xt::xarray&lt;int&gt; a{
  {1, 2, 3, 4}, 
  {5, 6, 7, 8}, 
  {9, 10, 11, 12}, 
  {13, 14, 15, 16}
};
auto b = xt::view(a, xt::range(1, 3), xt::range(1, 3));</pre>			<p>This operation takes a rectangular block from the tensor, which looks <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer033">
					<img alt="" role="presentation" src="image/B19849_Formula_26.jpg"/>
				</div>
			</div>
			<p>The <strong class="source-inline">xtensor</strong> library implements<a id="_idIndexMarker068"/> automatic broadcasting in most cases. When the operation involves two arrays of different dimensions, it transmits the array with the smaller dimension across the leading dimension of the other array, so we can directly add a vector to a matrix. The following code sample shows how easy <span class="No-Break">it is:</span></p>
			<pre class="source-code">
auto m = xt::random::rand&lt;double&gt;({2,2});
auto v = xt::random::rand&lt;double&gt;({2,1});
a<a id="_idTextAnchor050"/>u<a id="_idTextAnchor051"/>to c = m + v;</pre>			<h3>Using Blaze</h3>
			<p><strong class="source-inline">Blaze</strong> is a general-purpose <a id="_idIndexMarker069"/>high-performance C++ library for dense and sparse linear algebra. There are different classes to<a id="_idIndexMarker070"/> represent matrices and vectors <span class="No-Break">in Blaze.</span></p>
			<p>We can define the type for a matrix with known dimensions and a floating-point data type, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
typedef blaze::
    StaticMatrix&lt;float, 3UL, 3UL, blaze::columnMajor&gt;
        MyMatrix33f;</pre>			<p>We can define a vector in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
typedef blaze::StaticVector&lt;float, 3UL&gt; MyVector3f;</pre>			<p>Also, Blaze is not limited to matrices whose dimensions we know at compile time. We can define matrix types that will take the number of rows or columns at initialization during runtime. To define such types, we <a id="_idIndexMarker071"/>can use the <strong class="source-inline">blaze::DynamicMatrix</strong> or <strong class="source-inline">blaze::DynamicVector</strong> classes. For example, to define a matrix of doubles with dynamic dimensions, we can use the <span class="No-Break">following definition:</span></p>
			<pre class="source-code">
typedef blaze::DynamicMatrix&lt;double&gt; MyMatrix;</pre>			<p>Objects initialized from the types we defined will look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
MyMatrix33f a;
MyVector3f v;
MyMatrix m(10, 15);</pre>			<p>To put some values into these objects, we can use several approaches. We can use special predefined initialization functions, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
a = blaze::zero&lt;float&gt;(3UL, 3UL); //  Zero matrix
a = blaze::IdentityMatrix&lt;float&gt;(3UL); //  Identity matrix
blaze::Rand&lt;float&gt; rnd;
v = blaze::generate(3UL, [&amp;](size_t) { return rnd.generate(); }); 
<strong class="bold">// Random generated vector</strong>
// Matrix filled from the initializer list
a = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};
// Matrix filled with a single value
a = blaze::uniform(3UL, 3UL, 3.f);</pre>			<p>This code construction initializes the matrix values in the <span class="No-Break">following way:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer034">
					<img alt="" role="presentation" src="image/B19849_Formula_27.jpg"/>
				</div>
			</div>
			<p>We can use direct element <a id="_idIndexMarker072"/>access to set or change matrix coefficients. The following code sample shows how to use the <strong class="source-inline">()</strong> operator for such <span class="No-Break">an operation:</span></p>
			<pre class="source-code">
a(0,0) = 3;</pre>			<p>We can use the object of the <strong class="source-inline">blaze::CustomVector</strong> type to wrap an existent C++ array or vector into the <strong class="source-inline">Matrix</strong> or <strong class="source-inline">Vector</strong> type object. This kind of mapping object will use memory and<a id="_idIndexMarker073"/> values from the underlying object, and will not allocate the additional memory and copy the values. The following snippet shows how to use <span class="No-Break">this approach:</span></p>
			<pre class="source-code">
std::array&lt;int, 4&gt; data = {1, 2, 3, 4};
blaze::CustomVector&lt;int,
                    blaze::unaligned,
                    blaze::unpadded,
                    blaze::rowMajor&gt;
    v2(data.data(), data.size());
std::vector&lt;float&gt; mdata = {1, 2, 3, 4, 5, 6, 7, 8, 9};
blaze::CustomMatrix&lt;float,
                    blaze::unaligned,
                    blaze::unpadded,
                    blaze::rowMajor&gt;
    a2(mdata.data(), 3UL, 3UL);</pre>			<p>We can use initialized matrix objects in mathematical operations. Notice that we used two parameters: <strong class="source-inline">blaze::unaligned</strong> and <strong class="source-inline">blaze::unpadded</strong>. The <strong class="source-inline">unpadded</strong> parameter may be used in some functions or methods to control the behavior of padding or truncation of arrays. This parameter can be important in certain scenarios where you want to avoid unnecessary padding or truncating of data during operations such as reshaping, slicing, or concatenating arrays. The <strong class="source-inline">blaze::unaligned</strong> parameter allows users to perform <a id="_idIndexMarker074"/>operations on unaligned data, which can be useful in certain scenarios where the data is not aligned to specific <span class="No-Break">memory boundaries.</span></p>
			<p>Matrix and vector arithmetic operations in the <strong class="source-inline">Blaze</strong> library are offered either through overloads of standard C++ arithmetic operators such as <strong class="source-inline">+</strong>, <strong class="source-inline">-</strong>, or <strong class="source-inline">*</strong>, or through methods such as <strong class="source-inline">dot()</strong> and <strong class="source-inline">cross()</strong>. The following code sample shows how to express general math operations <span class="No-Break">in Blaze:</span></p>
			<pre class="source-code">
blaze::StaticMatrix&lt;float, 2UL, 2UL&gt; a = {{1, 2}, {3, 4}};
auto b = a;
// element wise operations
blaze::StaticMatrix&lt;float, 2UL, 2UL&gt; result = a % b;
a = b * 4;
// matrix operations
result = a + b;
a += b;
result = a * b;</pre>			<p>Notice that, in Blaze, arithmetic operators such as <strong class="source-inline">+</strong> do not perform any computation by themselves. These operators return an <em class="italic">expression object</em>, which describes what computation to perform. The actual computation happens later when the whole expression is evaluated, typically in the <strong class="source-inline">=</strong> arithmetic operator or in a constructor of a concrete object. It can lead to <a id="_idIndexMarker075"/>some non-obvious behaviors, primarily if a developer uses the <strong class="source-inline">auto</strong> keyword too frequently. The library provides two functions, <strong class="source-inline">eval()</strong> and <strong class="source-inline">evaluate()</strong>, to evaluate a given expression. The <strong class="source-inline">evaluate()</strong> function assists in deducing the exact result type of the operation via the <strong class="source-inline">auto</strong> keyword, and the <strong class="source-inline">eval()</strong> function should be used to explicitly evaluate a sub-expression within a <span class="No-Break">larger expression.</span></p>
			<p>Sometimes, we need to perform operations only on a part of the matrix. For this purpose, Blaze provides the <strong class="source-inline">blaze::submatrix</strong> and <strong class="source-inline">blaze::subvector</strong> classes, which can be parameterized with template parameters. These parameters are the top-left starting point and the<a id="_idIndexMarker076"/> width and height of a region. Also, there are functions with the same names that take the same arguments and can be used in a runtime. The following code shows how to use <span class="No-Break">this class:</span></p>
			<pre class="source-code">
blaze::StaticMatrix&lt;float, 4UL, 4UL&gt; m = {{1, 2, 3, 4},
                                          {5, 6, 7, 8},
                                          {9, 10, 11, 12},
                                          {13, 14, 15, 16}}
// make a view of the middle part of matrix
auto b = blaze::submatrix&lt;1UL, 1UL, 2UL, 2UL&gt;(m);
// change values in original matrix
blaze::submatrix&lt;1UL, 1UL, 2UL, 2UL&gt;(m) *= 0;</pre>			<p>There are two more functions to<a id="_idIndexMarker077"/> access rows and columns by index, which are also a type of <strong class="source-inline">block</strong> operation. The following snippet shows how to use the <strong class="source-inline">col</strong> and <span class="No-Break"><strong class="source-inline">row</strong></span><span class="No-Break"> functions:</span></p>
			<pre class="source-code">
blaze::row&lt;1UL&gt;(m) += 3;
blaze::column&lt;2UL&gt;(m) /= 4;</pre>			<p>In contrast to Eigen, Blaze doesn’t support implicit broadcasting. But there is the <strong class="source-inline">blaze::expand()</strong> function that can virtually expand a matrix or vector without actual memory allocation. The following <a id="_idIndexMarker078"/>cod<a id="_idTextAnchor052"/>e shows how to <span class="No-Break">use it:</span></p>
			<pre class="source-code">
blaze::DynamicMatrix&lt;float, blaze::rowVector&gt; mat = 
  blaze::uniform(4UL, 4UL, 2);
blaze::DynamicVector&lt;float, blaze::rowVector&gt; vec = {1, 2, 3, 4};
auto ex_vec = blaze::expand(vec, 4UL);
mat += ex_vec;</pre>			<p>The result of this operation will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
( 3 4 5 6 )
( 3 4 5 6 )
( 3 4 5 6 )
( 3 4 5 6 )</pre>			<h3>Using ArrayFire</h3>
			<p><strong class="source-inline">ArrayFire</strong> is a general-purpose high-performance C++ library for <span class="No-Break">parallel computing.</span></p>
			<p><strong class="bold">Parallel computing</strong> is a method of solving <a id="_idIndexMarker079"/>complex problems by dividing them into smaller tasks and executing them simultaneously across multiple processors or cores. This approach can significantly speed up the<a id="_idIndexMarker080"/> processing time compared to sequential computing, making it an essential tool for data-intensive applications such <span class="No-Break">as ML.</span></p>
			<p>It provides a single <strong class="source-inline">array</strong> type to represent matrices, volumes, and vectors. This array-based notation expresses computational algorithms in readable mathematical notation so users don’t need to express parallel computations explicitly. It has extensive vectorization and parallel batched operations. This library supports accelerated execution on CUDA and OpenCL devices. Another interesting feature of the <strong class="source-inline">ArrayFire</strong> library is that it optimizes memory usage and arithmetic calculations by runtime analysis of the executed code. This becomes possible by avoiding many <span class="No-Break">temporary allocations.</span></p>
			<p>We can define the type for a matrix with known dimensions and a floating-point data type <span class="No-Break">like this:</span></p>
			<pre class="source-code">
af::array a(3, 3, af::dtype::f32);</pre>			<p>We can define a 64-bit floating<a id="_idIndexMarker081"/> vector in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
af::array v(3, af::dtype::f64);</pre>			<p>To put some values into these objects, we can use several approaches. We can use special predefined initialization functions, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
a = af::constant(0, 3, 3);  //  Zero matrix
a = af::identity(3, 3);    //  Identity matrix
v = af::randu(3);         // Random generated vector
// Matrix filled with a single value
a = af::constant(3, 3, 3);
// Matrix filled from the initializer list
a = af::array(
    af::dim4(3, 3),
    {1.f, 2.f, 3.f, 4.f, 5.f, 6.f, 7.f, 8.f, 9.f});</pre>			<p>This code construction initializes the matrix values in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
1.0 4.0 7.0
2.0 5.0 8.0
3.0 6.0 9.0</pre>			<p>Notice that the matrix was initialized in the column-major format. The <strong class="source-inline">ArrayFire</strong> library doesn’t support <span class="No-Break">row-major initialization.</span></p>
			<p>We can use direct element access to set or change matrix coefficients. The following code sample shows how to <a id="_idIndexMarker082"/>use the <strong class="source-inline">()</strong> operator for such <span class="No-Break">an operation:</span></p>
			<pre class="source-code">
a(0,0) = 3;</pre>			<p>One important difference from other libraries that we discussed is that you can’t map existent C/C++ array data to the ArrayFire <strong class="source-inline">array</strong> object, as it will be copied. The following snippet shows <span class="No-Break">this </span><span class="No-Break"><a id="_idIndexMarker083"/></span><span class="No-Break">situation:</span></p>
			<pre class="source-code">
std::vector&lt;float&gt; mdata = {1, 2, 3, 4, 5, 6, 7, 8, 9};
a = af::array(3, 3, mdata.data());</pre>			<p>Only memory allocated in CUDA or OpenCL devices will not be copied, but ArrayFire will take ownership of <span class="No-Break">a pointer.</span></p>
			<p>We can use initialized array objects in mathematical operations. Arithmetic operations in the <strong class="source-inline">ArrayFire</strong> library are offered either through overloads of standard C++ arithmetic operators such as <strong class="source-inline">+</strong>, <strong class="source-inline">-</strong>, or <strong class="source-inline">*</strong>, or through methods such as <strong class="source-inline">af::matmul</strong>. The following code sample shows how to express general math operations <span class="No-Break">in ArrayFire:</span></p>
			<pre class="source-code">
auto a = af::array(af::dim4(2, 2), {1, 2, 3, 4});
a = a.as(af::dtype::f32);
auto b = a.copy();
// element wise operations
auto result = a * b;
a = b * 4;
// matrix operations
result = a + b;
a += b;
result = af::matmul(a, b);</pre>			<p>In contrast to other libraries we already discussed, ArrayFire doesn’t extensively use template expression for joining arithmetical operations. This <a id="_idIndexMarker084"/>library uses a <strong class="bold">just-in-time</strong> (<strong class="bold">JIT</strong>) compilation engine that converts mathematical expressions into computational kernels for CUDA, OpenCL, or CPU <a id="_idIndexMarker085"/>devices. Also, this engine merges different operations together to provide the best performance. This operation fusion technology decreases the number of kernel calls and reduces global <span class="No-Break">memory operations.</span></p>
			<p>Sometimes, we need to perform <a id="_idIndexMarker086"/>operations only on a part of the array. For this purpose, ArrayFire provides a special indexing technique. There are particular classes that can be used to express index sub-ranges for given dimensions. They are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
seq - representing a linear sequence
end - representing the last element of a dimension
span - representing the entire dimension</pre>			<p>The following code shows an example of how to access only the central part of <span class="No-Break">a matrix:</span></p>
			<pre class="source-code">
auto m = af::iota(af::dim4(4, 4));
auto center = m(af::seq(1, 2), af::seq(1, 2));
// modify a part of the matrix
center *= 2;</pre>			<p>To access and update a particular row or column in the array, there are <strong class="source-inline">row(i)</strong> and <strong class="source-inline">col(i)</strong> methods specifying a single row or column. They can be used in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
m.row(1) += 3;
m.col(2) /= 4;</pre>			<p>Also, to work with several rows or columns, there are the <strong class="source-inline">rows(first,last)</strong> and <strong class="source-inline">cols(first,last)</strong> methods specifying a span of rows <span class="No-Break">or columns.</span></p>
			<p>ArrayFire doesn’t support implicit broadcasting but there is the <strong class="source-inline">af::batchFunc</strong> function that can be used to simulate and parallelize such functionality. In general, this function finds a batch dimension of <a id="_idIndexMarker087"/>data and applies the given function to multiple data chunks in parallel. The following code shows how to <span class="No-Break">use it:</span></p>
			<pre class="source-code">
auto mat = af::constant(2, 4, 4);
auto vec = af::array(4, {1, 2, 3, 4});
mat = af::batchFunc(
    vec,
    mat,
    [](const auto&amp; a, const auto&amp; b) { return a + b; });</pre>			<p>The result of this operation will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
3.0  3.0  3.0  3.0
4.0  4.0  4.0  4.0
5.0  5.0  5.0  5.0
6.0  6.0  6.0  6.0</pre>			<p>Notice that the<a id="_idTextAnchor053"/> vector was a<a id="_idIndexMarker088"/> <span class="No-Break">column-major one.</span></p>
			<h3>Using Dlib</h3>
			<p><strong class="source-inline">Dlib</strong> is a modern C++ toolkit <a id="_idIndexMarker089"/>containing ML algorithms and tools for creating computer vision software in C++. Most of the linear algebra tools in Dlib deal with <a id="_idIndexMarker090"/>dense matrices. However, there is also limited support for <a id="_idIndexMarker091"/>working with sparse matrices and vectors. In particular, the <strong class="source-inline">Dlib</strong> tools represent sparse vectors using the containers from the C++ <strong class="bold">standard template </strong><span class="No-Break"><strong class="bold">library</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">STL</strong></span><span class="No-Break">).</span></p>
			<p>There are two main container types in Dlib to work with linear algebra: the <strong class="source-inline">matrix</strong> and <strong class="source-inline">vector</strong> classes. Matrix operations in Dlib are implemented using the expression templates technique, which allows them to eliminate the temporary matrix objects that would usually be returned from expressions such as <strong class="source-inline">M = </strong><span class="No-Break"><strong class="source-inline">A+B+C+D</strong></span><span class="No-Break">.</span></p>
			<p>We can create a matrix sized <a id="_idIndexMarker092"/>at compile time in the following way, by specifying dimensions as <span class="No-Break">template arguments:</span></p>
			<pre class="source-code">
Dlib::matrix&lt;double,3,1&gt; y;</pre>			<p>Alternatively, we can create dynamically sized matrix objects. In such a case, we pass the matrix dimensions to the constructor, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
Dlib::matrix&lt;double&gt; m(3,3);</pre>			<p>Later, we can change the size of this matrix with the <span class="No-Break">following method:</span></p>
			<pre class="source-code">
m.set_size(6,6);</pre>			<p>We can initialize matrix values with a comma operator, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
m = 54.2, 7.4, 12.1,
  1, 2, 3,
  5.9, 0.05, 1;</pre>			<p>As in the previous libraries, we<a id="_idIndexMarker093"/> can wrap an existing C++ array to the matrix object, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
double data[] = {1,2,3,4,5,6};
auto a = Dlib::mat(data, 2,3); // create matrix with size 2x3</pre>			<p>Also, we can access matrix elements with the <strong class="source-inline">()</strong> operator to modify or get a particular value, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
m(1,2) = 3;</pre>			<p>The <strong class="source-inline">Dlib</strong> library has a set of predefined functions to initialize a matrix with values such as the identity matrix, ones, or random values, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
auto a = Dlib::identity_matrix&lt;double&gt;(3);
auto b = Dlib::ones_matrix&lt;double&gt;(3,4);
auto c = Dlib::randm(3,4); // matrix with random values
                          //with size 3x3</pre>			<p>Most linear algebra arithmetic <a id="_idIndexMarker094"/>operations in the <strong class="source-inline">Dlib</strong> library are implemented through overloads of standard C++ arithmetic operators such as <strong class="source-inline">+</strong>, <strong class="source-inline">-</strong>, or <strong class="source-inline">*</strong>. Other complex operations are provided by the library as <span class="No-Break">standalone functions.</span></p>
			<p>The following example shows the use of arithmetic operations in the <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
auto c = a + b;
auto e = a * b; // real matrix multiplication
auto d  = Dlib::pointwise_multiply(a, b); // element wise
                                          //multiplication
a += 5;
auto t = Dlib::trans(a); // transpose matrix</pre>			<p>To work with partial access to matrices, Dlib provides a set of special functions. The following code sample<a id="_idIndexMarker095"/> shows how to use some <span class="No-Break">of them:</span></p>
			<pre class="source-code">
a = Dlib::rowm(b,0); // takes first row of matrix
a = Dlib::rowm(b,Dlib::range(0,1));//takes first two rows
a = Dlib::colm(b,0); // takes first column
// takes a rectangular part from center: 
a = Dlib::subm(b, range(1,2), range(1,2)); 
// initialize part of the matrix: 
Dlib::set_subm(b,range(0,1), range(0,1)) = 7; 
// add a value to the part of the matrix:
Dlib::set_subm(b,range(0,1), range(0,1)) += 7;</pre>			<p>Broadcasting in the <strong class="source-inline">Dlib</strong> library can be modeled with the <strong class="source-inline">set_rowm()</strong>, <strong class="source-inline">set_colm()</strong>, and <strong class="source-inline">set_subm()</strong> functions that give modifier objects for a particular matrix row, column, or rectangular part of the original matrix. Objects returned from these functions support all set <a id="_idIndexMarker096"/>or arithmetic operations. The following code snippet shows how to add a vector to <span class="No-Break">the columns:</span></p>
			<pre class="source-code">
Dlib::matrix&lt;float, 2,1&gt; x;
Dlib::matrix&lt;float, 2,3&gt; m;
Dlib::set_colm(b,Dlib::range(0,1)) += x;</pre>			<p>In this section, we learned about the main concepts of linear algebra and their implementation in different C++ libraries. We saw how to create matrices and tensors, and how to perform different mathematical operations with them. In the following section, we will see our first complete ML example—solving the regression prob<a id="_idTextAnchor054"/>lem with the linear <span class="No-Break">regression </span><span class="No-Break"><a id="_idIndexMarker097"/></span><span class="No-Break">approach.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor055"/>An overview of linear regression</h1>
			<p>Consider an example of the real-world supervised<a id="_idIndexMarker098"/> ML algorithm called linear regression. In general, <strong class="bold">linear regression</strong> is an approach for modeling a target value (dependent value) based on an explanatory value (independent value). This method is used for forecasting and finding relationships between values. We can classify regression methods by the number of inputs (independent variables) and the type of relationship between the inputs and outputs (<span class="No-Break">dependent variables).</span></p>
			<p>Simple linear regression is the case where the number of independent variables is <em class="italic">1</em>, and there is a linear relationship between the independent (<em class="italic">x</em>) and dependent (<span class="No-Break"><em class="italic">y</em></span><span class="No-Break">) variables.</span></p>
			<p>Linear regression is widely used in different areas such as scientific research, where it can describe relationships between variables, as well as in applications within industry, such as revenue prediction. For example, it can estimate a trend line that represents the long-term movement in the stock price time-series data. It tells whether the interest value of a specific dataset has increased or decreased over the given period, as illustrated in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer035">
					<img alt="Figure 1.1 – Linear regression visualization" src="image/B19849_01_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Linear regression visualization</p>
			<p>If we have one input variable (independent variable) and one output variable (dependent variable), the regression is <a id="_idIndexMarker099"/>called simple, and we use the term <strong class="bold">simple linear regression</strong> for it. With multiple independent variables, we call this <strong class="bold">multiple linear regression</strong> or <strong class="bold">multivariable linear regression</strong>. Usually, when we are <a id="_idIndexMarker100"/>dealing with real-world problems, we have a lot <a id="_idIndexMarker101"/>of independent variables, so we model such problems with multiple regression<a id="_idIndexMarker102"/> models. Multiple regression models have a universal definition that covers other types, so even simple linear regression is often def<a id="_idTextAnchor056"/>i<a id="_idTextAnchor057"/>ned using the multiple <span class="No-Break">regression definition.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor058"/>Solving linear regression tasks with different libraries</h2>
			<p>Assume that we <a id="_idIndexMarker103"/>have a dataset, <img alt="" role="presentation" src="image/B19849_Formula_28.png"/>, so that we can express the linear relation between <em class="italic">y</em> and <em class="italic">x</em> with mathematical formula in the <span class="No-Break">following way:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer037">
					<img alt="" role="presentation" src="image/B19849_Formula_29.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">p</em> is the dimension of the independent variable, and <em class="italic">T</em> denotes the transpose, so that <img alt="" role="presentation" src="image/B19849_Formula_30.png"/> is the inner product between vectors <img alt="" role="presentation" src="image/B19849_Formula_31.png"/> and <em class="italic">β</em>. Also, we can rewrite the previous expression in matrix notation, <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" role="presentation" src="image/B19849_Formula_32.png"/></p>
			<div>
				<div class="IMG---Figure" id="_idContainer041">
					<img alt="" role="presentation" src="image/B19849_Formula_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure">,</p>
			<div>
				<div class="IMG---Figure" id="_idContainer042">
					<img alt="" role="presentation" src="image/B19849_Formula_34.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer043">
					<img alt="" role="presentation" src="image/B19849_Formula_35.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer044">
					<img alt="" role="presentation" src="image/B19849_Formula_36.jpg"/>
				</div>
			</div>
			<p>The preceding matrix notation can be explained <span class="No-Break">as follows:</span></p>
			<ul>
				<li><em class="italic">y</em>: This is a vector of<a id="_idIndexMarker104"/> observed <span class="No-Break">target values.</span></li>
				<li><em class="italic">x</em>: This is a matrix of row-vectors, <img alt="" role="presentation" src="image/B19849_Formula_31.png"/>, which are known as explanatory or <span class="No-Break">independent values.</span></li>
				<li>ß: This is a (<em class="italic">p+1</em>) dimensional <span class="No-Break">parameters vector.</span></li>
				<li>ε: This is called an error term or noise. This variable captures all other factors that influence the <em class="italic">y</em>-dependent variable other than <span class="No-Break">the regressors.</span></li>
			</ul>
			<p>When we are considering simple linear regression, <em class="italic">p</em> is equal to 1, and the equation will look <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer046">
					<img alt="" role="presentation" src="image/B19849_Formula_38.jpg"/>
				</div>
			</div>
			<p>The goal of the linear regression task is to find parameter vectors that satisfy the previous equation. Usually, there is no exact solution to such a system of linear equations, so the task is to estimate parameters that satisfy these equations with some assumptions. One of the most popular estimation approaches is one based on the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent<a id="_idIndexMarker105"/> variable in the given dataset and those predicted by the linear function. This is called the <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) estimator. So, the task can be formulated with the<a id="_idIndexMarker106"/> <span class="No-Break">following formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer047">
					<img alt="" role="presentation" src="image/B19849_Formula_39.jpg"/>
				</div>
			</div>
			<p>In the preceding formula, the objective function, <em class="italic">S</em>, is given by the following <span class="No-Break">matrix notation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="" role="presentation" src="image/B19849_Formula_40.jpg"/>
				</div>
			</div>
			<p>This minimization problem has a unique solution, in the case that the <em class="italic">p</em> columns of the <em class="italic">x</em> matrix are linearly independent. We can get this solution by solving the <em class="italic">normal equation</em>, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="" role="presentation" src="image/B19849_Formula_41.jpg"/>
				</div>
			</div>
			<p>Linear algebra libraries can solve such equations directly with an analytical approach, but it has one significant disadvantage—computational cost. In the case of large dimensions of <em class="italic">y</em> and <em class="italic">x</em>, requirements for computer memory amount and computational time are too big to solve <span class="No-Break">real-world tasks.</span></p>
			<p>So, usually, this minimization task is solved <a id="_idIndexMarker107"/>with iterative approaches. <strong class="bold">gradient descent</strong> (<strong class="bold">GD</strong>) is an example of such an algorithm. GD is a technique based on the observation that if the function <img alt="" role="presentation" src="image/B19849_Formula_42.png"/> is defined and is differentiable in a neighborhood of a point <img alt="" role="presentation" src="image/B19849_Formula_43.png"/>, then <img alt="" role="presentation" src="image/B19849_Formula_44.png"/> decreases fastest when it goes in the direction of the negative gradient of <em class="italic">S</em> at <span class="No-Break">point <img alt="" role="presentation" src="image/B19849_Formula_43.png"/>.</span></p>
			<p>We can change our <img alt="" role="presentation" src="image/B19849_Formula_46.png"/> objective function to <a id="_idIndexMarker108"/>a form more suitable for an iterative approach. We can use the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) function, which measures the difference between the estimator and the estimated value, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="" role="presentation" src="image/B19849_Formula_47.jpg"/>
				</div>
			</div>
			<p>In the case of multiple regression, we take partial derivatives for this function for each of <em class="italic">x</em> components, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer056">
					<img alt="" role="presentation" src="image/B19849_Formula_48.jpg"/>
				</div>
			</div>
			<p>So, in the case of linear regression, we take the <span class="No-Break">following derivatives:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer057">
					<img alt="" role="presentation" src="image/B19849_Formula_49.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer058">
					<img alt="" role="presentation" src="image/B19849_Formula_50.jpg"/>
				</div>
			</div>
			<p>The whole algorithm has the <a id="_idIndexMarker109"/><span class="No-Break">following description:</span></p>
			<ol>
				<li>Initialize β <span class="No-Break">with zeros.</span></li>
				<li>Define a value for the learning rate parameter that controls how much we are adjusting parameters during the <span class="No-Break">learning procedure.</span></li>
				<li>Calculate the following values <span class="No-Break">of β:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer059">
					<img alt="" role="presentation" src="image/B19849_Formula_51.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer060">
					<img alt="" role="presentation" src="image/B19849_Formula_52.jpg"/>
				</div>
			</div>
			<ol>
				<li value="4">Repeat steps 1–3 a number of times or until the MSE value reaches a <span class="No-Break">reasonable amount.</span></li>
			</ol>
			<p>The previously described algorithm is one of the simplest supervised ML algorithms. We described it with the linear algebra concepts we introduced earlier in the chapter. Later, it became more evident that almost all ML algorithms use linear algebra under the hood. Linear regression is widely used in various industries for predictive analysis, forecasting, and decision-making. Here are some real-world examples of linear regression applications in finance, marketing, and healthcare. Linear regression can be used to predict stock prices based on historical data such as company earnings, interest rates, and economic indicators. This helps investors make informed decisions about when to buy or sell stocks. Linear regression models can be built to predict customer behavior based on demographic information, purchase history, and other relevant data. This allows marketers to target their campaigns more effectively and optimize their marketing spend. Linear regression is used to analyze medical data to identify patterns and relationships that can help <a id="_idIndexMarker110"/>improve patient outcomes. For example, it can be used to study the impact of certain treatments on <span class="No-Break">patient health.</span></p>
			<p>The following samples show the higher-level API in different linear algebra libraries for solving the <em class="italic">linear regression</em> task, and we provide them to show how libraries can simplify the complicated math used underneath. We will give the details of the APIs used in these sampl<a id="_idTextAnchor059"/>e<a id="_idTextAnchor060"/>s in the <span class="No-Break">following chapters.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor061"/>Solving linear regression tasks with Eigen</h2>
			<p>There are several iterative <a id="_idIndexMarker111"/>methods for solving problems of the <img alt="" role="presentation" src="image/B19849_Formula_53.png"/> form in the <strong class="source-inline">Eigen</strong> library. The <strong class="source-inline">LeastSquaresConjugateGradient</strong> class is one of them, which allows us to solve linear regression problems with the conjugate gradient algorithm. The <strong class="source-inline">ConjugateGradient</strong> algorithm can <a id="_idIndexMarker112"/>converge more quickly to the function’s minimum than regular GD but requires that matrix <em class="italic">A</em> is positively defined to guarantee numerical stability. The <strong class="source-inline">LeastSquaresConjugateGradient</strong> class has two main settings: the maximum number of iterations and a tolerance threshold value that is used as a stopping criterion as an upper bound to the relative residual error, as illustrated in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
typedef float DType;
using Matrix = Eigen::Matrix&lt;DType, Eigen::Dynamic, Eigen::Dynamic&gt;;
int n = 10000;
Matrix x(n,1);
Matrix y(n,1);
Eigen::LeastSquaresConjugateGradient&lt;Matrix&gt; gd;
gd.setMaxIterations(1000);
gd.setTolerance(0.001) ;
gd.compute(x);
auto b = gddg.solve(y);</pre>			<p>For new <strong class="source-inline">x</strong> inputs, we can<a id="_idIndexMarker113"/> predict new <strong class="source-inline">y</strong> values with matrix operations, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Eigen::Matrixxf new_x(5, 2);
new_x &lt;&lt; 1, 1, 1, 2, 1, 3, 1, 4, 1, 5;
auto new_y = new_x.array().rowwise() * b.transpose().array();</pre>			<p>Also, we can calculate the<a id="_idIndexMarker114"/> parameter’s <strong class="source-inline">b</strong> vector (the linear regression task solution) by solving the <em class="italic">normal equation</em> directly, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto b = (x.transpose() * x).ldlt(<a id="_idTextAnchor062"/>)<a id="_idTextAnchor063"/>.solve(x.transpose() * y);</pre>			<h2 id="_idParaDest-29"><a id="_idTextAnchor064"/>Solving linear regression tasks with Blaze</h2>
			<p>Since Blaze is just a mathematical library, there <a id="_idIndexMarker115"/>are no special classes or functions to solve linear regression tasks. However, the normal equation approach can be<a id="_idIndexMarker116"/> easily implemented. Let’s see how to define and solve linear regression tasks <span class="No-Break">with Blaze:</span></p>
			<p>Assume we have our <span class="No-Break">training data:</span></p>
			<pre class="source-code">
typedef blaze::DynamicMatrix&lt;float,blaze::columnMajor&gt; Matrix;
typedef blaze::DynamicVector&lt;float,blaze::columnVector&gt; Vector;
// the first column of X is just 1 for the bias term
Matrix x(n, 2UL);
Matrix y(n, 1UL);</pre>			<p>So we can find linear<a id="_idIndexMarker117"/> regression coefficients in the <a id="_idIndexMarker118"/><span class="No-Break">following way:</span></p>
			<pre class="source-code">
// calculate X^T*X
auto xtx = blaze::trans(x) * x;
// calculate the inverse of X^T*X
auto inv_xtx = blaze::inv(xtx);
// calculate X^T*y
auto xty = blaze::trans(x) * y;
// calculate the coefficients of the linear regression
Matrix beta = inv_xtx * xty;</pre>			<p>Then, we can use estimated coefficients for making predictions on new data. The following code snippet shows how to <span class="No-Break">do it:</span></p>
			<pre class="source-code">
auto line_coeffs = blaze::expand(
  blaze::row&lt;0UL&gt;(blaze::trans(beta)), new_x.rows());
auto new_y = new_x % line_coeffs;</pre>			<p>Notice that we virtually expand the coefficients vector to perform element-w<a id="_idTextAnchor065"/>ise multiplication with the <span class="No-Break">x data.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor066"/>Solving linear regression tasks with ArrayFire</h2>
			<p>ArrayFire also doesn’t have special functions and classes to solve this type of problem. However, as it has all the necessary<a id="_idIndexMarker119"/> mathematical abstraction, the normal equation approach can be applied. Another approach is an iterative one that uses<a id="_idIndexMarker120"/> GD. This algorithm was described in the first section of this chapter. Such a technique eliminates the necessity of calculating inverse matrices, making it possible to apply it to a larger amount of training data. Calculating an inverse for a large matrix is a very <span class="No-Break">performance-expensive operation.</span></p>
			<p>Let’s define a lambda function to calculate prediction values from data <span class="No-Break">and coefficients:</span></p>
			<pre class="source-code">
auto predict = [](auto&amp; v, auto&amp; w) {
  return af::batchFunc(v, w, [](const auto&amp; a, const auto&amp; b) {
    return af::sum(a * b, /*dim*/ 1);
  });
};</pre>			<p>Assume that we have training data defined in the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables. We define the <strong class="source-inline">train_weights</strong> variable to hold and update coefficients that want to learn from the <span class="No-Break">training data:</span></p>
			<pre class="source-code">
// the first column is for the bias term
af::dim4 weights_dim(1, 2);
auto train_weights = af::constant(0.f, weights_dim, af::dtype::f32);</pre>			<p>Then, we can define the GD loop where we will iteratively update coefficients. The following code snippet shows how to <span class="No-Break">implement it:</span></p>
			<pre class="source-code">
af::array j, dj;  // cost value and its gradient
float lr = 0.1f;  // learning rate
int n_iter = 300;
for (int i = 0; i &lt; n_iter; ++i) {
  std::cout &lt;&lt; "Iteration " &lt;&lt; i &lt;&lt; ":\n";
  // get the cost
  auto h = predict(x, train_weights);
  auto diff = (y - h);
  auto j = af::sum(diff * diff) / n;
  af_print(j);
  // find the gradient of cost
  auto dm = (-2.f / n) * af::sum(x.col(1) * diff);
  auto dc = (-2.f / n) * af::sum(diff);
  auto dj = af::join(1, dc, dm);
  // update the parameters via gradient descent
  train_weights = train_weights - lr * dj;
}</pre>			<p>The most important part <a id="_idIndexMarker121"/>of this loop is calculating the <span class="No-Break">prediction</span><span class="No-Break"><a id="_idIndexMarker122"/></span><span class="No-Break"> error:</span></p>
			<pre class="source-code">
  auto h = predict(x, train_weights);
  auto diff = (y – h);</pre>			<p>Another important part is calculating the gradient values based on partial derivatives related to each of <span class="No-Break">the coefficients:</span></p>
			<pre class="source-code">
  auto dm = (-2.f / n) * af::sum(x.col(1) * diff);
  auto dc = (-2.f / n) * af::sum(diff);</pre>			<p>We join these values into one vector to make a single expression for updating the <span class="No-Break">training parameters:</span></p>
			<pre class="source-code">
  auto dj = af::join(1, dc, dm);
  train_weights = train_weights - lr * dj;</pre>			<p>We can stop this training loop after a number of iterations or when the cost function value reaches some appropriate <a id="_idIndexMarker123"/>convergence value. The cost function value can be calculated in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
auto j = af::sum(diff * diff) / n;</pre>			<p>You can see that this is just a <a id="_idIndexMarker124"/>sum of<a id="_idTextAnchor067"/> squared errors for all <span class="No-Break">training samples.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor068"/>Linear regression with Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library provides the <strong class="source-inline">krr_trainer</strong> class, which can get the template argument of the <strong class="source-inline">linear_kernel</strong> type to solve<a id="_idIndexMarker125"/> linear regression tasks. This class implements <a id="_idIndexMarker126"/>direct analytical solving for this type of problem with the kernel ridge regression algorithm, as illustrated in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
std::vector&lt;matrix&lt;double&gt;&gt; x;
std::vector&lt;float&gt; y;
krr_trainer&lt;KernelType&gt; trainer;
trainer.set_kernel(KernelType());
decision_function&lt;KernelType&gt; df = trainer.train(x, y);</pre>			<p>For new <strong class="source-inline">x</strong> inputs, we can predict new <strong class="source-inline">y</strong> values in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
std::vector&lt;matrix&lt;double&gt;&gt; new_x;
for (auto&amp; v : x) {
    auto prediction = df(v);<a id="_idTextAnchor069"/>
    std::cout &lt;&lt; prediction &lt;&lt; std::endl;
}</pre>			<p>In this section, we learned how to solve the linear regression problem with different C++ libraries. We saw that some of<a id="_idIndexMarker127"/> them contain the complete algorithm implementation that can be easily applied, and <a id="_idIndexMarker128"/>we saw how to implement this approach from scratc<a id="_idTextAnchor070"/>h using just basic linear <span class="No-Break">algebra primitives.</span></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor071"/>Summary</h1>
			<p>In this chapter, we learned what ML is, how it differs from other computer algorithms, and how it became so popular. We also became familiar with the necessary mathematical background required to begin working with ML algorithms. We looked at software libraries that provide APIs for linear algebra and also implemented our first ML <span class="No-Break">algorithm—linear regression.</span></p>
			<p>There are other linear algebra libraries for C++. Moreover, the popular deep learning frameworks use their own implementations of linear algebra libraries. For example, the MXNet framework is based on the <strong class="source-inline">mshadow</strong> library, and the PyTorch framework is based on the ATen library. Some of these libraries can use GPU or special CPU instructions to speed up calculations. Such features do not usually change the API but require some additional library initialization settings or explicit object conversion to different backends such as CPUs <span class="No-Break">or GPUs.</span></p>
			<p>Real ML projects can be challenging and complex. Common pitfalls include data quality issues, overfitting and underfitting, choosing the wrong model, and not having enough computing resources. Poor data quality can also affect model performance. It’s essential to clean and preprocess data to remove outliers, handle missing values, and transform features for better representation. The model should be chosen based on the nature of the problem and the available data. Overfitting occurs when the model memorizes the training data instead of learning general patterns, while underfitting happens when the model cannot capture the underlying structure of the data. To avoid these pitfalls, it is important to have a clear understanding of the problem, use appropriate preprocessing techniques for data, choose the right model, and evaluate the performance using metrics that are relevant to the task. Best practices in ML also include monitoring the model’s performance in production and making adjustments as needed. We will discuss the details of these techniques throughout the book. In the next two chapters, we will learn more about available software tools that are necessary to implement more complicated algorithms, and we will also learn more theoretic<a id="_idTextAnchor072"/><a id="_idTextAnchor073"/>al background on how to manage <span class="No-Break">ML algorithms.</span></p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor074"/>Further reading</h1>
			<ul>
				<li><em class="italic">Basic Linear Algebra for Deep </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23"><span class="No-Break">https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23</span></a></li>
				<li><em class="italic">Deep Learning</em>, an MIT Press <span class="No-Break">book: </span><a href="https://www.deeplearningbook.org/contents/linear_algebra.html"><span class="No-Break">https://www.deeplearningbook.org/contents/linear_algebra.html</span></a></li>
				<li><em class="italic">What is Machine </em><span class="No-Break"><em class="italic">Learning?</em></span><span class="No-Break">: </span><a href="https://www.mathworks.com/discovery/machine-learning.html"><span class="No-Break">https://www.mathworks.com/discovery/machine-learning.html</span></a></li>
				<li>The <strong class="source-inline">Eigen</strong> library <span class="No-Break">documentation: </span><a href="https://gitlab.com/libeigen/eigen"><span class="No-Break">https://gitlab.com/libeigen/eigen</span></a></li>
				<li>The <strong class="source-inline">xtensor</strong> library <span class="No-Break">documentation: </span><a href="https://xtensor.readthedocs.io/en/latest/"><span class="No-Break">https://xtensor.readthedocs.io/en/latest/</span></a></li>
				<li>The <strong class="source-inline">Dlib</strong> library <span class="No-Break">documentation: </span><a href="http://dlib.net/"><span class="No-Break">http://dlib.net/</span></a></li>
				<li>The <strong class="source-inline">blaze</strong> library <span class="No-Break">documentation: </span><a href="https://bitbucket.org/blaze-lib/blaze/wiki/Home"><span class="No-Break">https://bitbucket.org/blaze-lib/blaze/wiki/Home</span></a></li>
				<li>The ArrayFire library <span class="No-Break">documentation: </span><a href="https://arrayfire.org/docs/index.htm"><span class="No-Break">https://arrayfire.org/docs/index.htm</span></a></li>
			</ul>
		</div>
	</body></html>