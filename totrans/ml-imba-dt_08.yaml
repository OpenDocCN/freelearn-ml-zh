- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithm-Level Deep Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data-level deep learning techniques have problems very similar to classical
    ML techniques. Since deep learning algorithms are quite different from classical
    ML techniques, we’ll explore some algorithm-level techniques for addressing data
    imbalance in this chapter. These algorithm-level techniques won’t change the data
    but accommodate the model instead. This exploration might uncover new insights
    or methods to better handle imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will be on the same lines as [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, extending the ideas to deep learning models. We will
    look at algorithm-level deep learning techniques to handle the imbalance in data.
    Generally, these techniques do not modify the training data and often require
    no pre-processing steps, offering the benefit of no increased training times or
    additional runtime hardware costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for algorithm-level techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explicit loss function modification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing other algorithm-based techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll understand how to manage imbalanced data
    through model weight adjustments and loss function modifications using PyTorch
    APIs. We’ll also explore other algorithmic strategies, equipping you to make informed
    decisions in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mostly be using standard functions from PyTorch and `torchvision` throughout
    this chapter. We will also use the Hugging Face Datasets library for dealing with
    text data.
  prefs: []
  type: TYPE_NORMAL
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter’s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for algorithm-level techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will concentrate on deep learning techniques that have gained
    popularity in both the vision and text domains. We will mostly use a long-tailed
    imbalanced version of the MNIST dataset, similar to what we used in [*Chapter
    7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep Learning Methods*. We
    will also consider CIFAR10-LT, the long-tailed version of CIFAR10, which is quite
    popular among researchers working with long-tailed datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the ideas will be very similar to what we learned in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*, where the high-level
    idea was to increase the weight of the positive (minority) class and decrease
    the weight of the negative (majority) class in the cost function of the model.
    To facilitate this adjustment to the loss function, frameworks such as `scikit-learn`
    and XGBoost offer specific parameters. `scikit-learn` provides options such as
    `class_weight` and `sample_weight`, while XGBoost offers `scale_pos_weight` as
    a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the idea remains the same, and PyTorch provides a `weight`
    parameter in the `torch.nn.CrossEntropyLoss` class to implement this weighting
    idea.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will see some advanced techniques that are more relevant and might
    give better results for the deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: With imbalanced datasets, the majority class examples contribute much more to
    the overall loss than the minority class examples. This happens because the majority
    class examples heavily outnumber the minority class examples. This means that
    the loss function being used is naturally biased toward the majority classes,
    and it fails to capture the error from minority classes. Keeping this in mind,
    can we change the loss function to account for this discrepancy for imbalanced
    datasets? Let’s try to figure this out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-entropy loss for binary classification is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CrossEntropyLoss(p) = {− log(p) if y = 1 (minority class term)    − log(1 −
    p) otherwise (majority class term)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say y = 1 represents the minority class and it’s the class we are trying
    to predict. So, we can try to increase the minority class term by multiplying
    it with a higher value of weight to increase its attribution to the overall loss.
  prefs: []
  type: TYPE_NORMAL
- en: Weighting techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s continue to use the imbalanced MNIST dataset from the previous chapter,
    which has long-tailed data distribution, as shown in the following bar chart (*Figure
    8**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Imbalanced MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here, the *x* axis is the class label, and the *y* axis is the count of samples
    of various classes. In the next section, we will see how to use the weight parameter
    in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following model code for all the vision-related tasks in this
    chapter. We have defined a PyTorch neural network class called `Net` with two
    convolutional layers, a dropout layer, and two fully connected layers. The `forward`
    method applies these layers sequentially along with ReLU activations and max-pooling
    to process the input, `x`. Finally, it returns the `log_softmax` activation of
    the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Since our final layer of the model uses `log_softmax`, we will be using negative
    log-likelihood loss (`torch.nn.functional.nll_loss` or `torch.nn.NLLLoss`) from
    PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch’s weight parameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `torch.nn.CrossEntropyLoss` API, we have a `weight` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `weight` is a one-dimensional tensor that assigns weight to each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `compute_class_weight` function from `scikit-learn` to get the
    weights of various classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `compute_class_weight` function computes the weights according to the following
    formula for each class, as we saw in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*:'
  prefs: []
  type: TYPE_NORMAL
- en: weight _ class _ a =  1  _______________________   total _ num _ samples _ for
    _ class _ a  *  total _ number _ of _ samples  ___________________  number _ of
    _ classes
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8**.2*, these weights have been plotted using a bar chart to help
    us see how they relate to the class frequency (*y* axis) for each class (*x* axis):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Bar chart showing weights corresponding to each class
  prefs: []
  type: TYPE_NORMAL
- en: As this figure shows, the fewer the number of samples a class has, the higher
    its weight.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway here is that the weight of a class is inversely proportional
    to the number of samples of that class, also called inverse class frequency weighting.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to remember is that the class weights should always be computed
    from the training data. Using validation data or test data to compute the class
    weights might lead to the infamous data leakage or label leakage problem in ML.
    Formally, data leakage can happen when some information from outside of the training
    data is fed to the model. In this case, if we use test data to compute the class
    weights, then our evaluation of the model’s performance is going to be biased
    and invalid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comic in *Figure 8**.3* shows a juggler managing weights of different sizes,
    each labeled with a distinct class label, symbolizing the varying weights assigned
    to different classes to tackle class imbalance during model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Comic illustrating the core idea behind class weighting
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Another way to compute weights is to empirically tune the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A lot of other loss functions in PyTorch, including `NLLLoss`, `MultiLabelSoftMarginLoss`,
    `MultiMarginLoss`, and `BCELoss`, accept `weight` as a parameter as well.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.4* compares the accuracy of various classes when using class weights
    versus when not using class weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Performance comparison of a model trained using cross-entropy loss
    with no class weights and with class weights
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, although the accuracy dropped for classes 0-4, it improved dramatically
    for the most imbalanced classes of 5-9\. The overall accuracy of the model went
    up as well.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that some loss functions, such as `BCEWithLogitsLoss`, provide
    two weighting parameters (`BCEWithLogitsLoss` can be used for binary classification
    or multi-label classification):'
  prefs: []
  type: TYPE_NORMAL
- en: • The `weight` parameter is the manual rescaling weight parameter for each example
    of the batch. This is more like the `sample_weight` parameter of the `sklearn`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: • The `pos_weight` parameter is used to specify a weight for the positive class.
    It is similar to the `class_weight` parameter in the `sklearn` library.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Class reweighting in production at OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI was trying to solve the problem of bias in training data using the image
    generation model DALL-E 2 [1]. DALL-E 2 is trained on a massive dataset of images
    from the internet, which can contain biases. For example, the dataset may contain
    more images of men than women or more images of people from certain racial or
    ethnic groups than others.
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit undesirable model capabilities (such as generating violent images),
    they first filtered out such images from the training dataset. However, filtering
    training data can amplify biases. Why? In their blog [1], they explain using an
    example that when generating images for the prompt “a CEO,” their filtered model
    showed a stronger male bias than the unfiltered one. They suspected this amplification
    arose from two sources: dataset bias toward sexualizing women and potential classifier
    bias, despite their efforts to mitigate them. This may have resulted in the filter
    removing more images of women, skewing the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: To fix this bias, OpenAI applied reweighting to the DALL-E 2 training data by
    training a classifier to predict whether an image was from the unfiltered dataset.
    The weights for each image were then computed based on the classifier’s prediction.
    This scheme was shown to reduce the frequency change induced by filtering, which
    means that it was effective at counteracting the biases in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to show its extensive applicability, we will apply the class weighting
    technique to textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Handling textual data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s work with some text data. We will use the `datasets` and `transformers`
    libraries from Hugging Face. Let’s import the `trec` dataset (the **Text Retrieval
    Conference** (**TREC**), a question classification dataset containing 5,500 labeled
    questions in the training set and 500 in the test set):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset is balanced, so we randomly remove examples from classes ABBR
    and DESC, making those classes the most imbalanced. Here is how the distribution
    of various classes looks like in this dataset, confirming the imbalance in data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Frequency of various classes in the trec dataset from the Hugging
    Face Datasets library
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a tokenizer (that splits text into words or sub-words) for the
    pre-trained DistilBERT language model vocabulary with a maximum input token length
    of 512:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create tokenized train and test sets from the dataset we just
    imported by invoking the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s instantiate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s define and invoke a function to get training arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `custom_compute_metrics()` function returns a dictionary containing
    the precision, recall, and F1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s implement the class containing the loss function that uses class
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can initialize the weights similar to how we did previously using the `compute_class_weight`
    function in `sklearn`, and then feed it to the `CrossEntropyLoss` function in
    our `CustomTrainerWeighted` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in *Figure 8**.6*, we can see improvements in performance for the
    most imbalanced classes. However, a slight reduction was observed for the majority
    class (trade-off!):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Confusion matrix using no class weighting (left) and with class
    weights (right)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the minority classes, **ABBR** and **DESC**, have improved performance
    after class weighting at the cost of reduced performance for the **ENTY** class.
    Also, looking at some of the off-diagonal entries, we can see that the confusion
    between the **ABBR** and **DESC** classes (0.33 in *Figure 8**.6* (left)) and
    between the **DESC** and **ENTY** classes (0.08 in *Figure 8**.6* (left)) significantly
    dropped when using class weights (0.22 and 0.04, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: Some variants that deal with NLP tasks in particular suggest weighting the samples
    as the inverse of the square root of class frequency for their corresponding class
    instead of using the previously used inverse class frequency weighting technique.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, class weighting can usually help with any kind of deep learning
    model, including textual data, when working with imbalanced data. Since data augmentation
    techniques are not as straightforward for text as they are for images, class weighting
    can be a useful technique for NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Class reweighting in production at Wayfair
  prefs: []
  type: TYPE_NORMAL
- en: Wayfair used the BERT language model to improve the accuracy of its product
    search and recommendation system [2]. This was a challenging problem because the
    number of products that Wayfair sells is very large, and the number of products
    that a customer is likely to be interested in is much smaller.
  prefs: []
  type: TYPE_NORMAL
- en: There was an imbalance in data because the number of products that a customer
    had interacted with (for example, viewed, added to cart, or purchased) was much
    smaller than the number of products that the customer hadn’t interacted with.
    This made it difficult for BERT to learn to accurately predict which products
    a customer was likely to be interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Wayfair used class weighting to address the data imbalance problem. They assigned
    a higher weight to positive examples (that is, products that a customer had interacted
    with) than to negative examples (that is, products that a customer had not interacted
    with). This helped ensure that BERT learned to accurately classify both positive
    and negative examples, even when the data was imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: The model was deployed to production. Wayfair is using the model to improve
    the accuracy of its product search and recommendation system and to provide a
    better experience for customers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a minor variant of class weighting that
    can sometimes be more helpful than just the weighting technique.
  prefs: []
  type: TYPE_NORMAL
- en: Deferred re-weighting – a minor variant of the class weighting technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a deferred re-weighting technique (mentioned by Cao et al. [3]) similar
    to the two-phase sampling approach we discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*. Here, we defer the re-weighting to later,
    wherein in the first phase of training, we train the model on the full imbalanced
    dataset without any weighting or sampling. In the second phase, we re-train the
    same model from the first phase with class weights (that are inversely proportional
    to the class frequencies) that have been applied to the loss function and, optionally,
    use a smaller learning rate. The first phase of training serves as a good form
    of initialization for the model for the second phase of training with reweighted
    losses. Since we use a smaller learning rate in the second phase of training,
    the weights of the model do not move very far from what they were in the first
    phase of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The deferred re-weighting technique
  prefs: []
  type: TYPE_NORMAL
- en: 'The comic in *Figure 8**.8* shows a magician who pulls out a large rabbit from
    a hat, followed by a smaller one, illustrating the two-phase process of initially
    training on the imbalanced dataset and subsequently applying re-weighting for
    more balanced training in the second phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – A comic illustrating the core idea of deferred re-weighting
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to the notebook titled `Deferred_reweighting_DRW.ipynb` in this
    book’s GitHub repository for more details. After applying the two-phase training
    part of the deferred re-weighting technique, we can see that the accuracy of our
    most imbalanced classes improves compared to training with cross-entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Performance comparison of deferred re-weighting with cross-entropy
    loss
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at defining custom loss functions when the PyTorch standard
    loss functions don’t do everything that we want them to do.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit loss function modification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In PyTorch, we can formulate custom loss functions by deriving a subclass from
    the `nn.Module` class and overriding the `forward()` method. The `forward()` method
    for a loss function accepts the predicted and actual outputs as inputs, subsequently
    returning the computed loss value.
  prefs: []
  type: TYPE_NORMAL
- en: Even though class weighting does assign different weights to balance the majority
    and minority class examples, this alone is often insufficient, especially in cases
    of extreme class imbalance. What we would like is to reduce the loss from easily
    classified examples as well. The reason is that such easily classified examples
    usually belong to the majority class, and since they are higher in number, they
    dominate our training loss. This is the main idea of focal loss and allows for
    a more nuanced handling of examples, irrespective of the class they belong to.
    We’ll look at this in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the forward() method in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, you’ll encounter the `forward()` method in both neural network
    layers and loss functions. That’s because both a neural network layer and a loss
    function are derived from `nn.Module`. While it might seem confusing at first,
    understanding the context can help clarify its role:'
  prefs: []
  type: TYPE_NORMAL
- en: '**🟠** **In neural** **network layers**:'
  prefs: []
  type: TYPE_NORMAL
- en: The `forward()` method defines the transformation that input data undergoes
    as it passes through the layer. This could involve operations such as linear transformations,
    activation functions, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '**🟢** **In** **loss functions**:'
  prefs: []
  type: TYPE_NORMAL
- en: The `forward()` method computes the loss between the predicted output and the
    actual target values. This loss serves as a measure of how well the model is performing.
  prefs: []
  type: TYPE_NORMAL
- en: '**🔑****Key takeaway**:'
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, both neural network layers and loss functions inherit from `nn.Module`,
    providing a unified interface. The `forward()` method is central to both, serving
    as the computational engine for data transformation in layers and loss computation
    in loss functions. Think of `forward()` as the “engine” for either process.
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The techniques we’ve studied so far presume that minority classes need higher
    weights due to weak representation. However, some minority classes may be adequately
    represented, and over-weighting their samples could degrade the overall model
    performance. Hence, Tsung-Yi et al. [4] from Facebook (now Meta) introduced **focal
    loss**, a sample-based weighting technique where each example’s weight is determined
    by its difficulty and measured by the loss the model incurs on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The focal loss technique has roots in dense object detection tasks, where there
    are significantly more observations in one class than the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Class imbalance in object detection – majority as background,
    few as foreground
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss downweighs easy-to-classify examples and focuses on hard-to-classify
    examples. What this means is that it would reduce the model’s overconfidence;
    this overconfidence usually prevents the model from generalizing well.
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss is an extension of cross-entropy loss. It is especially good for
    multi-class classification, where some classes are easy and others are difficult
    to classify.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with our well-known cross-entropy loss for binary classification.
    If we let p be the predicted probability that y = 1, then the cross-entropy loss
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be rewritten as CrossEntropyLoss(p) = − log( p t), where p t, the
    probability of the true class, can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p t = {p if y = 1  1 − p otherwise
  prefs: []
  type: TYPE_NORMAL
- en: Here, p is the predicted probability that y = 1 from the model.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this loss function is that in the case of imbalanced datasets,
    this loss function is dominated by the loss contribution from majority classes,
    and the loss contribution from the minority class is very small. This can be fixed
    via focal loss.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is focal loss?
  prefs: []
  type: TYPE_NORMAL
- en: FocalLoss( p t) = − α (1 − p t) γ log( p t)
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula looks slightly different from cross-entropy loss. There are two
    extra terms – α and (1 − p t) γ. Let’s try to understand the significance of each
    of these terms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'α: This value can be set to be inversely proportional to the number of examples
    of positive (minority) classes and is used to weigh the minority class examples
    more than the majority class. It can also be treated as a hyperparameter that
    can be tuned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(1 − p t) γ: This term is called the **modulating factor**. If an example is
    too easy for the model to classify, that would mean that p t is very high and
    the whole modulating factor value will be close to zero (assuming γ > 1), and
    the model won’t focus on this example much. On the other hand, if an example is
    hard – that is, p t is low – then the modulating factor value will be high, and
    the model will focus on this example more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the implementation of focal loss from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Though the focal loss technique has roots in computer vision and object detection,
    we can potentially reap its benefits while working with tabular data and text
    data too. Some recent research has ported focal loss to classical ML frameworks
    such as XGBoost [5] and LightGBM [6], as well as to text data that uses transformer-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comic in *Figure 8**.11* shows an archer aiming at a small distant target,
    overlooking a large nearby target, symbolizing the focal loss’s emphasis on challenging
    minority class examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Illustration of focal loss
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch’s `torchvision` library already has this loss implemented for us to
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `alpha` and `gamma` values can be challenging to tune for the model and
    dataset being used. Using an `alpha` value of `0.25` and a `gamma` value of `2`
    with `reduction= ''mean''` on CIFAR10-LT (the long-tailed version of the CIFAR10
    dataset) seems to do better than the regular cross-entropy loss, as shown in the
    following graph. For more details, please check the `CIFAR10_LT_Focal_Loss.ipynb`
    notebook in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Model accuracy using cross-entropy loss versus focal loss (alpha=0.25,
    gamma=2) on the CIFAR10-LT dataset as training progresses
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Pascal VOC dataset for object detection [7], the focal loss helps detect
    a motorbike in the image, while the cross-entropy loss wasn’t able to detect it
    (*Figure 8**.13*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13 – Motorbike not detected by cross-entropy loss (left), while focal
    loss does detect it (right) on the Pascal VOC dataset. Source: fastai library
    GitHub repo [8]'
  prefs: []
  type: TYPE_NORMAL
- en: Though focal loss was initially designed for dense object detection, it has
    gained traction in class-imbalanced tasks due to its ability to assign higher
    weights to challenging examples that are commonly found in minority classes. While
    the proportion of such samples is higher in minority classes, the absolute count
    is higher in the majority class due to its larger size. Consequently, assigning
    high weights to challenging samples across all classes could still cause bias
    in the neural network’s performance. This motivates us to explore other loss functions
    that can reduce this bias.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Focal loss in production at Meta
  prefs: []
  type: TYPE_NORMAL
- en: There was a need to detect harmful content, such as hate speech and violence,
    at Meta (previously Facebook) [9]. ML models were trained on a massive dataset
    of text and images that included both harmful and non-harmful content. However,
    the system was struggling to learn from the harmful content examples because they
    were much fewer in number than the non-harmful examples. This was causing the
    system to overfit the non-harmful examples, and it was not performing well in
    terms of detecting harmful content in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the problem, Meta used focal loss. Focal loss, as we’ve seen, is a
    technique that down-weighs the easy-to-classify examples so that the system focuses
    on learning from the hard-to-classify examples. Meta implemented focal loss in
    their training pipeline and was able to improve the performance of their AI system
    when it came to detecting harmful content by up to 10%. This is a significant
    improvement, and it shows that focal loss is a promising technique for training
    AI systems to detect rare or difficult-to-classify events. The new system has
    been deployed into production at Meta, and it has helped to substantially improve
    the safety of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Class-balanced loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper by Cui et al. [10] made a minor change to the equation for cross-entropy
    loss by adding a multiplicative coefficient of  (1 − β) _ (1− β n) to the loss
    function – that is, we use a value of α =  (1 − β) _ (1− β n), where β is a hyperparameter
    between 0 and 1, and n is the number of samples of a class:'
  prefs: []
  type: TYPE_NORMAL
- en: ClassBalancedCrossEntropyLoss(p) = −  (1 − β) _ (1− β n)  log( p t)
  prefs: []
  type: TYPE_NORMAL
- en: β = 0 means no weighting at all, while β → 1 means re-weighting by inverse class
    frequency. So, we can consider this method to be a way for the class weight of
    a particular class to be adjustable between 0 and (1/frequency of a class), depending
    on the value of the hyperparameter, β, which is a tunable parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This same term can be used in place of the alpha value. It can be used in conjunction
    with focal loss too:'
  prefs: []
  type: TYPE_NORMAL
- en: ClassBalancedFocalLoss( p t) = −  (1 − β) _ (1− β n)  (1 − p t) γ log( p t)
  prefs: []
  type: TYPE_NORMAL
- en: According to Cui et al., the recommended setting for the beta value is (N-1)/N,
    where N is the total number of training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comic in *Figure 8**.14* illustrates the core idea of this loss. It shows
    a tightrope walker who maintains balance using a pole with weights labeled “beta”
    on both ends, representing the adjustment of class weights to address class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Illustration of class-balanced loss
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code for implementing class-balanced loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `forward()` function, `effective_num` effectively computes (1-β n) as
    a vector, where `n` is a vector containing the number of samples per class. So,
    the `weights` vector is  (1 − β) _ (1− β n). Using these weights, we compute the
    loss by using `NLLLoss` between the output of the model and the corresponding
    labels. *Table 8.1* shows the class-wise accuracy when the model is trained using
    class-balanced cross-entropy loss for 20 epochs. Here, we can see an accuracy
    improvement for the most imbalanced classes of 9, 8, 7, 6, and 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **CrossEntropyLoss** | **ClassBalancedLoss** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 99.9 | 99.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 99.6 | 99.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 98.1 | 97.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 96.8 | 94.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 97.7 | 97.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 94.2 | 97.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 92.8 | 98.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 81.2 | 94.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 63.6 | 93.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 49.1 | 91.4 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Class-wise accuracy using cross-entropy loss (left) and class-balanced
    cross-entropy loss (right)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.15* compares the performance of class-balanced loss and cross-entropy
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Overall accuracy versus class-wise accuracy using class-balanced
    loss compared to the baseline model
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Class-balanced loss in production at Apple
  prefs: []
  type: TYPE_NORMAL
- en: The accessibility team at Apple aimed to ensure usability for all by addressing
    the lack of proper accessibility information in many apps. They made these apps
    usable for individuals with disabilities through features such as screen recognition.
    The researchers aimed to automatically generate accessibility metadata [11] for
    mobile apps based on their visual interfaces, a problem that had significant class
    imbalance due to the diverse range of UI elements. UI elements such as text, icons,
    and sliders were identified from app screenshots. The text elements were highly
    represented with 741,285 annotations, while sliders were least represented with
    1,808 annotations.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consisted of 77,637 screens from 4,068 iPhone apps, with a wide
    variety of UI elements, leading to a highly imbalanced dataset, especially considering
    the hierarchical nature of the UI elements.
  prefs: []
  type: TYPE_NORMAL
- en: A class-balanced loss function and data augmentation were employed to handle
    the class imbalance effectively. This allowed the model to focus more on underrepresented
    UI classes, thereby improving the overall performance. The model was designed
    to be robust and fast, enabling on-device deployment. This ensured that the accessibility
    features could be generated in real time, enhancing the user experience for screen
    reader users.
  prefs: []
  type: TYPE_NORMAL
- en: Modern ConvNet classifiers tend to overfit the minority classes in imbalanced
    datasets. What if we could prevent that from happening? The **Class-Dependent
    Temperature** (**CDT**) loss function aims to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Class-dependent temperature Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addressing imbalanced datasets, traditional explanations suggest that a model’s
    inferior performance on minority classes, compared to majority classes, stems
    from its inclination to minimize average per-instance loss. This biases the model
    toward predicting majority classes. To counteract this, re-sampling and re-weighting
    strategies have been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: However, Ye et al. [12] introduced the **Class-Dependent Temperature** (**CDT**)
    Loss, presenting a novel perspective. Their research indicates that ConvNets tend
    to overfit minority class examples, as evident from a larger feature deviation
    between training and test sets for minority classes compared to majority ones.
    Feature deviation occurs when a model learns the training data distribution of
    feature values excessively well, subsequently failing to generalize to new data.
    With CDT loss, the model’s decision values for training examples are divided by
    a “temperature” factor, dependent on each class’s frequency. This division makes
    the training more attuned to feature deviation and aids in effective learning
    across both prevalent and scarce categories.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.16* portrays how CDT loss modifies class weights according to class
    frequencies, using the visual analogy of a juggler on a unicycle handling items
    marked with different class names:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – A unicyclist juggling items, adjusting class weights based on
    frequencies
  prefs: []
  type: TYPE_NORMAL
- en: 'The following class implements this loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an explanation of the `CDT` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.num_class_list` stores the number of examples in each class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.cdt_weight = torch.FloatTensor([...]).to(device)` computes the class-dependent
    temperature weights for each class. For each class, the weight is computed as
    `(max(num_class_list) / num_class_list[i]) **` `gamma`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger the number of examples in a class, the smaller its value in the `self.cdt_weight`
    list. Majority class examples have lower values, while minority class examples
    have higher values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs = inputs /self.cdt_weight` scales the log probabilities (as inputs)
    from the model by the class-dependent temperature weights. This increases the
    absolute values of the negative log probabilities for minority class examples,
    making them more significant in the loss calculation than those for the majority
    class. This intends to make the model focus more on the minority class examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In *Figure 8**.17*, we’re plotting the overall accuracy of CDT loss and cross-entropy
    loss (left) and the accuracies of various classes (right):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17259_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Performance comparison between cross-entropy loss and CDT loss
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there is an accuracy improvement for some classes, such as 9,
    7, 6, 5, and 3, but a decrease in performance for some of the other classes. It
    seems to give a lukewarm performance on the imbalanced MNIST dataset that we used,
    but it can potentially be helpful for other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: What if we could dynamically adjust the weights of the classes according to
    their difficulty for the model during training? We could measure the class difficulty
    by the accuracy of its predictions for the examples’ class and then use this difficulty
    to compute the weight for that class.
  prefs: []
  type: TYPE_NORMAL
- en: Class-wise difficulty-balanced loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper from Sinha et al. [13] proposed that the weight for a class, c, after
    training time, t, should be directly proportional to the difficulty of the class.
    The lower the accuracy of the class, the higher its difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: w c, t = ( d c, t) τ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, w c, t is the weight of class c after training time t, and d c, t is
    the class difficulty, which is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: d c, t = (1 − Accuracy c, t)
  prefs: []
  type: TYPE_NORMAL
- en: Here, Accuracy c, t is the accuracy of class c on the validation dataset after
    time t, and τ is a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point here is that we would want to dynamically increase the weight of
    the class for which the model’s accuracy is lower as training progresses. We could
    do this every epoch or every few epochs of training and feed the updated weights
    to the cross-entropy loss. Please look at the corresponding notebook titled `Class_wise_difficulty_balanced_loss.ipynb`
    in this book’s GitHub repository for the full training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 8**.18* illustrates the concept of difficulty-balanced loss using a
    comic with an acrobat on a trampoline. Each bounce is labeled with an accuracy
    score, highlighting how classes with lower accuracy receive increasing weight
    as the acrobat bounces higher each time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Illustration of difficulty-balanced loss – the acrobat’s bounces
    show increasing weight for lower-accuracy classes
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.19* shows the performance of class-wise difficulty-balanced loss
    compared to cross-entropy loss as the baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Performance comparison of models trained using class-wise difficulty-balanced
    loss and cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the performance of several classes improves, including
    the biggest jump of 40% to 63.5% for the most imbalanced class (9).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some of the other miscellaneous algorithm-based techniques
    that can still help us deal with imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing other algorithm-based techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore a diverse set of algorithm-level techniques
    that we haven’t covered so far. Intriguingly, these methods – from regularization
    techniques that mitigate overfitting to Siamese networks skilled in one-shot and
    few-shot learning, to deeper neural architectures and threshold adjustments –
    also have a beneficial side effect: they can occasionally mitigate the impact
    of class imbalance.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper from S. Alshammari et al. [14] found that well-known regularization
    techniques such as L2-regularization and the MaxNorm constraint are quite helpful
    in long-tailed recognition. The paper proposes to do these only at the last layer
    of classification (sigmoid or softmax, for example). Here are their findings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2-regularization** (also called weight decay) generally keeps the weights
    in check and helps the model generalize better by preventing the model from overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tf.keras.constraints.MaxNorm`, while PyTorch has `torch.clamp` to help
    with this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siamese networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On a similar note, previous research has found **Siamese networks** to be very
    robust to the adverse effects of class imbalance. Siamese networks have been quite
    useful in the areas of one-shot learning (classifying new data when we have only
    one example of each class in the training data) and few-shot learning (classifying
    new data when we have only a few examples of each class in the training data).
    Siamese networks use a contrastive loss function that takes in pairs of input
    images and then computes a similarity metric (Euclidean distance, Manhattan distance,
    or cosine distance) to figure out how similar or dissimilar they are. This can
    be used to compute the embeddings of each unique class of images in the training
    data. At inference time or test time, the distance of the new input image from
    each unique class can be computed to find the appropriate class of the image.
    The best part of this technique is that it provides a way to learn the feature
    representation of each class. Siamese networks have found a wide variety of practical
    applications in the industry regarding vision problems (for example, whether two
    images are of the same person or not) as well as NLP problems (for example, finding
    out whether two questions/queries are similar or not on, say, platforms such as
    Stack Overflow, Quora, Google, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.20* shows a Siamese network where two inputs are fed into the model
    to get their embeddings, which are then compared for similarity using a distance
    metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – High-level working of the Siamese network model
  prefs: []
  type: TYPE_NORMAL
- en: Deeper neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A study by Ding et al. 2017 [15] discovered that deeper neural networks (more
    than 10 layers) are more helpful in general with imbalanced datasets for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: A faster rate of convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better overall performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is attributed to the fact that deep networks are exponentially more efficient
    at capturing the complexity of data. Though their experiment was for facial action
    recognition tasks, this may be useful for trying out deeper networks on other
    kinds of data and domains.
  prefs: []
  type: TYPE_NORMAL
- en: However, the cons of longer training times, increased hardware cost, and increased
    complexity may not always be worth the hassle in industry settings.
  prefs: []
  type: TYPE_NORMAL
- en: Threshold adjustment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive
    Learning*, threshold adjustment is a cost-sensitive meta-learning technique. Threshold
    adjustment applies equally well to deep learning models, and it can be critical
    to make sure that the thresholds for classification are properly tuned and adjusted,
    especially when the training data distribution has been changed (for example,
    oversampled or undersampled) or even when class weights or new loss functions
    are used.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various loss functions as remedies to class imbalance.
    We started with the class-weighting technique and deferred re-weighting, both
    designed to penalize errors on minority class samples. As we progressed, we encountered
    focal loss, where we shifted from class-centric to sample-centric weighting, focusing
    on the difficulty of samples. Despite its merits, we learned that focal loss may
    still be biased toward the majority class when assigning weights to challenging
    samples across all classes. Subsequent discussions on class-balanced loss, CDT
    loss, and class-wise difficulty-balanced loss were provided, each introducing
    unique strategies to dynamically adjust weights or modulate the model’s focus
    between easy and challenging samples, aiming to enhance performance on imbalanced
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, algorithm-level techniques usually modify the loss functions used
    by the model in some way to accommodate for imbalances in the dataset. They typically
    do not increase the training time and cost, unlike data-level techniques. They
    are well suited for problems or domains with large amounts of data or where gathering
    more data is hard or expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Even though these techniques improve the performance of minority classes, the
    majority classes may sometimes suffer as a result. In the next chapter, we will
    look at some of the hybrid techniques that can combine the data-level and algorithm-level
    techniques so that we can get the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mean false error and mean squared false error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang et al. [16]proposed that regular loss functions poorly capture the errors
    from minority classes in the case of high data imbalance due to lots of negative
    samples that dominate the loss function. Hence, they proposed a new loss function
    where the main idea was to split the training error into four different kinds
    of errors:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: False Positive Error (FPE) = (1/number_of_negative_samples) * (error from negative
    samples)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negative Error (FNE) = (1/number_of_positive_samples) * (error from positive
    samples)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean False Error (MFE) = FPE+ FNE
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Squared False Error (MSFE) = FPE2 + FNE2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The error here could be computed using the usual cross-entropy loss or any other
    loss used for classification. Implement the MFE and MSFE loss functions for both
    the imbalanced MNIST and CIFAR10-LT datasets, and see whether the model performance
    improves over the baseline of cross-entropy loss.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this chapter, while implementing the CDT loss function, replace the imbalanced
    MNIST dataset with CIFAR10-LT (the long-tailed version of CIFAR-10). Check whether
    you still achieve improved performance over the baseline. You may have to play
    with the gamma value or perform any of the other tricks mentioned in the original
    paper [12] to get an improvement over the baseline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tversky Loss was introduced in the paper by Salehi et al. [17]. Please read
    this paper to understand the Tversky loss function and its implementation details.
    Finally, implement the Tversky loss on an imbalanced MNIST dataset and compare
    its performance with a baseline model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the class-weighting technique and cross-entropy loss with the `trec`
    dataset in this chapter. Replace cross-entropy loss with focal loss, and see whether
    model performance improves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*DALL·E 2 pre-training mitigations*, 2022, [https://openai.com/research/dall-e-2-pre-training-mitigations](https://openai.com/research/dall-e-2-pre-training-mitigations).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*BERT Does Business: Implementing the BERT Model for Natural Language Processing
    at Wayfair*, 2019, [https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair](https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*, [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, *Focal Loss for Dense
    Object Detection*. arXiv, Feb. 07, 2018, [http://arxiv.org/abs/1708.02002](http://arxiv.org/abs/1708.02002).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang et al., *Imbalance-XGBoost: leveraging weighted and focal losses for binary
    label-imbalanced classification with* *XGBoost*, [https://arxiv.org/pdf/1908.01672.pdf](https://arxiv.org/pdf/1908.01672.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Focal loss implementation for* *LightGBM*, [https://maxhalford.github.io/blog/lightgbm-focal-loss](https://maxhalford.github.io/blog/lightgbm-focal-loss).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The PASCAL VOC* *project*, [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*fastai library*, 2018, [https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb](https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Community Standards report*, 2019, [https://ai.meta.com/blog/community-standards-report/](https://ai.meta.com/blog/community-standards-report/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, *Class-Balanced Loss Based
    on Effective Number of Samples*, p. 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Zhang et al., *Screen Recognition: Creating Accessibility Metadata for Mobile
    Applications from Pixels*, in Proceedings of the 2021 CHI Conference on Human
    Factors in Computing Systems, Yokohama Japan: ACM, May 2021, pp. 1–15\. doi: 10.1145/3411764.3445186\.
    Blog: [https://machinelearning.apple.com/research/mobile-applications-accessible](https://machinelearning.apple.com/research/mobile-applications-accessible).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available: [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Sinha, H. Ohashi, and K. Nakamura, *Class-Wise Difficulty-Balanced Loss
    for Solving Class-Imbalance*. arXiv, Oct. 05, 2020\. Accessed: Dec. 17, 2022\.
    [Online]. Available at [http://arxiv.org/abs/2010.01824](http://arxiv.org/abs/2010.01824).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Alshammari, Y.-X. Wang, D. Ramanan, and S. Kong, *Long-Tailed Recognition
    via Weight Balancing*, in 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 6887–6897\. Doi: 10.1109/CVPR52688.2022.00677.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. Ding, D.-Y. Huang, Z. Chen, X. Yu, and W. Lin, *Facial action recognition
    using very deep networks for highly imbalanced class distribution*, in 2017 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), Kuala Lumpur, Dec. 2017, pp. 1368–1372\. doi: 10.1109/APSIPA.2017.8282246.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, *Training deep
    neural networks on imbalanced datasets*, in 2016 International Joint Conference
    on Neural Networks (IJCNN), Vancouver, BC, Canada, Jul. 2016, pp. 4368–4374\.
    doi: 10.1109/IJCNN.2016.7727770.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. S. M. Salehi, D. Erdogmus, and A. Gholipour, *Tversky loss function for
    image segmentation using 3D fully convolutional deep networks*. arXiv, Jun. 18,
    2017\. Accessed: Dec. 23, 2022\. [Online]. Available at [http://arxiv.org/abs/1706.05721](http://arxiv.org/abs/1706.05721).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
