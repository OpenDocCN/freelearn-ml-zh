- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Algorithm-Level Deep Learning Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法级深度学习技术
- en: The data-level deep learning techniques have problems very similar to classical
    ML techniques. Since deep learning algorithms are quite different from classical
    ML techniques, we’ll explore some algorithm-level techniques for addressing data
    imbalance in this chapter. These algorithm-level techniques won’t change the data
    but accommodate the model instead. This exploration might uncover new insights
    or methods to better handle imbalanced data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据级深度学习技术存在与经典机器学习技术非常相似的问题。由于深度学习算法与经典机器学习技术有很大不同，因此在本章中，我们将探讨一些算法级技术来解决数据不平衡问题。这些算法级技术不会改变数据，而是适应模型。这种探索可能会揭示新的见解或方法，以更好地处理不平衡数据。
- en: This chapter will be on the same lines as [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, extending the ideas to deep learning models. We will
    look at algorithm-level deep learning techniques to handle the imbalance in data.
    Generally, these techniques do not modify the training data and often require
    no pre-processing steps, offering the benefit of no increased training times or
    additional runtime hardware costs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将与[*第五章*](B17259_05.xhtml#_idTextAnchor151)的*成本敏感学习*保持一致，将思想扩展到深度学习模型。我们将探讨算法级深度学习技术来处理数据不平衡问题。通常，这些技术不会修改训练数据，并且通常不需要预处理步骤，从而提供了无需增加训练时间或额外运行时硬件成本的优点。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Motivation for algorithm-level techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法级技术的动机
- en: Weighting techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权技术
- en: Explicit loss function modification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确损失函数修改
- en: Discussing other algorithm-based techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论其他基于算法的技术
- en: By the end of this chapter, you’ll understand how to manage imbalanced data
    through model weight adjustments and loss function modifications using PyTorch
    APIs. We’ll also explore other algorithmic strategies, equipping you to make informed
    decisions in real-world applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何通过使用PyTorch API对模型权重进行调整和修改损失函数来管理不平衡数据。我们还将探索其他算法策略，使您能够在现实世界的应用中做出明智的决策。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will mostly be using standard functions from PyTorch and `torchvision` throughout
    this chapter. We will also use the Hugging Face Datasets library for dealing with
    text data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要使用PyTorch和`torchvision`的标准函数。我们还将使用Hugging Face Datasets库来处理文本数据。
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter’s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08)。像往常一样，您可以通过点击本章笔记本顶部的**在Colab中打开**图标或通过使用笔记本的GitHub
    URL从[https://colab.research.google.com](https://colab.research.google.com)启动它来打开GitHub笔记本。
- en: Motivation for algorithm-level techniques
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法级技术的动机
- en: In this chapter, we will concentrate on deep learning techniques that have gained
    popularity in both the vision and text domains. We will mostly use a long-tailed
    imbalanced version of the MNIST dataset, similar to what we used in [*Chapter
    7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep Learning Methods*. We
    will also consider CIFAR10-LT, the long-tailed version of CIFAR10, which is quite
    popular among researchers working with long-tailed datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于在视觉和文本领域都受到欢迎的深度学习技术。我们将主要使用与[*第七章*](B17259_07.xhtml#_idTextAnchor205)的*数据级深度学习方法*中使用的类似的MNIST数据集的长尾不平衡版本。我们还将考虑CIFAR10-LT，这是CIFAR10的长尾版本，在处理长尾数据集的研究者中相当受欢迎。
- en: In this chapter, the ideas will be very similar to what we learned in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*, where the high-level
    idea was to increase the weight of the positive (minority) class and decrease
    the weight of the negative (majority) class in the cost function of the model.
    To facilitate this adjustment to the loss function, frameworks such as `scikit-learn`
    and XGBoost offer specific parameters. `scikit-learn` provides options such as
    `class_weight` and `sample_weight`, while XGBoost offers `scale_pos_weight` as
    a parameter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论的想法将与我们在[*第五章*](B17259_05.xhtml#_idTextAnchor151)中学习的相似，即*成本敏感学习*，其中高级思想是在模型的成本函数中增加正类（少数类）的权重并减少负类（多数类）的权重。为了便于调整损失函数，`scikit-learn`和XGBoost等框架提供了特定的参数。`scikit-learn`提供了如`class_weight`和`sample_weight`等选项，而XGBoost提供了`scale_pos_weight`作为参数。
- en: In deep learning, the idea remains the same, and PyTorch provides a `weight`
    parameter in the `torch.nn.CrossEntropyLoss` class to implement this weighting
    idea.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，这个想法保持不变，PyTorch在`torch.nn.CrossEntropyLoss`类中提供了一个`weight`参数来实现这个加权思想。
- en: However, we will see some advanced techniques that are more relevant and might
    give better results for the deep learning models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将看到一些更相关且可能为深度学习模型带来更好结果的先进技术。
- en: With imbalanced datasets, the majority class examples contribute much more to
    the overall loss than the minority class examples. This happens because the majority
    class examples heavily outnumber the minority class examples. This means that
    the loss function being used is naturally biased toward the majority classes,
    and it fails to capture the error from minority classes. Keeping this in mind,
    can we change the loss function to account for this discrepancy for imbalanced
    datasets? Let’s try to figure this out.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡的数据集中，多数类别的示例对整体损失的贡献远大于少数类别的示例。这是因为多数类别的示例数量远远超过少数类别的示例。这意味着所使用的损失函数自然地偏向多数类别，并且无法捕捉到少数类别的错误。考虑到这一点，我们能否改变损失函数来考虑这种不平衡数据集的差异？让我们试着找出答案。
- en: 'The cross-entropy loss for binary classification is defined as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类的交叉熵损失定义为以下：
- en: CrossEntropyLoss(p) = {− log(p) if y = 1 (minority class term)    − log(1 −
    p) otherwise (majority class term)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CrossEntropyLoss(p) = {− log(p) if y = 1 (minority class term)    − log(1 −
    p) otherwise (majority class term)
- en: Let’s say y = 1 represents the minority class and it’s the class we are trying
    to predict. So, we can try to increase the minority class term by multiplying
    it with a higher value of weight to increase its attribution to the overall loss.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设y = 1代表少数类别，这是我们试图预测的类别。因此，我们可以通过乘以一个更高的权重值来增加少数类别的权重，从而增加其对整体损失的贡献。
- en: Weighting techniques
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权技术
- en: 'Let’s continue to use the imbalanced MNIST dataset from the previous chapter,
    which has long-tailed data distribution, as shown in the following bar chart (*Figure
    8**.1*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用上一章中的不平衡MNIST数据集，它具有长尾数据分布，如下面的条形图（*图8.1*）所示：
- en: '![](img/B17259_08_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_01.jpg)'
- en: Figure 8.1 – Imbalanced MNIST dataset
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 不平衡MNIST数据集
- en: Here, the *x* axis is the class label, and the *y* axis is the count of samples
    of various classes. In the next section, we will see how to use the weight parameter
    in PyTorch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*轴是类别标签，*y*轴是各种类别的样本计数。在下一节中，我们将看到如何在PyTorch中使用权重参数。
- en: 'We will use the following model code for all the vision-related tasks in this
    chapter. We have defined a PyTorch neural network class called `Net` with two
    convolutional layers, a dropout layer, and two fully connected layers. The `forward`
    method applies these layers sequentially along with ReLU activations and max-pooling
    to process the input, `x`. Finally, it returns the `log_softmax` activation of
    the output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下模型代码来完成本章中所有与视觉相关任务。我们定义了一个名为`Net`的PyTorch神经网络类，包含两个卷积层、一个dropout层和两个全连接层。`forward`方法按顺序应用这些层，包括ReLU激活和最大池化来处理输入`x`。最后，它返回输出的`log_softmax`激活：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since our final layer of the model uses `log_softmax`, we will be using negative
    log-likelihood loss (`torch.nn.functional.nll_loss` or `torch.nn.NLLLoss`) from
    PyTorch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型最终层使用`log_softmax`，我们将使用PyTorch的负对数似然损失（`torch.nn.functional.nll_loss`或`torch.nn.NLLLoss`）。
- en: Using PyTorch’s weight parameter
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch的权重参数
- en: 'In the `torch.nn.CrossEntropyLoss` API, we have a `weight` parameter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在`torch.nn.CrossEntropyLoss` API中，我们有一个`weight`参数：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `weight` is a one-dimensional tensor that assigns weight to each class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`weight`是一个一维张量，为每个类别分配权重。
- en: 'We can use the `compute_class_weight` function from `scikit-learn` to get the
    weights of various classes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit-learn`中的`compute_class_weight`函数来获取各种类别的权重：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This outputs the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `compute_class_weight` function computes the weights according to the following
    formula for each class, as we saw in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_class_weight` 函数根据以下公式为每个类别计算权重，正如我们在[*第五章*](B17259_05.xhtml#_idTextAnchor151)中看到的，*成本敏感学习*：'
- en: weight _ class _ a =  1  _______________________   total _ num _ samples _ for
    _ class _ a  *  total _ number _ of _ samples  ___________________  number _ of
    _ classes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: weight_class_a = 1 / (total_num_samples_for_class_a * total_number_of_samples
    / number_of_classes)
- en: 'In *Figure 8**.2*, these weights have been plotted using a bar chart to help
    us see how they relate to the class frequency (*y* axis) for each class (*x* axis):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.2中，这些权重已经用条形图绘制出来，以帮助我们了解它们如何与每个类别的频率（*y*轴）和类别（*x*轴）相关：
- en: '![](img/B17259_08_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_02.jpg)'
- en: Figure 8.2 – Bar chart showing weights corresponding to each class
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 每个类别对应的权重条形图
- en: As this figure shows, the fewer the number of samples a class has, the higher
    its weight.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如此图所示，一个类别的样本数量越少，其权重就越高。
- en: Tip
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The key takeaway here is that the weight of a class is inversely proportional
    to the number of samples of that class, also called inverse class frequency weighting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键点是，一个类别的权重与该类别的样本数量成反比，也称为逆类别频率加权。
- en: Another point to remember is that the class weights should always be computed
    from the training data. Using validation data or test data to compute the class
    weights might lead to the infamous data leakage or label leakage problem in ML.
    Formally, data leakage can happen when some information from outside of the training
    data is fed to the model. In this case, if we use test data to compute the class
    weights, then our evaluation of the model’s performance is going to be biased
    and invalid.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，类别权重应该始终从训练数据中计算。使用验证数据或测试数据来计算类别权重可能会导致机器学习中的著名的数据泄露或标签泄露问题。正式来说，数据泄露可能发生在训练数据之外的信息被输入到模型中。在这种情况下，如果我们使用测试数据来计算类别权重，那么我们对模型性能的评价将会是有偏的且无效的。
- en: 'The comic in *Figure 8**.3* shows a juggler managing weights of different sizes,
    each labeled with a distinct class label, symbolizing the varying weights assigned
    to different classes to tackle class imbalance during model training:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3中的漫画展示了一位魔术师管理不同大小的权重，每个权重都标有不同的类别标签，象征着在模型训练过程中为处理类别不平衡而分配给不同类别的不同权重：
- en: '![](img/B17259_08_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_03.jpg)'
- en: Figure 8.3 – Comic illustrating the core idea behind class weighting
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 描述类别加权核心思想的漫画
- en: Tip
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Another way to compute weights is to empirically tune the weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 计算权重的另一种方法是经验性地调整权重。
- en: 'Let’s write the training loop:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写训练循环：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A lot of other loss functions in PyTorch, including `NLLLoss`, `MultiLabelSoftMarginLoss`,
    `MultiMarginLoss`, and `BCELoss`, accept `weight` as a parameter as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的许多其他损失函数，包括`NLLLoss`、`MultiLabelSoftMarginLoss`、`MultiMarginLoss`和`BCELoss`，也接受`weight`作为参数。
- en: '*Figure 8**.4* compares the accuracy of various classes when using class weights
    versus when not using class weights:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.4*。4比较了使用类别权重和不使用类别权重时各种类别的准确率：'
- en: '![](img/B17259_08_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_04.jpg)'
- en: Figure 8.4 – Performance comparison of a model trained using cross-entropy loss
    with no class weights and with class weights
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 使用无类别权重和有类别权重训练的模型性能比较
- en: As we can see, although the accuracy dropped for classes 0-4, it improved dramatically
    for the most imbalanced classes of 5-9\. The overall accuracy of the model went
    up as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，尽管0-4类别的准确率有所下降，但5-9类这种最不平衡的类别的准确率有了显著提高。模型的总体准确率也有所上升。
- en: Warning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'Please note that some loss functions, such as `BCEWithLogitsLoss`, provide
    two weighting parameters (`BCEWithLogitsLoss` can be used for binary classification
    or multi-label classification):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些损失函数，如`BCEWithLogitsLoss`，提供了两个加权参数（`BCEWithLogitsLoss`可用于二分类或多标签分类）：
- en: • The `weight` parameter is the manual rescaling weight parameter for each example
    of the batch. This is more like the `sample_weight` parameter of the `sklearn`
    library.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: • `weight`参数是每个批次的示例的手动缩放权重参数。这更像是`sklearn`库中的`sample_weight`参数。
- en: • The `pos_weight` parameter is used to specify a weight for the positive class.
    It is similar to the `class_weight` parameter in the `sklearn` library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: • `pos_weight`参数用于指定正类的权重。它与`sklearn`库中的`class_weight`参数类似。
- en: 🚀 Class reweighting in production at OpenAI
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 OpenAI在生产中实现类重新加权
- en: OpenAI was trying to solve the problem of bias in training data using the image
    generation model DALL-E 2 [1]. DALL-E 2 is trained on a massive dataset of images
    from the internet, which can contain biases. For example, the dataset may contain
    more images of men than women or more images of people from certain racial or
    ethnic groups than others.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI试图通过图像生成模型DALL-E 2 [1]来解决训练数据中的偏差问题。DALL-E 2在来自互联网的大量图像数据集上训练，这些数据集可能包含偏差。例如，数据集中可能包含比女性更多的男性图像，或者比其他种族或民族群体更多的图像。
- en: 'To limit undesirable model capabilities (such as generating violent images),
    they first filtered out such images from the training dataset. However, filtering
    training data can amplify biases. Why? In their blog [1], they explain using an
    example that when generating images for the prompt “a CEO,” their filtered model
    showed a stronger male bias than the unfiltered one. They suspected this amplification
    arose from two sources: dataset bias toward sexualizing women and potential classifier
    bias, despite their efforts to mitigate them. This may have resulted in the filter
    removing more images of women, skewing the training data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制不希望有的模型能力（例如生成暴力图像），他们首先从训练数据集中过滤掉了这样的图像。然而，过滤训练数据可能会放大偏差。为什么？在他们的博客[1]中，他们用一个例子解释说，当为提示“一个CEO”生成图像时，他们的过滤模型比未过滤的模型显示出更强的男性偏差。他们怀疑这种放大可能源于两个来源：数据集对女性性化的偏差以及潜在的分类器偏差，尽管他们努力减轻这些偏差。这可能导致过滤器移除更多女性的图像，从而扭曲训练数据。
- en: To fix this bias, OpenAI applied reweighting to the DALL-E 2 training data by
    training a classifier to predict whether an image was from the unfiltered dataset.
    The weights for each image were then computed based on the classifier’s prediction.
    This scheme was shown to reduce the frequency change induced by filtering, which
    means that it was effective at counteracting the biases in the training data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题偏差，OpenAI通过对DALL-E 2训练数据进行重新加权来应用，通过训练一个分类器来预测图像是否来自未过滤的数据集。然后根据分类器的预测计算每个图像的权重。这种方案已被证明可以减少由过滤引起的频率变化，这意味着它在对抗训练数据中的偏差方面是有效的。
- en: Next, to show its extensive applicability, we will apply the class weighting
    technique to textual data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了展示其广泛的应用性，我们将对文本数据应用类加权技术。
- en: Handling textual data
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: 'Let’s work with some text data. We will use the `datasets` and `transformers`
    libraries from Hugging Face. Let’s import the `trec` dataset (the **Text Retrieval
    Conference** (**TREC**), a question classification dataset containing 5,500 labeled
    questions in the training set and 500 in the test set):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一些文本数据。我们将使用Hugging Face的`datasets`和`transformers`库。让我们导入`trec`数据集（**文本检索会议**（**TREC**），一个包含训练集5,500个标记问题和测试集500个问题的问答分类数据集）：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This dataset is balanced, so we randomly remove examples from classes ABBR
    and DESC, making those classes the most imbalanced. Here is how the distribution
    of various classes looks like in this dataset, confirming the imbalance in data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是平衡的，因此我们随机从ABBRA和DESC类中移除示例，使这些类成为最不平衡的。以下是这个数据集中各种类的分布情况，证实了数据的不平衡性：
- en: '![](img/B17259_08_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_05.jpg)'
- en: Figure 8.5 – Frequency of various classes in the trec dataset from the Hugging
    Face Datasets library
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – Hugging Face Datasets库中trec数据集中各种类的频率
- en: 'Let’s create a tokenizer (that splits text into words or sub-words) for the
    pre-trained DistilBERT language model vocabulary with a maximum input token length
    of 512:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为预训练的DistilBERT语言模型词汇创建一个最大输入标记长度为512的分词器（将文本分割成单词或子词）：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will create tokenized train and test sets from the dataset we just
    imported by invoking the tokenizer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过调用分词器从我们刚刚导入的数据集中创建标记化的训练集和测试集：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s instantiate the model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实例化模型：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s define and invoke a function to get training arguments:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义和调用一个函数来获取训练参数：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following `custom_compute_metrics()` function returns a dictionary containing
    the precision, recall, and F1 score:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`custom_compute_metrics()`函数返回一个包含精确率、召回率和F1分数的字典：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let’s implement the class containing the loss function that uses class
    weights:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现包含使用类权重的损失函数的类：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can initialize the weights similar to how we did previously using the `compute_class_weight`
    function in `sklearn`, and then feed it to the `CrossEntropyLoss` function in
    our `CustomTrainerWeighted` class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样使用`sklearn`中的`compute_class_weight`函数初始化权重，然后将其输入到我们的`CustomTrainerWeighted`类中的`CrossEntropyLoss`函数：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As shown in *Figure 8**.6*, we can see improvements in performance for the
    most imbalanced classes. However, a slight reduction was observed for the majority
    class (trade-off!):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图8**.6所示，我们可以看到在处理最不平衡的类别时性能有所提升。然而，对于多数类别（权衡！）观察到轻微的下降：
- en: '![](img/B17259_08_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_06.jpg)'
- en: Figure 8.6 – Confusion matrix using no class weighting (left) and with class
    weights (right)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 使用无类别加权（左）和有类别加权（右）的混淆矩阵
- en: As we can see, the minority classes, **ABBR** and **DESC**, have improved performance
    after class weighting at the cost of reduced performance for the **ENTY** class.
    Also, looking at some of the off-diagonal entries, we can see that the confusion
    between the **ABBR** and **DESC** classes (0.33 in *Figure 8**.6* (left)) and
    between the **DESC** and **ENTY** classes (0.08 in *Figure 8**.6* (left)) significantly
    dropped when using class weights (0.22 and 0.04, respectively).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，少数类别**ABBR**和**DESC**在类别加权后性能有所提升，但**ENTY**类别的性能有所下降。此外，观察一些非对角线项，我们可以看到**ABBR**和**DESC**类别之间的混淆（*图8**.6*（左）中的0.33）以及**DESC**和**ENTY**类别之间的混淆（*图8**.6*（左）中的0.08）在使用类别加权时显著下降（分别为0.22和0.04）。
- en: Some variants that deal with NLP tasks in particular suggest weighting the samples
    as the inverse of the square root of class frequency for their corresponding class
    instead of using the previously used inverse class frequency weighting technique.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特定于NLP任务的变体建议将样本的权重设置为对应类别的平方根倒数，而不是使用之前使用的逆类别频率加权技术。
- en: In essence, class weighting can usually help with any kind of deep learning
    model, including textual data, when working with imbalanced data. Since data augmentation
    techniques are not as straightforward for text as they are for images, class weighting
    can be a useful technique for NLP problems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，类别加权通常可以帮助任何类型的深度学习模型，包括文本数据，在处理不平衡数据时。由于数据增强技术在文本上的应用不如图像直接，因此类别加权对于NLP问题来说可以是一个有用的技术。
- en: 🚀 Class reweighting in production at Wayfair
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Wayfair生产中的类别重加权
- en: Wayfair used the BERT language model to improve the accuracy of its product
    search and recommendation system [2]. This was a challenging problem because the
    number of products that Wayfair sells is very large, and the number of products
    that a customer is likely to be interested in is much smaller.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Wayfair使用BERT语言模型来提高其产品搜索和推荐系统的准确性[2]。这是一个具有挑战性的问题，因为Wayfair销售的产品数量非常大，而客户可能感兴趣的产品数量则相对较小。
- en: There was an imbalance in data because the number of products that a customer
    had interacted with (for example, viewed, added to cart, or purchased) was much
    smaller than the number of products that the customer hadn’t interacted with.
    This made it difficult for BERT to learn to accurately predict which products
    a customer was likely to be interested in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存在不平衡，因为客户互动过的产品数量（例如，查看、加入购物车或购买）远小于客户未互动过的产品数量。这使得BERT难以学习准确预测客户可能感兴趣的产品。
- en: Wayfair used class weighting to address the data imbalance problem. They assigned
    a higher weight to positive examples (that is, products that a customer had interacted
    with) than to negative examples (that is, products that a customer had not interacted
    with). This helped ensure that BERT learned to accurately classify both positive
    and negative examples, even when the data was imbalanced.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Wayfair使用类别加权来解决数据不平衡问题。他们给正例（即客户互动过的产品）分配比负例（即客户未互动过的产品）更高的权重。这有助于确保BERT能够准确分类正例和负例，即使数据不平衡。
- en: The model was deployed to production. Wayfair is using the model to improve
    the accuracy of its product search and recommendation system and to provide a
    better experience for customers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已部署到生产环境中。Wayfair正在使用该模型来提高其产品搜索和推荐系统的准确性，并为顾客提供更好的体验。
- en: In the next section, we will discuss a minor variant of class weighting that
    can sometimes be more helpful than just the weighting technique.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一个类加权的小变体，有时它比单纯的加权技术更有帮助。
- en: Deferred re-weighting – a minor variant of the class weighting technique
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟重新加权 – 类权重技术的一个小变体
- en: 'There is a deferred re-weighting technique (mentioned by Cao et al. [3]) similar
    to the two-phase sampling approach we discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*. Here, we defer the re-weighting to later,
    wherein in the first phase of training, we train the model on the full imbalanced
    dataset without any weighting or sampling. In the second phase, we re-train the
    same model from the first phase with class weights (that are inversely proportional
    to the class frequencies) that have been applied to the loss function and, optionally,
    use a smaller learning rate. The first phase of training serves as a good form
    of initialization for the model for the second phase of training with reweighted
    losses. Since we use a smaller learning rate in the second phase of training,
    the weights of the model do not move very far from what they were in the first
    phase of training:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种延迟重新加权技术（由Cao等人[3]提到），类似于我们在[*第7章*](B17259_07.xhtml#_idTextAnchor205)“数据级深度学习方法”中讨论的两阶段采样方法。在这里，我们将重新加权推迟到后面，即在训练的第一阶段，我们在没有任何加权或采样的情况下在完整的不平衡数据集上训练模型。在第二阶段，我们使用已经应用于损失函数的类权重（与类频率成反比）重新训练第一阶段相同的模型，并且可以选择使用较小的学习率。训练的第一阶段为第二阶段使用重新加权损失的训练提供了良好的模型初始化形式。由于我们在训练的第二阶段使用较小的学习率，因此模型的权重不会从第一阶段训练的权重移动得太远：
- en: '![](img/B17259_08_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_07.jpg)'
- en: Figure 8.7 – The deferred re-weighting technique
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 延迟重新加权技术
- en: 'The comic in *Figure 8**.8* shows a magician who pulls out a large rabbit from
    a hat, followed by a smaller one, illustrating the two-phase process of initially
    training on the imbalanced dataset and subsequently applying re-weighting for
    more balanced training in the second phase:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8*.8中的漫画展示了一位魔术师从一个帽子里变出一只大兔子，然后又变出一只小兔子，这描绘了两个阶段的过程：最初在不平衡数据集上训练，然后在第二阶段应用重新加权以实现更平衡的训练：'
- en: '![](img/B17259_08_08.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_08.jpg)'
- en: Figure 8.8 – A comic illustrating the core idea of deferred re-weighting
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 一幅描绘延迟重新加权核心思想的漫画
- en: 'Please refer to the notebook titled `Deferred_reweighting_DRW.ipynb` in this
    book’s GitHub repository for more details. After applying the two-phase training
    part of the deferred re-weighting technique, we can see that the accuracy of our
    most imbalanced classes improves compared to training with cross-entropy loss:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本书GitHub仓库中标题为`Deferred_reweighting_DRW.ipynb`的笔记本以获取更多详细信息。在应用延迟重新加权技术的两阶段训练部分之后，我们可以看到与使用交叉熵损失训练相比，我们最不平衡的类的准确率有所提高：
- en: '![](img/B17259_08_09.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_09.jpg)'
- en: Figure 8.9 – Performance comparison of deferred re-weighting with cross-entropy
    loss
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 延迟重新加权与交叉熵损失的性能比较
- en: Next, we will look at defining custom loss functions when the PyTorch standard
    loss functions don’t do everything that we want them to do.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨在PyTorch标准损失函数无法完成我们想要的所有事情时如何定义自定义损失函数。
- en: Explicit loss function modification
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式损失函数修改
- en: In PyTorch, we can formulate custom loss functions by deriving a subclass from
    the `nn.Module` class and overriding the `forward()` method. The `forward()` method
    for a loss function accepts the predicted and actual outputs as inputs, subsequently
    returning the computed loss value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们可以通过从`nn.Module`类派生一个子类并重写`forward()`方法来定义自定义损失函数。损失函数的`forward()`方法接受预测输出和实际输出作为输入，随后返回计算出的损失值。
- en: Even though class weighting does assign different weights to balance the majority
    and minority class examples, this alone is often insufficient, especially in cases
    of extreme class imbalance. What we would like is to reduce the loss from easily
    classified examples as well. The reason is that such easily classified examples
    usually belong to the majority class, and since they are higher in number, they
    dominate our training loss. This is the main idea of focal loss and allows for
    a more nuanced handling of examples, irrespective of the class they belong to.
    We’ll look at this in this section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管类权重确实为多数类和少数类示例分配了不同的权重以实现平衡，但这通常是不够的，尤其是在极端类别不平衡的情况下。我们希望的是减少易于分类示例的损失。原因是这样的易于分类示例通常属于多数类，由于数量较多，它们主导了我们的训练损失。这就是焦点损失的主要思想，它允许对示例进行更细致的处理，无论它们属于哪个类别。我们将在本节中探讨这一点。
- en: Understanding the forward() method in PyTorch
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 理解PyTorch中的forward()方法
- en: 'In PyTorch, you’ll encounter the `forward()` method in both neural network
    layers and loss functions. That’s because both a neural network layer and a loss
    function are derived from `nn.Module`. While it might seem confusing at first,
    understanding the context can help clarify its role:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，你将在神经网络层和损失函数中遇到 `forward()` 方法。这是因为神经网络层和损失函数都是从 `nn.Module` 派生出来的。虽然一开始可能看起来有些混乱，但理解上下文可以帮助阐明其作用：
- en: '**🟠** **In neural** **network layers**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**🟠** **在神经网络层**：'
- en: The `forward()` method defines the transformation that input data undergoes
    as it passes through the layer. This could involve operations such as linear transformations,
    activation functions, and more.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` 方法定义了输入数据在通过层时经历的转换。这可能涉及线性变换、激活函数等操作。'
- en: '**🟢** **In** **loss functions**:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**🟢** **在损失函数中**：'
- en: The `forward()` method computes the loss between the predicted output and the
    actual target values. This loss serves as a measure of how well the model is performing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` 方法计算预测输出和实际目标值之间的损失。这个损失作为衡量模型性能好坏的指标。'
- en: '**🔑****Key takeaway**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**🔑****关键要点**：'
- en: In PyTorch, both neural network layers and loss functions inherit from `nn.Module`,
    providing a unified interface. The `forward()` method is central to both, serving
    as the computational engine for data transformation in layers and loss computation
    in loss functions. Think of `forward()` as the “engine” for either process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，神经网络层和损失函数都继承自 `nn.Module`，提供了一个统一的接口。`forward()` 方法对两者都是核心的，作为层中数据转换和损失函数中损失计算的计算引擎。将
    `forward()` 视为这两个过程的“引擎”。
- en: Focal loss
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 焦点损失
- en: The techniques we’ve studied so far presume that minority classes need higher
    weights due to weak representation. However, some minority classes may be adequately
    represented, and over-weighting their samples could degrade the overall model
    performance. Hence, Tsung-Yi et al. [4] from Facebook (now Meta) introduced **focal
    loss**, a sample-based weighting technique where each example’s weight is determined
    by its difficulty and measured by the loss the model incurs on it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止研究的技术假定由于代表性较弱，少数类需要更高的权重。然而，一些少数类可能得到了充分的代表，过度加权它们的样本可能会降低整体模型性能。因此，Facebook（现Meta）的
    Tsung-Yi 等人 [4] 引入了**焦点损失**，这是一种基于样本的加权技术，其中每个示例的权重由其难度决定，并通过模型对其造成的损失来衡量。
- en: 'The focal loss technique has roots in dense object detection tasks, where there
    are significantly more observations in one class than the other:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失技术源于密集目标检测任务，在这些任务中，某一类别的观察结果比另一类多得多：
- en: '![](img/B17259_08_10.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_10.jpg)'
- en: Figure 8.10 – Class imbalance in object detection – majority as background,
    few as foreground
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 目标检测中的类别不平衡 – 多数类作为背景，少数类作为前景
- en: Focal loss downweighs easy-to-classify examples and focuses on hard-to-classify
    examples. What this means is that it would reduce the model’s overconfidence;
    this overconfidence usually prevents the model from generalizing well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失降低了易于分类的示例的权重，并专注于难以分类的示例。这意味着它会减少模型的过度自信；这种过度自信通常阻止模型很好地泛化。
- en: Focal loss is an extension of cross-entropy loss. It is especially good for
    multi-class classification, where some classes are easy and others are difficult
    to classify.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失是交叉熵损失的扩展。它特别适用于多类分类，其中一些类别易于分类，而其他类别则难以分类。
- en: 'Let’s start with our well-known cross-entropy loss for binary classification.
    If we let p be the predicted probability that y = 1, then the cross-entropy loss
    can be defined as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们熟知的二元分类的交叉熵损失开始。如果我们让 p 表示 y = 1 的预测概率，那么交叉熵损失可以定义为以下：
- en: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
- en: 'This can be rewritten as CrossEntropyLoss(p) = − log( p t), where p t, the
    probability of the true class, can be defined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重写为 CrossEntropyLoss(p) = − log( p t)，其中 p t，真实类的概率，可以定义为以下：
- en: p t = {p if y = 1  1 − p otherwise
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: p t = {p if y = 1  1 − p otherwise
- en: Here, p is the predicted probability that y = 1 from the model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，p 是模型预测 y = 1 的概率。
- en: The problem with this loss function is that in the case of imbalanced datasets,
    this loss function is dominated by the loss contribution from majority classes,
    and the loss contribution from the minority class is very small. This can be fixed
    via focal loss.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数的问题在于，在数据不平衡的情况下，这个损失函数主要由多数类的损失贡献所主导，而少数类的损失贡献非常小。这可以通过焦点损失来修复。
- en: So, what is focal loss?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是焦点损失？
- en: FocalLoss( p t) = − α (1 − p t) γ log( p t)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: FocalLoss( p t) = − α (1 − p t) γ log( p t)
- en: 'This formula looks slightly different from cross-entropy loss. There are two
    extra terms – α and (1 − p t) γ. Let’s try to understand the significance of each
    of these terms:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式看起来与交叉熵损失略有不同。有两个额外的项 – α 和 (1 − p t) γ。让我们尝试理解这些项的每个含义：
- en: 'α: This value can be set to be inversely proportional to the number of examples
    of positive (minority) classes and is used to weigh the minority class examples
    more than the majority class. It can also be treated as a hyperparameter that
    can be tuned.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α：这个值可以设置为与正（少数）类样本的数量成反比，并用于使少数类样本比多数类样本更重。它也可以被视为一个可以调整的超参数。
- en: '(1 − p t) γ: This term is called the **modulating factor**. If an example is
    too easy for the model to classify, that would mean that p t is very high and
    the whole modulating factor value will be close to zero (assuming γ > 1), and
    the model won’t focus on this example much. On the other hand, if an example is
    hard – that is, p t is low – then the modulating factor value will be high, and
    the model will focus on this example more.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1 − p t) γ：这个项被称为**调制因子**。如果一个样本对模型来说太容易分类，这意味着 p t 非常高，整个调制因子值将接近零（假设 γ >
    1），模型不会太关注这个样本。另一方面，如果一个样本很难分类 – 即 p t 低 – 那么调制因子值将很高，模型将更关注这个样本。
- en: Implementation
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: 'Here’s the implementation of focal loss from scratch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从头实现焦点损失的方法：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Though the focal loss technique has roots in computer vision and object detection,
    we can potentially reap its benefits while working with tabular data and text
    data too. Some recent research has ported focal loss to classical ML frameworks
    such as XGBoost [5] and LightGBM [6], as well as to text data that uses transformer-based
    models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管焦点损失技术在计算机视觉和目标检测领域有根源，但我们也可以在处理表格数据和文本数据时从中受益。一些最近的研究将焦点损失移植到经典的机器学习框架，如XGBoost
    [5] 和LightGBM [6]，以及使用基于transformer的模型的文本数据。
- en: 'The comic in *Figure 8**.11* shows an archer aiming at a small distant target,
    overlooking a large nearby target, symbolizing the focal loss’s emphasis on challenging
    minority class examples:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.11* 中的漫画展示了一位弓箭手瞄准一个远处的目标，却忽略了附近的大目标，象征着焦点损失对具有挑战性的少数类样本的重视：'
- en: '![](img/B17259_08_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_11.jpg)'
- en: Figure 8.11 – Illustration of focal loss
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 焦点损失的示意图
- en: 'PyTorch’s `torchvision` library already has this loss implemented for us to
    use:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`torchvision`库已经为我们实现了这个损失函数：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `alpha` and `gamma` values can be challenging to tune for the model and
    dataset being used. Using an `alpha` value of `0.25` and a `gamma` value of `2`
    with `reduction= ''mean''` on CIFAR10-LT (the long-tailed version of the CIFAR10
    dataset) seems to do better than the regular cross-entropy loss, as shown in the
    following graph. For more details, please check the `CIFAR10_LT_Focal_Loss.ipynb`
    notebook in this book’s GitHub repository:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所使用的模型和数据集，`alpha` 和 `gamma` 值可能难以调整。在CIFAR10-LT（CIFAR10数据集的长尾版本）上使用`alpha`值为`0.25`和`gamma`值为`2`，并且`reduction=
    'mean'`似乎比常规的交叉熵损失表现更好，如以下图表所示。更多详情，请查看本书GitHub仓库中的`CIFAR10_LT_Focal_Loss.ipynb`笔记本：
- en: '![](img/B17259_08_12.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_12.jpg)'
- en: Figure 8.12 – Model accuracy using cross-entropy loss versus focal loss (alpha=0.25,
    gamma=2) on the CIFAR10-LT dataset as training progresses
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 随着训练的进行，使用交叉熵损失与焦点损失（alpha=0.25，gamma=2）在CIFAR10-LT数据集上的模型准确度
- en: 'In the Pascal VOC dataset for object detection [7], the focal loss helps detect
    a motorbike in the image, while the cross-entropy loss wasn’t able to detect it
    (*Figure 8**.13*):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测的Pascal VOC数据集 [7] 中，焦点损失有助于检测图像中的摩托车，而交叉熵损失则无法检测到它 (*图8**.13*)：
- en: '![](img/B17259_08_13.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_08_13.jpg)'
- en: 'Figure 8.13 – Motorbike not detected by cross-entropy loss (left), while focal
    loss does detect it (right) on the Pascal VOC dataset. Source: fastai library
    GitHub repo [8]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 在Pascal VOC数据集中，交叉熵损失无法检测到摩托车（左），而焦点损失则可以检测到（右）。来源：fastai库GitHub仓库 [8]
- en: Though focal loss was initially designed for dense object detection, it has
    gained traction in class-imbalanced tasks due to its ability to assign higher
    weights to challenging examples that are commonly found in minority classes. While
    the proportion of such samples is higher in minority classes, the absolute count
    is higher in the majority class due to its larger size. Consequently, assigning
    high weights to challenging samples across all classes could still cause bias
    in the neural network’s performance. This motivates us to explore other loss functions
    that can reduce this bias.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管焦点损失最初是为密集目标检测而设计的，但由于其能够为在少数类中常见的具有挑战性的示例分配更高的权重，因此在类别不平衡的任务中获得了关注。虽然这种样本在少数类中的比例较高，但由于其规模较大，在多数类中的绝对数量更高。因此，对所有类别的挑战性样本分配高权重仍然可能导致神经网络性能的偏差。这促使我们探索其他可以减少这种偏差的损失函数。
- en: 🚀 Focal loss in production at Meta
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Meta生产环境中的焦点损失
- en: There was a need to detect harmful content, such as hate speech and violence,
    at Meta (previously Facebook) [9]. ML models were trained on a massive dataset
    of text and images that included both harmful and non-harmful content. However,
    the system was struggling to learn from the harmful content examples because they
    were much fewer in number than the non-harmful examples. This was causing the
    system to overfit the non-harmful examples, and it was not performing well in
    terms of detecting harmful content in the real world.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在Meta（之前称为Facebook）[9]中，需要检测有害内容，如仇恨言论和暴力。机器学习模型在包含有害和非有害内容的庞大文本和图像数据集上进行了训练。然而，由于有害内容的例子比非有害内容的例子少得多，系统在从有害内容示例中学习方面遇到了困难。这导致系统过度拟合非有害示例，在现实世界中检测有害内容的性能不佳。
- en: To solve the problem, Meta used focal loss. Focal loss, as we’ve seen, is a
    technique that down-weighs the easy-to-classify examples so that the system focuses
    on learning from the hard-to-classify examples. Meta implemented focal loss in
    their training pipeline and was able to improve the performance of their AI system
    when it came to detecting harmful content by up to 10%. This is a significant
    improvement, and it shows that focal loss is a promising technique for training
    AI systems to detect rare or difficult-to-classify events. The new system has
    been deployed into production at Meta, and it has helped to substantially improve
    the safety of the platform.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Meta使用了焦点损失。正如我们所见，焦点损失是一种降低易于分类的示例权重的技术，以便系统专注于从难以分类的示例中学习。Meta在其训练流程中实现了焦点损失，并在检测有害内容方面提高了其AI系统的性能，最高可达10%。这是一个重大的改进，表明焦点损失是训练AI系统检测罕见或难以分类事件的有前途的技术。新的系统已在Meta的生产环境中部署，并有助于显著提高平台的安全性。
- en: Class-balanced loss
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别平衡损失
- en: 'The paper by Cui et al. [10] made a minor change to the equation for cross-entropy
    loss by adding a multiplicative coefficient of  (1 − β) _ (1− β n) to the loss
    function – that is, we use a value of α =  (1 − β) _ (1− β n), where β is a hyperparameter
    between 0 and 1, and n is the number of samples of a class:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Cui等人[10]的论文通过在损失函数中添加乘性系数(1 − β)^(1− βn)对交叉熵损失方程进行微小修改——也就是说，我们使用α = (1 − β)^(1−
    βn)的值，其中β是一个介于0和1之间的超参数，n是某一类别的样本数量：
- en: ClassBalancedCrossEntropyLoss(p) = −  (1 − β) _ (1− β n)  log( p t)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ClassBalancedCrossEntropyLoss(p) = −(1 − β)^(1− βn) log(p_t)
- en: β = 0 means no weighting at all, while β → 1 means re-weighting by inverse class
    frequency. So, we can consider this method to be a way for the class weight of
    a particular class to be adjustable between 0 and (1/frequency of a class), depending
    on the value of the hyperparameter, β, which is a tunable parameter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: β = 0表示完全不进行加权，而β → 1表示通过类频率的倒数进行重新加权。因此，我们可以认为这是一种使特定类别的类别权重在0和(1/类别频率)之间可调的方法，这取决于超参数β的值，β是一个可调参数。
- en: 'This same term can be used in place of the alpha value. It can be used in conjunction
    with focal loss too:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相同的项可以用作alpha值的替代。它可以与焦点损失一起使用：
- en: ClassBalancedFocalLoss( p t) = −  (1 − β) _ (1− β n)  (1 − p t) γ log( p t)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ClassBalancedFocalLoss(p_t) = −(1 − β)^(1− βn) (1 − p_t)^γ log(p_t)
- en: According to Cui et al., the recommended setting for the beta value is (N-1)/N,
    where N is the total number of training examples.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Cui等人的研究，建议的beta值设置为(N-1)/N，其中N是训练示例的总数。
- en: 'The comic in *Figure 8**.14* illustrates the core idea of this loss. It shows
    a tightrope walker who maintains balance using a pole with weights labeled “beta”
    on both ends, representing the adjustment of class weights to address class imbalance:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.14* 中的漫画展示了这种损失的核心思想。它展示了一个使用带有两端标有“beta”重量的杆来保持平衡的走钢丝者，这代表着调整类别权重以解决类别不平衡：'
- en: '![](img/B17259_08_14.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_14.jpg)'
- en: Figure 8.14 – Illustration of class-balanced loss
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 类平衡损失的示意图
- en: Implementation
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: 'Let’s take a look at the code for implementing class-balanced loss:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实现类平衡损失的代码：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the `forward()` function, `effective_num` effectively computes (1-β n) as
    a vector, where `n` is a vector containing the number of samples per class. So,
    the `weights` vector is  (1 − β) _ (1− β n). Using these weights, we compute the
    loss by using `NLLLoss` between the output of the model and the corresponding
    labels. *Table 8.1* shows the class-wise accuracy when the model is trained using
    class-balanced cross-entropy loss for 20 epochs. Here, we can see an accuracy
    improvement for the most imbalanced classes of 9, 8, 7, 6, and 5:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward()`函数中，`effective_num`有效地计算了一个向量（1-βn），其中`n`是一个包含每个类样本数量的向量。因此，`weights`向量是(1
    − β) _ (1− βn)。使用这些权重，我们通过使用`NLLLoss`在模型的输出和相应的标签之间计算损失。*表8.1*显示了当模型使用类平衡交叉熵损失训练20个epoch时的类准确率。在这里，我们可以看到最不平衡的类别9、8、7、6和5的准确率有所提高：
- en: '| **Class** | **CrossEntropyLoss** | **ClassBalancedLoss** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **交叉熵损失** | **类平衡损失** |'
- en: '| 0 | 99.9 | 99.0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 99.9 | 99.0 |'
- en: '| 1 | 99.6 | 99.0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 99.6 | 99.0 |'
- en: '| 2 | 98.1 | 97.3 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 98.1 | 97.3 |'
- en: '| 3 | 96.8 | 94.7 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 96.8 | 94.7 |'
- en: '| 4 | 97.7 | 97.5 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 97.7 | 97.5 |'
- en: '| 5 | 94.2 | 97.4 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 94.2 | 97.4 |'
- en: '| 6 | 92.8 | 98.3 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 92.8 | 98.3 |'
- en: '| 7 | 81.2 | 94.3 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 81.2 | 94.3 |'
- en: '| 8 | 63.6 | 93.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 63.6 | 93.8 |'
- en: '| 9 | 49.1 | 91.4 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 49.1 | 91.4 |'
- en: Table 8.1 – Class-wise accuracy using cross-entropy loss (left) and class-balanced
    cross-entropy loss (right)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – 使用交叉熵损失（左）和类平衡交叉熵损失（右）的类准确率
- en: '*Figure 8**.15* compares the performance of class-balanced loss and cross-entropy
    loss:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.15* 比较了类平衡损失和交叉熵损失的性能：'
- en: '![](img/B17259_08_15.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_15.jpg)'
- en: Figure 8.15 – Overall accuracy versus class-wise accuracy using class-balanced
    loss compared to the baseline model
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 使用类平衡损失与基线模型相比的总体准确率与类准确率
- en: 🚀 Class-balanced loss in production at Apple
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 苹果公司生产中的类平衡损失
- en: The accessibility team at Apple aimed to ensure usability for all by addressing
    the lack of proper accessibility information in many apps. They made these apps
    usable for individuals with disabilities through features such as screen recognition.
    The researchers aimed to automatically generate accessibility metadata [11] for
    mobile apps based on their visual interfaces, a problem that had significant class
    imbalance due to the diverse range of UI elements. UI elements such as text, icons,
    and sliders were identified from app screenshots. The text elements were highly
    represented with 741,285 annotations, while sliders were least represented with
    1,808 annotations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果公司的可访问性团队旨在确保所有人都能使用，通过解决许多应用中缺乏适当的可访问性信息。他们通过屏幕识别等特性使这些应用对残疾人可用。研究人员旨在根据移动应用的视觉界面自动生成可访问性元数据[11]，由于UI元素的多样性，这个问题存在显著的类别不平衡。从应用截图中识别出UI元素，如文本、图标和滑块。文本元素高度表示，有741,285个注释，而滑块表示最少，有1,808个注释。
- en: The dataset consisted of 77,637 screens from 4,068 iPhone apps, with a wide
    variety of UI elements, leading to a highly imbalanced dataset, especially considering
    the hierarchical nature of the UI elements.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由来自4,068个iPhone应用的77,637个屏幕组成，包含各种UI元素，导致数据集高度不平衡，尤其是考虑到UI元素的层次性质。
- en: A class-balanced loss function and data augmentation were employed to handle
    the class imbalance effectively. This allowed the model to focus more on underrepresented
    UI classes, thereby improving the overall performance. The model was designed
    to be robust and fast, enabling on-device deployment. This ensured that the accessibility
    features could be generated in real time, enhancing the user experience for screen
    reader users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地处理类别不平衡，采用了类平衡损失函数和数据增强。这使得模型能够更多地关注代表性不足的UI类别，从而提高了整体性能。该模型被设计成健壮且快速，能够实现设备上的部署。这确保了实时生成可访问性功能，提高了屏幕阅读器用户的用户体验。
- en: Modern ConvNet classifiers tend to overfit the minority classes in imbalanced
    datasets. What if we could prevent that from happening? The **Class-Dependent
    Temperature** (**CDT**) loss function aims to do that.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现代卷积神经网络分类器倾向于在不平衡数据集中过拟合少数类。如果我们能防止这种情况发生会怎样？**类依赖温度**（**CDT**）损失函数旨在做到这一点。
- en: Class-dependent temperature Loss
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类依赖温度损失
- en: In addressing imbalanced datasets, traditional explanations suggest that a model’s
    inferior performance on minority classes, compared to majority classes, stems
    from its inclination to minimize average per-instance loss. This biases the model
    toward predicting majority classes. To counteract this, re-sampling and re-weighting
    strategies have been proposed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理不平衡数据集时，传统的解释认为，模型在少数类上的表现不如多数类，这源于其倾向于最小化平均实例损失。这使模型偏向于预测多数类。为了对抗这一点，已经提出了重采样和重新加权策略。
- en: However, Ye et al. [12] introduced the **Class-Dependent Temperature** (**CDT**)
    Loss, presenting a novel perspective. Their research indicates that ConvNets tend
    to overfit minority class examples, as evident from a larger feature deviation
    between training and test sets for minority classes compared to majority ones.
    Feature deviation occurs when a model learns the training data distribution of
    feature values excessively well, subsequently failing to generalize to new data.
    With CDT loss, the model’s decision values for training examples are divided by
    a “temperature” factor, dependent on each class’s frequency. This division makes
    the training more attuned to feature deviation and aids in effective learning
    across both prevalent and scarce categories.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Ye等人[12]引入了**类依赖温度**（**CDT**）损失，提出了一个新颖的视角。他们的研究表明，卷积神经网络倾向于过拟合少数类示例，正如少数类与多数类相比，训练集和测试集之间的特征偏差更大所证明的那样。当模型过度学习特征值的训练数据分布时，就会发生特征偏差，随后无法推广到新数据。使用CDT损失，模型的训练示例决策值被除以一个“温度”因子，该因子取决于每个类的频率。这种除法使训练更适应特征偏差，并有助于在常见和稀缺类别之间进行有效学习。
- en: '*Figure 8**.16* portrays how CDT loss modifies class weights according to class
    frequencies, using the visual analogy of a juggler on a unicycle handling items
    marked with different class names:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.16* 展示了CDT损失如何根据类频率调整类权重，使用杂技演员骑独轮车处理标有不同类名的物品的视觉类比：'
- en: '![](img/B17259_08_16.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_16.jpg)'
- en: Figure 8.16 – A unicyclist juggling items, adjusting class weights based on
    frequencies
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 独轮车手抛接物品，根据频率调整类权重
- en: 'The following class implements this loss function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下类实现了这个损失函数：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is an explanation of the `CDT` class:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对 `CDT` 类的解释：
- en: '`self.num_class_list` stores the number of examples in each class.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.num_class_list` 存储每个类中的示例数量。'
- en: '`self.cdt_weight = torch.FloatTensor([...]).to(device)` computes the class-dependent
    temperature weights for each class. For each class, the weight is computed as
    `(max(num_class_list) / num_class_list[i]) **` `gamma`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.cdt_weight = torch.FloatTensor([...]).to(device)` 计算每个类的类依赖温度权重。对于每个类，权重计算为
    `(max(num_class_list) / num_class_list[i]) **gamma`。'
- en: The larger the number of examples in a class, the smaller its value in the `self.cdt_weight`
    list. Majority class examples have lower values, while minority class examples
    have higher values.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类中示例的数量越多，其在 `self.cdt_weight` 列表中的值就越小。多数类示例具有较低的值，而少数类示例具有较高的值。
- en: '`inputs = inputs /self.cdt_weight` scales the log probabilities (as inputs)
    from the model by the class-dependent temperature weights. This increases the
    absolute values of the negative log probabilities for minority class examples,
    making them more significant in the loss calculation than those for the majority
    class. This intends to make the model focus more on the minority class examples.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs = inputs /self.cdt_weight` 通过类依赖温度权重对模型的输入（作为输入）进行缩放。这增加了少数类示例的负对数概率的绝对值，使它们在损失计算中比多数类示例更重要。这旨在使模型更多地关注少数类示例。'
- en: 'In *Figure 8**.17*, we’re plotting the overall accuracy of CDT loss and cross-entropy
    loss (left) and the accuracies of various classes (right):'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 *图8.17* 中，我们绘制了CDT损失和交叉熵损失的整体准确率（左）以及各个类的准确率（右）：
- en: '![](img/B17259_08_17.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_17.jpg)'
- en: Figure 8.17 – Performance comparison between cross-entropy loss and CDT loss
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 交叉熵损失和CDT损失的性能比较
- en: As we can see, there is an accuracy improvement for some classes, such as 9,
    7, 6, 5, and 3, but a decrease in performance for some of the other classes. It
    seems to give a lukewarm performance on the imbalanced MNIST dataset that we used,
    but it can potentially be helpful for other datasets.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，一些类别（如 9、7、6、5 和 3）的准确率有所提高，但其他一些类别的性能有所下降。它似乎在我们使用的不平衡 MNIST 数据集上给出了温和的性能，但它可能对其他数据集有潜在的帮助。
- en: What if we could dynamically adjust the weights of the classes according to
    their difficulty for the model during training? We could measure the class difficulty
    by the accuracy of its predictions for the examples’ class and then use this difficulty
    to compute the weight for that class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够在训练过程中根据模型对类别难度的估计动态调整类别的权重会怎样？我们可以通过预测示例类别的准确率来衡量类别难度，然后使用这个难度来计算该类别的权重。
- en: Class-wise difficulty-balanced loss
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别难度平衡损失
- en: The paper from Sinha et al. [13] proposed that the weight for a class, c, after
    training time, t, should be directly proportional to the difficulty of the class.
    The lower the accuracy of the class, the higher its difficulty.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Sinha 等人 [13] 的论文提出，类别 c 在训练时间 t 后的权重应直接与类别的难度成正比。类别的准确率越低，其难度越高。
- en: 'Mathematically, this can be represented as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这可以表示如下：
- en: w c, t = ( d c, t) τ
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: w c, t = ( d c, t) τ
- en: 'Here, w c, t is the weight of class c after training time t, and d c, t is
    the class difficulty, which is defined by the following equation:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，w c, t 是训练时间 t 后类别 c 的权重，d c, t 是类别难度，其定义如下：
- en: d c, t = (1 − Accuracy c, t)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: d c, t = (1 − Accuracy c, t)
- en: Here, Accuracy c, t is the accuracy of class c on the validation dataset after
    time t, and τ is a hyperparameter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Accuracy c, t 是训练时间 t 后类别 c 在验证数据集上的准确率，τ 是一个超参数。
- en: 'The point here is that we would want to dynamically increase the weight of
    the class for which the model’s accuracy is lower as training progresses. We could
    do this every epoch or every few epochs of training and feed the updated weights
    to the cross-entropy loss. Please look at the corresponding notebook titled `Class_wise_difficulty_balanced_loss.ipynb`
    in this book’s GitHub repository for the full training loop:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是，我们希望在训练过程中动态增加模型准确率较低的类别的权重。我们可以每轮或每几轮训练后这样做，并将更新的权重输入到交叉熵损失中。请参阅本书 GitHub
    仓库中标题为 `Class_wise_difficulty_balanced_loss.ipynb` 的相应笔记本，以获取完整的训练循环：
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 8**.18* illustrates the concept of difficulty-balanced loss using a
    comic with an acrobat on a trampoline. Each bounce is labeled with an accuracy
    score, highlighting how classes with lower accuracy receive increasing weight
    as the acrobat bounces higher each time:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8**.18* 使用一个飞人在蹦床上跳跃的漫画来阐述难度平衡损失的概念。每次跳跃都标有准确率分数，突出了低准确率类别如何随着飞人每次跳跃得更高而增加权重：'
- en: '![](img/B17259_08_18.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_18.jpg)'
- en: Figure 8.18 – Illustration of difficulty-balanced loss – the acrobat’s bounces
    show increasing weight for lower-accuracy classes
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 – 难度平衡损失的示意图 – 飞人跳跃显示了低准确率类别的权重增加
- en: '*Figure 8**.19* shows the performance of class-wise difficulty-balanced loss
    compared to cross-entropy loss as the baseline:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8**.19* 展示了与交叉熵损失作为基线相比，类别难度平衡损失的性能：'
- en: '![](img/B17259_08_19.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_19.jpg)'
- en: Figure 8.19 – Performance comparison of models trained using class-wise difficulty-balanced
    loss and cross-entropy loss
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.19 – 使用类别难度平衡损失和交叉熵损失训练的模型性能比较
- en: Here, we can see that the performance of several classes improves, including
    the biggest jump of 40% to 63.5% for the most imbalanced class (9).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到几个类别的性能有所提高，包括最不平衡的类别（9）从 40% 上升到 63.5% 的最大跳跃。
- en: Next, we will look at some of the other miscellaneous algorithm-based techniques
    that can still help us deal with imbalanced datasets.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨一些其他基于算法的技术，这些技术仍然可以帮助我们处理不平衡数据集。
- en: Discussing other algorithm-based techniques
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论其他基于算法的技术
- en: 'In this section, we’ll explore a diverse set of algorithm-level techniques
    that we haven’t covered so far. Intriguingly, these methods – from regularization
    techniques that mitigate overfitting to Siamese networks skilled in one-shot and
    few-shot learning, to deeper neural architectures and threshold adjustments –
    also have a beneficial side effect: they can occasionally mitigate the impact
    of class imbalance.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些我们之前尚未涉及的算法级技术。有趣的是，这些方法——从减轻过拟合的正则化技术到擅长单样本和少样本学习的Siamese网络，再到更深的神经网络架构和阈值调整——还具有有益的副作用：它们有时可以减轻类别不平衡的影响。
- en: Regularization techniques
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化技术
- en: 'The paper from S. Alshammari et al. [14] found that well-known regularization
    techniques such as L2-regularization and the MaxNorm constraint are quite helpful
    in long-tailed recognition. The paper proposes to do these only at the last layer
    of classification (sigmoid or softmax, for example). Here are their findings:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: S. Alshammari等人[14]的论文发现，如L2-正则化和MaxNorm约束等知名的正则化技术在长尾识别中非常有帮助。该论文建议只在分类（例如sigmoid或softmax）的最后一层进行这些操作。以下是他们的发现：
- en: '**L2-regularization** (also called weight decay) generally keeps the weights
    in check and helps the model generalize better by preventing the model from overfitting.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2-正则化**（也称为权重衰减）通常可以控制权重，并通过防止模型过拟合来帮助模型更好地泛化。'
- en: The `tf.keras.constraints.MaxNorm`, while PyTorch has `torch.clamp` to help
    with this.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.keras.constraints.MaxNorm`，而PyTorch有`torch.clamp`来帮助实现这一点。'
- en: Siamese networks
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Siamese网络
- en: On a similar note, previous research has found **Siamese networks** to be very
    robust to the adverse effects of class imbalance. Siamese networks have been quite
    useful in the areas of one-shot learning (classifying new data when we have only
    one example of each class in the training data) and few-shot learning (classifying
    new data when we have only a few examples of each class in the training data).
    Siamese networks use a contrastive loss function that takes in pairs of input
    images and then computes a similarity metric (Euclidean distance, Manhattan distance,
    or cosine distance) to figure out how similar or dissimilar they are. This can
    be used to compute the embeddings of each unique class of images in the training
    data. At inference time or test time, the distance of the new input image from
    each unique class can be computed to find the appropriate class of the image.
    The best part of this technique is that it provides a way to learn the feature
    representation of each class. Siamese networks have found a wide variety of practical
    applications in the industry regarding vision problems (for example, whether two
    images are of the same person or not) as well as NLP problems (for example, finding
    out whether two questions/queries are similar or not on, say, platforms such as
    Stack Overflow, Quora, Google, and so on).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的研究中，先前的研究发现**Siamese网络**对类别不平衡的负面影响非常稳健。Siamese网络在单样本学习（在训练数据中每个类只有一个示例时对新的数据进行分类）和少样本学习（在训练数据中每个类只有少数示例时对新的数据进行分类）领域非常有用。Siamese网络使用对比损失函数，该函数接受成对的输入图像，然后计算相似性度量（欧几里得距离、曼哈顿距离或余弦距离）以确定它们有多相似或不相似。这可以用来计算训练数据中每个独特图像类别的嵌入表示。这种技术的最好之处在于，它提供了一种学习每个类别特征表示的方法。Siamese网络在视觉问题（例如，两个图像是否为同一个人）以及NLP问题（例如，在Stack
    Overflow、Quora、Google等平台上，确定两个问题/查询是否相似）等领域的实际应用中找到了广泛的应用。
- en: '*Figure 8**.20* shows a Siamese network where two inputs are fed into the model
    to get their embeddings, which are then compared for similarity using a distance
    metric:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.20* 展示了一个Siamese网络，其中两个输入被输入到模型中以获取它们的嵌入表示，然后使用距离度量来比较它们的相似性：'
- en: '![](img/B17259_08_20.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_20.jpg)'
- en: Figure 8.20 – High-level working of the Siamese network model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – Siamese网络模型的高级工作原理
- en: Deeper neural networks
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: 'A study by Ding et al. 2017 [15] discovered that deeper neural networks (more
    than 10 layers) are more helpful in general with imbalanced datasets for two reasons:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Ding等人2017年的研究[15]发现，对于不平衡数据集来说，深度神经网络（超过10层）更有帮助，原因有两个：
- en: A faster rate of convergence
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛速度更快
- en: Better overall performance
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的整体性能
- en: This is attributed to the fact that deep networks are exponentially more efficient
    at capturing the complexity of data. Though their experiment was for facial action
    recognition tasks, this may be useful for trying out deeper networks on other
    kinds of data and domains.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这归因于深度网络在捕捉数据复杂性方面具有指数级的效率。尽管他们的实验是针对面部动作识别任务，但这可能有助于在其他类型的数据和领域尝试更深的网络。
- en: However, the cons of longer training times, increased hardware cost, and increased
    complexity may not always be worth the hassle in industry settings.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更长的训练时间、增加的硬件成本和增加的复杂性在工业环境中可能并不总是值得麻烦。
- en: Threshold adjustment
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阈值调整
- en: As we discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive
    Learning*, threshold adjustment is a cost-sensitive meta-learning technique. Threshold
    adjustment applies equally well to deep learning models, and it can be critical
    to make sure that the thresholds for classification are properly tuned and adjusted,
    especially when the training data distribution has been changed (for example,
    oversampled or undersampled) or even when class weights or new loss functions
    are used.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第五章*](B17259_05.xhtml#_idTextAnchor151)中讨论的，*成本敏感学习*，阈值调整是一种成本敏感的元学习技术。阈值调整同样适用于深度学习模型，并且确保分类的阈值得到适当的调整和微调至关重要，尤其是在训练数据分布发生变化（例如，过采样或欠采样）或者甚至当使用类权重或新的损失函数时。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various loss functions as remedies to class imbalance.
    We started with the class-weighting technique and deferred re-weighting, both
    designed to penalize errors on minority class samples. As we progressed, we encountered
    focal loss, where we shifted from class-centric to sample-centric weighting, focusing
    on the difficulty of samples. Despite its merits, we learned that focal loss may
    still be biased toward the majority class when assigning weights to challenging
    samples across all classes. Subsequent discussions on class-balanced loss, CDT
    loss, and class-wise difficulty-balanced loss were provided, each introducing
    unique strategies to dynamically adjust weights or modulate the model’s focus
    between easy and challenging samples, aiming to enhance performance on imbalanced
    datasets.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种损失函数作为解决类别不平衡的补救措施。我们从类别加权技术和延迟重新加权开始，这两个技术都是为了惩罚对少数类样本的错误。随着我们的进展，我们遇到了焦点损失，我们从以类别为中心的加权转向以样本为中心的加权，关注样本的难度。尽管它有其优点，但我们了解到焦点损失在分配所有类别的挑战性样本权重时，仍然可能对多数类有偏见。随后对类别平衡损失、CDT损失和类别难度平衡损失的讨论提供了，每个都引入了独特的策略来动态调整权重或调节模型在简单样本和挑战性样本之间的关注点，旨在提高不平衡数据集上的性能。
- en: To summarize, algorithm-level techniques usually modify the loss functions used
    by the model in some way to accommodate for imbalances in the dataset. They typically
    do not increase the training time and cost, unlike data-level techniques. They
    are well suited for problems or domains with large amounts of data or where gathering
    more data is hard or expensive.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，算法级技术通常以某种方式修改模型使用的损失函数，以适应数据集中的不平衡。它们通常不会增加训练时间和成本，与数据级技术不同。它们非常适合数据量大的问题或领域，或者收集更多数据困难或昂贵的情况。
- en: Even though these techniques improve the performance of minority classes, the
    majority classes may sometimes suffer as a result. In the next chapter, we will
    look at some of the hybrid techniques that can combine the data-level and algorithm-level
    techniques so that we can get the best of both worlds.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些技术提高了少数类的性能，但有时多数类可能会因此受到影响。在下一章中，我们将探讨一些混合技术，这些技术可以结合数据级和算法级技术，以便我们可以兼得两者之长。
- en: Questions
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Mean false error and mean squared false error:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平均假错误和平均平方假错误：
- en: 'Wang et al. [16]proposed that regular loss functions poorly capture the errors
    from minority classes in the case of high data imbalance due to lots of negative
    samples that dominate the loss function. Hence, they proposed a new loss function
    where the main idea was to split the training error into four different kinds
    of errors:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 王等人[16]提出，在高数据不平衡的情况下，由于大量占主导地位的负样本，常规损失函数无法很好地捕捉少数类别的错误。因此，他们提出了一种新的损失函数，其主要思想是将训练错误分为四种不同的错误：
- en: False Positive Error (FPE) = (1/number_of_negative_samples) * (error from negative
    samples)
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性错误（FPE）= (1/负样本数量) * (负样本中的错误)
- en: False Negative Error (FNE) = (1/number_of_positive_samples) * (error from positive
    samples)
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性错误（FNE）= (正样本数量/1) * (正样本错误)
- en: Mean False Error (MFE) = FPE+ FNE
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均错误（MFE）= FPE + FNE
- en: Mean Squared False Error (MSFE) = FPE2 + FNE2
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差（MSFE）= FPE² + FNE²
- en: The error here could be computed using the usual cross-entropy loss or any other
    loss used for classification. Implement the MFE and MSFE loss functions for both
    the imbalanced MNIST and CIFAR10-LT datasets, and see whether the model performance
    improves over the baseline of cross-entropy loss.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此处的错误可以使用常用的交叉熵损失或任何其他用于分类的损失来计算。为不平衡的 MNIST 和 CIFAR10-LT 数据集实现 MFE 和 MSFE 损失函数，并查看模型性能是否优于交叉熵损失的基线。
- en: In this chapter, while implementing the CDT loss function, replace the imbalanced
    MNIST dataset with CIFAR10-LT (the long-tailed version of CIFAR-10). Check whether
    you still achieve improved performance over the baseline. You may have to play
    with the gamma value or perform any of the other tricks mentioned in the original
    paper [12] to get an improvement over the baseline.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，在实现 CDT 损失函数时，将不平衡的 MNIST 数据集替换为 CIFAR10-LT（CIFAR-10 的长尾版本）。检查您是否仍然能够超过基线实现改进性能。您可能需要调整
    gamma 值或执行原始论文[12]中提到的其他技巧之一，以在基线之上获得改进。
- en: Tversky Loss was introduced in the paper by Salehi et al. [17]. Please read
    this paper to understand the Tversky loss function and its implementation details.
    Finally, implement the Tversky loss on an imbalanced MNIST dataset and compare
    its performance with a baseline model.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tversky 损失函数在 Salehi 等人发表的论文[17]中提出。请阅读这篇论文以了解 Tversky 损失函数及其实现细节。最后，在一个不平衡的
    MNIST 数据集上实现 Tversky 损失，并比较其与基线模型的表现。
- en: We used the class-weighting technique and cross-entropy loss with the `trec`
    dataset in this chapter. Replace cross-entropy loss with focal loss, and see whether
    model performance improves.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了类权重技术和交叉熵损失与 `trec` 数据集。将交叉熵损失替换为焦点损失，并查看模型性能是否有所提高。
- en: References
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*DALL·E 2 pre-training mitigations*, 2022, [https://openai.com/research/dall-e-2-pre-training-mitigations](https://openai.com/research/dall-e-2-pre-training-mitigations).'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*DALL·E 2 预训练缓解措施*，2022，[https://openai.com/research/dall-e-2-pre-training-mitigations](https://openai.com/research/dall-e-2-pre-training-mitigations).'
- en: '*BERT Does Business: Implementing the BERT Model for Natural Language Processing
    at Wayfair*, 2019, [https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair](https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair).'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*BERT Does Business：在 Wayfair 实施BERT模型进行自然语言处理*，2019，[https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair](https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair).'
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*, [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Cao, C. Wei, A. Gaidon, N. Arechiga, 和 T. Ma，*通过标签分布感知边缘损失学习不平衡数据集*，[在线]。可在[https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf)获取。
- en: T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, *Focal Loss for Dense
    Object Detection*. arXiv, Feb. 07, 2018, [http://arxiv.org/abs/1708.02002](http://arxiv.org/abs/1708.02002).
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T.-Y. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár，*密集目标检测的焦点损失*。arXiv，2018年2月7日，[http://arxiv.org/abs/1708.02002](http://arxiv.org/abs/1708.02002)。
- en: 'Wang et al., *Imbalance-XGBoost: leveraging weighted and focal losses for binary
    label-imbalanced classification with* *XGBoost*, [https://arxiv.org/pdf/1908.01672.pdf](https://arxiv.org/pdf/1908.01672.pdf).'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang 等人，*Imbalance-XGBoost：利用加权损失和焦点损失进行 XGBoost 的二进制标签不平衡分类*，[https://arxiv.org/pdf/1908.01672.pdf](https://arxiv.org/pdf/1908.01672.pdf)。
- en: '*Focal loss implementation for* *LightGBM*, [https://maxhalford.github.io/blog/lightgbm-focal-loss](https://maxhalford.github.io/blog/lightgbm-focal-loss).'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*LightGBM 的焦点损失实现*，[https://maxhalford.github.io/blog/lightgbm-focal-loss](https://maxhalford.github.io/blog/lightgbm-focal-loss)。'
- en: '*The PASCAL VOC* *project*, [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/).'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*PASCAL VOC* *项目*，[http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/).'
- en: '*fastai library*, 2018, [https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb](https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb).'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*fastai库*，2018年，[https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb](https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb)。'
- en: '*Community Standards report*, 2019, [https://ai.meta.com/blog/community-standards-report/](https://ai.meta.com/blog/community-standards-report/).'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*社区标准报告*，2019年，[https://ai.meta.com/blog/community-standards-report/](https://ai.meta.com/blog/community-standards-report/)。'
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, *Class-Balanced Loss Based
    on Effective Number of Samples*, p. 10.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, 和 S. Belongie，*基于有效样本数量的类别平衡损失*，第10页。
- en: 'X. Zhang et al., *Screen Recognition: Creating Accessibility Metadata for Mobile
    Applications from Pixels*, in Proceedings of the 2021 CHI Conference on Human
    Factors in Computing Systems, Yokohama Japan: ACM, May 2021, pp. 1–15\. doi: 10.1145/3411764.3445186\.
    Blog: [https://machinelearning.apple.com/research/mobile-applications-accessible](https://machinelearning.apple.com/research/mobile-applications-accessible).'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Zhang 等人，*屏幕识别：从像素创建移动应用程序的可访问元数据*，载于2021年CHI会议关于人机交互系统中的因素论文集，日本横滨：ACM，2021年5月，第1–15页。doi:
    10.1145/3411764.3445186。博客：[https://machinelearning.apple.com/research/mobile-applications-accessible](https://machinelearning.apple.com/research/mobile-applications-accessible)。'
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available: [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H.-J. Ye, H.-Y. Chen, D.-C. Zhan, 和 W.-L. Chao, *在不平衡深度学习中识别和补偿特征偏差*。arXiv，2022年7月10日。访问时间：2022年12月14日。[在线]。可在[http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385)获取。
- en: 'S. Sinha, H. Ohashi, and K. Nakamura, *Class-Wise Difficulty-Balanced Loss
    for Solving Class-Imbalance*. arXiv, Oct. 05, 2020\. Accessed: Dec. 17, 2022\.
    [Online]. Available at [http://arxiv.org/abs/2010.01824](http://arxiv.org/abs/2010.01824).'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Sinha, H. Ohashi, 和 K. Nakamura，*用于解决类别不平衡的类别难度平衡损失*。arXiv，2020年10月5日。访问时间：2022年12月17日。[在线]。可在[http://arxiv.org/abs/2010.01824](http://arxiv.org/abs/2010.01824)获取。
- en: 'S. Alshammari, Y.-X. Wang, D. Ramanan, and S. Kong, *Long-Tailed Recognition
    via Weight Balancing*, in 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 6887–6897\. Doi: 10.1109/CVPR52688.2022.00677.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'S. Alshammari, Y.-X. Wang, D. Ramanan, 和 S. Kong，*通过权重平衡进行长尾识别*，载于2022年IEEE/CVF计算机视觉和模式识别会议（CVPR），美国路易斯安那州新奥尔良，2022年6月，第6887–6897页。Doi:
    10.1109/CVPR52688.2022.00677。'
- en: 'W. Ding, D.-Y. Huang, Z. Chen, X. Yu, and W. Lin, *Facial action recognition
    using very deep networks for highly imbalanced class distribution*, in 2017 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), Kuala Lumpur, Dec. 2017, pp. 1368–1372\. doi: 10.1109/APSIPA.2017.8282246.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'W. Ding, D.-Y. Huang, Z. Chen, X. Yu, 和 W. Lin，*使用非常深的网络进行高度不平衡类别分布的人脸动作识别*，载于2017年亚太信号与信息处理协会年会和会议（APSIPA
    ASC），吉隆坡，2017年12月，第1368–1372页。doi: 10.1109/APSIPA.2017.8282246。'
- en: 'S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, *Training deep
    neural networks on imbalanced datasets*, in 2016 International Joint Conference
    on Neural Networks (IJCNN), Vancouver, BC, Canada, Jul. 2016, pp. 4368–4374\.
    doi: 10.1109/IJCNN.2016.7727770.'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, 和 P. J. Kennedy，*在不平衡数据集上训练深度神经网络*，载于2016年国际神经网络联合会议（IJCNN），加拿大不列颠哥伦比亚省温哥华，2016年7月，第4368–4374页。doi:
    10.1109/IJCNN.2016.7727770。'
- en: 'S. S. M. Salehi, D. Erdogmus, and A. Gholipour, *Tversky loss function for
    image segmentation using 3D fully convolutional deep networks*. arXiv, Jun. 18,
    2017\. Accessed: Dec. 23, 2022\. [Online]. Available at [http://arxiv.org/abs/1706.05721](http://arxiv.org/abs/1706.05721).'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. S. M. Salehi, D. Erdogmus, 和 A. Gholipour，*使用3D全卷积深度网络进行图像分割的Tversky损失函数*。arXiv，2017年6月18日。访问时间：2022年12月23日。[在线]。可在[http://arxiv.org/abs/1706.05721](http://arxiv.org/abs/1706.05721)获取。
