- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Algorithm-Level Deep Learning Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯
- en: The data-level deep learning techniques have problems very similar to classical
    ML techniques. Since deep learning algorithms are quite different from classical
    ML techniques, weâ€™ll explore some algorithm-level techniques for addressing data
    imbalance in this chapter. These algorithm-level techniques wonâ€™t change the data
    but accommodate the model instead. This exploration might uncover new insights
    or methods to better handle imbalanced data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯å­˜åœ¨ä¸ç»å…¸æœºå™¨å­¦ä¹ æŠ€æœ¯éå¸¸ç›¸ä¼¼çš„é—®é¢˜ã€‚ç”±äºæ·±åº¦å­¦ä¹ ç®—æ³•ä¸ç»å…¸æœºå™¨å­¦ä¹ æŠ€æœ¯æœ‰å¾ˆå¤§ä¸åŒï¼Œå› æ­¤åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€äº›ç®—æ³•çº§æŠ€æœ¯æ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚è¿™äº›ç®—æ³•çº§æŠ€æœ¯ä¸ä¼šæ”¹å˜æ•°æ®ï¼Œè€Œæ˜¯é€‚åº”æ¨¡å‹ã€‚è¿™ç§æ¢ç´¢å¯èƒ½ä¼šæ­ç¤ºæ–°çš„è§è§£æˆ–æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ä¸å¹³è¡¡æ•°æ®ã€‚
- en: This chapter will be on the same lines as [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, extending the ideas to deep learning models. We will
    look at algorithm-level deep learning techniques to handle the imbalance in data.
    Generally, these techniques do not modify the training data and often require
    no pre-processing steps, offering the benefit of no increased training times or
    additional runtime hardware costs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†ä¸[*ç¬¬äº”ç« *](B17259_05.xhtml#_idTextAnchor151)çš„*æˆæœ¬æ•æ„Ÿå­¦ä¹ *ä¿æŒä¸€è‡´ï¼Œå°†æ€æƒ³æ‰©å±•åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¢è®¨ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥å¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚é€šå¸¸ï¼Œè¿™äº›æŠ€æœ¯ä¸ä¼šä¿®æ”¹è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”é€šå¸¸ä¸éœ€è¦é¢„å¤„ç†æ­¥éª¤ï¼Œä»è€Œæä¾›äº†æ— éœ€å¢åŠ è®­ç»ƒæ—¶é—´æˆ–é¢å¤–è¿è¡Œæ—¶ç¡¬ä»¶æˆæœ¬çš„ä¼˜ç‚¹ã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Motivation for algorithm-level techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®—æ³•çº§æŠ€æœ¯çš„åŠ¨æœº
- en: Weighting techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ æƒæŠ€æœ¯
- en: Explicit loss function modification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜ç¡®æŸå¤±å‡½æ•°ä¿®æ”¹
- en: Discussing other algorithm-based techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºå…¶ä»–åŸºäºç®—æ³•çš„æŠ€æœ¯
- en: By the end of this chapter, youâ€™ll understand how to manage imbalanced data
    through model weight adjustments and loss function modifications using PyTorch
    APIs. Weâ€™ll also explore other algorithmic strategies, equipping you to make informed
    decisions in real-world applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œæ‚¨å°†äº†è§£å¦‚ä½•é€šè¿‡ä½¿ç”¨PyTorch APIå¯¹æ¨¡å‹æƒé‡è¿›è¡Œè°ƒæ•´å’Œä¿®æ”¹æŸå¤±å‡½æ•°æ¥ç®¡ç†ä¸å¹³è¡¡æ•°æ®ã€‚æˆ‘ä»¬è¿˜å°†æ¢ç´¢å…¶ä»–ç®—æ³•ç­–ç•¥ï¼Œä½¿æ‚¨èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: We will mostly be using standard functions from PyTorch and `torchvision` throughout
    this chapter. We will also use the Hugging Face Datasets library for dealing with
    text data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸»è¦ä½¿ç”¨PyTorchå’Œ`torchvision`çš„æ ‡å‡†å‡½æ•°ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨Hugging Face Datasetsåº“æ¥å¤„ç†æ–‡æœ¬æ•°æ®ã€‚
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapterâ€™s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08)ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»æœ¬ç« ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡æˆ–é€šè¿‡ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLä»[https://colab.research.google.com](https://colab.research.google.com)å¯åŠ¨å®ƒæ¥æ‰“å¼€GitHubç¬”è®°æœ¬ã€‚
- en: Motivation for algorithm-level techniques
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®—æ³•çº§æŠ€æœ¯çš„åŠ¨æœº
- en: In this chapter, we will concentrate on deep learning techniques that have gained
    popularity in both the vision and text domains. We will mostly use a long-tailed
    imbalanced version of the MNIST dataset, similar to what we used in [*Chapter
    7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep Learning Methods*. We
    will also consider CIFAR10-LT, the long-tailed version of CIFAR10, which is quite
    popular among researchers working with long-tailed datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºåœ¨è§†è§‰å’Œæ–‡æœ¬é¢†åŸŸéƒ½å—åˆ°æ¬¢è¿çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚æˆ‘ä»¬å°†ä¸»è¦ä½¿ç”¨ä¸[*ç¬¬ä¸ƒç« *](B17259_07.xhtml#_idTextAnchor205)çš„*æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•*ä¸­ä½¿ç”¨çš„ç±»ä¼¼çš„MNISTæ•°æ®é›†çš„é•¿å°¾ä¸å¹³è¡¡ç‰ˆæœ¬ã€‚æˆ‘ä»¬è¿˜å°†è€ƒè™‘CIFAR10-LTï¼Œè¿™æ˜¯CIFAR10çš„é•¿å°¾ç‰ˆæœ¬ï¼Œåœ¨å¤„ç†é•¿å°¾æ•°æ®é›†çš„ç ”ç©¶è€…ä¸­ç›¸å½“å—æ¬¢è¿ã€‚
- en: In this chapter, the ideas will be very similar to what we learned in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*, where the high-level
    idea was to increase the weight of the positive (minority) class and decrease
    the weight of the negative (majority) class in the cost function of the model.
    To facilitate this adjustment to the loss function, frameworks such as `scikit-learn`
    and XGBoost offer specific parameters. `scikit-learn` provides options such as
    `class_weight` and `sample_weight`, while XGBoost offers `scale_pos_weight` as
    a parameter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the idea remains the same, and PyTorch provides a `weight`
    parameter in the `torch.nn.CrossEntropyLoss` class to implement this weighting
    idea.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: However, we will see some advanced techniques that are more relevant and might
    give better results for the deep learning models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: With imbalanced datasets, the majority class examples contribute much more to
    the overall loss than the minority class examples. This happens because the majority
    class examples heavily outnumber the minority class examples. This means that
    the loss function being used is naturally biased toward the majority classes,
    and it fails to capture the error from minority classes. Keeping this in mind,
    can we change the loss function to account for this discrepancy for imbalanced
    datasets? Letâ€™s try to figure this out.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-entropy loss for binary classification is defined as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: CrossEntropyLoss(p) = {âˆ’ log(p) if y = 1 (minority class term)Â Â Â Â âˆ’ log(1 âˆ’
    p) otherwise (majority class term)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s say y = 1 represents the minority class and itâ€™s the class we are trying
    to predict. So, we can try to increase the minority class term by multiplying
    it with a higher value of weight to increase its attribution to the overall loss.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Weighting techniques
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Letâ€™s continue to use the imbalanced MNIST dataset from the previous chapter,
    which has long-tailed data distribution, as shown in the following bar chart (*Figure
    8**.1*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 â€“ Imbalanced MNIST dataset
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Here, the *x* axis is the class label, and the *y* axis is the count of samples
    of various classes. In the next section, we will see how to use the weight parameter
    in PyTorch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following model code for all the vision-related tasks in this
    chapter. We have defined a PyTorch neural network class called `Net` with two
    convolutional layers, a dropout layer, and two fully connected layers. The `forward`
    method applies these layers sequentially along with ReLU activations and max-pooling
    to process the input, `x`. Finally, it returns the `log_softmax` activation of
    the output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since our final layer of the model uses `log_softmax`, we will be using negative
    log-likelihood loss (`torch.nn.functional.nll_loss` or `torch.nn.NLLLoss`) from
    PyTorch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorchâ€™s weight parameter
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `torch.nn.CrossEntropyLoss` API, we have a `weight` parameter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `weight` is a one-dimensional tensor that assigns weight to each class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`weight`æ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…æƒé‡ã€‚
- en: 'We can use the `compute_class_weight` function from `scikit-learn` to get the
    weights of various classes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`scikit-learn`ä¸­çš„`compute_class_weight`å‡½æ•°æ¥è·å–å„ç§ç±»åˆ«çš„æƒé‡ï¼š
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This outputs the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `compute_class_weight` function computes the weights according to the following
    formula for each class, as we saw in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_class_weight` å‡½æ•°æ ¹æ®ä»¥ä¸‹å…¬å¼ä¸ºæ¯ä¸ªç±»åˆ«è®¡ç®—æƒé‡ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬äº”ç« *](B17259_05.xhtml#_idTextAnchor151)ä¸­çœ‹åˆ°çš„ï¼Œ*æˆæœ¬æ•æ„Ÿå­¦ä¹ *ï¼š'
- en: weight _ class _ a = Â 1Â Â _______________________Â Â Â total _ num _ samples _ for
    _ class _ aÂ  * Â total _ number _ of _ samplesÂ Â ___________________Â Â number _ of
    _ classes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: weight_class_a = 1 / (total_num_samples_for_class_a * total_number_of_samples
    / number_of_classes)
- en: 'In *Figure 8**.2*, these weights have been plotted using a bar chart to help
    us see how they relate to the class frequency (*y* axis) for each class (*x* axis):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾8.2ä¸­ï¼Œè¿™äº›æƒé‡å·²ç»ç”¨æ¡å½¢å›¾ç»˜åˆ¶å‡ºæ¥ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£å®ƒä»¬å¦‚ä½•ä¸æ¯ä¸ªç±»åˆ«çš„é¢‘ç‡ï¼ˆ*y*è½´ï¼‰å’Œç±»åˆ«ï¼ˆ*x*è½´ï¼‰ç›¸å…³ï¼š
- en: '![](img/B17259_08_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_02.jpg)'
- en: Figure 8.2 â€“ Bar chart showing weights corresponding to each class
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.2 â€“ æ¯ä¸ªç±»åˆ«å¯¹åº”çš„æƒé‡æ¡å½¢å›¾
- en: As this figure shows, the fewer the number of samples a class has, the higher
    its weight.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ­¤å›¾æ‰€ç¤ºï¼Œä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡è¶Šå°‘ï¼Œå…¶æƒé‡å°±è¶Šé«˜ã€‚
- en: Tip
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤º
- en: The key takeaway here is that the weight of a class is inversely proportional
    to the number of samples of that class, also called inverse class frequency weighting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„å…³é”®ç‚¹æ˜¯ï¼Œä¸€ä¸ªç±»åˆ«çš„æƒé‡ä¸è¯¥ç±»åˆ«çš„æ ·æœ¬æ•°é‡æˆåæ¯”ï¼Œä¹Ÿç§°ä¸ºé€†ç±»åˆ«é¢‘ç‡åŠ æƒã€‚
- en: Another point to remember is that the class weights should always be computed
    from the training data. Using validation data or test data to compute the class
    weights might lead to the infamous data leakage or label leakage problem in ML.
    Formally, data leakage can happen when some information from outside of the training
    data is fed to the model. In this case, if we use test data to compute the class
    weights, then our evaluation of the modelâ€™s performance is going to be biased
    and invalid.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç‚¹éœ€è¦è®°ä½çš„æ˜¯ï¼Œç±»åˆ«æƒé‡åº”è¯¥å§‹ç»ˆä»è®­ç»ƒæ•°æ®ä¸­è®¡ç®—ã€‚ä½¿ç”¨éªŒè¯æ•°æ®æˆ–æµ‹è¯•æ•°æ®æ¥è®¡ç®—ç±»åˆ«æƒé‡å¯èƒ½ä¼šå¯¼è‡´æœºå™¨å­¦ä¹ ä¸­çš„è‘—åçš„æ•°æ®æ³„éœ²æˆ–æ ‡ç­¾æ³„éœ²é—®é¢˜ã€‚æ­£å¼æ¥è¯´ï¼Œæ•°æ®æ³„éœ²å¯èƒ½å‘ç”Ÿåœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–çš„ä¿¡æ¯è¢«è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•æ•°æ®æ¥è®¡ç®—ç±»åˆ«æƒé‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„è¯„ä»·å°†ä¼šæ˜¯æœ‰åçš„ä¸”æ— æ•ˆçš„ã€‚
- en: 'The comic in *Figure 8**.3* shows a juggler managing weights of different sizes,
    each labeled with a distinct class label, symbolizing the varying weights assigned
    to different classes to tackle class imbalance during model training:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.3ä¸­çš„æ¼«ç”»å±•ç¤ºäº†ä¸€ä½é­”æœ¯å¸ˆç®¡ç†ä¸åŒå¤§å°çš„æƒé‡ï¼Œæ¯ä¸ªæƒé‡éƒ½æ ‡æœ‰ä¸åŒçš„ç±»åˆ«æ ‡ç­¾ï¼Œè±¡å¾ç€åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡è€Œåˆ†é…ç»™ä¸åŒç±»åˆ«çš„ä¸åŒæƒé‡ï¼š
- en: '![](img/B17259_08_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_03.jpg)'
- en: Figure 8.3 â€“ Comic illustrating the core idea behind class weighting
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.3 â€“ æè¿°ç±»åˆ«åŠ æƒæ ¸å¿ƒæ€æƒ³çš„æ¼«ç”»
- en: Tip
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤º
- en: Another way to compute weights is to empirically tune the weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æƒé‡çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ç»éªŒæ€§åœ°è°ƒæ•´æƒé‡ã€‚
- en: 'Letâ€™s write the training loop:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¼–å†™è®­ç»ƒå¾ªç¯ï¼š
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A lot of other loss functions in PyTorch, including `NLLLoss`, `MultiLabelSoftMarginLoss`,
    `MultiMarginLoss`, and `BCELoss`, accept `weight` as a parameter as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchä¸­çš„è®¸å¤šå…¶ä»–æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬`NLLLoss`ã€`MultiLabelSoftMarginLoss`ã€`MultiMarginLoss`å’Œ`BCELoss`ï¼Œä¹Ÿæ¥å—`weight`ä½œä¸ºå‚æ•°ã€‚
- en: '*Figure 8**.4* compares the accuracy of various classes when using class weights
    versus when not using class weights:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾8.4*ã€‚4æ¯”è¾ƒäº†ä½¿ç”¨ç±»åˆ«æƒé‡å’Œä¸ä½¿ç”¨ç±»åˆ«æƒé‡æ—¶å„ç§ç±»åˆ«çš„å‡†ç¡®ç‡ï¼š'
- en: '![](img/B17259_08_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_04.jpg)'
- en: Figure 8.4 â€“ Performance comparison of a model trained using cross-entropy loss
    with no class weights and with class weights
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.4 â€“ ä½¿ç”¨æ— ç±»åˆ«æƒé‡å’Œæœ‰ç±»åˆ«æƒé‡è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- en: As we can see, although the accuracy dropped for classes 0-4, it improved dramatically
    for the most imbalanced classes of 5-9\. The overall accuracy of the model went
    up as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå°½ç®¡0-4ç±»åˆ«çš„å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™ï¼Œä½†5-9ç±»è¿™ç§æœ€ä¸å¹³è¡¡çš„ç±»åˆ«çš„å‡†ç¡®ç‡æœ‰äº†æ˜¾è‘—æé«˜ã€‚æ¨¡å‹çš„æ€»ä½“å‡†ç¡®ç‡ä¹Ÿæœ‰æ‰€ä¸Šå‡ã€‚
- en: Warning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: 'Please note that some loss functions, such as `BCEWithLogitsLoss`, provide
    two weighting parameters (`BCEWithLogitsLoss` can be used for binary classification
    or multi-label classification):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸€äº›æŸå¤±å‡½æ•°ï¼Œå¦‚`BCEWithLogitsLoss`ï¼Œæä¾›äº†ä¸¤ä¸ªåŠ æƒå‚æ•°ï¼ˆ`BCEWithLogitsLoss`å¯ç”¨äºäºŒåˆ†ç±»æˆ–å¤šæ ‡ç­¾åˆ†ç±»ï¼‰ï¼š
- en: â€¢ The `weight` parameter is the manual rescaling weight parameter for each example
    of the batch. This is more like the `sample_weight` parameter of the `sklearn`
    library.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ `weight`å‚æ•°æ˜¯æ¯ä¸ªæ‰¹æ¬¡çš„ç¤ºä¾‹çš„æ‰‹åŠ¨ç¼©æ”¾æƒé‡å‚æ•°ã€‚è¿™æ›´åƒæ˜¯`sklearn`åº“ä¸­çš„`sample_weight`å‚æ•°ã€‚
- en: â€¢ The `pos_weight` parameter is used to specify a weight for the positive class.
    It is similar to the `class_weight` parameter in the `sklearn` library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ `pos_weight`å‚æ•°ç”¨äºæŒ‡å®šæ­£ç±»çš„æƒé‡ã€‚å®ƒä¸`sklearn`åº“ä¸­çš„`class_weight`å‚æ•°ç±»ä¼¼ã€‚
- en: ğŸš€ Class reweighting in production at OpenAI
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ OpenAIåœ¨ç”Ÿäº§ä¸­å®ç°ç±»é‡æ–°åŠ æƒ
- en: OpenAI was trying to solve the problem of bias in training data using the image
    generation model DALL-E 2 [1]. DALL-E 2 is trained on a massive dataset of images
    from the internet, which can contain biases. For example, the dataset may contain
    more images of men than women or more images of people from certain racial or
    ethnic groups than others.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIè¯•å›¾é€šè¿‡å›¾åƒç”Ÿæˆæ¨¡å‹DALL-E 2 [1]æ¥è§£å†³è®­ç»ƒæ•°æ®ä¸­çš„åå·®é—®é¢˜ã€‚DALL-E 2åœ¨æ¥è‡ªäº’è”ç½‘çš„å¤§é‡å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½åŒ…å«åå·®ã€‚ä¾‹å¦‚ï¼Œæ•°æ®é›†ä¸­å¯èƒ½åŒ…å«æ¯”å¥³æ€§æ›´å¤šçš„ç”·æ€§å›¾åƒï¼Œæˆ–è€…æ¯”å…¶ä»–ç§æ—æˆ–æ°‘æ—ç¾¤ä½“æ›´å¤šçš„å›¾åƒã€‚
- en: 'To limit undesirable model capabilities (such as generating violent images),
    they first filtered out such images from the training dataset. However, filtering
    training data can amplify biases. Why? In their blog [1], they explain using an
    example that when generating images for the prompt â€œa CEO,â€ their filtered model
    showed a stronger male bias than the unfiltered one. They suspected this amplification
    arose from two sources: dataset bias toward sexualizing women and potential classifier
    bias, despite their efforts to mitigate them. This may have resulted in the filter
    removing more images of women, skewing the training data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é™åˆ¶ä¸å¸Œæœ›æœ‰çš„æ¨¡å‹èƒ½åŠ›ï¼ˆä¾‹å¦‚ç”Ÿæˆæš´åŠ›å›¾åƒï¼‰ï¼Œä»–ä»¬é¦–å…ˆä»è®­ç»ƒæ•°æ®é›†ä¸­è¿‡æ»¤æ‰äº†è¿™æ ·çš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿‡æ»¤è®­ç»ƒæ•°æ®å¯èƒ½ä¼šæ”¾å¤§åå·®ã€‚ä¸ºä»€ä¹ˆï¼Ÿåœ¨ä»–ä»¬çš„åšå®¢[1]ä¸­ï¼Œä»–ä»¬ç”¨ä¸€ä¸ªä¾‹å­è§£é‡Šè¯´ï¼Œå½“ä¸ºæç¤ºâ€œä¸€ä¸ªCEOâ€ç”Ÿæˆå›¾åƒæ—¶ï¼Œä»–ä»¬çš„è¿‡æ»¤æ¨¡å‹æ¯”æœªè¿‡æ»¤çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ›´å¼ºçš„ç”·æ€§åå·®ã€‚ä»–ä»¬æ€€ç–‘è¿™ç§æ”¾å¤§å¯èƒ½æºäºä¸¤ä¸ªæ¥æºï¼šæ•°æ®é›†å¯¹å¥³æ€§æ€§åŒ–çš„åå·®ä»¥åŠæ½œåœ¨çš„åˆ†ç±»å™¨åå·®ï¼Œå°½ç®¡ä»–ä»¬åŠªåŠ›å‡è½»è¿™äº›åå·®ã€‚è¿™å¯èƒ½å¯¼è‡´è¿‡æ»¤å™¨ç§»é™¤æ›´å¤šå¥³æ€§çš„å›¾åƒï¼Œä»è€Œæ‰­æ›²è®­ç»ƒæ•°æ®ã€‚
- en: To fix this bias, OpenAI applied reweighting to the DALL-E 2 training data by
    training a classifier to predict whether an image was from the unfiltered dataset.
    The weights for each image were then computed based on the classifierâ€™s prediction.
    This scheme was shown to reduce the frequency change induced by filtering, which
    means that it was effective at counteracting the biases in the training data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜åå·®ï¼ŒOpenAIé€šè¿‡å¯¹DALL-E 2è®­ç»ƒæ•°æ®è¿›è¡Œé‡æ–°åŠ æƒæ¥åº”ç”¨ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨æ¥é¢„æµ‹å›¾åƒæ˜¯å¦æ¥è‡ªæœªè¿‡æ»¤çš„æ•°æ®é›†ã€‚ç„¶åæ ¹æ®åˆ†ç±»å™¨çš„é¢„æµ‹è®¡ç®—æ¯ä¸ªå›¾åƒçš„æƒé‡ã€‚è¿™ç§æ–¹æ¡ˆå·²è¢«è¯æ˜å¯ä»¥å‡å°‘ç”±è¿‡æ»¤å¼•èµ·çš„é¢‘ç‡å˜åŒ–ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨å¯¹æŠ—è®­ç»ƒæ•°æ®ä¸­çš„åå·®æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚
- en: Next, to show its extensive applicability, we will apply the class weighting
    technique to textual data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä¸ºäº†å±•ç¤ºå…¶å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œæˆ‘ä»¬å°†å¯¹æ–‡æœ¬æ•°æ®åº”ç”¨ç±»åŠ æƒæŠ€æœ¯ã€‚
- en: Handling textual data
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤„ç†æ–‡æœ¬æ•°æ®
- en: 'Letâ€™s work with some text data. We will use the `datasets` and `transformers`
    libraries from Hugging Face. Letâ€™s import the `trec` dataset (the **Text Retrieval
    Conference** (**TREC**), a question classification dataset containing 5,500 labeled
    questions in the training set and 500 in the test set):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¤„ç†ä¸€äº›æ–‡æœ¬æ•°æ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Hugging Faceçš„`datasets`å’Œ`transformers`åº“ã€‚è®©æˆ‘ä»¬å¯¼å…¥`trec`æ•°æ®é›†ï¼ˆ**æ–‡æœ¬æ£€ç´¢ä¼šè®®**ï¼ˆ**TREC**ï¼‰ï¼Œä¸€ä¸ªåŒ…å«è®­ç»ƒé›†5,500ä¸ªæ ‡è®°é—®é¢˜å’Œæµ‹è¯•é›†500ä¸ªé—®é¢˜çš„é—®ç­”åˆ†ç±»æ•°æ®é›†ï¼‰ï¼š
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This dataset is balanced, so we randomly remove examples from classes ABBR
    and DESC, making those classes the most imbalanced. Here is how the distribution
    of various classes looks like in this dataset, confirming the imbalance in data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•°æ®é›†æ˜¯å¹³è¡¡çš„ï¼Œå› æ­¤æˆ‘ä»¬éšæœºä»ABBRAå’ŒDESCç±»ä¸­ç§»é™¤ç¤ºä¾‹ï¼Œä½¿è¿™äº›ç±»æˆä¸ºæœ€ä¸å¹³è¡¡çš„ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸ªæ•°æ®é›†ä¸­å„ç§ç±»çš„åˆ†å¸ƒæƒ…å†µï¼Œè¯å®äº†æ•°æ®çš„ä¸å¹³è¡¡æ€§ï¼š
- en: '![](img/B17259_08_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_05.jpg)'
- en: Figure 8.5 â€“ Frequency of various classes in the trec dataset from the Hugging
    Face Datasets library
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.5 â€“ Hugging Face Datasetsåº“ä¸­trecæ•°æ®é›†ä¸­å„ç§ç±»çš„é¢‘ç‡
- en: 'Letâ€™s create a tokenizer (that splits text into words or sub-words) for the
    pre-trained DistilBERT language model vocabulary with a maximum input token length
    of 512:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºé¢„è®­ç»ƒçš„DistilBERTè¯­è¨€æ¨¡å‹è¯æ±‡åˆ›å»ºä¸€ä¸ªæœ€å¤§è¾“å…¥æ ‡è®°é•¿åº¦ä¸º512çš„åˆ†è¯å™¨ï¼ˆå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯æˆ–å­è¯ï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will create tokenized train and test sets from the dataset we just
    imported by invoking the tokenizer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡è°ƒç”¨åˆ†è¯å™¨ä»æˆ‘ä»¬åˆšåˆšå¯¼å…¥çš„æ•°æ®é›†ä¸­åˆ›å»ºæ ‡è®°åŒ–çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, letâ€™s instantiate the model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å®ä¾‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, letâ€™s define and invoke a function to get training arguments:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®šä¹‰å’Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°æ¥è·å–è®­ç»ƒå‚æ•°ï¼š
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following `custom_compute_metrics()` function returns a dictionary containing
    the precision, recall, and F1 score:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹`custom_compute_metrics()`å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°çš„å­—å…¸ï¼š
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, letâ€™s implement the class containing the loss function that uses class
    weights:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®ç°åŒ…å«ä½¿ç”¨ç±»æƒé‡çš„æŸå¤±å‡½æ•°çš„ç±»ï¼š
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can initialize the weights similar to how we did previously using the `compute_class_weight`
    function in `sklearn`, and then feed it to the `CrossEntropyLoss` function in
    our `CustomTrainerWeighted` class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·ä½¿ç”¨`sklearn`ä¸­çš„`compute_class_weight`å‡½æ•°åˆå§‹åŒ–æƒé‡ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°æˆ‘ä»¬çš„`CustomTrainerWeighted`ç±»ä¸­çš„`CrossEntropyLoss`å‡½æ•°ï¼š
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As shown in *Figure 8**.6*, we can see improvements in performance for the
    most imbalanced classes. However, a slight reduction was observed for the majority
    class (trade-off!):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚**å›¾8**.6æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨å¤„ç†æœ€ä¸å¹³è¡¡çš„ç±»åˆ«æ—¶æ€§èƒ½æœ‰æ‰€æå‡ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šæ•°ç±»åˆ«ï¼ˆæƒè¡¡ï¼ï¼‰è§‚å¯Ÿåˆ°è½»å¾®çš„ä¸‹é™ï¼š
- en: '![](img/B17259_08_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_06.jpg)'
- en: Figure 8.6 â€“ Confusion matrix using no class weighting (left) and with class
    weights (right)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.6 â€“ ä½¿ç”¨æ— ç±»åˆ«åŠ æƒï¼ˆå·¦ï¼‰å’Œæœ‰ç±»åˆ«åŠ æƒï¼ˆå³ï¼‰çš„æ··æ·†çŸ©é˜µ
- en: As we can see, the minority classes, **ABBR** and **DESC**, have improved performance
    after class weighting at the cost of reduced performance for the **ENTY** class.
    Also, looking at some of the off-diagonal entries, we can see that the confusion
    between the **ABBR** and **DESC** classes (0.33 in *Figure 8**.6* (left)) and
    between the **DESC** and **ENTY** classes (0.08 in *Figure 8**.6* (left)) significantly
    dropped when using class weights (0.22 and 0.04, respectively).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå°‘æ•°ç±»åˆ«**ABBR**å’Œ**DESC**åœ¨ç±»åˆ«åŠ æƒåæ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†**ENTY**ç±»åˆ«çš„æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿä¸€äº›éå¯¹è§’çº¿é¡¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°**ABBR**å’Œ**DESC**ç±»åˆ«ä¹‹é—´çš„æ··æ·†ï¼ˆ*å›¾8**.6*ï¼ˆå·¦ï¼‰ä¸­çš„0.33ï¼‰ä»¥åŠ**DESC**å’Œ**ENTY**ç±»åˆ«ä¹‹é—´çš„æ··æ·†ï¼ˆ*å›¾8**.6*ï¼ˆå·¦ï¼‰ä¸­çš„0.08ï¼‰åœ¨ä½¿ç”¨ç±»åˆ«åŠ æƒæ—¶æ˜¾è‘—ä¸‹é™ï¼ˆåˆ†åˆ«ä¸º0.22å’Œ0.04ï¼‰ã€‚
- en: Some variants that deal with NLP tasks in particular suggest weighting the samples
    as the inverse of the square root of class frequency for their corresponding class
    instead of using the previously used inverse class frequency weighting technique.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç‰¹å®šäºNLPä»»åŠ¡çš„å˜ä½“å»ºè®®å°†æ ·æœ¬çš„æƒé‡è®¾ç½®ä¸ºå¯¹åº”ç±»åˆ«çš„å¹³æ–¹æ ¹å€’æ•°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¹‹å‰ä½¿ç”¨çš„é€†ç±»åˆ«é¢‘ç‡åŠ æƒæŠ€æœ¯ã€‚
- en: In essence, class weighting can usually help with any kind of deep learning
    model, including textual data, when working with imbalanced data. Since data augmentation
    techniques are not as straightforward for text as they are for images, class weighting
    can be a useful technique for NLP problems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ¬è´¨ä¸Šè®²ï¼Œç±»åˆ«åŠ æƒé€šå¸¸å¯ä»¥å¸®åŠ©ä»»ä½•ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ•°æ®ï¼Œåœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®æ—¶ã€‚ç”±äºæ•°æ®å¢å¼ºæŠ€æœ¯åœ¨æ–‡æœ¬ä¸Šçš„åº”ç”¨ä¸å¦‚å›¾åƒç›´æ¥ï¼Œå› æ­¤ç±»åˆ«åŠ æƒå¯¹äºNLPé—®é¢˜æ¥è¯´å¯ä»¥æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æŠ€æœ¯ã€‚
- en: ğŸš€ Class reweighting in production at Wayfair
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Wayfairç”Ÿäº§ä¸­çš„ç±»åˆ«é‡åŠ æƒ
- en: Wayfair used the BERT language model to improve the accuracy of its product
    search and recommendation system [2]. This was a challenging problem because the
    number of products that Wayfair sells is very large, and the number of products
    that a customer is likely to be interested in is much smaller.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Wayfairä½¿ç”¨BERTè¯­è¨€æ¨¡å‹æ¥æé«˜å…¶äº§å“æœç´¢å’Œæ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§[2]ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºWayfairé”€å”®çš„äº§å“æ•°é‡éå¸¸å¤§ï¼Œè€Œå®¢æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„äº§å“æ•°é‡åˆ™ç›¸å¯¹è¾ƒå°ã€‚
- en: There was an imbalance in data because the number of products that a customer
    had interacted with (for example, viewed, added to cart, or purchased) was much
    smaller than the number of products that the customer hadnâ€™t interacted with.
    This made it difficult for BERT to learn to accurately predict which products
    a customer was likely to be interested in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å­˜åœ¨ä¸å¹³è¡¡ï¼Œå› ä¸ºå®¢æˆ·äº’åŠ¨è¿‡çš„äº§å“æ•°é‡ï¼ˆä¾‹å¦‚ï¼ŒæŸ¥çœ‹ã€åŠ å…¥è´­ç‰©è½¦æˆ–è´­ä¹°ï¼‰è¿œå°äºå®¢æˆ·æœªäº’åŠ¨è¿‡çš„äº§å“æ•°é‡ã€‚è¿™ä½¿å¾—BERTéš¾ä»¥å­¦ä¹ å‡†ç¡®é¢„æµ‹å®¢æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„äº§å“ã€‚
- en: Wayfair used class weighting to address the data imbalance problem. They assigned
    a higher weight to positive examples (that is, products that a customer had interacted
    with) than to negative examples (that is, products that a customer had not interacted
    with). This helped ensure that BERT learned to accurately classify both positive
    and negative examples, even when the data was imbalanced.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Wayfairä½¿ç”¨ç±»åˆ«åŠ æƒæ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚ä»–ä»¬ç»™æ­£ä¾‹ï¼ˆå³å®¢æˆ·äº’åŠ¨è¿‡çš„äº§å“ï¼‰åˆ†é…æ¯”è´Ÿä¾‹ï¼ˆå³å®¢æˆ·æœªäº’åŠ¨è¿‡çš„äº§å“ï¼‰æ›´é«˜çš„æƒé‡ã€‚è¿™æœ‰åŠ©äºç¡®ä¿BERTèƒ½å¤Ÿå‡†ç¡®åˆ†ç±»æ­£ä¾‹å’Œè´Ÿä¾‹ï¼Œå³ä½¿æ•°æ®ä¸å¹³è¡¡ã€‚
- en: The model was deployed to production. Wayfair is using the model to improve
    the accuracy of its product search and recommendation system and to provide a
    better experience for customers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å·²éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚Wayfairæ­£åœ¨ä½¿ç”¨è¯¥æ¨¡å‹æ¥æé«˜å…¶äº§å“æœç´¢å’Œæ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºé¡¾å®¢æä¾›æ›´å¥½çš„ä½“éªŒã€‚
- en: In the next section, we will discuss a minor variant of class weighting that
    can sometimes be more helpful than just the weighting technique.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ä¸ªç±»åŠ æƒçš„å°å˜ä½“ï¼Œæœ‰æ—¶å®ƒæ¯”å•çº¯çš„åŠ æƒæŠ€æœ¯æ›´æœ‰å¸®åŠ©ã€‚
- en: Deferred re-weighting â€“ a minor variant of the class weighting technique
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿé‡æ–°åŠ æƒ â€“ ç±»æƒé‡æŠ€æœ¯çš„ä¸€ä¸ªå°å˜ä½“
- en: 'There is a deferred re-weighting technique (mentioned by Cao et al. [3]) similar
    to the two-phase sampling approach we discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*. Here, we defer the re-weighting to later,
    wherein in the first phase of training, we train the model on the full imbalanced
    dataset without any weighting or sampling. In the second phase, we re-train the
    same model from the first phase with class weights (that are inversely proportional
    to the class frequencies) that have been applied to the loss function and, optionally,
    use a smaller learning rate. The first phase of training serves as a good form
    of initialization for the model for the second phase of training with reweighted
    losses. Since we use a smaller learning rate in the second phase of training,
    the weights of the model do not move very far from what they were in the first
    phase of training:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ç§å»¶è¿Ÿé‡æ–°åŠ æƒæŠ€æœ¯ï¼ˆç”±Caoç­‰äºº[3]æåˆ°ï¼‰ï¼Œç±»ä¼¼äºæˆ‘ä»¬åœ¨[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)â€œæ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•â€ä¸­è®¨è®ºçš„ä¸¤é˜¶æ®µé‡‡æ ·æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é‡æ–°åŠ æƒæ¨è¿Ÿåˆ°åé¢ï¼Œå³åœ¨è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•åŠ æƒæˆ–é‡‡æ ·çš„æƒ…å†µä¸‹åœ¨å®Œæ•´çš„ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å·²ç»åº”ç”¨äºæŸå¤±å‡½æ•°çš„ç±»æƒé‡ï¼ˆä¸ç±»é¢‘ç‡æˆåæ¯”ï¼‰é‡æ–°è®­ç»ƒç¬¬ä¸€é˜¶æ®µç›¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ã€‚è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µä¸ºç¬¬äºŒé˜¶æ®µä½¿ç”¨é‡æ–°åŠ æƒæŸå¤±çš„è®­ç»ƒæä¾›äº†è‰¯å¥½çš„æ¨¡å‹åˆå§‹åŒ–å½¢å¼ã€‚ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒçš„ç¬¬äºŒé˜¶æ®µä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œå› æ­¤æ¨¡å‹çš„æƒé‡ä¸ä¼šä»ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„æƒé‡ç§»åŠ¨å¾—å¤ªè¿œï¼š
- en: '![](img/B17259_08_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_07.jpg)'
- en: Figure 8.7 â€“ The deferred re-weighting technique
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.7 â€“ å»¶è¿Ÿé‡æ–°åŠ æƒæŠ€æœ¯
- en: 'The comic in *Figure 8**.8* shows a magician who pulls out a large rabbit from
    a hat, followed by a smaller one, illustrating the two-phase process of initially
    training on the imbalanced dataset and subsequently applying re-weighting for
    more balanced training in the second phase:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾8*.8ä¸­çš„æ¼«ç”»å±•ç¤ºäº†ä¸€ä½é­”æœ¯å¸ˆä»ä¸€ä¸ªå¸½å­é‡Œå˜å‡ºä¸€åªå¤§å…”å­ï¼Œç„¶ååˆå˜å‡ºä¸€åªå°å…”å­ï¼Œè¿™æç»˜äº†ä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼šæœ€åˆåœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œç„¶ååœ¨ç¬¬äºŒé˜¶æ®µåº”ç”¨é‡æ–°åŠ æƒä»¥å®ç°æ›´å¹³è¡¡çš„è®­ç»ƒï¼š'
- en: '![](img/B17259_08_08.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_08.jpg)'
- en: Figure 8.8 â€“ A comic illustrating the core idea of deferred re-weighting
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.8 â€“ ä¸€å¹…æç»˜å»¶è¿Ÿé‡æ–°åŠ æƒæ ¸å¿ƒæ€æƒ³çš„æ¼«ç”»
- en: 'Please refer to the notebook titled `Deferred_reweighting_DRW.ipynb` in this
    bookâ€™s GitHub repository for more details. After applying the two-phase training
    part of the deferred re-weighting technique, we can see that the accuracy of our
    most imbalanced classes improves compared to training with cross-entropy loss:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜…æœ¬ä¹¦GitHubä»“åº“ä¸­æ ‡é¢˜ä¸º`Deferred_reweighting_DRW.ipynb`çš„ç¬”è®°æœ¬ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚åœ¨åº”ç”¨å»¶è¿Ÿé‡æ–°åŠ æƒæŠ€æœ¯çš„ä¸¤é˜¶æ®µè®­ç»ƒéƒ¨åˆ†ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±è®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬æœ€ä¸å¹³è¡¡çš„ç±»çš„å‡†ç¡®ç‡æœ‰æ‰€æé«˜ï¼š
- en: '![](img/B17259_08_09.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_09.jpg)'
- en: Figure 8.9 â€“ Performance comparison of deferred re-weighting with cross-entropy
    loss
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.9 â€“ å»¶è¿Ÿé‡æ–°åŠ æƒä¸äº¤å‰ç†µæŸå¤±çš„æ€§èƒ½æ¯”è¾ƒ
- en: Next, we will look at defining custom loss functions when the PyTorch standard
    loss functions donâ€™t do everything that we want them to do.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨åœ¨PyTorchæ ‡å‡†æŸå¤±å‡½æ•°æ— æ³•å®Œæˆæˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰äº‹æƒ…æ—¶å¦‚ä½•å®šä¹‰è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚
- en: Explicit loss function modification
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ˜¾å¼æŸå¤±å‡½æ•°ä¿®æ”¹
- en: In PyTorch, we can formulate custom loss functions by deriving a subclass from
    the `nn.Module` class and overriding the `forward()` method. The `forward()` method
    for a loss function accepts the predicted and actual outputs as inputs, subsequently
    returning the computed loss value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»`nn.Module`ç±»æ´¾ç”Ÿä¸€ä¸ªå­ç±»å¹¶é‡å†™`forward()`æ–¹æ³•æ¥å®šä¹‰è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚æŸå¤±å‡½æ•°çš„`forward()`æ–¹æ³•æ¥å—é¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œéšåè¿”å›è®¡ç®—å‡ºçš„æŸå¤±å€¼ã€‚
- en: Even though class weighting does assign different weights to balance the majority
    and minority class examples, this alone is often insufficient, especially in cases
    of extreme class imbalance. What we would like is to reduce the loss from easily
    classified examples as well. The reason is that such easily classified examples
    usually belong to the majority class, and since they are higher in number, they
    dominate our training loss. This is the main idea of focal loss and allows for
    a more nuanced handling of examples, irrespective of the class they belong to.
    Weâ€™ll look at this in this section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç±»æƒé‡ç¡®å®ä¸ºå¤šæ•°ç±»å’Œå°‘æ•°ç±»ç¤ºä¾‹åˆ†é…äº†ä¸åŒçš„æƒé‡ä»¥å®ç°å¹³è¡¡ï¼Œä½†è¿™é€šå¸¸æ˜¯ä¸å¤Ÿçš„ï¼Œå°¤å…¶æ˜¯åœ¨æç«¯ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬å¸Œæœ›çš„æ˜¯å‡å°‘æ˜“äºåˆ†ç±»ç¤ºä¾‹çš„æŸå¤±ã€‚åŸå› æ˜¯è¿™æ ·çš„æ˜“äºåˆ†ç±»ç¤ºä¾‹é€šå¸¸å±äºå¤šæ•°ç±»ï¼Œç”±äºæ•°é‡è¾ƒå¤šï¼Œå®ƒä»¬ä¸»å¯¼äº†æˆ‘ä»¬çš„è®­ç»ƒæŸå¤±ã€‚è¿™å°±æ˜¯ç„¦ç‚¹æŸå¤±çš„ä¸»è¦æ€æƒ³ï¼Œå®ƒå…è®¸å¯¹ç¤ºä¾‹è¿›è¡Œæ›´ç»†è‡´çš„å¤„ç†ï¼Œæ— è®ºå®ƒä»¬å±äºå“ªä¸ªç±»åˆ«ã€‚æˆ‘ä»¬å°†åœ¨æœ¬èŠ‚ä¸­æ¢è®¨è¿™ä¸€ç‚¹ã€‚
- en: Understanding the forward() method in PyTorch
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£PyTorchä¸­çš„forward()æ–¹æ³•
- en: 'In PyTorch, youâ€™ll encounter the `forward()` method in both neural network
    layers and loss functions. Thatâ€™s because both a neural network layer and a loss
    function are derived from `nn.Module`. While it might seem confusing at first,
    understanding the context can help clarify its role:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ PyTorch ä¸­ï¼Œä½ å°†åœ¨ç¥ç»ç½‘ç»œå±‚å’ŒæŸå¤±å‡½æ•°ä¸­é‡åˆ° `forward()` æ–¹æ³•ã€‚è¿™æ˜¯å› ä¸ºç¥ç»ç½‘ç»œå±‚å’ŒæŸå¤±å‡½æ•°éƒ½æ˜¯ä» `nn.Module` æ´¾ç”Ÿå‡ºæ¥çš„ã€‚è™½ç„¶ä¸€å¼€å§‹å¯èƒ½çœ‹èµ·æ¥æœ‰äº›æ··ä¹±ï¼Œä½†ç†è§£ä¸Šä¸‹æ–‡å¯ä»¥å¸®åŠ©é˜æ˜å…¶ä½œç”¨ï¼š
- en: '**ğŸŸ ** **In neural** **network layers**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸŸ ** **åœ¨ç¥ç»ç½‘ç»œå±‚**ï¼š'
- en: The `forward()` method defines the transformation that input data undergoes
    as it passes through the layer. This could involve operations such as linear transformations,
    activation functions, and more.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` æ–¹æ³•å®šä¹‰äº†è¾“å…¥æ•°æ®åœ¨é€šè¿‡å±‚æ—¶ç»å†çš„è½¬æ¢ã€‚è¿™å¯èƒ½æ¶‰åŠçº¿æ€§å˜æ¢ã€æ¿€æ´»å‡½æ•°ç­‰æ“ä½œã€‚'
- en: '**ğŸŸ¢** **In** **loss functions**:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸŸ¢** **åœ¨æŸå¤±å‡½æ•°ä¸­**ï¼š'
- en: The `forward()` method computes the loss between the predicted output and the
    actual target values. This loss serves as a measure of how well the model is performing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` æ–¹æ³•è®¡ç®—é¢„æµ‹è¾“å‡ºå’Œå®é™…ç›®æ ‡å€¼ä¹‹é—´çš„æŸå¤±ã€‚è¿™ä¸ªæŸå¤±ä½œä¸ºè¡¡é‡æ¨¡å‹æ€§èƒ½å¥½åçš„æŒ‡æ ‡ã€‚'
- en: '**ğŸ”‘****Key takeaway**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ”‘****å…³é”®è¦ç‚¹**ï¼š'
- en: In PyTorch, both neural network layers and loss functions inherit from `nn.Module`,
    providing a unified interface. The `forward()` method is central to both, serving
    as the computational engine for data transformation in layers and loss computation
    in loss functions. Think of `forward()` as the â€œengineâ€ for either process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ PyTorch ä¸­ï¼Œç¥ç»ç½‘ç»œå±‚å’ŒæŸå¤±å‡½æ•°éƒ½ç»§æ‰¿è‡ª `nn.Module`ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ã€‚`forward()` æ–¹æ³•å¯¹ä¸¤è€…éƒ½æ˜¯æ ¸å¿ƒçš„ï¼Œä½œä¸ºå±‚ä¸­æ•°æ®è½¬æ¢å’ŒæŸå¤±å‡½æ•°ä¸­æŸå¤±è®¡ç®—çš„è®¡ç®—å¼•æ“ã€‚å°†
    `forward()` è§†ä¸ºè¿™ä¸¤ä¸ªè¿‡ç¨‹çš„â€œå¼•æ“â€ã€‚
- en: Focal loss
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹æŸå¤±
- en: The techniques weâ€™ve studied so far presume that minority classes need higher
    weights due to weak representation. However, some minority classes may be adequately
    represented, and over-weighting their samples could degrade the overall model
    performance. Hence, Tsung-Yi et al. [4] from Facebook (now Meta) introduced **focal
    loss**, a sample-based weighting technique where each exampleâ€™s weight is determined
    by its difficulty and measured by the loss the model incurs on it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ç ”ç©¶çš„æŠ€æœ¯å‡å®šç”±äºä»£è¡¨æ€§è¾ƒå¼±ï¼Œå°‘æ•°ç±»éœ€è¦æ›´é«˜çš„æƒé‡ã€‚ç„¶è€Œï¼Œä¸€äº›å°‘æ•°ç±»å¯èƒ½å¾—åˆ°äº†å……åˆ†çš„ä»£è¡¨ï¼Œè¿‡åº¦åŠ æƒå®ƒä»¬çš„æ ·æœ¬å¯èƒ½ä¼šé™ä½æ•´ä½“æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼ŒFacebookï¼ˆç°Metaï¼‰çš„
    Tsung-Yi ç­‰äºº [4] å¼•å…¥äº†**ç„¦ç‚¹æŸå¤±**ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ ·æœ¬çš„åŠ æƒæŠ€æœ¯ï¼Œå…¶ä¸­æ¯ä¸ªç¤ºä¾‹çš„æƒé‡ç”±å…¶éš¾åº¦å†³å®šï¼Œå¹¶é€šè¿‡æ¨¡å‹å¯¹å…¶é€ æˆçš„æŸå¤±æ¥è¡¡é‡ã€‚
- en: 'The focal loss technique has roots in dense object detection tasks, where there
    are significantly more observations in one class than the other:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹æŸå¤±æŠ€æœ¯æºäºå¯†é›†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼ŒæŸä¸€ç±»åˆ«çš„è§‚å¯Ÿç»“æœæ¯”å¦ä¸€ç±»å¤šå¾—å¤šï¼š
- en: '![](img/B17259_08_10.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_08_10.jpg)'
- en: Figure 8.10 â€“ Class imbalance in object detection â€“ majority as background,
    few as foreground
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.10 â€“ ç›®æ ‡æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡ â€“ å¤šæ•°ç±»ä½œä¸ºèƒŒæ™¯ï¼Œå°‘æ•°ç±»ä½œä¸ºå‰æ™¯
- en: Focal loss downweighs easy-to-classify examples and focuses on hard-to-classify
    examples. What this means is that it would reduce the modelâ€™s overconfidence;
    this overconfidence usually prevents the model from generalizing well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹æŸå¤±é™ä½äº†æ˜“äºåˆ†ç±»çš„ç¤ºä¾‹çš„æƒé‡ï¼Œå¹¶ä¸“æ³¨äºéš¾ä»¥åˆ†ç±»çš„ç¤ºä¾‹ã€‚è¿™æ„å‘³ç€å®ƒä¼šå‡å°‘æ¨¡å‹çš„è¿‡åº¦è‡ªä¿¡ï¼›è¿™ç§è¿‡åº¦è‡ªä¿¡é€šå¸¸é˜»æ­¢æ¨¡å‹å¾ˆå¥½åœ°æ³›åŒ–ã€‚
- en: Focal loss is an extension of cross-entropy loss. It is especially good for
    multi-class classification, where some classes are easy and others are difficult
    to classify.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹æŸå¤±æ˜¯äº¤å‰ç†µæŸå¤±çš„æ‰©å±•ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºå¤šç±»åˆ†ç±»ï¼Œå…¶ä¸­ä¸€äº›ç±»åˆ«æ˜“äºåˆ†ç±»ï¼Œè€Œå…¶ä»–ç±»åˆ«åˆ™éš¾ä»¥åˆ†ç±»ã€‚
- en: 'Letâ€™s start with our well-known cross-entropy loss for binary classification.
    If we let p be the predicted probability that y = 1, then the cross-entropy loss
    can be defined as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æˆ‘ä»¬ç†ŸçŸ¥çš„äºŒå…ƒåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å¼€å§‹ã€‚å¦‚æœæˆ‘ä»¬è®© p è¡¨ç¤º y = 1 çš„é¢„æµ‹æ¦‚ç‡ï¼Œé‚£ä¹ˆäº¤å‰ç†µæŸå¤±å¯ä»¥å®šä¹‰ä¸ºä»¥ä¸‹ï¼š
- en: CrossEntropyLoss(p) = {âˆ’ log(p) if y = 1Â Â âˆ’ log(1 âˆ’ p) otherwise
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: CrossEntropyLoss(p) = {âˆ’ log(p) if y = 1Â Â âˆ’ log(1 âˆ’ p) otherwise
- en: 'This can be rewritten as CrossEntropyLoss(p) = âˆ’ log( pÂ t), where pÂ t, the
    probability of the true class, can be defined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥é‡å†™ä¸º CrossEntropyLoss(p) = âˆ’ log( pÂ t)ï¼Œå…¶ä¸­ pÂ tï¼ŒçœŸå®ç±»çš„æ¦‚ç‡ï¼Œå¯ä»¥å®šä¹‰ä¸ºä»¥ä¸‹ï¼š
- en: pÂ t = {p if y = 1Â Â 1 âˆ’ p otherwise
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: pÂ t = {p if y = 1Â Â 1 âˆ’ p otherwise
- en: Here, p is the predicted probability that y = 1 from the model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œp æ˜¯æ¨¡å‹é¢„æµ‹ y = 1 çš„æ¦‚ç‡ã€‚
- en: The problem with this loss function is that in the case of imbalanced datasets,
    this loss function is dominated by the loss contribution from majority classes,
    and the loss contribution from the minority class is very small. This can be fixed
    via focal loss.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±å‡½æ•°çš„é—®é¢˜åœ¨äºï¼Œåœ¨æ•°æ®ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°ä¸»è¦ç”±å¤šæ•°ç±»çš„æŸå¤±è´¡çŒ®æ‰€ä¸»å¯¼ï¼Œè€Œå°‘æ•°ç±»çš„æŸå¤±è´¡çŒ®éå¸¸å°ã€‚è¿™å¯ä»¥é€šè¿‡ç„¦ç‚¹æŸå¤±æ¥ä¿®å¤ã€‚
- en: So, what is focal loss?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯ç„¦ç‚¹æŸå¤±ï¼Ÿ
- en: FocalLoss( pÂ t) = âˆ’ Î± (1 âˆ’ pÂ t)Â Î³ log( pÂ t)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: FocalLoss( pÂ t) = âˆ’ Î± (1 âˆ’ pÂ t)Â Î³ log( pÂ t)
- en: 'This formula looks slightly different from cross-entropy loss. There are two
    extra terms â€“ Î± and (1 âˆ’ pÂ t)Â Î³. Letâ€™s try to understand the significance of each
    of these terms:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå…¬å¼çœ‹èµ·æ¥ä¸äº¤å‰ç†µæŸå¤±ç•¥æœ‰ä¸åŒã€‚æœ‰ä¸¤ä¸ªé¢å¤–çš„é¡¹ â€“ Î± å’Œ (1 âˆ’ pÂ t)Â Î³ã€‚è®©æˆ‘ä»¬å°è¯•ç†è§£è¿™äº›é¡¹çš„æ¯ä¸ªå«ä¹‰ï¼š
- en: 'Î±: This value can be set to be inversely proportional to the number of examples
    of positive (minority) classes and is used to weigh the minority class examples
    more than the majority class. It can also be treated as a hyperparameter that
    can be tuned.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î±ï¼šè¿™ä¸ªå€¼å¯ä»¥è®¾ç½®ä¸ºä¸æ­£ï¼ˆå°‘æ•°ï¼‰ç±»æ ·æœ¬çš„æ•°é‡æˆåæ¯”ï¼Œå¹¶ç”¨äºä½¿å°‘æ•°ç±»æ ·æœ¬æ¯”å¤šæ•°ç±»æ ·æœ¬æ›´é‡ã€‚å®ƒä¹Ÿå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ã€‚
- en: '(1 âˆ’ pÂ t)Â Î³: This term is called the **modulating factor**. If an example is
    too easy for the model to classify, that would mean that pÂ t is very high and
    the whole modulating factor value will be close to zero (assuming Î³ > 1), and
    the model wonâ€™t focus on this example much. On the other hand, if an example is
    hard â€“ that is, pÂ t is low â€“ then the modulating factor value will be high, and
    the model will focus on this example more.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1 âˆ’ pÂ t)Â Î³ï¼šè¿™ä¸ªé¡¹è¢«ç§°ä¸º**è°ƒåˆ¶å› å­**ã€‚å¦‚æœä¸€ä¸ªæ ·æœ¬å¯¹æ¨¡å‹æ¥è¯´å¤ªå®¹æ˜“åˆ†ç±»ï¼Œè¿™æ„å‘³ç€ pÂ t éå¸¸é«˜ï¼Œæ•´ä¸ªè°ƒåˆ¶å› å­å€¼å°†æ¥è¿‘é›¶ï¼ˆå‡è®¾ Î³ >
    1ï¼‰ï¼Œæ¨¡å‹ä¸ä¼šå¤ªå…³æ³¨è¿™ä¸ªæ ·æœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä¸€ä¸ªæ ·æœ¬å¾ˆéš¾åˆ†ç±» â€“ å³ pÂ t ä½ â€“ é‚£ä¹ˆè°ƒåˆ¶å› å­å€¼å°†å¾ˆé«˜ï¼Œæ¨¡å‹å°†æ›´å…³æ³¨è¿™ä¸ªæ ·æœ¬ã€‚
- en: Implementation
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'Hereâ€™s the implementation of focal loss from scratch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä»å¤´å®ç°ç„¦ç‚¹æŸå¤±çš„æ–¹æ³•ï¼š
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Though the focal loss technique has roots in computer vision and object detection,
    we can potentially reap its benefits while working with tabular data and text
    data too. Some recent research has ported focal loss to classical ML frameworks
    such as XGBoost [5] and LightGBM [6], as well as to text data that uses transformer-based
    models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç„¦ç‚¹æŸå¤±æŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰å’Œç›®æ ‡æ£€æµ‹é¢†åŸŸæœ‰æ ¹æºï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨å¤„ç†è¡¨æ ¼æ•°æ®å’Œæ–‡æœ¬æ•°æ®æ—¶ä»ä¸­å—ç›Šã€‚ä¸€äº›æœ€è¿‘çš„ç ”ç©¶å°†ç„¦ç‚¹æŸå¤±ç§»æ¤åˆ°ç»å…¸çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå¦‚XGBoost
    [5] å’ŒLightGBM [6]ï¼Œä»¥åŠä½¿ç”¨åŸºäºtransformerçš„æ¨¡å‹çš„æ–‡æœ¬æ•°æ®ã€‚
- en: 'The comic in *Figure 8**.11* shows an archer aiming at a small distant target,
    overlooking a large nearby target, symbolizing the focal lossâ€™s emphasis on challenging
    minority class examples:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾8**.11* ä¸­çš„æ¼«ç”»å±•ç¤ºäº†ä¸€ä½å¼“ç®­æ‰‹ç„å‡†ä¸€ä¸ªè¿œå¤„çš„ç›®æ ‡ï¼Œå´å¿½ç•¥äº†é™„è¿‘çš„å¤§ç›®æ ‡ï¼Œè±¡å¾ç€ç„¦ç‚¹æŸå¤±å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘æ•°ç±»æ ·æœ¬çš„é‡è§†ï¼š'
- en: '![](img/B17259_08_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_11.jpg)'
- en: Figure 8.11 â€“ Illustration of focal loss
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.11 â€“ ç„¦ç‚¹æŸå¤±çš„ç¤ºæ„å›¾
- en: 'PyTorchâ€™s `torchvision` library already has this loss implemented for us to
    use:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„`torchvision`åº“å·²ç»ä¸ºæˆ‘ä»¬å®ç°äº†è¿™ä¸ªæŸå¤±å‡½æ•°ï¼š
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `alpha` and `gamma` values can be challenging to tune for the model and
    dataset being used. Using an `alpha` value of `0.25` and a `gamma` value of `2`
    with `reduction= ''mean''` on CIFAR10-LT (the long-tailed version of the CIFAR10
    dataset) seems to do better than the regular cross-entropy loss, as shown in the
    following graph. For more details, please check the `CIFAR10_LT_Focal_Loss.ipynb`
    notebook in this bookâ€™s GitHub repository:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€ä½¿ç”¨çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œ`alpha` å’Œ `gamma` å€¼å¯èƒ½éš¾ä»¥è°ƒæ•´ã€‚åœ¨CIFAR10-LTï¼ˆCIFAR10æ•°æ®é›†çš„é•¿å°¾ç‰ˆæœ¬ï¼‰ä¸Šä½¿ç”¨`alpha`å€¼ä¸º`0.25`å’Œ`gamma`å€¼ä¸º`2`ï¼Œå¹¶ä¸”`reduction=
    'mean'`ä¼¼ä¹æ¯”å¸¸è§„çš„äº¤å‰ç†µæŸå¤±è¡¨ç°æ›´å¥½ï¼Œå¦‚ä»¥ä¸‹å›¾è¡¨æ‰€ç¤ºã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥çœ‹æœ¬ä¹¦GitHubä»“åº“ä¸­çš„`CIFAR10_LT_Focal_Loss.ipynb`ç¬”è®°æœ¬ï¼š
- en: '![](img/B17259_08_12.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_12.jpg)'
- en: Figure 8.12 â€“ Model accuracy using cross-entropy loss versus focal loss (alpha=0.25,
    gamma=2) on the CIFAR10-LT dataset as training progresses
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.12 â€“ éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±ä¸ç„¦ç‚¹æŸå¤±ï¼ˆalpha=0.25ï¼Œgamma=2ï¼‰åœ¨CIFAR10-LTæ•°æ®é›†ä¸Šçš„æ¨¡å‹å‡†ç¡®åº¦
- en: 'In the Pascal VOC dataset for object detection [7], the focal loss helps detect
    a motorbike in the image, while the cross-entropy loss wasnâ€™t able to detect it
    (*Figure 8**.13*):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç›®æ ‡æ£€æµ‹çš„Pascal VOCæ•°æ®é›† [7] ä¸­ï¼Œç„¦ç‚¹æŸå¤±æœ‰åŠ©äºæ£€æµ‹å›¾åƒä¸­çš„æ‘©æ‰˜è½¦ï¼Œè€Œäº¤å‰ç†µæŸå¤±åˆ™æ— æ³•æ£€æµ‹åˆ°å®ƒ (*å›¾8**.13*)ï¼š
- en: '![](img/B17259_08_13.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_08_13.jpg)'
- en: 'Figure 8.13 â€“ Motorbike not detected by cross-entropy loss (left), while focal
    loss does detect it (right) on the Pascal VOC dataset. Source: fastai library
    GitHub repo [8]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.13 â€“ åœ¨Pascal VOCæ•°æ®é›†ä¸­ï¼Œäº¤å‰ç†µæŸå¤±æ— æ³•æ£€æµ‹åˆ°æ‘©æ‰˜è½¦ï¼ˆå·¦ï¼‰ï¼Œè€Œç„¦ç‚¹æŸå¤±åˆ™å¯ä»¥æ£€æµ‹åˆ°ï¼ˆå³ï¼‰ã€‚æ¥æºï¼šfastaiåº“GitHubä»“åº“ [8]
- en: Though focal loss was initially designed for dense object detection, it has
    gained traction in class-imbalanced tasks due to its ability to assign higher
    weights to challenging examples that are commonly found in minority classes. While
    the proportion of such samples is higher in minority classes, the absolute count
    is higher in the majority class due to its larger size. Consequently, assigning
    high weights to challenging samples across all classes could still cause bias
    in the neural networkâ€™s performance. This motivates us to explore other loss functions
    that can reduce this bias.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Focal loss in production at Meta
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: There was a need to detect harmful content, such as hate speech and violence,
    at Meta (previously Facebook) [9]. ML models were trained on a massive dataset
    of text and images that included both harmful and non-harmful content. However,
    the system was struggling to learn from the harmful content examples because they
    were much fewer in number than the non-harmful examples. This was causing the
    system to overfit the non-harmful examples, and it was not performing well in
    terms of detecting harmful content in the real world.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: To solve the problem, Meta used focal loss. Focal loss, as weâ€™ve seen, is a
    technique that down-weighs the easy-to-classify examples so that the system focuses
    on learning from the hard-to-classify examples. Meta implemented focal loss in
    their training pipeline and was able to improve the performance of their AI system
    when it came to detecting harmful content by up to 10%. This is a significant
    improvement, and it shows that focal loss is a promising technique for training
    AI systems to detect rare or difficult-to-classify events. The new system has
    been deployed into production at Meta, and it has helped to substantially improve
    the safety of the platform.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Class-balanced loss
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper by Cui et al. [10] made a minor change to the equation for cross-entropy
    loss by adding a multiplicative coefficient of Â (1 âˆ’ Î²)Â _Â (1âˆ’ Î²Â n) to the loss
    function â€“ that is, we use a value of Î± = Â (1 âˆ’ Î²)Â _Â (1âˆ’ Î²Â n), where Î² is a hyperparameter
    between 0 and 1, and n is the number of samples of a class:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ClassBalancedCrossEntropyLoss(p) = âˆ’ Â (1 âˆ’ Î²)Â _Â (1âˆ’ Î²Â n)Â  log( pÂ t)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Î² = 0 means no weighting at all, while Î² â†’ 1 means re-weighting by inverse class
    frequency. So, we can consider this method to be a way for the class weight of
    a particular class to be adjustable between 0 and (1/frequency of a class), depending
    on the value of the hyperparameter, Î², which is a tunable parameter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'This same term can be used in place of the alpha value. It can be used in conjunction
    with focal loss too:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: ClassBalancedFocalLoss( pÂ t) = âˆ’ Â (1 âˆ’ Î²)Â _Â (1âˆ’ Î²Â n)Â  (1 âˆ’ pÂ t)Â Î³ log( pÂ t)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: According to Cui et al., the recommended setting for the beta value is (N-1)/N,
    where N is the total number of training examples.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The comic in *Figure 8**.14* illustrates the core idea of this loss. It shows
    a tightrope walker who maintains balance using a pole with weights labeled â€œbetaâ€
    on both ends, representing the adjustment of class weights to address class imbalance:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_14.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 â€“ Illustration of class-balanced loss
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Letâ€™s take a look at the code for implementing class-balanced loss:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the `forward()` function, `effective_num` effectively computes (1-Î²Â n) as
    a vector, where `n` is a vector containing the number of samples per class. So,
    the `weights` vector is Â (1 âˆ’ Î²)Â _Â (1âˆ’ Î²Â n). Using these weights, we compute the
    loss by using `NLLLoss` between the output of the model and the corresponding
    labels. *Table 8.1* shows the class-wise accuracy when the model is trained using
    class-balanced cross-entropy loss for 20 epochs. Here, we can see an accuracy
    improvement for the most imbalanced classes of 9, 8, 7, 6, and 5:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **CrossEntropyLoss** | **ClassBalancedLoss** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| 0 | 99.9 | 99.0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| 1 | 99.6 | 99.0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| 2 | 98.1 | 97.3 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| 3 | 96.8 | 94.7 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| 4 | 97.7 | 97.5 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| 5 | 94.2 | 97.4 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| 6 | 92.8 | 98.3 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| 7 | 81.2 | 94.3 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| 8 | 63.6 | 93.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| 9 | 49.1 | 91.4 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: Table 8.1 â€“ Class-wise accuracy using cross-entropy loss (left) and class-balanced
    cross-entropy loss (right)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.15* compares the performance of class-balanced loss and cross-entropy
    loss:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_15.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 â€“ Overall accuracy versus class-wise accuracy using class-balanced
    loss compared to the baseline model
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Class-balanced loss in production at Apple
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The accessibility team at Apple aimed to ensure usability for all by addressing
    the lack of proper accessibility information in many apps. They made these apps
    usable for individuals with disabilities through features such as screen recognition.
    The researchers aimed to automatically generate accessibility metadata [11] for
    mobile apps based on their visual interfaces, a problem that had significant class
    imbalance due to the diverse range of UI elements. UI elements such as text, icons,
    and sliders were identified from app screenshots. The text elements were highly
    represented with 741,285 annotations, while sliders were least represented with
    1,808 annotations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consisted of 77,637 screens from 4,068 iPhone apps, with a wide
    variety of UI elements, leading to a highly imbalanced dataset, especially considering
    the hierarchical nature of the UI elements.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: A class-balanced loss function and data augmentation were employed to handle
    the class imbalance effectively. This allowed the model to focus more on underrepresented
    UI classes, thereby improving the overall performance. The model was designed
    to be robust and fast, enabling on-device deployment. This ensured that the accessibility
    features could be generated in real time, enhancing the user experience for screen
    reader users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Modern ConvNet classifiers tend to overfit the minority classes in imbalanced
    datasets. What if we could prevent that from happening? The **Class-Dependent
    Temperature** (**CDT**) loss function aims to do that.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Class-dependent temperature Loss
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addressing imbalanced datasets, traditional explanations suggest that a modelâ€™s
    inferior performance on minority classes, compared to majority classes, stems
    from its inclination to minimize average per-instance loss. This biases the model
    toward predicting majority classes. To counteract this, re-sampling and re-weighting
    strategies have been proposed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: However, Ye et al. [12] introduced the **Class-Dependent Temperature** (**CDT**)
    Loss, presenting a novel perspective. Their research indicates that ConvNets tend
    to overfit minority class examples, as evident from a larger feature deviation
    between training and test sets for minority classes compared to majority ones.
    Feature deviation occurs when a model learns the training data distribution of
    feature values excessively well, subsequently failing to generalize to new data.
    With CDT loss, the modelâ€™s decision values for training examples are divided by
    a â€œtemperatureâ€ factor, dependent on each classâ€™s frequency. This division makes
    the training more attuned to feature deviation and aids in effective learning
    across both prevalent and scarce categories.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.16* portrays how CDT loss modifies class weights according to class
    frequencies, using the visual analogy of a juggler on a unicycle handling items
    marked with different class names:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_16.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 â€“ A unicyclist juggling items, adjusting class weights based on
    frequencies
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'The following class implements this loss function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is an explanation of the `CDT` class:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '`self.num_class_list` stores the number of examples in each class.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.cdt_weight = torch.FloatTensor([...]).to(device)` computes the class-dependent
    temperature weights for each class. For each class, the weight is computed as
    `(max(num_class_list) / num_class_list[i]) **` `gamma`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger the number of examples in a class, the smaller its value in the `self.cdt_weight`
    list. Majority class examples have lower values, while minority class examples
    have higher values.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs = inputs /self.cdt_weight` scales the log probabilities (as inputs)
    from the model by the class-dependent temperature weights. This increases the
    absolute values of the negative log probabilities for minority class examples,
    making them more significant in the loss calculation than those for the majority
    class. This intends to make the model focus more on the minority class examples.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In *Figure 8**.17*, weâ€™re plotting the overall accuracy of CDT loss and cross-entropy
    loss (left) and the accuracies of various classes (right):'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17259_08_17.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 â€“ Performance comparison between cross-entropy loss and CDT loss
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there is an accuracy improvement for some classes, such as 9,
    7, 6, 5, and 3, but a decrease in performance for some of the other classes. It
    seems to give a lukewarm performance on the imbalanced MNIST dataset that we used,
    but it can potentially be helpful for other datasets.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: What if we could dynamically adjust the weights of the classes according to
    their difficulty for the model during training? We could measure the class difficulty
    by the accuracy of its predictions for the examplesâ€™ class and then use this difficulty
    to compute the weight for that class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Class-wise difficulty-balanced loss
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper from Sinha et al. [13] proposed that the weight for a class, c, after
    training time, t, should be directly proportional to the difficulty of the class.
    The lower the accuracy of the class, the higher its difficulty.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this can be represented as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: wÂ c, t = ( dÂ c, t)Â Ï„
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, wÂ c, t is the weight of class c after training time t, and dÂ c, t is
    the class difficulty, which is defined by the following equation:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: dÂ c, t = (1 âˆ’ AccuracyÂ c, t)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Here, AccuracyÂ c, t is the accuracy of class c on the validation dataset after
    time t, and Ï„ is a hyperparameter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The point here is that we would want to dynamically increase the weight of
    the class for which the modelâ€™s accuracy is lower as training progresses. We could
    do this every epoch or every few epochs of training and feed the updated weights
    to the cross-entropy loss. Please look at the corresponding notebook titled `Class_wise_difficulty_balanced_loss.ipynb`
    in this bookâ€™s GitHub repository for the full training loop:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 8**.18* illustrates the concept of difficulty-balanced loss using a
    comic with an acrobat on a trampoline. Each bounce is labeled with an accuracy
    score, highlighting how classes with lower accuracy receive increasing weight
    as the acrobat bounces higher each time:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_18.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 â€“ Illustration of difficulty-balanced loss â€“ the acrobatâ€™s bounces
    show increasing weight for lower-accuracy classes
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.19* shows the performance of class-wise difficulty-balanced loss
    compared to cross-entropy loss as the baseline:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_19.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 â€“ Performance comparison of models trained using class-wise difficulty-balanced
    loss and cross-entropy loss
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the performance of several classes improves, including
    the biggest jump of 40% to 63.5% for the most imbalanced class (9).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some of the other miscellaneous algorithm-based techniques
    that can still help us deal with imbalanced datasets.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Discussing other algorithm-based techniques
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, weâ€™ll explore a diverse set of algorithm-level techniques
    that we havenâ€™t covered so far. Intriguingly, these methods â€“ from regularization
    techniques that mitigate overfitting to Siamese networks skilled in one-shot and
    few-shot learning, to deeper neural architectures and threshold adjustments â€“
    also have a beneficial side effect: they can occasionally mitigate the impact
    of class imbalance.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Regularization techniques
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper from S. Alshammari et al. [14] found that well-known regularization
    techniques such as L2-regularization and the MaxNorm constraint are quite helpful
    in long-tailed recognition. The paper proposes to do these only at the last layer
    of classification (sigmoid or softmax, for example). Here are their findings:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '**L2-regularization** (also called weight decay) generally keeps the weights
    in check and helps the model generalize better by preventing the model from overfitting.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tf.keras.constraints.MaxNorm`, while PyTorch has `torch.clamp` to help
    with this.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siamese networks
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On a similar note, previous research has found **Siamese networks** to be very
    robust to the adverse effects of class imbalance. Siamese networks have been quite
    useful in the areas of one-shot learning (classifying new data when we have only
    one example of each class in the training data) and few-shot learning (classifying
    new data when we have only a few examples of each class in the training data).
    Siamese networks use a contrastive loss function that takes in pairs of input
    images and then computes a similarity metric (Euclidean distance, Manhattan distance,
    or cosine distance) to figure out how similar or dissimilar they are. This can
    be used to compute the embeddings of each unique class of images in the training
    data. At inference time or test time, the distance of the new input image from
    each unique class can be computed to find the appropriate class of the image.
    The best part of this technique is that it provides a way to learn the feature
    representation of each class. Siamese networks have found a wide variety of practical
    applications in the industry regarding vision problems (for example, whether two
    images are of the same person or not) as well as NLP problems (for example, finding
    out whether two questions/queries are similar or not on, say, platforms such as
    Stack Overflow, Quora, Google, and so on).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.20* shows a Siamese network where two inputs are fed into the model
    to get their embeddings, which are then compared for similarity using a distance
    metric:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_08_20.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 â€“ High-level working of the Siamese network model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Deeper neural networks
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A study by Ding et al. 2017 [15] discovered that deeper neural networks (more
    than 10 layers) are more helpful in general with imbalanced datasets for two reasons:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: A faster rate of convergence
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better overall performance
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is attributed to the fact that deep networks are exponentially more efficient
    at capturing the complexity of data. Though their experiment was for facial action
    recognition tasks, this may be useful for trying out deeper networks on other
    kinds of data and domains.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: However, the cons of longer training times, increased hardware cost, and increased
    complexity may not always be worth the hassle in industry settings.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Threshold adjustment
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive
    Learning*, threshold adjustment is a cost-sensitive meta-learning technique. Threshold
    adjustment applies equally well to deep learning models, and it can be critical
    to make sure that the thresholds for classification are properly tuned and adjusted,
    especially when the training data distribution has been changed (for example,
    oversampled or undersampled) or even when class weights or new loss functions
    are used.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various loss functions as remedies to class imbalance.
    We started with the class-weighting technique and deferred re-weighting, both
    designed to penalize errors on minority class samples. As we progressed, we encountered
    focal loss, where we shifted from class-centric to sample-centric weighting, focusing
    on the difficulty of samples. Despite its merits, we learned that focal loss may
    still be biased toward the majority class when assigning weights to challenging
    samples across all classes. Subsequent discussions on class-balanced loss, CDT
    loss, and class-wise difficulty-balanced loss were provided, each introducing
    unique strategies to dynamically adjust weights or modulate the modelâ€™s focus
    between easy and challenging samples, aiming to enhance performance on imbalanced
    datasets.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, algorithm-level techniques usually modify the loss functions used
    by the model in some way to accommodate for imbalances in the dataset. They typically
    do not increase the training time and cost, unlike data-level techniques. They
    are well suited for problems or domains with large amounts of data or where gathering
    more data is hard or expensive.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Even though these techniques improve the performance of minority classes, the
    majority classes may sometimes suffer as a result. In the next chapter, we will
    look at some of the hybrid techniques that can combine the data-level and algorithm-level
    techniques so that we can get the best of both worlds.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mean false error and mean squared false error:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang et al. [16]proposed that regular loss functions poorly capture the errors
    from minority classes in the case of high data imbalance due to lots of negative
    samples that dominate the loss function. Hence, they proposed a new loss function
    where the main idea was to split the training error into four different kinds
    of errors:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: False Positive Error (FPE) = (1/number_of_negative_samples) * (error from negative
    samples)
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negative Error (FNE) = (1/number_of_positive_samples) * (error from positive
    samples)
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean False Error (MFE) = FPE+ FNE
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Squared False Error (MSFE) = FPE2 + FNE2
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The error here could be computed using the usual cross-entropy loss or any other
    loss used for classification. Implement the MFE and MSFE loss functions for both
    the imbalanced MNIST and CIFAR10-LT datasets, and see whether the model performance
    improves over the baseline of cross-entropy loss.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this chapter, while implementing the CDT loss function, replace the imbalanced
    MNIST dataset with CIFAR10-LT (the long-tailed version of CIFAR-10). Check whether
    you still achieve improved performance over the baseline. You may have to play
    with the gamma value or perform any of the other tricks mentioned in the original
    paper [12] to get an improvement over the baseline.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tversky Loss was introduced in the paper by Salehi et al. [17]. Please read
    this paper to understand the Tversky loss function and its implementation details.
    Finally, implement the Tversky loss on an imbalanced MNIST dataset and compare
    its performance with a baseline model.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the class-weighting technique and cross-entropy loss with the `trec`
    dataset in this chapter. Replace cross-entropy loss with focal loss, and see whether
    model performance improves.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*DALLÂ·E 2 pre-training mitigations*, 2022, [https://openai.com/research/dall-e-2-pre-training-mitigations](https://openai.com/research/dall-e-2-pre-training-mitigations).'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*BERT Does Business: Implementing the BERT Model for Natural Language Processing
    at Wayfair*, 2019, [https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair](https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair).'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*, [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. DollÃ¡r, *Focal Loss for Dense
    Object Detection*. arXiv, Feb. 07, 2018, [http://arxiv.org/abs/1708.02002](http://arxiv.org/abs/1708.02002).
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang et al., *Imbalance-XGBoost: leveraging weighted and focal losses for binary
    label-imbalanced classification with* *XGBoost*, [https://arxiv.org/pdf/1908.01672.pdf](https://arxiv.org/pdf/1908.01672.pdf).'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Focal loss implementation for* *LightGBM*, [https://maxhalford.github.io/blog/lightgbm-focal-loss](https://maxhalford.github.io/blog/lightgbm-focal-loss).'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The PASCAL VOC* *project*, [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/).'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*fastai library*, 2018, [https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb](https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb).'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Community Standards report*, 2019, [https://ai.meta.com/blog/community-standards-report/](https://ai.meta.com/blog/community-standards-report/).'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, *Class-Balanced Loss Based
    on Effective Number of Samples*, p. 10.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Zhang et al., *Screen Recognition: Creating Accessibility Metadata for Mobile
    Applications from Pixels*, in Proceedings of the 2021 CHI Conference on Human
    Factors in Computing Systems, Yokohama Japan: ACM, May 2021, pp. 1â€“15\. doi: 10.1145/3411764.3445186\.
    Blog: [https://machinelearning.apple.com/research/mobile-applications-accessible](https://machinelearning.apple.com/research/mobile-applications-accessible).'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available: [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Sinha, H. Ohashi, and K. Nakamura, *Class-Wise Difficulty-Balanced Loss
    for Solving Class-Imbalance*. arXiv, Oct. 05, 2020\. Accessed: Dec. 17, 2022\.
    [Online]. Available at [http://arxiv.org/abs/2010.01824](http://arxiv.org/abs/2010.01824).'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Alshammari, Y.-X. Wang, D. Ramanan, and S. Kong, *Long-Tailed Recognition
    via Weight Balancing*, in 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 6887â€“6897\. Doi: 10.1109/CVPR52688.2022.00677.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. Ding, D.-Y. Huang, Z. Chen, X. Yu, and W. Lin, *Facial action recognition
    using very deep networks for highly imbalanced class distribution*, in 2017 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), Kuala Lumpur, Dec. 2017, pp. 1368â€“1372\. doi: 10.1109/APSIPA.2017.8282246.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, *Training deep
    neural networks on imbalanced datasets*, in 2016 International Joint Conference
    on Neural Networks (IJCNN), Vancouver, BC, Canada, Jul. 2016, pp. 4368â€“4374\.
    doi: 10.1109/IJCNN.2016.7727770.'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S. S. M. Salehi, D. Erdogmus, and A. Gholipour, *Tversky loss function for
    image segmentation using 3D fully convolutional deep networks*. arXiv, Jun. 18,
    2017\. Accessed: Dec. 23, 2022\. [Online]. Available at [http://arxiv.org/abs/1706.05721](http://arxiv.org/abs/1706.05721).'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
