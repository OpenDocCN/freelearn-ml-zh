- en: Chapter 3. Customer Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customer analytics is a process in which we use the data of customer behavior
    to derive the most important business decisions using market segmentation and
    predictive analytics. Market segmentation is the process of dividing the user
    base into subgroups based on their behavior and other types of shared characteristics.
    This will help companies in providing customized products for each user segment.
    The result of this kind of analysis will lead the company to grow their business
    in an effective manner. Companies also make more profit. There are a lot of advantages.
    I know this is only a brief discussion about market segmentation, but just bear
    with me for a while. I will give you all the necessary information in the upcoming
    sections.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Companies can use the result generated by market segmentation and predictive
    models for direct marketing, site selection, customer acquisition, and customer
    relationship management. In short, with the help of customer analytics, the company
    can decide the most optimal and effective marketing strategy as well as growth
    strategy. The company can achieve great results with a limited amount of marking
    expenditure. Customer analytics include various methods. You can refer to the
    names of these methods in the following diagram:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer Analytics](img/B08394_03_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Variety of methods for customer analytics'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we won''t be covering all the methods given in the previous
    figure, but we will cover the methods that are most widely used in the industry.
    We will build a customer segmentation application. In this chapter, we will cover
    the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing customer segmentation:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the baseline approach for customer segmentation:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the result of the baseline approach
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the revised approach for customer segmentation:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the revised approach
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best approach for customer segmentation:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the best approach
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer segmentation for various domains
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start with customer segmentation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Introducing customer segmentation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover customer segmentation in detail. Initially, I
    provided just a brief introduction of customer segmentation so that you could
    understand the term a bit. Here, we will understand a lot more about customer
    segmentation, which will help us further when we build the customer segmentation
    analysis.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, customer segmentation is a process where we divide the
    consumer base of the company into subgroups. We need to generate the subgroups
    by using some specific characteristics so that the company sells more products
    with less marketing expenditure. Before moving forward, we need to understand
    the basics, for example, what do I mean by customer base? What do I mean by segment?
    How do we generate the consumer subgroup? What are the characteristics that we
    consider while we are segmenting the consumers? Let's answers these questions
    one by one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，客户细分是一个过程，我们将公司的消费者基础划分为子群体。我们需要通过使用一些特定的特征来生成子群体，以便公司以更少的营销支出销售更多产品。在继续前进之前，我们需要了解基础知识，例如，我所说的消费者基础是什么？我所说的细分是什么？我们是如何生成消费者子群体的？我们在细分消费者时考虑哪些特征？让我们逐一回答这些问题。
- en: 'Basically, the consumer base of any company consists of two types of consumers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，任何公司的消费者基础由两种类型的消费者组成：
- en: Existing consumers
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现有消费者
- en: Potential consumers
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在消费者
- en: Generally, we need to categorize our consumer base into subgroups. These subgroups
    are called segments. We need to create the groups in such a way that each subgroup
    of customers has some shared characteristics. In order to explain how to generate
    the subgroup, let me give you an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要将我们的消费者基础划分为子群体。这些子群体被称为细分市场。我们需要以这种方式创建群体，使得每个客户子群体都有一些共同的特征。为了解释如何生成子群体，让我给你举一个例子。
- en: Suppose a company is selling baby products. Then, it needs to come up with a
    consumer segment (consumer subgroup) that includes the consumers who want to buy
    the baby products. We can build the first segment (subgroup) with the help of
    a simple criterion. We will include consumers who have one baby in their family
    and bought a baby product in the last month. Now, the company launches a baby
    product that is too costly or premium. In that case, we can further divide the
    first subgroup into monthly income and socio-economic status. Based on these new
    criteria, we can generate the second subgroup of consumers. The company will target
    the consumers of the second subgroup for the costly and premium products, and
    for general products, the company will target consumers who are part of the first
    subgroup.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家公司正在销售婴儿产品。那么，它需要提出一个消费者细分市场（消费者子群体），包括想要购买婴儿产品的消费者。我们可以借助一个简单的标准来构建第一个细分市场（子群体）。我们将包括那些家庭中有一个孩子并在过去一个月购买了婴儿产品的消费者。现在，公司推出了一款成本过高或高端的婴儿产品。在这种情况下，我们可以进一步将第一个子群体细分为月收入和社会经济状况。基于这些新的标准，我们可以生成第二个消费者子群体。公司将为第二个子群体的消费者定位高端和高端产品，而对于一般产品，公司将针对属于第一个子群体的消费者。
- en: 'When we have different segments, we can design a customized marketing strategy
    as well as customized products that suit the customer of the particular segment.
    This segment-wise marketing will help the company sell more products with lower
    marketing expenses. Thus, the company will make more profit. This is the main
    reason why companies use customer segmentation analysis nowadays. Customer segmentation
    is used among other domain such as the retail domain, finance domain, and in customer
    relationship management (CRM)-based products. I have provided a list of the basic
    features that can be considered during the segmentation. You can refer to them
    in the following screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有不同的细分市场时，我们可以设计定制化的营销策略以及适合特定细分市场客户的定制产品。这种按细分市场进行的营销将帮助公司以更低的营销费用销售更多产品。因此，公司会获得更多利润。这就是为什么公司现在使用客户细分分析的主要原因。客户细分在零售领域、金融领域以及基于客户关系管理（CRM）的产品等领域也被使用。我已经提供了一份在细分过程中可以考虑的基本特征列表。您可以在以下屏幕截图中参考它们：
- en: '![Introducing customer segmentation](img/B08394_03_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![介绍客户细分](img/B08394_03_02.jpg)'
- en: 'Figure 3.2: List of basic features used in customer segmentation'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：客户细分中使用的基本特征列表
- en: 'You may wonder how companies are making marketing strategies based on the customer
    segmentation analysis. The answer is companies are using the STP approach to make
    the marketing strategy firm. What is the STP approach? First of all, STP stands
    for Segmentation-Targeting-Positioning. In this approach, there are three stages.
    The points that we handle in each stage are explained as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道公司是如何基于客户细分分析制定营销策略的。答案是公司正在使用STP方法来制定营销策略。什么是STP方法？首先，STP代表细分-定位-定位。在这个方法中，有三个阶段。我们在每个阶段处理的问题如下所述：
- en: '**Segmentation**: In this stage, we create segments of our customer base using
    their profile characteristics as well as consider features provided in the preceding
    figure. Once the segmentation is firm, we move on to the next stage.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targeting**: In this stage, marketing teams evaluate segments and try to
    understand which kind of product is suited to which particular segment(s). The
    team performs this exercise for each segment, and finally, the team designs customized
    products that will attract the customers of one or many segments. They will also
    select which product should be offered to which segment.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positioning**: This is the last stage of the STP process. In this stage,
    companies study the market opportunity and what their product is offering to the
    customer. The marketing team should come up with a unique selling proposition.
    Here, the team also tries to understand how a particular segment perceives the
    products, brand, or service. This is a way for companies to determine how to best
    position their offering. The marketing and product teams of companies create a
    value proposition that clearly explains how their offering is better than any
    other competitors. Lastly, the companies start their campaign representing this
    value proposition in such a way that the consumer base will be happy about what
    they are getting.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I have summarized all the preceding points in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing customer segmentation](img/B08394_03_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Summarization of the STP approach'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We have covered most of the basic parts of customer segmentation. Now it's time
    to move on to the problem statement.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know, customer segmentation helps companies retain existing customers
    as well as acquire new potential customers. Based on the segmentation, companies
    can create customized products for a particular customer segment, but so far,
    we don't know how to generate the segments. This is the point that we will focus
    on in this chapter. You need to learn how to create customer segmentation. There
    are many domains for which we can build customer segmentation, such as e-commerce,
    travel, finance, telecom, and so on. Here, we will focus only on the e-commerce
    domain.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a detailed explanation of the problem statement, input, and output
    for the e-commerce customer segmentation application that we will be building:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem statement**: The goal of our customer segmentation application is
    to come up with a solution for the given questions:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we categorize the customers in a particular segment based on their buying
    patterns?
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we predict which kind of items they will buy in future based on their segmentation?
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input**: We will be using e-commerce data that contains the list of purchases
    in 1 year for 4,000 customers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: The first goal is that we need to categorize our consumer base
    into appropriate customer segments. The second goal is we need to predict the
    purchases for the current year and the next year based on the customers'' first
    purchase.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may wonder how we can achieve a prediction about the upcoming purchases
    using segmentation. Well, let me tell you how segmentation helps us! So, we don't
    know the purchase pattern of the new customer, but we know the customer profile.
    We also know which product the customer has bought. So, we can put the customer
    into one of the segments where all other customers have purchased similar items
    and share similar kinds of profile.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Let me give you an example. Say, a person has bought a Harry Potter book and
    that person lives in the UK. The age group of the customer is from 13-22\. If
    we have already generated a customer segment that satisfies these characteristics,
    then we will put this new customer in that particular subgroup. We will derive
    the list of items that the customer may buy in future. We will also offer similar
    services that other customers in the subgroup have.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The approach that we will be using in order to develop customer segmentation
    for the e-commerce domain can also be used in other domains, but data points (features)
    will differ for each domain. Later on in the chapter, we will discuss what kind
    of data points you may consider for other domains, such as travelling, finance,
    and so on. I will provide the list of data points for other domains that will
    help you build the customer segmentation application from scratch.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time to understand the dataset for building customer segmentation
    for the e-commerce domain.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the datasets
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding out an appropriate dataset is a challenging task in data science. Sometimes,
    you find a dataset but it is not in the appropriate format. Our problem statement
    will decide what type of dataset and data format we need. These kinds of activities
    are a part of data wrangling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data wrangling is defined as the process of transforming and mapping data from
    one data form into another. With transformation and mapping, our intention should
    be to create an appropriate and valuable dataset that can be useful in order to
    develop analytics products. Data wrangling is also referred to as data munging
    and is a crucial part of any data science application.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Generally, e-commerce datasets are proprietary datasets, and it's rare that
    you get transactions of real users. Fortunately, *The UCI Machine Learning Repository*
    hosts a dataset named *Online Retail*. This dataset contains actual transactions
    from UK retailers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This Online Retail dataset contains the actual transactions between December
    1, 2010 and December 9, 2011\. All the transactions are taken from the registered
    non-store online retail platform. These online retail platforms are mostly based
    in the UK. The online retail platforms are selling unique all-occasion gifts.
    Many consumers of these online retail platforms are wholesalers. There are 532610
    records in this dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the dataset
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download this dataset by using either of the following links:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attributes of the dataset
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the attributes in this dataset. We will take a look at a short description
    for each of them:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'InvoiceNo: This data attribute indicates the invoice numbers. It is a six-digit
    integer number. The records are uniquely assigned for each transaction. If the
    invoice number starts with the letter ''c'', then it indicates a cancellation.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'StockCode: This data attribute indicates the product (item) code. It is a five-digit
    integer number. All the item codes are uniquely assigned to each distinct product.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Description: This data attribute contains the description about the item.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantity: This data attribute contains the quantities for each product per
    transaction. The data is in a numeric format.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'InvoiceDate: The data attribute contains the invoice date and time. It indicates
    the day and time when each transaction was generated.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UnitPrice: The price indicates the product price per unit in sterling.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CustomerID: This column has the customer identification number. It is a five-digit
    integer number uniquely assigned to each customer.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Country: This column contains the geographic information about the customer.
    It records the country name for the customers.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can refer to the sample of the dataset given in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![Attributes of the dataset](img/B08394_03_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Sample recodes from the dataset'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Now we will start building the customer segmentation application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline approach
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will start implementing the basic model for the customer
    segmentation application. Furthermore, we will improve this baseline approach.
    While implementing, we will cover the necessary concepts, technical aspects, and
    significance of performing that particular step. You can find the code for the
    customer-segmentation application at this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation](https://github.com/jalajthanaki/Customer_segmentation)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The code related to this chapter is given in a single iPython notebook. You
    can access the notebook using this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb](https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code given on GitHub because it will help you understand things
    better. Now let's begin the implementation!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement the customer segmentation model, our implementation will
    have the following steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating customer categories
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classifying customers
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's begin with data preparation!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a basic step when you try to build any analytics application. First,
    we need to be sure that the format of the data is in an appropriate form. If it
    is not, then we need to prepare our dataset in such a way that we can build our
    application easily. In this step, we will find out whether we have a good quality
    dataset or not. We can also find out some basic facts about the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, we don't need to change the format of our e-commerce dataset, but we
    will be exploring the dataset in such a way that we can find out the quality of
    the dataset. If format of the dataset is not proper then you need to decide the
    format of the dataset in such a way that any kind of analysis can be performed
    using the dataset. You can convert the data records either in CSV format or in
    JSON format or in XML format. In addition, we can derive general facts about the
    dataset, such as whether our dataset is biased or not, whether the dataset contains
    any null values, the mapping of the customers with `Customer_ID` is proper or
    not, whether their purchases are properly recorded in dataset or not, and so on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to load the dataset, we will use the pandas `read_csv` API. You can
    find the code snippet given in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_03_05.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Code snippet for loading the dataset'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the dimensions of the dataset are (541909, 8). This means that
    there are 541,909 records in the dataset and eight data attributes. We have already
    covered these eight data attributes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to perform exploratory data analysis (EDA), which can help us preprocess
    our dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA)
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we need to check the statistical properties of the dataset
    and perform some preprocessing steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Removing null data entries
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing duplicate data entries
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EDA for various data attributes
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing null data entries
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we need to check the data type of each of the attributes as well as
    find out which column has a null value. You can refer to the code snippet shown
    in the following screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing null data entries](img/B08394_03_06.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Code snippet for exploring the dataset'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the code, we have generated the total number of null values
    for each data attribute. We have also generated the percentage of null values
    for each data attribute. We can observe that for the `CustomerID` column, there
    are ~25% data entries that are null. That means there is no `CustomerID` value
    available for ~25% of the dataset. This indicates that there are many entries
    that do not belong to any customer. These are abended data entries. We cannot
    map them to the existing CustomerIDs. As a result, we need to delete them. You
    can find the code snippet for deleting null data entries from the dataset in the
    following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing null data entries](img/B08394_03_07.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Deleting null data entries'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Removing duplicate data entries
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After this step, we will check whether there are any duplicate data entries
    present in the dataset. In order to answer this question, we will use the pandas
    `duplicate()` function. You can refer to the code snippet shown in the following
    screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing duplicate data entries](img/B08394_03_08.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Removing duplicate data entries'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we found 5,225 duplicate data entries. Therefore, we have removed
    them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Now let's analyze each data attribute in detail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: EDA for various data attributes
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EDA for each data attribute will help us get more insight into the dataset.
    Later on, we will use these facts to build an accurate customer segmentation application.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start exploring data attributes in the following order:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Country
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Customer and products
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Product categories
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining product categories
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Country**'
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We need to find out facts such as how many countries there are in our dataset.
    In order to answer this question, we need to execute the code shown in the following
    screenshot:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Country](img/B08394_03_09.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Code snippet for generating the number of counties present in the
    dataset'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to find the country from which we receive the maximum number of
    orders. We can find that out by using the pandas `groupby()` and `count()` functions.
    We sort the number of orders in descending order. You can refer to the code snippet
    in the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Country](img/B08394_03_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Code snippet for generating country-wise number of orders'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding snippet, there are a majority of orders from
    UK-based customers. Now we need to explore the customer and products variables.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer and products**'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here, we have approximately 400,000 data items. We need to know the number
    of users and products that are present in these data entries. We will be using
    the `value_counts()` function from the `pandas` library. Take a look at the code
    snippet in the following screenshot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_11.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Code for exploring customer and products'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the above screen shot that this dataset contains the records
    of 4372 users who bought 3684 different items
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: We have derived some interesting facts. In the given dataset, there are 4,372
    customers who have bought 3,684 different products. The total number of transactions
    is 22,190.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also find out how many products have been purchased for each transaction.
    For that, we will use the `InvoiceNo` and `InvoiceDate` data attributes, and we
    will calculate the number of products purchased for every transaction. You can
    refer to the code snippet shown in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Code snippet for exploring the number of products per transaction'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding code snippet, we can make the following observations:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: There are some users who have made a purchase only once on the e-commerce platform
    and bought one item. An example of this kind of user is `customerID 12346`.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some users who frequently buy a large number of items per order. An
    example of this kind of user is `customerID 12347`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you look at the InvoiceNo data attribute, then you can see that there is
    the prefix `C` for one invoice. This `'C'` indicates that the particular transaction
    has been canceled.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we know, there can be a couple of canceled orders present in our dataset,
    and we need to count the number of transactions corresponding to the canceled
    orders. We have used a simple check condition using the lambda expression. Now
    we will calculate the percentage of canceled orders. You can refer to the code
    snippet given in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Code snippet for generating the percentage of canceled orders'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list down some of the canceled order entries so that we can find out
    how to handle them. Take a look at the following screenshot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: List of canceled orders'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, in order to handle the canceled orders, we will need to take the
    following steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, if the order is canceled, then there is another transaction
    that will mostly have an identical transaction except for the quantity and invoice
    date. First, we need to check whether this is true for all entries.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform this checking operation by using simple logic. Mostly, the canceled
    order has a negative quantity, so we will check whether there is an order indicating
    the same quantity (but positive), with the same description values.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some discount entries as well, and we need to handle them. We will
    discard the discount entries.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code for this, as shown in the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Code for handelling cancel orders'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the preceding code, we find out that there are no similar entries
    present in our dataset for all canceled transactions. In order to overcome this
    situation, we will create a new variable in our dataframe, which indicates whether
    the transaction has been canceled or not. There are three possibilities for canceled
    orders:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: There are some transactions that were canceled without counterparts. A few of
    them are probably due to the fact that the buy orders were performed before December
    2010\. We have the dataset from December 2010 to December 2011.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some orders that were canceled with exactly one counterpart. We will
    consider them as well.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some entries that are doubtful. We will check whether there is at
    least one counterpart with the exact same quantity available. If available, then
    we can mark those entries as doubtful.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code shown in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_16.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Code snippet for generating flags for canceled orders'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the preceding code snippet, there are 7,521 entries that show
    the canceled orders with their counterpart. There are 1,226 entries that show
    canceled orders without their counterpart. For the sake of simplicity, we are
    going to delete all the entries related to the canceled orders. The code for deleting
    these records is given in the following screenshot:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_17.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Code snippet for deleting canceled orders'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s analyze the entries based on the stock code because we know that
    during the identification of the canceled order, we discover discount items based
    on the *stock code D*. So first of all, we will be listing down all the stock
    codes and their meaning. You can refer to the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_18.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Code snippet for stock code'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s focus on the pricing of the individual order. In the given dataset,
    the order from a single customer has been split into several lines. What do I
    mean by several lines? In order to understand that, refer to the following screenshot:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_19.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: Understanding data entries for orders'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Each entry in our dataset indicates prizes for a single kind of product. If
    the order including different products is placed by a single customer, then there
    are multiple entries for that particular order. The number of data entries depends
    on how many different products that order has. As you can see in the preceding
    figure, there were three different products included in one order. We need to
    obtain the total price for each order. In order to achieve that, we will add a
    column named *TotalPrice*, which gives us the total value of the order or the
    basket price for a single order. The main logic for deriving *TotalPrice* is that
    we are multiplying *UnitPrice* with the net quantity. We obtain the net quantity
    by deducting the canceled quantity from the total quantity. Take a look at the
    code snippet shown in the following screenshot:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_20.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Code for obtaining TotalPrice'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we obtain the total price, we will generate the sum for individual orders
    and then group our entries based on the invoice data. We will list only those
    data entries that have a basket price greater than 0\. The code to achieve this
    is given in the following screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_21.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Code for generating the basket price based on the invoice date'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to get an idea about the distribution of the orders'' amounts
    for the given dataset. What do I mean by distribution of the orders'' amounts?
    Well, we should be aware about the prices for all the orders present in the dataset,
    and we need to put in the ranges based on the amount of all the orders. This will
    help us derive the number of orders in the dataset that are above £200\. It will
    also help us identify the number of orders that are below £100\. This kind of
    information helps us know the data distribution based on the number of orders.
    This will give us a basic picture about sales on the e-commerce platform. The
    code snippet for generating data distribution based on the orders'' amounts is
    displayed in the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_22.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.22: Code snippet for generating data distribution based on orders''
    amounts'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the pictorial representation of this data distribution as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_23.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.23: Pictorial representation of the data distribution'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, approximately 65% of the orders are above £200\. We have explored
    orders in great detail. Now let's begin with the analysis of product categories.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**Product categories**'
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will be doing an EDA of the product-related data attribute.
    We will include the following kinds of analysis in this section:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the product description
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the product categories
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterizing the content of clusters
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analyzing the product description**'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, we will be using two data attributes. We will use the `StockCode`
    data attribute, which contains a unique ID for each product. We will also use
    the `Description` data attribute in order to group the products in different categories.
    Let's start with the product description.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the function that will take the dataframe as input, and
    then we will perform the the following operations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We will extract names (nouns) from the product description.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we will generate the root form of the extracted names. We will store the
    root of the name as the key and all associated names as its value. We will use
    a stemmer from the NLTK library for this step. A stemmer basically generates the
    root form of the words by removing suffixes and prefixes.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will count the frequency of the roots of the names, which means we will count
    how many times the root form of each name appears.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If various names have the same root, then we consider the root form as the keyword
    tag.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the code for this function in the following screenshot:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_24.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.24: Code snippet of the function for generating keywords from the
    product description'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to call this function and feed the input dataframe. You can take
    a look at the code snippet given in the following screenshot:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_25.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Code snippet that actually generates keywords'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are returning three variables:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '`Keyword:` This is the list of extracted names'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Keywords_roots:` This is a dictionary where the keys are the root of the name
    and values are the list of names associated with root name.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Count_keywords:` This is a dictionary that keeps track of the frequency of
    each name. The count indicates the number of times a particular name appeared
    in the description. Later on, we will convert the dictionary into a list.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s plot the keywords versus their frequency graphs. The code is given
    in the following screenshot:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_26.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Code snippet for generating the frequency graph'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, the word (meaning the noun or the name)
    heart has appeared the maximum number of times in the product description. You
    might wonder what the significance of generating this word frequency is. Well,
    we are using this to categorize products. Now it's time to look into how to come
    up with product categories.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining product categories**'
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here we will obtain the product categories. We have obtained more than 1,400
    keywords, and the most frequent names have appeared in more than 200 products.
    Now we need to remove words that are less important. We can observe some useless
    words, such as names of colors and discard them. So, we will consider words that
    appear in the dataset more than 13 times. You can refer to the code snippet shown
    in the following screenshot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_27.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Code snippet for preserving important words'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to encode the data. Here, we have textual data and we need to convert
    it into a numerical format. For this, we will use one-hot encoding. One-hot encoding
    is a simple concept. In order to understand it, refer to the given matrix x. Take
    a look at the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_28.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.28: Table for understanding one-hot data encoding'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'If a particular word is present in the product description, then the value
    of the coefficient is 1, and if the word is not present in the product description,
    then the value of the coefficient is 0\. You can refer to the the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_29.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.29: Intuitive example for one-hot data encoding'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this data encoding is a binary kind of vectorization because
    we are placing either zero or one. We will get a sparse vector for each word after
    encoding. In layman's terms, we can say that this kind of vectorization indicates
    the presence of the word in the product description.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create the groups or cluster for the product based on the price
    range. For that, we will be using the keyword list that we have generated, check
    whether the product description has the words that are present in the keywords,
    and take the mean value of `UnitPrice`. You can refer to the code given in the
    following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_30.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.30: Code snippet for generating the product group based on the price
    range'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Now we will create clusters of the products. We will be using the k-means clustering
    algorithm. We will also be using the scikit-learn library to implement the K-means
    clustering algorithm. The algorithm from scikit-learn uses Euclidean distance.
    In our case, this is not the best choice. We should use Hamming distance. The
    most suitable library for that is `Kmods`, but this library is not available for
    all operating systems, so we have to use the scikit-learn library. We need to
    define the number of clusters that can represent the data perfectly. We will come
    up with the ideal number of clusters, and then we will use the silhouette score.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at how the k-means clustering algorithm works by using
    the link of this book: [https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing](https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing),
    Refer section K-means clustering form [Chapter 8](ch08.xhtml "Chapter 8. Developing
    Chatbots"), *Machine Learning for NLP problems*.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a step back and understand the silhouette score first. The silhouette
    coefficient is calculated using two things. The first is the mean intra-cluster
    distance (a) and the second is the mean nearest-cluster distance (b) for each
    sample in our dataset. So, the equation is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '*(b-a) / max (a, b)*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The *b* indicates the distance between a sample and the nearest cluster that
    the sample is not a part of. This score works if the number of labels is *2<=
    n_labels <= n_samples –1*. The best possible value for this score is 1, and worst
    value is –1\. Value 0 shows that we have overlapping clusters. Negative values
    indicate that the sample has been assigned to the wrong cluster. Refer to the
    code snippet shown in the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_31.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.31: Code snippet for choosing the ideal number of clusters using silhouette
    score'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have implemented the code using the scikit-learn API. As we can see,
    beyond five clusters, a cluster may contain very few elements, so we choose to
    categorize the products into five clusters. We will try to increase the value
    of the silhouette score. For that, we will iterate through the dataset. You can
    refer to the code shown in the following screenshot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_32.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.32: Code snippet to improvise the silhouette score'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to characterizing the content of the clusters section, which
    can help us understand how well the products have been classified into particular
    clusters.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '**Characterizing the content of clusters**'
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will analyze the properties of the product cluster. There
    will be three subsections here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette intra-cluster score analysis
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis using a word cloud
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we jump into this analysis, we need to check the number of products
    in each cluster. For that, we will be using the code snippet given in the following
    screenshot:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Characterizing the content of clusters](img/B08394_03_33.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.33: Code snippet for counting the number of products for each cluster'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the output, there are 1,009 products that belong to cluster
    number 3, whereas there are only 470 products that belong to cluster number 4\.
    We will start an in-depth analysis of these five clusters and their elements.
    First, we will start with the silhouette intra-cluster score analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '**Silhouette intra-cluster score analysis**'
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Basically, in this section, we will be checking the intra-cluster score for
    each element. We will sort the silhouette intra-cluster score. After sorting,
    we will draw a graph where the *x* *axis* represents the silhouette coefficient
    value and the *y* *axis* represents the cluster label. We generate the silhouette
    intra-cluster score for all the samples. We are building this graph because we
    want to choose an optimal value for `n_clusters` based on the silhouette intra-cluster
    score.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have generated the silhouette intra-cluster score earlier, we know `n_clusters
    = 5` is the ideal choice for us, so we will represent the clusters in a pictorial
    manner. You can refer to the function that generates graphs in the following screenshot:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_34.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.34: Code snippet of the function for silhouette intra-cluster score
    analysis'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing and calling this function, we can obtain the graph displayed
    in the following screenshot:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_35.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.35: Code snippet and graph for silhouette intra-cluster analysis'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that here, we obtain the graph for the optimal `n_cluster` value. This
    value is 5 in our case.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis using a word cloud**'
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this section, we will analyze the clusters based on the keywords. We will
    check what words each cluster has. For this analysis, we will be using the word
    cloud library. You must be wondering why we are using this type of analysis. In
    our clusters, we are expecting similar kinds of products to belong to one cluster.
    We, as humans, know the language. When we see the words for the entire cluster,
    we can easily conclude whether our clusters have similar kinds of products or
    not. We will generate graphs that are intuitive enough for us to judge the accuracy
    of clustering.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following screenshot:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_36.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.36: Code snippet for generating a word cloud'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following screenshot:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_37.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.37: Code snippet for generating word cloud graphs'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the graphs given in the following screenshot:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_38.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.38: Word cloud graphs for all five clusters'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding graphs, we can conclude the following points:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Cluster number 2 contains all the words related to gifts, such as Christmas,
    packaging, gift, cards, and so on.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster number 4 contains all the words related to luxury items and jewelry.
    So, keywords such as necklace, silver, lace, and so on are present in this cluster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some words that are present in every cluster, so it is difficult to
    clearly distinguish them.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's jump to the next section, where we will perform principal component
    analysis.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In order to check whether all the clusters have truly distinct values, we need
    to focus on their composition. As we know, the one-hot encoded matrix of the keywords
    has a large number of dimensions or a large number of variables. There may be
    a situation where because of the large number of variables, our clustering algorithm
    may over-fit the dataset. First of all, we need to reduce the number of variables,
    but we cannot reduce them randomly. We need to choose the most important variables
    that can represent most of the characteristics of the dataset. The procedure for
    reducing the number of variables logically is called dimensionality reduction.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve this, we will be using PCA, which is a statistical technique
    in which we will perform orthogonal transformation in order to convert a highly
    correlated set of data samples into a set of values that are linearly uncorrelated
    variables, and these variables are referred to as principal components. So basically,
    we will be using PCA because we want to reduce the number of variables that we
    have considered so far. PCA is a famous technique for dimensionality reduction.
    By using PCA, we can avoid the over-fitting issue.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might want to know situations in which you can use PCA, and they are
    as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: If we want to reduce the number of variables (the number of features or the
    number of dimensions) but we cannot identify which variables can be considered
    and which can't
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to ensure that our variables are independent of each other
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are comfortable making our independent variables less interpretable
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we need to reduce the number of variables. For that, we are going
    to implement the code given in the following screenshot:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_39.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.39: Code snippet for implementing PCA'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code, we are checking the amount of variance
    explained by each component. We need to consider more than 100 components to explain
    90% of the variance of our dataset.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I will consider a limited number of components because this decomposition
    is performed only to visualize the data. You can refer to the code shown in the
    following screenshot:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_40.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.40: Code snippet for generating PCA decomposition graphs'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have used PCA components using `PCA(n_components=50)`, and
    we have stored the values in dataframe `mat`, which we can use in future.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is in the form of graphs. So, you can refer
    to the following screenshot:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_41.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.41: Graphs for the PCA for each cluster'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have used `tight_layout`, which is the reason why the graphs shrank
    a bit.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have performed enough EDA to help us generate a basic insight into
    the dataset. Now we will move on to the next section, where we will start building
    customer categories or customer segmentation. We will take into account all the
    findings that we have implemented so far.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Generating customer categories
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you know, our first goal is to develop customer segmentation. From this section
    onward, we will focus mainly on how we can come up with customer segmentation.
    So far, we have done an analysis of orders, products, prices, and so on. Here,
    our main focus is on generating customer categories based on the insights that
    we got during EDA.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps that we are going to follow in order to develop the customer
    categories:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Formatting data:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping products
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the dataset
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping orders
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating customer categories:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data encoding
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating customer categories
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's see what we are going to do in each of these steps.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Formatting data
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we will be using the findings that we generated during
    EDA. In the previous section, we generated five clusters for products. In order
    to perform the rest of the analysis, we will use this already generated list of
    keywords, matrices, and clusters. By using them, we will be generating a new categorical
    variable, `categ_product`. This variable indicates the cluster of each product.
    You can refer to the code snippet shown in the following screenshot:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Formatting data](img/B08394_03_42.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.42: Code snippet for generating new categorical variable categ_product'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the new variable indicates the cluster number for each data
    entry. Now let's group the products.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Grouping products
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You might wonder that if we have already developed the categories of the product,
    then why are we performing the grouping step here. Well, here, we will perform
    grouping in such a way that we can know what amount has been spent in each product
    category. For this, we will add five new variables, for example, categ_0, categ_1,
    categ_2, categ_3, and categ_4\. You can refer to the code snippet displayed in
    the following screenshot:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping products](img/B08394_03_43.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.43: Code snippet for generating the amount spent in each product category'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Orders are split into multiple entries, so we need to use the basket price.
    This time, we will merge the basket price as well as the way it is distributed
    over five product categories. We will put all this information into the new dataframe.
    Refer to the following screenshot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping products](img/B08394_03_44.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.44: Code snippet for obtaining the distribution of basket prices for
    five clusters'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the basket price for each order, and we also know the price
    distribution over five clusters. The new dataframe is `basket_price`. Now let's
    move on to the next section.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will be using the dataframe `basket_price`, which contains
    data entries for the past 12 months. The second goal of this application is to
    predict the customer purchase behavior based on their first site visit or purchase.
    So, in order to achieve that goal right now, we will split the dataset. We will
    use 10 months'' dataset for training and 2 months'' dataset for testing. I''m
    including this step here because later on, we can use these training and testing
    datasets and you can easily get to use the new dataframe. You can refer to the
    code given in the following screenshot:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the dataset](img/B08394_03_45.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.45: Code snippet for splitting the dataset using time'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Now we will group the customers and their orders along with the basket price
    distribution.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Grouping orders
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here, we will merge the customers and their orders so that we can learn which
    customer placed how many orders. We will also generate the minimum order amount,
    the maximum order amount, and the mean order amount. Refer to the code given in
    the following screenshot:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_46.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.46: Code snippet for generating order-wise stats for each customer'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also generate two variables that indicate the number of days elapsed
    since the last purchase and the first purchase. The names of these variables are
    `FirstPurchase` and `LastPurchase`. Refer to the code snippet given in the following
    screenshot:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_47.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.47: Code snippet for generating elapsed days for the last and first
    purchase'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The customer categories in which we are interested are the ones that make only
    one order. One of our main objectives is to target these customers in such a way
    that we can retain them. We need to obtain the data for the number of customers
    that belong to this category. For that, refer to the code given in the following
    screenshot:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_48.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.48: Code snippet for generating the number of customers with one purchase'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding code, we can find out that 40% of the customer base has placed
    only one order, and we need to retain them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build the customer categories.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Creating customer categories
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we will be generating the customer segmentation here. So, we will
    work on achieving the first goal of the chapter in this section. We will build
    customer segmentation based on the customers'' purchase pattern. There are two
    steps in this section:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Data encoding
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating customer categories or customer segmentation
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start with data encoding.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Data encoding
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will be generating the dataframe that contains the summary of all operations
    we have performed so far. Each record of this dataframe is associated with a single
    client. We can use this information to characterize various types of customers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataframe that we have generated has different variables. All these variables
    have different ranges and variations. So, we need to generate a matrix where these
    data entries are standardized. You can refer to the code given in the following
    screenshot:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Data encoding](img/B08394_03_49.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.49: Code snippet for generating summary data entries for each client'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Before creating customer segmentation, we need to create the base. This base
    should include important variables. We need to include a small number of important
    variables. In order to select the important variables, we will be using principal
    component analysis. So, that we can describe the segmentation accurately. We will
    use PCA for this task. The code snippet is given in the following screenshot:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Data encoding](img/B08394_03_50.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.50: Code snippet for PCA in order to generate the customer segmentation'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that there are eight principal components. Now let's move on
    to the next section, where we will generate the customer segmentation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Generating customer categories
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will be using the k-means clustering algorithm to generate segmentation.
    The number of clusters will be derived by using the silhouette score. We have
    used the silhouette score earlier, and by using the same method, we can derive
    the number of clusters. Here, we obtain 11 clusters based on the silhouette score.
    You can refer to the following screenshot:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating customer categories](img/B08394_03_51.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.51: Code snippet for generating customer segmentations'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a large difference in the size of the segmentation,
    so we need to analyze the components of the clusters using PCA.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: PCA analysis
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will use six components here. The code snippet and graphical representation
    of PCA for 11 clusters are given in the following screenshot:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA analysis](img/B08394_03_52.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.52: Code snippet for implementing PCA and generating graphs'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'As an output, the following graphs have been generated:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA analysis](img/B08394_03_53.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.53: Graphs of PCA for customer segmentation'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: I have displayed only three graphs here. In the code, there are nine graphs.
    When you run the code, you can see them all. Note that the first component separates
    the tiniest cluster from the rest. For this dataset, we can say that there will
    always be a representation in which two segments will appear to be distinct. Now
    let's obtain silhouette scores.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the cluster using silhouette scores
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will generate the silhouette score for each cluster. This
    will indicate the quality of the separation of data samples. You can refer to
    the code snippet and graph shown in the following screenshot:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_54.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.54: Code snippet for generating graphs for silhouette scores'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding graphs, we can ensure that all the clusters are disjointed.
    Now we need to learn more about the habits of the customers of each cluster. To
    do that, we will add variables that define the cluster to which each customer
    belongs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will be generating a new dataframe, `selected_customers`. After
    generating the new dataframe, we will average the content of the dataframe. This
    will provide us with the average basket price, total visits, and so on. You can
    refer to the code shown in the following screenshot:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_55.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.55: Code snippet for storing the habits of the customers'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to reorganize the content of the dataframe. We will be considering
    two points here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: We need to reorganize the data based on the amount spent in each product category
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we will reorganize the content based on the total amount spent
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can take a look at the implementation shown in the following screenshot:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_56.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.56: Code snippet for reorganizing the dataset'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have obtained the behavior of the customer for each segment.
    Now we can recommend the items based on these characteristics. We can design the
    marketing campaign based on the generated facts.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: The particular marketing strategy can be applied to customers who belong to
    cluster 4 and cluster 8\. We should recommend the premium products to the cluster
    1 clients.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have achieved our first goal. Now it's time to aim for the second
    goal. So let's begin!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Classifying customers
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we begin, let's have a refresher on what our goal is. This helps you
    understand things in a clearer manner. The objective is that we are going to build
    a classifier that will classify the customers into different customer segments
    that were established in the previous section. We also need one more feature.
    Our classifier should generate this classification result when the customer visits
    the platform for the first time. In order to implement this kind of functionality,
    we will be using various supervised machine learning algorithms. We will use the
    scikit-learn API.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to develop the baseline classifier, we need to perform the following
    steps:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Defining the helper functions
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the data into training and testing
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Machine Learning (ML) algorithm
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining helper functions
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we define a class named `class_fit` and then we define various functions
    that can help us when we train the ML model. These are the helper functions that
    we will be using:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: The `train` function helps us train the model
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `predict` function helps us predict the result for the test dataset or the
    new data sample
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_search` function helps us find out appropriate hyperparameters and
    the value of cross-validation(CV) folds
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_fit` function helps us train the model using cross-validation and
    generate the optimal hyperparameters .
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_predict` function helps us generate prediction as well as the accuracy
    score.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet shown in the following screenshot:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining helper functions](img/B08394_03_57.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.57: Code snippet for the helper function'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the next section.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training and testing
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will be using the data that we have stored in the `selected_customers` dataframe.
    You can see some entries of the dataset on which we will apply the ML algorithm.
    Take a look at the following screenshot:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the data into training and testing](img/B08394_03_58.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.58: Sample entries in the dataset'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we will predict the cluster number for the new customer, so
    we have stored that value as `Y`, and columns such as `mean, categ_0 to categ_4`
    are used as input features for the ML model, so we have stored them in the `X`
    variable. Now we need to split this data into training and testing. For that,
    we use the sklearn API `train_test_split()`. We are using 80% of the data for
    training and 20% of data for testing. Take a look at the following screenshot:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the data into training and testing](img/B08394_03_59.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.59: Code snippet for splitting the dataset into training and testing'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: We have the training and testing datasets with us. Now, we need to start implementing
    the ML algorithm.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Machine Learning (ML) algorithm
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the baseline approach, we will be implementing the Support Vector machine
    (SVM) classifier. We will be using helper functions that we have previously defined.
    Here, I will create an instance of the class and call the methods that we have
    declared previously. Take a look at the code snippet shown in the following screenshot:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Machine Learning (ML) algorithm](img/B08394_03_60.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.60: Code snippet for training the model using the SVM classifier'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippet, `svc` is the class instance. We are using
    linear SVM. We have used `grid_search` to search optimal hyperparameters as well
    as obtain the number of CV folds. After that, we have called the `grid_fit` method,
    which is used to train the ML model using our training dataset.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: This is the way we have implemented our baseline approach. Now let's test the
    result.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the confusion matrix and the learning curve to evaluate the
    ML models. So before starting with the testing, we need to understand what the
    confusion matrix and the learning curve are. We will cover these concepts one
    by one.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we are implementing a multi-class classifier, naturally, we have multiple
    classes and the number of data entries belonging to all the classes is different,
    so during testing, we need to know whether the classifier performs equally well
    for all the classes or whether it is biased toward some classes. This analysis
    can be done using the confusion matrix. It will have a count of how many data
    entries are correctly classified and how many are misclassified.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. Say, there is a total of 10 data entries that belong
    to a class, and the label for that class is 1\. Now when we generate the prediction
    from our ML model, we will check how many data entries out of the 10 entries get
    the predicted class label 1\. Suppose six data entries are correctly classified
    and get the class label 1\. In this case, for six entries, the *predicted label*
    and *True label* is the same, so the accuracy is 60%, whereas for the remaining
    data entries, the ML model misclassifies them. The ML model predicts class labels
    other than 1.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding example, you can see that the confusion matrix gives us
    an idea about how many data entries are classified correctly and how many are
    misclassified. We can explore the class-wise accuracy of the classifier. Take
    a look at the following screenshot:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![Confusion matrix](img/B08394_03_61.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.61: Example of confusion matrix'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take a look at the learning curve.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Learning curve
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are plotting two lines here. One line indicates the training score, and
    the other line indicates the testing score. Here, the training and testing scores
    determine cross-validated training and testing scores for different training dataset
    sizes. By using this learning curve, we can monitor whether the ML model is converging
    properly or not. Both the CV score and the training score will help us determine
    whether training is going in the right direction or the ML model is suffering
    from over-fitting or under-fitting. With the increased size of dataset, if the
    CV score and training scores achieve a low score, then it means that the training
    was not performed in a proper manner. However, if the CV score and training score
    increase with the increased size of dataset, then it means that the training is
    moving in the right direction. Refer to the following screenshot:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning curve](img/B08394_03_62.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.62: Bad and good examples for the learning curve'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the basic intuition behind the testing matrix, we
    can start testing our baseline approach.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Testing the result of the baseline approach
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will test the baseline model using the following approaches:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Generating the accuracy score for the classifier
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the confusion matrix for the classifier
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the learning curve for the classifier
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the accuracy score for classifier
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will use `grid_predict` to generate the accuracy score for testing
    the dataset. We will check the accuracy of the SVM algorithm. For that, the code
    snippet is given in the following screenshot:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the accuracy score for classifier](img/B08394_03_63.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.63: Code snippet for generating the accuracy score'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: We got a 79.50% precision for the baseline approach. Now let's look at the quality
    of the prediction using the confusion matrix.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Generating the confusion matrix for the classifier
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we will generate the confusion matrix, which will give us a fair idea about
    which class is classified correctly and which classes have misclassified the data
    most of the time. To generate the confusion matrix, refer to the code given in
    the following screenshot:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_64.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.64: Code snippet for generating the confusion matrix'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used the `confusion_matrix` API for sklearn. To draw the plot, we will
    define a method with the name `plot_confusion_matrix`. With the help of the preceding
    code, we have generated the confusion matrix given in the following screenshot:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_65.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.65: Confusion matrix for the baseline approach'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the classifier was able to classify the data into class labels
    0, 2, 4, 6, and 10 accurately, whereas for class labels 1, 5, 7, and 8, the classifier
    is not performing so well.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Let's draw the learning curve for the baseline approach.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Generating the learning curve for the classifier
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A learning curve indicates whether the classifier is facing the over-fitting
    or under-fitting issue. The `plot_learning_curve` method is used to draw the learning
    curve for the classifier. You can refer to the code snippet in the following screenshot:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the learning curve for the classifier](img/B08394_03_66.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.66: Code snippet for generating the learning curve for the baseline
    approach'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curve is displayed in the following screenshot:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the learning curve for the classifier](img/B08394_03_67.jpg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.67: Learning curve for the baseline approach'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the CV curve converges at the same limit when we increase the
    sample size. This means that we have low variance and we are not suffering from
    over-fitting. Variance is the value that indicates how much our target function
    will change if we will provide different training dataset. Ideally the value of
    the target function is derived from the training dataset by Machine Learning algorithm
    however the value of estimated function should not change too much if we use another
    training dataset. Minor change (minor variance) in the estimated function is expected.
    Here, the accuracy score has a low bias, which means the model is not facing the
    under-fitting issue as well.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing the problems we are facing with the
    baseline approach so that we can optimize the current approach. The problems are
    as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: The precision score is low. There is scope for improvement.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to try other ML algorithms so that we can compare the results. Later
    on, if there is a need, then we can build the voting mechanism.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, in the revised approach, we need to try out various ML algorithms
    so that we will be sure which algorithm we can use and which ones we should not
    use.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will take all the problems into consideration and discuss
    the approach through which we will increase the accuracy of our classifier. As
    discussed in the previous section, we need to implement other ML algorithms. These
    are the six algorithms that we are going to implement with the revised approach:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbor
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaboost classifier
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting classifier
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the precision score of all the preceding algorithms, we will decide
    which algorithm can be used and which can't be used.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Without wasting time, let's start implementing the revised approach.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Building the revised approach
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement the various ML algorithms, check their precision
    score, and monitor their learning curve. There is a total of six ML algorithms
    that will be used to identify which one is the best suited for our application.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing logistic regression, K-nearest neighbor,
    decision tree, random forest, Adaboost, and gradient descent. In order to implement
    this, we will be using the helper class that we built earlier. You can take a
    look at the code snippet given in the following screenshot:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_03_68.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.68: Code snippet for performing training using various ML classifiers'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: We have already generated a precision score for all the classifiers. We can
    see random forest and gradient-boosting classifiers with great precision. However,
    we have still not checked their learning curve. First, we will check their learning
    curve and then conclude whether any classifier has been facing the over-fitting
    or under-fitting issue.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be checking the learning curves for all the classifiers.
    You can refer to the learning curves in the following screenshot:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_03_69.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.69: Learning curve for various ML classifiers'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: You can see that all the classifiers are trained appropriately. There is no
    under-fitting or over-fitting issue. With the increase data size, the scores are
    improving as well.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The major problem with this approach is that we need to decide which algorithm
    we need to use and which one we should stop using. We will discard the Adaboost
    classifier as its precision score is too low.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: There is another catch that I need to highlight here. There is no single classifier
    that works well for all class labels. There may be a classifier that works well
    for class label 0, whereas another may work well for class label 8\. I believe,
    we should not discard any other classifier. We need to come up with a voting mechanism.
    In more technical terms, we need to develop an ensemble model so that the quality
    of our prediction is great and accurate.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Now we will take a look at what our approach will be in order to build a voting
    classifier that can give us the best possible accuracy.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed, in order to improve the revised approach, we will be using a voting
    mechanism. For that, we will be using scikit-learn voting classifier APIs. First
    of all, we will use grid searching in order to generate appropriate hyperparameters
    for each classifier. After that, we will use voting-classifier APIs of scikit-learn
    and train the model. The approach is simple, so let's start implementing it.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-486
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classifier model that we will be generating in this approach should give
    us the best possible accuracy. We have already discussed this approach. If you
    are new to ensemble ML models, then let me give you a basic intuitive idea behind
    it. In layman's terms, ensemble ML models basically use a combination of various
    ML algorithms. What is the benefit of combining various ML models together? Well,
    we know there is no single classifier that can perfectly classify all the samples,
    so if we combine more than one classifier, then we can get more accuracy because
    the problem with one classifier can be overcome by another classifier. Due to
    this reason, we will use a voting classifier that is a type of ensemble classifier.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you know, we use grid search and voting classifier APIs to implement the
    best approach. As discussed, first, we will use grid search to obtain the best
    possible hyperparameters and then use the voting classifier API. The step-by-step
    implementation is given in the following screenshot:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_03_70.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.70: Code snippet for the best approach'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we get 90% precision for this approach. This time, we need to
    test the approach on our hold out corpus of two months so that we can find out
    how the voting classifier is performing on the unseen dataset.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be testing this approach.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Testing the best approach
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We test our ML model on 20% of the dataset, which we put aside before even
    starting the training. This dataset is kind of a dev dataset for us. For training,
    we have considered 10 months'' dataset. Now it is time to test the model on the
    hold out corpus. Here, our hold-out corpus consists of 2 months'' data entries.
    These are the steps that we need to implement:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the hold-out corpus in the form of the training dataset
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the transformed dataset into a matrix form
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the predictions
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's start with the first step.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the hold-out corpus in the form of the training dataset
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First of all, we need to convert the data that resides in the `set_test` dataframe
    in the form of the training dataset. For that, we will store the copy in the new
    dataframe with the name `basket_price`.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will generate the user characteristic data with the help of the same
    operation that we perform for the baseline approach. Don''t worry. When you see
    the code, you will remember the steps that we performed earlier. After transforming
    the dataset, we will store it in the dataframe, `transactions_per_user`. You can
    refer to the code snippet shown in the following screenshot:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '![Transforming the hold-out corpus in the form of the training dataset](img/B08394_03_71.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.71: Code snippet for transforming the test dataset into the same form
    of training dataset'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Now let's convert the dataset into a matrix form.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Converting the transformed dataset into a matrix form
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our classifiers take the matrix as an input, so we need to convert the transformed
    dataset into the matrix format. For that, we will use the code snippet shown in
    the following screenshot:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting the transformed dataset into a matrix form](img/B08394_03_72.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.72: Code snippet for converting the test dataset into the matrix format'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: We are using a basic type conversion here.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Generating the predictions
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be generating the precision score using voting classifiers.
    So, in order to generate the prediction for the test dataset, we need to use the
    code snippet given in the following screenshot:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the predictions](img/B08394_03_73.jpg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.73: Code snippet for generating the precision score for the test dataset'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we will achieve 76% of accuracy on our hold-out corpus. This
    is nice because we just use 10 months of data to build this model. By using 10
    months' dataset, we achieve the best possible accuracy for this domain. If we
    consider more number of datarecords, then we can still improve the results. This
    can be an exercise for you guys to consider more datasets and improvise the result.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation for various domains
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we are considering e-commerce data here, but you can consider other
    datasets of various domains. You can build customer segmentation for a company
    providing travel services, financial services, and so on. The data points will
    vary from domain to domain.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: For travel services, you could consider how frequently a user is booking flights
    or rooms using the traveling platform. Demographic and professional information
    helps a great deal, say, how many times a user uses promotional offers. The data
    for user activity is important as well.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are building a segmentation application for the financial domain, then
    you can consider the data points such as: the transaction history of the account
    holder, for example, the frequency of using a debit card or a credit card, per-month
    income, per-month expenditure, the average balance the customer is maintaining
    in their bank account(s), the type of account user have, professional information
    of the customer, and so on. There are other common data points that you can consider
    for both the domains, such as the time spent on the website or the mobile app.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Right now, I will limit myself to these two domains, but you can perform customer
    segmentation for the telecom domain, the marketing domain, the educational domain,
    the entertainment domain, and so on.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the given analytics models we have developed so far are critical for running
    a successful business. In this chapter, we developed customer segmentation based
    on the behavior of the customers. In order to do that, we used various algorithms,
    such as SVM, linear regression, decision tree, random forest, gradient boosting,
    voting-based models, and so on. By using the voting-based model, we achieved the
    best possible accuracy. Customer segmentation analysis is important for small
    and midsized organizations because these analysis help them optimize their marketing
    strategy as well as significantly improve the customer acquisition cost. I developed
    the code for the customer churn analysis, available at: [https://github.com/jalajthanaki/Customer_churn_analysis](https://github.com/jalajthanaki/Customer_churn_analysis),
    and for customer life-time value analysis at: [https://github.com/jalajthanaki/Customer_lifetime_value_analysis](https://github.com/jalajthanaki/Customer_lifetime_value_analysis)
    . You can refer to them to learn more about customer analytics. You can read about
    customer analytics at: [https://github.com/Acrotrend/Awesome-Customer-Analytics](https://github.com/Acrotrend/Awesome-Customer-Analytics).'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will build a recommendation system that is specific
    to e-commerce products. We will build a recommendation application that will recommend
    books to users based on their browsing and purchasing activities on the platform.
    We will implement various techniques to build the best possible recommendation
    engine. So keep reading!
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
