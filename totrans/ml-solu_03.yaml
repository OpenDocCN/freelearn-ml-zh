- en: Chapter 3. Customer Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customer analytics is a process in which we use the data of customer behavior
    to derive the most important business decisions using market segmentation and
    predictive analytics. Market segmentation is the process of dividing the user
    base into subgroups based on their behavior and other types of shared characteristics.
    This will help companies in providing customized products for each user segment.
    The result of this kind of analysis will lead the company to grow their business
    in an effective manner. Companies also make more profit. There are a lot of advantages.
    I know this is only a brief discussion about market segmentation, but just bear
    with me for a while. I will give you all the necessary information in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Companies can use the result generated by market segmentation and predictive
    models for direct marketing, site selection, customer acquisition, and customer
    relationship management. In short, with the help of customer analytics, the company
    can decide the most optimal and effective marketing strategy as well as growth
    strategy. The company can achieve great results with a limited amount of marking
    expenditure. Customer analytics include various methods. You can refer to the
    names of these methods in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer Analytics](img/B08394_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Variety of methods for customer analytics'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we won''t be covering all the methods given in the previous
    figure, but we will cover the methods that are most widely used in the industry.
    We will build a customer segmentation application. In this chapter, we will cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing customer segmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the baseline approach for customer segmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the result of the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the revised approach for customer segmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best approach for customer segmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer segmentation for various domains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start with customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing customer segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover customer segmentation in detail. Initially, I
    provided just a brief introduction of customer segmentation so that you could
    understand the term a bit. Here, we will understand a lot more about customer
    segmentation, which will help us further when we build the customer segmentation
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, customer segmentation is a process where we divide the
    consumer base of the company into subgroups. We need to generate the subgroups
    by using some specific characteristics so that the company sells more products
    with less marketing expenditure. Before moving forward, we need to understand
    the basics, for example, what do I mean by customer base? What do I mean by segment?
    How do we generate the consumer subgroup? What are the characteristics that we
    consider while we are segmenting the consumers? Let's answers these questions
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, the consumer base of any company consists of two types of consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: Existing consumers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Potential consumers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generally, we need to categorize our consumer base into subgroups. These subgroups
    are called segments. We need to create the groups in such a way that each subgroup
    of customers has some shared characteristics. In order to explain how to generate
    the subgroup, let me give you an example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a company is selling baby products. Then, it needs to come up with a
    consumer segment (consumer subgroup) that includes the consumers who want to buy
    the baby products. We can build the first segment (subgroup) with the help of
    a simple criterion. We will include consumers who have one baby in their family
    and bought a baby product in the last month. Now, the company launches a baby
    product that is too costly or premium. In that case, we can further divide the
    first subgroup into monthly income and socio-economic status. Based on these new
    criteria, we can generate the second subgroup of consumers. The company will target
    the consumers of the second subgroup for the costly and premium products, and
    for general products, the company will target consumers who are part of the first
    subgroup.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have different segments, we can design a customized marketing strategy
    as well as customized products that suit the customer of the particular segment.
    This segment-wise marketing will help the company sell more products with lower
    marketing expenses. Thus, the company will make more profit. This is the main
    reason why companies use customer segmentation analysis nowadays. Customer segmentation
    is used among other domain such as the retail domain, finance domain, and in customer
    relationship management (CRM)-based products. I have provided a list of the basic
    features that can be considered during the segmentation. You can refer to them
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing customer segmentation](img/B08394_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: List of basic features used in customer segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder how companies are making marketing strategies based on the customer
    segmentation analysis. The answer is companies are using the STP approach to make
    the marketing strategy firm. What is the STP approach? First of all, STP stands
    for Segmentation-Targeting-Positioning. In this approach, there are three stages.
    The points that we handle in each stage are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Segmentation**: In this stage, we create segments of our customer base using
    their profile characteristics as well as consider features provided in the preceding
    figure. Once the segmentation is firm, we move on to the next stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targeting**: In this stage, marketing teams evaluate segments and try to
    understand which kind of product is suited to which particular segment(s). The
    team performs this exercise for each segment, and finally, the team designs customized
    products that will attract the customers of one or many segments. They will also
    select which product should be offered to which segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positioning**: This is the last stage of the STP process. In this stage,
    companies study the market opportunity and what their product is offering to the
    customer. The marketing team should come up with a unique selling proposition.
    Here, the team also tries to understand how a particular segment perceives the
    products, brand, or service. This is a way for companies to determine how to best
    position their offering. The marketing and product teams of companies create a
    value proposition that clearly explains how their offering is better than any
    other competitors. Lastly, the companies start their campaign representing this
    value proposition in such a way that the consumer base will be happy about what
    they are getting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I have summarized all the preceding points in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing customer segmentation](img/B08394_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Summarization of the STP approach'
  prefs: []
  type: TYPE_NORMAL
- en: We have covered most of the basic parts of customer segmentation. Now it's time
    to move on to the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know, customer segmentation helps companies retain existing customers
    as well as acquire new potential customers. Based on the segmentation, companies
    can create customized products for a particular customer segment, but so far,
    we don't know how to generate the segments. This is the point that we will focus
    on in this chapter. You need to learn how to create customer segmentation. There
    are many domains for which we can build customer segmentation, such as e-commerce,
    travel, finance, telecom, and so on. Here, we will focus only on the e-commerce
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a detailed explanation of the problem statement, input, and output
    for the e-commerce customer segmentation application that we will be building:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem statement**: The goal of our customer segmentation application is
    to come up with a solution for the given questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we categorize the customers in a particular segment based on their buying
    patterns?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we predict which kind of items they will buy in future based on their segmentation?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input**: We will be using e-commerce data that contains the list of purchases
    in 1 year for 4,000 customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: The first goal is that we need to categorize our consumer base
    into appropriate customer segments. The second goal is we need to predict the
    purchases for the current year and the next year based on the customers'' first
    purchase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may wonder how we can achieve a prediction about the upcoming purchases
    using segmentation. Well, let me tell you how segmentation helps us! So, we don't
    know the purchase pattern of the new customer, but we know the customer profile.
    We also know which product the customer has bought. So, we can put the customer
    into one of the segments where all other customers have purchased similar items
    and share similar kinds of profile.
  prefs: []
  type: TYPE_NORMAL
- en: Let me give you an example. Say, a person has bought a Harry Potter book and
    that person lives in the UK. The age group of the customer is from 13-22\. If
    we have already generated a customer segment that satisfies these characteristics,
    then we will put this new customer in that particular subgroup. We will derive
    the list of items that the customer may buy in future. We will also offer similar
    services that other customers in the subgroup have.
  prefs: []
  type: TYPE_NORMAL
- en: The approach that we will be using in order to develop customer segmentation
    for the e-commerce domain can also be used in other domains, but data points (features)
    will differ for each domain. Later on in the chapter, we will discuss what kind
    of data points you may consider for other domains, such as travelling, finance,
    and so on. I will provide the list of data points for other domains that will
    help you build the customer segmentation application from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time to understand the dataset for building customer segmentation
    for the e-commerce domain.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding out an appropriate dataset is a challenging task in data science. Sometimes,
    you find a dataset but it is not in the appropriate format. Our problem statement
    will decide what type of dataset and data format we need. These kinds of activities
    are a part of data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data wrangling is defined as the process of transforming and mapping data from
    one data form into another. With transformation and mapping, our intention should
    be to create an appropriate and valuable dataset that can be useful in order to
    develop analytics products. Data wrangling is also referred to as data munging
    and is a crucial part of any data science application.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, e-commerce datasets are proprietary datasets, and it's rare that
    you get transactions of real users. Fortunately, *The UCI Machine Learning Repository*
    hosts a dataset named *Online Retail*. This dataset contains actual transactions
    from UK retailers.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This Online Retail dataset contains the actual transactions between December
    1, 2010 and December 9, 2011\. All the transactions are taken from the registered
    non-store online retail platform. These online retail platforms are mostly based
    in the UK. The online retail platforms are selling unique all-occasion gifts.
    Many consumers of these online retail platforms are wholesalers. There are 532610
    records in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download this dataset by using either of the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attributes of the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the attributes in this dataset. We will take a look at a short description
    for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'InvoiceNo: This data attribute indicates the invoice numbers. It is a six-digit
    integer number. The records are uniquely assigned for each transaction. If the
    invoice number starts with the letter ''c'', then it indicates a cancellation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'StockCode: This data attribute indicates the product (item) code. It is a five-digit
    integer number. All the item codes are uniquely assigned to each distinct product.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Description: This data attribute contains the description about the item.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantity: This data attribute contains the quantities for each product per
    transaction. The data is in a numeric format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'InvoiceDate: The data attribute contains the invoice date and time. It indicates
    the day and time when each transaction was generated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UnitPrice: The price indicates the product price per unit in sterling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CustomerID: This column has the customer identification number. It is a five-digit
    integer number uniquely assigned to each customer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Country: This column contains the geographic information about the customer.
    It records the country name for the customers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can refer to the sample of the dataset given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Attributes of the dataset](img/B08394_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Sample recodes from the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will start building the customer segmentation application.
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will start implementing the basic model for the customer
    segmentation application. Furthermore, we will improve this baseline approach.
    While implementing, we will cover the necessary concepts, technical aspects, and
    significance of performing that particular step. You can find the code for the
    customer-segmentation application at this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation](https://github.com/jalajthanaki/Customer_segmentation)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code related to this chapter is given in a single iPython notebook. You
    can access the notebook using this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb](https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code given on GitHub because it will help you understand things
    better. Now let's begin the implementation!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement the customer segmentation model, our implementation will
    have the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating customer categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classifying customers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's begin with data preparation!
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a basic step when you try to build any analytics application. First,
    we need to be sure that the format of the data is in an appropriate form. If it
    is not, then we need to prepare our dataset in such a way that we can build our
    application easily. In this step, we will find out whether we have a good quality
    dataset or not. We can also find out some basic facts about the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, we don't need to change the format of our e-commerce dataset, but we
    will be exploring the dataset in such a way that we can find out the quality of
    the dataset. If format of the dataset is not proper then you need to decide the
    format of the dataset in such a way that any kind of analysis can be performed
    using the dataset. You can convert the data records either in CSV format or in
    JSON format or in XML format. In addition, we can derive general facts about the
    dataset, such as whether our dataset is biased or not, whether the dataset contains
    any null values, the mapping of the customers with `Customer_ID` is proper or
    not, whether their purchases are properly recorded in dataset or not, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to load the dataset, we will use the pandas `read_csv` API. You can
    find the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the dimensions of the dataset are (541909, 8). This means that
    there are 541,909 records in the dataset and eight data attributes. We have already
    covered these eight data attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to perform exploratory data analysis (EDA), which can help us preprocess
    our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we need to check the statistical properties of the dataset
    and perform some preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing null data entries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing duplicate data entries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EDA for various data attributes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing null data entries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we need to check the data type of each of the attributes as well as
    find out which column has a null value. You can refer to the code snippet shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing null data entries](img/B08394_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Code snippet for exploring the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the code, we have generated the total number of null values
    for each data attribute. We have also generated the percentage of null values
    for each data attribute. We can observe that for the `CustomerID` column, there
    are ~25% data entries that are null. That means there is no `CustomerID` value
    available for ~25% of the dataset. This indicates that there are many entries
    that do not belong to any customer. These are abended data entries. We cannot
    map them to the existing CustomerIDs. As a result, we need to delete them. You
    can find the code snippet for deleting null data entries from the dataset in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing null data entries](img/B08394_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Deleting null data entries'
  prefs: []
  type: TYPE_NORMAL
- en: Removing duplicate data entries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After this step, we will check whether there are any duplicate data entries
    present in the dataset. In order to answer this question, we will use the pandas
    `duplicate()` function. You can refer to the code snippet shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing duplicate data entries](img/B08394_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Removing duplicate data entries'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we found 5,225 duplicate data entries. Therefore, we have removed
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's analyze each data attribute in detail.
  prefs: []
  type: TYPE_NORMAL
- en: EDA for various data attributes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EDA for each data attribute will help us get more insight into the dataset.
    Later on, we will use these facts to build an accurate customer segmentation application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start exploring data attributes in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Country
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Customer and products
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Product categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining product categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Country**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We need to find out facts such as how many countries there are in our dataset.
    In order to answer this question, we need to execute the code shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Country](img/B08394_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Code snippet for generating the number of counties present in the
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to find the country from which we receive the maximum number of
    orders. We can find that out by using the pandas `groupby()` and `count()` functions.
    We sort the number of orders in descending order. You can refer to the code snippet
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Country](img/B08394_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Code snippet for generating country-wise number of orders'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding snippet, there are a majority of orders from
    UK-based customers. Now we need to explore the customer and products variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer and products**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here, we have approximately 400,000 data items. We need to know the number
    of users and products that are present in these data entries. We will be using
    the `value_counts()` function from the `pandas` library. Take a look at the code
    snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Code for exploring customer and products'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the above screen shot that this dataset contains the records
    of 4372 users who bought 3684 different items
  prefs: []
  type: TYPE_NORMAL
- en: We have derived some interesting facts. In the given dataset, there are 4,372
    customers who have bought 3,684 different products. The total number of transactions
    is 22,190.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also find out how many products have been purchased for each transaction.
    For that, we will use the `InvoiceNo` and `InvoiceDate` data attributes, and we
    will calculate the number of products purchased for every transaction. You can
    refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Code snippet for exploring the number of products per transaction'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding code snippet, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: There are some users who have made a purchase only once on the e-commerce platform
    and bought one item. An example of this kind of user is `customerID 12346`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some users who frequently buy a large number of items per order. An
    example of this kind of user is `customerID 12347`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you look at the InvoiceNo data attribute, then you can see that there is
    the prefix `C` for one invoice. This `'C'` indicates that the particular transaction
    has been canceled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we know, there can be a couple of canceled orders present in our dataset,
    and we need to count the number of transactions corresponding to the canceled
    orders. We have used a simple check condition using the lambda expression. Now
    we will calculate the percentage of canceled orders. You can refer to the code
    snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Code snippet for generating the percentage of canceled orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list down some of the canceled order entries so that we can find out
    how to handle them. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: List of canceled orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, in order to handle the canceled orders, we will need to take the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, if the order is canceled, then there is another transaction
    that will mostly have an identical transaction except for the quantity and invoice
    date. First, we need to check whether this is true for all entries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform this checking operation by using simple logic. Mostly, the canceled
    order has a negative quantity, so we will check whether there is an order indicating
    the same quantity (but positive), with the same description values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some discount entries as well, and we need to handle them. We will
    discard the discount entries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code for this, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Code for handelling cancel orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the preceding code, we find out that there are no similar entries
    present in our dataset for all canceled transactions. In order to overcome this
    situation, we will create a new variable in our dataframe, which indicates whether
    the transaction has been canceled or not. There are three possibilities for canceled
    orders:'
  prefs: []
  type: TYPE_NORMAL
- en: There are some transactions that were canceled without counterparts. A few of
    them are probably due to the fact that the buy orders were performed before December
    2010\. We have the dataset from December 2010 to December 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some orders that were canceled with exactly one counterpart. We will
    consider them as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some entries that are doubtful. We will check whether there is at
    least one counterpart with the exact same quantity available. If available, then
    we can mark those entries as doubtful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Code snippet for generating flags for canceled orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the preceding code snippet, there are 7,521 entries that show
    the canceled orders with their counterpart. There are 1,226 entries that show
    canceled orders without their counterpart. For the sake of simplicity, we are
    going to delete all the entries related to the canceled orders. The code for deleting
    these records is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Code snippet for deleting canceled orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s analyze the entries based on the stock code because we know that
    during the identification of the canceled order, we discover discount items based
    on the *stock code D*. So first of all, we will be listing down all the stock
    codes and their meaning. You can refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Code snippet for stock code'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s focus on the pricing of the individual order. In the given dataset,
    the order from a single customer has been split into several lines. What do I
    mean by several lines? In order to understand that, refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: Understanding data entries for orders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each entry in our dataset indicates prizes for a single kind of product. If
    the order including different products is placed by a single customer, then there
    are multiple entries for that particular order. The number of data entries depends
    on how many different products that order has. As you can see in the preceding
    figure, there were three different products included in one order. We need to
    obtain the total price for each order. In order to achieve that, we will add a
    column named *TotalPrice*, which gives us the total value of the order or the
    basket price for a single order. The main logic for deriving *TotalPrice* is that
    we are multiplying *UnitPrice* with the net quantity. We obtain the net quantity
    by deducting the canceled quantity from the total quantity. Take a look at the
    code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Code for obtaining TotalPrice'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we obtain the total price, we will generate the sum for individual orders
    and then group our entries based on the invoice data. We will list only those
    data entries that have a basket price greater than 0\. The code to achieve this
    is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Code for generating the basket price based on the invoice date'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to get an idea about the distribution of the orders'' amounts
    for the given dataset. What do I mean by distribution of the orders'' amounts?
    Well, we should be aware about the prices for all the orders present in the dataset,
    and we need to put in the ranges based on the amount of all the orders. This will
    help us derive the number of orders in the dataset that are above £200\. It will
    also help us identify the number of orders that are below £100\. This kind of
    information helps us know the data distribution based on the number of orders.
    This will give us a basic picture about sales on the e-commerce platform. The
    code snippet for generating data distribution based on the orders'' amounts is
    displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.22: Code snippet for generating data distribution based on orders''
    amounts'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the pictorial representation of this data distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customer and products](img/B08394_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.23: Pictorial representation of the data distribution'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, approximately 65% of the orders are above £200\. We have explored
    orders in great detail. Now let's begin with the analysis of product categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Product categories**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will be doing an EDA of the product-related data attribute.
    We will include the following kinds of analysis in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the product description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the product categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterizing the content of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analyzing the product description**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, we will be using two data attributes. We will use the `StockCode`
    data attribute, which contains a unique ID for each product. We will also use
    the `Description` data attribute in order to group the products in different categories.
    Let's start with the product description.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the function that will take the dataframe as input, and
    then we will perform the the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: We will extract names (nouns) from the product description.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we will generate the root form of the extracted names. We will store the
    root of the name as the key and all associated names as its value. We will use
    a stemmer from the NLTK library for this step. A stemmer basically generates the
    root form of the words by removing suffixes and prefixes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will count the frequency of the roots of the names, which means we will count
    how many times the root form of each name appears.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If various names have the same root, then we consider the root form as the keyword
    tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the code for this function in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.24: Code snippet of the function for generating keywords from the
    product description'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to call this function and feed the input dataframe. You can take
    a look at the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Code snippet that actually generates keywords'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are returning three variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Keyword:` This is the list of extracted names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Keywords_roots:` This is a dictionary where the keys are the root of the name
    and values are the list of names associated with root name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Count_keywords:` This is a dictionary that keeps track of the frequency of
    each name. The count indicates the number of times a particular name appeared
    in the description. Later on, we will convert the dictionary into a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s plot the keywords versus their frequency graphs. The code is given
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the product description](img/B08394_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Code snippet for generating the frequency graph'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, the word (meaning the noun or the name)
    heart has appeared the maximum number of times in the product description. You
    might wonder what the significance of generating this word frequency is. Well,
    we are using this to categorize products. Now it's time to look into how to come
    up with product categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining product categories**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here we will obtain the product categories. We have obtained more than 1,400
    keywords, and the most frequent names have appeared in more than 200 products.
    Now we need to remove words that are less important. We can observe some useless
    words, such as names of colors and discard them. So, we will consider words that
    appear in the dataset more than 13 times. You can refer to the code snippet shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Code snippet for preserving important words'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to encode the data. Here, we have textual data and we need to convert
    it into a numerical format. For this, we will use one-hot encoding. One-hot encoding
    is a simple concept. In order to understand it, refer to the given matrix x. Take
    a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.28: Table for understanding one-hot data encoding'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a particular word is present in the product description, then the value
    of the coefficient is 1, and if the word is not present in the product description,
    then the value of the coefficient is 0\. You can refer to the the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.29: Intuitive example for one-hot data encoding'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this data encoding is a binary kind of vectorization because
    we are placing either zero or one. We will get a sparse vector for each word after
    encoding. In layman's terms, we can say that this kind of vectorization indicates
    the presence of the word in the product description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create the groups or cluster for the product based on the price
    range. For that, we will be using the keyword list that we have generated, check
    whether the product description has the words that are present in the keywords,
    and take the mean value of `UnitPrice`. You can refer to the code given in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.30: Code snippet for generating the product group based on the price
    range'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will create clusters of the products. We will be using the k-means clustering
    algorithm. We will also be using the scikit-learn library to implement the K-means
    clustering algorithm. The algorithm from scikit-learn uses Euclidean distance.
    In our case, this is not the best choice. We should use Hamming distance. The
    most suitable library for that is `Kmods`, but this library is not available for
    all operating systems, so we have to use the scikit-learn library. We need to
    define the number of clusters that can represent the data perfectly. We will come
    up with the ideal number of clusters, and then we will use the silhouette score.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at how the k-means clustering algorithm works by using
    the link of this book: [https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing](https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing),
    Refer section K-means clustering form [Chapter 8](ch08.xhtml "Chapter 8. Developing
    Chatbots"), *Machine Learning for NLP problems*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a step back and understand the silhouette score first. The silhouette
    coefficient is calculated using two things. The first is the mean intra-cluster
    distance (a) and the second is the mean nearest-cluster distance (b) for each
    sample in our dataset. So, the equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(b-a) / max (a, b)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *b* indicates the distance between a sample and the nearest cluster that
    the sample is not a part of. This score works if the number of labels is *2<=
    n_labels <= n_samples –1*. The best possible value for this score is 1, and worst
    value is –1\. Value 0 shows that we have overlapping clusters. Negative values
    indicate that the sample has been assigned to the wrong cluster. Refer to the
    code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.31: Code snippet for choosing the ideal number of clusters using silhouette
    score'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have implemented the code using the scikit-learn API. As we can see,
    beyond five clusters, a cluster may contain very few elements, so we choose to
    categorize the products into five clusters. We will try to increase the value
    of the silhouette score. For that, we will iterate through the dataset. You can
    refer to the code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining product categories](img/B08394_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.32: Code snippet to improvise the silhouette score'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to characterizing the content of the clusters section, which
    can help us understand how well the products have been classified into particular
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Characterizing the content of clusters**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will analyze the properties of the product cluster. There
    will be three subsections here:'
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette intra-cluster score analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis using a word cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we jump into this analysis, we need to check the number of products
    in each cluster. For that, we will be using the code snippet given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Characterizing the content of clusters](img/B08394_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.33: Code snippet for counting the number of products for each cluster'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the output, there are 1,009 products that belong to cluster
    number 3, whereas there are only 470 products that belong to cluster number 4\.
    We will start an in-depth analysis of these five clusters and their elements.
    First, we will start with the silhouette intra-cluster score analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Silhouette intra-cluster score analysis**'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Basically, in this section, we will be checking the intra-cluster score for
    each element. We will sort the silhouette intra-cluster score. After sorting,
    we will draw a graph where the *x* *axis* represents the silhouette coefficient
    value and the *y* *axis* represents the cluster label. We generate the silhouette
    intra-cluster score for all the samples. We are building this graph because we
    want to choose an optimal value for `n_clusters` based on the silhouette intra-cluster
    score.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have generated the silhouette intra-cluster score earlier, we know `n_clusters
    = 5` is the ideal choice for us, so we will represent the clusters in a pictorial
    manner. You can refer to the function that generates graphs in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.34: Code snippet of the function for silhouette intra-cluster score
    analysis'
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing and calling this function, we can obtain the graph displayed
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.35: Code snippet and graph for silhouette intra-cluster analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that here, we obtain the graph for the optimal `n_cluster` value. This
    value is 5 in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis using a word cloud**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this section, we will analyze the clusters based on the keywords. We will
    check what words each cluster has. For this analysis, we will be using the word
    cloud library. You must be wondering why we are using this type of analysis. In
    our clusters, we are expecting similar kinds of products to belong to one cluster.
    We, as humans, know the language. When we see the words for the entire cluster,
    we can easily conclude whether our clusters have similar kinds of products or
    not. We will generate graphs that are intuitive enough for us to judge the accuracy
    of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.36: Code snippet for generating a word cloud'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.37: Code snippet for generating word cloud graphs'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the graphs given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analysis using a word cloud](img/B08394_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.38: Word cloud graphs for all five clusters'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding graphs, we can conclude the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster number 2 contains all the words related to gifts, such as Christmas,
    packaging, gift, cards, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster number 4 contains all the words related to luxury items and jewelry.
    So, keywords such as necklace, silver, lace, and so on are present in this cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some words that are present in every cluster, so it is difficult to
    clearly distinguish them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's jump to the next section, where we will perform principal component
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In order to check whether all the clusters have truly distinct values, we need
    to focus on their composition. As we know, the one-hot encoded matrix of the keywords
    has a large number of dimensions or a large number of variables. There may be
    a situation where because of the large number of variables, our clustering algorithm
    may over-fit the dataset. First of all, we need to reduce the number of variables,
    but we cannot reduce them randomly. We need to choose the most important variables
    that can represent most of the characteristics of the dataset. The procedure for
    reducing the number of variables logically is called dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve this, we will be using PCA, which is a statistical technique
    in which we will perform orthogonal transformation in order to convert a highly
    correlated set of data samples into a set of values that are linearly uncorrelated
    variables, and these variables are referred to as principal components. So basically,
    we will be using PCA because we want to reduce the number of variables that we
    have considered so far. PCA is a famous technique for dimensionality reduction.
    By using PCA, we can avoid the over-fitting issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might want to know situations in which you can use PCA, and they are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to reduce the number of variables (the number of features or the
    number of dimensions) but we cannot identify which variables can be considered
    and which can't
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to ensure that our variables are independent of each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are comfortable making our independent variables less interpretable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we need to reduce the number of variables. For that, we are going
    to implement the code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.39: Code snippet for implementing PCA'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code, we are checking the amount of variance
    explained by each component. We need to consider more than 100 components to explain
    90% of the variance of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I will consider a limited number of components because this decomposition
    is performed only to visualize the data. You can refer to the code shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.40: Code snippet for generating PCA decomposition graphs'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have used PCA components using `PCA(n_components=50)`, and
    we have stored the values in dataframe `mat`, which we can use in future.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is in the form of graphs. So, you can refer
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis (PCA)](img/B08394_03_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.41: Graphs for the PCA for each cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have used `tight_layout`, which is the reason why the graphs shrank
    a bit.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have performed enough EDA to help us generate a basic insight into
    the dataset. Now we will move on to the next section, where we will start building
    customer categories or customer segmentation. We will take into account all the
    findings that we have implemented so far.
  prefs: []
  type: TYPE_NORMAL
- en: Generating customer categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you know, our first goal is to develop customer segmentation. From this section
    onward, we will focus mainly on how we can come up with customer segmentation.
    So far, we have done an analysis of orders, products, prices, and so on. Here,
    our main focus is on generating customer categories based on the insights that
    we got during EDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps that we are going to follow in order to develop the customer
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formatting data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping products
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping orders
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating customer categories:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data encoding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating customer categories
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's see what we are going to do in each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we will be using the findings that we generated during
    EDA. In the previous section, we generated five clusters for products. In order
    to perform the rest of the analysis, we will use this already generated list of
    keywords, matrices, and clusters. By using them, we will be generating a new categorical
    variable, `categ_product`. This variable indicates the cluster of each product.
    You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formatting data](img/B08394_03_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.42: Code snippet for generating new categorical variable categ_product'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the new variable indicates the cluster number for each data
    entry. Now let's group the products.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping products
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You might wonder that if we have already developed the categories of the product,
    then why are we performing the grouping step here. Well, here, we will perform
    grouping in such a way that we can know what amount has been spent in each product
    category. For this, we will add five new variables, for example, categ_0, categ_1,
    categ_2, categ_3, and categ_4\. You can refer to the code snippet displayed in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping products](img/B08394_03_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.43: Code snippet for generating the amount spent in each product category'
  prefs: []
  type: TYPE_NORMAL
- en: 'Orders are split into multiple entries, so we need to use the basket price.
    This time, we will merge the basket price as well as the way it is distributed
    over five product categories. We will put all this information into the new dataframe.
    Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping products](img/B08394_03_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.44: Code snippet for obtaining the distribution of basket prices for
    five clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the basket price for each order, and we also know the price
    distribution over five clusters. The new dataframe is `basket_price`. Now let's
    move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will be using the dataframe `basket_price`, which contains
    data entries for the past 12 months. The second goal of this application is to
    predict the customer purchase behavior based on their first site visit or purchase.
    So, in order to achieve that goal right now, we will split the dataset. We will
    use 10 months'' dataset for training and 2 months'' dataset for testing. I''m
    including this step here because later on, we can use these training and testing
    datasets and you can easily get to use the new dataframe. You can refer to the
    code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the dataset](img/B08394_03_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.45: Code snippet for splitting the dataset using time'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will group the customers and their orders along with the basket price
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping orders
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here, we will merge the customers and their orders so that we can learn which
    customer placed how many orders. We will also generate the minimum order amount,
    the maximum order amount, and the mean order amount. Refer to the code given in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.46: Code snippet for generating order-wise stats for each customer'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also generate two variables that indicate the number of days elapsed
    since the last purchase and the first purchase. The names of these variables are
    `FirstPurchase` and `LastPurchase`. Refer to the code snippet given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.47: Code snippet for generating elapsed days for the last and first
    purchase'
  prefs: []
  type: TYPE_NORMAL
- en: 'The customer categories in which we are interested are the ones that make only
    one order. One of our main objectives is to target these customers in such a way
    that we can retain them. We need to obtain the data for the number of customers
    that belong to this category. For that, refer to the code given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping orders](img/B08394_03_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.48: Code snippet for generating the number of customers with one purchase'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding code, we can find out that 40% of the customer base has placed
    only one order, and we need to retain them.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build the customer categories.
  prefs: []
  type: TYPE_NORMAL
- en: Creating customer categories
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we will be generating the customer segmentation here. So, we will
    work on achieving the first goal of the chapter in this section. We will build
    customer segmentation based on the customers'' purchase pattern. There are two
    steps in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Data encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating customer categories or customer segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start with data encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Data encoding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will be generating the dataframe that contains the summary of all operations
    we have performed so far. Each record of this dataframe is associated with a single
    client. We can use this information to characterize various types of customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataframe that we have generated has different variables. All these variables
    have different ranges and variations. So, we need to generate a matrix where these
    data entries are standardized. You can refer to the code given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data encoding](img/B08394_03_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.49: Code snippet for generating summary data entries for each client'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before creating customer segmentation, we need to create the base. This base
    should include important variables. We need to include a small number of important
    variables. In order to select the important variables, we will be using principal
    component analysis. So, that we can describe the segmentation accurately. We will
    use PCA for this task. The code snippet is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data encoding](img/B08394_03_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.50: Code snippet for PCA in order to generate the customer segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that there are eight principal components. Now let's move on
    to the next section, where we will generate the customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Generating customer categories
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will be using the k-means clustering algorithm to generate segmentation.
    The number of clusters will be derived by using the silhouette score. We have
    used the silhouette score earlier, and by using the same method, we can derive
    the number of clusters. Here, we obtain 11 clusters based on the silhouette score.
    You can refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating customer categories](img/B08394_03_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.51: Code snippet for generating customer segmentations'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a large difference in the size of the segmentation,
    so we need to analyze the components of the clusters using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: PCA analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will use six components here. The code snippet and graphical representation
    of PCA for 11 clusters are given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA analysis](img/B08394_03_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.52: Code snippet for implementing PCA and generating graphs'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an output, the following graphs have been generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA analysis](img/B08394_03_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.53: Graphs of PCA for customer segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: I have displayed only three graphs here. In the code, there are nine graphs.
    When you run the code, you can see them all. Note that the first component separates
    the tiniest cluster from the rest. For this dataset, we can say that there will
    always be a representation in which two segments will appear to be distinct. Now
    let's obtain silhouette scores.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the cluster using silhouette scores
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we will generate the silhouette score for each cluster. This
    will indicate the quality of the separation of data samples. You can refer to
    the code snippet and graph shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.54: Code snippet for generating graphs for silhouette scores'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding graphs, we can ensure that all the clusters are disjointed.
    Now we need to learn more about the habits of the customers of each cluster. To
    do that, we will add variables that define the cluster to which each customer
    belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will be generating a new dataframe, `selected_customers`. After
    generating the new dataframe, we will average the content of the dataframe. This
    will provide us with the average basket price, total visits, and so on. You can
    refer to the code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.55: Code snippet for storing the habits of the customers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to reorganize the content of the dataframe. We will be considering
    two points here:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to reorganize the data based on the amount spent in each product category
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we will reorganize the content based on the total amount spent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can take a look at the implementation shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.56: Code snippet for reorganizing the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have obtained the behavior of the customer for each segment.
    Now we can recommend the items based on these characteristics. We can design the
    marketing campaign based on the generated facts.
  prefs: []
  type: TYPE_NORMAL
- en: The particular marketing strategy can be applied to customers who belong to
    cluster 4 and cluster 8\. We should recommend the premium products to the cluster
    1 clients.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have achieved our first goal. Now it's time to aim for the second
    goal. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Classifying customers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we begin, let's have a refresher on what our goal is. This helps you
    understand things in a clearer manner. The objective is that we are going to build
    a classifier that will classify the customers into different customer segments
    that were established in the previous section. We also need one more feature.
    Our classifier should generate this classification result when the customer visits
    the platform for the first time. In order to implement this kind of functionality,
    we will be using various supervised machine learning algorithms. We will use the
    scikit-learn API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to develop the baseline classifier, we need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the helper functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the data into training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Machine Learning (ML) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining helper functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we define a class named `class_fit` and then we define various functions
    that can help us when we train the ML model. These are the helper functions that
    we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: The `train` function helps us train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `predict` function helps us predict the result for the test dataset or the
    new data sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_search` function helps us find out appropriate hyperparameters and
    the value of cross-validation(CV) folds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_fit` function helps us train the model using cross-validation and
    generate the optimal hyperparameters .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `grid_predict` function helps us generate prediction as well as the accuracy
    score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining helper functions](img/B08394_03_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.57: Code snippet for the helper function'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training and testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will be using the data that we have stored in the `selected_customers` dataframe.
    You can see some entries of the dataset on which we will apply the ML algorithm.
    Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the data into training and testing](img/B08394_03_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.58: Sample entries in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we will predict the cluster number for the new customer, so
    we have stored that value as `Y`, and columns such as `mean, categ_0 to categ_4`
    are used as input features for the ML model, so we have stored them in the `X`
    variable. Now we need to split this data into training and testing. For that,
    we use the sklearn API `train_test_split()`. We are using 80% of the data for
    training and 20% of data for testing. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the data into training and testing](img/B08394_03_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.59: Code snippet for splitting the dataset into training and testing'
  prefs: []
  type: TYPE_NORMAL
- en: We have the training and testing datasets with us. Now, we need to start implementing
    the ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Machine Learning (ML) algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the baseline approach, we will be implementing the Support Vector machine
    (SVM) classifier. We will be using helper functions that we have previously defined.
    Here, I will create an instance of the class and call the methods that we have
    declared previously. Take a look at the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Machine Learning (ML) algorithm](img/B08394_03_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.60: Code snippet for training the model using the SVM classifier'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippet, `svc` is the class instance. We are using
    linear SVM. We have used `grid_search` to search optimal hyperparameters as well
    as obtain the number of CV folds. After that, we have called the `grid_fit` method,
    which is used to train the ML model using our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This is the way we have implemented our baseline approach. Now let's test the
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the confusion matrix and the learning curve to evaluate the
    ML models. So before starting with the testing, we need to understand what the
    confusion matrix and the learning curve are. We will cover these concepts one
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we are implementing a multi-class classifier, naturally, we have multiple
    classes and the number of data entries belonging to all the classes is different,
    so during testing, we need to know whether the classifier performs equally well
    for all the classes or whether it is biased toward some classes. This analysis
    can be done using the confusion matrix. It will have a count of how many data
    entries are correctly classified and how many are misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. Say, there is a total of 10 data entries that belong
    to a class, and the label for that class is 1\. Now when we generate the prediction
    from our ML model, we will check how many data entries out of the 10 entries get
    the predicted class label 1\. Suppose six data entries are correctly classified
    and get the class label 1\. In this case, for six entries, the *predicted label*
    and *True label* is the same, so the accuracy is 60%, whereas for the remaining
    data entries, the ML model misclassifies them. The ML model predicts class labels
    other than 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding example, you can see that the confusion matrix gives us
    an idea about how many data entries are classified correctly and how many are
    misclassified. We can explore the class-wise accuracy of the classifier. Take
    a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Confusion matrix](img/B08394_03_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.61: Example of confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take a look at the learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are plotting two lines here. One line indicates the training score, and
    the other line indicates the testing score. Here, the training and testing scores
    determine cross-validated training and testing scores for different training dataset
    sizes. By using this learning curve, we can monitor whether the ML model is converging
    properly or not. Both the CV score and the training score will help us determine
    whether training is going in the right direction or the ML model is suffering
    from over-fitting or under-fitting. With the increased size of dataset, if the
    CV score and training scores achieve a low score, then it means that the training
    was not performed in a proper manner. However, if the CV score and training score
    increase with the increased size of dataset, then it means that the training is
    moving in the right direction. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning curve](img/B08394_03_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.62: Bad and good examples for the learning curve'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the basic intuition behind the testing matrix, we
    can start testing our baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the result of the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will test the baseline model using the following approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating the accuracy score for the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the confusion matrix for the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the learning curve for the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the accuracy score for classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will use `grid_predict` to generate the accuracy score for testing
    the dataset. We will check the accuracy of the SVM algorithm. For that, the code
    snippet is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the accuracy score for classifier](img/B08394_03_63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.63: Code snippet for generating the accuracy score'
  prefs: []
  type: TYPE_NORMAL
- en: We got a 79.50% precision for the baseline approach. Now let's look at the quality
    of the prediction using the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the confusion matrix for the classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we will generate the confusion matrix, which will give us a fair idea about
    which class is classified correctly and which classes have misclassified the data
    most of the time. To generate the confusion matrix, refer to the code given in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.64: Code snippet for generating the confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used the `confusion_matrix` API for sklearn. To draw the plot, we will
    define a method with the name `plot_confusion_matrix`. With the help of the preceding
    code, we have generated the confusion matrix given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_65.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.65: Confusion matrix for the baseline approach'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the classifier was able to classify the data into class labels
    0, 2, 4, 6, and 10 accurately, whereas for class labels 1, 5, 7, and 8, the classifier
    is not performing so well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's draw the learning curve for the baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the learning curve for the classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A learning curve indicates whether the classifier is facing the over-fitting
    or under-fitting issue. The `plot_learning_curve` method is used to draw the learning
    curve for the classifier. You can refer to the code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the learning curve for the classifier](img/B08394_03_66.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.66: Code snippet for generating the learning curve for the baseline
    approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curve is displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the learning curve for the classifier](img/B08394_03_67.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.67: Learning curve for the baseline approach'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the CV curve converges at the same limit when we increase the
    sample size. This means that we have low variance and we are not suffering from
    over-fitting. Variance is the value that indicates how much our target function
    will change if we will provide different training dataset. Ideally the value of
    the target function is derived from the training dataset by Machine Learning algorithm
    however the value of estimated function should not change too much if we use another
    training dataset. Minor change (minor variance) in the estimated function is expected.
    Here, the accuracy score has a low bias, which means the model is not facing the
    under-fitting issue as well.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing the problems we are facing with the
    baseline approach so that we can optimize the current approach. The problems are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The precision score is low. There is scope for improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to try other ML algorithms so that we can compare the results. Later
    on, if there is a need, then we can build the voting mechanism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, in the revised approach, we need to try out various ML algorithms
    so that we will be sure which algorithm we can use and which ones we should not
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will take all the problems into consideration and discuss
    the approach through which we will increase the accuracy of our classifier. As
    discussed in the previous section, we need to implement other ML algorithms. These
    are the six algorithms that we are going to implement with the revised approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaboost classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the precision score of all the preceding algorithms, we will decide
    which algorithm can be used and which can't be used.
  prefs: []
  type: TYPE_NORMAL
- en: Without wasting time, let's start implementing the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Building the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement the various ML algorithms, check their precision
    score, and monitor their learning curve. There is a total of six ML algorithms
    that will be used to identify which one is the best suited for our application.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing logistic regression, K-nearest neighbor,
    decision tree, random forest, Adaboost, and gradient descent. In order to implement
    this, we will be using the helper class that we built earlier. You can take a
    look at the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_03_68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.68: Code snippet for performing training using various ML classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: We have already generated a precision score for all the classifiers. We can
    see random forest and gradient-boosting classifiers with great precision. However,
    we have still not checked their learning curve. First, we will check their learning
    curve and then conclude whether any classifier has been facing the over-fitting
    or under-fitting issue.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be checking the learning curves for all the classifiers.
    You can refer to the learning curves in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_03_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.69: Learning curve for various ML classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that all the classifiers are trained appropriately. There is no
    under-fitting or over-fitting issue. With the increase data size, the scores are
    improving as well.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The major problem with this approach is that we need to decide which algorithm
    we need to use and which one we should stop using. We will discard the Adaboost
    classifier as its precision score is too low.
  prefs: []
  type: TYPE_NORMAL
- en: There is another catch that I need to highlight here. There is no single classifier
    that works well for all class labels. There may be a classifier that works well
    for class label 0, whereas another may work well for class label 8\. I believe,
    we should not discard any other classifier. We need to come up with a voting mechanism.
    In more technical terms, we need to develop an ensemble model so that the quality
    of our prediction is great and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will take a look at what our approach will be in order to build a voting
    classifier that can give us the best possible accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed, in order to improve the revised approach, we will be using a voting
    mechanism. For that, we will be using scikit-learn voting classifier APIs. First
    of all, we will use grid searching in order to generate appropriate hyperparameters
    for each classifier. After that, we will use voting-classifier APIs of scikit-learn
    and train the model. The approach is simple, so let's start implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classifier model that we will be generating in this approach should give
    us the best possible accuracy. We have already discussed this approach. If you
    are new to ensemble ML models, then let me give you a basic intuitive idea behind
    it. In layman's terms, ensemble ML models basically use a combination of various
    ML algorithms. What is the benefit of combining various ML models together? Well,
    we know there is no single classifier that can perfectly classify all the samples,
    so if we combine more than one classifier, then we can get more accuracy because
    the problem with one classifier can be overcome by another classifier. Due to
    this reason, we will use a voting classifier that is a type of ensemble classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you know, we use grid search and voting classifier APIs to implement the
    best approach. As discussed, first, we will use grid search to obtain the best
    possible hyperparameters and then use the voting classifier API. The step-by-step
    implementation is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_03_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.70: Code snippet for the best approach'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we get 90% precision for this approach. This time, we need to
    test the approach on our hold out corpus of two months so that we can find out
    how the voting classifier is performing on the unseen dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be testing this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We test our ML model on 20% of the dataset, which we put aside before even
    starting the training. This dataset is kind of a dev dataset for us. For training,
    we have considered 10 months'' dataset. Now it is time to test the model on the
    hold out corpus. Here, our hold-out corpus consists of 2 months'' data entries.
    These are the steps that we need to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the hold-out corpus in the form of the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the transformed dataset into a matrix form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's start with the first step.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the hold-out corpus in the form of the training dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First of all, we need to convert the data that resides in the `set_test` dataframe
    in the form of the training dataset. For that, we will store the copy in the new
    dataframe with the name `basket_price`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will generate the user characteristic data with the help of the same
    operation that we perform for the baseline approach. Don''t worry. When you see
    the code, you will remember the steps that we performed earlier. After transforming
    the dataset, we will store it in the dataframe, `transactions_per_user`. You can
    refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transforming the hold-out corpus in the form of the training dataset](img/B08394_03_71.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.71: Code snippet for transforming the test dataset into the same form
    of training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's convert the dataset into a matrix form.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the transformed dataset into a matrix form
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our classifiers take the matrix as an input, so we need to convert the transformed
    dataset into the matrix format. For that, we will use the code snippet shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting the transformed dataset into a matrix form](img/B08394_03_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.72: Code snippet for converting the test dataset into the matrix format'
  prefs: []
  type: TYPE_NORMAL
- en: We are using a basic type conversion here.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be generating the precision score using voting classifiers.
    So, in order to generate the prediction for the test dataset, we need to use the
    code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the predictions](img/B08394_03_73.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.73: Code snippet for generating the precision score for the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we will achieve 76% of accuracy on our hold-out corpus. This
    is nice because we just use 10 months of data to build this model. By using 10
    months' dataset, we achieve the best possible accuracy for this domain. If we
    consider more number of datarecords, then we can still improve the results. This
    can be an exercise for you guys to consider more datasets and improvise the result.
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation for various domains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we are considering e-commerce data here, but you can consider other
    datasets of various domains. You can build customer segmentation for a company
    providing travel services, financial services, and so on. The data points will
    vary from domain to domain.
  prefs: []
  type: TYPE_NORMAL
- en: For travel services, you could consider how frequently a user is booking flights
    or rooms using the traveling platform. Demographic and professional information
    helps a great deal, say, how many times a user uses promotional offers. The data
    for user activity is important as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are building a segmentation application for the financial domain, then
    you can consider the data points such as: the transaction history of the account
    holder, for example, the frequency of using a debit card or a credit card, per-month
    income, per-month expenditure, the average balance the customer is maintaining
    in their bank account(s), the type of account user have, professional information
    of the customer, and so on. There are other common data points that you can consider
    for both the domains, such as the time spent on the website or the mobile app.'
  prefs: []
  type: TYPE_NORMAL
- en: Right now, I will limit myself to these two domains, but you can perform customer
    segmentation for the telecom domain, the marketing domain, the educational domain,
    the entertainment domain, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the given analytics models we have developed so far are critical for running
    a successful business. In this chapter, we developed customer segmentation based
    on the behavior of the customers. In order to do that, we used various algorithms,
    such as SVM, linear regression, decision tree, random forest, gradient boosting,
    voting-based models, and so on. By using the voting-based model, we achieved the
    best possible accuracy. Customer segmentation analysis is important for small
    and midsized organizations because these analysis help them optimize their marketing
    strategy as well as significantly improve the customer acquisition cost. I developed
    the code for the customer churn analysis, available at: [https://github.com/jalajthanaki/Customer_churn_analysis](https://github.com/jalajthanaki/Customer_churn_analysis),
    and for customer life-time value analysis at: [https://github.com/jalajthanaki/Customer_lifetime_value_analysis](https://github.com/jalajthanaki/Customer_lifetime_value_analysis)
    . You can refer to them to learn more about customer analytics. You can read about
    customer analytics at: [https://github.com/Acrotrend/Awesome-Customer-Analytics](https://github.com/Acrotrend/Awesome-Customer-Analytics).'
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will build a recommendation system that is specific
    to e-commerce products. We will build a recommendation application that will recommend
    books to users based on their browsing and purchasing activities on the platform.
    We will implement various techniques to build the best possible recommendation
    engine. So keep reading!
  prefs: []
  type: TYPE_NORMAL
