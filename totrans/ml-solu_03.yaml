- en: Chapter 3. Customer Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 客户分析
- en: Customer analytics is a process in which we use the data of customer behavior
    to derive the most important business decisions using market segmentation and
    predictive analytics. Market segmentation is the process of dividing the user
    base into subgroups based on their behavior and other types of shared characteristics.
    This will help companies in providing customized products for each user segment.
    The result of this kind of analysis will lead the company to grow their business
    in an effective manner. Companies also make more profit. There are a lot of advantages.
    I know this is only a brief discussion about market segmentation, but just bear
    with me for a while. I will give you all the necessary information in the upcoming
    sections.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 客户分析是一个过程，我们使用客户行为数据，通过市场细分和预测分析来推导出最重要的商业决策。市场细分是根据用户的行为和其他类型的共享特征将用户基础划分为子组的过程。这将帮助公司为每个用户细分提供定制化产品。此类分析的结果将引导公司以有效的方式扩展业务。公司也能获得更多的利润。有很多优势。我知道这只是一个关于市场细分简短的讨论，但请稍等片刻。我将在接下来的部分中为您提供所有必要的信息。
- en: 'Companies can use the result generated by market segmentation and predictive
    models for direct marketing, site selection, customer acquisition, and customer
    relationship management. In short, with the help of customer analytics, the company
    can decide the most optimal and effective marketing strategy as well as growth
    strategy. The company can achieve great results with a limited amount of marking
    expenditure. Customer analytics include various methods. You can refer to the
    names of these methods in the following diagram:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 公司可以使用市场细分和预测模型生成的结果进行直接营销、选址、客户获取和客户关系管理。简而言之，借助客户分析，公司可以决定最优化和有效的营销策略以及增长策略。公司可以在有限的营销支出下取得显著成果。客户分析包括各种方法。您可以在以下图表中查看这些方法的名称：
- en: '![Customer Analytics](img/B08394_03_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![客户分析](img/B08394_03_01.jpg)'
- en: 'Figure 3.1: Variety of methods for customer analytics'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：客户分析的各种方法
- en: 'In this chapter, we won''t be covering all the methods given in the previous
    figure, but we will cover the methods that are most widely used in the industry.
    We will build a customer segmentation application. In this chapter, we will cover
    the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会涵盖前图中给出的所有方法，但我们将介绍在行业中应用最广泛的方法。我们将构建一个客户细分应用。在本章中，我们将涵盖以下主题：
- en: 'Introducing customer segmentation:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍客户细分：
- en: Introducing the problem statement
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'Building the baseline approach for customer segmentation:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建客户细分基线方法：
- en: Implementing the baseline approach
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施基线方法
- en: Understanding the testing matrix
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the result of the baseline approach
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线方法的结果
- en: Problems with the baseline approach
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线方法的局限性
- en: Optimizing the baseline approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化基线方法
- en: 'Building the revised approach for customer segmentation:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建客户细分的修订方法：
- en: Implementing the revised approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施修订方法
- en: Testing the revised approach
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试修订方法
- en: Problems with the revised approach
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修订方法的局限性
- en: Understanding how to improve the revised approach
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何改进修订方法
- en: 'The best approach for customer segmentation:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分最佳方法：
- en: Implementing the best approach
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Testing the best approach
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试最佳方法
- en: Customer segmentation for various domains
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同领域的客户细分
- en: Summary
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: We will start with customer segmentation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从客户细分开始。
- en: Introducing customer segmentation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍客户细分
- en: In this section, we will cover customer segmentation in detail. Initially, I
    provided just a brief introduction of customer segmentation so that you could
    understand the term a bit. Here, we will understand a lot more about customer
    segmentation, which will help us further when we build the customer segmentation
    analysis.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细介绍客户细分。最初，我仅提供了一个关于客户细分的基本介绍，以便您对这一术语有所了解。在这里，我们将更深入地了解客户细分，这将有助于我们在构建客户细分分析时进一步工作。
- en: As mentioned earlier, customer segmentation is a process where we divide the
    consumer base of the company into subgroups. We need to generate the subgroups
    by using some specific characteristics so that the company sells more products
    with less marketing expenditure. Before moving forward, we need to understand
    the basics, for example, what do I mean by customer base? What do I mean by segment?
    How do we generate the consumer subgroup? What are the characteristics that we
    consider while we are segmenting the consumers? Let's answers these questions
    one by one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，客户细分是一个过程，我们将公司的消费者基础划分为子群体。我们需要通过使用一些特定的特征来生成子群体，以便公司以更少的营销支出销售更多产品。在继续前进之前，我们需要了解基础知识，例如，我所说的消费者基础是什么？我所说的细分是什么？我们是如何生成消费者子群体的？我们在细分消费者时考虑哪些特征？让我们逐一回答这些问题。
- en: 'Basically, the consumer base of any company consists of two types of consumers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，任何公司的消费者基础由两种类型的消费者组成：
- en: Existing consumers
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现有消费者
- en: Potential consumers
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在消费者
- en: Generally, we need to categorize our consumer base into subgroups. These subgroups
    are called segments. We need to create the groups in such a way that each subgroup
    of customers has some shared characteristics. In order to explain how to generate
    the subgroup, let me give you an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要将我们的消费者基础划分为子群体。这些子群体被称为细分市场。我们需要以这种方式创建群体，使得每个客户子群体都有一些共同的特征。为了解释如何生成子群体，让我给你举一个例子。
- en: Suppose a company is selling baby products. Then, it needs to come up with a
    consumer segment (consumer subgroup) that includes the consumers who want to buy
    the baby products. We can build the first segment (subgroup) with the help of
    a simple criterion. We will include consumers who have one baby in their family
    and bought a baby product in the last month. Now, the company launches a baby
    product that is too costly or premium. In that case, we can further divide the
    first subgroup into monthly income and socio-economic status. Based on these new
    criteria, we can generate the second subgroup of consumers. The company will target
    the consumers of the second subgroup for the costly and premium products, and
    for general products, the company will target consumers who are part of the first
    subgroup.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家公司正在销售婴儿产品。那么，它需要提出一个消费者细分市场（消费者子群体），包括想要购买婴儿产品的消费者。我们可以借助一个简单的标准来构建第一个细分市场（子群体）。我们将包括那些家庭中有一个孩子并在过去一个月购买了婴儿产品的消费者。现在，公司推出了一款成本过高或高端的婴儿产品。在这种情况下，我们可以进一步将第一个子群体细分为月收入和社会经济状况。基于这些新的标准，我们可以生成第二个消费者子群体。公司将为第二个子群体的消费者定位高端和高端产品，而对于一般产品，公司将针对属于第一个子群体的消费者。
- en: 'When we have different segments, we can design a customized marketing strategy
    as well as customized products that suit the customer of the particular segment.
    This segment-wise marketing will help the company sell more products with lower
    marketing expenses. Thus, the company will make more profit. This is the main
    reason why companies use customer segmentation analysis nowadays. Customer segmentation
    is used among other domain such as the retail domain, finance domain, and in customer
    relationship management (CRM)-based products. I have provided a list of the basic
    features that can be considered during the segmentation. You can refer to them
    in the following screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有不同的细分市场时，我们可以设计定制化的营销策略以及适合特定细分市场客户的定制产品。这种按细分市场进行的营销将帮助公司以更低的营销费用销售更多产品。因此，公司会获得更多利润。这就是为什么公司现在使用客户细分分析的主要原因。客户细分在零售领域、金融领域以及基于客户关系管理（CRM）的产品等领域也被使用。我已经提供了一份在细分过程中可以考虑的基本特征列表。您可以在以下屏幕截图中参考它们：
- en: '![Introducing customer segmentation](img/B08394_03_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![介绍客户细分](img/B08394_03_02.jpg)'
- en: 'Figure 3.2: List of basic features used in customer segmentation'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：客户细分中使用的基本特征列表
- en: 'You may wonder how companies are making marketing strategies based on the customer
    segmentation analysis. The answer is companies are using the STP approach to make
    the marketing strategy firm. What is the STP approach? First of all, STP stands
    for Segmentation-Targeting-Positioning. In this approach, there are three stages.
    The points that we handle in each stage are explained as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道公司是如何基于客户细分分析制定营销策略的。答案是公司正在使用STP方法来制定营销策略。什么是STP方法？首先，STP代表细分-定位-定位。在这个方法中，有三个阶段。我们在每个阶段处理的问题如下所述：
- en: '**Segmentation**: In this stage, we create segments of our customer base using
    their profile characteristics as well as consider features provided in the preceding
    figure. Once the segmentation is firm, we move on to the next stage.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细分**：在这个阶段，我们使用客户的个人特征以及前图中提供的特征来创建客户基础细分。一旦细分确定，我们就进入下一阶段。'
- en: '**Targeting**: In this stage, marketing teams evaluate segments and try to
    understand which kind of product is suited to which particular segment(s). The
    team performs this exercise for each segment, and finally, the team designs customized
    products that will attract the customers of one or many segments. They will also
    select which product should be offered to which segment.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：在这个阶段，营销团队评估细分，并试图了解哪种产品适合哪个特定的细分市场（s）。团队对每个细分进行这项练习，最后，团队设计出吸引一个或多个细分市场客户的定制产品。他们还将选择哪些产品应该提供给哪个细分市场。'
- en: '**Positioning**: This is the last stage of the STP process. In this stage,
    companies study the market opportunity and what their product is offering to the
    customer. The marketing team should come up with a unique selling proposition.
    Here, the team also tries to understand how a particular segment perceives the
    products, brand, or service. This is a way for companies to determine how to best
    position their offering. The marketing and product teams of companies create a
    value proposition that clearly explains how their offering is better than any
    other competitors. Lastly, the companies start their campaign representing this
    value proposition in such a way that the consumer base will be happy about what
    they are getting.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定位**：这是STP过程的最后阶段。在这个阶段，公司研究市场机会以及他们的产品向客户提供的价值。营销团队应该提出一个独特的销售主张。在这里，团队还试图了解特定细分市场如何看待产品、品牌或服务。这是公司确定如何最好地定位其产品的一种方式。公司的营销和产品团队创建一个价值主张，清楚地解释他们的产品如何优于任何其他竞争对手。最后，公司开始他们的宣传活动，以这种方式展示价值主张，让消费者对他们所得到的东西感到满意。'
- en: 'I have summarized all the preceding points in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在以下图中总结了所有前面的要点：
- en: '![Introducing customer segmentation](img/B08394_03_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![介绍客户细分](img/B08394_03_03.jpg)'
- en: 'Figure 3.3: Summarization of the STP approach'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：STP方法的总结
- en: We have covered most of the basic parts of customer segmentation. Now it's time
    to move on to the problem statement.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了客户细分的大部分基本部分。现在，是时候进入问题陈述了。
- en: Introducing the problem statement
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: As you know, customer segmentation helps companies retain existing customers
    as well as acquire new potential customers. Based on the segmentation, companies
    can create customized products for a particular customer segment, but so far,
    we don't know how to generate the segments. This is the point that we will focus
    on in this chapter. You need to learn how to create customer segmentation. There
    are many domains for which we can build customer segmentation, such as e-commerce,
    travel, finance, telecom, and so on. Here, we will focus only on the e-commerce
    domain.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，客户细分有助于公司保留现有客户以及吸引新的潜在客户。基于细分，公司可以为特定客户细分创建定制产品，但到目前为止，我们还不知道如何生成细分。这就是我们在本章中要关注的问题。你需要学习如何创建客户细分。我们可以为许多领域构建客户细分，例如电子商务、旅游、金融、电信等。在这里，我们将仅关注电子商务领域。
- en: 'Here is a detailed explanation of the problem statement, input, and output
    for the e-commerce customer segmentation application that we will be building:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对我们将要构建的电子商务客户细分应用的问题陈述、输入和输出的详细说明：
- en: '**Problem statement**: The goal of our customer segmentation application is
    to come up with a solution for the given questions:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题陈述**：我们客户细分应用的目标是为以下问题提供解决方案：'
- en: Can we categorize the customers in a particular segment based on their buying
    patterns?
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否根据客户的购买模式将客户分类到特定的细分市场？
- en: Can we predict which kind of items they will buy in future based on their segmentation?
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否根据他们的细分情况预测他们未来会购买哪些类型的商品？
- en: '**Input**: We will be using e-commerce data that contains the list of purchases
    in 1 year for 4,000 customers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：我们将使用包含4000名客户1年内购买列表的电子商务数据。'
- en: '**Output**: The first goal is that we need to categorize our consumer base
    into appropriate customer segments. The second goal is we need to predict the
    purchases for the current year and the next year based on the customers'' first
    purchase.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：第一个目标是我们需要将我们的消费者基础分类到合适的客户细分中。第二个目标是，我们需要根据客户的首次购买预测当前年和下一年度的购买。'
- en: You may wonder how we can achieve a prediction about the upcoming purchases
    using segmentation. Well, let me tell you how segmentation helps us! So, we don't
    know the purchase pattern of the new customer, but we know the customer profile.
    We also know which product the customer has bought. So, we can put the customer
    into one of the segments where all other customers have purchased similar items
    and share similar kinds of profile.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道我们如何通过细分来预测即将到来的购买。好吧，让我告诉你细分如何帮助我们！所以，我们不知道新客户的购买模式，但我们知道客户档案。我们还知道客户购买过哪些产品。因此，我们可以将客户放入一个细分中，其中所有其他客户都购买过类似的产品，并具有类似的档案。
- en: Let me give you an example. Say, a person has bought a Harry Potter book and
    that person lives in the UK. The age group of the customer is from 13-22\. If
    we have already generated a customer segment that satisfies these characteristics,
    then we will put this new customer in that particular subgroup. We will derive
    the list of items that the customer may buy in future. We will also offer similar
    services that other customers in the subgroup have.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你举一个例子。比如说，一个人买了一本哈利·波特的书，这个人住在英国。客户的年龄组是13-22岁。如果我们已经生成了一个满足这些特征的客户细分，那么我们将把这个新客户放入那个特定的子组中。我们将推导出客户未来可能购买的商品列表。我们还将提供该子组中其他客户所享有的类似服务。
- en: The approach that we will be using in order to develop customer segmentation
    for the e-commerce domain can also be used in other domains, but data points (features)
    will differ for each domain. Later on in the chapter, we will discuss what kind
    of data points you may consider for other domains, such as travelling, finance,
    and so on. I will provide the list of data points for other domains that will
    help you build the customer segmentation application from scratch.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于开发电子商务领域客户细分的方法也可以用于其他领域，但每个领域的数据点（特征）将不同。在章节的后面，我们将讨论你可能考虑用于其他领域（如旅行、金融等）的数据点类型。我将提供其他领域的数据点列表，这将帮助你从头开始构建客户细分应用程序。
- en: Now it is time to understand the dataset for building customer segmentation
    for the e-commerce domain.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候理解用于构建电子商务领域客户细分的数据集了。
- en: Understanding the datasets
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Finding out an appropriate dataset is a challenging task in data science. Sometimes,
    you find a dataset but it is not in the appropriate format. Our problem statement
    will decide what type of dataset and data format we need. These kinds of activities
    are a part of data wrangling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，找到合适的数据集是一项具有挑战性的任务。有时，你找到了一个数据集，但它不是合适的格式。我们的问题陈述将决定我们需要什么类型的数据集和数据格式。这类活动是数据清洗的一部分。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Data wrangling is defined as the process of transforming and mapping data from
    one data form into another. With transformation and mapping, our intention should
    be to create an appropriate and valuable dataset that can be useful in order to
    develop analytics products. Data wrangling is also referred to as data munging
    and is a crucial part of any data science application.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗被定义为将数据从一种数据形式转换到另一种数据形式的过程。通过转换和映射，我们的目标应该是创建一个合适且有价值的数据集，以便用于开发分析产品。数据清洗也被称为数据整理，并且是任何数据科学应用的关键部分。
- en: Generally, e-commerce datasets are proprietary datasets, and it's rare that
    you get transactions of real users. Fortunately, *The UCI Machine Learning Repository*
    hosts a dataset named *Online Retail*. This dataset contains actual transactions
    from UK retailers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，电子商务数据集是专有数据集，你很少能获得真实用户的交易数据。幸运的是，*UCI机器学习仓库*托管了一个名为*在线零售*的数据集。这个数据集包含了来自英国零售商的实际交易数据。
- en: Description of the dataset
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集描述
- en: This Online Retail dataset contains the actual transactions between December
    1, 2010 and December 9, 2011\. All the transactions are taken from the registered
    non-store online retail platform. These online retail platforms are mostly based
    in the UK. The online retail platforms are selling unique all-occasion gifts.
    Many consumers of these online retail platforms are wholesalers. There are 532610
    records in this dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此在线零售数据集包含2010年12月1日至2011年12月9日之间的实际交易。所有交易均来自注册的非实体店在线零售平台。这些在线零售平台大多位于英国。这些在线零售平台销售独特的全场合礼物。这些在线零售平台的许多消费者是批发商。此数据集中有532610条记录。
- en: Downloading the dataset
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据集
- en: 'You can download this dataset by using either of the following links:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下任一链接下载此数据集：
- en: '[http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)'
- en: '[https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)'
- en: Attributes of the dataset
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集的属性
- en: 'These are the attributes in this dataset. We will take a look at a short description
    for each of them:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据集中的属性。我们将查看每个属性的简要描述：
- en: 'InvoiceNo: This data attribute indicates the invoice numbers. It is a six-digit
    integer number. The records are uniquely assigned for each transaction. If the
    invoice number starts with the letter ''c'', then it indicates a cancellation.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'InvoiceNo: 此数据属性表示发票号码。它是一个六位数的整数。记录为每笔交易唯一分配。如果发票号码以字母''c''开头，则表示取消。'
- en: 'StockCode: This data attribute indicates the product (item) code. It is a five-digit
    integer number. All the item codes are uniquely assigned to each distinct product.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'StockCode: 此数据属性表示产品（项目）代码。它是一个五位数的整数。所有项目代码都是唯一分配给每个不同产品的。'
- en: 'Description: This data attribute contains the description about the item.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Description: 此数据属性包含关于项目的描述。'
- en: 'Quantity: This data attribute contains the quantities for each product per
    transaction. The data is in a numeric format.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Quantity: 此数据属性包含每笔交易中每种产品的数量。数据为数值格式。'
- en: 'InvoiceDate: The data attribute contains the invoice date and time. It indicates
    the day and time when each transaction was generated.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'InvoiceDate: 此数据属性包含发票日期和时间。它表示每次交易生成的日期和时间。'
- en: 'UnitPrice: The price indicates the product price per unit in sterling.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'UnitPrice: 价格表示每单位产品的英镑价格。'
- en: 'CustomerID: This column has the customer identification number. It is a five-digit
    integer number uniquely assigned to each customer.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'CustomerID: 此列包含客户识别号码。它是一个五位数的整数，唯一分配给每个客户。'
- en: 'Country: This column contains the geographic information about the customer.
    It records the country name for the customers.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Country: 此列包含有关客户的地理信息。它记录了客户的国家名称。'
- en: 'You can refer to the sample of the dataset given in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下截图中的数据集样本：
- en: '![Attributes of the dataset](img/B08394_03_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![数据集的属性](img/B08394_03_04.jpg)'
- en: 'Figure 3.4: Sample recodes from the dataset'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：数据集的样本重编码
- en: Now we will start building the customer segmentation application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开始构建客户细分应用程序。
- en: Building the baseline approach
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基线方法
- en: 'In this section, we will start implementing the basic model for the customer
    segmentation application. Furthermore, we will improve this baseline approach.
    While implementing, we will cover the necessary concepts, technical aspects, and
    significance of performing that particular step. You can find the code for the
    customer-segmentation application at this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation](https://github.com/jalajthanaki/Customer_segmentation)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始实现客户细分应用程序的基本模型。此外，我们将改进这个基线方法。在实现过程中，我们将涵盖执行该特定步骤所必需的概念、技术方面和重要性。您可以在以下GitHub链接找到客户细分应用程序的代码：[https://github.com/jalajthanaki/Customer_segmentation](https://github.com/jalajthanaki/Customer_segmentation)
- en: 'The code related to this chapter is given in a single iPython notebook. You
    can access the notebook using this GitHub link: [https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb](https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的代码在一个单独的iPython笔记本中给出。你可以使用此GitHub链接访问笔记本：[https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb](https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb)。
- en: Refer to the code given on GitHub because it will help you understand things
    better. Now let's begin the implementation!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考GitHub上给出的代码，因为它将帮助你更好地理解事物。现在让我们开始实施！
- en: Implementing the baseline approach
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施基线方法
- en: 'In order to implement the customer segmentation model, our implementation will
    have the following steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现客户细分模型，我们的实现将包括以下步骤：
- en: Data preparation
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Exploratory data analysis (EDA)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: Generating customer categories
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成客户类别
- en: Classifying customers
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户分类
- en: Let's begin with data preparation!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据准备开始！
- en: Data preparation
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: This is a basic step when you try to build any analytics application. First,
    we need to be sure that the format of the data is in an appropriate form. If it
    is not, then we need to prepare our dataset in such a way that we can build our
    application easily. In this step, we will find out whether we have a good quality
    dataset or not. We can also find out some basic facts about the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在尝试构建任何分析应用程序时的一个基本步骤。首先，我们需要确保数据格式是适当的。如果不是，那么我们需要以这种方式准备我们的数据集，以便我们可以轻松构建我们的应用程序。在这个步骤中，我们将找出我们是否有一个高质量的数据集。我们还可以找出有关数据集的一些基本事实。
- en: Luckily, we don't need to change the format of our e-commerce dataset, but we
    will be exploring the dataset in such a way that we can find out the quality of
    the dataset. If format of the dataset is not proper then you need to decide the
    format of the dataset in such a way that any kind of analysis can be performed
    using the dataset. You can convert the data records either in CSV format or in
    JSON format or in XML format. In addition, we can derive general facts about the
    dataset, such as whether our dataset is biased or not, whether the dataset contains
    any null values, the mapping of the customers with `Customer_ID` is proper or
    not, whether their purchases are properly recorded in dataset or not, and so on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要更改我们电子商务数据集的格式，但我们将以这种方式探索数据集，以便我们可以找出数据集的质量。如果数据集的格式不正确，那么你需要决定数据集的格式，以便可以使用数据集进行任何类型的分析。你可以将数据记录转换为CSV格式、JSON格式或XML格式。此外，我们还可以推导出关于数据集的一般事实，例如我们的数据集是否存在偏差，数据集是否包含任何空值，客户与`Customer_ID`的映射是否正确，以及他们的购买是否在数据集中正确记录，等等。
- en: Loading the dataset
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'In order to load the dataset, we will use the pandas `read_csv` API. You can
    find the code snippet given in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载数据集，我们将使用pandas的`read_csv` API。你可以在下面的屏幕截图中找到给出的代码片段：
- en: '![Loading the dataset](img/B08394_03_05.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_03_05.jpg)'
- en: 'Figure 3.5: Code snippet for loading the dataset'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：加载数据集的代码片段
- en: As you can see, the dimensions of the dataset are (541909, 8). This means that
    there are 541,909 records in the dataset and eight data attributes. We have already
    covered these eight data attributes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据集的维度是（541909，8）。这意味着数据集中有541,909条记录和八个数据属性。我们已涵盖了这八个数据属性。
- en: Now we need to perform exploratory data analysis (EDA), which can help us preprocess
    our dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要进行探索性数据分析（EDA），这可以帮助我们预处理我们的数据集。
- en: Exploratory data analysis (EDA)
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: 'In this section, we need to check the statistical properties of the dataset
    and perform some preprocessing steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们需要检查数据集的统计属性并执行一些预处理步骤：
- en: Removing null data entries
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除空数据条目
- en: Removing duplicate data entries
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除重复数据条目
- en: EDA for various data attributes
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对各种数据属性进行EDA
- en: Removing null data entries
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 删除空数据条目
- en: 'First, we need to check the data type of each of the attributes as well as
    find out which column has a null value. You can refer to the code snippet shown
    in the following screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要检查每个属性的 数据类型，并找出哪个列有空值。你可以参考以下屏幕截图中显示的代码片段：
- en: '![Removing null data entries](img/B08394_03_06.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![删除空数据条目](img/B08394_03_06.jpg)'
- en: 'Figure 3.6: Code snippet for exploring the dataset'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：探索数据集的代码片段
- en: 'As you can see in the code, we have generated the total number of null values
    for each data attribute. We have also generated the percentage of null values
    for each data attribute. We can observe that for the `CustomerID` column, there
    are ~25% data entries that are null. That means there is no `CustomerID` value
    available for ~25% of the dataset. This indicates that there are many entries
    that do not belong to any customer. These are abended data entries. We cannot
    map them to the existing CustomerIDs. As a result, we need to delete them. You
    can find the code snippet for deleting null data entries from the dataset in the
    following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，我们已经为每个数据属性生成了总空值数。我们还为每个数据属性生成了空值百分比。我们可以观察到，对于`CustomerID`列，大约有25%的数据条目是空的。这意味着大约25%的数据集中没有`CustomerID`值。这表明有许多条目不属于任何客户。这些都是异常数据条目。我们无法将它们映射到现有的客户ID上。因此，我们需要删除它们。您可以在以下截图中找到从数据集中删除空数据条目的代码片段：
- en: '![Removing null data entries](img/B08394_03_07.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![删除空数据条目](img/B08394_03_07.jpg)'
- en: 'Figure 3.7: Deleting null data entries'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：删除空数据条目
- en: Removing duplicate data entries
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 删除重复数据条目
- en: 'After this step, we will check whether there are any duplicate data entries
    present in the dataset. In order to answer this question, we will use the pandas
    `duplicate()` function. You can refer to the code snippet shown in the following
    screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们将检查数据集中是否存在重复的数据条目。为了回答这个问题，我们将使用pandas的`duplicate()`函数。您可以参考以下截图中的代码片段：
- en: '![Removing duplicate data entries](img/B08394_03_08.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![删除重复数据条目](img/B08394_03_08.jpg)'
- en: 'Figure 3.8: Removing duplicate data entries'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：删除重复数据条目
- en: As you can see, we found 5,225 duplicate data entries. Therefore, we have removed
    them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们找到了5,225个重复数据条目。因此，我们已经删除了它们。
- en: Now let's analyze each data attribute in detail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来详细分析每个数据属性。
- en: EDA for various data attributes
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对各种数据属性进行EDA
- en: EDA for each data attribute will help us get more insight into the dataset.
    Later on, we will use these facts to build an accurate customer segmentation application.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个数据属性进行EDA可以帮助我们更深入地了解数据集。稍后，我们将使用这些事实来构建一个准确的客户细分应用程序。
- en: 'We will start exploring data attributes in the following order:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下顺序开始探索数据属性：
- en: Country
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 国家
- en: Customer and products
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户和产品
- en: Product categories
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品类别
- en: Defining product categories
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义产品类别
- en: '**Country**'
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**国家**'
- en: 'We need to find out facts such as how many countries there are in our dataset.
    In order to answer this question, we need to execute the code shown in the following
    screenshot:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找出诸如数据集中有多少个国家等事实。为了回答这个问题，我们需要执行以下截图中的代码：
- en: '![Country](img/B08394_03_09.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![国家](img/B08394_03_09.jpg)'
- en: 'Figure 3.9: Code snippet for generating the number of counties present in the
    dataset'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：生成数据集中存在的县数的代码片段
- en: 'We also need to find the country from which we receive the maximum number of
    orders. We can find that out by using the pandas `groupby()` and `count()` functions.
    We sort the number of orders in descending order. You can refer to the code snippet
    in the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要找出我们收到订单数量最多的国家。我们可以通过使用pandas的`groupby()`和`count()`函数来找出这一点。我们将按订单数量降序排序。您可以参考以下截图中的代码片段：
- en: '![Country](img/B08394_03_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![国家](img/B08394_03_10.jpg)'
- en: 'Figure 3.10: Code snippet for generating country-wise number of orders'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：生成按国家划分的订单数量的代码片段
- en: As you can see in the preceding snippet, there are a majority of orders from
    UK-based customers. Now we need to explore the customer and products variables.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的代码片段所示，大多数订单来自英国客户。现在我们需要探索客户和产品变量。
- en: '**Customer and products**'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**客户和产品**'
- en: 'Here, we have approximately 400,000 data items. We need to know the number
    of users and products that are present in these data entries. We will be using
    the `value_counts()` function from the `pandas` library. Take a look at the code
    snippet in the following screenshot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们大约有40万个数据项。我们需要知道这些数据条目中包含的用户和产品数量。我们将使用`pandas`库中的`value_counts()`函数。请查看以下截图中的代码片段：
- en: '![Customer and products](img/B08394_03_11.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_11.jpg)'
- en: 'Figure 3.11: Code for exploring customer and products'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：探索客户和产品的代码
- en: As you can see in the above screen shot that this dataset contains the records
    of 4372 users who bought 3684 different items
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，这个数据集包含了4372个用户的记录，他们购买了3684种不同的商品
- en: We have derived some interesting facts. In the given dataset, there are 4,372
    customers who have bought 3,684 different products. The total number of transactions
    is 22,190.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经推导出一些有趣的事实。在给定的数据集中，有4,372位客户购买了3,684种不同的产品。交易总数为22,190。
- en: 'We should also find out how many products have been purchased for each transaction.
    For that, we will use the `InvoiceNo` and `InvoiceDate` data attributes, and we
    will calculate the number of products purchased for every transaction. You can
    refer to the code snippet shown in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该找出每个交易中购买了多少产品。为此，我们将使用`InvoiceNo`和`InvoiceDate`数据属性，并且我们将计算每个交易购买的产品数量。你可以参考以下屏幕截图中的代码片段：
- en: '![Customer and products](img/B08394_03_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_12.jpg)'
- en: 'Figure 3.12: Code snippet for exploring the number of products per transaction'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：探索每个交易产品数量的代码片段
- en: 'As shown in the preceding code snippet, we can make the following observations:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，我们可以得出以下观察结果：
- en: There are some users who have made a purchase only once on the e-commerce platform
    and bought one item. An example of this kind of user is `customerID 12346`.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些用户在电子商务平台上只购买过一次，并且只购买了一件商品。这类用户的例子是`customerID 12346`。
- en: There are some users who frequently buy a large number of items per order. An
    example of this kind of user is `customerID 12347`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些用户经常在每笔订单中购买大量商品。这类用户的例子是`customerID 12347`。
- en: If you look at the InvoiceNo data attribute, then you can see that there is
    the prefix `C` for one invoice. This `'C'` indicates that the particular transaction
    has been canceled.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你查看`InvoiceNo`数据属性，那么你可以看到有一个发票的前缀`C`。这个`'C'`表示特定的交易已被取消。
- en: 'As we know, there can be a couple of canceled orders present in our dataset,
    and we need to count the number of transactions corresponding to the canceled
    orders. We have used a simple check condition using the lambda expression. Now
    we will calculate the percentage of canceled orders. You can refer to the code
    snippet given in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，在我们的数据集中可能存在几个取消订单，我们需要计算与取消订单对应的交易数量。我们使用了一个简单的lambda表达式检查条件。现在我们将计算取消订单的百分比。你可以参考以下屏幕截图中的代码片段：
- en: '![Customer and products](img/B08394_03_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_13.jpg)'
- en: 'Figure 3.13: Code snippet for generating the percentage of canceled orders'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：生成取消订单百分比的代码片段
- en: 'Let''s list down some of the canceled order entries so that we can find out
    how to handle them. Take a look at the following screenshot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出一些取消订单的条目，以便我们可以找出如何处理它们。请看以下屏幕截图：
- en: '![Customer and products](img/B08394_03_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_14.jpg)'
- en: 'Figure 3.14: List of canceled orders'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：取消订单列表
- en: 'Basically, in order to handle the canceled orders, we will need to take the
    following steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，为了处理取消订单，我们需要采取以下步骤：
- en: As you can observe, if the order is canceled, then there is another transaction
    that will mostly have an identical transaction except for the quantity and invoice
    date. First, we need to check whether this is true for all entries.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如你所观察到的，如果订单被取消，那么将会有另一个交易，除了数量和发票日期外，其他方面大致相同。首先，我们需要检查这是否适用于所有条目。
- en: We can perform this checking operation by using simple logic. Mostly, the canceled
    order has a negative quantity, so we will check whether there is an order indicating
    the same quantity (but positive), with the same description values.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过简单的逻辑执行此检查操作。大多数情况下，取消订单的数量是负数，因此我们将检查是否存在一个数量相同（但为正数）且描述值相同的订单。
- en: There are some discount entries as well, and we need to handle them. We will
    discard the discount entries.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时也有一些折扣条目，我们需要处理它们。我们将丢弃这些折扣条目。
- en: 'You can refer to the code for this, as shown in the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下屏幕截图中的代码，如下所示：
- en: '![Customer and products](img/B08394_03_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_15.jpg)'
- en: 'Figure 3.15: Code for handelling cancel orders'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15：处理取消订单的代码
- en: 'When we run the preceding code, we find out that there are no similar entries
    present in our dataset for all canceled transactions. In order to overcome this
    situation, we will create a new variable in our dataframe, which indicates whether
    the transaction has been canceled or not. There are three possibilities for canceled
    orders:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们发现数据集中没有所有已取消交易的类似条目。为了克服这种情况，我们将在我们的数据框中创建一个新的变量，该变量指示交易是否已被取消。已取消订单有三种可能性：
- en: There are some transactions that were canceled without counterparts. A few of
    them are probably due to the fact that the buy orders were performed before December
    2010\. We have the dataset from December 2010 to December 2011.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些交易在没有对应项的情况下被取消了。其中一些可能是因为购买订单是在2010年12月之前执行的。我们拥有从2010年12月到2011年12月的数据集。
- en: There are some orders that were canceled with exactly one counterpart. We will
    consider them as well.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些订单是与恰好一个对应项一起取消的。我们也将考虑这些订单。
- en: There are some entries that are doubtful. We will check whether there is at
    least one counterpart with the exact same quantity available. If available, then
    we can mark those entries as doubtful.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些条目是可疑的。我们将检查是否存在至少一个与确切数量相同的对应项。如果存在，则我们可以将这些条目标记为可疑。
- en: 'You can refer to the code shown in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下截图中的代码：
- en: '![Customer and products](img/B08394_03_16.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_16.jpg)'
- en: 'Figure 3.16: Code snippet for generating flags for canceled orders'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16：生成已取消订单标志的代码片段
- en: 'As we can see in the preceding code snippet, there are 7,521 entries that show
    the canceled orders with their counterpart. There are 1,226 entries that show
    canceled orders without their counterpart. For the sake of simplicity, we are
    going to delete all the entries related to the canceled orders. The code for deleting
    these records is given in the following screenshot:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，有7,521条条目显示了带有对应项的已取消订单。有1,226条条目显示了没有对应项的已取消订单。为了简化，我们将删除所有与已取消订单相关的条目。删除这些记录的代码在以下截图给出：
- en: '![Customer and products](img/B08394_03_17.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_17.jpg)'
- en: 'Figure 3.17: Code snippet for deleting canceled orders'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17：删除已取消订单的代码片段
- en: 'Now let''s analyze the entries based on the stock code because we know that
    during the identification of the canceled order, we discover discount items based
    on the *stock code D*. So first of all, we will be listing down all the stock
    codes and their meaning. You can refer to the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据股票代码分析条目，因为我们知道在识别已取消订单的过程中，我们根据*股票代码D*发现了折扣商品。因此，首先，我们将列出所有股票代码及其含义。您可以参考以下截图：
- en: '![Customer and products](img/B08394_03_18.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_18.jpg)'
- en: 'Figure 3.18: Code snippet for stock code'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18：股票代码的代码片段
- en: 'Now let''s focus on the pricing of the individual order. In the given dataset,
    the order from a single customer has been split into several lines. What do I
    mean by several lines? In order to understand that, refer to the following screenshot:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于单个订单的定价。在给定的数据集中，单个客户的订单已被拆分为几行。我所说的几行是什么意思？为了理解这一点，请参考以下截图：
- en: '![Customer and products](img/B08394_03_19.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_19.jpg)'
- en: 'Figure 3.19: Understanding data entries for orders'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19：理解订单数据条目
- en: 'Each entry in our dataset indicates prizes for a single kind of product. If
    the order including different products is placed by a single customer, then there
    are multiple entries for that particular order. The number of data entries depends
    on how many different products that order has. As you can see in the preceding
    figure, there were three different products included in one order. We need to
    obtain the total price for each order. In order to achieve that, we will add a
    column named *TotalPrice*, which gives us the total value of the order or the
    basket price for a single order. The main logic for deriving *TotalPrice* is that
    we are multiplying *UnitPrice* with the net quantity. We obtain the net quantity
    by deducting the canceled quantity from the total quantity. Take a look at the
    code snippet shown in the following screenshot:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的每个条目都表示一种产品的价格。如果一个订单包含不同的产品，并且由同一个客户下单，那么该订单将有多个条目。数据条目的数量取决于订单中有多少种不同的产品。正如您在前面的图中看到的，一个订单中包含了三种不同的产品。我们需要获取每个订单的总价格。为了实现这一点，我们将添加一个名为*TotalPrice*的列，它给出了订单的总价值或单个订单的篮子价格。计算*TotalPrice*的主要逻辑是我们将*UnitPrice*与净数量相乘。我们通过从总量中扣除取消的数量来获得净数量。请查看以下截图所示的代码片段：
- en: '![Customer and products](img/B08394_03_20.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_20.jpg)'
- en: 'Figure 3.20: Code for obtaining TotalPrice'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20：获取TotalPrice的代码
- en: 'Once we obtain the total price, we will generate the sum for individual orders
    and then group our entries based on the invoice data. We will list only those
    data entries that have a basket price greater than 0\. The code to achieve this
    is given in the following screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得总价格，我们将为单个订单生成金额总和，然后根据发票数据对条目进行分组。我们只列出篮子价格大于0的数据条目。实现这一点的代码如下截图所示：
- en: '![Customer and products](img/B08394_03_21.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_21.jpg)'
- en: 'Figure 3.21: Code for generating the basket price based on the invoice date'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21：基于发票日期生成篮子价格的代码
- en: 'Now it''s time to get an idea about the distribution of the orders'' amounts
    for the given dataset. What do I mean by distribution of the orders'' amounts?
    Well, we should be aware about the prices for all the orders present in the dataset,
    and we need to put in the ranges based on the amount of all the orders. This will
    help us derive the number of orders in the dataset that are above £200\. It will
    also help us identify the number of orders that are below £100\. This kind of
    information helps us know the data distribution based on the number of orders.
    This will give us a basic picture about sales on the e-commerce platform. The
    code snippet for generating data distribution based on the orders'' amounts is
    displayed in the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候了解给定数据集中订单金额的分布情况了。我所说的订单金额分布是什么意思呢？嗯，我们应该了解数据集中所有订单的价格，并根据所有订单的金额范围进行分类。这将帮助我们得出数据集中金额超过200英镑的订单数量。这也有助于我们识别金额低于100英镑的订单数量。这类信息有助于我们了解基于订单数量的数据分布。这将给我们一个电子商务平台上销售的基本情况。以下截图显示了基于订单金额生成数据分布的代码片段：
- en: '![Customer and products](img/B08394_03_22.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_22.jpg)'
- en: 'Figure 3.22: Code snippet for generating data distribution based on orders''
    amounts'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22：基于订单金额生成数据分布的代码片段
- en: 'You can see the pictorial representation of this data distribution as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以如下看到此数据分布的图示：
- en: '![Customer and products](img/B08394_03_23.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![客户和产品](img/B08394_03_23.jpg)'
- en: 'Figure 3.23: Pictorial representation of the data distribution'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23：数据分布的图示
- en: As we can see, approximately 65% of the orders are above £200\. We have explored
    orders in great detail. Now let's begin with the analysis of product categories.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大约65%的订单金额超过200英镑。我们已经对订单进行了详细的探索。现在让我们开始分析产品类别。
- en: '**Product categories**'
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**产品类别**'
- en: 'In this section, we will be doing an EDA of the product-related data attribute.
    We will include the following kinds of analysis in this section:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对产品相关的数据属性进行EDA。我们将在本节中包含以下类型的分析：
- en: Analyzing the product description
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析产品描述
- en: Defining the product categories
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义产品类别
- en: Characterizing the content of clusters
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述聚类内容
- en: '**Analyzing the product description**'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '**分析产品描述**'
- en: In this section, we will be using two data attributes. We will use the `StockCode`
    data attribute, which contains a unique ID for each product. We will also use
    the `Description` data attribute in order to group the products in different categories.
    Let's start with the product description.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用两个数据属性。我们将使用`StockCode`数据属性，它包含每个产品的唯一ID。我们还将使用`Description`数据属性来对产品进行分类。让我们从产品描述开始。
- en: 'First, we will define the function that will take the dataframe as input, and
    then we will perform the the following operations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个函数，该函数将数据框作为输入，然后我们将执行以下操作：
- en: We will extract names (nouns) from the product description.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从产品描述中提取名称（名词）。
- en: Then, we will generate the root form of the extracted names. We will store the
    root of the name as the key and all associated names as its value. We will use
    a stemmer from the NLTK library for this step. A stemmer basically generates the
    root form of the words by removing suffixes and prefixes.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将生成提取名称的词根形式。我们将名称的词根作为键，所有相关名称作为其值。我们将使用NLTK库中的词干提取器来完成这一步骤。词干提取器基本上通过删除后缀和前缀来生成单词的词根形式。
- en: We will count the frequency of the roots of the names, which means we will count
    how many times the root form of each name appears.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将计算名称根的频率，这意味着我们将计算每个名称的词根形式出现的次数。
- en: If various names have the same root, then we consider the root form as the keyword
    tag.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果各种名称有相同的根，那么我们将根形式视为关键词标签。
- en: 'You can see the code for this function in the following screenshot:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在下面的屏幕截图中查看此函数的代码：
- en: '![Analyzing the product description](img/B08394_03_24.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![分析产品描述](img/B08394_03_24.jpg)'
- en: 'Figure 3.24: Code snippet of the function for generating keywords from the
    product description'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24：从产品描述生成关键词的函数代码片段
- en: 'Now we need to call this function and feed the input dataframe. You can take
    a look at the code snippet given in the following screenshot:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要调用这个函数并传入输入的数据框。您可以在下面的屏幕截图中查看提供的代码片段：
- en: '![Analyzing the product description](img/B08394_03_25.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![分析产品描述](img/B08394_03_25.jpg)'
- en: 'Figure 3.25: Code snippet that actually generates keywords'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25：实际生成关键词的代码片段
- en: 'Here, we are returning three variables:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在返回三个变量：
- en: '`Keyword:` This is the list of extracted names'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keyword:` 这是提取的名称列表'
- en: '`Keywords_roots:` This is a dictionary where the keys are the root of the name
    and values are the list of names associated with root name.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keywords_roots:` 这是一个字典，其中键是名称的根，值是与根名称关联的名称列表。'
- en: '`Count_keywords:` This is a dictionary that keeps track of the frequency of
    each name. The count indicates the number of times a particular name appeared
    in the description. Later on, we will convert the dictionary into a list.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Count_keywords:` 这是一个字典，用于跟踪每个名称的频率。计数表示特定名称在描述中出现的次数。稍后，我们将字典转换为列表。'
- en: 'Now let''s plot the keywords versus their frequency graphs. The code is given
    in the following screenshot:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制关键词与其频率的图表。代码如下所示：
- en: '![Analyzing the product description](img/B08394_03_26.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![分析产品描述](img/B08394_03_26.jpg)'
- en: 'Figure 3.26: Code snippet for generating the frequency graph'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26：生成频率图的代码片段
- en: As you can see in the preceding figure, the word (meaning the noun or the name)
    heart has appeared the maximum number of times in the product description. You
    might wonder what the significance of generating this word frequency is. Well,
    we are using this to categorize products. Now it's time to look into how to come
    up with product categories.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，单词（指名词或名称）"heart"在产品描述中出现的次数最多。您可能会想知道生成这个单词频率的意义是什么。嗯，我们正在使用它来对产品进行分类。现在，让我们看看如何提出产品类别。
- en: '**Defining product categories**'
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '**定义产品类别**'
- en: 'Here we will obtain the product categories. We have obtained more than 1,400
    keywords, and the most frequent names have appeared in more than 200 products.
    Now we need to remove words that are less important. We can observe some useless
    words, such as names of colors and discard them. So, we will consider words that
    appear in the dataset more than 13 times. You can refer to the code snippet shown
    in the following screenshot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将获取产品类别。我们已经获得了1400多个关键词，最频繁出现的名称出现在200多个产品中。现在我们需要删除不太重要的单词。我们可以观察到一些无用的单词，例如颜色名称，并将它们丢弃。因此，我们将考虑在数据集中出现超过13次的单词。您可以在下面的屏幕截图中查看代码片段：
- en: '![Defining product categories](img/B08394_03_27.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_27.jpg)'
- en: 'Figure 3.27: Code snippet for preserving important words'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.27：保留重要单词的代码片段
- en: 'Now we need to encode the data. Here, we have textual data and we need to convert
    it into a numerical format. For this, we will use one-hot encoding. One-hot encoding
    is a simple concept. In order to understand it, refer to the given matrix x. Take
    a look at the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要对数据进行编码。在这里，我们有文本数据，我们需要将其转换为数值格式。为此，我们将使用one-hot编码。one-hot编码是一个简单的概念。为了理解它，请参考给出的矩阵x。看一下以下截图：
- en: '![Defining product categories](img/B08394_03_28.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_28.jpg)'
- en: 'Figure 3.28: Table for understanding one-hot data encoding'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.28：理解one-hot数据编码的表格
- en: 'If a particular word is present in the product description, then the value
    of the coefficient is 1, and if the word is not present in the product description,
    then the value of the coefficient is 0\. You can refer to the the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个单词出现在产品描述中，则系数的值为1，如果单词没有出现在产品描述中，则系数的值为0。您可以参考以下截图：
- en: '![Defining product categories](img/B08394_03_29.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_29.jpg)'
- en: 'Figure 3.29: Intuitive example for one-hot data encoding'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.29：one-hot数据编码的直观示例
- en: As you can see, this data encoding is a binary kind of vectorization because
    we are placing either zero or one. We will get a sparse vector for each word after
    encoding. In layman's terms, we can say that this kind of vectorization indicates
    the presence of the word in the product description.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这种数据编码是一种二进制类型的向量化，因为我们放置的是零或一。编码后，我们将为每个单词得到一个稀疏向量。用通俗易懂的话来说，这种向量化表示了单词在产品描述中的存在。
- en: 'Now let''s create the groups or cluster for the product based on the price
    range. For that, we will be using the keyword list that we have generated, check
    whether the product description has the words that are present in the keywords,
    and take the mean value of `UnitPrice`. You can refer to the code given in the
    following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据价格范围创建产品的组或聚类。为此，我们将使用我们生成的关键词列表，检查产品描述中是否包含关键词中的单词，并取`UnitPrice`的平均值。您可以参考以下截图中的代码：
- en: '![Defining product categories](img/B08394_03_30.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_30.jpg)'
- en: 'Figure 3.30: Code snippet for generating the product group based on the price
    range'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.30：基于价格范围生成产品组的代码片段
- en: Now we will create clusters of the products. We will be using the k-means clustering
    algorithm. We will also be using the scikit-learn library to implement the K-means
    clustering algorithm. The algorithm from scikit-learn uses Euclidean distance.
    In our case, this is not the best choice. We should use Hamming distance. The
    most suitable library for that is `Kmods`, but this library is not available for
    all operating systems, so we have to use the scikit-learn library. We need to
    define the number of clusters that can represent the data perfectly. We will come
    up with the ideal number of clusters, and then we will use the silhouette score.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建产品的聚类。我们将使用k-means聚类算法。我们还将使用scikit-learn库来实现K-means聚类算法。scikit-learn中的算法使用欧几里得距离。在我们的情况下，这并不是最佳选择。我们应该使用汉明距离。最适合的库是`Kmods`，但这个库并不是所有操作系统都可用，所以我们必须使用scikit-learn库。我们需要定义可以完美表示数据的簇数量。我们将得出理想的簇数量，然后使用轮廓分数。
- en: 'You can take a look at how the k-means clustering algorithm works by using
    the link of this book: [https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing](https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing),
    Refer section K-means clustering form [Chapter 8](ch08.xhtml "Chapter 8. Developing
    Chatbots"), *Machine Learning for NLP problems*.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用本书的链接了解k-means聚类算法的工作原理：[https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing](https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing)，参考第8章的K-means聚类部分[第8章](ch08.xhtml
    "第8章。开发聊天机器人")，*机器学习解决NLP问题*。
- en: 'Let''s take a step back and understand the silhouette score first. The silhouette
    coefficient is calculated using two things. The first is the mean intra-cluster
    distance (a) and the second is the mean nearest-cluster distance (b) for each
    sample in our dataset. So, the equation is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步，首先了解轮廓分数。轮廓系数是通过两个因素计算的。第一个是每个样本在数据集中的平均簇内距离（a），第二个是平均最近簇距离（b）。因此，方程如下：
- en: '*(b-a) / max (a, b)*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*(b-a) / max (a, b)*'
- en: 'The *b* indicates the distance between a sample and the nearest cluster that
    the sample is not a part of. This score works if the number of labels is *2<=
    n_labels <= n_samples –1*. The best possible value for this score is 1, and worst
    value is –1\. Value 0 shows that we have overlapping clusters. Negative values
    indicate that the sample has been assigned to the wrong cluster. Refer to the
    code snippet shown in the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*b*表示样本与其不属于的最近簇的距离。当标签数量为*2<= n_labels <= n_samples –1*时，此评分有效。此评分的最佳可能值为1，最差值为-1。值为0表示我们有重叠的簇。负值表示样本已被分配到错误的簇。请参考以下截图中的代码片段：'
- en: '![Defining product categories](img/B08394_03_31.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_31.jpg)'
- en: 'Figure 3.31: Code snippet for choosing the ideal number of clusters using silhouette
    score'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.31：使用轮廓评分选择理想簇数量的代码片段
- en: 'Here, we have implemented the code using the scikit-learn API. As we can see,
    beyond five clusters, a cluster may contain very few elements, so we choose to
    categorize the products into five clusters. We will try to increase the value
    of the silhouette score. For that, we will iterate through the dataset. You can
    refer to the code shown in the following screenshot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用scikit-learn API实现了代码。正如我们所见，超过五个簇时，簇中可能包含非常少的元素，因此我们选择将产品分类为五个簇。我们将尝试提高轮廓评分。为此，我们将遍历数据集。你可以参考以下截图中的代码：
- en: '![Defining product categories](img/B08394_03_32.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![定义产品类别](img/B08394_03_32.jpg)'
- en: 'Figure 3.32: Code snippet to improvise the silhouette score'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.32：改进轮廓评分的代码片段
- en: Now let's move on to characterizing the content of the clusters section, which
    can help us understand how well the products have been classified into particular
    clusters.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续到描述簇的内容部分，这有助于我们了解产品被分类到特定簇中的效果如何。
- en: '**Characterizing the content of clusters**'
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**描述簇的内容**'
- en: 'In this section, we will analyze the properties of the product cluster. There
    will be three subsections here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析产品簇的性质。这里将分为三个小节：
- en: Silhouette intra-cluster score analysis
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓簇内评分分析
- en: Analysis using a word cloud
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词云进行分析
- en: Principal component analysis (PCA)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: 'Before we jump into this analysis, we need to check the number of products
    in each cluster. For that, we will be using the code snippet given in the following
    screenshot:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入分析之前，我们需要检查每个簇中的产品数量。为此，我们将使用以下截图中的代码片段：
- en: '![Characterizing the content of clusters](img/B08394_03_33.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![描述簇的内容](img/B08394_03_33.jpg)'
- en: 'Figure 3.33: Code snippet for counting the number of products for each cluster'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.33：计算每个簇产品数量的代码片段
- en: As you can see in the output, there are 1,009 products that belong to cluster
    number 3, whereas there are only 470 products that belong to cluster number 4\.
    We will start an in-depth analysis of these five clusters and their elements.
    First, we will start with the silhouette intra-cluster score analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，有1,009个产品属于第3簇，而只有470个产品属于第4簇。我们将开始对这些五个簇及其元素进行深入分析。首先，我们将从轮廓簇内评分分析开始。
- en: '**Silhouette intra-cluster score analysis**'
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '**轮廓簇内评分分析**'
- en: Basically, in this section, we will be checking the intra-cluster score for
    each element. We will sort the silhouette intra-cluster score. After sorting,
    we will draw a graph where the *x* *axis* represents the silhouette coefficient
    value and the *y* *axis* represents the cluster label. We generate the silhouette
    intra-cluster score for all the samples. We are building this graph because we
    want to choose an optimal value for `n_clusters` based on the silhouette intra-cluster
    score.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将检查每个元素的簇内评分。我们将对轮廓簇内评分进行排序。排序后，我们将绘制一个图表，其中*x*轴代表轮廓系数值，*y*轴代表簇标签。我们为所有样本生成轮廓簇内评分。我们绘制这个图表是为了根据轮廓簇内评分选择`n_clusters`的最佳值。
- en: 'As we have generated the silhouette intra-cluster score earlier, we know `n_clusters
    = 5` is the ideal choice for us, so we will represent the clusters in a pictorial
    manner. You can refer to the function that generates graphs in the following screenshot:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们之前已经生成了轮廓簇内评分，我们知道`n_clusters = 5`是我们理想的选项，因此我们将以图形方式表示这些簇。你可以参考以下截图中生成图表的函数：
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_34.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![轮廓簇内评分分析](img/B08394_03_34.jpg)'
- en: 'Figure 3.34: Code snippet of the function for silhouette intra-cluster score
    analysis'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.34：轮廓内聚群得分分析函数的代码片段
- en: 'After executing and calling this function, we can obtain the graph displayed
    in the following screenshot:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 执行并调用此函数后，我们可以获得以下截图显示的图表：
- en: '![Silhouette intra-cluster score analysis](img/B08394_03_35.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![轮廓内聚群得分分析](img/B08394_03_35.jpg)'
- en: 'Figure 3.35: Code snippet and graph for silhouette intra-cluster analysis'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.35：轮廓内聚群分析的代码片段和图表
- en: Note
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that here, we obtain the graph for the optimal `n_cluster` value. This
    value is 5 in our case.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们获得了最优`n_cluster`值的图表。在我们的例子中，这个值是5。
- en: '**Analysis using a word cloud**'
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用词云进行分析**'
- en: In this section, we will analyze the clusters based on the keywords. We will
    check what words each cluster has. For this analysis, we will be using the word
    cloud library. You must be wondering why we are using this type of analysis. In
    our clusters, we are expecting similar kinds of products to belong to one cluster.
    We, as humans, know the language. When we see the words for the entire cluster,
    we can easily conclude whether our clusters have similar kinds of products or
    not. We will generate graphs that are intuitive enough for us to judge the accuracy
    of clustering.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将根据关键词分析聚类。我们将检查每个聚类包含哪些单词。为此分析，我们将使用词云库。你可能想知道为什么我们使用这种分析方法。在我们的聚类中，我们期望类似的产品属于一个聚类。作为人类，我们知道语言。当我们看到整个聚类的单词时，我们可以很容易地判断我们的聚类是否包含类似的产品。我们将生成足够直观的图表，以便我们判断聚类的准确性。
- en: 'You can refer to the code snippet given in the following screenshot:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下截图中的代码片段：
- en: '![Analysis using a word cloud](img/B08394_03_36.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![使用词云进行分析](img/B08394_03_36.jpg)'
- en: 'Figure 3.36: Code snippet for generating a word cloud'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.36：生成词云的代码片段
- en: 'You can refer to the code snippet given in the following screenshot:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下截图中的代码片段：
- en: '![Analysis using a word cloud](img/B08394_03_37.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![使用词云进行分析](img/B08394_03_37.jpg)'
- en: 'Figure 3.37: Code snippet for generating word cloud graphs'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.37：生成词云图表的代码片段
- en: 'You can refer to the graphs given in the following screenshot:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下截图中的图表：
- en: '![Analysis using a word cloud](img/B08394_03_38.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![使用词云进行分析](img/B08394_03_38.jpg)'
- en: 'Figure 3.38: Word cloud graphs for all five clusters'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.38：所有五个聚类的词云图表
- en: 'From the preceding graphs, we can conclude the following points:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下结论：
- en: Cluster number 2 contains all the words related to gifts, such as Christmas,
    packaging, gift, cards, and so on.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2号聚类包含所有与礼物相关的单词，如圣诞节、包装、礼物、卡片等。
- en: Cluster number 4 contains all the words related to luxury items and jewelry.
    So, keywords such as necklace, silver, lace, and so on are present in this cluster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4号聚类包含所有与奢侈品和珠宝相关的单词。因此，项链、银、蕾丝等关键词都存在于这个聚类中。
- en: There are some words that are present in every cluster, so it is difficult to
    clearly distinguish them.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些单词在所有聚类中都存在，因此很难清楚地区分它们。
- en: Now let's jump to the next section, where we will perform principal component
    analysis.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们跳到下一节，我们将进行主成分分析。
- en: Principal component analysis (PCA)
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: In order to check whether all the clusters have truly distinct values, we need
    to focus on their composition. As we know, the one-hot encoded matrix of the keywords
    has a large number of dimensions or a large number of variables. There may be
    a situation where because of the large number of variables, our clustering algorithm
    may over-fit the dataset. First of all, we need to reduce the number of variables,
    but we cannot reduce them randomly. We need to choose the most important variables
    that can represent most of the characteristics of the dataset. The procedure for
    reducing the number of variables logically is called dimensionality reduction.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查所有聚类是否确实具有不同的值，我们需要关注它们的组成。正如我们所知，关键词的一热编码矩阵具有大量的维度或变量。可能存在这样的情况，由于变量的数量庞大，我们的聚类算法可能会过度拟合数据集。首先，我们需要减少变量的数量，但我们不能随意减少。我们需要选择最能代表数据集大部分特征的最重要的变量。减少变量数量的逻辑过程称为降维。
- en: In order to achieve this, we will be using PCA, which is a statistical technique
    in which we will perform orthogonal transformation in order to convert a highly
    correlated set of data samples into a set of values that are linearly uncorrelated
    variables, and these variables are referred to as principal components. So basically,
    we will be using PCA because we want to reduce the number of variables that we
    have considered so far. PCA is a famous technique for dimensionality reduction.
    By using PCA, we can avoid the over-fitting issue.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用PCA，这是一种统计技术，我们将执行正交变换，将高度相关的数据样本集转换为线性不相关的变量集，这些变量被称为主成分。所以，基本上，我们将使用PCA，因为我们想减少我们迄今为止考虑的变量数量。PCA是降维的著名技术。通过使用PCA，我们可以避免过拟合问题。
- en: 'Now, you might want to know situations in which you can use PCA, and they are
    as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道可以使用PCA的情况，如下所示：
- en: If we want to reduce the number of variables (the number of features or the
    number of dimensions) but we cannot identify which variables can be considered
    and which can't
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想减少变量的数量（特征或维度的数量），但又无法确定哪些变量可以考虑，哪些不能
- en: If we want to ensure that our variables are independent of each other
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想确保我们的变量彼此独立
- en: If we are comfortable making our independent variables less interpretable
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们愿意使我们的独立变量不那么可解释
- en: 'In our case, we need to reduce the number of variables. For that, we are going
    to implement the code given in the following screenshot:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们需要减少变量的数量。为此，我们将实施以下截图中的代码：
- en: '![Principal component analysis (PCA)](img/B08394_03_39.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析（PCA）](img/B08394_03_39.jpg)'
- en: 'Figure 3.39: Code snippet for implementing PCA'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.39：实现PCA的代码片段
- en: As you can see in the preceding code, we are checking the amount of variance
    explained by each component. We need to consider more than 100 components to explain
    90% of the variance of our dataset.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在前面的代码中正在检查每个成分解释的方差量。我们需要考虑超过100个成分来解释我们数据集90%的方差。
- en: 'Here, I will consider a limited number of components because this decomposition
    is performed only to visualize the data. You can refer to the code shown in the
    following screenshot:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将考虑有限数量的成分，因为这种分解只是为了可视化数据。你可以参考以下截图中的代码：
- en: '![Principal component analysis (PCA)](img/B08394_03_40.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析（PCA）](img/B08394_03_40.jpg)'
- en: 'Figure 3.40: Code snippet for generating PCA decomposition graphs'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.40：生成PCA分解图形的代码片段
- en: As you can see, we have used PCA components using `PCA(n_components=50)`, and
    we have stored the values in dataframe `mat`, which we can use in future.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们使用了`PCA(n_components=50)`的PCA成分，并将值存储在数据框`mat`中，我们可以在未来使用。
- en: 'The output of the preceding code is in the form of graphs. So, you can refer
    to the following screenshot:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是图形形式。因此，你可以参考以下截图：
- en: '![Principal component analysis (PCA)](img/B08394_03_41.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析（PCA）](img/B08394_03_41.jpg)'
- en: 'Figure 3.41: Graphs for the PCA for each cluster'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.41：每个聚类的PCA图形
- en: Here, we have used `tight_layout`, which is the reason why the graphs shrank
    a bit.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`tight_layout`，这是图形缩小一点的原因。
- en: So far, we have performed enough EDA to help us generate a basic insight into
    the dataset. Now we will move on to the next section, where we will start building
    customer categories or customer segmentation. We will take into account all the
    findings that we have implemented so far.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经进行了足够的EDA，以帮助我们生成对数据集的基本洞察。现在我们将进入下一节，我们将开始构建客户类别或客户细分。我们将考虑我们迄今为止实施的所有发现。
- en: Generating customer categories
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成客户类别
- en: As you know, our first goal is to develop customer segmentation. From this section
    onward, we will focus mainly on how we can come up with customer segmentation.
    So far, we have done an analysis of orders, products, prices, and so on. Here,
    our main focus is on generating customer categories based on the insights that
    we got during EDA.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，我们的第一个目标是开发客户细分。从本节开始，我们将主要关注我们如何进行客户细分。到目前为止，我们已经对订单、产品、价格等方面进行了分析。在这里，我们的主要重点是依据我们在EDA过程中获得的洞察来生成客户类别。
- en: 'These are the steps that we are going to follow in order to develop the customer
    categories:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将遵循的步骤来开发客户类别：
- en: 'Formatting data:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据格式化：
- en: Grouping products
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组产品
- en: Splitting the dataset
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集拆分
- en: Grouping orders
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组订单
- en: 'Creating customer categories:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建客户类别：
- en: Data encoding
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据编码
- en: Generating customer categories
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成客户类别
- en: Now let's see what we are going to do in each of these steps.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看在每个步骤中我们将做什么。
- en: Formatting data
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 格式化数据
- en: 'As mentioned earlier, we will be using the findings that we generated during
    EDA. In the previous section, we generated five clusters for products. In order
    to perform the rest of the analysis, we will use this already generated list of
    keywords, matrices, and clusters. By using them, we will be generating a new categorical
    variable, `categ_product`. This variable indicates the cluster of each product.
    You can refer to the code snippet shown in the following screenshot:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用我们在EDA过程中生成的发现。在前一节中，我们为产品生成了五个聚类。为了执行剩余的分析，我们将使用这个已经生成的关键词列表、矩阵和聚类列表。通过使用它们，我们将生成一个新的分类变量`categ_product`。这个变量表示每个产品的聚类。您可以参考以下截图所示的代码片段：
- en: '![Formatting data](img/B08394_03_42.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![格式化数据](img/B08394_03_42.jpg)'
- en: 'Figure 3.42: Code snippet for generating new categorical variable categ_product'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.42：生成新分类变量categ_product的代码片段
- en: As you can see, the new variable indicates the cluster number for each data
    entry. Now let's group the products.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，新变量表示每个数据条目的聚类编号。现在让我们对产品进行分组。
- en: Grouping products
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分组产品
- en: 'You might wonder that if we have already developed the categories of the product,
    then why are we performing the grouping step here. Well, here, we will perform
    grouping in such a way that we can know what amount has been spent in each product
    category. For this, we will add five new variables, for example, categ_0, categ_1,
    categ_2, categ_3, and categ_4\. You can refer to the code snippet displayed in
    the following screenshot:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道，如果我们已经开发了产品的类别，那么为什么我们在这里执行分组步骤。在这里，我们将以这种方式进行分组，以便我们知道在每个产品类别中花费了多少钱。为此，我们将添加五个新变量，例如，categ_0、categ_1、categ_2、categ_3和categ_4。您可以参考以下截图显示的代码片段：
- en: '![Grouping products](img/B08394_03_43.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![分组产品](img/B08394_03_43.jpg)'
- en: 'Figure 3.43: Code snippet for generating the amount spent in each product category'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.43：生成每个产品类别的花费金额的代码片段
- en: 'Orders are split into multiple entries, so we need to use the basket price.
    This time, we will merge the basket price as well as the way it is distributed
    over five product categories. We will put all this information into the new dataframe.
    Refer to the following screenshot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 订单被分割成多个条目，因此我们需要使用篮子价格。这次，我们将合并篮子价格以及它在五个产品类别中的分布方式。我们将把所有这些信息放入新的数据框中。请参考以下截图：
- en: '![Grouping products](img/B08394_03_44.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![分组产品](img/B08394_03_44.jpg)'
- en: 'Figure 3.44: Code snippet for obtaining the distribution of basket prices for
    five clusters'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.44：获取五个聚类篮子价格分布的代码片段
- en: Finally, we have the basket price for each order, and we also know the price
    distribution over five clusters. The new dataframe is `basket_price`. Now let's
    move on to the next section.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有每个订单的篮子价格，我们也知道价格在五个聚类中的分布。新的数据框是`basket_price`。现在让我们进入下一节。
- en: Splitting the dataset
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分割数据集
- en: 'In this section, we will be using the dataframe `basket_price`, which contains
    data entries for the past 12 months. The second goal of this application is to
    predict the customer purchase behavior based on their first site visit or purchase.
    So, in order to achieve that goal right now, we will split the dataset. We will
    use 10 months'' dataset for training and 2 months'' dataset for testing. I''m
    including this step here because later on, we can use these training and testing
    datasets and you can easily get to use the new dataframe. You can refer to the
    code given in the following screenshot:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用包含过去12个月数据条目的数据框`basket_price`。本应用的第二个目标是根据客户的首次网站访问或购买预测客户购买行为。因此，为了现在实现这个目标，我们将分割数据集。我们将使用10个月的数据集进行训练，2个月的数据集进行测试。我包括这一步是因为稍后我们可以使用这些训练和测试数据集，您也可以轻松地使用新的数据框。您可以参考以下截图给出的代码：
- en: '![Splitting the dataset](img/B08394_03_45.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![分割数据集](img/B08394_03_45.jpg)'
- en: 'Figure 3.45: Code snippet for splitting the dataset using time'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.45：使用时间分割数据集的代码片段
- en: Now we will group the customers and their orders along with the basket price
    distribution.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将根据篮子价格分布对客户及其订单进行分组。
- en: Grouping orders
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分组订单
- en: 'Here, we will merge the customers and their orders so that we can learn which
    customer placed how many orders. We will also generate the minimum order amount,
    the maximum order amount, and the mean order amount. Refer to the code given in
    the following screenshot:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将合并客户及其订单，以便我们可以了解哪个客户下了多少订单。我们还将生成最小订单金额、最大订单金额和平均订单金额。请参考以下截图中的代码：
- en: '![Grouping orders](img/B08394_03_46.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![订单分组](img/B08394_03_46.jpg)'
- en: 'Figure 3.46: Code snippet for generating order-wise stats for each customer'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.46：生成每个客户订单统计信息的代码片段
- en: 'We will also generate two variables that indicate the number of days elapsed
    since the last purchase and the first purchase. The names of these variables are
    `FirstPurchase` and `LastPurchase`. Refer to the code snippet given in the following
    screenshot:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将生成两个变量，表示自最后一次购买和第一次购买以来过去的天数。这些变量的名称是`FirstPurchase`和`LastPurchase`。请参考以下截图中的代码片段：
- en: '![Grouping orders](img/B08394_03_47.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![订单分组](img/B08394_03_47.jpg)'
- en: 'Figure 3.47: Code snippet for generating elapsed days for the last and first
    purchase'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.47：生成最后和第一次购买过去天数的代码片段
- en: 'The customer categories in which we are interested are the ones that make only
    one order. One of our main objectives is to target these customers in such a way
    that we can retain them. We need to obtain the data for the number of customers
    that belong to this category. For that, refer to the code given in the following
    screenshot:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的客户类别是只下过一次订单的客户。我们的主要目标之一就是以这种方式定位这些客户，以便我们能够保留他们。我们需要获取属于这个类别的客户数量数据。为此，请参考以下截图中的代码：
- en: '![Grouping orders](img/B08394_03_48.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![订单分组](img/B08394_03_48.jpg)'
- en: 'Figure 3.48: Code snippet for generating the number of customers with one purchase'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.48：生成只有一个购买客户的数量的代码片段
- en: From the preceding code, we can find out that 40% of the customer base has placed
    only one order, and we need to retain them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以发现40%的客户基础只下过一次订单，我们需要保留他们。
- en: Now let's build the customer categories.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建客户类别。
- en: Creating customer categories
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建客户类别
- en: 'Basically, we will be generating the customer segmentation here. So, we will
    work on achieving the first goal of the chapter in this section. We will build
    customer segmentation based on the customers'' purchase pattern. There are two
    steps in this section:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们将在这里生成客户细分。因此，我们将在这个部分努力实现章节的第一个目标。我们将基于客户的购买模式构建客户细分。本节有两个步骤：
- en: Data encoding
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据编码
- en: Generating customer categories or customer segmentation
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成客户类别或客户细分
- en: We will start with data encoding.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数据编码开始。
- en: Data encoding
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据编码
- en: We will be generating the dataframe that contains the summary of all operations
    we have performed so far. Each record of this dataframe is associated with a single
    client. We can use this information to characterize various types of customers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成包含我们迄今为止所执行的所有操作的汇总的dataframe。这个dataframe的每一条记录都与单个客户相关联。我们可以使用这些信息来描述各种类型的客户。
- en: 'The dataframe that we have generated has different variables. All these variables
    have different ranges and variations. So, we need to generate a matrix where these
    data entries are standardized. You can refer to the code given in the following
    screenshot:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的dataframe有不同的变量。所有这些变量都有不同的范围和变化。因此，我们需要生成一个矩阵，其中这些数据条目被标准化。你可以参考以下截图中的代码：
- en: '![Data encoding](img/B08394_03_49.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![数据编码](img/B08394_03_49.jpg)'
- en: 'Figure 3.49: Code snippet for generating summary data entries for each client'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.49：为每个客户生成汇总数据条目的代码片段
- en: 'Before creating customer segmentation, we need to create the base. This base
    should include important variables. We need to include a small number of important
    variables. In order to select the important variables, we will be using principal
    component analysis. So, that we can describe the segmentation accurately. We will
    use PCA for this task. The code snippet is given in the following screenshot:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建客户细分之前，我们需要创建基础。这个基础应该包括重要变量。我们需要包括少量重要变量。为了选择重要变量，我们将使用主成分分析。这样我们就可以准确地描述细分。我们将为此任务使用PCA。代码片段如下截图所示：
- en: '![Data encoding](img/B08394_03_50.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![数据编码](img/B08394_03_50.jpg)'
- en: 'Figure 3.50: Code snippet for PCA in order to generate the customer segmentation'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.50：用于生成客户细分的PCA代码片段
- en: Here, we can see that there are eight principal components. Now let's move on
    to the next section, where we will generate the customer segmentation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有八个主成分。现在让我们进入下一节，我们将在这里生成客户细分。
- en: Generating customer categories
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成客户类别
- en: 'We will be using the k-means clustering algorithm to generate segmentation.
    The number of clusters will be derived by using the silhouette score. We have
    used the silhouette score earlier, and by using the same method, we can derive
    the number of clusters. Here, we obtain 11 clusters based on the silhouette score.
    You can refer to the following screenshot:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用k-means聚类算法来生成细分。聚类数量将通过使用轮廓分数来推导。我们之前已经使用过轮廓分数，通过使用相同的方法，我们可以推导出聚类数量。在这里，根据轮廓分数，我们获得了11个聚类。您可以参考以下截图：
- en: '![Generating customer categories](img/B08394_03_51.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![生成客户类别](img/B08394_03_51.jpg)'
- en: 'Figure 3.51: Code snippet for generating customer segmentations'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.51：生成客户细分代码片段
- en: As you can see, there is a large difference in the size of the segmentation,
    so we need to analyze the components of the clusters using PCA.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，细分的大小存在很大差异，因此我们需要使用PCA分析来分析聚类的成分。
- en: PCA analysis
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PCA分析
- en: 'We will use six components here. The code snippet and graphical representation
    of PCA for 11 clusters are given in the following screenshot:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用六个成分。11个聚类的PCA代码片段和图形表示在以下截图给出：
- en: '![PCA analysis](img/B08394_03_52.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![PCA分析](img/B08394_03_52.jpg)'
- en: 'Figure 3.52: Code snippet for implementing PCA and generating graphs'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.52：实现PCA和生成图表的代码片段
- en: 'As an output, the following graphs have been generated:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出，以下图表已经生成：
- en: '![PCA analysis](img/B08394_03_53.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![PCA分析](img/B08394_03_53.jpg)'
- en: 'Figure 3.53: Graphs of PCA for customer segmentation'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.53：客户细分的PCA图表
- en: I have displayed only three graphs here. In the code, there are nine graphs.
    When you run the code, you can see them all. Note that the first component separates
    the tiniest cluster from the rest. For this dataset, we can say that there will
    always be a representation in which two segments will appear to be distinct. Now
    let's obtain silhouette scores.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里只显示了三个图表。在代码中，有九个图表。当您运行代码时，您可以看到它们全部。请注意，第一个成分将最小的聚类与其他聚类分开。对于这个数据集，我们可以说，总会有一种表示，其中两个细分将看起来是不同的。现在让我们获取轮廓分数。
- en: Analyzing the cluster using silhouette scores
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用轮廓分数分析聚类
- en: 'In this section, we will generate the silhouette score for each cluster. This
    will indicate the quality of the separation of data samples. You can refer to
    the code snippet and graph shown in the following screenshot:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为每个聚类生成轮廓分数。这将表明数据样本分离的质量。您可以参考以下截图中的代码片段和图表：
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_54.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![使用轮廓分数分析聚类](img/B08394_03_54.jpg)'
- en: 'Figure 3.54: Code snippet for generating graphs for silhouette scores'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.54：生成轮廓分数图表的代码片段
- en: From the preceding graphs, we can ensure that all the clusters are disjointed.
    Now we need to learn more about the habits of the customers of each cluster. To
    do that, we will add variables that define the cluster to which each customer
    belongs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以确保所有聚类都是不相交的。现在我们需要更多地了解每个聚类的客户习惯。为此，我们将添加定义每个客户所属聚类的变量。
- en: 'For this, we will be generating a new dataframe, `selected_customers`. After
    generating the new dataframe, we will average the content of the dataframe. This
    will provide us with the average basket price, total visits, and so on. You can
    refer to the code shown in the following screenshot:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将生成一个新的数据框，`selected_customers`。在生成新的数据框后，我们将平均数据框的内容。这将为我们提供平均篮子价格、总访问量等。您可以参考以下截图中的代码：
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_55.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![使用轮廓分数分析聚类](img/B08394_03_55.jpg)'
- en: 'Figure 3.55: Code snippet for storing the habits of the customers'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.55：存储客户习惯的代码片段
- en: 'Now we need to reorganize the content of the dataframe. We will be considering
    two points here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要重新组织数据框的内容。我们将考虑以下两点：
- en: We need to reorganize the data based on the amount spent in each product category
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要根据每个产品类别的花费金额重新组织数据
- en: After that, we will reorganize the content based on the total amount spent
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将根据总花费金额重新组织内容
- en: 'You can take a look at the implementation shown in the following screenshot:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看以下截图中的实现：
- en: '![Analyzing the cluster using silhouette scores](img/B08394_03_56.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![使用轮廓分数分析聚类](img/B08394_03_56.jpg)'
- en: 'Figure 3.56: Code snippet for reorganizing the dataset'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.56：重新组织数据集的代码片段
- en: As you can see, we have obtained the behavior of the customer for each segment.
    Now we can recommend the items based on these characteristics. We can design the
    marketing campaign based on the generated facts.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经获得了每个细分市场的客户行为。现在我们可以根据这些特征进行推荐。我们可以根据生成的数据设计营销活动。
- en: The particular marketing strategy can be applied to customers who belong to
    cluster 4 and cluster 8\. We should recommend the premium products to the cluster
    1 clients.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的营销策略可以应用于属于第4个和第8个聚类的客户。我们应该向第1个聚类的客户推荐高端产品。
- en: So far, we have achieved our first goal. Now it's time to aim for the second
    goal. So let's begin!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了我们的第一个目标。现在是我们瞄准第二个目标的时候了。让我们开始吧！
- en: Classifying customers
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户分类
- en: Before we begin, let's have a refresher on what our goal is. This helps you
    understand things in a clearer manner. The objective is that we are going to build
    a classifier that will classify the customers into different customer segments
    that were established in the previous section. We also need one more feature.
    Our classifier should generate this classification result when the customer visits
    the platform for the first time. In order to implement this kind of functionality,
    we will be using various supervised machine learning algorithms. We will use the
    scikit-learn API.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们回顾一下我们的目标。这有助于您更清晰地理解事物。目标是我们要构建一个分类器，将客户分类到上一节中建立的不同客户细分市场。我们还需要一个额外的功能。我们的分类器应该在客户第一次访问平台时生成这个分类结果。为了实现这种功能，我们将使用各种监督机器学习算法。我们将使用scikit-learn
    API。
- en: 'In order to develop the baseline classifier, we need to perform the following
    steps:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发基线分类器，我们需要执行以下步骤：
- en: Defining the helper functions
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义辅助函数
- en: Splitting the data into training and testing
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: Implementing the Machine Learning (ML) algorithm
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施机器学习（ML）算法
- en: Defining helper functions
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义辅助函数
- en: 'Basically, we define a class named `class_fit` and then we define various functions
    that can help us when we train the ML model. These are the helper functions that
    we will be using:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们定义一个名为 `class_fit` 的类，然后定义各种函数，这些函数在训练机器学习模型时可以帮助我们。这些是我们将使用的辅助函数：
- en: The `train` function helps us train the model
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train` 函数帮助我们训练模型'
- en: The `predict` function helps us predict the result for the test dataset or the
    new data sample
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predict` 函数帮助我们预测测试数据集或新数据样本的结果。'
- en: The `grid_search` function helps us find out appropriate hyperparameters and
    the value of cross-validation(CV) folds
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`grid_search` 函数帮助我们找到合适的超参数和交叉验证（CV）折的值。'
- en: The `grid_fit` function helps us train the model using cross-validation and
    generate the optimal hyperparameters .
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`grid_fit` 函数帮助我们使用交叉验证训练模型并生成最佳超参数。'
- en: The `grid_predict` function helps us generate prediction as well as the accuracy
    score.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`grid_predict` 函数帮助我们生成预测以及准确度评分。'
- en: 'You can refer to the code snippet shown in the following screenshot:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下截图所示的代码片段：
- en: '![Defining helper functions](img/B08394_03_57.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![定义辅助函数](img/B08394_03_57.jpg)'
- en: 'Figure 3.57: Code snippet for the helper function'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.57：辅助函数的代码片段
- en: Now let's move on to the next section.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入下一节。
- en: Splitting the data into training and testing
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: 'We will be using the data that we have stored in the `selected_customers` dataframe.
    You can see some entries of the dataset on which we will apply the ML algorithm.
    Take a look at the following screenshot:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用存储在 `selected_customers` 数据框中的数据。您可以看到我们将应用机器学习算法的数据集的一些条目。请查看以下截图：
- en: '![Splitting the data into training and testing](img/B08394_03_58.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![将数据分为训练集和测试集](img/B08394_03_58.jpg)'
- en: 'Figure 3.58: Sample entries in the dataset'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.58：数据集的样本条目
- en: 'As you can see, we will predict the cluster number for the new customer, so
    we have stored that value as `Y`, and columns such as `mean, categ_0 to categ_4`
    are used as input features for the ML model, so we have stored them in the `X`
    variable. Now we need to split this data into training and testing. For that,
    we use the sklearn API `train_test_split()`. We are using 80% of the data for
    training and 20% of data for testing. Take a look at the following screenshot:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们将预测新客户的聚类编号，因此我们将该值存储为 `Y`，而 `mean, categ_0 到 categ_4` 等列被用作机器学习模型的输入特征，因此我们将它们存储在
    `X` 变量中。现在我们需要将此数据拆分为训练集和测试集。为此，我们使用 sklearn API 的 `train_test_split()`。我们使用80%的数据进行训练，20%的数据进行测试。请看以下截图：
- en: '![Splitting the data into training and testing](img/B08394_03_59.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![将数据拆分为训练集和测试集](img/B08394_03_59.jpg)'
- en: 'Figure 3.59: Code snippet for splitting the dataset into training and testing'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.59：将数据集拆分为训练集和测试集的代码片段
- en: We have the training and testing datasets with us. Now, we need to start implementing
    the ML algorithm.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了训练集和测试集。现在，我们需要开始实现机器学习算法。
- en: Implementing the Machine Learning (ML) algorithm
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现机器学习（ML）算法
- en: 'For the baseline approach, we will be implementing the Support Vector machine
    (SVM) classifier. We will be using helper functions that we have previously defined.
    Here, I will create an instance of the class and call the methods that we have
    declared previously. Take a look at the code snippet shown in the following screenshot:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线方法，我们将实现支持向量机（SVM）分类器。我们将使用之前定义的辅助函数。在这里，我将创建一个类的实例并调用之前声明的函数。请看以下截图中的代码片段：
- en: '![Implementing the Machine Learning (ML) algorithm](img/B08394_03_60.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![实现机器学习（ML）算法](img/B08394_03_60.jpg)'
- en: 'Figure 3.60: Code snippet for training the model using the SVM classifier'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.60：使用SVM分类器训练模型的代码片段
- en: As you can see in the code snippet, `svc` is the class instance. We are using
    linear SVM. We have used `grid_search` to search optimal hyperparameters as well
    as obtain the number of CV folds. After that, we have called the `grid_fit` method,
    which is used to train the ML model using our training dataset.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码片段所示，`svc` 是类实例。我们正在使用线性SVM。我们使用了 `grid_search` 来搜索最优超参数以及获取交叉验证的折数。之后，我们调用了
    `grid_fit` 方法，该方法用于使用我们的训练数据集来训练机器学习模型。
- en: This is the way we have implemented our baseline approach. Now let's test the
    result.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的基线方法实现方式。现在让我们测试一下结果。
- en: Understanding the testing matrix
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: We will be using the confusion matrix and the learning curve to evaluate the
    ML models. So before starting with the testing, we need to understand what the
    confusion matrix and the learning curve are. We will cover these concepts one
    by one.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用混淆矩阵和学习曲线来评估机器学习模型。所以在开始测试之前，我们需要了解混淆矩阵和学习曲线是什么。我们将逐一介绍这些概念。
- en: Confusion matrix
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: When we are implementing a multi-class classifier, naturally, we have multiple
    classes and the number of data entries belonging to all the classes is different,
    so during testing, we need to know whether the classifier performs equally well
    for all the classes or whether it is biased toward some classes. This analysis
    can be done using the confusion matrix. It will have a count of how many data
    entries are correctly classified and how many are misclassified.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实现多类分类器时，自然会有多个类，并且属于所有类的数据条目数量不同，所以在测试时，我们需要知道分类器是否对所有类都表现良好，或者它是否偏向某些类。这种分析可以使用混淆矩阵来完成。它将记录有多少数据条目被正确分类，有多少被错误分类。
- en: Let's take an example. Say, there is a total of 10 data entries that belong
    to a class, and the label for that class is 1\. Now when we generate the prediction
    from our ML model, we will check how many data entries out of the 10 entries get
    the predicted class label 1\. Suppose six data entries are correctly classified
    and get the class label 1\. In this case, for six entries, the *predicted label*
    and *True label* is the same, so the accuracy is 60%, whereas for the remaining
    data entries, the ML model misclassifies them. The ML model predicts class labels
    other than 1.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子。假设有10个属于一个类的数据条目，该类的标签为1。现在当我们从我们的机器学习模型生成预测时，我们将检查10个条目中有多少被预测为类标签1。假设有六个数据条目被正确分类并得到类标签1。在这种情况下，对于六个条目，*预测标签*和*真实标签*是相同的，所以准确率是60%，而对于剩余的数据条目，机器学习模型将它们错误分类。机器学习模型预测的类标签不是1。
- en: 'From the preceding example, you can see that the confusion matrix gives us
    an idea about how many data entries are classified correctly and how many are
    misclassified. We can explore the class-wise accuracy of the classifier. Take
    a look at the following screenshot:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中，你可以看到混淆矩阵让我们了解有多少数据条目被正确分类，以及有多少被错误分类。我们可以探索分类器的类别准确度。请看以下截图：
- en: '![Confusion matrix](img/B08394_03_61.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵](img/B08394_03_61.jpg)'
- en: 'Figure 3.61: Example of confusion matrix'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.61：混淆矩阵的示例
- en: Now let's take a look at the learning curve.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下学习曲线。
- en: Learning curve
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线
- en: 'We are plotting two lines here. One line indicates the training score, and
    the other line indicates the testing score. Here, the training and testing scores
    determine cross-validated training and testing scores for different training dataset
    sizes. By using this learning curve, we can monitor whether the ML model is converging
    properly or not. Both the CV score and the training score will help us determine
    whether training is going in the right direction or the ML model is suffering
    from over-fitting or under-fitting. With the increased size of dataset, if the
    CV score and training scores achieve a low score, then it means that the training
    was not performed in a proper manner. However, if the CV score and training score
    increase with the increased size of dataset, then it means that the training is
    moving in the right direction. Refer to the following screenshot:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里绘制了两条线。一条线表示训练分数，另一条线表示测试分数。在这里，训练和测试分数决定了不同训练数据集大小的交叉验证训练和测试分数。通过使用这个学习曲线，我们可以监控ML模型是否正确收敛。CV分数和训练分数都将帮助我们确定训练是否朝着正确的方向进行，或者ML模型是否过度拟合或欠拟合。随着数据集大小的增加，如果CV分数和训练分数达到低分，那么这意味着训练没有以正确的方式进行。然而，如果CV分数和训练分数随着数据集大小的增加而增加，那么这意味着训练正在朝着正确的方向进行。请参考以下截图：
- en: '![Learning curve](img/B08394_03_62.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![学习曲线](img/B08394_03_62.jpg)'
- en: 'Figure 3.62: Bad and good examples for the learning curve'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.62：学习曲线的良例和劣例
- en: Now that we have understood the basic intuition behind the testing matrix, we
    can start testing our baseline approach.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了测试矩阵背后的基本直觉，我们可以开始测试我们的基线方法。
- en: Testing the result of the baseline approach
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试基线方法的结果
- en: 'In this section, we will test the baseline model using the following approaches:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用以下方法测试基线模型：
- en: Generating the accuracy score for the classifier
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成分类器的准确度分数
- en: Generating the confusion matrix for the classifier
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成分类器的混淆矩阵
- en: Generating the learning curve for the classifier
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成分类器的学习曲线
- en: Generating the accuracy score for classifier
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成分类器的准确度分数
- en: 'First, we will use `grid_predict` to generate the accuracy score for testing
    the dataset. We will check the accuracy of the SVM algorithm. For that, the code
    snippet is given in the following screenshot:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `grid_predict` 来生成测试数据集的准确度分数。我们将检查SVM算法的准确度。为此，以下截图给出了代码片段：
- en: '![Generating the accuracy score for classifier](img/B08394_03_63.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![生成分类器的准确度分数](img/B08394_03_63.jpg)'
- en: 'Figure 3.63: Code snippet for generating the accuracy score'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.63：生成准确度分数的代码片段
- en: We got a 79.50% precision for the baseline approach. Now let's look at the quality
    of the prediction using the confusion matrix.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线方法，我们得到了79.50%的精确度。现在让我们通过混淆矩阵来看看预测的质量。
- en: Generating the confusion matrix for the classifier
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成分类器的混淆矩阵
- en: 'Now we will generate the confusion matrix, which will give us a fair idea about
    which class is classified correctly and which classes have misclassified the data
    most of the time. To generate the confusion matrix, refer to the code given in
    the following screenshot:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将生成混淆矩阵，这将给我们一个关于哪个类别被正确分类以及哪些类别大多数时候错误分类数据的公平概念。要生成混淆矩阵，请参考以下截图中的代码：
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_64.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![生成分类器的混淆矩阵](img/B08394_03_64.jpg)'
- en: 'Figure 3.64: Code snippet for generating the confusion matrix'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.64：生成混淆矩阵的代码片段
- en: 'We have used the `confusion_matrix` API for sklearn. To draw the plot, we will
    define a method with the name `plot_confusion_matrix`. With the help of the preceding
    code, we have generated the confusion matrix given in the following screenshot:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了sklearn的`confusion_matrix` API。为了绘制图表，我们将定义一个名为`plot_confusion_matrix`的方法。借助前面的代码，我们已经生成了以下截图中的混淆矩阵：
- en: '![Generating the confusion matrix for the classifier](img/B08394_03_65.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![为分类器生成混淆矩阵](img/B08394_03_65.jpg)'
- en: 'Figure 3.65: Confusion matrix for the baseline approach'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.65：基准方法的混淆矩阵
- en: As you can see, the classifier was able to classify the data into class labels
    0, 2, 4, 6, and 10 accurately, whereas for class labels 1, 5, 7, and 8, the classifier
    is not performing so well.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，分类器能够准确地将数据分类为标签 0、2、4、6 和 10，而对于标签 1、5、7 和 8，分类器的表现并不理想。
- en: Let's draw the learning curve for the baseline approach.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制基准方法的 learning curve。
- en: Generating the learning curve for the classifier
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成分类器的学习曲线
- en: 'A learning curve indicates whether the classifier is facing the over-fitting
    or under-fitting issue. The `plot_learning_curve` method is used to draw the learning
    curve for the classifier. You can refer to the code snippet in the following screenshot:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线表明分类器是否面临过拟合或欠拟合的问题。`plot_learning_curve` 方法用于绘制分类器的学习曲线。您可以通过以下截图中的代码片段进行参考：
- en: '![Generating the learning curve for the classifier](img/B08394_03_66.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![为分类器生成学习曲线](img/B08394_03_66.jpg)'
- en: 'Figure 3.66: Code snippet for generating the learning curve for the baseline
    approach'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.66：生成基准方法学习曲线的代码片段
- en: 'The learning curve is displayed in the following screenshot:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线显示在以下截图：
- en: '![Generating the learning curve for the classifier](img/B08394_03_67.jpg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![为分类器生成学习曲线](img/B08394_03_67.jpg)'
- en: 'Figure 3.67: Learning curve for the baseline approach'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.67：基准方法的学习曲线
- en: As you can see, the CV curve converges at the same limit when we increase the
    sample size. This means that we have low variance and we are not suffering from
    over-fitting. Variance is the value that indicates how much our target function
    will change if we will provide different training dataset. Ideally the value of
    the target function is derived from the training dataset by Machine Learning algorithm
    however the value of estimated function should not change too much if we use another
    training dataset. Minor change (minor variance) in the estimated function is expected.
    Here, the accuracy score has a low bias, which means the model is not facing the
    under-fitting issue as well.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当我们增加样本大小时，CV 曲线收敛到相同的极限。这意味着我们具有低方差，并且没有过拟合的问题。方差是表示如果我们提供不同的训练数据集，我们的目标函数将如何变化的价值。理想情况下，目标函数的值是通过机器学习算法从训练数据集中推导出来的，然而，如果我们使用另一个训练数据集，估计函数的值不应该变化太多。预计估计函数会有轻微的变化（轻微的方差）。在这里，准确度分数具有低偏差，这意味着模型也没有面临欠拟合的问题。
- en: Problems with the baseline approach
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准方法的问题
- en: 'In this section, we will be discussing the problems we are facing with the
    baseline approach so that we can optimize the current approach. The problems are
    as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论我们在基准方法中遇到的问题，以便优化当前方法。问题如下：
- en: The precision score is low. There is scope for improvement.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度分数较低。有改进的空间。
- en: We need to try other ML algorithms so that we can compare the results. Later
    on, if there is a need, then we can build the voting mechanism.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要尝试其他机器学习算法，以便进行比较。稍后，如果有需要，我们可以构建投票机制。
- en: Basically, in the revised approach, we need to try out various ML algorithms
    so that we will be sure which algorithm we can use and which ones we should not
    use.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进方法中，基本上我们需要尝试各种机器学习算法，以确保我们知道哪些算法可以使用，哪些不应该使用。
- en: Optimizing the baseline approach
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化基准方法
- en: 'In this section, we will take all the problems into consideration and discuss
    the approach through which we will increase the accuracy of our classifier. As
    discussed in the previous section, we need to implement other ML algorithms. These
    are the six algorithms that we are going to implement with the revised approach:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑所有问题，并讨论我们将通过哪些方法来提高分类器的准确率。如前节所述，我们需要实现其他机器学习算法。以下是我们要用改进方法实现的六个算法：
- en: Logistic regression
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: K-nearest neighbor
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻
- en: Decision tree
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forest
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Adaboost classifier
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost 分类器
- en: Gradient boosting classifier
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升分类器
- en: Based on the precision score of all the preceding algorithms, we will decide
    which algorithm can be used and which can't be used.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所有先前算法的精确度分数，我们将决定哪些算法可以使用，哪些不能使用。
- en: Without wasting time, let's start implementing the revised approach.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 不浪费任何时间，让我们开始实现改进方法。
- en: Building the revised approach
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建改进方法
- en: In this section, we will implement the various ML algorithms, check their precision
    score, and monitor their learning curve. There is a total of six ML algorithms
    that will be used to identify which one is the best suited for our application.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现各种机器学习算法，检查它们的精确度得分，并监控它们的学习曲线。总共有六种机器学习算法将被用于确定哪一种最适合我们的应用。
- en: Implementing the revised approach
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现改进的方法
- en: 'In this section, we will be implementing logistic regression, K-nearest neighbor,
    decision tree, random forest, Adaboost, and gradient descent. In order to implement
    this, we will be using the helper class that we built earlier. You can take a
    look at the code snippet given in the following screenshot:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现逻辑回归、K最近邻、决策树、随机森林、Adaboost和梯度下降。为了实现这一点，我们将使用我们之前构建的辅助类。您可以在下面的屏幕截图中查看提供的代码片段：
- en: '![Implementing the revised approach](img/B08394_03_68.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![实现改进的方法](img/B08394_03_68.jpg)'
- en: 'Figure 3.68: Code snippet for performing training using various ML classifiers'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.68：使用各种机器学习分类器进行训练的代码片段
- en: We have already generated a precision score for all the classifiers. We can
    see random forest and gradient-boosting classifiers with great precision. However,
    we have still not checked their learning curve. First, we will check their learning
    curve and then conclude whether any classifier has been facing the over-fitting
    or under-fitting issue.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为所有分类器生成了精确度得分。我们可以看到随机森林和梯度提升分类器具有很高的精确度。然而，我们还没有检查它们的学习曲线。首先，我们将检查它们的学习曲线，然后判断是否有分类器面临过拟合或欠拟合的问题。
- en: Testing the revised approach
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试改进的方法
- en: 'In this section, we will be checking the learning curves for all the classifiers.
    You can refer to the learning curves in the following screenshot:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将检查所有分类器的学习曲线。您可以参考以下屏幕截图中的学习曲线：
- en: '![Testing the revised approach](img/B08394_03_69.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![测试改进的方法](img/B08394_03_69.jpg)'
- en: 'Figure 3.69: Learning curve for various ML classifiers'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.69：各种机器学习分类器的学习曲线
- en: You can see that all the classifiers are trained appropriately. There is no
    under-fitting or over-fitting issue. With the increase data size, the scores are
    improving as well.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到所有分类器都得到了适当的训练。没有欠拟合或过拟合的问题。随着数据量的增加，得分也在提高。
- en: Problems with the revised approach
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进方法的问题
- en: The major problem with this approach is that we need to decide which algorithm
    we need to use and which one we should stop using. We will discard the Adaboost
    classifier as its precision score is too low.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的主要问题是，我们需要决定使用哪种算法，以及应该停止使用哪种算法。我们将放弃Adaboost分类器，因为它的精确度得分太低。
- en: There is another catch that I need to highlight here. There is no single classifier
    that works well for all class labels. There may be a classifier that works well
    for class label 0, whereas another may work well for class label 8\. I believe,
    we should not discard any other classifier. We need to come up with a voting mechanism.
    In more technical terms, we need to develop an ensemble model so that the quality
    of our prediction is great and accurate.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个需要注意的问题。没有一种分类器对所有类别标签都有效。可能有一种分类器对类别标签0有效，而另一种可能对类别标签8有效。我相信，我们不应该放弃任何其他分类器。我们需要提出一个投票机制。用更技术性的话来说，我们需要开发一个集成模型，以便我们的预测质量高且准确。
- en: Now we will take a look at what our approach will be in order to build a voting
    classifier that can give us the best possible accuracy.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨我们的方法，以便构建一个投票分类器，它可以给我们提供最佳可能的准确度。
- en: Understanding how to improve the revised approach
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解如何改进改进的方法
- en: As discussed, in order to improve the revised approach, we will be using a voting
    mechanism. For that, we will be using scikit-learn voting classifier APIs. First
    of all, we will use grid searching in order to generate appropriate hyperparameters
    for each classifier. After that, we will use voting-classifier APIs of scikit-learn
    and train the model. The approach is simple, so let's start implementing it.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了改进改进的方法，我们将使用投票机制。为此，我们将使用scikit-learn投票分类器API。首先，我们将使用网格搜索来为每个分类器生成适当的超参数。然后，我们将使用scikit-learn的投票分类器API来训练模型。方法很简单，让我们开始实现它。
- en: The best approach
  id: totrans-486
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳方法
- en: The classifier model that we will be generating in this approach should give
    us the best possible accuracy. We have already discussed this approach. If you
    are new to ensemble ML models, then let me give you a basic intuitive idea behind
    it. In layman's terms, ensemble ML models basically use a combination of various
    ML algorithms. What is the benefit of combining various ML models together? Well,
    we know there is no single classifier that can perfectly classify all the samples,
    so if we combine more than one classifier, then we can get more accuracy because
    the problem with one classifier can be overcome by another classifier. Due to
    this reason, we will use a voting classifier that is a type of ensemble classifier.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中我们将生成的分类器模型应该给我们提供最佳可能的准确度。我们之前已经讨论了这种方法。如果你对集成机器学习模型还不熟悉，那么让我给你一个基本的直观想法。用通俗易懂的话来说，集成机器学习模型基本上是使用各种机器学习算法的组合。将各种机器学习模型组合在一起有什么好处呢？我们知道没有单一的分类器可以完美地分类所有样本，所以如果我们结合多个分类器，那么我们可以获得更高的准确度，因为一个分类器的问题可以被另一个分类器克服。正因为如此，我们将使用投票分类器，这是一种集成分类器。
- en: Implementing the best approach
  id: totrans-488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: 'As you know, we use grid search and voting classifier APIs to implement the
    best approach. As discussed, first, we will use grid search to obtain the best
    possible hyperparameters and then use the voting classifier API. The step-by-step
    implementation is given in the following screenshot:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，我们使用网格搜索和投票分类器API来实现最佳方法。正如讨论的那样，首先，我们将使用网格搜索来获得最佳的超参数，然后使用投票分类器API。逐步实现如下屏幕截图所示：
- en: '![Implementing the best approach](img/B08394_03_70.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![实施最佳方法](img/B08394_03_70.jpg)'
- en: 'Figure 3.70: Code snippet for the best approach'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.70：最佳方法的代码片段
- en: As you can see, we get 90% precision for this approach. This time, we need to
    test the approach on our hold out corpus of two months so that we can find out
    how the voting classifier is performing on the unseen dataset.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这种方法达到了90%的精确度。这次，我们需要在我们的两个月保留数据集上测试这种方法，以便我们可以找出投票分类器在未见数据集上的表现如何。
- en: In the next section, we will be testing this approach.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将测试这种方法。
- en: Testing the best approach
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试最佳方法
- en: 'We test our ML model on 20% of the dataset, which we put aside before even
    starting the training. This dataset is kind of a dev dataset for us. For training,
    we have considered 10 months'' dataset. Now it is time to test the model on the
    hold out corpus. Here, our hold-out corpus consists of 2 months'' data entries.
    These are the steps that we need to implement:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在20%的数据集上测试我们的机器学习模型，这个数据集在我们开始训练之前就已经被留出来了。这个数据集对我们来说相当于一个开发数据集。对于训练，我们考虑了10个月的数据集。现在是我们测试模型在保留数据集上的时间了。在这里，我们的保留数据集由2个月的数据条目组成。以下是我们需要实施的步骤：
- en: Transforming the hold-out corpus in the form of the training dataset
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将保留数据集转换为训练数据集的形式
- en: Converting the transformed dataset into a matrix form
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将转换后的数据集转换为矩阵形式
- en: Generating the predictions
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成预测结果
- en: So let's start with the first step.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始。
- en: Transforming the hold-out corpus in the form of the training dataset
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将保留数据集转换为训练数据集的形式
- en: First of all, we need to convert the data that resides in the `set_test` dataframe
    in the form of the training dataset. For that, we will store the copy in the new
    dataframe with the name `basket_price`.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将存储在`set_test`数据框中的数据转换为训练数据集的形式。为此，我们将将其副本存储在新数据框中，命名为`basket_price`。
- en: 'Now we will generate the user characteristic data with the help of the same
    operation that we perform for the baseline approach. Don''t worry. When you see
    the code, you will remember the steps that we performed earlier. After transforming
    the dataset, we will store it in the dataframe, `transactions_per_user`. You can
    refer to the code snippet shown in the following screenshot:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用与基线方法相同的操作来生成用户特征数据。不用担心。当你看到代码时，你会记得我们之前执行的步骤。在转换数据集后，我们将将其存储在数据框`transactions_per_user`中。你可以参考以下屏幕截图所示的代码片段：
- en: '![Transforming the hold-out corpus in the form of the training dataset](img/B08394_03_71.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![将保留数据集转换为训练数据集的形式](img/B08394_03_71.jpg)'
- en: 'Figure 3.71: Code snippet for transforming the test dataset into the same form
    of training dataset'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.71：将测试数据集转换为与训练数据集相同形式的代码片段
- en: Now let's convert the dataset into a matrix form.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将数据集转换为矩阵形式。
- en: Converting the transformed dataset into a matrix form
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将转换后的数据集转换为矩阵形式
- en: 'Our classifiers take the matrix as an input, so we need to convert the transformed
    dataset into the matrix format. For that, we will use the code snippet shown in
    the following screenshot:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器以矩阵作为输入，因此我们需要将转换后的数据集转换为矩阵格式。为此，我们将使用以下截图所示的代码片段：
- en: '![Converting the transformed dataset into a matrix form](img/B08394_03_72.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![将转换后的数据集转换为矩阵形式](img/B08394_03_72.jpg)'
- en: 'Figure 3.72: Code snippet for converting the test dataset into the matrix format'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.72：将测试数据集转换为矩阵格式的代码片段
- en: We are using a basic type conversion here.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用基本的类型转换。
- en: Generating the predictions
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成预测
- en: 'In this section, we will be generating the precision score using voting classifiers.
    So, in order to generate the prediction for the test dataset, we need to use the
    code snippet given in the following screenshot:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用投票分类器生成精确分数。因此，为了生成测试数据集的预测，我们需要使用以下截图给出的代码片段：
- en: '![Generating the predictions](img/B08394_03_73.jpg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![生成预测](img/B08394_03_73.jpg)'
- en: 'Figure 3.73: Code snippet for generating the precision score for the test dataset'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.73：生成测试数据集精确分数的代码片段
- en: As you can see, we will achieve 76% of accuracy on our hold-out corpus. This
    is nice because we just use 10 months of data to build this model. By using 10
    months' dataset, we achieve the best possible accuracy for this domain. If we
    consider more number of datarecords, then we can still improve the results. This
    can be an exercise for you guys to consider more datasets and improvise the result.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们将在我们的保留语料库上达到76%的准确率。这很好，因为我们只用了10个月的数据来构建这个模型。通过使用10个月的数据库，我们达到了这个领域的最佳可能准确率。如果我们考虑更多的数据记录，我们仍然可以改进结果。这可以成为你们考虑更多数据集并改进结果的一个练习。
- en: Customer segmentation for various domains
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 各领域的客户细分
- en: Note that we are considering e-commerce data here, but you can consider other
    datasets of various domains. You can build customer segmentation for a company
    providing travel services, financial services, and so on. The data points will
    vary from domain to domain.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里考虑的是电子商务数据，但你也可以考虑其他各种领域的数据集。你可以为提供旅行服务、金融服务等公司构建客户细分。数据点会因领域而异。
- en: For travel services, you could consider how frequently a user is booking flights
    or rooms using the traveling platform. Demographic and professional information
    helps a great deal, say, how many times a user uses promotional offers. The data
    for user activity is important as well.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旅行服务，你可以考虑用户使用旅行平台预订航班或房间的频率。人口统计信息和专业信息有很大帮助，例如，用户使用促销优惠的次数。用户活动数据同样重要。
- en: 'If you are building a segmentation application for the financial domain, then
    you can consider the data points such as: the transaction history of the account
    holder, for example, the frequency of using a debit card or a credit card, per-month
    income, per-month expenditure, the average balance the customer is maintaining
    in their bank account(s), the type of account user have, professional information
    of the customer, and so on. There are other common data points that you can consider
    for both the domains, such as the time spent on the website or the mobile app.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在为金融领域构建细分应用，那么你可以考虑以下数据点：账户持有人的交易历史，例如，使用借记卡或信用卡的频率，每月收入，每月支出，客户在银行账户中保持的平均余额，账户用户的类型，客户的专业信息，等等。对于这两个领域，还有其他一些常见的数据点可以考虑，例如在网站或移动应用上花费的时间。
- en: Right now, I will limit myself to these two domains, but you can perform customer
    segmentation for the telecom domain, the marketing domain, the educational domain,
    the entertainment domain, and so on.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我将限制自己在这两个领域，但你可以为电信领域、营销领域、教育领域、娱乐领域等执行客户细分。
- en: Summary
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'All the given analytics models we have developed so far are critical for running
    a successful business. In this chapter, we developed customer segmentation based
    on the behavior of the customers. In order to do that, we used various algorithms,
    such as SVM, linear regression, decision tree, random forest, gradient boosting,
    voting-based models, and so on. By using the voting-based model, we achieved the
    best possible accuracy. Customer segmentation analysis is important for small
    and midsized organizations because these analysis help them optimize their marketing
    strategy as well as significantly improve the customer acquisition cost. I developed
    the code for the customer churn analysis, available at: [https://github.com/jalajthanaki/Customer_churn_analysis](https://github.com/jalajthanaki/Customer_churn_analysis),
    and for customer life-time value analysis at: [https://github.com/jalajthanaki/Customer_lifetime_value_analysis](https://github.com/jalajthanaki/Customer_lifetime_value_analysis)
    . You can refer to them to learn more about customer analytics. You can read about
    customer analytics at: [https://github.com/Acrotrend/Awesome-Customer-Analytics](https://github.com/Acrotrend/Awesome-Customer-Analytics).'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止开发的所有分析模型对于运营一家成功的业务至关重要。在本章中，我们根据客户的行为开发了客户细分。为了做到这一点，我们使用了各种算法，例如支持向量机（SVM）、线性回归、决策树、随机森林、梯度提升、基于投票的模型等等。通过使用基于投票的模型，我们实现了最佳可能的准确率。客户细分分析对于小型和中型组织非常重要，因为这些分析有助于它们优化其营销策略，以及显著降低客户获取成本。我为客户流失分析开发了代码，可在以下链接找到：[https://github.com/jalajthanaki/Customer_churn_analysis](https://github.com/jalajthanaki/Customer_churn_analysis)，以及客户终身价值分析在以下链接：[https://github.com/jalajthanaki/Customer_lifetime_value_analysis](https://github.com/jalajthanaki/Customer_lifetime_value_analysis)。您可以参考它们来了解更多关于客户分析的信息。您可以在以下链接阅读有关客户分析的内容：[https://github.com/Acrotrend/Awesome-Customer-Analytics](https://github.com/Acrotrend/Awesome-Customer-Analytics)。
- en: In the upcoming chapter, we will build a recommendation system that is specific
    to e-commerce products. We will build a recommendation application that will recommend
    books to users based on their browsing and purchasing activities on the platform.
    We will implement various techniques to build the best possible recommendation
    engine. So keep reading!
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的章节中，我们将构建一个针对电子商务产品的特定推荐系统。我们将构建一个推荐应用，根据用户在平台上的浏览和购买活动向他们推荐书籍。我们将实施各种技术来构建最佳可能的推荐引擎。所以请继续阅读！
