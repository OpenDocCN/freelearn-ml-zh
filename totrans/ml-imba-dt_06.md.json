["```py\nimport torch\n# Create a tensor with specific data\nt1 = torch.tensor([[1, 2], [3, 4]])\n# Create a 2x3 tensor filled with zeros\nt2 = torch.zeros(2, 3)\n# Create a 2x3 tensor filled with random values\nt3 = torch.randn(2, 3)\n```", "```py\n# moving the tensor to GPU (assuming a GPU with CUDA support is available)\nx = torch.rand(2,2).to(\"cuda\") \ny = torch.rand(2,2) # this tensor remains on the CPU\nx+y     # adding a GPU tensor & a CPU tensor will lead to an error\n```", "```py\n--------------------------------------------------------------------------\nRuntimeError Traceback (most recent call last)\n----> 1 x+y\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n```", "```py\n# Create a tensor with autograd enabled\nx = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n# Perform operations on the tensor\ny = x + 2\nz = y * y * 3\n# Compute gradients with respect to the input tensor x\nz.backward(torch.ones_like(x))\n# Display the gradients\nprint(x.grad)\n```", "```py\ntensor([[18., 24.],\n        [30., 36.]])\n```", "```py\ntensor([[18., 24.],\n        [30., 36.]])\n```", "```py\nimport torch\nimport torch.optim as optim\nclass Net(torch.nn.Module):\n     def __init__(self):\n           super(Net, self).__init__()\n           self.fc1 = torch.nn.Linear(4, 10)\n           self.fc2 = torch.nn.Linear(10, 3)\n     def forward (self, x):\n           x = torch.relu(self.fc1(x))\n           x = self.fc2(x)\n           return x\n# Create an instance of the network\nnet = Net()\n# Define a loss function and an optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n```", "```py\n# Example dataset\ninputs = torch.randn(100, 4)\ntargets = torch.randint(0, 3, (100,))\n# Train for 100 epochs\nfor epoch in range(100):\n     # Forward pass\n     outputs = net(inputs)\n     # Compute loss\n     loss = criterion(outputs, targets)\n     # Backward pass and optimization\n     # reset gradients of model parameters\n     optimizer.zero_grad()\n     # compute the gradient of loss w.r.t model parameters\n     loss.backward()\n     # update model parameters based on computed gradients\n     optimizer.step()\n     # Print the loss for this epoch\n     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n```", "```py\nEpoch 1, Loss: 1.106634497642517\nEpoch 2, Loss: 1.1064313650131226\n…\nEpoch 100, Loss: 1.026018738746643\n```"]