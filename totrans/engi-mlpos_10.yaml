- en: 'Chapter 8: APIs and Microservice Management'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about APIs and microservice management. So far,
    we have deployed ML applications that are served as APIs. Now we will look into
    how to develop, organize, manage, and serve APIs. You will learn the principles
    of API and microservice design for ML inference so that you can design your own
    custom ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn by doing as we build a microservice using FastAPI
    and Docker and serve it as an API. For this, we will go through the fundamentals
    of designing an API and microservice for an ML model trained previously (in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*).
    Lastly, we will reflect on some key principles, challenges, and tips to design
    a robust and scalable microservice and API for test and production environments.
    The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to APIs and microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for microservices for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Old is gold – REST API-based microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on implementation of serving an ML model as an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a microservice using Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the API service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to APIs and microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: APIs and microservices are powerful tools that help to enable your **ML** (**ML**)
    models to become useful in production or legacy systems for serving the models
    or communicating with other components of the system. Using APIs and microservices,
    you can design a robust and scalable ML solution to cater to your business needs.
    Let's take a look at what APIs and microservices are and how they realize your
    model's potential in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: What is an Application Programming Interface (API)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An **API** is the gateway that enables developers to communicate with an application.
    APIs enable two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to an application's data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of an application's functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By accessing and communicating with application data and functionalities, APIs
    have enabled the world's electronics, applications, and web pages to communicate
    with each other in order to work together to accomplish business or operations-centric
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Workings of an API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Workings of an API
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8.1*, we can see the role of an API as it enables access to application
    data (from the database) and communication with third parties or other applications
    such as mobile applications (for mobile users), weather applications (on mobile
    or the web), electric cars, and so on. APIs have been in operation since the dawn
    of computers, intending to enable inter-application communication. Over time,
    we have seen developers come to a consensus with protocols such as **Simple Object
    Access Protocol** (**SOAP**) and **Representational State Transfer** (**REST**)
    in the early 2000s. In recent years, a generation of new types of API protocols
    have been developed, such as **Remote Procedure Call** (**RPC**) and GraphQL as
    seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.1 – API protocols comparison'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 – API protocols comparison
  prefs: []
  type: TYPE_NORMAL
- en: It is valuable to understand the mainstream API protocols if you are a developer
    of applications (hosted on the cloud or communicating with other services). It
    helps you design your APIs as per your business or functionality needs. As a programmer,
    count yourself fortunate to have many API protocols at your disposal, as 20 years
    ago, only SOAP and REST were available. Now a variety of choices are at your disposal
    depending on your needs, for example, GraphQL, Thrift, and JSON-RPC. These protocols
    have various advantages and drawbacks, making it easy to find the best suited
    to your situation.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices are a modern way of designing and deploying apps to run a service.
    Microservices enable distributed applications rather than one big monolithic application
    where functionalities are broken up into smaller fragments (called microservices).
    A microservice is an individual application in a microservice architecture. This
    is contrary to centralized or monolithic architectures, where all functionalities
    are tied up together in one big app. Microservices have grown in popularity due
    to **Service-Oriented Architecture** (**SOA**), an alternative to developing traditional
    monolithic (singular and self-sufficient applications).
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices gained massive adoption as they enable developers to develop,
    integrate, and maintain applications with ease. Eventually, this comes down to
    the fact that individual functionalities are treated independently, at first permitting
    you to develop an individual functionality of a service step by step. Lastly,
    it allows you to work on each functionality independently while integrating the
    whole system to orchestrate the service. This way, you can add, improve, or fix
    it without risking breaking the entire application. Microservices are valuable
    for bigger companies since they allow teams to work on isolated things without
    any complicated organization. In *Figure 8.2*, we can see the difference between
    monoliths and microservices. Microservices enable distributed applications compared
    to monoliths, which are non-distributed applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Microservices versus monoliths'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Microservices versus monoliths
  prefs: []
  type: TYPE_NORMAL
- en: Software development teams are empowered to work independently and within well-understood
    service responsibilities. Microservices-based architecture encourages software
    development teams to take ownership of their services or modules. One possible
    downside to microservices-based architecture is if you break an application up
    into parts, there is a severe need for those parts to communicate effectively
    in order to keep the service running.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between APIs and microservices is fascinating as it has two
    sides. As a result of microservices-based architecture, an API is a direct outcome
    of implementing that architecture in your application. Whereas at the same time,
    an API is an essential tool for communicating between services in a microservices-based
    architecture to function efficiently. Let's have a look at the next section, where
    we will glance through some examples of ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: The need for microservices for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the need for microservices-based architecture for ML applications,
    let's look at a hypothetical use case and go through various phases of developing
    a ML application for the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A large car repair facility needs a solution to estimate the number of cars
    in the facility and their accurate positions. A bunch of IP cameras is installed
    in the repair stations for monitoring the facility. Design an ML system to monitor
    and manage the car repair facility.
  prefs: []
  type: TYPE_NORMAL
- en: Stage 1 – Proof of concept (a monolith)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A quick PoC is developed in a typical case using available data points and applying
    ML to showcase and validate the use case and prove to the business stakeholders
    that ML can solve their problems or improve their business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our hypothetical use case, a monolith Python app is developed that does
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetches streams from all cameras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determines the positions of cars (head or tail) from each camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregates all estimations into a facility state estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see in *Figure 8.3*, the app is dockerized and deployed to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Monolith ML application (PoC for hypothetical use case)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Monolith ML application (PoC for hypothetical use case)
  prefs: []
  type: TYPE_NORMAL
- en: All cameras are connected to this server via the local network. The algorithms
    for car position estimation and the facility state estimator work but need further
    improvements, and overall the PoC works. This monolith app is highly prone to
    crashing due to the instability of the cameras, the local network, and other errors.
    Such instabilities can be handled better by microservices. Let's see this in practice
    in stage 2.
  prefs: []
  type: TYPE_NORMAL
- en: Stage 2 – Production (microservices)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this stage, an application that is less prone to crashing is essential to
    run the car repair facility''s monitoring operations continuously. For this reason,
    a monolith application is replaced with microservices-based architecture as shown
    in *Figure 8.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Microservices (production-ready application for hypothetical
    use case)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Microservices (production-ready application for hypothetical use
    case)
  prefs: []
  type: TYPE_NORMAL
- en: 'The application is fragmented into multiple services in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Video stream collector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image processor**: This aggregates images – it receives, processes, and caches
    images, and generates packets for further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Position classifier**: Estimates a car''s position (head or tail) parked
    in the repair facility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facility setup estimator**: This asynchronously receives car position estimations
    and calibrates the facility setup and sends real-time data to the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cloud collects and stores data using MQTT (a standard lightweight, publish-subscribe
    network protocol that transports messages between devices). The data is portrayed
    on a dashboard for the car facility operators to analyze operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the communication between each microservice is facilitated using APIs.
    The advantages of microservice architecture are that if any of the services crash
    or errors take place, that particular microservice is spawned to replace the failed
    one to keep the whole service running. Secondly, each microservice can be maintained
    and improved continuously by a dedicated team (of data scientists, developers,
    and DevOps engineers), unlike coordinating teams, to work on a monolithic system.
  prefs: []
  type: TYPE_NORMAL
- en: Old is gold – REST API-based microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Old is gold. Plus, it's better to start somewhere where there are various API
    protocols. The **Representational State Transfer** (**REST**) protocol has become
    a gold standard for many applications over the years, and it's not so very different
    for ML applications today. The majority of companies prefer developing their ML
    applications based on the REST API protocol.
  prefs: []
  type: TYPE_NORMAL
- en: A REST API or RESTful API is based on REST, an architectural method used to
    communicate mainly in web services development.
  prefs: []
  type: TYPE_NORMAL
- en: 'RESTful APIs are widely used; companies such as Amazon, Google, LinkedIn, and
    Twitter use them. Serving our ML models via RESTful APIs has many benefits, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Serve predictions on the fly to multiple users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more instances to scale up the application behind a load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possibly combine multiple models using different API endpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate our model operating environment from the user-facing environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable microservices-based architecture. Hence, teams can work independently
    to develop and enhance the services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A RESTful API uses existing HTTP methodologies that are defined by the RFC 2616
    protocol. *Table 8.2* summarizes the HTTP methods in combination with their CRUD
    operations and purpose in ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.2 – REST API HTTP methods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 – REST API HTTP methods
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental HTTP methods are `GET`, `POST`, `PUT`, `PATCH`, and `DELETE`.
    These methods correspond to `create`, `read`, `update`, and `delete`. Using these
    methods, we can develop RESTful APIs to serve ML models. RESTful APIs have gained
    significant adoption due to drivers such as OpenAPI. The OpenAPI Specification
    is a standardized REST API description format. It has become a standardized format
    for humans and machines; it enables REST API understandability and provides extended
    tooling such as API validation, testing, and an interactive documentation generator.
    In practice, the OpenAPI file enables you to describe an entire API with critical
    information such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Available endpoints (`/names`) and operations on each endpoint (`GET /names`,
    `POST /names`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input and output for each operation (operation parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developer documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terms of use, license, and other information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find more about OpenAPI on this site: [https://swagger.io/specification/](https://swagger.io/specification/).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will develop a RESTful API to serve an ML model and
    test it using an OpenAPI based interface called Swagger UI.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on implementation of serving an ML model as an API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will apply the principles of APIs and microservices that
    we have learned previously (in the section *Introduction to APIs and microservices*)
    and develop a RESTful API service to serve the ML model. The ML model we'll serve
    will be for the business problem (weather prediction using ML) we worked on previously.
    We will use the FastAPI framework to serve the model as an API and Docker to containerize
    the API service into a microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'FastAPI is a framework for deploying ML models. It is easy and fast to code
    and enables high performance with features such as asynchronous calls and data
    integrity checks. FastAPI is easy to use and follows the OpenAPI Specification,
    making it easy to test and validate APIs. Find out more about FastAPI here: [https://fastapi.tiangolo.com/.](https://fastapi.tiangolo.com/.)'
  prefs: []
  type: TYPE_NORMAL
- en: API design and development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will develop the API service and run it on a local computer. (This could
    also be developed on the VM we created earlier in the Azure Machine learning workspace.
    For learning, it is recommended to practice it locally for ease.)
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, clone the book repository on your PC or laptop and go to the
    `08_API_Microservices` folder. We will use these files to build the API service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The files listed in the directory tree for the folder `08_API_Microservices`
    include a Dockerfile (used to build a Docker image and container from the `FASTAPI`
    service) and a folder named `app`. The `app` folder contains the files `weather_api.py`
    (contains the code for API endpoint definitions), `variables.py` (contains the
    input variables definition), and `requirements.txt` (contains Python packages
    needed for running the API service), and a folder with model artifacts such as
    a model scaler (used to scale incoming data) and a serialized model file (`svc.onnx`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The model was serialized previously, in the model training and evaluation stage,
    as seen in [*Chapter 5*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093), *Model
    Evaluation and Packaging*. The model is downloaded and placed in the folder from
    the model registry in the Azure Machine learning workspace (`Learn_MLOps`) as
    shown in *Figure 8.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Download the serialized model file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Download the serialized model file
  prefs: []
  type: TYPE_NORMAL
- en: You can replace the `svc.onnx` and `model-scalar.pkl` files with the files you
    have trained in your Azure Machine learning workspace or else just continue using
    these files for quick experimentation. Now we will look into the code of each
    file. Let's start with `variables.py`.
  prefs: []
  type: TYPE_NORMAL
- en: variables.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use only one package for defining input variables. The package we use is
    called `pydantic`; it is a data validation and settings management package using
    Python-type annotations. Using `pydantic`, we will define input variables in the
    class named `WeatherVariables` used for the `fastAPI` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the `WeatherVariables` class, define variables and their types as shown in
    the preceding code. The same variables that were used for training the model in
    [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning
    Pipelines*, will be used for inference. We define those input variables here as
    `temp_c`, `humidity`, `wind_speed_kmph`, `wind_bearing_degree`, `visibility_km`,
    `pressure_millibars`, and `current_weather_condition`. Data types for these variables
    are defined as `float`. We will import the `WeatherVariables` class and use the
    defined input variables in the `fastAPI` service. Let's look at how we can use
    the variables defined in the `WeatherVariables` class in the `fastAPI` service
    using the `Weather_api.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: Weather_api.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file is used to define the `fastAPI` service. The needed model artifacts
    are imported and used to serve API endpoints to infer the model for making predictions
    in real time or in production:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required packages as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We imported the required packages, such as `uvicorn` (an ASGI server implementation
    package), `fastapi`, `numpy`, `pickle`, `pandas`, and `onnxruntime` (used to deserialize
    and infer `onnx` models).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We imported the `WeatherVariables` class previously created in the `variables.py`
    file. We will use the variables defined in this file for procuring input data
    for the `fastAPI` service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we create an `app` object. You will notice some syntactic similarities
    of `fastAPI` with the Flask web framework (if you have ever used Flask).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, in the next step, we create the `app` object using the function
    `FastAPI()` to create the `app` object. Creating an `app` object is similar to
    how we do it via the `Flask` example: from Flask, import `Flask` and then we use
    the `Flask` function to create the `app` object in the manner `app = Flask ()`.
    You will notice such similarities as we build API endpoints using `fastAPI`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After creating the `app` object, we will import the necessary model artifacts
    for inference in the endpoints. `Pickle` is used to deserialize the data scaler
    file `model-scaler.pkl`. This file was used to train the model (in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*), and now we'll use it to scale the incoming data
    before model inference. We will use the previously trained support vector classifier
    model, which was serialized into the file named `scv.onnx` (we can access and
    download the file as shown in *Figure 8.3*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ONNX` Runtime is used to load the serialized model into inference sessions
    (`input_name` and `label_name`) for making ML model predictions. Next, we can
    move to the core part of defining the API endpoints to infer the ML model. To
    begin, we make a `GET` request to the index route using the wrapper function `@app.get(''/'')`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A function named `index()` is defined for the index route. It returns the welcome
    message, pointing to the docs link. This message is geared toward guiding the
    users to the docs link to access and test the API endpoints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will define the core API endpoint, `/predict`, which is used to infer
    the ML model. A wrapper function, `@app.post(''/predict'')`, is used to make a
    `POST` request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A function named `predict_weather()` is initiated for the endpoint `/predict`.
    Inside the function, we have created a variable called `data` that will capture
    the input data; this variable captures the `JSON` data we are getting through
    the `POST` request and points to `WeatherVariables`. As soon as we do the `POST`
    request, all the variables in the incoming data will be mapped to variables in
    the `WeatherVariables` class from the `variables.py` file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we convert the data into a dictionary, fetch each input variable from
    the dictionary, and compress them into a `numpy` array variable, `data_to_pred`.
    We will use this variable to scale the data and infer the ML model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data (`data_to_pred`) is reshaped and scaled using the scaler loaded previously
    using the `fit_transform()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the model inference step, which is the key step, is performed by inferencing
    scaled data to the model, as shown in the preceding code. The prediction inferred
    from the model is then returned as the output to the `prediction` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Lastly, we will convert the model inference into a human-readable format by
    suggesting `rain` or `no_rain` based on the ML model's predictions and return
    the `prediction` for the `POST` call to the `/predict` endpoint. This brings us
    to the end of the `weather_api.py` file. When a `POST` request is made by passing
    input data, the service returns the model prediction in the form of `0` or `1`.
    The service will return `rain` or `not_rain` based on the model prediction. When
    you get such a prediction, your service is working and is robust enough to serve
    production needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Requirement.txt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This text file contains all the packages needed to run the `fastAPI` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These packages should be installed in the environment where you would like to
    run the API service. We will use `numpy`, `fastapi` (an ML framework for creating
    robust APIs), `uvicorn` (an AGSI server), `scikit-learn`, `pandas`, `onnx`, and
    `onnxruntime` (to deserialize and infer `onnx` models) to run the FastAPI service.
    To deploy and run the API service in a standardized way, we will use Docker to
    run the FastAPI service in a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at how to create a Dockerfile for the service.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a microservice using Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will package the FastAPI service in a standardized way using
    Docker. This way, we can deploy the Docker image or container on the deployment
    target of your choice within around 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker has several advantages, such as replicability, security, development
    simplicity, and so on. We can use the official Docker image of `fastAPI` (`tiangolo/uvicorn-gunicorn-fastapi`)
    from Docker Hub. Here is a snippet of the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, we use an official `fastAPI` Docker image from Docker Hub by using
    the `FROM` command and pointing to the image – `tiangolo/uvicorn-gunicorn-fastapi:python3.7`.
    The image uses Python 3.7, which is compatible with `fastAPI`. Next, we copy the
    `app` folder into a directory named `app` inside `docker image/container`. After
    the folder `app` is copied inside the Docker image/container, we will install
    the necessary packages listed in the file `requirements.txt` by using the `RUN`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: As the `uvicorn` server (AGSI server) for `fastAPI` uses port `80` by default,
    we will `EXPOSE` port `80` for the Docker image/container. Lastly, we will spin
    up the server inside the Docker image/container using the command `CMD "uvicorn
    weather_api:app –host 0.0.0.0 –port 80"`. This command points to the `weather_api.py`
    file to access the `fastAPI` app object for the service and host it on port `80`
    of the image/container.
  prefs: []
  type: TYPE_NORMAL
- en: Congrats, you are almost there. Now we will test the microservice for readiness
    and see whether and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the API for readiness, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by building the Docker image. For this, a prerequisite is to have
    Docker installed. Go to your terminal or Command Prompt and clone the repository
    to your desired location and access the folder `08_API_Microservices`. Execute
    the following Docker command to build the Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: (base) user ~ docker images
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: fastapi      latest    1745e964f57f   56 seconds ago   1.31GB
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a Docker container locally. Now, we can spawn a running Docker container
    from the Docker image created previously. To run a Docker container, we use the
    `RUN` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the API service using sample data. We will check whether the container
    is running successfully or not. To check this, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, test the `/predict` endpoint (by selecting the endpoint and clicking the
    **Try it out** button) using input data of your choice, as shown in *Figure 8.6*:![Figure
    8.7 – Input for the request body of the FastAPI service
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_08_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.7 – Input for the request body of the FastAPI service
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `POST` call and test the endpoint. The input is inferred with the model
    in the service and the model prediction `Rain` or `No_Rain` is the output of the
    `POST` call, as shown in *Figure 8.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Output for the POST call (/predict endpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Output for the POST call (/predict endpoint)
  prefs: []
  type: TYPE_NORMAL
- en: The successful execution of the `POST` call for the `/predict` API will result
    in the output model prediction as shown in *Figure 8.6*. The model running in
    the Docker container outputs the weather condition as `Rain` for the `POST` call.
    Congratulations, you have successfully spawned a `fastAPI` container and tested
    it. This exercise should have equipped you with the skills to build, deploy, and
    test ML-based API services for your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned the key principles of API design and microservice
    deployment in production. We touched upon the basics of API design methods and
    learned about FastAPI. For our business problem, we have learned by doing a practical
    implementation of developing an API service in the *Hands-on implementation of
    serving an ML model as an API* section using FastAPI and Docker. Using the practical
    knowledge gained in this chapter, you can design and develop robust API services
    to serve your ML models. Developing API services for ML models is a stepping stone
    to take ML models to production.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the concepts of testing and security.
    We will implement a testing method to test the robustness of an API service using
    Locust. Let's go!
  prefs: []
  type: TYPE_NORMAL
