- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stabilizing the Machine Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last two chapters, we went over the different concepts in machine learning
    and how we can create a comprehensive machine learning system pipeline that can
    work and adapt to our needs.
  prefs: []
  type: TYPE_NORMAL
- en: While our pipeline can address our expectations, it is important for us to be
    able to maintain our system in the face of external factors to which it may be
    hard for the system to self-adjust.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the phenomenon of dataset shifts and how we
    can optimize our machine learning system to help address these issues while maintaining
    its functional goal without having to rebuild our system from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be going over the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning parameterization and dataset shifts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causes of dataset shifts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying dataset shifts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling and stabilizing dataset shifts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning parameterization and dataset shifts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maintaining our machine learning models is an integral part of creating a robust
    model. As time progresses, our data begins to morph and shift based on our environment,
    and while most models can detect and self-repair, sometimes, human intervention
    will be required to guide them back on track.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will briefly go over two main concepts that will help us
    understand the impact on our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameterization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset shifts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our machine learning model is represented by certain specifications that help
    define the learning process of our model. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameters**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will first look at parameters. These specifications are internal within the
    model. During the training process, these parameters are updated and learned while
    the model is trying to learn the mapping between the input features and the target
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, these parameters are set to an initial value of either zeros
    or random values. As the training process happens, the values are continuously
    updated by an optimization method, such as gradient descent. At the end of the
    training process, the final weights of the values are what constitute the model
    itself. These weights can even be used for other models, especially those with
    similar applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of parameters include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Node weights and bias values for artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients of linear and logistic regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster centroids for clustering models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While parameters play a core role in determining the performance of a model,
    they are mostly out of our control since the model itself is what updates the
    weights. This leads us to hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are parameters that control the learning process of our machine
    learning model, which, in turn, affects the output weights that our model learns.
    These values are set from the beginning and stay fixed throughout the learning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: We, as users, determine which values to set in the beginning for our model to
    use during the training process. As a result, it takes time and experience to
    figure out which values produce the best results. There is effort involved in
    testing and training multiple variations of hyperparameters to see which performs
    the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many hyperparameters and each model has its own unique set of hyperparameters
    that the user can modify. These hyperparameters can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The split ratio between the training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate used in optimization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of optimization algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of epochs or iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes in each hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of cost or loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of ![](img/Formula_06_001.png) clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there can be many hyperparameters to adjust and many different combinations
    to try, it can be very time-consuming to test these changes one by one. As discussed
    in the last chapter, it can be useful to have a section in our pipeline that automates
    this process by running multiple models with different combinations of hyperparameters
    to speed up the testing process and find the most optimal combination of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Hyperparameter and parameter tuning](img/B18934_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Hyperparameter and parameter tuning'
  prefs: []
  type: TYPE_NORMAL
- en: There may be cases where adjusting our parameters and hyperparameters is not
    enough for us to prevent our model from degrading.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we create a machine learning model with a model accuracy
    of 85%. This model continues to perform well for some time. We then begin to see
    our model accuracy deteriorate until it becomes unusable, as the model is unable
    to properly predict the new test data we collect.
  prefs: []
  type: TYPE_NORMAL
- en: As we analyze our model, we can begin to see that our training data does not
    reflect the testing data we have recently collected. Here, we can see that there
    is a shift between the data distribution for our training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Before we work on resolving dataset shifts, we must first understand the background
    of dataset shifts, how they occur, and how we can adjust our machine learning
    system to help prevent dataset shifts from impacting our model.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning systems are built under the assumption that the data distribution
    between the training and test sets is similar. Since the real world is ever-changing,
    new data distributions emerge and there may be a significant difference between
    the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major difference in data distribution between the training and test sets
    is considered a dataset shift. This drastic difference will eventually degrade
    the model, as the model is biased to the training set and is unable to adapt to
    the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Outcome of a machine learning model due to a dataset shift](img/B18934_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Outcome of a machine learning model due to a dataset shift'
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of this occurring include a shift in consumer habits, a socioeconomic
    shift, or a global influence, such as a pandemic. These events can heavily impact
    the data we collect and observe, which, in turn, can sway our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: First, try adjusting the hyperparameters of your machine learning model and
    see whether the newly learned parameters can improve your model significantly.
    If you still encounter major issues, it may be best to analyze the data and see
    whether a dataset shift has occurred.
  prefs: []
  type: TYPE_NORMAL
- en: The causes of dataset shifts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have learned what dataset shifts are, we can start to investigate
    the different causes of dataset shifts. While there are many different reasons
    dataset shifts can occur, we can split them into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample** **selection bias**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-stationary environments**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample selection bias is self-explanatory in that there is a bias or issue when
    it comes to labeling or collecting the training data used for the model. Collecting
    biased data will result in a non-uniform sample selection for the training set.
    That bias, in essence, will fail to represent the actual sample distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Non-stationary environments are another cause for dataset shifts – we will go
    into further detail about the different types later in the chapter. Let’s assume
    that we have a model with a set of input features, ![](img/Formula_06_002.png),
    a target or output variable ![](img/Formula_06_003.png). From there, we can also
    define the prior probability as ![](img/Formula_06_004.png), the conditional probability
    as ![](img/Formula_06_005.png), and the joint distribution as ![](img/Formula_06_006.png).
    This dataset shift is caused by temporal or spatial changes, defined as ![](img/Formula_06_007.png),
    which reflect very much how the real world operates.
  prefs: []
  type: TYPE_NORMAL
- en: 'This causal effect can lead to different types of shifts:'
  prefs: []
  type: TYPE_NORMAL
- en: For ![](img/Formula_06_008.png) problems, non-stationary environments can make
    changes to either ![](img/Formula_06_009.png) or ![](img/Formula_06_010.png),
    giving us a covariate or concept shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For ![](img/Formula_06_011.png) problems, a change in ![](img/Formula_06_012.png)
    or ![](img/Formula_06_013.png) can give us a prior probability or concept shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look into the different types of shifts and how
    we can identify them.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying dataset shifts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After looking into the different causes of dataset shifts, we can begin to classify
    certain shifts into different groups that can help us easily identify the type
    of dataset shift we are dealing with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the different dataset shifts we can encounter, we can classify data shifts
    into these categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariate shifts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prior** **probability shifts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept shifts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will first look at covariate shifts. This is the most common dataset shift,
    as a covariate shift occurs when there is a change in the distribution of one
    or more of the input features of the training or test data. Despite the change,
    the target value remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, this dataset shift occurs only in X > Y problems. Whenever
    the input distribution, ![](img/Formula_06_014.png), changes between the training
    and testing datasets, ![](img/Formula_06_015.png), but the conditional probability
    of the training and testing dataset stays the same, ![](img/Formula_06_016.png),
    this will cause a covariate shift.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can create a model that predicts the salary of the employees
    of a certain city. Let’s say that the majority of the employees in your training
    set consist of younger individuals. After time passes, the employees get older.
    If you were to try to predict the salary of the older employees, you would begin
    to see a significant error. This is due to the model being heavily biased toward
    the training set, which consisted of mostly younger employees and is unable to
    find the relationship among the older employees.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Covariate dataset shifts](img/B18934_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Covariate dataset shifts'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will be looking into prior probability shifts, also known as label
    shifts. This is the opposite of a covariate shift, as this shift occurs when the
    output distribution changes for a given output but the input distribution remains
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, this occurs only in Y -> X problems. When the prior
    probability changes, ![](img/Formula_06_017.png), but the conditional probability
    remains the same, ![](img/Formula_06_018.png), a prior probability shift occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Prior probability shifts](img/B18934_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Prior probability shifts'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will discuss concept shifts, also known as concept drifts. This
    shift occurs when the distribution of the training data remains the same but the
    conditional distribution for the output given the training data changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, this can occur both in X -> Y or Y -> X problems:'
  prefs: []
  type: TYPE_NORMAL
- en: For X -> Y problems, this occurs when the prior probability of the input variables
    remains the same in the training and testing datasets, (![](img/Formula_06_019.png),
    but the conditional probability changes, ![](img/Formula_06_020.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Y -> X problems, this occurs when the prior probability of the target variables
    remains the same in the training and testing datasets, ![](img/Formula_06_021.png),
    but the conditional probability changes, ![](img/Formula_06_022.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, a user’s purchasing behavior is affected due to the economy,
    but neither our training nor our test data contains any information regarding
    the economy’s performance. As a result, our model’s performance will degrade.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Concept shifts](img/B18934_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Concept shifts'
  prefs: []
  type: TYPE_NORMAL
- en: This can be a tricky dataset shift since the distribution shift is not related
    to the data that we train on, but rather external information that our model may
    not have. Most of the time, these dataset shifts are cyclical and/or seasonal.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing your data and calculating the different probabilities with regard
    to your data is the best way to help determine and identify which dataset shift
    you are dealing with. From there, you can decide how you will address your dataset
    shift.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to identifying most dataset shifts, there is a process that we
    can follow to help us. It includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating random samples of your training and test sets on their own
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the random samples into one dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a model using one feature at a time while using the origin as the output
    value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting on the test set and calculating the **Area Under Curve – Receiver
    Operating Characteristics** **Curve** (**AUC-ROC**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the AUC-ROC is greater than a certain threshold, for example, 80%, we can
    classify the data as having experienced a dataset shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates
    a strong model)](img/B18934_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates a
    strong model)'
  prefs: []
  type: TYPE_NORMAL
- en: Handling and stabilizing dataset shifts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have established the methods for identifying the different types
    of dataset shifts, we can discuss the different ways of addressing these shifts
    and stabilizing our machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many ways to address dataset shifts, we will be looking at
    the three main methods. They consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature dropping**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial search**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density** **ratio estimation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will first look at feature dropping. This is the simplest form of adjusting
    dataset shifts. As we determine which features are classified as drifting, we
    can simply drop them from the machine learning model. We can also define a simple
    rule where any features with a drift value greater than a certain threshold, for
    example, 80%, can be dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: Feature Dropping Process](img/B18934_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Feature Dropping Process'
  prefs: []
  type: TYPE_NORMAL
- en: While this is a simple change, this is something that needs to be considered
    carefully. If this feature is considered important when training your machine
    learning model, then it is worth reconsidering whether this feature needs to be
    dropped. Also, if the majority of your features pass the threshold for being dropped,
    you may want to revisit your data as a whole and consider a different approach
    when addressing your dataset shift.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at adversarial search. This is a technique that requires
    training a binary classifier to predict whether the sample data is within the
    training or test datasets. We can then evaluate the performance of the classifier
    to determine whether there has been a dataset shift. If the performance of our
    classifier is close to that of a random guess (~50%), we can confidently determine
    that our training and test dataset distribution is consistent. On the other hand,
    if our classifier performs better than a random guess, then that will indicate
    an inconsistency between the distribution of the training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adversarial search can be split into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: From the original dataset, we will remove the target value column and replace
    it with a new column that indicates the source of data (train = 0 and test = 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will create and train the new classifier with the new dataset. The output
    of the classifier is the probability that the sample data is part of the test
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can observe the results and measure the performance of our classifier.
    If our classifier performance is close to 50%, then this indicates that the model
    is unable to differentiate whether the data is coming from the training or test
    set. This can tell us that the data distribution between the training and test
    datasets is consistent. On the flip side, if our performance is close to 100%,
    then the model is confident enough to find the difference between the training
    and test datasets, which then indicates a major difference between the distribution
    of the training and test datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8: Adversarial search process](img/B18934_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Adversarial search process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using adversarial search, we can establish three methods to address the dataset
    shifts we encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the results, we can use them as sample weights for the training process.
    The weights correspond to the nature of how the data is distributed. The data
    that is similar in the actual distribution will be assigned a larger weight while
    that with inconsistent distribution will be given a lower weight. This will help
    the model emphasize the data that actually represents the real distribution it
    is trying to learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use only the top-ranked adversarial validation results. Rather than mitigating
    the weights of inconsistent samples in the testing dataset, we can remove them
    altogether.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All data is used for training except for the top-ranked adversarial validation
    results. This method can address the issues that can arise from the second method
    by using all the data rather than dropping features. Rather than discarding unimportant
    data, we can incorporate some of the data in the training data for each fold when
    using K-fold cross-validation during training. This helps maintain consistency
    while using all the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final method used to address dataset shifts is called the density ratio
    estimation method. This method is still under research and not a commonly used
    method to address dataset shifts.
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, we would first estimate the training and test dataset densities
    separately. Once we have done this, we will then estimate the importance of the
    dataset by taking the ratio of the estimated densities of the training and test
    datasets. Using this density ratio, we can use it as the weight for each data
    entry in our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this method is not preferred and is still under research is that
    it is computationally expensive, especially for higher dimensional datasets. Even
    then, the improvements it can bring to addressing dataset shifts are negligible
    and not worth the effort of pursuing this method.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Feature dropping is the easiest and simplest way to address dataset shifts.
    Consider using this approach before using the adversarial search approach, as
    that option, while effective, can be a little involved and may require more effort
    and resources to help mitigate the effect of dataset shifts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went over the general concepts of dataset shifts and how
    they can negatively impact our machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we delved in deeper into what causes these dataset shifts to occur
    and what different characteristics dataset shifts can exhibit. Using these characteristics,
    we can better identify the type of dataset shift – whether it was a covariate
    shift, prior probability shift, or concept shift.
  prefs: []
  type: TYPE_NORMAL
- en: Once we were able to analyze our data and identify the type of dataset shift,
    we looked at different methods to help us handle and stabilize these dataset shifts
    so that we could maintain our machine learning model. We went over some techniques,
    such as feature searching, adversarial search, and density ratio estimation, that
    can assist us when dealing with dataset shifts.
  prefs: []
  type: TYPE_NORMAL
- en: Using these processes and methods, we can prevent our model from suffering from
    common dataset shifts that occur in the real world and continuously maintain our
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a firm understanding of machine learning and how to maintain
    a robust model, we can start looking into how we can incorporate our machine learning
    models into our **Microservices** **Architecture** (**MSA**).
  prefs: []
  type: TYPE_NORMAL
