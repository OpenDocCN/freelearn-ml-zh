- en: Case Studies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: 'This book has gone through several different feature engineering algorithms
    and we have worked with many different datasets. In this chapter, we will go through
    a few case studies to help you deepen your understanding of the topics we have
    covered in the book. We will work through two full-length case studies from beginning
    to end to further understand how feature engineering tasks can help us create
    machine learning pipelines for real-life applications. For each case study, we
    will go through:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书已经经过了几种不同的特征工程算法，并且我们与许多不同的数据集进行了合作。在本章中，我们将通过几个案例研究来帮助你加深对书中所涵盖主题的理解。我们将从头到尾完成两个完整的案例研究，以进一步了解特征工程任务如何帮助我们为实际应用创建机器学习流程。对于每个案例研究，我们将进行以下步骤：
- en: The application that we are working towards
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在努力实现的应用
- en: The data in question that we are using
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用的数据
- en: A brief exploratory data analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要的探索性数据分析
- en: Setting up our machine learning pipelines and gathering metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们的机器学习流程并收集指标
- en: 'Moreover, we will be going through the following cases:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将探讨以下案例：
- en: Facial recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部识别
- en: Predicting hotel reviews data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测酒店评论数据
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Case study 1 - facial recognition
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 1 - 面部识别
- en: Our first case study will be to predict the labels for image data with a popular
    dataset called the **Labeled Faces** in the `Wild` dataset from the scikit-learn
    library. The dataset is called the `Olivetti Face` dataset and it comprises pictures
    of famous people's faces, with appropriate labels. Our task is that of **facial
    recognition**, a supervised machine learning model that is able to predict the
    name of the person given an image of their face.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一项案例研究将是使用名为scikit-learn库中`Wild`数据集的**Labeled Faces**的流行数据集来预测图像数据的标签。该数据集被称为`Olivetti
    Face`数据集，它包含了名人的面部照片，并附有相应的标签。我们的任务是**面部识别**，这是一个监督机器学习模型，能够根据人脸图像预测人的名字。
- en: Applications of facial recognition
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部识别的应用
- en: Image processing and facial recognition are far-reaching. The ability to quickly
    discern people's faces from a crowd of people in video/images is vital for physical
    security as well as for giant social media companies. Search engines such as Google,
    with their image search capabilities, are using image recognition algorithms to
    match images and quantify similarities to a point where we can upload a photo
    of someone to get all other images of that same person.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理和面部识别具有广泛的应用。从人群中的视频/图像中快速识别人的面部对于物理安全和大型社交媒体公司至关重要。像Google这样的搜索引擎，凭借其图像搜索功能，正在使用图像识别算法来匹配图像并量化相似度，直到我们可以上传某人的照片来获取该人的所有其他图像。
- en: The data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Let's start with loading in our dataset and several other import statements
    we will be using to plot our data. It is good practice to begin a Jupyter notebook
    (iPython) with all the import statements you will be using. Obviously, you may
    get partway through your work and realize that you need to import a new package;
    also, to stay organized, it is a good idea to keep them in the beginning of your
    work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载我们的数据集和我们将用于绘制数据的几个其他导入语句开始。在Jupyter笔记本（iPython）中开始使用所有你将使用的导入语句是一种良好的做法。显然，你可能在进行工作的中途意识到你需要导入一个新的包；同样，为了保持组织有序，将它们放在你工作的开头是一个好主意。
- en: 'The following code block includes the `import` statements we will be using
    for this case study. We will utilize each import in the example and it will become
    clear to you what each of them is used for as we work out our example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含了我们将用于本案例研究的`import`语句。我们将利用示例中的每个导入，随着我们例子的展开，它们各自的作用将变得对你来说很清晰：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we can get started! We proceed as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始了！我们按以下步骤进行：
- en: 'First, let''s load in our dataset and see what we are working with. We will
    use the `fetch_flw_people` function built in with scikit-learn:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载我们的数据集，看看我们正在处理什么。我们将使用scikit-learn内置的`fetch_flw_people`函数：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we have a couple of optional parameters that we've invoked,
    specifically, `min_faces_per_person` and `resize`. The first parameter will only
    retain the pictures of people who are in the minimum number of different pictures
    that we specify. We have set this to be a minimum of `70` different pictures per
    person. The `resize` parameter is the ratio used to resize each face picture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有一些可选参数已经调用，特别是`min_faces_per_person`和`resize`。第一个参数将仅保留我们指定的人的最小数量的不同图片。我们将此设置为每人至少`70`张不同图片。`resize`参数是用于调整每个面部图片的缩放比例。
- en: 'Let''s inspect the image arrays to find shapes for plotting the images. We
    can do this with the following code:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查图像数组以找到用于绘制图像的形状。我们可以使用以下代码来完成：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that we have `1288` samples (images) and each image has a height of `50`
    pixels and a width of `37` pixels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们有`1288`个样本（图像），每个图像的高度为`50`像素，宽度为`37`像素。
- en: 'Now, let''s set up the `X` and `y` for our machine learning pipeline. We will
    grab the `data` attribute of the `lfw_people` object:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置我们的机器学习流程中的`X`和`y`。我们将获取`lfw_people`对象的`data`属性：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The fact that `n_features` ends up having `1,850` columns comes from the fact
    that:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_features`最终有`1,850`列的事实源于以下事实：'
- en: '![](img/15bd7220-096c-4238-a4ec-57a8f079f069.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15bd7220-096c-4238-a4ec-57a8f079f069.png)'
- en: 'We can now see the full shape of our data, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到我们数据的完整形状，如下所示：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Some data exploration
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些数据探索
- en: 'We have 1,288 rows by 1,850 columns. To do some brief exploratory analysis,
    we can plot one of the images by using this code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有1,288行和1,850列。为了进行一些简要的探索性分析，我们可以使用以下代码绘制其中一张图像：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give us the following label:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下标签：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The image is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图像如下所示：
- en: '![](img/5c5a295c-e25a-4aba-a87b-eb10055ee563.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c5a295c-e25a-4aba-a87b-eb10055ee563.png)'
- en: 'Now, let''s plot the same image after applying a scaling module, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制应用缩放模块后的相同图像，如下所示：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Which gives us this output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We get the following image for the preceding code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码给出了以下图像：
- en: '![](img/cab13f64-db92-4d96-85e1-c5a09eac1391.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cab13f64-db92-4d96-85e1-c5a09eac1391.png)'
- en: 'Here, you can see that the image is slightly different, with darker pixels
    around the face. Now, let''s set up the label to predict:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到图像略有不同，面部周围有较暗的像素。现在，让我们设置预测的标签：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives us the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Applied facial recognition
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用面部识别
- en: 'Now, we can move on to the machine learning pipelines that will be used to
    create our facial recognition models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续到将用于创建我们的面部识别模型的机器学习流程：
- en: 'We can start by creating `train`, `test`, and `split` in our dataset, as shown
    in the following code block:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以开始创建数据集中的`train`、`test`和`split`，如下所示：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We are ready to perform a **Principal Component Analysis** (**PCA**) on our
    dataset. We will want to instantiate a `PCA` first and ensure that we `scale`
    our data before applying PCA in our pipeline. This can be done as follows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们准备好对我们的数据集执行**主成分分析**（**PCA**）。我们首先需要实例化一个`PCA`，并在流程中应用PCA之前确保我们`scale`我们的数据。这可以按以下方式完成：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can `fit` our pipeline:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以`fit`我们的流程：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be our print statement:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将是我们的打印语句：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s look at the scree plot:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看散点图：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that starting at 100 components captures over 90% of the variance,
    compared to the 1,850 original features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从100个组件开始，可以捕捉到超过90%的方差，与原始的1,850个特征相比。
- en: 'We can create a function to plot our PCA components, like so:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建一个函数来绘制我们的PCA组件，如下所示：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now call our `plot_gallery` function, like so:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以调用我们的`plot_gallery`函数，如下所示：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output gives us these images:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出给我们以下图像：
- en: '![](img/cfc5f3aa-df26-487f-8d23-c0c237f96fe6.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfc5f3aa-df26-487f-8d23-c0c237f96fe6.png)'
- en: 'This lets us see our PCA components for a specific row and column! These **eigen-faces**
    are extracted features of humans that the PCA module is finding. Compare this
    to our result in [Chapter 7](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml), *Feature
    Learning*, where we used PCA to extract **eigen-digits**. Each of these components
    is meant to house vital information about faces that can be used to distinguish
    between different people. For example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们可以看到特定行和列的PCA组件！这些**主成分脸**是PCA模块找到的人类提取特征。与我们在[第7章](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml)，“特征学习”中使用PCA提取**主成分数字**的结果进行比较。每个组件都旨在存储有关面部的重要信息，这些信息可以用来区分不同的人。例如：
- en: The eigen-face in the third row, fourth column seems to be highlighting the
    moustache and beard areas in order to quantify how much facial hair would help
    in separating out our classes
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行，第四列的主成分似乎在突出显示胡须和 beard区域，以量化面部毛发在区分我们的类别中能起到多大帮助
- en: The eigen-face in the first row, fourth column seems to be showing a contrast
    between the background and the face, putting a number to the lighting situation
    of the image
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，第四列的主成分似乎显示了背景和面部之间的对比，给图像的照明情况赋予了一个数值
- en: 'Of course, these are interpretations by us, and different eigen-faces for different
    face datasets will output different images/components. We will move on to create
    a function that will allow us to clearly print a more readable confusion matrix
    with heat labels and options for normalization:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些都是我们的解读，对于不同的面部数据集，不同的主成分将输出不同的图像/组件。我们将继续创建一个函数，使我们能够清晰地打印出带有热标签和归一化选项的更易读的混淆矩阵：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can fit without using PCA to see the difference. We will invoke our
    `plot_confusion_matrix` function so that we can visualize the accuracy of our
    model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以不使用PCA进行拟合，看看会有什么不同。我们将调用我们的`plot_confusion_matrix`函数，以便我们可以可视化我们模型的准确率：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the plot as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的图如下：
- en: '![](img/f34a3ca8-41f8-436c-abc1-1316a76b1da3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f34a3ca8-41f8-436c-abc1-1316a76b1da3.png)'
- en: 'Using only raw pixels, our linear model was able to achieve **81.3%** accuracy.This
    time, let''s apply PCA to see what the difference will be. We will hardcode the
    number of components to extract to 200 for now:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用原始像素，我们的线性模型能够达到**81.3**%的准确率。这次，让我们应用PCA来看看会有什么不同。我们将硬编码要提取的组件数量为200：
- en: '[PRE21]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output with PCA looks like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA的输出看起来如下：
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We get the plot as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的图如下：
- en: '![](img/1b072798-7cb0-4dcd-bfe7-c8da8d0daee0.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b072798-7cb0-4dcd-bfe7-c8da8d0daee0.png)'
- en: Interesting! We can see that our accuracy went down to **73.9%** and our time
    to predict went up by applying PCA. We should not get discouraged, however; this
    likely means that we have not found the optimal number of components to use yet.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 真是令人惊讶！我们可以看到，通过应用主成分分析（PCA），我们的准确率下降到了**73.9**%，而预测时间有所增加。然而，我们不应该气馁；这很可能意味着我们还没有找到使用最佳组件数量的方法。
- en: 'Let''s plot a few of the predicted names versus the true names within our test
    set to see some of the errors/correct labels that our models are producing:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制测试集中一些预测名称与真实名称的对比图，以查看我们模型产生的某些错误/正确标签：
- en: '![](img/2c5e4b2b-e868-4921-8619-7696a6be742a.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c5e4b2b-e868-4921-8619-7696a6be742a.png)'
- en: This is a great way to visualize our results when working with images.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在处理图像时可视化结果的一个很好的方法。
- en: 'Let''s now implement a grid search to find the best model and accuracy for
    our data. First, we will create a function that will perform the grid search for
    us and print the accuracy, parameters, average time to fit, and average time to
    score neatly for us. This function is created like so:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个网格搜索来找到我们数据最佳模型和准确率。首先，我们将创建一个函数，该函数将为我们执行网格搜索并清晰地打印出准确率、参数、平均拟合时间和平均评分时间。这个函数创建如下：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can create a larger grid search pipeline that includes many more components,
    namely:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个更大的网格搜索管道，包括许多更多的组件，具体如下：
- en: A scaling module
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个缩放模块
- en: A PCA module to extract the best features that capture the variance in the data
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个PCA模块，用于提取捕捉数据变异性最佳的特征
- en: A **Linear Discriminat Analysis** (**LDA**) module to create features that best
    separate the faces from one another
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**线性判别分析（LDA**）模块，用于创建最佳特征，以区分彼此的面部
- en: Our linear classifier, which will reap the benefits of our three feature engineering
    modules and attempt to distinguish between our faces
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的线性分类器，将利用我们三个特征工程模块的优势，并尝试区分我们的面部
- en: 'The code for creating large grid search pipeline is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 创建大型网格搜索管道的代码如下：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here are the results:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see that our model accuracy has improved by a good amount, and also our
    time to predict and train is very fast!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的模型准确率有显著提高，而且预测和训练时间非常快！
- en: Case study 2 - predicting topics of hotel reviews data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究2 - 预测酒店评论数据的主题
- en: Our second case study will take a look at hotel reviews data and attempt to
    cluster the reviews into topics. We will be employing a **latent semantic analysis**
    (**LSA**), which is a name given to the process of applying a PCA on sparse text
    document—term matrices. It is done to find latent structures in text for the purpose
    of classification and clustering.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个案例研究将研究酒店评论数据，并尝试将评论聚类到主题中。我们将使用**潜在语义分析**（**LSA**），这是一种在稀疏文本文档—词矩阵上应用PCA的过程。这是为了在文本中找到潜在结构，以便进行分类和聚类。
- en: Applications of text clustering
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本聚类的应用
- en: Text **clustering** is the act of assigning different topics to pieces of text
    for the purpose of understanding what documents are about. Imagine a large hotel
    chain that gets thousands of reviews a week from around the world. Employees of
    the hotel would like to know what people are saying in order to have a better
    idea of what they are doing well and what can be improved.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 文本**聚类**是将不同的主题分配给文本片段的行为，目的是为了理解文档的内容。想象一下，一家大型酒店连锁店每周都会收到来自世界各地的数千条评论。酒店的员工希望了解人们都在说什么，以便更好地了解他们做得好的地方和需要改进的地方。
- en: Of course, the limiting factor here is the ability for humans to read all of
    these texts quickly and correctly. We can train machines to identify the types
    of things that people are talking about and then predict the topics of new and
    incoming reviews in order to automate this process.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里的限制因素是人类快速且正确地阅读所有这些文本的能力。我们可以训练机器识别人们谈论的事物类型，然后预测新评论和即将到来的评论的主题，以自动化这一过程。
- en: Hotel review data
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 酒店评论数据
- en: 'The dataset that we will use to achieve this result comes from Kaggle and can
    be found here at: [https://www.kaggle.com/datafiniti/hotel-reviews](https://www.kaggle.com/datafiniti/hotel-reviews).
    It contains over 35,000 distinct reviews of 1,000 different hotels around the
    world. Our job will be to isolate the text of the reviews and identify *topics*
    (what people are talking about). Then, we''ll create a machine learning model
    that can predict/identify the topics of incoming reviews:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自Kaggle的数据集来实现这一结果，可以在以下链接找到：[https://www.kaggle.com/datafiniti/hotel-reviews](https://www.kaggle.com/datafiniti/hotel-reviews)。它包含来自世界各地1,000家不同酒店的超过35,000条独特的评论。我们的工作将是隔离评论的文本，并识别*主题*（人们谈论的内容）。然后，我们将创建一个机器学习模型，可以预测/识别即将到来的评论的主题：
- en: 'First, let''s organize our import statements, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们组织我们的导入语句，如下所示：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let''s load in our data, as shown in the following code snippet:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载我们的数据，如下面的代码片段所示：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once we have imported our data, let's work to take a peek into what our raw
    text data looks like.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们导入了数据，让我们来看看我们的原始文本数据是什么样的。
- en: Exploration of the data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'Let''s look at the `shape` of our dataset:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们数据集的`shape`：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is showing us that we are working with 35,912 rows and 19 columns. Eventually,
    we will be concerned only with the column that contains the text data, but for
    now, let''s see what the first few rows look like to get a better sense of what
    is included in our data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们正在处理35,912行和19列的数据。最终，我们只会关注包含文本数据的列，但就目前而言，让我们看看前几行看起来是什么样子，以便更好地了解我们的数据中包含的内容：
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This gives us the following table:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下表格：
- en: '|  | **address** | **categories** | **city** | **country** | **latitude** |
    **longitude** | **name** | **postalCode** | **province** | **reviews. date** |
    **reviews. dateAdded** | **reviews. doRecommend** | **reviews. id** | **reviews.
    rating** | **reviews. text** | **reviews.****title** | **reviews.****userCity**
    | **reviews.****username** | **reviews.****userProvince** |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | **地址** | **类别** | **城市** | **国家** | **纬度** | **经度** | **名称** | **邮政编码**
    | **省份** | **评论日期** | **评论添加日期** | **推荐** | **评论ID** | **评分** | **评论内容** | **评论标题**
    | **评论者城市** | **评论者用户名** | **评论者省份** |'
- en: '| **0** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-09-22T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 4.0 | Pleasant 10 min walk along the sea front to th... | Good location
    away from the crouds | NaN | Russ (kent) | NaN |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **0** | Riviera San Nicol 11/a | 酒店 | Mableton | 美国 | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-09-22T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 4.0 | 沿着海边的愉快10分钟步行 | 离人群远的好位置 | NaN | Russ (kent) | NaN |'
- en: '| **1** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-04-03T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | Really lovely hotel. Stayed on the very top fl... | Great
    hotel with Jacuzzi bath! | NaN | A Traveler | NaN |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **1** | Riviera San Nicol 11/a | 酒店 | Mableton | 美国 | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-04-03T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | 真的很棒的一家酒店。我们住在顶楼。… | 有按摩浴缸的酒店太棒了！ | NaN | A Traveler | NaN
    |'
- en: '| **2** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2014-05-13T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | Ett mycket bra hotell. Det som drog ner betyge... | Lugnt
    l��ge | NaN | Maud | NaN |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **2** | Riviera San Nicol 11/a | 酒店 | Mableton | 美国 | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2014-05-13T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | 非常好的酒店。唯一降低评分的是… | 地点安静。 | NaN | Maud | NaN |'
- en: '| **3** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-10-27T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | We stayed here for four nights in October. The... | Good location
    on the Lido. | NaN | Julie | NaN |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **3** | Riviera San Nicol 11/a | 酒店 | Mableton | 美国 | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-10-27T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | 我们在十月份在这里住了四晚。… | 位置很好，在 Lido 上。 | NaN | Julie | NaN |'
- en: '| **4** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-03-05T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | We stayed here for four nights in October. The... | ������
    ��������������� | NaN | sungchul | NaN |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **4** | Riviera San Nicol 11/a | 酒店 | Mableton | 美国 | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-03-05T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | 我们在十月份在这里住了四晚。… | ������ ��������������� | NaN | sungchul
    | NaN |'
- en: 'Let''s only include reviews from the United States in order to try and include
    only English reviews. First, let''s plot our data, like so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试只包括英语评论，让我们只包括来自美国的评论。首先，让我们像这样绘制我们的数据：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output looks something like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '![](img/2fadf710-709b-4507-b163-984b4fa244b5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fadf710-709b-4507-b163-984b4fa244b5.png)'
- en: 'For the purpose of making our dataset a bit easier to work with, let''s use
    pandas to subset the reviews and only include those that came from the United
    States:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的数据集更容易处理，让我们使用 pandas 来对评论进行子集化，只包括来自美国的评论：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/d4a20b9e-82a0-47d8-803f-9b2dc532bd15.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d4a20b9e-82a0-47d8-803f-9b2dc532bd15.png)'
- en: 'It looks like a map of the U.S.! Let''s `shape` our filtered dataset now:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来像是美国的地图！现在让我们`shape`我们的过滤数据集：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have 30,692 rows and 19 columns. When we write reviews for hotels, we usually
    write about different things in the same review. For this reason, we will attempt
    to assign topics to single sentences rather than to the entire review.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 30,692 行和 19 列。当我们为酒店写评论时，我们通常在同一个评论中写关于不同的事情。因此，我们将尝试将主题分配给单个句子，而不是整个评论。
- en: 'To do so, let''s grab the text column from our data, like so:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，让我们从我们的数据中获取文本列，像这样：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The clustering model
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类模型
- en: 'We can tokenize the text into sentences so that we expand our dataset. We imported
    a function called `sent_tokenize` from the `nltk` package (natural language toolkit).
    This function will take in a single string and output the sentence as an ordered
    list of sentences separated by punctuation. For example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将文本分词成句子，这样就可以扩展我们的数据集。我们从 `nltk`（自然语言工具包）包中导入了一个名为 `sent_tokenize` 的函数。此函数将接受一个字符串并输出句子，作为由标点符号分隔的有序句子列表。例如：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will apply this function to our entire corpus using some reduce logic in
    Python. Essentially, we are applying the `sent_tokenize` function to each review
    and creating a single list called `sentences` that will hold all of our sentences:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 中的某些 reduce 逻辑将此函数应用于整个语料库。本质上，我们正在将 `sent_tokenize` 函数应用于每个评论，创建一个名为
    `sentences` 的单个列表，该列表将包含我们所有的句子：
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can now see how many sentences we have:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到我们有多少句子：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This gives us 118,151—the number of sentences we have to work with. To create
    a document-term matrix, let''s use `TfidfVectorizer` on our sentences:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了 118,151 — 我们要处理的句子数量。为了创建一个文档-词矩阵，让我们在我们的句子中使用 `TfidfVectorizer`：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We get the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, let''s try to fit a PCA to this data, like so:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将 PCA 应用于这些数据，像这样：
- en: '[PRE39]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Upon running this code, we get the following error:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们得到以下错误：
- en: '[PRE40]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The is error tells us that for PCA, we cannot have a sparse input, and it suggests
    that we use **TruncatedSVD**. **singular value decomposition **(**SVD**) is a
    matrix *trick* for computing the same PCA components (when the data is centered)
    that allow us to work with sparse matrices. Let's take this suggestion and use
    the `TruncatedSVD` module.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误告诉我们，对于 PCA，我们不能有稀疏的输入，并建议我们使用 **TruncatedSVD**。**奇异值分解**（**SVD**）是一个矩阵
    *技巧*，用于计算（当数据居中时）与 PCA 相同的成分，使我们能够处理稀疏矩阵。让我们接受这个建议并使用 `TruncatedSVD` 模块。
- en: SVD versus PCA components
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVD 与 PCA 成分比较
- en: 'Before we move on with our hotel data, let''s do a quick experiment with our
    `iris` data to see whether our SVD and PCA really give us the same components:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续使用我们的酒店数据之前，让我们对我们的 `iris` 数据进行一个快速实验，看看我们的 SVD 和 PCA 是否真的给出了相同的成分：
- en: 'Let''s start by grabbing our iris data and creating both a centered and a scaled
    version:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从获取我们的 iris 数据并创建一个居中和缩放版本开始：
- en: '[PRE41]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s continue by instantiating an `SVD` and a `PCA` object:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过实例化一个 `SVD` 和一个 `PCA` 对象来继续：
- en: '[PRE42]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let''s apply both `SVD` and `PCA` to our raw `iris` data, centered version,
    and scaled version to compare:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将 `SVD` 和 `PCA` 都应用于我们的原始 `iris` 数据、居中版本和缩放版本以进行比较：
- en: '[PRE43]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This shows us that the SVD module will return the same components as PCA if
    our data is scaled, but different components when using the raw unscaled data.
    Let''s continue with our hotel data:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这表明，如果我们的数据是缩放的，SVD 模块将返回与 PCA 相同的成分，但在使用原始未缩放数据时，成分将不同。让我们继续使用我们的酒店数据：
- en: '[PRE44]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s make a scree plot as we would with our PCA module to see the explained
    variance of our SVD components:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们制作一个类似于我们 PCA 模块的 scree 图，以查看我们的 SVD 成分的解释方差：
- en: '[PRE46]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This gives us the following plot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下图表：
- en: '![](img/bc448fa3-a329-490b-b08d-645646befc7b.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bc448fa3-a329-490b-b08d-645646befc7b.png)'
- en: We can see that 1,000 components capture about 30% of the variance. Now, let's
    set up our LSA pipeline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，1000 个成分捕捉了大约 30% 的方差。现在，让我们设置我们的 LSA 管道。
- en: Latent semantic analysis
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在语义分析
- en: '**Latent semantic analysis** (**LSA**) is a feature extraction tool. It is
    helpful for text that is a series of these three steps, which we have already
    learned in this book:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义分析**（**LSA**）是一个特征提取工具。它对于文本是一系列这三个步骤的情况很有帮助，这些步骤我们已经在本书中学过了：'
- en: A tfidf vectorization
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tfidf 向量化
- en: A PCA (SVD in this case to account for the sparsity of text)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA（在这种情况下使用 SVD 以处理文本的稀疏性）
- en: Row normalization
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行归一化
- en: 'We can create a scikit-learn pipeline to perform LSA:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个 scikit-learn 管道来执行 LSA：
- en: '[PRE47]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we can fit and transform our sentences data, like so:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以拟合和转换我们的句子数据，如下所示：
- en: '[PRE48]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We have `118151` rows and `10` columns. These 10 columns come from the 10 extracted
    PCA/SVD components. We can now apply a `KMeans` clustering to our `lsa_sentences`,
    as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `118151` 行和 `10` 列。这 10 列来自 10 个提取的 PCA/SVD 成分。我们现在可以对我们的 `lsa_sentences`
    应用 `KMeans` 聚类，如下所示：
- en: '[PRE49]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We are assuming that the reader has basic familiarity with clustering. For more
    information on clustering and how clustering works, please refer to* Principles
    of Data Science* by Packt: [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设读者对聚类有基本的了解。有关聚类和聚类如何工作的更多信息，请参阅 Packt 的《数据科学原理》：[https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
- en: It should be noted that we have chosen both 10 for the `KMeans` and our PCA.
    This is not necessary. Generally, you may extract more columns in the SVD module.
    With the `10` clusters, we are basically saying here, *I think there are 10 topics
    that people are talking about. Please assign each sentence to be one of those
    topics*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，我们选择了 `KMeans` 和我们的 PCA 中的 `10`。这不是必要的。通常，你可能在 SVD 模块中提取更多的列。有了这 `10`
    个簇，我们在这里基本上是在说，*我认为有 10 个主题是人们正在讨论的。请将每个句子分配给这些主题之一*。
- en: 'The output is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE50]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s time our fit and predict for our original document-term matrix of shape
    (`118151, 280901`) and then for our latent semantic analysis of shape (`118151,
    10`) to see the differences:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的原始文档-词矩阵（形状为 `118151, 280901`）和我们的潜在语义分析（形状为 `118151, 10`）计时，以查看差异：
- en: 'First, the original dataset:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，原始数据集：
- en: '[PRE51]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This gives us:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们：
- en: '[PRE52]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will also time the prediction of `Kmeans`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将计时 `Kmeans` 的预测：
- en: '[PRE53]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This gives us:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们：
- en: '[PRE54]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, the LSA:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，LSA：
- en: '[PRE55]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This gives us:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们：
- en: '[PRE56]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can see that the LSA is over 80 times faster than fitting on the original
    tfidf dataset. Suppose we time the prediction of the clustering with LSA, like
    so:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到，LSA 在拟合原始 tfidf 数据集上比原来快了80多倍。假设我们用 LSA 来预测聚类的耗时，如下所示：
- en: '[PRE57]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This gives us:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们：
- en: '[PRE58]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We can see that the LSA dataset is over four times faster than predicting on
    the original `tfidf` dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，LSA 数据集在预测上比在原始 `tfidf` 数据集上快了四倍以上。
- en: 'Now, let''s transform the texts to a cluster distance space where each row
    represents an observation, like so:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将文本转换为一个聚类距离空间，其中每一行代表一个观察结果，如下所示：
- en: '[PRE59]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output gives us:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出给我们：
- en: '[PRE60]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can now get the distribution of topics, as follows:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以得到主题的分布，如下所示：
- en: '[PRE61]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This gives us each topic with a list of the most *interesting* phrases (according
    to our `TfidfVectorizer`):'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给我们每个主题一个包含最*有趣*短语（根据我们的 `TfidfVectorizer`）的列表：
- en: '[PRE62]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We can see the top terms by cluster, and some of them make a lot of sense. For
    example, cluster 1 seems to be about how people would recommend this hotel to
    their family and friends, while cluster 9 is about the staff and how they are
    friendly and helpful. In order to complete this application, we want to be able
    to predict new reviews with topics.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个聚类的顶级术语，其中一些非常有意义。例如，聚类1似乎是在讨论人们如何向家人和朋友推荐这家酒店，而聚类9则是关于员工以及他们如何友好和乐于助人。为了完成这个应用，我们希望能够用主题来预测新的评论。
- en: 'Now, we can try to predict the cluster of a new review, like so:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试预测一条新评论的聚类，如下所示：
- en: '[PRE63]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output gives us cluster 1 for the first prediction and cluster 9 for the
    second prediction, as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出给我们第一个预测的聚类1和第二个预测的聚类9，如下所示：
- en: '[PRE64]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Cool! `Cluster 1` corresponds to the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！`Cluster 1` 对应以下内容：
- en: '[PRE65]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`Cluster 9` corresponds to the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cluster 9` 对应以下内容：'
- en: '[PRE66]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Looks like `Cluster 1` is recommending a hotel and `Cluster 9` is more staff-centered.
    Our predictions appear to be fairly accurate!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 `Cluster 1` 是在推荐酒店，而 `Cluster 9` 则更侧重于员工。我们的预测似乎相当准确！
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw two different case studies from two vastly different
    domains using many of the feature engineering methods learned in this book.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了两个来自截然不同领域的不同案例研究，使用了本书中学到的许多特征工程方法。
- en: We do hope that you have found the contents of this book interesting and that
    you'll continue your learning! We leave it to you to keep exploring the world
    of feature engineering, machine learning, and data science. It is hoped that this
    book has been a catalyst for you to go out and learn even more about the subject.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实希望您觉得这本书的内容有趣，并且会继续您的学习！我们将探索特征工程、机器学习和数据科学的世界留给您。希望这本书能成为您进一步学习的催化剂，去学习更多关于这个主题的知识。
- en: 'For further reading past this book, I highly recommend looking into well-known
    data science books and blogs, such as:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完这本书之后，我强烈推荐查阅一些知名的数据科学书籍和博客，例如：
- en: '*Principles of Data Science* by Sinan Ozdemir, available through Packt at: [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinan Ozdemir 著的*数据科学原理*，可在 Packt 购买：[https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
- en: '*Machine Learning* and *AI* blog, KD-nuggets ([https://www.kdnuggets.com/](https://www.kdnuggets.com/))'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习*和*人工智能*博客，KD-nuggets ([https://www.kdnuggets.com/](https://www.kdnuggets.com/))'
