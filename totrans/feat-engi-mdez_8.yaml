- en: Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book has gone through several different feature engineering algorithms
    and we have worked with many different datasets. In this chapter, we will go through
    a few case studies to help you deepen your understanding of the topics we have
    covered in the book. We will work through two full-length case studies from beginning
    to end to further understand how feature engineering tasks can help us create
    machine learning pipelines for real-life applications. For each case study, we
    will go through:'
  prefs: []
  type: TYPE_NORMAL
- en: The application that we are working towards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data in question that we are using
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up our machine learning pipelines and gathering metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreover, we will be going through the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting hotel reviews data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Case study 1 - facial recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first case study will be to predict the labels for image data with a popular
    dataset called the **Labeled Faces** in the `Wild` dataset from the scikit-learn
    library. The dataset is called the `Olivetti Face` dataset and it comprises pictures
    of famous people's faces, with appropriate labels. Our task is that of **facial
    recognition**, a supervised machine learning model that is able to predict the
    name of the person given an image of their face.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of facial recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image processing and facial recognition are far-reaching. The ability to quickly
    discern people's faces from a crowd of people in video/images is vital for physical
    security as well as for giant social media companies. Search engines such as Google,
    with their image search capabilities, are using image recognition algorithms to
    match images and quantify similarities to a point where we can upload a photo
    of someone to get all other images of that same person.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with loading in our dataset and several other import statements
    we will be using to plot our data. It is good practice to begin a Jupyter notebook
    (iPython) with all the import statements you will be using. Obviously, you may
    get partway through your work and realize that you need to import a new package;
    also, to stay organized, it is a good idea to keep them in the beginning of your
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block includes the `import` statements we will be using
    for this case study. We will utilize each import in the example and it will become
    clear to you what each of them is used for as we work out our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can get started! We proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load in our dataset and see what we are working with. We will
    use the `fetch_flw_people` function built in with scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have a couple of optional parameters that we've invoked,
    specifically, `min_faces_per_person` and `resize`. The first parameter will only
    retain the pictures of people who are in the minimum number of different pictures
    that we specify. We have set this to be a minimum of `70` different pictures per
    person. The `resize` parameter is the ratio used to resize each face picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect the image arrays to find shapes for plotting the images. We
    can do this with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see that we have `1288` samples (images) and each image has a height of `50`
    pixels and a width of `37` pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s set up the `X` and `y` for our machine learning pipeline. We will
    grab the `data` attribute of the `lfw_people` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The fact that `n_features` ends up having `1,850` columns comes from the fact
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bd7220-096c-4238-a4ec-57a8f079f069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now see the full shape of our data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Some data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have 1,288 rows by 1,850 columns. To do some brief exploratory analysis,
    we can plot one of the images by using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c5a295c-e25a-4aba-a87b-eb10055ee563.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s plot the same image after applying a scaling module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives us this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following image for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cab13f64-db92-4d96-85e1-c5a09eac1391.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that the image is slightly different, with darker pixels
    around the face. Now, let''s set up the label to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Applied facial recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can move on to the machine learning pipelines that will be used to
    create our facial recognition models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by creating `train`, `test`, and `split` in our dataset, as shown
    in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to perform a **Principal Component Analysis** (**PCA**) on our
    dataset. We will want to instantiate a `PCA` first and ensure that we `scale`
    our data before applying PCA in our pipeline. This can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can `fit` our pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be our print statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the scree plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can see that starting at 100 components captures over 90% of the variance,
    compared to the 1,850 original features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a function to plot our PCA components, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call our `plot_gallery` function, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output gives us these images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfc5f3aa-df26-487f-8d23-c0c237f96fe6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This lets us see our PCA components for a specific row and column! These **eigen-faces**
    are extracted features of humans that the PCA module is finding. Compare this
    to our result in [Chapter 7](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml), *Feature
    Learning*, where we used PCA to extract **eigen-digits**. Each of these components
    is meant to house vital information about faces that can be used to distinguish
    between different people. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigen-face in the third row, fourth column seems to be highlighting the
    moustache and beard areas in order to quantify how much facial hair would help
    in separating out our classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigen-face in the first row, fourth column seems to be showing a contrast
    between the background and the face, putting a number to the lighting situation
    of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, these are interpretations by us, and different eigen-faces for different
    face datasets will output different images/components. We will move on to create
    a function that will allow us to clearly print a more readable confusion matrix
    with heat labels and options for normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit without using PCA to see the difference. We will invoke our
    `plot_confusion_matrix` function so that we can visualize the accuracy of our
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f34a3ca8-41f8-436c-abc1-1316a76b1da3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using only raw pixels, our linear model was able to achieve **81.3%** accuracy.This
    time, let''s apply PCA to see what the difference will be. We will hardcode the
    number of components to extract to 200 for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output with PCA looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b072798-7cb0-4dcd-bfe7-c8da8d0daee0.png)'
  prefs: []
  type: TYPE_IMG
- en: Interesting! We can see that our accuracy went down to **73.9%** and our time
    to predict went up by applying PCA. We should not get discouraged, however; this
    likely means that we have not found the optimal number of components to use yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot a few of the predicted names versus the true names within our test
    set to see some of the errors/correct labels that our models are producing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c5e4b2b-e868-4921-8619-7696a6be742a.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a great way to visualize our results when working with images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now implement a grid search to find the best model and accuracy for
    our data. First, we will create a function that will perform the grid search for
    us and print the accuracy, parameters, average time to fit, and average time to
    score neatly for us. This function is created like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a larger grid search pipeline that includes many more components,
    namely:'
  prefs: []
  type: TYPE_NORMAL
- en: A scaling module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PCA module to extract the best features that capture the variance in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Linear Discriminat Analysis** (**LDA**) module to create features that best
    separate the faces from one another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our linear classifier, which will reap the benefits of our three feature engineering
    modules and attempt to distinguish between our faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for creating large grid search pipeline is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our model accuracy has improved by a good amount, and also our
    time to predict and train is very fast!
  prefs: []
  type: TYPE_NORMAL
- en: Case study 2 - predicting topics of hotel reviews data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our second case study will take a look at hotel reviews data and attempt to
    cluster the reviews into topics. We will be employing a **latent semantic analysis**
    (**LSA**), which is a name given to the process of applying a PCA on sparse text
    document—term matrices. It is done to find latent structures in text for the purpose
    of classification and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of text clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text **clustering** is the act of assigning different topics to pieces of text
    for the purpose of understanding what documents are about. Imagine a large hotel
    chain that gets thousands of reviews a week from around the world. Employees of
    the hotel would like to know what people are saying in order to have a better
    idea of what they are doing well and what can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the limiting factor here is the ability for humans to read all of
    these texts quickly and correctly. We can train machines to identify the types
    of things that people are talking about and then predict the topics of new and
    incoming reviews in order to automate this process.
  prefs: []
  type: TYPE_NORMAL
- en: Hotel review data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset that we will use to achieve this result comes from Kaggle and can
    be found here at: [https://www.kaggle.com/datafiniti/hotel-reviews](https://www.kaggle.com/datafiniti/hotel-reviews).
    It contains over 35,000 distinct reviews of 1,000 different hotels around the
    world. Our job will be to isolate the text of the reviews and identify *topics*
    (what people are talking about). Then, we''ll create a machine learning model
    that can predict/identify the topics of incoming reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s organize our import statements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s load in our data, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Once we have imported our data, let's work to take a peek into what our raw
    text data looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration of the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the `shape` of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is showing us that we are working with 35,912 rows and 19 columns. Eventually,
    we will be concerned only with the column that contains the text data, but for
    now, let''s see what the first few rows look like to get a better sense of what
    is included in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **address** | **categories** | **city** | **country** | **latitude** |
    **longitude** | **name** | **postalCode** | **province** | **reviews. date** |
    **reviews. dateAdded** | **reviews. doRecommend** | **reviews. id** | **reviews.
    rating** | **reviews. text** | **reviews.****title** | **reviews.****userCity**
    | **reviews.****username** | **reviews.****userProvince** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-09-22T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 4.0 | Pleasant 10 min walk along the sea front to th... | Good location
    away from the crouds | NaN | Russ (kent) | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-04-03T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | Really lovely hotel. Stayed on the very top fl... | Great
    hotel with Jacuzzi bath! | NaN | A Traveler | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2014-05-13T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | Ett mycket bra hotell. Det som drog ner betyge... | Lugnt
    l��ge | NaN | Maud | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2013-10-27T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | We stayed here for four nights in October. The... | Good location
    on the Lido. | NaN | Julie | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | Riviera San Nicol 11/a | Hotels | Mableton | US | 45.421611 | 12.376187
    | Hotel Russo Palace | 30126 | GA | 2015-03-05T00:00:00Z | 2016-10-24T00:00:25Z
    | NaN | NaN | 5.0 | We stayed here for four nights in October. The... | ������
    ��������������� | NaN | sungchul | NaN |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s only include reviews from the United States in order to try and include
    only English reviews. First, let''s plot our data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fadf710-709b-4507-b163-984b4fa244b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the purpose of making our dataset a bit easier to work with, let''s use
    pandas to subset the reviews and only include those that came from the United
    States:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4a20b9e-82a0-47d8-803f-9b2dc532bd15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks like a map of the U.S.! Let''s `shape` our filtered dataset now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We have 30,692 rows and 19 columns. When we write reviews for hotels, we usually
    write about different things in the same review. For this reason, we will attempt
    to assign topics to single sentences rather than to the entire review.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, let''s grab the text column from our data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The clustering model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can tokenize the text into sentences so that we expand our dataset. We imported
    a function called `sent_tokenize` from the `nltk` package (natural language toolkit).
    This function will take in a single string and output the sentence as an ordered
    list of sentences separated by punctuation. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We will apply this function to our entire corpus using some reduce logic in
    Python. Essentially, we are applying the `sent_tokenize` function to each review
    and creating a single list called `sentences` that will hold all of our sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see how many sentences we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us 118,151—the number of sentences we have to work with. To create
    a document-term matrix, let''s use `TfidfVectorizer` on our sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try to fit a PCA to this data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running this code, we get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The is error tells us that for PCA, we cannot have a sparse input, and it suggests
    that we use **TruncatedSVD**. **singular value decomposition **(**SVD**) is a
    matrix *trick* for computing the same PCA components (when the data is centered)
    that allow us to work with sparse matrices. Let's take this suggestion and use
    the `TruncatedSVD` module.
  prefs: []
  type: TYPE_NORMAL
- en: SVD versus PCA components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we move on with our hotel data, let''s do a quick experiment with our
    `iris` data to see whether our SVD and PCA really give us the same components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by grabbing our iris data and creating both a centered and a scaled
    version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s continue by instantiating an `SVD` and a `PCA` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s apply both `SVD` and `PCA` to our raw `iris` data, centered version,
    and scaled version to compare:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that the SVD module will return the same components as PCA if
    our data is scaled, but different components when using the raw unscaled data.
    Let''s continue with our hotel data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a scree plot as we would with our PCA module to see the explained
    variance of our SVD components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc448fa3-a329-490b-b08d-645646befc7b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that 1,000 components capture about 30% of the variance. Now, let's
    set up our LSA pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Latent semantic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent semantic analysis** (**LSA**) is a feature extraction tool. It is
    helpful for text that is a series of these three steps, which we have already
    learned in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: A tfidf vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PCA (SVD in this case to account for the sparsity of text)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create a scikit-learn pipeline to perform LSA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit and transform our sentences data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We have `118151` rows and `10` columns. These 10 columns come from the 10 extracted
    PCA/SVD components. We can now apply a `KMeans` clustering to our `lsa_sentences`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We are assuming that the reader has basic familiarity with clustering. For more
    information on clustering and how clustering works, please refer to* Principles
    of Data Science* by Packt: [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that we have chosen both 10 for the `KMeans` and our PCA.
    This is not necessary. Generally, you may extract more columns in the SVD module.
    With the `10` clusters, we are basically saying here, *I think there are 10 topics
    that people are talking about. Please assign each sentence to be one of those
    topics*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s time our fit and predict for our original document-term matrix of shape
    (`118151, 280901`) and then for our latent semantic analysis of shape (`118151,
    10`) to see the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the original dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also time the prediction of `Kmeans`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the LSA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the LSA is over 80 times faster than fitting on the original
    tfidf dataset. Suppose we time the prediction of the clustering with LSA, like
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the LSA dataset is over four times faster than predicting on
    the original `tfidf` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s transform the texts to a cluster distance space where each row
    represents an observation, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now get the distribution of topics, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us each topic with a list of the most *interesting* phrases (according
    to our `TfidfVectorizer`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We can see the top terms by cluster, and some of them make a lot of sense. For
    example, cluster 1 seems to be about how people would recommend this hotel to
    their family and friends, while cluster 9 is about the staff and how they are
    friendly and helpful. In order to complete this application, we want to be able
    to predict new reviews with topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can try to predict the cluster of a new review, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output gives us cluster 1 for the first prediction and cluster 9 for the
    second prediction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Cool! `Cluster 1` corresponds to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`Cluster 9` corresponds to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Looks like `Cluster 1` is recommending a hotel and `Cluster 9` is more staff-centered.
    Our predictions appear to be fairly accurate!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw two different case studies from two vastly different
    domains using many of the feature engineering methods learned in this book.
  prefs: []
  type: TYPE_NORMAL
- en: We do hope that you have found the contents of this book interesting and that
    you'll continue your learning! We leave it to you to keep exploring the world
    of feature engineering, machine learning, and data science. It is hoped that this
    book has been a catalyst for you to go out and learn even more about the subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'For further reading past this book, I highly recommend looking into well-known
    data science books and blogs, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Principles of Data Science* by Sinan Ozdemir, available through Packt at: [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning* and *AI* blog, KD-nuggets ([https://www.kdnuggets.com/](https://www.kdnuggets.com/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
