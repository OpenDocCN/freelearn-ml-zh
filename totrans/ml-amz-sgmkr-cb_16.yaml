- en: '*Chapter 13*: Model Governance and MLOps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to build, understand, and deploy models.
    We will now learn how to govern these models and how to responsibly use these
    models in operations. In earlier chapters, we discussed the methods for understanding
    the business problem, the system in which the model will operate, and the potential
    consequences of using a model's predictions. **MLOps** is a word made up of **machine
    learning and DevOps**. It is made of processes and practices to efficiently, reliably,
    and effectively operationalize the production of **machine learning** (**ML**)
    models within an enterprise. MLOps aims to ensure commercial value and regulatory
    requirements are met continuously by ensuring production models' outcomes are
    of good quality and automation is in place. It provides a centralized system to
    manage the entire life cycle of all ML models in production.
  prefs: []
  type: TYPE_NORMAL
- en: Activities within MLOps cover all aspects of model deployment, provide real-time
    tracking accuracy of models in production, offer a champion challenger process
    that continuously learns and evaluates models using real-time data, track model
    bias and fairness, and provide a **model governance** framework to ensure that
    models continue to deliver business impact while meeting the regulatory requirements.
    In [*Chapter 8*](B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116), *Model Scoring
    and Deployment*, we covered model deployment on the DataRobot platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, in [*Chapter 8*](B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116),
    *Model Scoring and Deployment*, we extensively covered aspects of monitoring models
    in production. Given the crucial role model governance plays within the MLOps
    process, in this chapter, we will introduce the model governance framework. One
    key aspect of model monitoring is to ensure that models are not biased and are
    fair towards all people impacted by the model, which we will explore in this chapter.
    After that, we will take a deeper look at how to enable other aspects of MLOps,
    including how to maintain and monitor models. Thus, we''re going to cover the
    following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Governing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing model bias and fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notifications and changing models in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most parts of this chapter require access to the DataRobot software. The example
    utilizes a relatively small dataset, **Book-Crossing**, consisting of three tables,
    whose manipulation was described earlier in [*Chapter 10*](B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139),
    *Recommender Systems*. As will be covered in the data description, we will create
    new fields in addition to those used in [*Chapter 10*](B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139),
    *Recommender Systems*.
  prefs: []
  type: TYPE_NORMAL
- en: Book-Crossing dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example used to illustrate the aspects of model governance is the same as
    the one used for building recommendation systems in [*Chapter 10*](B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139),
    *Recommender Systems*. The dataset is based on the Book-Crossing dataset by Cai-Nicolas
    Ziegler and colleagues ([http://www2.informatik.uni-freiburg.de/~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)).
    The data was collected during a 4-week crawl from the Book-Crossing community
    between August and September 2004\.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using this dataset, the authors of this book have informed the owner
    of the dataset about its usage in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen (2005).
    Improving Recommendation Lists Through Topic Diversification. Proceedings of the
    14th International World Wide Web Conference (WWW 2005). May 2010–2014, 2005,
    Chiba, Japan*'
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent three tables, provided in `.csv` format, make up this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Users: This table presents the profile of the users, with anonymized `User-ID`
    and presented as integers. Also provided are the user `Location` and `Age`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Books: This table contains the characteristics of the books. Its features include
    `ISBM`, `Book-Title`, `Book-Author`, `Year-Of-Publication`, and `Publisher`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ratings: This table shows the ratings. Each row provides a user''s rating for
    a book. The `Book-Rating` is either implicit as `0` or explicit between `1` and
    `10` (the higher, the more appreciated). However, within the context of this project,
    we will focus solely on ratings that are explicit for the model development. The
    table also includes the `User-ID` and `ISBN` fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, using Excel, we created two extra fields using age and a rating
    column. We created the `RatingClass` field, which considers a rating over `7`
    as a `High` rating or else it is `Low`. Similarly, we created the `AgeGroup` field;
    this classes ages over 40 as `Over Forty` and those under 25 as `Under 25`, or
    else they are considered simply `Between 25 and 40`. Finally, we dropped out data
    rows with a missing age column.
  prefs: []
  type: TYPE_NORMAL
- en: Governing models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Organizations using ML governance define a framework of rules and controls for
    managing the ML workflows pertaining to model development, production, and post-production
    monitoring. The commercial importance of ML is well established. Still, only a
    fraction of companies investing in ML are realizing the benefits. Some establishments
    have struggled to ensure that the outcomes of ML projects are well aligned with
    their strategic direction. Importantly, many organizations are subject to regulations,
    such as the recently implemented General Data Protection Regulation within the
    European Union and European Economic Area, which affect the use of these models
    and their outputs. Businesses, in general, need to steer their ML use to ensure
    regulatory requirements are satisfied and strategic goals and values are continually
    realized.
  prefs: []
  type: TYPE_NORMAL
- en: Having an established governance framework in place ensures that data scientists
    can focus on the innovative part of their role, which is solving new problems.
    With governance, data scientists spend less time assessing the commercial value
    their models are delivering to the business, evaluating models' performance of
    production, and examining whether there has been data drift. Model governance
    simplifies the model versioning and change tracking process for all production
    models. This is always a key aspect of ML audit trailing. In addition, notifications
    could be set up to alert stakeholders when a model in production encounters anomalies
    and changes in performance. When there is a significant decline in performance,
    models in production could be swapped with better-performing challenger models
    in a seamless fashion. Although this process might require reviews and authorization
    from other stakeholders, it is much more simple and straightforward than a typical
    data science workflow.
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that governing models throughout the entire process is a complex
    and time-consuming undertaking. Without tool support, it is easy for the data
    science teams to miss key steps. Tools such as DataRobot make this task easier,
    ensuring that many required tasks are performed automatically. This ease of use
    can also sometimes make the teams use these tools without thinking. This can be
    dangerous too. Thus, a judicious mix of using the tools such as DataRobot and
    setting up process controls and reviews is needed to ensure proper governance.
  prefs: []
  type: TYPE_NORMAL
- en: DataRobot's MLOps provides organizations with an ML model governance framework
    that helps in the management of risks. Using the model governance tool, a business
    executive can track important business metrics and ensure that regulatory requirements
    are met on a continuous basis. They can easily assess the model performance in
    production to ensure that models are fit for purpose. Furthermore, with governance
    in place, the commercial criticality of models is defined before deployment. This
    ensures that when models are critical to the business, certain changes to the
    model need to be reviewed and authorized by stakeholders before such changes are
    fully implemented. In line with ethics, the use of ML models is expected to enable
    a fair process. So, models' outputs should be purged of any form of biases. In
    subsequent sections in this chapter, in addition to other aspects of MLOps, we
    will examine how bias could be mitigated in the ML model in development as well
    as in production.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing model bias and fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key characteristic of ML lies in its learning from the past to predict the
    future. This implies that future predictions would be influenced by the past.
    Some training datasets are structured in ways that could introduce bias into ML
    models. These biases are based on unspoken unfairness evident in human systems.
    Bias is known to maintain prejudice and unfairness that preexisted the models
    and could lead to unintended consequences. An AI system that is unable to understand
    human bias mirrors, if not exacerbates, the bias present in the training dataset.
    It is easy to see why women are more likely to receive lower salary predictions
    by ML models than their male counterparts. In a similar example, credit card companies
    using historic data-driven ML models could be steered into offering higher rates
    to individuals from minority backgrounds. Such **unwarranted associated** are
    caused by human bias that is inherent in the training dataset. It is unfair to
    include bias-laden features with unbiased ones in model development. A fair process
    considers an individual's payment history in making predictions about their credit,
    but unfair outcomes are possible when predictions are made based on the payment
    history of their family.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning models can be particularly unfair, as certain data has circular
    dependency. For instance, to obtain a credit card, people need a credit history,
    and to have credit histories, credit cards are required. Since models are critical
    to credit assessment, it becomes nearly impossible for some people to get a credit
    card. Also, limited data about certain subgroups makes them more vulnerable to
    bias. This is because a minimal outcome distribution change in training data for
    such groups could skew the prediction outcomes for members of the group. These
    all point toward the extent to which ML models should manage bias and support
    a fair process.
  prefs: []
  type: TYPE_NORMAL
- en: Many industries – for instance, health care, insurance, and banking – take specific
    measures to guard against any form of bias and unfairness as a regulatory expectation.
    While it is inherently challenging to address bias in humans, it is somewhat easier
    to address ML bias. So, as part of ML governance, addressing ML bias could be
    pivotal in ensuring that their products don't amplify the skepticism about the
    ethical aspects of ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addressing potential unwarranted outcomes, DataRobot has introduced a bias
    and fairness monitoring and control capability. This capability is selected and
    configured during model development. Let''s step back and demonstrate how bias
    could be addressed in DataRobot. As with the typical platform, we upload the data
    as described in the preceding chapters. In the project configuration window (as
    within *Figure 13.1*), we open **Advanced Options** and the **Bias and Fairness**
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: It is within this tab that we define protected features, how fairness is established
    and measured, as well as the target variable. We specify the fields in the prediction
    dataset that need to be protected. These are entered in the `AgeGroup` field is
    selected as to be protected (see *Figure 13.1*). In some industry datasets, attributes
    such as sex, ethnicity, age, and religion must be selected. In this way, DataRobot
    manages and presents metrics to measure any potential model bias within each of
    the protected fields:![Figure 13.1 – Configuring bias and fairness during the
    model development
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_13_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.1 – Configuring bias and fairness during the model development
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, the `RatingClass` level of `High`. This enables the measurement of bias
    on this level of the target variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Proportional Parity`, `Equal Parity`, `Prediction Balance`, `True Favorable
    Rate & True Unfavorable Rate Parity`, and `Favorable Predictive & Unfavorable
    Predictive Value Parity`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a user is unsure of the metric to choose, they can click on **Help Me Choose**,
    which presents a further set of questions. Answering these questions presents
    a recommendation of a **Fairness Metric** value, as shown in *Figure 13.2*:![Figure
    13.2 – Fairness Metric recommendation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_13_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.2 – Fairness Metric recommendation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In choosing our metric, because we are keenly interested in our model having
    similar prediction accuracy across age group membership, the **Equal Error** option
    is selected in response to how we want to measure model fairness. Since our outcome
    distribution is somewhat balanced between high and low, we choose **No** to the
    **Does the favorable target outcome occur for a very small percentage of the population?**
    question. Following this, DataRobot suggests **True Favorable Rate & True Unfavorable
    Rate Parity**. All throughout the process, the platform offers a description of
    the options and presents an explanation of the recommended metric, as well as
    those for other metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A click on `AgeGroup` `Light Gradient Boosting on Elastic Predictions` model
    presented in *Figure 13.3* is below the default threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Per-Class Bias exploration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – Per-Class Bias exploration
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this outcome, the accuracy of the model in predicting the true
    unfavorable outcome (of a `Low` rating) for individuals within the `Between 25
    and 40` class is lower than the other two classes. The score for this class falls
    below the default 80% threshold. The default threshold of 80% was applied for
    `Between 25 and 40` class. *Figure 13.4* demonstrates how **Cross-Class Accuracy**,
    a set of more holistic accuracy metrics, could be used to assess accuracy across
    the protected classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Cross-Class Accuracy examination'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Cross-Class Accuracy examination
  prefs: []
  type: TYPE_NORMAL
- en: '`AgeGroup` class. As the outcome in *Figure 13.4* suggests, the accuracy of
    the model seems to be lower for the `Between 25 and 40` class across all accuracy
    measures. Because, as earlier alluded to, the performance of the model is similar
    across classes when it is the favorable class, only the lower true rate for the
    unfavorable outcome for the `Between 25 and 40` class seems to affect the fairness
    of the model. Because models learn from past data, exploring the features that
    might be responsible for this bias might be crucial in taking further actions.
    *Figure 13.5* shows the **Cross-Class Data Disparity** capability, which presents
    deep dives into why bias exists in ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – A Cross-Class Data Disparity comparison between two age classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – A Cross-Class Data Disparity comparison between two age classes
  prefs: []
  type: TYPE_NORMAL
- en: To explore the rationale behind the model bias, the `Age-Group` feature seems
    to affect the model's accuracy. This is because `Age-Group`, being the predicted
    variable, will have the largest disparity in comparison to other variables, as
    it is identical to the predicted variable. The `year` book had a lower data disparity
    but had greater importance than the `Age-Group` feature. Further examination of
    the distribution of the year in the right-hand chart (*Figure 13.6*) shows that
    older books and books with a missing year seem to have been rated more by the
    `Over Forty` group in comparison with the `Between 25 and 40` group. On the contrary,
    the `Between 25 and 40` cohorts seem to be rated more of the newer books than
    their older counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: 'When model bias exceeds an enterprise-established threshold, steps need to
    be taken to manage this unfairness. Options to address this unfairness include
    dropping features that might be responsible for the bias and retraining the model,
    or changing the model for a more ethical model. Most of the time, these changes
    ultimately affect the overall accuracy of the model. However, in our example case,
    `Light Gradient Boosting on Elastic Predictions` wasn''t our best-performing model.
    DataRobot has within its bias and fairness toolkit the **Bias vs Accuracy** leaderboard
    comparison capability (see *Figure 13.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Bias vs Accuracy leaderboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 – Bias vs Accuracy leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: The `Keras Residual AutoInt Classifier using Training Schedule` was the most
    accurate model and met the ethic threshold. In this case, this model could be
    deployed into production. It is important to note that neural network-based models
    are generally not accepted by many regulators today, but this could change in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: Processes concerning the assessment of ML model bias and fairness are expected
    to be integrated into the data science workflow to ensure model outcomes support
    a fair process. This becomes more important as conversations pertaining to ethical
    AI are becoming more ubiquitous across industries. Having looked at ways to ensure
    models are fair, we now progress into deploying the fair model, monitoring model
    performance in production, and other aspects of implementing MLOps in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataRobot, through its MLOps suite, provides capabilities to enable users to
    not only deploy models in production, but govern, monitor, and manage the models
    in production. In previous chapters, we have looked at how models are deployed
    on the platform and using the Python API client. MLOps provides an automated model
    monitoring capability, which tracks the service health, accuracy, and data drift
    of models in production. The automated real-time monitoring of production models
    ensures that models have high-quality outputs. Also, when there is a performance
    degeneration, stakeholders are notified, so action can be taken.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on aspects of model monitoring that we didn't
    cover in [*Chapter 8*](B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116), *Model
    Scoring and Deployment*, of this book. We looked at how to examine the quality
    of deployment services, as well as changes in the underline feature distribution
    between the training and prediction data across time through the service health
    and data drift capabilities. As time passes, more recent data with target variables
    is introduced to the deployment. DataRobot can then examine models' initial predictions
    and establish models' actual accuracy in production. DataRobot also provides the
    capability to switch between alternative models in production. This section focuses
    on the evaluation of production model accuracy, setting up notifications, as well
    as switching models in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can guess by now, the job of the data science team does not end once
    a model is deployed. We now must monitor our models in production. After models
    have been deployed, before engaging in conversations pertaining to the monitoring
    of models, we need to control what individuals can do with those deployments.
    Stakeholder roles and responsibilities are important aspects of MLOps governance.
    Successful implementation of ML solutions depends on a clear definition of roles
    and what the actual duties of stakeholders are throughout ML models'' production
    life cycles. As *Figure 13.7* highlights, when deployments are shared with other
    stakeholders, each stakeholder is given a role that defines their access level
    to that deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Sharing deployments'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.7 – Sharing deployments
  prefs: []
  type: TYPE_NORMAL
- en: To open the deployment sharing window (as shown in *Figure 13.7*), after the
    model was deployed, the deployment action button (the triple dash icon) on the
    top right-hand side was selected. Then, **Share** was chosen. Here, this **RatingClass
    Predictions** deployment was shared with a stakeholder, [ben@aaaa.com](mailto:ben@aaaa.com).
    Importantly, this individual was given the role of **User**. With the **User**
    role, this stakeholder can write and read. In an actual sense, they can view the
    deployment, consume predictions, view deployment inventory, use the API to get
    data, and add other users to the deployment. The **Owner** level has additional
    administration rights and can perform business-critical operations, such as deleting
    the deployment, replacing the model, and editing the deployment metadata. The
    lowest user role is **Consumer**, which only allows stakeholders the right to
    consume predictions via the API route.
  prefs: []
  type: TYPE_NORMAL
- en: Production model monitoring ensures that models continue to deliver high-quality
    business impact as expected during development. A decline in this quality is a
    result of an alteration in the production data distribution or changes in the
    extent to which features affect the endogenous variable. For instance, changes
    in usage affect customer attrition, a variable of importance to a business. During
    a holiday period, the predictions for attrition would be higher. Such fluctuations
    in attrition prediction cause worry to the business if they are not expecting
    this change in distribution or data drift. In the same way, the extent to which
    predictor variables could influence a business outcome could also change. A point
    in case could be the effect of price on the propensity to buy. During the peak
    of a pandemic, individuals are far more conservative in their purchase of non-essentials.
    Now, imagine the chances of the accuracy of an in-production buying propensity
    model built for a non-essential product built before the pandemic. It is easy
    to see that the accuracy of the model will decay in production quite rapidly,
    thus having a significant impact on the business performance. Such situations
    raise the need to monitor the performance of models post-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116), *Model Scoring
    and Deployment*, we covered data drift, which examines changes in distribution
    between the training and production datasets, while accounting for their feature
    importance. Here, our focus will shift to monitoring the effect of variables on
    outcomes while in production. Changes in this effect could be established through
    the monitoring of production models'' accuracy, a capability DataRobot offers.
    As part of the **Deployments Settings** window, as shown in *Figure 13.8*, there
    is an **Accuracy** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Deployment window for accuracy setup'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.8 – Deployment window for accuracy setup
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Accuracy** tab offers insight into the accuracy of production models.
    This capability allows users the ability to examine the performance of their production
    models over time. To compute the accuracy of a production model, actual outcomes
    need to be provided. After actuals have been uploaded, to generate accuracies,
    a set of fields needs to be completed. These include the **Actual Response** and
    **Association ID** fields, as well as those that are optional, **Was acted on**
    and **Timestamp** (see *Figure 13.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Accuracy setup features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.9 – Accuracy setup features
  prefs: []
  type: TYPE_NORMAL
- en: 'The `RatingClass`. To link this to the earlier prediction dataset, `rowid`
    in this example, is requested to enable this connection. It is important to note
    that sometimes as a result of the models'' predictions, action is taken by the
    business that could ultimately influence the outcome. To account for this possibility
    in calculating accuracy, the **Was acted on** and **Timestamp** variables are
    optionally requested (see *Figure 13.10* for the selection of these features):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Production accuracy identification feature selection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.10 – Production accuracy identification feature selection
  prefs: []
  type: TYPE_NORMAL
- en: 'After the mandatory variables are selected, the **Save** button is clicked.
    This sets the computation off, thereafter opening the **Accuracy** window, displaying
    the production accuracy of the model. The performance of the production model
    is presented as tiles and as a graphical time series. *Figure 13.11* presents
    the **Deployment Accuracy** window. The **LogLoss**, **AUC**, **Accuracy**, **Kolmogorov-Smirmov**,
    and **Gini Norm** metrics tiles are selected. **Start** shows the model''s performance
    against the holdout dataset during the development process. It appears that this
    model is better in production than during the training. Through the customize
    tiles, other metrics and their order could be chosen. The **Accuracy over Time**
    graph shows how the accuracy of the model has changed over time. The leftmost
    green spot on the graph indicates the model accuracy against the holdout dataset
    during development:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – production model performance assessment over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.11 – production model performance assessment over time
  prefs: []
  type: TYPE_NORMAL
- en: The `Low`. There is an option to change the class being explored. It is important
    to note that with these, the accuracy of the model on the differing levels of
    the `AgeClass` protected variable could be monitored. This could be done by selecting
    `AgeClass` in the **Segment Attribute** option and then choosing either of the
    levels in the **Segment Value** field. While in the present scenario production
    accuracy mirrors those of data drift, it is possible to configure notifications
    so that stakeholders are notified when metrics depart in a manner that adversely
    affects the business. In the next section, we will cover these notifications,
    as well as how to change models in deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Notifications and changing models in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have established why the commercial impact of models can
    decay and ways to track this impact in the DataRobot platform. In cases where
    the end-to-end prediction process is fully automated and human intervention is
    limited, it becomes crucial that systems that notify stakeholders of any significant
    changes in the performance of production models are available. DataRobot can send
    notifications for significant changes in service health, data drift, and accuracy.
    These notifications can be set up and configured within the **Deployment** window:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the **Settings** tab, select **Notifications**. As shown in *Figure 13.12*,
    three options are presented: notifications being sent for all events, notifications
    for critical events, and no notifications being sent. Notifications for all events
    are sent by email; all changes to the deployments are emailed to the owner:![Figure
    13.12 – Deployment Notifications setup'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_13_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.12 – Deployment Notifications setup
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In *Figure 13.12*, notifications are set to notify me about only critical deployment
    activities. This setting implies that the stakeholders are notified when there
    are critical activities occurring on the deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `1:00`. There are options to set notifications to occur anywhere between
    an hourly and quarterly monitoring cadence. When the box is unchecked, the notification
    is disabled:![Figure 13.13 – Monitoring notification setup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_13_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.13 – Monitoring notification setup
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notification for `Last 7 days`, meaning that the data distribution for the preceding
    seven days is compared with that of the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being a feature drift metric, the `0.2`. Some features can be excluded from
    drift tracking using the `0.45` is entered as the `0.45` are deemed as `0.45`)
    drifts beyond a `0.2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b. Failing notifications to be sent when five or more low-importance features
    have significant drift
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Failing notifications to be sent when one or more high-importance features
    have a drift whose `0.45`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notifications pertaining to the `AUC`, `Accuracy`, `Balance Accuracy`, `LogLoss`,
    and `FVE Binomial`, among others. In this case, `Logloss` is chosen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b. The `percent` change is selected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Rules are then set for `Every day` at `1:00`. These could be configured to
    any cadence between daily and quarterly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After this setup, clicking the **Save new setting** button activates the notification
    routine. However, it is noteworthy that any stakeholder who has access to the
    deployment can configure the notifications they want to receive. When models'
    changes become significant, it might become necessary to replace the model in
    the deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The performance of production models tends to decay with time. This raises
    the need for models to replace those in a deployment. Within the MLOps offerings,
    DataRobot offers the model replacement functionality. To change the model in a
    deployment, you navigate to the **Deployment Overview** window. The **Replace
    model** option is selected from the **Action** button on the right-hand side of
    the **Deployment Overview** window (see *Figure 13.14*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking the **Replace model** option presents a **Paste DataRobot model URL**
    request. This URL is for the location at which the new model can be found when
    opened from the leaderboard:![Figure 13.14 – Production model replacement
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_13_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.14 – Production model replacement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When `Accuracy`, `Data Drift`, `Errors`, `Scheduled Refresh`, and `Scoring Speed`.
    As shown in *Figure 13.15*, `Data Drift` is selected in this case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, **Accept and replace** is clicked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Selecting the rationale for model replacement'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_13_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.15 – Selecting the rationale for model replacement
  prefs: []
  type: TYPE_NORMAL
- en: Having replaced the model in the deployment, future predictions from this deployment
    will use the updated model. It is important to highlight that model replacement
    can only be carried out by the deployment owner. There are situations when the
    commercial impact of models is significant. In such situations, it is advisable
    to test the new or challenger model in a synthetic or simulated environment before
    switching models. In the typical data science workflow, the champion/challenger
    model scenario is well established. Here, challenger models compute predictions
    and their performance is compared with the one in production, the champion. With
    the testing and impact analyses complete, we are now ready to deploy our model.
    DataRobot provides data scientists the ability to test multiple challenger models
    while the champion is in production. This simplifies the model selection process
    when a model is to be replaced.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps also offers the capability for changes in the model to be reviewed by
    different stakeholders. For this to happen, models are assigned importance levels
    as part of their deployments. These importance levels depend on the strategic
    commercial impact the model outcomes have on the business, the volume of predictions,
    and regulatory expectations. The importance levels thereafter drive who needs
    to review changes of the deployments before they are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we highlighted the value of establishing a framework guiding
    the use of ML models in businesses. ML governance capability supports users in
    ensuring that ML models continue to deliver commercial value while meeting regulatory
    expectations. Also, we set controls for what different levels of stakeholders
    can do with ML deployments. In some industries, there is a need to seriously consider
    the impact of bias in any decision process. Because ML models are based on data
    that might have been affected by human bias, it is possible that these models
    will compound such bias. As such, we explored ways to mitigate ML bias during
    and after model development.
  prefs: []
  type: TYPE_NORMAL
- en: We also examined the effects features have on the outcome variable. Such changes
    could have a critical bearing on business outcomes, hence the need to monitor
    the performance of model outcomes in production. During this chapter, we explored
    ways the performance of models could be assessed over time. Importantly, we learned
    how to configure notifications when there are significant changes in data drift
    or/and model accuracy. Additionally, we examined how a model in production could
    be switched to a challenger as needed.
  prefs: []
  type: TYPE_NORMAL
- en: We also highlighted some other MLOps features that were not covered in depth
    as part of this chapter. In the next chapter, we are going to look at what we
    think the future holds for DataRobot and automated ML in general. Also, given
    that this book is not all-encompassing with regards to DataRobot and the platform
    keeps expanding its capabilities, in the next chapter, we will point out some
    places where additional information for further development could be accessed.
  prefs: []
  type: TYPE_NORMAL
