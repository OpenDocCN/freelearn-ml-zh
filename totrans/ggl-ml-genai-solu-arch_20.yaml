- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Generative AI on Google Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌云上的生成式AI
- en: Now that we’ve covered many important topics in the world of generative AI,
    this chapter will explore generative AI specifically in Google Cloud. We will
    discuss Google’s proprietary models, such as the Gemini, PaLM, Codey, Imagen,
    and MedLM APIs, which are each designed for different kinds of tasks, from language
    processing to medical analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了生成式AI世界中的许多重要主题，本章将专门探讨谷歌云中的生成式AI。我们将讨论谷歌的专有模型，如Gemini、PaLM、Codey、Imagen和MedLM
    API，这些模型各自针对不同类型的任务而设计，从语言处理到医学分析。
- en: We will also review open source and third-party models available on Google Cloud
    via repositories such as Vertex AI Model Garden and Hugging Face.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将回顾谷歌云上通过Vertex AI Model Garden和Hugging Face等存储库提供的开源和第三方模型。
- en: Continuing from our discussion of vector databases in the previous chapter,
    we will explore various vector database options in Google Cloud, as well as potential
    use cases for each option.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上一章关于向量数据库的讨论，我们将探索谷歌云中的各种向量数据库选项，以及每个选项的潜在用例。
- en: Finally, we will use the information we’ll cover in this chapter to start building
    generative AI solutions in Google Cloud.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将利用本章所涵盖的信息开始构建谷歌云中的生成式AI解决方案。
- en: 'Specifically, this chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章涵盖了以下主题：
- en: Overview of generative AI in Google Cloud
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云中生成式AI概述
- en: Detailed exploration of Google Cloud generative AI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对谷歌云生成式AI的详细探索
- en: Implementing generative AI solutions in Google Cloud
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在谷歌云中实施生成式AI解决方案
- en: Let’s begin with a high-level overview of generative AI in Google Cloud.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对谷歌云中生成式AI的高层次概述开始。
- en: Overview of generative AI in Google Cloud
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌云中生成式AI概述
- en: The pace of generative AI development in Google Cloud is nothing short of astonishing.
    It seems like almost every day, a new model version, service, or feature is announced.
    In this section, I’ll introduce the various models and products at a high level,
    which will set the stage for deeper dives later in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云中生成式AI的发展速度令人惊叹。几乎每天都有新的模型版本、服务或功能被宣布。在本节中，我将从高层次介绍各种模型和产品，为本章后面的深入探讨奠定基础。
- en: Overall, the models and products are categorized by modality, such as text,
    code, image, and video. Let’s begin our journey with a discussion of Google’s
    various generative AI models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，模型和产品按模态分类，如文本、代码、图像和视频。让我们从讨论谷歌的各种生成式AI模型开始我们的旅程。
- en: Google’s generative AI models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌的生成式AI模型
- en: Google has created many generative AI models over the past few years, with a
    dramatic acceleration of new models and model versions launched in the past year.
    This section discusses the Google first-party foundation models that are currently
    available on Google Cloud in early 2024, starting with the quite famous “Gemini.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，谷歌创建了众多生成式AI模型，新模型和模型版本在去年推出的速度显著加快。本节讨论的是截至2024年初在谷歌云上可用的谷歌第一方基础模型，从相当著名的“Gemini”开始。
- en: Gemini API
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gemini API
- en: 'As of early 2024, Gemini is Google’s largest and most capable series of AI
    models. It comes in three model families:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2024年初，Gemini是谷歌最大、功能最强大的AI模型系列。它包括三个模型系列：
- en: '**Gemini Ultra**: This is the biggest model in the Gemini family and is intended
    for highly complex tasks. At the time of its announced launch in December 2023,
    it exceeded current state-of-the-art results on 30 of the 32 widely used academic
    benchmarks in LLM research.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini Ultra**：这是Gemini家族中最大的模型，旨在处理高度复杂任务。在2023年12月宣布推出时，它在LLM研究广泛使用的32个学术基准中的30个上超过了当前最先进的结果。'
- en: '**Gemini Pro**: While this is not the largest model, it is considered Google’s
    best model for scaling across a wide range of tasks. Its most recent version at
    the time of writing this in early 2024 is version 1.5, which introduced a one-million-token
    context window – the largest context window of any model in the industry at the
    time of its launch. This enables us to send very large amounts of data in a single
    prompt, opening new use cases and solving significant limitations of earlier LLMs.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini Pro**：虽然这不是最大的模型，但它被认为是谷歌在广泛任务中扩展的最佳模型。截至2024年初撰写本文时，其最新版本为1.5，引入了一百万个token的上下文窗口——在发布时是行业内最大的上下文窗口。这使得我们能够在单个提示中发送大量数据，开辟了新的用例，并解决了早期LLM的重大限制。'
- en: '**Gemini Nano**: This is Google’s most efficient Gemini model family and is
    intended to be loaded into the memory of a single device for performing on-device
    tasks. It was originally launched with two variants of slightly different sizes,
    Nano-1 and Nano-2.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini Nano**: 这是Google最有效的Gemini模型系列，旨在加载到单个设备的内存中，以执行设备上的任务。它最初以两种略有不同大小的变体推出，即Nano-1和Nano-2。'
- en: There are also multimodal variants – for example, **Gemini Ultra Vision** and
    **Gemini Pro Vision** – that are trained on multiple kinds of input data, including
    text, code, image, audio, and video. This means that we can mix the modalities
    in our interactions with Gemini, such as sending an image in our prompt, asking
    questions (via text) about the contents of the photo, and receiving textual outputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有多模态变体——例如，**Gemini Ultra Vision**和**Gemini Pro Vision**——这些变体在多种输入数据上进行了训练，包括文本、代码、图像、音频和视频。这意味着我们可以在与Gemini的交互中混合模态，例如在提示中发送图像，通过文本询问照片内容，并接收文本输出。
- en: PaLM API models
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PaLM API模型
- en: PaLM is Google’s **Pathways Language Model**, based on the “Pathways” system
    created by Google to build models that could perform more than one task (as opposed
    to traditional, single-purpose models). The current suite of PaLM models is based
    on the PaLM 2 release, and there are multiple offerings within this suite, such
    as PaLM 2 for Text, PaLM 2 for Chat, and Embeddings for Text, each with variants
    I’ll describe next.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM是Google的**路径语言模型**，基于Google创建的“路径”系统，旨在构建能够执行多个任务（与传统单一用途模型相对）的模型。当前PaLM模型套件基于PaLM
    2发布，并且在这个套件中有多个提供，例如PaLM 2 for Text、PaLM 2 for Chat和Text的嵌入，接下来我将描述这些变体。
- en: PaLM 2 for Text
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PaLM 2 for Text
- en: 'As the name suggests, this family of models is designed for text use cases.
    There are currently three variants of this model family:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，这个模型系列是为文本用例设计的。目前这个模型系列有三个变体：
- en: '**text-bison**: This can be used for multiple text-based language use cases,
    such as summarization, classification, and extraction. It can handle a maximum
    input of 8,192 tokens and a maximum output of 1,024 tokens.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**text-bison**: 这可以用于多种基于文本的语言用例，如摘要、分类和提取。它可以处理最多8,192个标记的输入和最多1,024个标记的输出。'
- en: '**text-unicorn**: The most advanced model in the PaLM family for complex natural
    language tasks. While it’s a more advanced model than text-bison, it has the same
    input and output token limits.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**text-unicorn**: PaLM家族中最先进的模型，用于复杂自然语言任务。虽然它比text-bison更先进，但输入和输出标记限制相同。'
- en: '**text-bison-32k**: This variant is similar to the aforementioned text-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**text-bison-32k**: 这个变体与上述text-bison类似，但可以处理最多8,192个标记的输出，以及总共最多32,768个标记（输入和输出总和）。'
- en: When we send a prompt to **PaLM 2 for Text** models, the model generates text
    as a response, and each interaction is independent. However, we can build external
    logic to chain multiple interactions. Next, we’ll discuss a set of Google Cloud
    products that are designed for interactive prompt integrations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向**PaLM 2 for Text**模型发送提示时，模型会生成文本作为响应，每次交互都是独立的。然而，我们可以构建外部逻辑来链接多个交互。接下来，我们将讨论一系列为交互式提示集成而设计的Google
    Cloud产品。
- en: PaLM 2 for Chat
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PaLM 2 for Chat
- en: 'With **PaLM 2 for Chat**, we can engage in a **multi-turn** conversation with
    the PaLM models. This product family consists of two variants:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**PaLM 2 for Chat**，我们可以与PaLM模型进行**多轮**对话。这个产品系列包括两个变体：
- en: '**chat-bison**: This variant is designed for interactive conversation use cases.
    It can handle a maximum input of 8,192 tokens and a maximum output of 2,048 tokens.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chat-bison**: 这个变体是为交互式对话用例设计的。它可以处理最多8,192个标记的输入和最多2,048个标记的输出。'
- en: '**chat-bison-32k**: This variant is similar to the aforementioned chat-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chat-bison-32k**: 这个变体与上述chat-bison类似，但可以处理最多8,192个标记的输出，以及总共最多32,768个标记（输入和输出总和）。'
- en: Given the nature of these product variants, they are well suited for use cases
    in which we want to maintain context across multiple prompts to provide a natural,
    human-like conversation experience. We can also use PaLM models to generate text
    embeddings. We’ll take a look at those models next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些产品变体的特性，它们非常适合需要跨多个提示保持上下文，以提供自然、类似人类的对话体验的用例。我们还可以使用PaLM模型生成文本嵌入。我们将在下一节中查看这些模型。
- en: PaLM 2 embedding models
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PaLM 2嵌入模型
- en: 'We’ve already generically discussed models that can be used to create embeddings.
    In this section, we’ll look at Google’s PaLM 2 embedding models, which come in
    two variants:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经泛泛地讨论了可以用来创建嵌入的模型。在本节中，我们将探讨谷歌的PaLM 2嵌入模型，这些模型也分为两种变体：
- en: '**textembedding-gecko**: The name “gecko” refers to smaller models. Considering
    that we are using these models to create embeddings for text, we generally don’t
    need a very large model for that use case, so these smaller and more efficient
    models make more sense in this context. This particular model focuses on words
    in the English language.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**textembedding-gecko**：名称“gecko”指的是较小的模型。考虑到我们使用这些模型来创建文本的嵌入，我们通常不需要一个非常大的模型来处理这种情况，因此在这些上下文中，这些较小且更高效的模型更有意义。这个特定的模型专注于英语单词。'
- en: '**textembedding-gecko-multilingual**: This variant is similar to the aforementioned
    textembedding-gecko, but this model can support over 100 languages.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**textembedding-gecko-multilingual**：这个变体与上述的textembedding-gecko类似，但这个模型可以支持超过100种语言。'
- en: So far, all of the PaLM 2 models I’ve described focus on natural human language.
    Next, we’ll discuss models that are designed for use cases involving computer
    programming languages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我描述的所有PaLM 2模型都专注于自然人类语言。接下来，我们将讨论为涉及计算机编程语言用例而设计的模型。
- en: Codey API models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Codey API模型
- en: Codey is a fun name for models that are designed to implement code use cases.
    Codey models can generate code based on natural language requests and can even
    convert code from one programming language into another. Like the text-based PaLM
    2 models, Codey models come in different variants based on whether we simply want
    the models to generate distinct responses to independent prompts, or we want to
    implement an interactive, chat-based use case. Let’s dive into each in more detail.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Codey是一个为实施代码用例而设计的模型的有趣名称。Codey模型可以根据自然语言请求生成代码，甚至可以将一种编程语言的代码转换为另一种语言。与基于文本的PaLM
    2模型一样，Codey模型根据我们是否只想让模型对独立提示生成不同的响应，或者我们想要实现一个交互式、基于聊天的用例而分为不同的变体。让我们更详细地探讨每一个。
- en: Codey for Code Generation
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Codey用于代码生成
- en: 'This set of offerings is designed for independent prompts (although, again,
    we can build logic to chain them together if we wish), and they come in two variants:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这套产品是为独立提示设计的（尽管，再次强调，如果我们愿意，我们可以构建逻辑将它们串联起来），并且它们也分为两种变体：
- en: '`write a Java function to fetch the customer_id and account_balance fields
    from the ''accounts'' table in the ''customer'' MySQL database.` This variant
    can handle a maximum input of 6,144 tokens and a maximum output of 1,024 tokens.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`编写一个Java函数，从''customer'' MySQL数据库中的''accounts''表中获取customer_id和account_balance字段。`
    这个变体可以处理最大输入为6,144个标记和最大输出为1,024个标记。'
- en: '**code-bison-32k**: This variant is similar to the aforementioned code-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**code-bison-32k**：这个变体与上述的code-bison类似，但它可以处理最大输出为8,192个标记，以及整体最大为32,768个标记（输入和输出总和）。'
- en: Next, let’s discuss model variants for interactive coding use cases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论用于交互式编码用例的模型变体。
- en: Codey for Code Chat
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Codey用于代码聊天
- en: 'This product family enables us to engage in multi-turn conversations regarding
    code. These models also come in two variants:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个产品系列使我们能够参与关于代码的多轮对话。这些模型也分为两种变体：
- en: '**codechat-bison**: This model can be used to implement chatbot conversations
    involving code-related questions. Like the code-bison model, it can handle a maximum
    input of 6,144 tokens and a maximum output of 1,024 tokens.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codechat-bison**：这个模型可以用来实现涉及代码相关问题的聊天机器人对话。与code-bison模型一样，它可以处理最大输入为6,144个标记和最大输出为1,024个标记。'
- en: '**codechat-bison-32k**: This is similar to the aforementioned codechat-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codechat-bison-32k**：这与上述的codechat-bison类似，但它可以处理最大输出为8,192个标记，以及整体最大为32,768个标记（输入和输出总和）。'
- en: In addition to the Code Generation and Code Chat models, there is also a version
    of Codey that’s used for code completion, which I’ll describe next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了代码生成和代码聊天模型之外，还有一个用于代码补全的Codey版本，我将在下面描述。
- en: Codey for Code Completion
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Codey用于代码补全
- en: The code completion variant of Codey is intended to be used in **integrated
    development environments** (**IDEs**). There is one variant of this model, called
    **code-gecko**, which is designed to act as a helpful coding assistant to help
    developers write effective code in real time. This means that it can suggest snippets
    of code within the IDE as developers are writing their code. We’re all familiar
    with predictive text and auto-correct features on our mobile phones. This is a
    similar concept, but for code, and it helps developers get their work done more
    quickly and effectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Codey 的代码补全变体旨在用于 **集成开发环境**（**IDEs**）。这个模型有一个变体，称为 **code-gecko**，它被设计成作为有用的编码助手，帮助开发者实时编写有效的代码。这意味着它可以在开发者编写代码时在
    IDE 中建议代码片段。我们都熟悉手机上的预测文本和自动纠错功能。这是一个类似的概念，但针对代码，它有助于开发者更快更有效地完成工作。
- en: 'Now, let’s switch our discussion to another modality: images.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将讨论转向另一种模式：图像。
- en: Imagen API models
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Imagen API 模型
- en: 'Imagen is the name of the suite of Google models that are designed for working
    with images, where each model in the suite can be used for different types of
    image-related use cases, such as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen 是谷歌为处理图像而设计的模型套件的名字，其中套件中的每个模型都可以用于不同类型的图像相关用例，例如以下内容：
- en: '**Image generation**: The **imagegeneration** model, as its name suggests,
    can be used to generate images based on natural language prompts. For example,
    the image in *Figure 17**.1* was generated by the prompt, “an impressionistic
    painting of a woman fishing late on a summer evening in a small boat on a placid
    river with reeds and trees in the background.” We can also interactively edit
    our pictures to refine them based on our desired outcomes:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成**：正如其名称所示，**imagegeneration** 模型可以用于根据自然语言提示生成图像。例如，*图 17.1* 中的图像是通过提示“一个夏末傍晚，一位女士在小船上在平静的河流上钓鱼的印象派画作。”生成的。我们还可以交互式地编辑我们的图片，根据我们期望的结果进行细化：'
- en: '![Figure 17.1: Image generated by Imagen](img/B18143_17_1.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.1：由 Imagen 生成的图像](img/B18143_17_1.jpg)'
- en: 'Figure 17.1: Image generated by Imagen'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1：由 Imagen 生成的图像
- en: '**Image captioning**: We can send an image in our prompt to the **imagetext**
    model, and it will generate descriptive text based on the contents of that image.
    This can be useful for many kinds of business use cases, such as generating product
    descriptions based on images in a product catalog on a retail website, generating
    captions for images in news articles, or generating “alt text” for images on websites.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像标题生成**：我们可以将图像发送到我们的 **imagetext** 模型，它将根据该图像的内容生成描述性文本。这可以用于许多商业用例，例如根据零售网站上产品目录中的图像生成产品描述，为新闻文章中的图像生成标题，或为网站上的图像生成“alt
    文本”。'
- en: '**Visual Question Answering (VQA)**: In addition to generating captions for
    our pictures, the **imagetext** model also allows us to interactively ask questions
    about the contents of images.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视觉问答（VQA）**：除了为我们的图片生成标题外，**imagetext** 模型还允许我们交互式地询问关于图像内容的问题。'
- en: '**Multimodal embeddings**: While we can use the **textembedding-gecko** models
    to generate text embeddings, the **multimodalembedding** model can generate embeddings
    for both images and text.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态嵌入**：虽然我们可以使用 **textembedding-gecko** 模型生成文本嵌入，但 **multimodalembedding**
    模型可以生成图像和文本的嵌入。'
- en: In addition to the general-purpose models I’ve just described, Google also provides
    models that have been designed to focus specifically on medical use cases. The
    next section briefly describes those models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我刚才描述的通用模型之外，谷歌还提供了专门针对医疗用例设计的模型。下一节将简要介绍这些模型。
- en: MedLM API models
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MedLM API 模型
- en: There are two model variants in this suite of models, **medlm-medium** and **medlm-large**,
    both of which are HIPAA-compliant and can be used for summarizing medical documents
    and helping healthcare practitioners with medical questions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型套件中有两种模型变体，**medlm-medium** 和 **medlm-large**，两者都符合 HIPAA 标准，可用于总结医疗文档并帮助医疗保健从业者解答医疗问题。
- en: We can expect many more models and variants to be added by Google continuously
    to support an ever-growing plethora of use cases. In addition to Google’s first-party
    models described in the previous subsections, Google Cloud supports and embraces
    open source development, something we will explore next.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以期待谷歌持续添加更多模型和变体，以支持不断增长的众多用例。除了上一小节中描述的谷歌的第一方模型之外，谷歌云支持并拥抱开源开发，这是我们接下来要探讨的。
- en: Open source and third-party generative AI models on Google Cloud
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌云上的开源和第三方生成式AI模型
- en: Google is a well-established contributor to open source communities, having
    created and contributed critically important inventions such as Android, Angular,
    Apache Beam, Go (programming language), Kubernetes, TensorFlow, and many others.
    In this section, we will explore Google’s open source generative AI models, as
    well as third-party (both open source and proprietary) generative AI models that
    we can easily use on Google Cloud. We’ll begin by discussing the Google Cloud
    Vertex AI Model Garden, which enables us to access such models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌是开源社区的一个知名贡献者，它创造了和贡献了许多至关重要的发明，如Android、Angular、Apache Beam、Go（编程语言）、Kubernetes、TensorFlow等。在本节中，我们将探讨谷歌的开源生成式AI模型，以及我们可以在谷歌云上轻松使用的第三方（开源和专有）生成式AI模型。我们将首先讨论谷歌云Vertex
    AI模型园地，它使我们能够访问这些模型。
- en: Google Cloud Vertex AI Model Garden
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI模型园地
- en: Vertex AI Model Garden is a centralized library that provides a “one-stop shop”
    that makes it easy for us to find, customize, and deploy pre-trained AI models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI模型园地是一个集中式库，提供了一个“一站式商店”，使我们能够轻松找到、定制和部署预训练的AI模型。
- en: Within Model Garden, we can access foundation models, such as Gemini and PaLM
    2, as well as open source models, such as Gemma (described shortly) and Llama
    2, and third-party models, such as Anthropic’s Claude 3\. We can also access task-specific
    models that cater to use cases such as content classification and sentiment analysis,
    among many more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型园地中，我们可以访问基础模型，如Gemini和PaLM 2，以及开源模型，如即将描述的Gemma和Llama 2，以及第三方模型，如Anthropic的Claude
    3。我们还可以访问针对特定任务的模型，例如内容分类和情感分析，以及其他许多用例。
- en: Vertex AI Model Garden is closely integrated with the rest of the Google Cloud
    and Vertex AI ecosystem, which enables us to easily build enterprise-grade solutions
    by providing access to all of the data processing and solution-building tools
    we’ve discussed in previous chapters, as well as many more that are beyond the
    scope of this book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI模型园地与谷歌云和Vertex AI生态系统紧密集成，使我们能够通过访问我们在前几章中讨论的所有数据处理和解决方案构建工具，以及许多超出本书范围的工具，轻松构建企业级解决方案。
- en: In addition to making models accessible via Vertex AI Model Garden, Google Cloud
    has established a strategic partnership with Hugging Face, which I’ll describe
    next.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过Vertex AI模型园地使模型可访问外，谷歌云还与Hugging Face建立了战略合作伙伴关系，我将在下文中进行描述。
- en: Hugging Face
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Hugging Face is both a company and a community that makes it easy to share ML
    models, tools, and datasets. While it started as a chatbot company, it rapidly
    grew in popularity due to its Model Hub and Transformer libraries, which amassed
    a broad community of contributors and made it easy to access and use large, pre-trained
    models. With the relatively new direct partnership between Google Cloud and Hugging
    Face, Google Cloud customers can now easily avail of the immense variety of models,
    tools, and datasets from within their Google Cloud environments running on services
    such as Vertex AI and **Google Kubernetes** **Engine** (**GKE**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一家公司，也是一个社区，它使分享ML模型、工具和数据集变得容易。虽然它最初是一家聊天机器人公司，但由于其模型中心和Transformer库的流行，它迅速获得了广泛的社区贡献者，并使得访问和使用大型预训练模型变得容易。随着谷歌云和Hugging
    Face之间相对较新的直接合作伙伴关系的建立，谷歌云客户现在可以轻松地从运行在Vertex AI和**Google Kubernetes Engine**（**GKE**）等服务上的谷歌云环境中获得大量模型、工具和数据集。
- en: Considering that this is a book about Google ML and generative AI, the descriptive
    discussions here will focus on Google’s models; I’ll refer you to the external
    documentation for the non-Google open source and third-party models. With that
    in mind, in the next section, I will introduce Google’s suite of open source models,
    named “Gemma.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这是一本关于谷歌机器学习和生成式AI的书，这里的描述性讨论将侧重于谷歌的模型；我将为您推荐外部文档来了解非谷歌开源和第三方模型。考虑到这一点，在下一节中，我将介绍谷歌的开源模型套件，命名为“Gemma”。
- en: Gemma
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gemma
- en: 'Gemma is a family of state-of-the-art and open source, lightweight models from
    Google (DOI citation: 10.34740/KAGGLE/M/3301) that were built from the same technology
    and research that was used to create the Gemini models. These are decoder-only,
    text-to-text LLMs with pre-trained, instruction-tuned variants and open weights,
    which were trained on data from a wide variety of sources, including web pages,
    documents, code, and mathematical texts. They are suitable for many different
    kinds of text-generation use cases, such as summarization or question answering,
    and their open weights mean that they can be customized for specific use cases.
    Also, since they are relatively lightweight, they don’t require specialized or
    large-scale computing resources to run, so we can use them in many environments,
    including a local laptop, for example, which makes it easy for developers to start
    experimenting with them.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Gemma是Google提供的一系列最先进和开源的轻量级模型（DOI引用：10.34740/KAGGLE/M/3301），这些模型是从创建Gemini模型所使用的相同技术和研究中构建的。这些是仅具有解码器功能的文本到文本的LLM，具有预训练的指令调整变体和开放权重，这些权重是在来自网页、文档、代码和数学文本等多种来源的数据上训练的。它们适用于许多不同类型的文本生成用例，如摘要或问答，它们的开放权重意味着它们可以根据特定用例进行定制。此外，由于它们相对较轻量，它们不需要专门的或大规模的计算资源来运行，因此我们可以在许多环境中使用它们，例如在本地笔记本电脑上，这使得开发者可以轻松地开始实验它们。
- en: Fun fact
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Gemma happens to be my sister’s name, so I was pleasantly surprised when that
    name was chosen for this suite of models, although I had no involvement whatsoever
    in the naming of these models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Gemma碰巧是我姐姐的名字，所以当这个名字被选用来命名这个模型系列时，我感到非常高兴，尽管我在这些模型的命名过程中没有任何参与。
- en: We will look at the Gemma models in more detail later. In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    I described the importance of embeddings and vector databases in generative AI.
    Now that we’ve explored the various generative AI models in Google Cloud, let’s
    discuss vector databases in Google Cloud.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后更详细地探讨Gemma模型。在[第15章](B18143_15.xhtml#_idTextAnchor371)中，我描述了嵌入和向量数据库在生成式AI中的重要性。现在我们已经探讨了Google
    Cloud中的各种生成式AI模型，让我们来讨论Google Cloud中的向量数据库。
- en: Vector databases in Google Cloud
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud中的向量数据库
- en: 'This section provides a brief introduction to vector databases in Google Cloud,
    and we’ll dive into them in more detail later in this chapter. I’ll begin this
    section with a simple question: “*Which Google Cloud database service provides
    vector database functionality?*” Quite simply, the answer is, “*Pretty much all
    of them, with just a* *few exceptions!*”'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了Google Cloud中的向量数据库，我们将在本章的后续部分更详细地探讨它们。我将从这个简单的问题开始： “*Google Cloud数据库服务中哪一个提供向量数据库功能？*”
    简单来说，答案是，“*几乎所有的都提供，只有少数例外!*”
- en: The next question, then, might be, “*Which one should I use?*” The answer to
    that question is a bit more nuanced. We’ll explore the options in more detail
    in this chapter, starting with a couple of easy decisions. Firstly, the reason
    almost all Google Cloud database services provide vector database functionality
    is that Google wants to make it as easy as possible for you to access this functionality.
    If you already use AlloyDB to manage your application’s operational data, you
    can easily go ahead and use AlloyDB AI for your vector database needs. If you
    use BigQuery for your analytical needs, go ahead and use BigQuery Vector Search,
    although bear in mind that BigQuery is designed primarily for large-scale data
    processing rather than for optimizing latency. If you have strict low-latency
    requirements, your workload may be better suited for one of the other Google Cloud
    options we’ll cover later.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，下一个问题可能是，“*我应该使用哪一个？*” 这个问题的答案要复杂一些。我们将在本章中更详细地探讨这些选项，从几个简单的决定开始。首先，几乎所有Google
    Cloud数据库服务都提供向量数据库功能的原因是，Google希望尽可能简化您访问此功能的过程。如果您已经使用AlloyDB来管理应用程序的操作数据，您可以轻松地继续使用AlloyDB
    AI来满足您的向量数据库需求。如果您使用BigQuery来满足您的分析需求，请继续使用BigQuery Vector Search，但请注意，BigQuery主要设计用于大规模数据处理，而不是优化延迟。如果您有严格的低延迟要求，您的负载可能更适合我们稍后将要介绍的Google
    Cloud的其他选项。
- en: If you’re starting fresh in Google Cloud and want to set up a vector database,
    begin your journey by exploring the offerings under Vertex AI. For a fully managed
    platform that enables developers to build Google-quality search experiences for
    websites, structured and unstructured data, or to integrate your applications
    with generative AI and search functionality that’s grounded in your enterprise
    data, start with Vertex AI Search and Conversation. If you want to create and
    store your vectors and rapidly search billions of semantically related items,
    look into Vertex AI Vector Search.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是 Google Cloud 的新用户并想设置矢量数据库，请从 Vertex AI 下的产品开始您的旅程。如果您需要一个完全托管的平台，使开发者能够为网站、结构化和非结构化数据构建
    Google 级别的搜索体验，或者将您的应用程序与基于企业数据的生成式 AI 和搜索功能集成，请从 Vertex AI Search 和 Conversation
    开始。如果您想创建和存储您的矢量，并快速搜索数十亿语义相关的项目，请考虑 Vertex AI Vector Search。
- en: In the next section, we will dive deeper into Google Cloud’s generative AI offerings,
    starting with a hands-on exploration of the models, and then covering the various
    vector database offerings in more detail.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更深入地探讨 Google Cloud 的生成式 AI 产品，从对模型的动手探索开始，然后更详细地介绍各种矢量数据库产品。
- en: A detailed exploration of Google Cloud generative AI
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 Google Cloud 生成式 AI 的详细探索
- en: In this section, we’ll start interacting with Google Cloud’s generative AI offerings.
    To set the stage, I’ll begin by briefly introducing Google Cloud Vertex AI Studio.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始与 Google Cloud 的生成式 AI 产品进行交互。为了设定场景，我将首先简要介绍 Google Cloud Vertex
    AI Studio。
- en: Google Cloud Vertex AI Studio
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI Studio
- en: Vertex AI Studio can be accessed within the Google Cloud console UI, and it
    provides an interface to easily start using all of the generative AI models I
    described in the previous section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Studio 可以在 Google Cloud 控制台 UI 中访问，它提供了一个界面，可以轻松开始使用我在上一节中描述的所有生成式
    AI 模型。
- en: 'To access the Google Cloud Vertex AI Studio UI, perform the following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Google Cloud Vertex AI Studio 用户界面，请执行以下步骤：
- en: In the Google Cloud console, navigate to the Google Cloud services menu and
    choose **Vertex AI**.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中，导航到 Google Cloud 服务菜单并选择 **Vertex AI**。
- en: 'Under the **Get started with Vertex AI** section, click **ENABLE ALL RECOMMENDED
    API**, as depicted in *Figure 17**.2*:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Get started with Vertex AI** 部分下，点击 *图 17.2* 中所示 **ENABLE ALL RECOMMENDED
    API**：
- en: '![Figure 17.2: Enabling the recommended APIs](img/B18143_17_2.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.2：启用推荐 API](img/B18143_17_2.jpg)'
- en: 'Figure 17.2: Enabling the recommended APIs'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2：启用推荐 API
- en: Give it a few minutes for the APIs to enable. Once the APIs have been enabled,
    you can start using them. You can interact with each of the models I described
    in the previous section by clicking on each of the modalities – such as **Language**,
    **Vision**, **Speech**, and **Multimodal** – in the menu on the left-hand side
    of the screen.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待几分钟以启用 API。一旦 API 已启用，您就可以开始使用它们。您可以通过点击屏幕左侧菜单中的每个模态（例如 **Language**、**Vision**、**Speech**
    和 **Multimodal**）来与上一节中描述的每个模型进行交互。
- en: 'For example, if we click on **Language**, a screen similar to what’s shown
    in *Figure 17**.3* will be displayed:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，如果我们点击 **Language**，将显示一个类似于 *图 17.3* 的屏幕：
- en: '![Figure 17.3: The Language section](img/B18143_17_3.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.3：语言部分](img/B18143_17_3.jpg)'
- en: 'Figure 17.3: The Language section'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：语言部分
- en: 'From here, we can click on the various links and buttons to try out the different
    models. For example, clicking the **TEXT CHAT** button will present us with a
    chat interface into which we can type our prompts, as shown in *Figure 17**.4*:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我们可以点击各种链接和按钮来尝试不同的模型。例如，点击 **TEXT CHAT** 按钮将展示一个聊天界面，我们可以在其中输入提示，如图 *图
    17.4* 所示：
- en: '![Figure 17.4: Text chat](img/B18143_17_4.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.4：文本聊天](img/B18143_17_4.jpg)'
- en: 'Figure 17.4: Text chat'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4：文本聊天
- en: 'The parameters we can configure can be found on the right-hand side of the
    screen. Let’s take a closer look:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置的参数可以在屏幕右侧找到。让我们更仔细地看看：
- en: '**Model**: The model to which we want to send a prompt.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：我们想要发送提示的模型。'
- en: '**Region**: The region in which we want to run our prompt.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域**：我们想要运行提示的区域。'
- en: '**Temperature**: This parameter configures the level of creativity or randomness
    in the model’s responses. This can be seen as the level of imagination we want
    the model to use when generating responses. If we consider the use case of an
    LLM predicting the next word in a sequence, the temperature parameter influences
    the overall probability distribution of the next word, such that a higher temperature
    can result in more creative outputs, while a lower temperature guides the model
    to provide more conservative and predictable outcomes. If we want the model to
    create imaginative art or text, for example, we could configure a high temperature,
    whereas if we want more formal, factual responses, we would configure a low temperature.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度**：此参数配置模型响应中的创造性和随机性水平。这可以看作是我们希望模型在生成响应时使用的想象力水平。如果我们考虑LLM预测序列中下一个单词的使用案例，温度参数会影响下一个单词的整体概率分布，这样更高的温度可以导致更富有创造性的输出，而更低的温度则引导模型提供更保守和可预测的结果。如果我们希望模型创作富有想象力的艺术或文本，例如，我们可以配置一个高温度，而如果我们希望得到更正式、事实性的响应，我们就会配置一个低温度。'
- en: '**Output token limit**: The maximum number of tokens we want the model to generate
    in its response.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出标记限制**：我们希望模型在其响应中生成的最大标记数。'
- en: 'We can click on the **Advanced** section at the bottom of the screen to expand
    it. Here, we will see additional parameters, such as **Max responses**, which
    configures the maximum number of example responses we want the model to return
    (note that this parameter is only valid for some types of interactions – for example,
    it is not applicable to chat interactions because chat will always provide one
    response in each turn of the conversation), as well as **Top-K** and **Top-P**.
    Like **Temperature**, **Top-K** and **Top-P** can be used to control the creativity
    and randomness of the model’s generated outputs, but they work in slightly different
    ways. Consider the case of an LLM selecting the next word (or token) in a sequence
    when generating a response. It does this by predicting the probability of the
    next word being the most likely to occur in the given sequence. If the LLM always
    only selects the word with the highest probability, then it does not allow for
    much flexibility when generating responses. However, we can use **Top-K** and
    **Top-P** to add a level of flexibility by understanding how they work. So, let’s
    take a closer look:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以点击屏幕底部的**高级**部分来展开它。在这里，我们将看到额外的参数，例如**最大响应数**，它配置了我们希望模型返回的最大示例响应数量（请注意，此参数仅适用于某些类型的交互
    – 例如，它不适用于聊天交互，因为聊天在对话的每一轮中总是会提供一个响应），以及**Top-K**和**Top-P**。与**温度**类似，**Top-K**和**Top-P**可以用来控制模型生成输出的创造性和随机性，但它们的工作方式略有不同。考虑一个LLM在生成响应时选择序列中的下一个单词（或标记）的情况。它是通过预测下一个单词在给定序列中最有可能发生的概率来做到这一点的。如果LLM总是只选择概率最高的单词，那么在生成响应时它不会提供太多的灵活性。然而，我们可以通过理解它们的工作方式来使用**Top-K**和**Top-P**来增加一层灵活性。所以，让我们更仔细地看看：
- en: '**Top-K**: This uses a probability distribution to predict the probabilities
    for all possible next words based on the current prompt and context. K is the
    number of top-probability words that we want the model to select from. For example,
    if the value of K is 3, then the model will pick the next word from the top three
    most likely words (that is, the top three words with the highest probabilities).
    If the value of K is 1, then the model will pick only the most likely (that is,
    highest probability) word. This is a specific case we refer to as **greedy selection**.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-K**：这使用概率分布来预测基于当前提示和上下文的下一个可能单词的概率。K是我们希望模型从中选择概率最高的单词的数量。例如，如果K的值为3，那么模型将从最有可能的三个单词中选择下一个单词（即概率最高的三个单词）。如果K的值为1，那么模型将只选择最有可能的（即概率最高的）单词。这是我们所说的**贪婪选择**的特定情况。'
- en: '**Top-P**: This is a bit more complex to explain, but only slightly. Rather
    than picking a specific number of the highest-probability words to choose from,
    the model will calculate the **cumulative** probability for all possible next
    words – that is, it will add up the probabilities of the most likely words until
    the sum of the probabilities reaches the specified threshold (P), at which point
    it will randomly select a word from the pool of words that fall under the probability
    threshold.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-P**：这稍微有点复杂，但也不算太复杂。模型不会选择一个特定数量的最高概率词进行选择，而是计算所有可能下一个词的**累积**概率——也就是说，它会将最可能词的概率相加，直到概率总和达到指定的阈值（P），此时它将从低于概率阈值的词池中随机选择一个词。'
- en: In the case of both **Top-K** and **Top-P**, lower values guide the outputs
    toward conservative responses, while higher values allow some wiggle room for
    more creative responses, similar to (and in conjunction with) the **Temperature**
    parameter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**Top-K**和**Top-P**，较低的值会引导输出趋向于保守的响应，而较高的值则允许一些灵活性，以便产生更具创造性的响应，类似于（并且与）**温度**参数一起。
- en: In the **Advanced** section, for some models, we can also enable **Grounding**,
    in which case we can ground our responses based on data we have stored in Vertex
    AI Search and Conversation. We also have the option to stream responses, which
    refers to printing responses as they are generated.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在**高级**部分，对于某些模型，我们还可以启用**基础**功能，在这种情况下，我们可以根据存储在 Vertex AI 搜索和对话中的数据进行响应。我们还有流式响应的选项，这指的是在生成时打印响应。
- en: I encourage you to explore the different modalities and types of models in Vertex
    AI Studio. If you need help thinking of creative prompts to write, you can ask
    one of the text-based models to suggest some examples for you to use!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励您探索 Vertex AI Studio 中的不同模态和模型类型。如果您需要帮助思考创意提示来写作，您可以要求一个基于文本的模型为您建议一些示例供您使用！
- en: In addition to accessing the various models through the Vertex AI Studio UI,
    you can also interact with the models programmatically via the REST APIs and SDK.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过 Vertex AI Studio UI 访问各种模型外，您还可以通过 REST API 和 SDK 以编程方式与模型交互。
- en: Next, let’s explore the vector database options available in Google Cloud.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索 Google Cloud 中可用的向量数据库选项。
- en: A detailed exploration of Google Cloud vector database options
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Google Cloud 向量数据库选项的详细探讨
- en: Earlier in this chapter, I mentioned that almost all of Google Cloud’s database
    services provide vector database support. In this section, we’ll take a look at
    each one in more detail, starting with Vertex AI Search and Conversation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面部分，我提到几乎所有的 Google Cloud 数据库服务都提供了向量数据库支持。在本节中，我们将更详细地查看每一个服务，从 Vertex
    AI 搜索和对话开始。
- en: Vertex AI Search and Conversation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI 搜索和对话
- en: While the main purpose of the Vertex AI Search and Conversation product suite,
    as its name suggests, is to build search and conversation applications, we can
    also use it as a vector database for implementing **retrieval augmented generation**
    (**RAG**) solutions. As we discussed earlier in this chapter, if you’re getting
    started with RAG on Google Cloud, unless you have a specific need to control the
    chunking, embedding, and indexing processes, or you have a specific operational
    need to link all of your data with another Google Cloud database service, Vertex
    AI Search and Conversation should be your first choice to consider. This is because
    it performs all of the chunking, embedding, and indexing processes for you, and
    it abstracts away all of those steps behind a simple and convenient orchestration
    interface, saving you a lot of time and effort. You can also use data connectors
    to ingest data from third-party applications, such as JIRA, Salesforce, and Confluence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Vertex AI 搜索和对话产品套件的主要目的是构建搜索和对话应用，正如其名称所暗示的，我们也可以将其用作向量数据库来实现**检索增强生成**（**RAG**）解决方案。正如我们在本章前面讨论的，如果您在
    Google Cloud 上开始使用 RAG，除非您有特别需要控制分块、嵌入和索引过程，或者您有特别的需求需要将所有数据与另一个 Google Cloud
    数据库服务链接起来，否则 Vertex AI 搜索和对话应该是您首先考虑的选择。这是因为它为您执行所有分块、嵌入和索引过程，并通过一个简单方便的编排界面抽象出所有这些步骤，为您节省了大量时间和精力。您还可以使用数据连接器从第三方应用程序，如
    JIRA、Salesforce 和 Confluence 中摄取数据。
- en: If you need to specifically control the chunking, embedding, and indexing processes,
    you can choose from other Google Cloud database offerings, as described next.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要特别控制分块、嵌入和索引过程，您可以选择其他 Google Cloud 数据库服务，如以下所述。
- en: Vertex AI Vector Search
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI 向量搜索
- en: As we’ve seen throughout this book, Vertex AI provides an entire ecosystem of
    tools and services for pretty much everything we could wish to do in the realm
    of ML and artificial intelligence. So, it makes sense that it would include a
    vector database, and that vector database is called Vertex AI Vector Search. It
    enables us to store and search through massive collections of embeddings to find
    the most similar or relevant items with high speed and low latency.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在整本书中看到的那样，Vertex AI 为我们提供了几乎在机器学习和人工智能领域所能做的一切的完整工具和服务生态系统。因此，它包括一个向量数据库是有意义的，而这个向量数据库被称为
    Vertex AI 向量搜索。它使我们能够以高速和低延迟存储和搜索大量嵌入，以找到最相似或相关的项目。
- en: Although we need to create the chunks, embeddings, and indexes (unlike when
    using Vertex AI Search and Conversation), it still provides a fully managed service
    in that we don’t need to worry about infrastructure or scaling because Vertex
    AI handles all of that for us. The fact that we need to create the chunks, embeddings,
    and indexes means that we can implement more granular control over those processes
    if needed, such as the models used to create our embeddings (note that this is
    also true for all of the other vector database options that we will discuss in
    the remainder of this chapter).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们需要创建块、嵌入和索引（与使用 Vertex AI 搜索和对话不同），但它仍然提供了一个完全托管的服务，我们不需要担心基础设施或扩展，因为 Vertex
    AI 会为我们处理所有这些。我们需要创建块、嵌入和索引的事实意味着，如果需要的话，我们可以对这些过程实施更细粒度的控制，例如用于创建我们的嵌入的模型（请注意，这也适用于本章余下部分我们将讨论的所有其他向量数据库选项）。
- en: Vertex AI Vector Search also integrates closely with the rest of the Vertex
    AI and Google Cloud ecosystem to serve as part of larger orchestration solutions
    requiring the use of embeddings. On the topic of the broader ecosystem, let’s
    consider additional Google Cloud vector databases beyond Vertex AI, starting with
    BigQuery.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 向量搜索还紧密集成了 Vertex AI 和 Google Cloud 生态系统中的其他部分，作为需要使用嵌入的大规模编排解决方案的一部分。在更广泛的生态系统话题上，让我们考虑
    Vertex AI 之外的额外 Google Cloud 向量数据库，从 BigQuery 开始。
- en: BigQuery Vector Search
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BigQuery 向量搜索
- en: BigQuery Vector Search is a feature within Google Cloud BigQuery that allows
    us to store and find embeddings. We can send a query embedding to BigQuery by
    using the BigQuery **VECTOR_SEARCH** function, which will quickly find similar
    embeddings within the vector database. This is a major benefit for users who are
    familiar with SQL syntax, and it enables semantic search and similarity-based
    analysis directly within your BigQuery data warehouse.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 向量搜索是 Google Cloud BigQuery 中的一个功能，允许我们存储和查找嵌入。我们可以通过使用 BigQuery 的
    **VECTOR_SEARCH** 函数将查询嵌入发送到 BigQuery，该函数将快速在向量数据库中找到相似的嵌入。这对于熟悉 SQL 语法的人来说是一个主要的好处，它使得语义搜索和基于相似性的分析可以直接在您的
    BigQuery 数据仓库中进行。
- en: We can also choose to create a vector index to speed up the embedding retrieval
    process. When we use a vector index, an **approximate nearest neighbor** (**ANN**)
    search is used to return approximate results, which is much quicker than doing
    a brute-force search to find an exact match. If we do not create a vector index,
    then a brute-force search will be performed. We also have the option to explicitly
    implement a brute-force search even when a vector index exists if we want to get
    an exact result.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择创建一个向量索引来加速嵌入检索过程。当我们使用向量索引时，会使用**近似最近邻**（**ANN**）搜索来返回近似结果，这比进行暴力搜索以找到精确匹配要快得多。如果我们不创建向量索引，那么将执行暴力搜索。我们还有选择在存在向量索引的情况下显式实现暴力搜索的选项，如果我们想要得到精确结果的话。
- en: BigQuery Vector Search is a great choice for data science teams who already
    store a lot of data in BigQuery, and it also provides the benefits of BigQuery’s
    automatic and enormous scalability. Continuing the theme of scalability, another
    database service in Google Cloud that is renowned for its impressive scalability
    is Google Cloud Spanner. Let’s take a closer look.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 向量搜索是数据科学团队的一个很好的选择，因为他们已经在 BigQuery 中存储了大量的数据，它还提供了 BigQuery 的自动和巨大可扩展性的好处。继续可扩展性的主题，Google
    Cloud 中另一个以其令人印象深刻的可扩展性而闻名的数据库服务是 Google Cloud Spanner。让我们更深入地了解一下。
- en: Spanner Vector Search
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spanner 向量搜索
- en: Google Cloud Spanner is a fully managed and highly performant database service
    that is capable of global scale. It’s often referred to as a “NewSQL” database
    because it combines the scalability and flexibility of non-relational (NoSQL)
    databases with the familiar SQL interface of relational databases. It can easily
    handle petabytes of data, and unlike many NoSQL databases, it can guarantee strong
    consistency across transactions, even when distributed globally.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Spanner 是一种完全托管且性能极高的数据库服务，能够实现全球规模。它通常被称为“NewSQL”数据库，因为它结合了非关系型（NoSQL）数据库的可扩展性和灵活性以及关系型数据库熟悉的
    SQL 接口。它可以轻松处理PB级的数据，并且与许多 NoSQL 数据库不同，即使在分布式全球的情况下，它也可以保证事务的强一致性。
- en: With all of this in mind, Spanner is suitable for applications that require
    highly distributed and strongly consistent data, such as online banking use cases.
    In early 2024, Spanner also launched support for cosine distance and Euclidean
    distance comparisons, and we can use these vector distance functions to perform
    **K-nearest neighbors** (**KNN**) vector searches for use cases such as similarity
    search or RAG. This means that this highly distributed database service that is
    commonly used for enterprise-scale, business-critical workloads now also supports
    vector database functionality.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，Spanner 适用于需要高度分布式和强一致性数据的应用程序，例如在线银行用例。到 2024 年初，Spanner 还推出了对余弦距离和欧几里得距离比较的支持，我们可以使用这些向量距离函数来执行
    **K-最近邻**（**KNN**）向量搜索，用于相似性搜索或 RAG 等用例。这意味着这种常用于企业级、业务关键工作负载的高度分布式数据库服务现在也支持向量数据库功能。
- en: While Spanner combines the best of NoSQL and relational database functionality,
    I’ll describe relational database options next before covering NoSQL options explicitly.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spanner 结合了 NoSQL 和关系型数据库功能的最佳之处，但我将先描述关系型数据库选项，然后再明确介绍 NoSQL 选项。
- en: Cloud SQL and pgvector
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Cloud SQL 和 pgvector
- en: Cloud SQL is a fully managed relational database service within Google Cloud
    that provides support for PostgreSQL, MySQL, and SQL Server. According to the
    Google Cloud documentation, “*More than 95% of Google Cloud’s top 100 customers
    use Cloud SQL to run their businesses.*” It is often seen as the default relational
    database service to use in Google Cloud, and considering that it’s fully managed,
    we don’t need to worry about server provisioning, operating system updates, or
    database patches because Google handles all of those activities for us.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud SQL 是 Google Cloud 中的一项完全托管的关系型数据库服务，它支持 PostgreSQL、MySQL 和 SQL Server。根据
    Google Cloud 文档，“*超过 95% 的 Google Cloud 前 100 名客户使用 Cloud SQL 来运营他们的业务。*”它通常被视为在
    Google Cloud 中使用的默认关系型数据库服务，考虑到它是完全托管的，我们无需担心服务器配置、操作系统更新或数据库补丁，因为 Google 会为我们处理所有这些活动。
- en: While Cloud SQL doesn’t natively support vector database capabilities, `<->`)
    to calculate distance metrics such as cosine similarity between vectors to help
    us find the most similar embeddings. What’s great is that it allows us to use
    SQL for all of those vector operations, which makes it easy to use for people
    who are familiar with PostgreSQL.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Cloud SQL 本身不支持向量数据库功能，但我们可以使用 `<->`) 来计算向量之间的距离度量，例如余弦相似度，以帮助我们找到最相似的嵌入。令人兴奋的是，它允许我们使用
    SQL 来执行所有这些向量操作，这使得对于熟悉 PostgreSQL 的人来说使用起来非常方便。
- en: In addition to using `pgvector` with Cloud SQL for PostgreSQL, we can also use
    it with AlloyDB, which I’ll describe next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在 Cloud SQL 中使用 `pgvector` 与 PostgreSQL 一起使用外，我们还可以将其与 AlloyDB 一起使用，我将在下面进行描述。
- en: AlloyDB AI Vector Search
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AlloyDB AI 向量搜索
- en: AlloyDB is a fully managed PostgreSQL-compatible database service on Google
    Cloud that’s designed for high-performance enterprise workloads. It significantly
    improves performance and scalability compared to standard PostgreSQL and provides
    advanced features such as auto-scaling and automatic failover, as well as built-in
    database backups and updates. It’s also suitable for hybrid transactional and
    analytical workloads, and recently, it has added multiple ML-related features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: AlloyDB 是 Google Cloud 上的一种完全托管且兼容 PostgreSQL 的数据库服务，专为高性能企业级工作负载而设计。与标准 PostgreSQL
    相比，它显著提高了性能和可扩展性，并提供了高级功能，如自动扩展、自动故障转移，以及内置的数据库备份和更新。它也适用于混合事务和分析工作负载，最近，它还增加了多个与机器学习相关的功能。
- en: AlloyDB AI now also includes vector similarity search as it uses `pgvector`
    natively within the database.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: AlloyDB AI 现在也包含了向量相似性搜索功能，因为它在数据库中原生地使用了 `pgvector`。
- en: Next, we’ll discuss NoSQL database options for vector similarity search in Google
    Cloud.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论在 Google Cloud 中用于向量相似性搜索的 NoSQL 数据库选项。
- en: NoSQL database options for vector similarity search in Google Cloud
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud 中用于向量相似搜索的 NoSQL 数据库选项
- en: Firestore is a NoSQL database within the Google Firebase ecosystem. Firebase
    itself is a Google Cloud framework that provides a collection of tools and services
    for developing mobile and web applications. In addition to the first-party functionality
    provided by Firebase, there’s also **Firebase Extensions Hub**, which provides
    a way for users to add more functionality to their solutions. While Firestore
    does not provide vector database support out-of-the-box, that functionality can
    be added via an extension named **Semantic Search with Vertex AI**, which adds
    text similarity search to a Firestore application by integrating with Vertex AI
    Vector Search.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Firestore 是 Google Firebase 生态系统中的 NoSQL 数据库。Firebase 本身是一个 Google Cloud 框架，它提供了一系列工具和服务，用于开发移动和
    Web 应用程序。除了 Firebase 提供的原始功能外，还有 **Firebase 扩展中心**，它为用户提供了一种向他们的解决方案添加更多功能的方式。虽然
    Firestore 默认不提供向量数据库支持，但可以通过名为 **Vertex AI 的语义搜索** 的扩展来添加该功能，该扩展通过集成 Vertex AI
    向量搜索将文本相似性搜索添加到 Firestore 应用程序中。
- en: Another highly popular NoSQL database in Google Cloud is Bigtable. Similar to
    Firestore, Bigtable does not currently provide native support for vector similarity
    search use cases, but it can integrate with Vertex AI Vector Search to help build
    a solution for that purpose.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 中，另一个非常流行的 NoSQL 数据库是 Bigtable。与 Firestore 类似，Bigtable 目前不提供对向量相似搜索用例的原生支持，但它可以与
    Vertex AI 向量搜索集成，以帮助构建该目的的解决方案。
- en: Now that we’ve covered both the relational and NoSQL database options that provide
    vector database support in Google Cloud, it’s important to describe one additional
    type of data store, called **Memorystore**, which is used to implement caching
    solutions for extremely low-latency workloads.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了在 Google Cloud 中提供向量数据库支持的数据库选项，包括关系型数据库和无数据库，现在重要的是要描述一种额外的数据存储类型，称为
    **Memorystore**，它用于实现针对极低延迟工作负载的缓存解决方案。
- en: Memorystore for Redis
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Redis 的 Memorystore
- en: Redis is an open source in-memory data store, meaning that, unlike traditional
    databases that store data on disk, Redis primarily stores data in RAM, giving
    it ultra-fast performance. Memorystore for Redis is a fully managed Redis service
    in Google Cloud that provides scalable, highly available, and secure Redis instances
    without requiring us to manage the instance or the related infrastructure.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 是一个开源的内存数据存储，这意味着与存储在磁盘上的传统数据库不同，Redis 主要在 RAM 中存储数据，从而提供超快的性能。Redis 的
    Memorystore 是 Google Cloud 中的一项完全托管 Redis 服务，它提供可扩展的、高度可用和安全的 Redis 实例，而无需我们管理实例或相关基础设施。
- en: Memorystore for Redis now also supports ANN and KNN vector search, enabling
    us to implement an in-memory vector store cache for applications that require
    extremely low-latency responses.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 的 Memorystore 现在也支持 ANN 和 KNN 向量搜索，使我们能够为需要极低延迟响应的应用程序实现内存中的向量存储缓存。
- en: At this point, we’ve covered most of the main generative AI offerings on Google
    Cloud that are available. Now, it’s time to put some of our knowledge into action
    as we begin to implement generative AI solutions on Google Cloud.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了 Google Cloud 上大多数主要的生成式人工智能服务。现在，是我们将一些知识付诸实践的时候了，我们将开始在 Google
    Cloud 上实施生成式人工智能解决方案。
- en: Implementing generative AI solutions on Google Cloud
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上实施生成式人工智能解决方案
- en: 'In this section, we will create a Vertex AI Search and Conversation application
    that will enable us to ask questions about the contents of a set of documents.
    We will use some publicly available medical study reports as examples, but this
    pattern can be applied to many different kinds of use cases. The following are
    some other popular applications of this pattern:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个 Vertex AI 搜索和对话应用程序，这将使我们能够询问关于一组文档内容的问题。我们将使用一些公开可用的医学研究报告作为示例，但这种模式可以应用于许多不同的用例。以下是一些其他流行的应用模式：
- en: Enabling users of a retail website to get information about products on the
    site via a question-and-answer interface, in which they can ask human language
    questions and get factual responses about products in the catalog.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过问答界面让零售网站的用户获取有关网站上产品的信息，他们可以提出人类语言问题，并获得有关目录中产品的实际回答。
- en: Asking questions about financial documents. For example, employees in a company’s
    finance department could upload the company’s financial documents, such as quarterly
    earnings reports, and ask natural language questions about the contents of the
    reports to identify trends or other important information.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 询问关于财务文件的问题。例如，公司财务部门的员工可以上传公司的财务文件，例如季度收益报告，并就报告内容提出自然语言问题，以识别趋势或其他重要信息。
- en: Providing natural language search over a company’s vast corpus of internal documentation.
    For example, given a simple search interface, an employee could ask, “How can
    I modify my retirement contributions?” and they can get a response that tells
    them how to do so, along with a link to the internal documentation that describes
    the process in detail.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公司庞大的内部文档库上提供自然语言搜索。例如，给定一个简单的搜索界面，员工可以询问，“我如何修改我的退休贡献？”他们可以得到一个告诉他们如何操作的响应，以及一个链接到详细描述该过程的内部文档。
- en: We will use Vertex AI Search and Conversation to build this application, as
    described in the following subsections.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Vertex AI搜索和对话API来构建此应用，具体内容将在以下小节中描述。
- en: Building a Vertex AI Search and Conversation application
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Vertex AI搜索和对话应用
- en: 'At a high level, we will perform three main activities to build our Vertex
    AI Search and Conversation application:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们将执行三个主要活动来构建我们的Vertex AI搜索和对话应用：
- en: Enable the Vertex AI Search and Conversation API.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用Vertex AI搜索和对话API。
- en: Create a data store to store the data we will use in our application.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据存储来存储我们在应用中将使用的数据。
- en: Create the Vertex AI Search and Conversation application so that it can interact
    with our data store.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Vertex AI搜索和对话应用，以便它可以与我们的数据存储交互。
- en: Let’s begin by enabling the Vertex AI Search and Conversation API.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从启用Vertex AI搜索和对话API开始。
- en: Enabling the Vertex AI Search and Conversation API
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用Vertex AI搜索和对话API
- en: 'To enable the Vertex AI Search and Conversation API, perform the following
    steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Vertex AI搜索和对话API，请执行以下步骤：
- en: In the Google Cloud console, type `search` into the search box and select **Search**
    **and Conversation**.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，在搜索框中输入`search`并选择**搜索和对话**。
- en: 'If this is your first time using this service, a page similar to what’s shown
    in *Figure 17**.5* will be displayed:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用此服务，将显示一个类似于*图17.5*的页面：
- en: '![Figure 17.5: Activating Vertex AI Search and Conversation](img/B18143_17_5.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图17.5：激活Vertex AI搜索和对话](img/B18143_17_5.jpg)'
- en: 'Figure 17.5: Activating Vertex AI Search and Conversation'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5：激活Vertex AI搜索和对话
- en: Click **CONTINUE AND ACTIVATE THE API** (this checkbox is optional – you can
    either select it or leave it blank).
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**继续并激活API**（此复选框为可选 – 您可以选择它或将其留空）。
- en: 'After a few seconds, your environment will be created. A page similar to what’s
    shown in *Figure 17**.6* will be displayed:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几秒钟后，您的环境将被创建。将显示一个类似于*图17.6*的页面：
- en: '![Figure 17.6: Vertex AI Search and Conversation environment page](img/B18143_17_6.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图17.6：Vertex AI搜索和对话环境页面](img/B18143_17_6.jpg)'
- en: 'Figure 17.6: Vertex AI Search and Conversation environment page'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6：Vertex AI搜索和对话环境页面
- en: Now that we’ve enabled the API, we can start the next prerequisite step to set
    up our application, which is to create a data store.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启用了API，我们可以开始设置应用的下一个先决步骤，即创建数据存储。
- en: Creating a data store for Vertex AI Search and Conversation
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为Vertex AI搜索和对话创建数据存储
- en: 'At the time of writing this, in March 2024, Vertex AI Search and Conversation
    can support the following data store sources:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，即2024年3月，Vertex AI搜索和对话API可以支持以下数据存储源：
- en: '**Website URLs**: Automatically crawl website content from a list of domains
    you define'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网站URL**：自动从您定义的域名列表中爬取网站内容'
- en: '**BigQuery**: Import data from your BigQuery table'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigQuery**：从您的BigQuery表中导入数据'
- en: '**Cloud Storage**: Import data from your storage bucket'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云存储**：从您的存储桶中导入数据'
- en: '**API**: Import data manually by calling the API'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API**：通过调用API手动导入数据'
- en: We’re going to use Google Cloud Storage as our data store, so we’ll begin by
    setting up a location and uploading our data there. The next subsection describes
    the process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Google Cloud Storage作为我们的数据存储，因此我们将首先设置一个位置并将我们的数据上传到那里。下一个小节将描述此过程。
- en: Note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In *Chapter 4* of this book, we created a local directory in our Cloud Shell
    environment and cloned our GitHub repository into that directory. If you did not
    perform those steps, please reference those instructions now. They can be found
    in the *Creating a directory and cloning our GitHub* *repository* section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的 *第 4 章* 中，我们在云壳环境中创建了一个本地目录，并将我们的 GitHub 仓库克隆到该目录中。如果您没有执行这些步骤，请现在参考这些说明。它们可以在
    *创建目录和克隆我们的 GitHub 仓库* 部分找到。
- en: Once you have ensured that the GitHub repository for this book has been cloned
    into the local directory in your Cloud Shell environment, continue with the following
    steps.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 确保本书的 GitHub 仓库已被克隆到您的云壳环境中的本地目录后，继续执行以下步骤。
- en: Storing our data in Google Cloud Storage
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将我们的数据存储在 Google Cloud Storage 中
- en: 'The easiest way to stage our data is to use Google Cloud Shell. To set up our
    data store, perform the following steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的数据部署到云存储的最简单方法是使用 Google Cloud Shell。要设置我们的数据存储，请执行以下步骤：
- en: 'Click the **Cloud Shell** symbol in the top-right corner of the screen, as
    shown in *Figure 17**.7*:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击屏幕右上角的**云壳**符号，如图 17.7* 所示：
- en: '![Figure 17.7: Activating Cloud Shell](img/B18143_17_7.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.7：激活云壳](img/B18143_17_7.jpg)'
- en: 'Figure 17.7: Activating Cloud Shell'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7：激活云壳
- en: This will activate the Cloud Shell environment, which will appear at the bottom
    of your screen. It will take a few seconds for your environment to activate. Once
    your environment has been activated, you can paste and run the commands mentioned
    in the following steps into Cloud Shell.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将激活云壳环境，它将出现在屏幕底部。激活环境需要几秒钟。一旦您的环境激活，您可以将以下步骤中提到的命令粘贴并运行到云壳中。
- en: 'Run the following commands to set up environment variables so that you can
    store your preferred region, your Cloud Storage bucket name, and the data path
    (**replace YOUR-REGION with your preferred region, such as us-central1, and YOUR-BUCKET-NAME
    with your** **bucket name**):'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以设置环境变量，以便您可以存储您首选的区域、云存储桶名称和数据路径（**将 YOUR-REGION 替换为您首选的区域，例如 us-central1，将
    YOUR-BUCKET-NAME 替换为您** **的桶名称**）：
- en: '[PRE0]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Verify the bucket path (you should also copy the contents of the response from
    this command and keep it for reference to be used in a later step):'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证桶路径（您还应该复制此命令的响应内容，以备后续步骤参考）：
- en: '[PRE1]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the bucket if it doesn’t already exist:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果尚不存在，则创建桶：
- en: '[PRE2]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Change directories to the location at which the data for this chapter is stored:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到存储本章数据的目录：
- en: '[PRE3]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upload the files to the bucket (this also creates the path within the bucket):'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件上传到桶中（这也会在桶内创建路径）：
- en: '[PRE4]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Verify that the files have been uploaded (the following command should list
    the files):'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证文件是否已上传（以下命令应列出文件）：
- en: '[PRE5]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we’ve uploaded the files to our Cloud Storage bucket, we can create
    the Vertex AI Search and Conversation data store for our application, as described
    in the next subsection.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文件上传到我们的云存储桶中，我们可以为我们的应用程序创建 Vertex AI 搜索和对话数据存储，具体步骤将在下一小节中描述。
- en: Note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The citations for the documents used in this exercise can be found in the **document_citations.txt**
    file in the **Chapter-17** directory of this book’s GitHub repository: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习中使用的文档的引用可以在本书 GitHub 仓库的 **Chapter-17** 目录中的 **document_citations.txt**
    文件中找到：[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt)。
- en: Creating the Vertex AI Search and Conversation data store
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建 Vertex AI 搜索和对话数据存储
- en: 'Within Vertex AI Search and Conversation, we will define a data store associated
    with the files we uploaded to Cloud Storage. To do that, perform the following
    steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Vertex AI 搜索和对话中，我们将定义一个与上传到云存储的文件关联的数据存储。为此，请执行以下步骤：
- en: In the Vertex AI Search and Conversation console UI, select **Data Stores**
    |**Create data store** | **Cloud Storage**.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Vertex AI 搜索和对话控制台 UI 中，选择 **数据存储** | **创建数据存储** | **云存储**。
- en: 'A screen similar to the one shown in *Figure 17**.8* will be displayed:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将显示与图 17.8* 类似的屏幕：
- en: '![Figure 17.8: Import data from Cloud Storage](img/B18143_17_8.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.8：从云存储导入数据](img/B18143_17_8.jpg)'
- en: 'Figure 17.8: Import data from Cloud Storage'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8：从云存储导入数据
- en: Enter the path to the files you uploaded (this is the value you specified for
    the `BUCKET_PATH` environment variable in your Cloud Shell environment in the
    previous section).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入上传文件的路径（这是您在上一节中在Cloud Shell环境中为`BUCKET_PATH`环境变量指定的值）。
- en: We will use unstructured documents in this example, so the **Unstructured documents**
    option should remain selected.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用非结构化文档，因此**非结构化文档**选项应保持选中状态。
- en: 'Click **Continue**. A screen similar to what’s shown in *Figure 17**.9* will
    be displayed:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**继续**。将显示一个类似于*图17.9*的屏幕：
- en: '![Figure 17.9: Configure your data store](img/B18143_17_9.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图17.9：配置您的数据存储](img/B18143_17_9.jpg)'
- en: 'Figure 17.9: Configure your data store'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9：配置您的数据存储
- en: Select **global (Global)** under **Multi-region**.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**多区域**下选择**全局（全球）**。
- en: Select **Digital Parser** under **Default** **document parser**.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**默认文档解析器**下选择**数字解析器**。
- en: Enter a name for the data store, such as `AIML-SA-DS`, and click **CREATE**.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据存储输入一个名称，例如`AIML-SA-DS`，然后点击**创建**。
- en: Now that the data store has been created, it’s time to create the Vertex AI
    Search and Conversation application.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据存储已创建，是时候创建Vertex AI搜索和对话应用了。
- en: Creating the Vertex AI Search and Conversation application
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Vertex AI搜索和对话应用
- en: 'To create the Vertex AI Search and Conversation application, perform the following
    steps:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Vertex AI搜索和对话应用，请执行以下步骤：
- en: In the Vertex AI Search and Conversation console UI, select **Apps** | **Create
    a new app** | **Search**.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI搜索和对话控制台UI中，选择**应用** | **创建新应用** | **搜索**。
- en: 'On the next screen that appears, ensure that the **Enterprise edition features**
    and **Advanced LLM features** checkboxes are enabled, as shown in *Figure 17**.10*
    (read the descriptions for each option to understand their features):'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后出现的屏幕上，确保**企业版功能**和**高级LLM功能**复选框已启用，如图17.10所示（阅读每个选项的描述以了解其功能）：
- en: '![Figure 17.10: Enabling Search and Conversation features](img/B18143_17_10.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图17.10：启用搜索和对话功能](img/B18143_17_10.jpg)'
- en: 'Figure 17.10: Enabling Search and Conversation features'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10：启用搜索和对话功能
- en: 'Next, enter an application name and a company name, select the location for
    the application, and select **CONTINUE**, as shown in *Figure 17**.11* (note the
    recommendation to choose a global location if you do not have compliance or regulatory
    reasons to locate your data in a particular multi-region):'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，输入应用名称和公司名称，选择应用的位置，然后选择**继续**，如图17.11所示（注意，如果没有合规性或监管原因需要在特定多区域定位您的数据，请选择全局位置）：
- en: '![Figure 17.11: Application details](img/B18143_17_11.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图17.11：应用详情](img/B18143_17_11.jpg)'
- en: 'Figure 17.11: Application details'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.11：应用详情
- en: 'Next, select the data store we created that you wish to integrate with our
    application, as shown in *Figure 17**.12*:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择您希望与我们的应用集成的数据存储，如图17.12所示：
- en: '![Figure 17.12: Selecting a data store](img/B18143_17_12.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图17.12：选择数据存储](img/B18143_17_12.jpg)'
- en: 'Figure 17.12: Selecting a data store'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.12：选择数据存储
- en: It will take some time for the data for our application to be processed. Periodically
    refresh the page to check its status. Once it’s complete, a list of files will
    be displayed. At that point, we are ready to configure how we want our application
    to behave, as described in the following subsection.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用的数据处理需要一些时间。定期刷新页面以检查其状态。一旦完成，将显示文件列表。到那时，我们就准备好配置我们希望应用如何表现，如以下小节所述。
- en: Configuring our newly created application
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置我们新创建的应用
- en: 'There are numerous ways in which our new Search and Conversation application
    can work, and we can specify how we want it to work by modifying its configuration.
    To do that, perform the following steps:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新搜索和对话应用有多种工作方式，我们可以通过修改其配置来指定我们希望它如何工作。为此，执行以下步骤：
- en: In the Vertex AI Search and Conversation console UI, select **Apps** from the
    menu on the left-hand side of the page.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI搜索和对话控制台UI中，从页面左侧菜单中选择**应用**。
- en: Click on our newly created app.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击我们新创建的应用。
- en: Select **Configurations** from the menu on the left-hand side of the page.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从页面左侧菜单中选择**配置**。
- en: 'For the **Search type** configuration, there are three options:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于**搜索类型**配置，有三个选项：
- en: '**Search**: Simply respond to the search query with a list of relevant results.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索**：只需用相关结果列表响应搜索查询。'
- en: '**Search with an answer**: Provide a generative summary above the list of search
    results.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用答案搜索**：在搜索结果列表上方提供生成摘要。'
- en: '**Search with follow-ups**: Provide conversational search with generative summaries
    and support for follow-up questions.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用后续问题搜索**：提供具有生成摘要和后续问题支持的对话搜索。'
- en: Select the **Search with** **follow-ups** option.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**使用后续问题搜索**选项。
- en: Scroll down to the **Large Language Models for summarization** section and select
    the latest version of Gemini.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到**用于摘要的大型语言模型**部分，并选择Gemini的最新版本。
- en: You can leave all other options at their default values unless you have any
    specific requirements to change them.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除非你有任何特定要求更改它们，否则你可以将所有其他选项保留在默认值。
- en: Click **SAVE** **AND PUBLISH**.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存并发布**。
- en: At this point, we are ready to start using our application, as described in
    the following subsection.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好开始使用我们的应用程序，如下一个子节所述。
- en: Using our newly created application
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用我们新创建的应用程序
- en: 'Now that we’ve created our application, we can start using it. To do that,
    perform the following steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的应用程序，我们可以开始使用它。为此，执行以下步骤：
- en: In the Vertex AI Search and Conversation console UI, select the **Preview**
    section for our newly created application.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI搜索和对话控制台UI中，选择我们新创建应用程序的**预览**部分。
- en: We will be presented with a search box in which we can ask questions about the
    contents of the documents in our data store.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将看到一个搜索框，我们可以用它来询问我们数据存储中文档的内容。
- en: 'Begin with the following question: `What are the effects of cinnamon` `on health?`'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下问题开始：`肉桂对健康有什么影响？`
- en: Click `What specific properties contribute to its ability to regulate` `glucose
    levels?`
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`哪些特定的属性有助于其调节血糖水平的能力？`
- en: Note that the follow-up question simply mentions “it” and does not mention cinnamon
    directly. However, the model uses the context from the previous question to understand
    our intent.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，后续问题只是简单地提到“它”，并没有直接提到肉桂。然而，模型使用前一个问题中的上下文来理解我们的意图。
- en: Also, note that the generated answers may contain references, and the referenced
    documents that were used to generate the answer are listed in the response.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，请注意，生成的答案可能包含参考文献，用于生成答案的参考文献列在响应中。
- en: 'Try out the following additional questions:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试以下附加问题：
- en: '`How does the daily dosage of cinnamon used in the study compare to typical
    dietary` `cinnamon intake?`'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`研究中使用的每日肉桂剂量与典型的饮食肉桂摄入量相比如何？`'
- en: '`Were there any observed long-term effects of cinnamon supplementation beyond
    the 4-week` `trial period?`'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`在4周试验期之后，是否有观察到肉桂补充剂的任何长期效果？`'
- en: '`How might cinnamon supplementation interact with other dietary or lifestyle
    interventions for` `prediabetes management?`'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`肉桂补充剂如何与其他饮食或生活方式干预措施相互作用以管理糖尿病前期？`'
- en: '`Could the study''s findings on cinnamon''s glucose-regulating effects extend
    to individuals with types 1 or` `2 diabetes?`'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`该研究关于肉桂调节血糖作用的发现能否扩展到1型或2型糖尿病患者？`'
- en: 'Next, let’s ask some questions that will cause the model to respond based on
    other documents in the collection. Try out the following questions:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们提出一些问题，这些问题将使模型根据集合中的其他文档进行响应。尝试以下问题：
- en: '`What are the main challenges FIM programs face when attempting to use EHR
    data` `for evaluation?`'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FIM项目在尝试使用EHR数据进行评估时面临的主要挑战是什么？`'
- en: '`How can FIM programs overcome the barriers to accessing and utilizing EHR`
    `data effectively?`'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FIM项目如何克服有效访问和利用EHR数据的障碍？`'
- en: '`What alternative data sources can FIM programs use to evaluate health outcomes
    and healthcare utilization, apart from` `EHR data?`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FIM项目除了EHR数据之外，还可以使用哪些替代数据源来评估健康结果和医疗保健利用？`'
- en: '`How do the privacy and liability concerns of healthcare partners impact the
    sharing of EHR data with` `FIM programs?`'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`医疗保健合作伙伴的隐私和责任担忧如何影响与FIM项目共享EHR数据？`'
- en: '`What role does albumin play in the nutritional status of children, and how
    are red bean cookies effective in` `improving it?`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`白蛋白在儿童的营养状况中扮演什么角色，红豆饼干如何有效改善它？`'
- en: '`What specific dietary interventions have shown promise in modifying the gut
    microbiome to improve outcomes for patients` `with diseases?`'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`哪些具体的饮食干预措施已显示出在改善患有疾病的患者肠道微生物群方面的前景？`'
- en: '`How does the gut microbiome''s interaction with the body impact mental health
    disorders, and what mechanisms` `are involved?`'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`肠道微生物群与身体的相互作用如何影响心理健康障碍，以及涉及哪些机制？`'
- en: '`Can changes in the gut microbiome serve as early indicators for the development
    of chronic` `kidney disease?`'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`肠道微生物组的改变能否作为慢性肾脏病发展的早期指标？`'
- en: These are just some examples – feel free to play around with additional documents
    and questions, and think about how this pattern can be extended to pretty much
    any use case due to the vast knowledge of these wondrous models.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是其中的一些例子——请随意尝试使用额外的文档和问题，并思考这种模式如何因为这些奇妙模型的广泛知识而扩展到几乎任何用例。
- en: That’s it! You have successfully built your first generative AI application
    in Google Cloud! As you can see, Vertex AI Search and Conversation makes it very
    easy for us to do this. Behind the scenes, this can be seen as a RAG solution
    because we are interacting with the Gemini model and getting it to generate responses
    that are grounded in the contents of the documents we uploaded to our data store,
    although Vertex AI Search and Conversation abstracts away and manages all of the
    complexities and steps required to implement the solution.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！你已经成功地在Google Cloud中构建了你的第一个生成式AI应用！正如你所见，Vertex AI搜索和对话使我们能够非常容易地做到这一点。幕后，这可以被视为一个RAG解决方案，因为我们正在与Gemini模型交互，并使其生成基于我们上传到数据存储的文档内容的响应，尽管Vertex
    AI搜索和对话抽象并管理了实现解决方案所需的全部复杂性和步骤。
- en: In the next chapter, we will build some additional and more complex use cases.
    First, however, let’s summarize what we’ve learned in this chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建一些额外的更复杂的用例。然而，首先，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dived into generative AI in Google Cloud, exploring Google’s
    native generative AI models, such as the Gemini, PaLM, Codey, Imagen, and MedLM
    APIs. We discussed the multiple versions of each model and some example use cases
    for each. Then, we introduced Vertex AI Studio and discussed open source and third-party
    models available on Google Cloud via repositories such as Vertex AI Model Garden
    and Hugging Face.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了Google Cloud中的生成式AI，探索了Google的本地生成式AI模型，如Gemini、PaLM、Codey、Imagen和MedLM
    API。我们讨论了每个模型的多个版本以及每个的一些示例用例。然后，我们介绍了Vertex AI Studio，并讨论了通过如Vertex AI模型花园和Hugging
    Face等存储库在Google Cloud上可用的开源和第三方模型。
- en: Next, we discussed vector databases in Google Cloud, covering various options
    available, such as Vertex AI Search and Conversation, Vertex AI Vector Search,
    BigQuery Vector Search, Spanner Vector Search, `pgvector`, and AlloyDB AI Vector
    Search, including some decision factors for choosing one solution over another.
    It is these kinds of decision points that are often most important in the role
    of a solutions architect, and the decisions will vary based on the specific needs
    of the customer or project, including the cost of each solution. I recommend always
    consulting the latest pricing information for each product and factoring that
    into your decision process.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了Google Cloud中的向量数据库，涵盖了各种可用的选项，例如Vertex AI搜索和对话、Vertex AI向量搜索、BigQuery向量搜索、Spanner向量搜索、`pgvector`和AlloyDB
    AI向量搜索，包括一些在选择一种解决方案而不是另一种时的决策因素。这些决策点通常在解决方案架构师的角色中最为重要，并且决策将根据客户或项目的具体需求而变化，包括每种解决方案的成本。我建议始终咨询每个产品的最新定价信息，并将其纳入你的决策过程中。
- en: Finally, we put some of this chapter’s topics into action and built a generative
    AI application in Google Cloud – specifically, we built a Vertex AI Search and
    Conversation application that enabled us to ask natural language questions about
    the contents of a collection of documents.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将本章的一些主题付诸实践，并在Google Cloud中构建了一个生成式AI应用——具体来说，我们构建了一个Vertex AI搜索和对话应用，使我们能够就文档集合的内容提出自然语言问题。
- en: In the next chapter, we will continue with this theme of using the topics we’ve
    covered throughout this book to build solutions in Google Cloud. Join me there
    to start diving in further!
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用本书中涵盖的主题，在Google Cloud中构建解决方案。请加入我，开始更深入地探索！
