- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve covered many important topics in the world of generative AI,
    this chapter will explore generative AI specifically in Google Cloud. We will
    discuss Google’s proprietary models, such as the Gemini, PaLM, Codey, Imagen,
    and MedLM APIs, which are each designed for different kinds of tasks, from language
    processing to medical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We will also review open source and third-party models available on Google Cloud
    via repositories such as Vertex AI Model Garden and Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing from our discussion of vector databases in the previous chapter,
    we will explore various vector database options in Google Cloud, as well as potential
    use cases for each option.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use the information we’ll cover in this chapter to start building
    generative AI solutions in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of generative AI in Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed exploration of Google Cloud generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing generative AI solutions in Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with a high-level overview of generative AI in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of generative AI in Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pace of generative AI development in Google Cloud is nothing short of astonishing.
    It seems like almost every day, a new model version, service, or feature is announced.
    In this section, I’ll introduce the various models and products at a high level,
    which will set the stage for deeper dives later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the models and products are categorized by modality, such as text,
    code, image, and video. Let’s begin our journey with a discussion of Google’s
    various generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Google’s generative AI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google has created many generative AI models over the past few years, with a
    dramatic acceleration of new models and model versions launched in the past year.
    This section discusses the Google first-party foundation models that are currently
    available on Google Cloud in early 2024, starting with the quite famous “Gemini.”
  prefs: []
  type: TYPE_NORMAL
- en: Gemini API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of early 2024, Gemini is Google’s largest and most capable series of AI
    models. It comes in three model families:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gemini Ultra**: This is the biggest model in the Gemini family and is intended
    for highly complex tasks. At the time of its announced launch in December 2023,
    it exceeded current state-of-the-art results on 30 of the 32 widely used academic
    benchmarks in LLM research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini Pro**: While this is not the largest model, it is considered Google’s
    best model for scaling across a wide range of tasks. Its most recent version at
    the time of writing this in early 2024 is version 1.5, which introduced a one-million-token
    context window – the largest context window of any model in the industry at the
    time of its launch. This enables us to send very large amounts of data in a single
    prompt, opening new use cases and solving significant limitations of earlier LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini Nano**: This is Google’s most efficient Gemini model family and is
    intended to be loaded into the memory of a single device for performing on-device
    tasks. It was originally launched with two variants of slightly different sizes,
    Nano-1 and Nano-2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also multimodal variants – for example, **Gemini Ultra Vision** and
    **Gemini Pro Vision** – that are trained on multiple kinds of input data, including
    text, code, image, audio, and video. This means that we can mix the modalities
    in our interactions with Gemini, such as sending an image in our prompt, asking
    questions (via text) about the contents of the photo, and receiving textual outputs.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM API models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PaLM is Google’s **Pathways Language Model**, based on the “Pathways” system
    created by Google to build models that could perform more than one task (as opposed
    to traditional, single-purpose models). The current suite of PaLM models is based
    on the PaLM 2 release, and there are multiple offerings within this suite, such
    as PaLM 2 for Text, PaLM 2 for Chat, and Embeddings for Text, each with variants
    I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM 2 for Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As the name suggests, this family of models is designed for text use cases.
    There are currently three variants of this model family:'
  prefs: []
  type: TYPE_NORMAL
- en: '**text-bison**: This can be used for multiple text-based language use cases,
    such as summarization, classification, and extraction. It can handle a maximum
    input of 8,192 tokens and a maximum output of 1,024 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-unicorn**: The most advanced model in the PaLM family for complex natural
    language tasks. While it’s a more advanced model than text-bison, it has the same
    input and output token limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-bison-32k**: This variant is similar to the aforementioned text-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we send a prompt to **PaLM 2 for Text** models, the model generates text
    as a response, and each interaction is independent. However, we can build external
    logic to chain multiple interactions. Next, we’ll discuss a set of Google Cloud
    products that are designed for interactive prompt integrations.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM 2 for Chat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With **PaLM 2 for Chat**, we can engage in a **multi-turn** conversation with
    the PaLM models. This product family consists of two variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**chat-bison**: This variant is designed for interactive conversation use cases.
    It can handle a maximum input of 8,192 tokens and a maximum output of 2,048 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chat-bison-32k**: This variant is similar to the aforementioned chat-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the nature of these product variants, they are well suited for use cases
    in which we want to maintain context across multiple prompts to provide a natural,
    human-like conversation experience. We can also use PaLM models to generate text
    embeddings. We’ll take a look at those models next.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM 2 embedding models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ve already generically discussed models that can be used to create embeddings.
    In this section, we’ll look at Google’s PaLM 2 embedding models, which come in
    two variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**textembedding-gecko**: The name “gecko” refers to smaller models. Considering
    that we are using these models to create embeddings for text, we generally don’t
    need a very large model for that use case, so these smaller and more efficient
    models make more sense in this context. This particular model focuses on words
    in the English language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**textembedding-gecko-multilingual**: This variant is similar to the aforementioned
    textembedding-gecko, but this model can support over 100 languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, all of the PaLM 2 models I’ve described focus on natural human language.
    Next, we’ll discuss models that are designed for use cases involving computer
    programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Codey API models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Codey is a fun name for models that are designed to implement code use cases.
    Codey models can generate code based on natural language requests and can even
    convert code from one programming language into another. Like the text-based PaLM
    2 models, Codey models come in different variants based on whether we simply want
    the models to generate distinct responses to independent prompts, or we want to
    implement an interactive, chat-based use case. Let’s dive into each in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Codey for Code Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This set of offerings is designed for independent prompts (although, again,
    we can build logic to chain them together if we wish), and they come in two variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '`write a Java function to fetch the customer_id and account_balance fields
    from the ''accounts'' table in the ''customer'' MySQL database.` This variant
    can handle a maximum input of 6,144 tokens and a maximum output of 1,024 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**code-bison-32k**: This variant is similar to the aforementioned code-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s discuss model variants for interactive coding use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Codey for Code Chat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This product family enables us to engage in multi-turn conversations regarding
    code. These models also come in two variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**codechat-bison**: This model can be used to implement chatbot conversations
    involving code-related questions. Like the code-bison model, it can handle a maximum
    input of 6,144 tokens and a maximum output of 1,024 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codechat-bison-32k**: This is similar to the aforementioned codechat-bison,
    but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768
    tokens (combined input and output).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the Code Generation and Code Chat models, there is also a version
    of Codey that’s used for code completion, which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: Codey for Code Completion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code completion variant of Codey is intended to be used in **integrated
    development environments** (**IDEs**). There is one variant of this model, called
    **code-gecko**, which is designed to act as a helpful coding assistant to help
    developers write effective code in real time. This means that it can suggest snippets
    of code within the IDE as developers are writing their code. We’re all familiar
    with predictive text and auto-correct features on our mobile phones. This is a
    similar concept, but for code, and it helps developers get their work done more
    quickly and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s switch our discussion to another modality: images.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagen API models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagen is the name of the suite of Google models that are designed for working
    with images, where each model in the suite can be used for different types of
    image-related use cases, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image generation**: The **imagegeneration** model, as its name suggests,
    can be used to generate images based on natural language prompts. For example,
    the image in *Figure 17**.1* was generated by the prompt, “an impressionistic
    painting of a woman fishing late on a summer evening in a small boat on a placid
    river with reeds and trees in the background.” We can also interactively edit
    our pictures to refine them based on our desired outcomes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 17.1: Image generated by Imagen](img/B18143_17_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.1: Image generated by Imagen'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image captioning**: We can send an image in our prompt to the **imagetext**
    model, and it will generate descriptive text based on the contents of that image.
    This can be useful for many kinds of business use cases, such as generating product
    descriptions based on images in a product catalog on a retail website, generating
    captions for images in news articles, or generating “alt text” for images on websites.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual Question Answering (VQA)**: In addition to generating captions for
    our pictures, the **imagetext** model also allows us to interactively ask questions
    about the contents of images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal embeddings**: While we can use the **textembedding-gecko** models
    to generate text embeddings, the **multimodalembedding** model can generate embeddings
    for both images and text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the general-purpose models I’ve just described, Google also provides
    models that have been designed to focus specifically on medical use cases. The
    next section briefly describes those models.
  prefs: []
  type: TYPE_NORMAL
- en: MedLM API models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two model variants in this suite of models, **medlm-medium** and **medlm-large**,
    both of which are HIPAA-compliant and can be used for summarizing medical documents
    and helping healthcare practitioners with medical questions.
  prefs: []
  type: TYPE_NORMAL
- en: We can expect many more models and variants to be added by Google continuously
    to support an ever-growing plethora of use cases. In addition to Google’s first-party
    models described in the previous subsections, Google Cloud supports and embraces
    open source development, something we will explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Open source and third-party generative AI models on Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google is a well-established contributor to open source communities, having
    created and contributed critically important inventions such as Android, Angular,
    Apache Beam, Go (programming language), Kubernetes, TensorFlow, and many others.
    In this section, we will explore Google’s open source generative AI models, as
    well as third-party (both open source and proprietary) generative AI models that
    we can easily use on Google Cloud. We’ll begin by discussing the Google Cloud
    Vertex AI Model Garden, which enables us to access such models.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI Model Garden
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vertex AI Model Garden is a centralized library that provides a “one-stop shop”
    that makes it easy for us to find, customize, and deploy pre-trained AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Within Model Garden, we can access foundation models, such as Gemini and PaLM
    2, as well as open source models, such as Gemma (described shortly) and Llama
    2, and third-party models, such as Anthropic’s Claude 3\. We can also access task-specific
    models that cater to use cases such as content classification and sentiment analysis,
    among many more.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Garden is closely integrated with the rest of the Google Cloud
    and Vertex AI ecosystem, which enables us to easily build enterprise-grade solutions
    by providing access to all of the data processing and solution-building tools
    we’ve discussed in previous chapters, as well as many more that are beyond the
    scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to making models accessible via Vertex AI Model Garden, Google Cloud
    has established a strategic partnership with Hugging Face, which I’ll describe
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hugging Face is both a company and a community that makes it easy to share ML
    models, tools, and datasets. While it started as a chatbot company, it rapidly
    grew in popularity due to its Model Hub and Transformer libraries, which amassed
    a broad community of contributors and made it easy to access and use large, pre-trained
    models. With the relatively new direct partnership between Google Cloud and Hugging
    Face, Google Cloud customers can now easily avail of the immense variety of models,
    tools, and datasets from within their Google Cloud environments running on services
    such as Vertex AI and **Google Kubernetes** **Engine** (**GKE**).
  prefs: []
  type: TYPE_NORMAL
- en: Considering that this is a book about Google ML and generative AI, the descriptive
    discussions here will focus on Google’s models; I’ll refer you to the external
    documentation for the non-Google open source and third-party models. With that
    in mind, in the next section, I will introduce Google’s suite of open source models,
    named “Gemma.”
  prefs: []
  type: TYPE_NORMAL
- en: Gemma
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gemma is a family of state-of-the-art and open source, lightweight models from
    Google (DOI citation: 10.34740/KAGGLE/M/3301) that were built from the same technology
    and research that was used to create the Gemini models. These are decoder-only,
    text-to-text LLMs with pre-trained, instruction-tuned variants and open weights,
    which were trained on data from a wide variety of sources, including web pages,
    documents, code, and mathematical texts. They are suitable for many different
    kinds of text-generation use cases, such as summarization or question answering,
    and their open weights mean that they can be customized for specific use cases.
    Also, since they are relatively lightweight, they don’t require specialized or
    large-scale computing resources to run, so we can use them in many environments,
    including a local laptop, for example, which makes it easy for developers to start
    experimenting with them.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Gemma happens to be my sister’s name, so I was pleasantly surprised when that
    name was chosen for this suite of models, although I had no involvement whatsoever
    in the naming of these models.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the Gemma models in more detail later. In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    I described the importance of embeddings and vector databases in generative AI.
    Now that we’ve explored the various generative AI models in Google Cloud, let’s
    discuss vector databases in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases in Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a brief introduction to vector databases in Google Cloud,
    and we’ll dive into them in more detail later in this chapter. I’ll begin this
    section with a simple question: “*Which Google Cloud database service provides
    vector database functionality?*” Quite simply, the answer is, “*Pretty much all
    of them, with just a* *few exceptions!*”'
  prefs: []
  type: TYPE_NORMAL
- en: The next question, then, might be, “*Which one should I use?*” The answer to
    that question is a bit more nuanced. We’ll explore the options in more detail
    in this chapter, starting with a couple of easy decisions. Firstly, the reason
    almost all Google Cloud database services provide vector database functionality
    is that Google wants to make it as easy as possible for you to access this functionality.
    If you already use AlloyDB to manage your application’s operational data, you
    can easily go ahead and use AlloyDB AI for your vector database needs. If you
    use BigQuery for your analytical needs, go ahead and use BigQuery Vector Search,
    although bear in mind that BigQuery is designed primarily for large-scale data
    processing rather than for optimizing latency. If you have strict low-latency
    requirements, your workload may be better suited for one of the other Google Cloud
    options we’ll cover later.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re starting fresh in Google Cloud and want to set up a vector database,
    begin your journey by exploring the offerings under Vertex AI. For a fully managed
    platform that enables developers to build Google-quality search experiences for
    websites, structured and unstructured data, or to integrate your applications
    with generative AI and search functionality that’s grounded in your enterprise
    data, start with Vertex AI Search and Conversation. If you want to create and
    store your vectors and rapidly search billions of semantically related items,
    look into Vertex AI Vector Search.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive deeper into Google Cloud’s generative AI offerings,
    starting with a hands-on exploration of the models, and then covering the various
    vector database offerings in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed exploration of Google Cloud generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll start interacting with Google Cloud’s generative AI offerings.
    To set the stage, I’ll begin by briefly introducing Google Cloud Vertex AI Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI Studio can be accessed within the Google Cloud console UI, and it
    provides an interface to easily start using all of the generative AI models I
    described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the Google Cloud Vertex AI Studio UI, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the Google Cloud services menu and
    choose **Vertex AI**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the **Get started with Vertex AI** section, click **ENABLE ALL RECOMMENDED
    API**, as depicted in *Figure 17**.2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.2: Enabling the recommended APIs](img/B18143_17_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.2: Enabling the recommended APIs'
  prefs: []
  type: TYPE_NORMAL
- en: Give it a few minutes for the APIs to enable. Once the APIs have been enabled,
    you can start using them. You can interact with each of the models I described
    in the previous section by clicking on each of the modalities – such as **Language**,
    **Vision**, **Speech**, and **Multimodal** – in the menu on the left-hand side
    of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, if we click on **Language**, a screen similar to what’s shown
    in *Figure 17**.3* will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.3: The Language section](img/B18143_17_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.3: The Language section'
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we can click on the various links and buttons to try out the different
    models. For example, clicking the **TEXT CHAT** button will present us with a
    chat interface into which we can type our prompts, as shown in *Figure 17**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.4: Text chat](img/B18143_17_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.4: Text chat'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters we can configure can be found on the right-hand side of the
    screen. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model**: The model to which we want to send a prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Region**: The region in which we want to run our prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature**: This parameter configures the level of creativity or randomness
    in the model’s responses. This can be seen as the level of imagination we want
    the model to use when generating responses. If we consider the use case of an
    LLM predicting the next word in a sequence, the temperature parameter influences
    the overall probability distribution of the next word, such that a higher temperature
    can result in more creative outputs, while a lower temperature guides the model
    to provide more conservative and predictable outcomes. If we want the model to
    create imaginative art or text, for example, we could configure a high temperature,
    whereas if we want more formal, factual responses, we would configure a low temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output token limit**: The maximum number of tokens we want the model to generate
    in its response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can click on the **Advanced** section at the bottom of the screen to expand
    it. Here, we will see additional parameters, such as **Max responses**, which
    configures the maximum number of example responses we want the model to return
    (note that this parameter is only valid for some types of interactions – for example,
    it is not applicable to chat interactions because chat will always provide one
    response in each turn of the conversation), as well as **Top-K** and **Top-P**.
    Like **Temperature**, **Top-K** and **Top-P** can be used to control the creativity
    and randomness of the model’s generated outputs, but they work in slightly different
    ways. Consider the case of an LLM selecting the next word (or token) in a sequence
    when generating a response. It does this by predicting the probability of the
    next word being the most likely to occur in the given sequence. If the LLM always
    only selects the word with the highest probability, then it does not allow for
    much flexibility when generating responses. However, we can use **Top-K** and
    **Top-P** to add a level of flexibility by understanding how they work. So, let’s
    take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Top-K**: This uses a probability distribution to predict the probabilities
    for all possible next words based on the current prompt and context. K is the
    number of top-probability words that we want the model to select from. For example,
    if the value of K is 3, then the model will pick the next word from the top three
    most likely words (that is, the top three words with the highest probabilities).
    If the value of K is 1, then the model will pick only the most likely (that is,
    highest probability) word. This is a specific case we refer to as **greedy selection**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-P**: This is a bit more complex to explain, but only slightly. Rather
    than picking a specific number of the highest-probability words to choose from,
    the model will calculate the **cumulative** probability for all possible next
    words – that is, it will add up the probabilities of the most likely words until
    the sum of the probabilities reaches the specified threshold (P), at which point
    it will randomly select a word from the pool of words that fall under the probability
    threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of both **Top-K** and **Top-P**, lower values guide the outputs
    toward conservative responses, while higher values allow some wiggle room for
    more creative responses, similar to (and in conjunction with) the **Temperature**
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Advanced** section, for some models, we can also enable **Grounding**,
    in which case we can ground our responses based on data we have stored in Vertex
    AI Search and Conversation. We also have the option to stream responses, which
    refers to printing responses as they are generated.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to explore the different modalities and types of models in Vertex
    AI Studio. If you need help thinking of creative prompts to write, you can ask
    one of the text-based models to suggest some examples for you to use!
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accessing the various models through the Vertex AI Studio UI,
    you can also interact with the models programmatically via the REST APIs and SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the vector database options available in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed exploration of Google Cloud vector database options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, I mentioned that almost all of Google Cloud’s database
    services provide vector database support. In this section, we’ll take a look at
    each one in more detail, starting with Vertex AI Search and Conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Search and Conversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the main purpose of the Vertex AI Search and Conversation product suite,
    as its name suggests, is to build search and conversation applications, we can
    also use it as a vector database for implementing **retrieval augmented generation**
    (**RAG**) solutions. As we discussed earlier in this chapter, if you’re getting
    started with RAG on Google Cloud, unless you have a specific need to control the
    chunking, embedding, and indexing processes, or you have a specific operational
    need to link all of your data with another Google Cloud database service, Vertex
    AI Search and Conversation should be your first choice to consider. This is because
    it performs all of the chunking, embedding, and indexing processes for you, and
    it abstracts away all of those steps behind a simple and convenient orchestration
    interface, saving you a lot of time and effort. You can also use data connectors
    to ingest data from third-party applications, such as JIRA, Salesforce, and Confluence.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to specifically control the chunking, embedding, and indexing processes,
    you can choose from other Google Cloud database offerings, as described next.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Vector Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we’ve seen throughout this book, Vertex AI provides an entire ecosystem of
    tools and services for pretty much everything we could wish to do in the realm
    of ML and artificial intelligence. So, it makes sense that it would include a
    vector database, and that vector database is called Vertex AI Vector Search. It
    enables us to store and search through massive collections of embeddings to find
    the most similar or relevant items with high speed and low latency.
  prefs: []
  type: TYPE_NORMAL
- en: Although we need to create the chunks, embeddings, and indexes (unlike when
    using Vertex AI Search and Conversation), it still provides a fully managed service
    in that we don’t need to worry about infrastructure or scaling because Vertex
    AI handles all of that for us. The fact that we need to create the chunks, embeddings,
    and indexes means that we can implement more granular control over those processes
    if needed, such as the models used to create our embeddings (note that this is
    also true for all of the other vector database options that we will discuss in
    the remainder of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Vector Search also integrates closely with the rest of the Vertex
    AI and Google Cloud ecosystem to serve as part of larger orchestration solutions
    requiring the use of embeddings. On the topic of the broader ecosystem, let’s
    consider additional Google Cloud vector databases beyond Vertex AI, starting with
    BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery Vector Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BigQuery Vector Search is a feature within Google Cloud BigQuery that allows
    us to store and find embeddings. We can send a query embedding to BigQuery by
    using the BigQuery **VECTOR_SEARCH** function, which will quickly find similar
    embeddings within the vector database. This is a major benefit for users who are
    familiar with SQL syntax, and it enables semantic search and similarity-based
    analysis directly within your BigQuery data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: We can also choose to create a vector index to speed up the embedding retrieval
    process. When we use a vector index, an **approximate nearest neighbor** (**ANN**)
    search is used to return approximate results, which is much quicker than doing
    a brute-force search to find an exact match. If we do not create a vector index,
    then a brute-force search will be performed. We also have the option to explicitly
    implement a brute-force search even when a vector index exists if we want to get
    an exact result.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery Vector Search is a great choice for data science teams who already
    store a lot of data in BigQuery, and it also provides the benefits of BigQuery’s
    automatic and enormous scalability. Continuing the theme of scalability, another
    database service in Google Cloud that is renowned for its impressive scalability
    is Google Cloud Spanner. Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Spanner Vector Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud Spanner is a fully managed and highly performant database service
    that is capable of global scale. It’s often referred to as a “NewSQL” database
    because it combines the scalability and flexibility of non-relational (NoSQL)
    databases with the familiar SQL interface of relational databases. It can easily
    handle petabytes of data, and unlike many NoSQL databases, it can guarantee strong
    consistency across transactions, even when distributed globally.
  prefs: []
  type: TYPE_NORMAL
- en: With all of this in mind, Spanner is suitable for applications that require
    highly distributed and strongly consistent data, such as online banking use cases.
    In early 2024, Spanner also launched support for cosine distance and Euclidean
    distance comparisons, and we can use these vector distance functions to perform
    **K-nearest neighbors** (**KNN**) vector searches for use cases such as similarity
    search or RAG. This means that this highly distributed database service that is
    commonly used for enterprise-scale, business-critical workloads now also supports
    vector database functionality.
  prefs: []
  type: TYPE_NORMAL
- en: While Spanner combines the best of NoSQL and relational database functionality,
    I’ll describe relational database options next before covering NoSQL options explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud SQL and pgvector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cloud SQL is a fully managed relational database service within Google Cloud
    that provides support for PostgreSQL, MySQL, and SQL Server. According to the
    Google Cloud documentation, “*More than 95% of Google Cloud’s top 100 customers
    use Cloud SQL to run their businesses.*” It is often seen as the default relational
    database service to use in Google Cloud, and considering that it’s fully managed,
    we don’t need to worry about server provisioning, operating system updates, or
    database patches because Google handles all of those activities for us.
  prefs: []
  type: TYPE_NORMAL
- en: While Cloud SQL doesn’t natively support vector database capabilities, `<->`)
    to calculate distance metrics such as cosine similarity between vectors to help
    us find the most similar embeddings. What’s great is that it allows us to use
    SQL for all of those vector operations, which makes it easy to use for people
    who are familiar with PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using `pgvector` with Cloud SQL for PostgreSQL, we can also use
    it with AlloyDB, which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: AlloyDB AI Vector Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AlloyDB is a fully managed PostgreSQL-compatible database service on Google
    Cloud that’s designed for high-performance enterprise workloads. It significantly
    improves performance and scalability compared to standard PostgreSQL and provides
    advanced features such as auto-scaling and automatic failover, as well as built-in
    database backups and updates. It’s also suitable for hybrid transactional and
    analytical workloads, and recently, it has added multiple ML-related features.
  prefs: []
  type: TYPE_NORMAL
- en: AlloyDB AI now also includes vector similarity search as it uses `pgvector`
    natively within the database.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss NoSQL database options for vector similarity search in Google
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL database options for vector similarity search in Google Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Firestore is a NoSQL database within the Google Firebase ecosystem. Firebase
    itself is a Google Cloud framework that provides a collection of tools and services
    for developing mobile and web applications. In addition to the first-party functionality
    provided by Firebase, there’s also **Firebase Extensions Hub**, which provides
    a way for users to add more functionality to their solutions. While Firestore
    does not provide vector database support out-of-the-box, that functionality can
    be added via an extension named **Semantic Search with Vertex AI**, which adds
    text similarity search to a Firestore application by integrating with Vertex AI
    Vector Search.
  prefs: []
  type: TYPE_NORMAL
- en: Another highly popular NoSQL database in Google Cloud is Bigtable. Similar to
    Firestore, Bigtable does not currently provide native support for vector similarity
    search use cases, but it can integrate with Vertex AI Vector Search to help build
    a solution for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered both the relational and NoSQL database options that provide
    vector database support in Google Cloud, it’s important to describe one additional
    type of data store, called **Memorystore**, which is used to implement caching
    solutions for extremely low-latency workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Memorystore for Redis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Redis is an open source in-memory data store, meaning that, unlike traditional
    databases that store data on disk, Redis primarily stores data in RAM, giving
    it ultra-fast performance. Memorystore for Redis is a fully managed Redis service
    in Google Cloud that provides scalable, highly available, and secure Redis instances
    without requiring us to manage the instance or the related infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Memorystore for Redis now also supports ANN and KNN vector search, enabling
    us to implement an in-memory vector store cache for applications that require
    extremely low-latency responses.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve covered most of the main generative AI offerings on Google
    Cloud that are available. Now, it’s time to put some of our knowledge into action
    as we begin to implement generative AI solutions on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing generative AI solutions on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create a Vertex AI Search and Conversation application
    that will enable us to ask questions about the contents of a set of documents.
    We will use some publicly available medical study reports as examples, but this
    pattern can be applied to many different kinds of use cases. The following are
    some other popular applications of this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling users of a retail website to get information about products on the
    site via a question-and-answer interface, in which they can ask human language
    questions and get factual responses about products in the catalog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking questions about financial documents. For example, employees in a company’s
    finance department could upload the company’s financial documents, such as quarterly
    earnings reports, and ask natural language questions about the contents of the
    reports to identify trends or other important information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing natural language search over a company’s vast corpus of internal documentation.
    For example, given a simple search interface, an employee could ask, “How can
    I modify my retirement contributions?” and they can get a response that tells
    them how to do so, along with a link to the internal documentation that describes
    the process in detail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use Vertex AI Search and Conversation to build this application, as
    described in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Vertex AI Search and Conversation application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, we will perform three main activities to build our Vertex
    AI Search and Conversation application:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable the Vertex AI Search and Conversation API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a data store to store the data we will use in our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the Vertex AI Search and Conversation application so that it can interact
    with our data store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s begin by enabling the Vertex AI Search and Conversation API.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Vertex AI Search and Conversation API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To enable the Vertex AI Search and Conversation API, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, type `search` into the search box and select **Search**
    **and Conversation**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If this is your first time using this service, a page similar to what’s shown
    in *Figure 17**.5* will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.5: Activating Vertex AI Search and Conversation](img/B18143_17_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.5: Activating Vertex AI Search and Conversation'
  prefs: []
  type: TYPE_NORMAL
- en: Click **CONTINUE AND ACTIVATE THE API** (this checkbox is optional – you can
    either select it or leave it blank).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a few seconds, your environment will be created. A page similar to what’s
    shown in *Figure 17**.6* will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.6: Vertex AI Search and Conversation environment page](img/B18143_17_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.6: Vertex AI Search and Conversation environment page'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve enabled the API, we can start the next prerequisite step to set
    up our application, which is to create a data store.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data store for Vertex AI Search and Conversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the time of writing this, in March 2024, Vertex AI Search and Conversation
    can support the following data store sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Website URLs**: Automatically crawl website content from a list of domains
    you define'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery**: Import data from your BigQuery table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Storage**: Import data from your storage bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API**: Import data manually by calling the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re going to use Google Cloud Storage as our data store, so we’ll begin by
    setting up a location and uploading our data there. The next subsection describes
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 4* of this book, we created a local directory in our Cloud Shell
    environment and cloned our GitHub repository into that directory. If you did not
    perform those steps, please reference those instructions now. They can be found
    in the *Creating a directory and cloning our GitHub* *repository* section.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have ensured that the GitHub repository for this book has been cloned
    into the local directory in your Cloud Shell environment, continue with the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Storing our data in Google Cloud Storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The easiest way to stage our data is to use Google Cloud Shell. To set up our
    data store, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Cloud Shell** symbol in the top-right corner of the screen, as
    shown in *Figure 17**.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.7: Activating Cloud Shell](img/B18143_17_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.7: Activating Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: This will activate the Cloud Shell environment, which will appear at the bottom
    of your screen. It will take a few seconds for your environment to activate. Once
    your environment has been activated, you can paste and run the commands mentioned
    in the following steps into Cloud Shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands to set up environment variables so that you can
    store your preferred region, your Cloud Storage bucket name, and the data path
    (**replace YOUR-REGION with your preferred region, such as us-central1, and YOUR-BUCKET-NAME
    with your** **bucket name**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the bucket path (you should also copy the contents of the response from
    this command and keep it for reference to be used in a later step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the bucket if it doesn’t already exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change directories to the location at which the data for this chapter is stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the files to the bucket (this also creates the path within the bucket):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the files have been uploaded (the following command should list
    the files):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve uploaded the files to our Cloud Storage bucket, we can create
    the Vertex AI Search and Conversation data store for our application, as described
    in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The citations for the documents used in this exercise can be found in the **document_citations.txt**
    file in the **Chapter-17** directory of this book’s GitHub repository: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Vertex AI Search and Conversation data store
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Within Vertex AI Search and Conversation, we will define a data store associated
    with the files we uploaded to Cloud Storage. To do that, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Vertex AI Search and Conversation console UI, select **Data Stores**
    |**Create data store** | **Cloud Storage**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A screen similar to the one shown in *Figure 17**.8* will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.8: Import data from Cloud Storage](img/B18143_17_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.8: Import data from Cloud Storage'
  prefs: []
  type: TYPE_NORMAL
- en: Enter the path to the files you uploaded (this is the value you specified for
    the `BUCKET_PATH` environment variable in your Cloud Shell environment in the
    previous section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use unstructured documents in this example, so the **Unstructured documents**
    option should remain selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Continue**. A screen similar to what’s shown in *Figure 17**.9* will
    be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.9: Configure your data store](img/B18143_17_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.9: Configure your data store'
  prefs: []
  type: TYPE_NORMAL
- en: Select **global (Global)** under **Multi-region**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Digital Parser** under **Default** **document parser**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a name for the data store, such as `AIML-SA-DS`, and click **CREATE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the data store has been created, it’s time to create the Vertex AI
    Search and Conversation application.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Vertex AI Search and Conversation application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create the Vertex AI Search and Conversation application, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Vertex AI Search and Conversation console UI, select **Apps** | **Create
    a new app** | **Search**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen that appears, ensure that the **Enterprise edition features**
    and **Advanced LLM features** checkboxes are enabled, as shown in *Figure 17**.10*
    (read the descriptions for each option to understand their features):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.10: Enabling Search and Conversation features](img/B18143_17_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.10: Enabling Search and Conversation features'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, enter an application name and a company name, select the location for
    the application, and select **CONTINUE**, as shown in *Figure 17**.11* (note the
    recommendation to choose a global location if you do not have compliance or regulatory
    reasons to locate your data in a particular multi-region):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.11: Application details](img/B18143_17_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.11: Application details'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, select the data store we created that you wish to integrate with our
    application, as shown in *Figure 17**.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.12: Selecting a data store](img/B18143_17_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.12: Selecting a data store'
  prefs: []
  type: TYPE_NORMAL
- en: It will take some time for the data for our application to be processed. Periodically
    refresh the page to check its status. Once it’s complete, a list of files will
    be displayed. At that point, we are ready to configure how we want our application
    to behave, as described in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring our newly created application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are numerous ways in which our new Search and Conversation application
    can work, and we can specify how we want it to work by modifying its configuration.
    To do that, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Vertex AI Search and Conversation console UI, select **Apps** from the
    menu on the left-hand side of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on our newly created app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Configurations** from the menu on the left-hand side of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the **Search type** configuration, there are three options:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Search**: Simply respond to the search query with a list of relevant results.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Search with an answer**: Provide a generative summary above the list of search
    results.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Search with follow-ups**: Provide conversational search with generative summaries
    and support for follow-up questions.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Search with** **follow-ups** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the **Large Language Models for summarization** section and select
    the latest version of Gemini.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can leave all other options at their default values unless you have any
    specific requirements to change them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **SAVE** **AND PUBLISH**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we are ready to start using our application, as described in
    the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Using our newly created application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve created our application, we can start using it. To do that,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Vertex AI Search and Conversation console UI, select the **Preview**
    section for our newly created application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will be presented with a search box in which we can ask questions about the
    contents of the documents in our data store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Begin with the following question: `What are the effects of cinnamon` `on health?`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `What specific properties contribute to its ability to regulate` `glucose
    levels?`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the follow-up question simply mentions “it” and does not mention cinnamon
    directly. However, the model uses the context from the previous question to understand
    our intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Also, note that the generated answers may contain references, and the referenced
    documents that were used to generate the answer are listed in the response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Try out the following additional questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`How does the daily dosage of cinnamon used in the study compare to typical
    dietary` `cinnamon intake?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Were there any observed long-term effects of cinnamon supplementation beyond
    the 4-week` `trial period?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`How might cinnamon supplementation interact with other dietary or lifestyle
    interventions for` `prediabetes management?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Could the study''s findings on cinnamon''s glucose-regulating effects extend
    to individuals with types 1 or` `2 diabetes?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, let’s ask some questions that will cause the model to respond based on
    other documents in the collection. Try out the following questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`What are the main challenges FIM programs face when attempting to use EHR
    data` `for evaluation?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`How can FIM programs overcome the barriers to accessing and utilizing EHR`
    `data effectively?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`What alternative data sources can FIM programs use to evaluate health outcomes
    and healthcare utilization, apart from` `EHR data?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`How do the privacy and liability concerns of healthcare partners impact the
    sharing of EHR data with` `FIM programs?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`What role does albumin play in the nutritional status of children, and how
    are red bean cookies effective in` `improving it?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`What specific dietary interventions have shown promise in modifying the gut
    microbiome to improve outcomes for patients` `with diseases?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`How does the gut microbiome''s interaction with the body impact mental health
    disorders, and what mechanisms` `are involved?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Can changes in the gut microbiome serve as early indicators for the development
    of chronic` `kidney disease?`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: These are just some examples – feel free to play around with additional documents
    and questions, and think about how this pattern can be extended to pretty much
    any use case due to the vast knowledge of these wondrous models.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! You have successfully built your first generative AI application
    in Google Cloud! As you can see, Vertex AI Search and Conversation makes it very
    easy for us to do this. Behind the scenes, this can be seen as a RAG solution
    because we are interacting with the Gemini model and getting it to generate responses
    that are grounded in the contents of the documents we uploaded to our data store,
    although Vertex AI Search and Conversation abstracts away and manages all of the
    complexities and steps required to implement the solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build some additional and more complex use cases.
    First, however, let’s summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dived into generative AI in Google Cloud, exploring Google’s
    native generative AI models, such as the Gemini, PaLM, Codey, Imagen, and MedLM
    APIs. We discussed the multiple versions of each model and some example use cases
    for each. Then, we introduced Vertex AI Studio and discussed open source and third-party
    models available on Google Cloud via repositories such as Vertex AI Model Garden
    and Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed vector databases in Google Cloud, covering various options
    available, such as Vertex AI Search and Conversation, Vertex AI Vector Search,
    BigQuery Vector Search, Spanner Vector Search, `pgvector`, and AlloyDB AI Vector
    Search, including some decision factors for choosing one solution over another.
    It is these kinds of decision points that are often most important in the role
    of a solutions architect, and the decisions will vary based on the specific needs
    of the customer or project, including the cost of each solution. I recommend always
    consulting the latest pricing information for each product and factoring that
    into your decision process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put some of this chapter’s topics into action and built a generative
    AI application in Google Cloud – specifically, we built a Vertex AI Search and
    Conversation application that enabled us to ask natural language questions about
    the contents of a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue with this theme of using the topics we’ve
    covered throughout this book to build solutions in Google Cloud. Join me there
    to start diving in further!
  prefs: []
  type: TYPE_NORMAL
