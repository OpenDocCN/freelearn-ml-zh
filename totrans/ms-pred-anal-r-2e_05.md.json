["```py\nstep_function <- function(x) {\n   if (x < 0) -1 else 1\n}\n\npocket_perceptron <- function(x, y, learning_rate, max_iterations) {\n  nObs = nrow(x)\n  nFeatures = ncol(x)\n  w = rnorm(nFeatures + 1, 0, 2) # Random weight initialization\n  current_iteration = 0\n  has_converged = F\n  best_weights = w\n  # Start by assuming you get all the examples wrong\n  best_error = nObs \n  while ((has_converged == F) &\n         (current_iteration < max_iterations)) {\n    # Assume we are done unless we misclassify an observation\n    has_converged = T \n    # Keep track of misclassified observations\n    current_error = 0\n    for (i in 1:nObs) {\n      xi = c(1, x[i,]) # Append 1 for the dummy input feature x0\n      yi = y[i]\n      y_predicted = step_function(sum(w * xi))\n      if (yi != y_predicted) {\n        current_error = current_error + 1\n        # We have at least one misclassified example\n        has_converged = F \n        w = w - learning_rate * sign(y_predicted - yi) * xi\n      }\n    }\n    if (current_error < best_error) {\n      best_error = current_error\n      best_weights = w\n    }\n    current_iteration = current_iteration+1\n  }\n  model <- list(\"weights\" = best_weights, \n\"converged\" = has_converged, \n\"iterations\" = current_iteration)\n  model\n}\n```", "```py\n> set.seed(4910341)\n> x1 <- runif(200, 0, 10)\n> set.seed(2125151)\n> x2 <- runif(200, 0, 10)\n> x <- cbind(x1, x2)\n> y <- sign(-0.89 + 2.07 * x[,1] - 3.09 * x[,2])\n> pmodel <- pocket_perceptron(x, y, 0.1, 1000)\n> pmodel\n$weights\n                 x1        x2 \n-1.738271  4.253327 -6.360326 \n\n$converged\n[1] TRUE\n\n$iterations\n[1] 32\n```", "```py\n> pmodel$weights / 2\n                   x1         x2 \n-0.8741571  2.1420697 -3.2122627\n```", "```py\n> library(xlsx)\n> eneff <- read.xlsx2(\"ENB2012_data.xlsx\", sheetIndex = 1, \n                      colClasses = rep(\"numeric\", 10))\n> names(eneff) <- c(\"relCompactness\", \"surfArea\", \"wallArea\", \"roofArea\", \"height\", \"orientation\", \"glazArea\", \"glazAreaDist\", \"heatLoad\", \"coolLoad\")\n> eneff <- eneff[complete.cases(eneff),]\n```", "```py\n> library(caret)\n> eneff$orientation <- factor(eneff$orientation)\n> eneff$glazAreaDist <- factor(eneff$glazAreaDist)\n> dummies <- dummyVars(heatLoad + coolLoad ~ ., data = eneff)\n> eneff_data <- cbind(as.data.frame(predict(dummies, newdata =  \n                                            eneff)), eneff[,9:10])\n> dim(eneff_data)\n[1] 768  18\n```", "```py\n> set.seed(474576)\n> eneff_sampling_vector <- createDataPartition(eneff_data$heatLoad, p \n  = 0.80, list = FALSE)\n> eneff_train <- eneff_data[eneff_sampling_vector, 1:16]\n> eneff_train_outputs <- eneff_data[eneff_sampling_vector, 17:18]\n> eneff_test <- eneff_data[-eneff_sampling_vector, 1:16]\n> eneff_test_outputs <- eneff_data[-eneff_sampling_vector, 17:18]\n```", "```py\n> eneff_pp <- preProcess(eneff_train, method = c(\"range\"))\n> eneff_train_pp <- predict(eneff_pp, eneff_train)\n> eneff_test_pp <- predict(eneff_pp, eneff_test)\n\n> eneff_train_out_pp <- preProcess(eneff_train_outputs, method = \n                        c(\"range\"))\n> eneff_train_outputs_pp <- \n  predict(eneff_train_out_pp, eneff_train_outputs)\n> eneff_test_outputs_pp <- \n  predict(eneff_train_out_pp, eneff_test_outputs)\n```", "```py\n> library(\"neuralnet\")\n> n <- names(eneff_data)\n> f <- as.formula(paste(\"heatLoad + coolLoad ~\", paste(n[!n %in% \n                  c(\"heatLoad\", \"coolLoad\")], collapse = \" + \")))\n> eneff_model <- neuralnet(f, \n  data = cbind(eneff_train_pp, eneff_train_outputs_pp), hidden = 10)\n> eneff_model\nCall: neuralnet(formula = f, data = cbind(eneff_train_pp, eneff_train_outputs_pp),     hidden = 10)\n\n1 repetition was calculated.\n\n         Error Reached Threshold Steps\n1 0.3339635783    0.009307995429  9998\n```", "```py\n> eneff_model <- neuralnet(f, \n  data = cbind(eneff_train_pp, eneff_train_outputs_pp), hidden = 10, act.fct = \"logistic\", linear.output = TRUE, err.fct = \"sse\", rep = 1)\n```", "```py\n> test_predictions <- compute(eneff_model, eneff_test_pp)\n```", "```py\n> head(test_predictions$net.result)\n            [,1]          [,2]\n7  0.38996108769 0.39770348145\n8  0.38508402576 0.46726904682\n14 0.29555228848 0.24157156896\n21 0.49912349400 0.51244876337\n23 0.50036257800 0.47436990729\n29 0.01133684342 0.01815294595\n```", "```py\n> eneff_train_out_pp$ranges\n     heatLoad coolLoad\n[1,]     6.01    10.90\n[2,]    42.96    48.03\n```", "```py\nreverse_range_scale <- function(v, ranges) {\n     return( (ranges[2] - ranges[1]) * v + ranges[1] )\n }\n```", "```py\n> output_ranges <- eneff_train_out_pp$ranges\n> test_predictions_unscaled <- sapply(1:2, function(x) \n  reverse_range_scale(test_predictions[,x], output_ranges[,x]))\n```", "```py\nmse <- function(y_p, y) {\n  return(mean((y - y_p) ^ 2))\n}\n\n> mse(test_predictions_unscaled[,1], eneff_test_outputs[,1])\n[1] 0.2940468477\n> mse(test_predictions_unscaled[,2], eneff_test_outputs[,2])\n[1] 1.440127075\n```", "```py\n> cor(test_predictions_unscaled[,1], eneff_test_outputs[,1])\n[1] 0.9986655316\n> cor(test_predictions_unscaled[,2], eneff_test_outputs[,2])\n[1] 0.9926735348\n```", "```py\n> glass <- read.csv(\"glass.data\", header = FALSE)\n> names(glass) <- c(\"id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \n\"Ba\", \"Fe\", \"Type\")\n> glass$id <- NULL\n```", "```py\n> glass$Type <- factor(glass$Type)\n> set.seed(4365677)\n> glass_sampling_vector <- createDataPartition(glass$Type, p = \n                           0.80, list = FALSE)\n> glass_train <- glass[glass_sampling_vector,]\n> glass_test <- glass[-glass_sampling_vector,]\n```", "```py\n> glass_pp <- preProcess(glass_train[1:9], method = c(\"range\"))\n> glass_train <- cbind(predict(glass_pp, glass_train[1:9]), Type = glass_train$Type)\n> glass_test  <- cbind(predict(glass_pp, glass_test[1:9]), Type = glass_test$Type)\n```", "```py\n> glass_model <- nnet(Type ~ ., data = glass_train, size = 10)\n# weights:  166\ninitial  value 343.685179 \niter  10 value 265.604188\niter  20 value 220.518320\niter  30 value 194.637078\niter  40 value 192.980203\niter  50 value 192.569751\niter  60 value 192.445198\niter  70 value 192.421655\niter  80 value 192.415382\niter  90 value 192.415166\niter 100 value 192.414794\nfinal  value 192.414794 \nstopped after 100 iterations\n```", "```py\n> glass_model <- nnet(Type ~ ., data = glass_train, size = 10, maxit = \n                      1000)\n```", "```py\n> train_predictions <- predict(glass_model, glass_train[,1:9], \n                               type = \"class\")\n> mean(train_predictions == glass_train$Type)\n[1] 0.7183908046\n```", "```py\n> glass_model2 <- nnet(Type ~ ., data = glass_train, size = 50, maxit = \n                       10000)\n> train_predictions2 <- predict(glass_model2, glass_train[,1:9], \n                                type = \"class\")\n> mean(train_predictions2 == glass_train$Type)\n[1] 1\n```", "```py\n> test_predictions2 <- predict(glass_model2, glass_test[,1:9], \n                               type = \"class\")\n> mean(test_predictions2 == glass_test$Type)\n[1] 0.6\n```", "```py\n> glass_model3 <- nnet(Type~., data = glass_train, size = 10, maxit = \n                       10000, decay = 0.01)\n> train_predictions3 <- predict(glass_model3, glass_train[,1:9], \n                                type = \"class\")\n> mean(train_predictions3 == glass_train$Type)\n[1] 0.9367816092\n> test_predictions3 <- predict(glass_model3, glass_test[,1:9],  \n                               type = \"class\")\n> mean(test_predictions3 == glass_test$Type)\n[1] 0.775\n```", "```py\n> library(caret)\n> nnet_grid <- expand.grid(.decay = c(0.1, 0.01, 0.001, 0.0001), \n                           .size = c(50, 100, 150, 200, 250))\n> nnetfit <- train(Type ~ ., data = glass_train, method = \"nnet\", \n  maxit = 10000, tuneGrid = nnet_grid, trace = F, MaxNWts = 10000)\n```", "```py\nread_idx_image_data <- function(image_file_path) {\n  con <- file(image_file_path, \"rb\")\n  magic_number <- readBin(con, what = \"integer\", n = 1, size = 4, \n                          endian = \"big\")\n  n_images <- readBin(con, what = \"integer\", n = 1, size = 4, \n                      endian=\"big\")\n  n_rows <- readBin(con, what = \"integer\", n = 1, size = 4, \n                    endian = \"big\")\n  n_cols <- readBin(con, what = \"integer\", n = 1, size = 4, \n                    endian = \"big\")\n  n_pixels <- n_images * n_rows * n_cols\n  pixels <- readBin(con, what = \"integer\", n = n_pixels, size = 1, \n                    signed = F)\n  image_data <- matrix(pixels, nrow = n_images, ncol = n_rows * \n                       n_cols, byrow = T)\n  close(con)\n  return(image_data)\n}\n\nread_idx_label_data <- function(label_file_path) {\n  con <- file(label_file_path, \"rb\")\n  magic_number <- readBin(con, what = \"integer\", n = 1, size = 4, \n                          endian = \"big\")\n  n_labels <- readBin(con, what = \"integer\", n = 1, size = 4, \n                      endian = \"big\")\n  label_data <- readBin(con, what = \"integer\", n = n_labels, size = 1, \n                        signed = F)\n  close(con)\n  return(label_data)\n}\n```", "```py\n> mnist_train <- read_idx_image_data(\"train-images-idx3-ubyte\")\n> mnist_train_labels <- read_idx_label_data(\"train-labels-idx1-\n                                            ubyte\")\n> str(mnist_train)\n int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n> str(mnist_train_labels)\n int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...\n```", "```py\n> mnist_input <- mnist_train / 255\n> mnist_output <- as.factor(mnist_train_labels)\n```", "```py\n> set.seed(252)\n> mnist_index <- sample(1:nrow(mnist_input), nrow(mnist_input))\n> mnist_data <- mnist_input[mnist_index, 1:ncol(mnist_input)]\n> mnist_out_shuffled <- mnist_output[mnist_index]\n```", "```py\n> library(\"RSNNS\")\n> mnist_out <- decodeClassLabels(mnist_out_shuffled)\n> mnist_split <- splitForTrainingAndTest(mnist_data, mnist_out, \n                                         ratio = 0.2)\n> mnist_norm <- normTrainingAndTestSet(mnist_split, type = \"0_1\")\n```", "```py\n> start_time <- proc.time()\n> mnist_mlp <- mlp(mnist_norm$inputsTrain, mnist_norm$targetsTrain, size = 100, inputsTest = mnist_norm$inputsTest, targetsTest = mnist_norm$targetsTest)\n> proc.time() - start_time\n    user   system  elapsed \n 2923.936    5.470 2927.415\n\n> start_time <- proc.time()\n> mnist_mlp2 <- mlp(mnist_norm$inputsTrain, mnist_norm$targetsTrain, size = 300, inputsTest = mnist_norm$inputsTest, targetsTest = mnist_norm$targetsTest)\n> proc.time() - start_time\n     user   system  elapsed \n 7141.687    7.488 7144.433\n```", "```py\n> mnist_class_test <- (0:9)[apply(mnist_norm$targetsTest, 1, which.max)]\n> mlp_class_test <- (0:9)[apply(mnist_mlp$fittedTestValues, 1, which.max)]\n> mlp2_class_test <- (0:9)[apply(mnist_mlp2$fittedTestValues, 1, which.max)]\n```", "```py\n> mean(mnist_class_test == mlp_class_test)\n[1] 0.974\n> mean(mnist_class_test == mlp2_class_test)\n[1] 0.981\n```", "```py\n> confusionMatrix(mnist_class_test, mlp2_class_test)\n       predictions\ntargets    0    1    2    3    4    5    6    7    8    9\n      0 1226    0    0    1    1    0    1    1    3    1\n      1    0 1330    5    3    0    0    0    3    0    1\n      2    3    0 1135    3    2    1    1    5    3    0\n      3    0    0    6 1173    0   11    1    5    6    1\n      4    0    5    0    0 1143    1    5    5    0   10\n      5    2    2    1   12    2 1077    7    3    5    4\n      6    3    0    2    1    1    3 1187    0    1    0\n      7    0    0    7    1    3    1    0 1227    1    4\n      8    5    4    3    5    1    4    4    0 1110    5\n      9    1    0    0    6    8    5    0   11    6 1164\n```"]