- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Generative AI Concepts and Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we’ve covered the basics of **generative AI** (**GenAI**), it’s time
    to start diving deeper. In this chapter, we will cover more advanced topics in
    the field of GenAI. We’ll begin by learning about some techniques to tune and
    optimize generative models for specific domains or tasks. Then, we’ll dive into
    more detail on the important topics of embeddings and vector databases and how
    they relate to a relatively new pattern of using **retrieval-augmented generation**
    (**RAG**) to ground our **large language model** (**LLM**) responses in our own
    data. Next, we’ll discuss multimodal models, how they differ from standard, text-based
    LLMs, and the kinds of use cases they support. Finally, we will introduce LangChain,
    which is a popular framework that enables us to design complex applications that
    can build on the functionalities provided by LLMs. Specifically, this chapter
    covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced tuning and optimization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings and vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive right in and start learning about advanced tuning and optimization
    techniques!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The term *LLM* has become almost synonymous with the field of GenAI in general,
    but considering that **LLM** stands for **large language model**, the term technically
    relates them to language processing. It’s important to note that there are other
    types of GenAI models, such as some image generation and multimodal models, that
    are not strictly language models. Additionally, a more recent concept of **large
    action models** or **large agentic models** (**LAMs**) has emerged, which combine
    the language understanding and generation capabilities of LLMs with the ability
    to perform actions (such as planning and booking a vacation) by using tools and
    agents to interact with their environment. However, for simplicity, throughout
    the rest of this book, I will use the terms *LLM* and *GenAI* *model* interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced tuning and optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the end of the previous chapter, we discussed LLMs and how they are trained
    and tuned. I mentioned some of the tuning approaches at a high level, and in this
    section, we will dive deeper into how we can tune LLMs to more adequately address
    our specific needs. Let’s set the stage by outlining how we interact with LLMs
    in the first place, which we generally do via **prompts**.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: A prompt is a piece of text or instruction that we provide to an LLM to guide
    its response or output. It tells the LLM what to do and, in some cases, provides
    guidance on how to do it; for example, “**summarize this financial document, specifically
    focusing on details relating to company performance in** **Q4, 2023**.”
  prefs: []
  type: TYPE_NORMAL
- en: The first LLM tuning technique we’ll explore is **prompt engineering**.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompts are the most straightforward method we can use to tune an LLM’s outputs
    to our specific needs. In fact, during the early days of the GenAI popularity
    explosion that happened in late 2022 and early 2023, you may recall seeing news
    headlines about a new type of extremely high-demand role that was emerging, called
    **prompt engineer**. In this section, we’ll discuss what prompt engineering is
    and how it can be used to improve the outputs of GenAI models, which will help
    provide context on why there was such a sudden increase in demand for talented
    people in this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin our discussion, consider the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a very simple prompt, and it does not provide much information to the
    LLM about what kind of poem we would like it to write for us. If we want a specific
    type of output, we could include additional instructions in our prompt, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The latter prompt will produce a much more specific poem that abides by the
    parameters we’ve outlined. Feel free to go ahead and try this out for yourself.
    Visit the following URL, enter each of those prompts, and see how different the
    responses appear: [https://gemini.google.com](https://gemini.google.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can use the aforementioned URL to test all of the prompts in this book.
  prefs: []
  type: TYPE_NORMAL
- en: This is a rudimentary form of prompt engineering that simply includes additional
    instructions in the prompt to help the LLM better understand how it should respond
    to our request. I’ll explain additional prompt engineering techniques in this
    section, beginning by outlining some standard best practices for crafting effective
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Core prompt design principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some principles we should keep in mind when creating prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be clear**: We should be clear about what we want the LLM to do and avoid
    using ambiguous language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Be specific**: Using specific instructions can help us get better-quality
    results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide adequate context**: Without appropriate context, the LLM’s responses
    might not be as relevant to what we are trying to achieve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I included some of those principles in the latter example prompt in the previous
    section, and I’ll outline them more explicitly here. In most cases, it’s important
    for prompts to be as clear and specific as possible due to the sheer and ever-growing
    versatility of LLMs. For example, if I ask an LLM to write a story about happiness,
    there’s almost no way of knowing what kinds of details it will come up with. While
    this is one of the wonderful properties of LLMs, it can be challenging when we
    want an LLM to provide results within specific parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before I provide additional examples regarding clarity and specificity, consider
    use cases in which we might not want to be clear and specific, such as when we’re
    not yet sure of our exact requirements, and we want the LLM to help us explore
    various potential options. In that case, we could start by sending a broad prompt
    to the LLM, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In response to a broad prompt such as this one, the LLM will likely come up
    with all kinds of random ideas, from baking cakes to developing video games that
    help kids learn mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: 'After getting some initial outputs from the LLM, we could then iteratively
    refine the process by diving in on certain ideas more specifically. For example,
    if we really like the idea of developing video games for kids, we could follow
    up with a more specific prompt, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, this prompt is much more specific than the previous prompt and
    will likely provide more targeted and actionable responses. This updated prompt
    abides by all of the principles we’ve outlined because it’s specific in terms
    of exactly what we want to achieve (starting a business to create a video game),
    it provides context such as the intended content of the game and the target audience,
    and it’s clear in terms of how we want the LLM to respond (that is, list and explain
    the steps).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we need to provide large amounts of context, and we will discuss
    how to do that later in this chapter. Next, however, let’s explore how we can
    use chaining to refine the outputs we get from an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process I described in the previous section, in which we use the output
    of the first prompt as an input to a subsequent prompt, is called **prompt chaining**.
    This can be done via an interactive process such as a chat interface, or in an
    automated fashion using tools I will describe later, such as LangChain. There’s
    also a framework called ReAct, in which we can chain multiple actions together
    to achieve a broader goal, which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct and agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I briefly touched upon the concept of LAMs at the beginning of this chapter,
    and I’ll cover it in more detail here.
  prefs: []
  type: TYPE_NORMAL
- en: '**ReAct** stands for **Reasoning and Acting**, and it’s a framework for combining
    the reasoning abilities of LLMs with the ability to take actions and interact
    with external tools or environments, which helps us build solutions that go beyond
    simply generating content to achieve more complex goals. Examples of external
    tools include software, APIs, code interpreters, search engines, or custom extensions.'
  prefs: []
  type: TYPE_NORMAL
- en: By using ReAct to go beyond generating content, we can build what are referred
    to as **agents**. The role of an agent is to interpret the user’s desired outcome
    and decide which tools to utilize to achieve the intended goal. Here, I will again
    highlight the example of planning and booking a vacation, which might include
    booking flights, accommodation, restaurant reservations, and excursions, among
    other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining is not to be confused with another prompt engineering approach, called
    **chain-of-thought** (**CoT**) prompting, which we can use to help LLMs work through
    complex reasoning tasks, which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: CoT prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CoT prompting involves creating prompts in a way that guides the model through
    a step-by-step reasoning process before arriving at a final answer, much like
    a human might do when solving a problem. Interestingly, simply adding the words
    *step by step* to a prompt can cause the model to respond differently and work
    through a problem in a logical manner, while it might not have done so in the
    absence of those words. We can also help the model if we describe tasks in a step-by-step
    manner. For example, consider the following prompt as a baseline, which does not
    implement CoT prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that LLMs are large language models and not large mathematics models,
    but we may be able to teach an LLM how to perform complex mathematical calculations
    by teaching it the step-by-step process. If the LLM were struggling to provide
    an accurate answer, we could explain the steps as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'While the model may not have been able to immediately provide the requested
    answer, we can help it to get there by teaching it about what should happen in
    each step, and then it can chain those steps together to get to the end result.
    To learn more about this fascinating topic, I recommend reading the research paper
    titled *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*
    (Wei et al., 2022), available at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll cover another popular prompt engineering technique, which I briefly
    mentioned in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), called **few-shot
    prompting**.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of the prompts I’ve included so far in this chapter are examples of **zero-shot
    prompting** because I simply asked the LLM to do something without providing any
    referenceable examples of how to do that thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Few-shot prompting is a rather simple concept, as it just means that we are
    providing some examples in our prompt that teach the LLM how to perform the task
    we are requesting. An example of few-shot prompting would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the prompt, I’ve instructed the LLM on how to categorize the
    sentiment of a review, and I provided detailed examples of the format in which
    I want the LLM to respond, including specific product properties that the sentiment
    appears to reference.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of few-shot prompting, we’re providing the LLM with a mini “training
    set” within the prompt itself, and it can use these examples to understand the
    expected mapping between the inputs and the desired outputs. This helps the LLM
    to understand what it should focus on and how it should structure its responses,
    hopefully providing results that more closely match the specific needs of our
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Although the examples can be seen as a small training set included in our prompt,
    it’s important to note that the underlying LLM’s weights are not changed by the
    examples we provide. This is true for most prompt engineering techniques, whereby
    the responses may change significantly based on our inputs, but the underlying
    LLM’s weights remain unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting and prompt engineering, in general, can be surprisingly effective,
    but in some cases, we need to provide many examples to the LLM, and we want to
    perform incremental training (that is, updating weights), especially for complex
    tasks. I will outline how we can approach those scenarios shortly, but first,
    I’ll introduce the important emerging field of prompt management.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt management practices and tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While new technologies bring new functionality and opportunities, they often
    also introduce new challenges. As prompt engineering continues to develop, a common
    challenge companies encounter is the need to store and manage their prompts effectively.
    For example, if a company’s employees are coming up with awesome prompts that
    provide wonderful results, they will want to track and reuse those prompts. They
    may also want to update the prompts over time as the LLMs and other components
    of their solutions evolve, in which case they will want to track different versions
    of prompts and their results.
  prefs: []
  type: TYPE_NORMAL
- en: In software development, we use versioning systems to track and manage updates
    to our application code. We can apply the same principles to prompt engineering,
    using versioning systems and templates to help us efficiently develop, reuse,
    and share prompts. Companies performing advanced prompt engineering practices
    will likely end up curating prompt libraries and repositories for these purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt templates** can be used to standardize the structures of effective
    prompts so that we don’t need to keep using trial and error and reinventing the
    wheel to create prompts that are best suited for specific use cases. For example,
    imagine that we work in the marketing department and we run a monthly report to
    measure the success of our marketing campaigns. We may want to use an LLM to summarize
    the reports, and there are likely specific pieces of information that certain
    team members want to review each time. We could create the following prompt template
    to formulate those requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, the marketing team members simply need to use this prompt template and
    fill in their desired values for each of the placeholders in the template. As
    is the case with individual prompts, we could maintain different versions of our
    prompt templates and evaluate how each version performs.
  prefs: []
  type: TYPE_NORMAL
- en: We will likely see many more prompt engineering and prompt management approaches
    being developed in the coming years. In addition to this, we can even apply **machine
    learning** (**ML**) techniques to find or suggest the best prompts for our use
    cases, either by asking an LLM to suggest the best prompts or by using traditional
    ML optimization approaches such as classification and regression to find or suggest
    prompts that will likely lead to the best outcome for a given use case. Expect
    to continue seeing interesting approaches emerging in this field.
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond prompt engineering and management, the next section describes larger-scale
    LLM tuning techniques, which can mainly be categorized under the umbrella of **transfer**
    **learning** (**TL**).
  prefs: []
  type: TYPE_NORMAL
- en: TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), I mentioned at a high level
    that LLMs usually go through an unsupervised pre-training phase followed by a
    supervised tuning phase. This section describes in more detail some of the supervised
    training techniques that are used to fine-tune LLMs. Let’s begin with a definition
    of TL.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: TL is an ML approach that uses a model that has already been trained on a certain
    task (or set of tasks) as a starting point for a new model that will perform a
    similar task (or set of tasks) with different but somewhat related parameters
    or data.
  prefs: []
  type: TYPE_NORMAL
- en: An example of TL would be to take a model that was pre-trained on generic image
    recognition tasks and then fine-tune it to identify objects that are specifically
    relevant to driving scenarios by incrementally training it on datasets containing
    road scenes.
  prefs: []
  type: TYPE_NORMAL
- en: TL approaches can be seen as a kind of spectrum, where some TL techniques and
    use cases only require updating a small portion of the model weights, while others
    involve much more extensive updates. Different points on this spectrum represent
    a trade-off between customizability and computational expense. For example, updating
    a lot of model weights provides more customizability but is more computationally
    expensive, while updating a small number of model weights offers less customizability
    but is also less computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin our discussion at one extreme of the spectrum, in which we will
    update all or a large portion of the original model’s weights, which we refer
    to as **full fine-tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: Full fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of fully fine-tuning LLMs, we could begin with a model that has
    been pre-trained on an enormous body of data and has learned a broad understanding
    of the concepts on which it was trained. We then introduce the model to a new
    dataset specific to the task at hand (for example, understanding the rules of
    the road). This dataset is usually smaller and more focused than the data used
    in the initial pre-training phase. During the fine-tuning process, the weights
    across all layers of the model are updated to minimize the loss in relation to
    our new task.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of things to bear in mind are that full fine-tuning can require a lot
    of computational resources, and there’s also a risk that the model might forget
    some of the useful representations it learned during pre-training (sometimes referred
    to as **catastrophic forgetting**), especially if the new task is considerably
    different from the pre-trained task.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that LLMs are large—in fact, they’re huge! With that in mind, many
    companies may find full fine-tuning infeasible due to the sheer amount of data,
    computational resources, and expense it often requires.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than tuning all of the weights in an enormous LLM, research has found
    that sometimes, we can get effective improvements on specific tasks by only changing
    some weights. In such cases, we can “freeze” some of the model’s weights (or parameters)
    to ensure they do not get updated while allowing others to be updated. These approaches
    can be more efficient because they require fewer parameters to be updated and
    are therefore referred to as **parameter-efficient fine-tuning** (**PEFT**), which
    I will describe in more detail in the following subsections, starting with **adapter
    tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: Adapter tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of adapter tuning, the original model layers remain unchanged, but
    we add **adapter layers** or **adapter modules**, which are small **neural networks**
    (**NNs**) inserted between the layers of a pre-trained model, consisting of just
    a few learnable parameters. As input data is fed into the model, it flows through
    the original pre-trained layers and the newly inserted adapter modules, and the
    adapters slightly alter the data processing flow, which introduces additional
    transformation steps in some of the layers.
  prefs: []
  type: TYPE_NORMAL
- en: The loss calculation steps that are performed after the data passes through
    the network are the same steps we learned about in our chapter on NNs, and the
    calculated loss is used to compute gradients for the model parameters. However,
    unlike traditional full-model fine-tuning, the gradients are only applied to update
    the weights of the adapter modules, while the weights of the pre-trained model
    itself remain frozen and unchanged from their initial values.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular PEFT technique is **low-rank adaptation** (**LoRA**), which
    I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LoRA is based on the premise that not all parameters in a NN are equally important
    for transferring learned knowledge to a new task, and that, by identifying and
    changing only a small subset of the model’s parameters, it’s possible to adapt
    the model for a specific task without completely retraining the model. Instead
    of modifying the original weight matrices directly, LoRA creates low-rank matrices
    that can be thought of as a simplified or compressed version of the full matrix
    that captures the most important properties with fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: This is what makes LoRA more parameter-efficient. As with adapter tuning, during
    backpropagation, only the parameters of the low-rank matrices are updated, and
    the original model parameters remain unchanged. Since the low-rank matrices have
    fewer parameters than the full matrices they represent, the training and updating
    process can be much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to improving a model’s performance on specific tasks, there are
    also more subtle performance improvement practices that are just as important,
    such as aligning with human values and expectations. Let’s explore that concept
    in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning with human values and expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you ever met somebody who is highly intelligent and talented, but has poor
    communication skills? Such a person might perform amazingly when it comes to troubleshooting
    and fixing technology issues, for example, but they may not be a suitable person
    to lead a customer meeting. They may say things that could be considered rude
    or inappropriate, or perhaps just a bit odd, due to their poor communication skills.
    This is the analogy I like to use to explain the concept of tuning GenAI models
    to align with human values and expectations because such values and expectations
    can often be more subtle than scientific and, therefore, require tailored approaches.
    For example, in addition to a model’s outputs being accurate, human expectations
    could require the model’s outputs to be friendly, safe, and unbiased, among other
    subtle, nuanced qualities. I’ll refer to this concept from here onward as **alignment**,
    and in this section I’ll describe two approaches that are commonly used for human
    value alignment today, starting with **reinforcement learning from human** **feedback**
    (**RLHF**).
  prefs: []
  type: TYPE_NORMAL
- en: RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We explored the concept of **reinforcement learning** (**RL**) in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015) of this book, and I explained that in RL,
    the model learns from rewards or penalties based on interactions with its environment
    as it pursues a specific goal. RLHF is one of those examples in which the name
    of the technology is highly descriptive and accurately captures what the technology
    entails. As the name suggests, RLHF is an extension of RL, in which feedback to
    the model is provided by humans.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of RLHF, we start with an LLM that has already been pre-trained
    on a large dataset. The model generates multiple possible responses to a prompt,
    and humans evaluate the responses based on various preferences. The human feedback
    data is then used to train a separate model called the **reward model**, which
    learns to predict the kinds of responses humans are likely to prefer, based on
    the feedback given. This process is intended to capture human preferences in a
    quantifiable way.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM then uses this feedback (like a reward signal) to update its parameters
    through RL techniques, making it more likely to generate responses that humans
    find desirable in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique that can be used to align with human values and expectations
    is **Direct Preference Optimization** (**DPO**). Let’s discuss that next.
  prefs: []
  type: TYPE_NORMAL
- en: DPO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DPO also uses human feedback to improve a model’s performance in aligning with
    human values and expectations. In the case of DPO, again, the model may provide
    multiple responses to a prompt, and a human can pick which response they prefer
    (similar to RLHF). However, DPO does not involve training a separate reward model.
    Instead, it uses pairwise comparisons as the optimization signal, and it directly
    optimizes policies based on user preferences rather than predefined reward functions,
    which is especially useful in cases where it’s difficult to define an explicit
    reward function.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that, while RLHF and DPO are valuable and important techniques,
    there are some challenges that human interaction inherently brings into scope.
    For example, any process that requires human interaction can be difficult to scale.
    This means that it can be difficult to gather large amounts of data via feedback
    from humans. Also, humans can make mistakes or introduce conscious or unconscious
    bias into the results. These are some factors you would need to monitor and mitigate
    if you implement an RLHF or DPO solution.
  prefs: []
  type: TYPE_NORMAL
- en: Other LLM tuning techniques continue to emerge in addition to the tuning approaches
    I’ve covered here, and a lot of research is being invested in this field. This
    is another space in which you can expect to continue seeing groundbreaking leaps
    forward in development.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive deeper into the role of embeddings and vector databases in
    GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and vector databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), we discussed the importance
    of embeddings and latent spaces, and I explained how they can be created in different
    ways. One way is when a generative model learns them intrinsically during its
    training process, and another is when we use specific types of models to create
    them explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: I also touched on why we would want to explicitly create them since they can
    be processed more efficiently and are a more suitable format for ML use cases.
    In this context, when we create an embedding for something, we are simply creating
    a vector of numeric values to represent it (how we actually do that is a more
    advanced topic we will cover shortly).
  prefs: []
  type: TYPE_NORMAL
- en: Another concept I briefly touched upon was the importance of relationships between
    embeddings in the vector space. For example, the proximity of embeddings to each
    other in the vector space can reflect the similarity between the concepts they
    represent. Let’s examine this relationship in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and similarity of concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A well-constructed embedding contains everything needed to describe and explain
    the concept it represents—that is, the “meaning” of the concept. That may seem
    somewhat abstract to us because we don’t often think of everything that goes into
    the meaning of something. For example, when I utter the word “car,” a certain
    image might appear in your mind, and a lot of other information is immediately
    contextualized by that word. You know that you can drive a car; you know that
    they are usually relatively expensive; you know that they have wheels and windows,
    and they’re traditionally made from some kind of metal. You know lots of pieces
    of information about the concept of a car, and this helps you to understand what
    it is. Imagine that all of the information required to conjure up the idea of
    a car is stored as a vector in your mind. Now, imagine that you have never heard
    of a truck before, and I suddenly show you a picture of a truck. It has wheels
    and windows; it’s made from steel; it looks like something you could possibly
    drive. Although you’ve never seen anything like it before, you can understand
    that this new object is similar to a car. That’s because the pieces of information
    (that is, the vector of information) associated with it are very similar to that
    of a car.
  prefs: []
  type: TYPE_NORMAL
- en: How do we make these kinds of associations? It feels somewhat intuitive, and
    most lay people (including myself) don’t understand how this all really happens
    in our brains. However, in the case of embeddings, it’s a lot easier to understand
    because our good old friend, mathematics (much loved by computers and ML models),
    comes to the rescue. As I mentioned already, we can mathematically compare different
    vectors by calculating the distances between them in their vector space, using
    well-established distance metrics such as Euclidean distance or cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'If concepts are close in the vector space, they are likely close in meaning,
    in which case, we can say that they are **semantically similar**. For example,
    in a text-based vector database, the phrases “The cat sat on the mat” and “The
    feline rested on the rug” would have very similar vector embeddings, even though
    their exact words differ. A query for one of these vectors is likely to also identify
    the other one as a highly relevant result. A classic example of this concept is
    the representation of the words “man,” “woman,” “king,” and “queen,” as represented
    in *Figure 16**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1: Embeddings and semantic similarity](img/B18143_16_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Embeddings and semantic similarity'
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in *Figure 16**.1*, depending on how embeddings of items are captured
    in a vector space, we may see somewhat consistent projections in the vector representations
    and distances between vectors. The example shows that the concept of binary gender
    (or some kind of consistent semantic relationship) is captured in the words “man,”
    “woman,” “king,” and “queen,” and moreover, the distance and direction from “man”
    to “woman” is similar to that as from “king” to “queen.” Additionally, the distance
    and direction from “man” to “king” is similar to that from “woman” to “queen.”
    In this scenario, hypothetically, you could infer the value “queen” by performing
    the following mathematical operation in the vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: king – man + woman = queen
  prefs: []
  type: TYPE_NORMAL
- en: That is, if you take the concept of a king, subtract the male gender, and add
    the female gender, you end up with the concept of a queen.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also apply this approach more broadly. For example, in a vector database
    storing image embeddings, a picture of a tangerine may lie in close proximity
    to a picture of a clementine. These objects have similar properties, such as size,
    shape, and color, and a vector representing a banana would likely not be as close
    to them as they are to each other because a banana does not share as many similarities.
    However, from a multimodal perspective, which we will describe later in this chapter,
    considering that they are all types of fruit, the banana vector may still be closer
    to them than a vector representing a car or an umbrella, for example. This concept
    is depicted in a simplified manner in *Figure 16**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2: Elaborated example of embeddings and semantic similarity](img/B18143_16_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Elaborated example of embeddings and semantic similarity'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in reality, the embedding space consists of many abstract dimensions
    and is not usually visualizable or directly interpretable to humans. Embeddings
    can be seen as a way to translate the messy complexity of the real world into
    a numerical language that ML models can understand.
  prefs: []
  type: TYPE_NORMAL
- en: If we create embeddings, we might want somewhere to store them and easily find
    them when needed. This is where vector databases come into the picture. Let’s
    dive into this in more detail and explain what vector databases are.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector databases are specialized databases designed to store and manage vector
    embeddings. Unlike traditional databases that focus on exact matches (for example,
    finding a customer with a specific ID), vector databases are more suitable for
    use cases such as **similarity search**, in which they use distance metrics to
    determine how close and, therefore, how similar two vector embeddings are in the
    latent space they occupy.
  prefs: []
  type: TYPE_NORMAL
- en: This approach significantly changes how we can retrieve and analyze data. While
    traditional methods rely on keywords or predefined attributes, vector databases
    allow us to perform semantic search, in which we can query based on meaning, and
    this enables new applications. If I search for “trousers,” for example, the responses
    can also include jeans, slacks, and chinos because these are all semantically
    similar concepts. Most modern search engines, such as Google, for example, use
    sophisticated mechanisms such as semantic search (and a combination of other techniques),
    rather than simple keyword matching.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search provides a much better customer experience, as well as a potential
    increase in revenue for the company implementing it. If you run a retail website,
    for example, bear in mind that customers will not buy a product if they cannot
    find it, and their business may go to your competitors instead. By implementing
    semantic search, we can understand the meaning and the intent of the user and,
    therefore, present the most relevant products to them.
  prefs: []
  type: TYPE_NORMAL
- en: These new search capabilities are also important in the context of GenAI for
    reasons I will explain shortly. First, however, let’s dive into more detail on
    how vector databases work.
  prefs: []
  type: TYPE_NORMAL
- en: How vector databases work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a precursor to explaining how vector databases work, let’s briefly talk about
    how databases work in general. The main purpose of any database is to store and
    organize data in a way that makes it easy to find as quickly as possible. Most
    databases make use of indexes for this purpose, which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing and neighbor search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the role of indexes, imagine I handed you a book and asked you
    to find information in the book about “anthropomorphism” as quickly as possible.
    Without any mechanisms to assist you, you would have to read through every word
    on every page until you found that word. This, of course, would be a very inefficient
    process, and it would likely take you a long time to find the information unless
    it happened to appear near the beginning of the book. This is referred to as a
    **full table scan** in the database world, which we usually want to avoid, if
    possible. This is where indexes come into play—if the book I handed you contains
    an index, you could look in the index to see which pages contain references to
    “anthropomorphism” and then go directly to those pages, cutting out a lot of inefficient
    searching.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases typically use tree-like (for example, B-trees) or hashing-based
    indexes for rapid lookups, but vector database indexes can be more complex due
    to the complexities of the embedding space and how each vector is represented
    in potentially high-dimensional space. Such indexes can resemble graph-like structures
    that map relationships between embeddings to enable proximity-based searches.
  prefs: []
  type: TYPE_NORMAL
- en: The equivalent of a full table scan in the vector database world is called a
    **brute-force** search, which borrows its name from cybersecurity and relates
    to the practice of trying every possible combination of inputs to find the desired
    result. This is a common way in which bad actors try to guess a person’s password—they
    try every possible combination of characters (within certain parameters). A longer
    password makes it difficult for them to guess it via brute force because each
    additional character exponentially increases the number of potential combinations.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of vector databases, our queries generally try to find items near
    the queried item in the vector space. If we have a lot of vectors in the database,
    then brute force would be impractical, especially in high-dimensional spaces where
    the **curse of dimensionality** (**CoD**) makes exact searches computationally
    expensive. Fortunately, brute force is usually unnecessary because it seeks to
    precisely find the vector that is closest to the vector being queried (referred
    to as the **exact nearest neighbor**), but we don’t always need the exact nearest
    neighbor. For example, use cases such as recommendation engines, which recommend
    items that are similar to a particular item, generally just need to find vectors
    that are in the close vicinity of the vector being queried, but it doesn’t have
    to be the exact nearest neighbor of the vector being queried. This is referred
    to as an **approximate nearest neighbor** (**ANN**) search, and it’s extremely
    useful in the context of vector databases because it allows a trade-off between
    the accuracy of the search results and the speed of the query – that is, for these
    kinds of use cases, it’s better to quickly get results that are close enough to
    the queried vector rather than spending a lot of time finding the exact best result.
  prefs: []
  type: TYPE_NORMAL
- en: Note – the CoD and vector proximity
  prefs: []
  type: TYPE_NORMAL
- en: In high-dimensional spaces, points become increasingly spread out, and as the
    number of dimensions increases, the distance between the nearest and farthest
    points becomes diluted, which makes it harder to distinguish between truly similar
    and dissimilar items based on distance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Some vector databases implement hybrid approaches that combine ANN with other
    indexing or search strategies, such as hierarchical indexing structures, multi-index
    hashing, and tree-based methods to improve accuracy, reduce search latency, or
    optimize resource usage. Graph-based methods such as **Navigable Small World**
    (**NSW**) graphs, **Hierarchical Navigable Small World** (**HNSW**) graphs, and
    others create a graph where nodes represent vectors, and queries then traverse
    the graph to find the nearest neighbors. Some vector databases also use partitioning
    or clustering algorithms to divide the dataset into smaller, more manageable chunks
    or clusters, and indexing can then be performed within these partitions or clusters,
    often using a combination of methods to first identify the relevant partition
    and then perform an ANN search within it.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can use approaches such as product quantization or scalar quantization
    to compress vectors by mapping them to a finite set of reference vectors, which
    reduces the dimensionality and storage requirements of the vectors before indexing
    and enables faster searches by approximating distances in the compressed space.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408), we’ll walk through the
    various vector database offerings in Google Cloud, but for now, let’s move on
    to discuss how we can create embeddings in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Creating embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already discussed **autoencoders** (**AEs**) in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    and we learned how they can be used to create latent space representations of
    their inputs. There are many more ways in which we can create embeddings, and
    I’ll describe some of the more popular methods in this section, starting with
    one of the most famous word embedding models, Word2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word2Vec (short for “word to vector”) is a group of algorithms invented by Google
    (Mikolov et al., 2013) to learn representations of words as vectors. It’s often
    considered to be the grandfather of word embedding approaches, and although it
    wasn’t the first word embedding technique to be invented, it promoted the idea
    of representing words as dense vectors that capture the semantic meaning and relationships
    between words in a high-dimensional space. It works by building a vocabulary of
    unique words and learning a vector representation for each word, where words with
    similar meanings have similar vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The two main approaches used in Word2Vec are the **Continuous Bag of Words**
    (**CBOW**) model, which predicts a target word based on its surrounding words,
    and **skip-gram**, which predicts surrounding words based on a given target word.
  prefs: []
  type: TYPE_NORMAL
- en: While Word2Vec has been a popular and useful approach, newer, transformer-based
    approaches have emerged that provide more advanced capabilities. Let’s take a
    look at those next.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve already introduced transformers in previous chapters, and in this section,
    I will briefly describe transformer-based approaches to create embeddings. While
    there are many options to choose from, I’ll focus on the famous **Bidirectional
    Encoder Representations from Transformers** (**BERT**) and its derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Earlier in this book, I mentioned that, every now and then, there’s a significant
    step forward in AI/ML research, and it’s well established that the invention of
    the transformer architecture by Google in 2017 (Vaswani et al., 2017) was one
    of those significant leaps. BERT, which was invented by Google in 2018 (Devlin
    et al., 2018), was another significant step.
  prefs: []
  type: TYPE_NORMAL
- en: As indicated in the name, BERT is based on the transformer architecture, and
    it is pre-trained on a large dataset of text and code. During its training, it
    learns to model complex patterns and relationships between words and understand
    nuances such as context, grammar, and how different parts of a sentence relate
    to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The two main approaches used in BERT are **masked language modeling** (**MLM**),
    which predicts missing words within a sentence based on the surrounding context,
    and **next sentence prediction** (**NSP**), which tries to determine if two sentences
    are logically connected. These are examples of the pre-training phase described
    in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371). By combining these two approaches,
    BERT develops an understanding of language structure.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of BERT are the transformer layers, which take the text we give
    it and produce word embeddings that capture meaning depending on the surrounding
    words, which is a major improvement over static embeddings such as Word2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: BERT was such an important breakthrough that many variants have been created
    since its initial release, such as DistilBERT and ALBERT, which are smaller, distilled
    versions of BERT with fewer parameters (trading off some accuracy for computational
    efficiency), as well as domain-specific variants, such as SciBERT (trained on
    scientific publications) and FinBERT (fine-tuned for financial industry use cases).
  prefs: []
  type: TYPE_NORMAL
- en: There are also more targeted models that build on top of transformer-based models
    such as BERT. For example, rather than focusing on individual words, **Sentence
    Transformers** use pooling strategies such as mean pooling or max pooling to create
    semantically meaningful embeddings of entire sentences.
  prefs: []
  type: TYPE_NORMAL
- en: For image embeddings, while we’ve discussed **convolutional NNs** (**CNNs**)
    in previous chapters for use cases such as image classification, it’s important
    to note that CNNs can also be used in creating image embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about some options for creating embeddings, as well as options
    for storing embeddings in vector databases, let’s explore a relatively new pattern
    that is becoming increasingly popular in the industry and that combines these
    topics, referred to as RAG.
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While LLMs are clearly impressive and powerful technologies, they have some
    limitations. Firstly, LLMs are only as good as the data on which they were trained,
    and they can be expensive and time-consuming to train, so for most companies,
    it would not be feasible to retrain them with new information every day. For this
    reason, in many cases, updated versions of LLMs tend to be released every few
    months. This means that the information provided in their responses depends on
    their most recent training dataset. If you want to ask about something that happened
    yesterday, but the LLM was last updated a month ago, it simply will not have any
    information on that topic. This can be quite limiting in today’s fast-paced business
    world, where people and applications constantly need up-to-date knowledge at their
    fingertips.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, unless you are one of the large corporations that have created the
    popular LLMs being used today, you most likely did not train the LLM yourself
    from scratch, so it has not been trained on your specific data. Imagine you are
    a retail company that wants to use an LLM to enable customers to find out about
    your products via a chat interface. Since the LLM was not trained specifically
    on your product catalog, it will not be familiar with the details of your products.
  prefs: []
  type: TYPE_NORMAL
- en: To combat these kinds of challenges, rather than relying solely on the data
    that was used to train an LLM, we can insert additional data into the context
    of the prompt being sent to the LLM. We already touched on this in the *Prompt
    engineering* section of this chapter, in which we provided additional information
    in the prompts in order to guide the LLM’s responses to more accurately match
    our required outputs. That approach works great for small amounts of data, but
    if we consider the aforementioned use case of getting the LLM to answer questions
    about a product catalog, we cannot include the entire product catalog in every
    prompt. This is where RAG can help.
  prefs: []
  type: TYPE_NORMAL
- en: With RAG, we can *augment* the responses *generated* by LLMs by first *retrieving*
    relevant information from a data store and then including that information in
    the prompt being sent to the LLM. That data store can contain whatever information
    we want, such as the contents of our product catalog. Let’s take a look at this
    pattern in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How RAG works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set the context for explaining how RAG works, let’s level-set the discussion
    by reviewing how LLM interactions typically work without RAG. This is very straightforward
    and is depicted in *Figure 16**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3: LLM prompt and response](img/B18143_16_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: LLM prompt and response'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the typical LLM interaction simply consists of sending a prompt
    and getting a response. Note that LLMs can only understand numerical values, so
    behind the scenes, the textual prompt sent by the user is broken down into tokens,
    and the tokens are encoded into vectors that the LLM can understand. The reverse
    process is performed when sending the response to the user. Next, let’s look at
    how RAG changes the process, as depicted in *Figure 16**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4: RAG](img/B18143_16_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: RAG'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 16**.4* outlines how RAG works at a high level. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The user or client application sends a textual request or prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The contents of the prompt are converted to vectors (embeddings).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the “retrieval” part of the process, the prompt embeddings are used to
    find similar embeddings in the vector database. These embeddings represent the
    data that we want to use to augment the prompt (for example, embeddings that represent
    data from our product catalog).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The embeddings from our vector database are combined with the embeddings of
    the user prompt and sent to the LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM uses the combined embeddings of the user prompt and the retrieved embeddings
    to augment its response, aiming to provide a response that is not only based on
    the data on which it was originally trained but that is also relevant in the context
    of the data retrieved from our vector database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The response embeddings are decoded to provide a text response back to the user
    or client application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will implement RAG in a later chapter in this book. Next, let’s begin to
    explore the topic of multimodal models.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that LLMs, as the name suggests, focus on language, the primary modality
    for many of the popular LLMs you are likely interacting with at the time of writing
    this in early 2024 is likely to be text. However, the concept of GenAI also goes
    beyond text and expands into other modalities, such as pictures, sound, and video.
  prefs: []
  type: TYPE_NORMAL
- en: The term *multimodal* is becoming ever more prominent within the field of GenAI.
    In this section, we explore what that means, starting with a definition of “modality.”
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: 'The Oxford English Dictionary defines modality as:'
  prefs: []
  type: TYPE_NORMAL
- en: '“*Those aspects of a thing which relate to its mode, or manner or state of
    being, as distinct from its substance or identity; the non-essential aspect or
    attributes of a concept or entity. Also: a particular quality or attribute denoting
    the mode or manner of being* *of something.*”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Oxford English Dictionary*, *s.v. “modality (n.), sense 1.a,”* *December*
    *2023*, [https://doi.org/10.1093/OED/1055677936](https://doi.org/10.1093/OED/1055677936).'
  prefs: []
  type: TYPE_NORMAL
- en: Considering this formal definition of modalities, unlike traditional models
    that focus on a single data type, multimodal models are designed to process information
    from multiple modalities or data types. This is another one of those important
    leaps forward in AI research because it opens up new use cases and applications
    for AI. Let’s dive into this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Why multimodality matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the section on RAG in this chapter, I mentioned an example of a company using
    an LLM to help customers explore product details via a chat interface and how
    that would require interacting with data from the company’s product catalog. Diving
    into this example in more detail, consider that items in a product catalog can
    have many different types of information associated with them. This may consist
    of structured data with standardized fields such as product dimensions, color,
    price, and other attributes. It may be associated with semi-structured or unstructured
    data such as customer reviews as well as product images. Imagine how much of a
    more detailed understanding our model could gain regarding the items in our catalog
    if it could ingest and understand all of those data modalities. If the written
    product description was somehow missing details regarding the color of the product,
    the model could instead learn that information from pictures of the product. Also,
    in addition to written customer reviews, some websites allow customers to post
    videos in their reviews. The model could learn more information from those videos.
    Overall, this could provide a much richer end-user experience. This is just one
    example of the power of multimodality, and this concept becomes even more important
    when creating models that need to interact with the physical world, such as robots
    and self-driving vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal implementations are also important in AI research because one of
    the main challenges in creating useful and powerful models is getting access to
    relevant data. Considering that there is a finite amount of text that has ever
    been created in the world, if text remained the only modality, it would eventually
    become a limiting factor in AI development. As we can all clearly see on various
    social media platforms and other websites, video, audio, and pictures have equaled
    or surpassed text as the primary modalities in user-generated content, and we
    can expect those trends to continue. We’ve all heard the expression, “A picture
    is worth a thousand words,” and this becomes ever more true (in addition to other
    modalities) when training and interacting with large generative models!
  prefs: []
  type: TYPE_NORMAL
- en: Having expressed the importance and advantages of multimodal model implementations,
    I’ll balance the discussion by highlighting some of the related challenges next.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodality challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ll begin this section with a challenge that consistently emerges in many contexts
    that we’ve discussed throughout this book. Generally, creating more powerful models
    requires more computing resources. Multimodal approaches are no exception here,
    and training multimodal models often requires significant computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge is the complexity—combining data from different modalities
    can be complex and require careful preprocessing, especially considering that
    the feature representations in different modalities usually have very different
    feature spaces, and some modalities have richer or more sparse data than others.
    For example, text is usually sequential, images are spatial, and audio and video
    have temporal properties, so aligning all of these for the model to build a unified
    understanding can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we looked at examples of data quality issues and the kinds
    of data preparation steps and pipelines required to address those issues. The
    tricky thing, however, is that data in different modalities has different kinds
    of potential quality issues, preparation steps, and pipelines that are required
    to get it ready for training models.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the challenges, multimodality is a rapidly developing space that will
    continue to get significant attention for the foreseeable future.
  prefs: []
  type: TYPE_NORMAL
- en: While this section focused mainly on training GenAI models (specifically, multimodal
    approaches), other parts of the model development life cycle also have unique
    approaches that must be factored in when dealing with GenAI models rather than
    traditional AI models. The next important topic to highlight is how we evaluate
    GenAI models and how this can differ in some cases from evaluating traditional
    AI models.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While evaluating traditional AI models usually involves measuring the model’s
    outputs against a ground truth dataset and calculating established, objective
    metrics such as accuracy, precision, recall, F1 score, **Mean Squared Error**
    (**MSE**), and others we’ve covered in this book, evaluating generative models
    is not always as straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating a GenAI model, we might want to focus on different factors,
    such as the model’s ability to produce creative and human-like outputs while staying
    relevant to the task at hand. An extra challenge in this case is that evaluating
    these properties can be somewhat vague and subjective. For example, if I ask a
    generative model to write a poem or create a photorealistic picture of a cat in
    a meadow, the quality of that poem or the realness of the picture is not always
    easy to represent with a mathematically calculated number, although some formulaic
    measurements do exist, which I will describe in this section. However, let’s first
    discuss human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most straightforward but also slow, cumbersome, and potentially
    expensive approach is for human evaluators to assess generated outputs for quality,
    creativity, coherence, and how well they align with the task requirements. There
    are tools, frameworks, services, and entire companies dedicated to performing
    these kinds of evaluations. Some common challenges with this approach are that
    the evaluations are only as good as the instructions provided to the evaluators,
    so it’s necessary to clearly understand and explain how the responses should be
    evaluated, and the evaluations can be subjective, based on the personal interpretations
    of the evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll describe some of the formalized methods that can be used in evaluating
    generative models, starting with metrics for textual language models such as **BiLingual
    Evaluation Understudy** (**BLEU**) and **Recall-Oriented Understudy for Gisting**
    **Evaluation** (**ROUGE**).
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEU is used mainly for evaluating machine translation use cases, and it works
    by measuring how similar the generated text is to human-written text, so when
    we’re using BLEU, we need a set of reference, human-written text examples, which
    are compared against the generated text. More specifically, BLEU measures something
    called **n-gram overlap**, which is the overlap in sequences of words between
    the generated text and the reference text.
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ROUGE also measures the overlap between generated text and human-written reference
    examples, but it focuses on summarization use cases by measuring the **recall**,
    which represents how well a generated summary captures important information from
    the reference summaries.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to overlap-based evaluations, we can also evaluate text generation
    performance with metrics such as **perplexity** and **negative log-likelihood**
    (**NLL**), which can be used to evaluate a model’s performance in predicting sequences
    of words, such as predicting the next word in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s consider some approaches for evaluating image generation models,
    such as the **Inception Score** (**IS**) and **Fréchet Inception** **Distance**
    (**FID**).
  prefs: []
  type: TYPE_NORMAL
- en: IS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IS is used to measure the quality and **diversity** of generated images, in
    which a good image generation model should be able to generate a wide variety
    of images, not just variations of the same thing, and the images should look clear
    and realistic. We can calculate the IS by using a pre-trained image classification
    model to classify images generated by a generative model.
  prefs: []
  type: TYPE_NORMAL
- en: FID
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the IS evaluates generated images independently, FID was designed to improve
    on it by measuring how similar a distribution of generated images is to a distribution
    of real images (it gets its name from using the Fréchet distance to measure the
    distance between two distributions).
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of this chapter, we mainly need to be aware that specific metrics
    exist for evaluating some generative models, but going into the mathematical detail
    of how these metrics are calculated would be more information than is required
    at this point. There are additional approaches and metrics beyond the ones we’ve
    covered in this section, but this section describes some popular ones to consider.
    Next, I’ll introduce a somewhat different approach to evaluating generative models,
    which is to use an **auto-rater**.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-raters and side-by-side evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, an auto-rater is an ML model that rates or evaluates the outputs
    of models. There are different ways in which this can be done, and I will explain
    the concept in the context of Google Cloud Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI recently launched a service called **Automatic side-by-side**
    (**AutoSxS**), which can be used to evaluate pre-generated predictions or GenAI
    models in the Vertex AI Model Registry. This means that, in addition to Vertex
    AI foundation models, we can also use it to evaluate third-party language models.
  prefs: []
  type: TYPE_NORMAL
- en: In order to compare the results of two models against each other, it uses an
    auto-rater to decide which model gives the better response to a prompt. The same
    prompt is sent to both models, and the auto-rater evaluates the responses from
    each model against various criteria, such as relevance, comprehensiveness, the
    model’s ability to follow instructions, and whether the responses are grounded
    in established facts.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned at the beginning of this section, evaluating generative models
    often requires some subjectivity and more flexibility than the simple objective
    evaluations of traditional ML models. The various criteria supported by Google
    Cloud Vertex AI AutoSxS provide such additional flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a high-level understanding of how the evaluation of GenAI models
    differs in some ways from that of traditional ML models, let’s move on to introduce
    another important topic in GenAI—LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By sending a prompt to an LLM, we can achieve amazing feats and get useful information.
    However, we may want to build applications that implement more complex logic than
    we could achieve in a single prompt, and these applications may require interacting
    with multiple systems in addition to an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is a popular framework for developing applications using LLMs, and
    it enables us to combine multiple steps into a chain, in which each step implements
    some logic, such as reading from a data store, sending a prompt to an LLM, taking
    the outputs from the LLM, and using them in subsequent steps.
  prefs: []
  type: TYPE_NORMAL
- en: One of LangChain’s advantages is that it uses a modular approach, so we can
    build complex workflows by combining smaller, simpler modules of logic. It provides
    tools for useful tasks such as creating prompt templates and managing context
    for interactive integrations with LLMs, and we can easily find integrations for
    accessing information from sources such as Google Search, knowledge bases, or
    custom data stores.
  prefs: []
  type: TYPE_NORMAL
- en: We will use LangChain later in this book to orchestrate a number of different
    steps in a workflow. For now, let’s summarize what we covered in this chapter
    before we move on to the next one.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter on advanced GenAI concepts and use cases, we started by diving
    into techniques for tuning and optimizing LLMs. We learned how prompt engineering
    practices can affect model outputs, and how tuning approaches such as full fine-tuning,
    adapter tuning, and LoRA enable pre-trained models to be adapted for specific
    domains or tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into embeddings and vector databases, including how they represent
    the meanings of concepts, and enable similarity-based searches. We looked into
    specific embedding models such as Word2Vec and transformer-based encodings.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to describe how RAG can help us to combine information from
    custom data stores into prompts being sent to an LLM, thereby enabling the LLM
    to modify its responses in alignment with the contents of our data stores.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we discussed multimodal models and how they can open up additional
    use cases beyond textual language. We then moved on to discuss how the evaluation
    of GenAI models differs in some ways from that of traditional ML models, and we
    introduced some new approaches and metrics for evaluating GenAI models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced the important topic of LangChain and how it can help
    us build applications that implement complex logic by chaining simple modules
    or building blocks together.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the various GenAI offerings and implementations
    on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
