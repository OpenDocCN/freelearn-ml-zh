- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Advanced Generative AI Concepts and Use Cases
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级生成式AI概念和应用案例
- en: 'Now that we’ve covered the basics of **generative AI** (**GenAI**), it’s time
    to start diving deeper. In this chapter, we will cover more advanced topics in
    the field of GenAI. We’ll begin by learning about some techniques to tune and
    optimize generative models for specific domains or tasks. Then, we’ll dive into
    more detail on the important topics of embeddings and vector databases and how
    they relate to a relatively new pattern of using **retrieval-augmented generation**
    (**RAG**) to ground our **large language model** (**LLM**) responses in our own
    data. Next, we’ll discuss multimodal models, how they differ from standard, text-based
    LLMs, and the kinds of use cases they support. Finally, we will introduce LangChain,
    which is a popular framework that enables us to design complex applications that
    can build on the functionalities provided by LLMs. Specifically, this chapter
    covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了生成式AI（GenAI）的基础知识，是时候开始深入探讨了。在本章中，我们将介绍GenAI领域的更多高级主题。我们将从学习一些针对特定领域或任务的生成模型调优和优化技术开始。然后，我们将更详细地探讨嵌入和向量数据库的重要主题以及它们如何与使用**检索增强生成**（RAG）的新模式相关联，以将我们的**大型语言模型**（LLM）响应基于我们的数据。接下来，我们将讨论多模态模型，它们与基于文本的LLM有何不同，以及它们支持的使用案例。最后，我们将介绍LangChain，这是一个流行的框架，使我们能够设计复杂的应用程序，这些应用程序可以建立在LLM提供的功能之上。具体来说，本章涵盖了以下主题：
- en: Advanced tuning and optimization techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级调优和优化技术
- en: Embeddings and vector databases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入和向量数据库
- en: RAG
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG
- en: Multimodal models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态模型
- en: GenAI model evaluation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI模型评估
- en: LangChain
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain
- en: Let’s dive right in and start learning about advanced tuning and optimization
    techniques!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接深入探讨，开始学习高级调优和优化技术！
- en: Note
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The term *LLM* has become almost synonymous with the field of GenAI in general,
    but considering that **LLM** stands for **large language model**, the term technically
    relates them to language processing. It’s important to note that there are other
    types of GenAI models, such as some image generation and multimodal models, that
    are not strictly language models. Additionally, a more recent concept of **large
    action models** or **large agentic models** (**LAMs**) has emerged, which combine
    the language understanding and generation capabilities of LLMs with the ability
    to perform actions (such as planning and booking a vacation) by using tools and
    agents to interact with their environment. However, for simplicity, throughout
    the rest of this book, I will use the terms *LLM* and *GenAI* *model* interchangeably.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “*LLM*”这个术语在GenAI领域几乎成了同义词，但考虑到**LLM**代表**大型语言模型**，这个术语在技术上将其与语言处理联系起来。需要注意的是，还有其他类型的GenAI模型，例如一些图像生成和多模态模型，它们并不是严格意义上的语言模型。此外，一个更近的概念是**大型动作模型**或**大型代理模型**（**LAMs**）已经出现，它们结合了LLM的语言理解和生成能力，以及通过使用工具和代理与环境交互来执行动作（如规划和管理假期）的能力。然而，为了简单起见，在本书的其余部分，我将交替使用*LLM*和*GenAI*
    *模型*这两个术语。
- en: Advanced tuning and optimization techniques
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级调优和优化技术
- en: At the end of the previous chapter, we discussed LLMs and how they are trained
    and tuned. I mentioned some of the tuning approaches at a high level, and in this
    section, we will dive deeper into how we can tune LLMs to more adequately address
    our specific needs. Let’s set the stage by outlining how we interact with LLMs
    in the first place, which we generally do via **prompts**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的结尾，我们讨论了LLM以及它们的训练和调优。我提到了一些高级别的调优方法，在本节中，我们将深入探讨如何调优LLM以满足我们的特定需求。让我们首先概述我们如何与LLM互动，我们通常是通过**提示**来做到这一点的。
- en: Definition
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: A prompt is a piece of text or instruction that we provide to an LLM to guide
    its response or output. It tells the LLM what to do and, in some cases, provides
    guidance on how to do it; for example, “**summarize this financial document, specifically
    focusing on details relating to company performance in** **Q4, 2023**.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们提供给LLM的一段文本或指令，以引导其响应或输出。它告诉LLM要做什么，在某些情况下，还提供了如何做的指导；例如，“**总结这份财务文件，特别关注**
    **2023年第四季度** **与公司业绩相关的细节**。”
- en: The first LLM tuning technique we’ll explore is **prompt engineering**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的第一个高级LLM调优技术是**提示工程**。
- en: Prompt engineering
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: Prompts are the most straightforward method we can use to tune an LLM’s outputs
    to our specific needs. In fact, during the early days of the GenAI popularity
    explosion that happened in late 2022 and early 2023, you may recall seeing news
    headlines about a new type of extremely high-demand role that was emerging, called
    **prompt engineer**. In this section, we’ll discuss what prompt engineering is
    and how it can be used to improve the outputs of GenAI models, which will help
    provide context on why there was such a sudden increase in demand for talented
    people in this space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们可以用来调整LLM输出以满足我们特定需求的最直接方法。事实上，在2022年底和2023年初GenAI流行爆炸的早期，您可能还记得看到关于一种新兴的极高需求角色的新闻标题，称为**提示工程师**。在本节中，我们将讨论提示工程是什么，以及它如何被用来改进GenAI模型的输出，这将有助于解释为什么对这一领域有如此突然的需求增加。
- en: 'To begin our discussion, consider the following prompt:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的讨论，考虑以下提示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a very simple prompt, and it does not provide much information to the
    LLM about what kind of poem we would like it to write for us. If we want a specific
    type of output, we could include additional instructions in our prompt, such as
    the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的提示，它并没有向LLM提供很多关于我们希望它为我们写什么类型的诗的信息。如果我们想要特定的输出类型，我们可以在我们的提示中包含额外的指令，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The latter prompt will produce a much more specific poem that abides by the
    parameters we’ve outlined. Feel free to go ahead and try this out for yourself.
    Visit the following URL, enter each of those prompts, and see how different the
    responses appear: [https://gemini.google.com](https://gemini.google.com).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 后者提示将生成一首更具体的诗，它遵循我们概述的参数。您可以自由尝试，访问以下网址，输入每个提示，看看不同的响应：[https://gemini.google.com](https://gemini.google.com)。
- en: Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can use the aforementioned URL to test all of the prompts in this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用上述网址测试本书中的所有提示。
- en: This is a rudimentary form of prompt engineering that simply includes additional
    instructions in the prompt to help the LLM better understand how it should respond
    to our request. I’ll explain additional prompt engineering techniques in this
    section, beginning by outlining some standard best practices for crafting effective
    prompts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种基本的提示工程形式，它只是在提示中包含额外的指令，以帮助LLM更好地理解它应该如何响应我们的请求。我将在本节中解释额外的提示工程技术，首先概述一些制定有效提示的标准最佳实践。
- en: Core prompt design principles
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心提示设计原则
- en: 'The following are some principles we should keep in mind when creating prompts:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们创建提示时应记住的一些原则：
- en: '**Be clear**: We should be clear about what we want the LLM to do and avoid
    using ambiguous language'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清晰表达**：我们应该清楚地了解我们希望LLM做什么，并避免使用含糊的语言。'
- en: '**Be specific**: Using specific instructions can help us get better-quality
    results'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体明确**：使用具体指令可以帮助我们获得更好的结果。'
- en: '**Provide adequate context**: Without appropriate context, the LLM’s responses
    might not be as relevant to what we are trying to achieve'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供足够的上下文**：如果没有适当的上下文，LLM的响应可能不会与我们试图实现的目标相关。'
- en: I included some of those principles in the latter example prompt in the previous
    section, and I’ll outline them more explicitly here. In most cases, it’s important
    for prompts to be as clear and specific as possible due to the sheer and ever-growing
    versatility of LLMs. For example, if I ask an LLM to write a story about happiness,
    there’s almost no way of knowing what kinds of details it will come up with. While
    this is one of the wonderful properties of LLMs, it can be challenging when we
    want an LLM to provide results within specific parameters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一个示例提示中包含了一些这些原则，我将在这里更明确地概述它们。在大多数情况下，由于LLM的纯粹和不断增长的多样性，提示尽可能清晰和具体是很重要的。例如，如果我要求LLM写一个关于幸福的小说，几乎无法知道它会想出什么样的细节。虽然这是LLM的一个美妙特性，但当我们希望LLM在特定参数内提供结果时，这可能会很具挑战性。
- en: 'Before I provide additional examples regarding clarity and specificity, consider
    use cases in which we might not want to be clear and specific, such as when we’re
    not yet sure of our exact requirements, and we want the LLM to help us explore
    various potential options. In that case, we could start by sending a broad prompt
    to the LLM, such as the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我提供关于清晰性和具体性的额外示例之前，考虑一下我们可能不希望清晰和具体的情况，例如当我们还不确定我们的确切需求时，我们希望LLM帮助我们探索各种潜在选项。在这种情况下，我们可以从向LLM发送一个广泛的提示开始，如下所示：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In response to a broad prompt such as this one, the LLM will likely come up
    with all kinds of random ideas, from baking cakes to developing video games that
    help kids learn mathematics.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个广泛的提示，LLM可能会提出各种各样的随机想法，从烘焙蛋糕到开发帮助儿童学习数学的视频游戏。
- en: 'After getting some initial outputs from the LLM, we could then iteratively
    refine the process by diving in on certain ideas more specifically. For example,
    if we really like the idea of developing video games for kids, we could follow
    up with a more specific prompt, such as the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在从LLM获得一些初步输出后，我们可以通过更具体地深入研究某些想法来迭代地改进这个过程。例如，如果我们非常喜欢为儿童开发视频游戏的想法，我们可以跟进一个更具体的提示，如下所示：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, this prompt is much more specific than the previous prompt and
    will likely provide more targeted and actionable responses. This updated prompt
    abides by all of the principles we’ve outlined because it’s specific in terms
    of exactly what we want to achieve (starting a business to create a video game),
    it provides context such as the intended content of the game and the target audience,
    and it’s clear in terms of how we want the LLM to respond (that is, list and explain
    the steps).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个提示比之前的提示更具体，可能会提供更针对性和可操作性的回应。这个更新的提示遵循了我们概述的所有原则，因为它在确切想要实现的目标方面是具体的（即开始创建视频游戏业务），它提供了上下文，例如游戏的内容和目标受众，并且它在我们希望LLM如何回应方面是清晰的（即列出并解释步骤）。
- en: Sometimes, we need to provide large amounts of context, and we will discuss
    how to do that later in this chapter. Next, however, let’s explore how we can
    use chaining to refine the outputs we get from an LLM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要提供大量的上下文，我们将在本章的后面讨论如何做到这一点。然而，接下来，让我们探讨如何使用链接来细化我们从LLM获得的输出。
- en: Chaining
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链接
- en: The process I described in the previous section, in which we use the output
    of the first prompt as an input to a subsequent prompt, is called **prompt chaining**.
    This can be done via an interactive process such as a chat interface, or in an
    automated fashion using tools I will describe later, such as LangChain. There’s
    also a framework called ReAct, in which we can chain multiple actions together
    to achieve a broader goal, which I’ll describe next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一个章节中描述的过程，即我们将第一个提示的输出作为后续提示的输入，这个过程被称为**提示链**。这可以通过交互式过程，如聊天界面，或者使用我稍后将要描述的工具（如LangChain）以自动化的方式完成。还有一个名为ReAct的框架，我们可以将多个动作链接在一起以实现更广泛的目标，我将在下一节中描述。
- en: ReAct and agents
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReAct和代理
- en: I briefly touched upon the concept of LAMs at the beginning of this chapter,
    and I’ll cover it in more detail here.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章的开头简要提到了LAMs的概念，在这里我将更详细地介绍它。
- en: '**ReAct** stands for **Reasoning and Acting**, and it’s a framework for combining
    the reasoning abilities of LLMs with the ability to take actions and interact
    with external tools or environments, which helps us build solutions that go beyond
    simply generating content to achieve more complex goals. Examples of external
    tools include software, APIs, code interpreters, search engines, or custom extensions.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReAct**代表**推理和行动**，它是一个将LLM的推理能力与采取行动和与外部工具或环境交互的能力相结合的框架，这有助于我们构建超越简单生成内容的解决方案，以实现更复杂的目标。外部工具的例子包括软件、API、代码解释器、搜索引擎或自定义扩展。'
- en: By using ReAct to go beyond generating content, we can build what are referred
    to as **agents**. The role of an agent is to interpret the user’s desired outcome
    and decide which tools to utilize to achieve the intended goal. Here, I will again
    highlight the example of planning and booking a vacation, which might include
    booking flights, accommodation, restaurant reservations, and excursions, among
    other tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用ReAct超越生成内容，我们可以构建被称为**代理**的东西。代理的作用是解释用户期望的结果，并决定使用哪些工具来实现既定目标。在这里，我再次强调规划并预订假期的例子，这可能包括预订航班、住宿、餐厅预订和远足等活动。
- en: Chaining is not to be confused with another prompt engineering approach, called
    **chain-of-thought** (**CoT**) prompting, which we can use to help LLMs work through
    complex reasoning tasks, which I’ll describe next.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 链接不应与另一种名为**思维链**（**CoT**）的提示工程方法混淆，我们可以使用它来帮助LLM处理复杂的推理任务，我将在下一节中描述。
- en: CoT prompting
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维链提示
- en: 'CoT prompting involves creating prompts in a way that guides the model through
    a step-by-step reasoning process before arriving at a final answer, much like
    a human might do when solving a problem. Interestingly, simply adding the words
    *step by step* to a prompt can cause the model to respond differently and work
    through a problem in a logical manner, while it might not have done so in the
    absence of those words. We can also help the model if we describe tasks in a step-by-step
    manner. For example, consider the following prompt as a baseline, which does not
    implement CoT prompting:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示涉及以引导模型通过逐步推理过程到达最终答案的方式创建提示，这与人类在解决问题时可能采取的方式非常相似。有趣的是，仅仅在提示中添加“逐步”这个词就可以导致模型以不同的方式响应，并以逻辑方式处理问题，而如果没有这些词，它可能不会这样做。我们还可以通过以逐步的方式描述任务来帮助模型。例如，考虑以下作为基线的提示，它没有实现CoT提示：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We know that LLMs are large language models and not large mathematics models,
    but we may be able to teach an LLM how to perform complex mathematical calculations
    by teaching it the step-by-step process. If the LLM were struggling to provide
    an accurate answer, we could explain the steps as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道LLM是大型语言模型，而不是大型数学模型，但我们可以通过教它逐步的过程来教LLM如何进行复杂的数学计算。如果LLM在提供准确答案时遇到困难，我们可以按以下步骤解释：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While the model may not have been able to immediately provide the requested
    answer, we can help it to get there by teaching it about what should happen in
    each step, and then it can chain those steps together to get to the end result.
    To learn more about this fascinating topic, I recommend reading the research paper
    titled *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*
    (Wei et al., 2022), available at the following URL:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型可能无法立即提供所需的答案，但我们可以通过教它每个步骤应该发生什么来帮助它达到目标，然后它可以依次连接这些步骤以得到最终结果。要了解更多关于这个有趣的主题，我建议阅读标题为《Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models》（Wei等人，2022年）的研究论文，该论文可在以下网址找到：
- en: '[https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)'
- en: Next, I’ll cover another popular prompt engineering technique, which I briefly
    mentioned in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), called **few-shot
    prompting**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将介绍另一种流行的提示工程技术，我在[*第15章*](B18143_15.xhtml#_idTextAnchor371)中简要提到了它，称为**少样本提示**。
- en: Few-shot prompting
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少样本提示
- en: All of the prompts I’ve included so far in this chapter are examples of **zero-shot
    prompting** because I simply asked the LLM to do something without providing any
    referenceable examples of how to do that thing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我包含的所有提示都是**零样本提示**的例子，因为我只是简单地要求LLM（大型语言模型）做某事，而没有提供任何关于如何做那件事的可参考示例。
- en: 'Few-shot prompting is a rather simple concept, as it just means that we are
    providing some examples in our prompt that teach the LLM how to perform the task
    we are requesting. An example of few-shot prompting would be the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示是一个相当简单的概念，它仅仅意味着我们在提示中提供了一些示例，教LLM如何执行我们请求的任务。少样本提示的一个例子如下：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see in the prompt, I’ve instructed the LLM on how to categorize the
    sentiment of a review, and I provided detailed examples of the format in which
    I want the LLM to respond, including specific product properties that the sentiment
    appears to reference.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在提示中看到的那样，我指导了LLM如何对评论的情感进行分类，并提供了我想要LLM以特定格式响应的详细示例，包括情感似乎参考的具体产品属性。
- en: In the case of few-shot prompting, we’re providing the LLM with a mini “training
    set” within the prompt itself, and it can use these examples to understand the
    expected mapping between the inputs and the desired outputs. This helps the LLM
    to understand what it should focus on and how it should structure its responses,
    hopefully providing results that more closely match the specific needs of our
    use case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本提示的情况下，我们在提示中为LLM提供了一个迷你“训练集”，它可以使用这些示例来理解输入和期望输出之间的预期映射。这有助于LLM理解它应该关注什么以及如何构建其响应，希望提供更符合我们特定用例需求的结果。
- en: Although the examples can be seen as a small training set included in our prompt,
    it’s important to note that the underlying LLM’s weights are not changed by the
    examples we provide. This is true for most prompt engineering techniques, whereby
    the responses may change significantly based on our inputs, but the underlying
    LLM’s weights remain unchanged.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些例子可以被视为包含在我们提示中的小训练集，但重要的是要注意，我们提供的示例不会改变底层LLM的权重。这对于大多数提示工程技术都是正确的，其中响应可能会根据我们的输入而显著变化，但底层LLM的权重保持不变。
- en: Few-shot prompting and prompt engineering, in general, can be surprisingly effective,
    but in some cases, we need to provide many examples to the LLM, and we want to
    perform incremental training (that is, updating weights), especially for complex
    tasks. I will outline how we can approach those scenarios shortly, but first,
    I’ll introduce the important emerging field of prompt management.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示和提示工程通常可以出奇地有效，但在某些情况下，我们需要向LLM提供许多示例，并且我们希望进行增量训练（即更新权重），尤其是在复杂任务中。我将在稍后概述我们如何处理这些场景，但首先，我将介绍一个重要的新兴领域——提示管理。
- en: Prompt management practices and tooling
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示管理实践和工具
- en: While new technologies bring new functionality and opportunities, they often
    also introduce new challenges. As prompt engineering continues to develop, a common
    challenge companies encounter is the need to store and manage their prompts effectively.
    For example, if a company’s employees are coming up with awesome prompts that
    provide wonderful results, they will want to track and reuse those prompts. They
    may also want to update the prompts over time as the LLMs and other components
    of their solutions evolve, in which case they will want to track different versions
    of prompts and their results.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然新技术带来了新的功能和机会，但它们通常也会带来新的挑战。随着提示工程的不断发展，公司面临的一个常见挑战是需要有效地存储和管理他们的提示。例如，如果公司的员工提出了提供出色结果的出色提示，他们希望跟踪和重用这些提示。随着时间的推移，LLM和其他解决方案组件的演变，他们可能还希望更新提示，在这种情况下，他们希望跟踪提示的不同版本及其结果。
- en: In software development, we use versioning systems to track and manage updates
    to our application code. We can apply the same principles to prompt engineering,
    using versioning systems and templates to help us efficiently develop, reuse,
    and share prompts. Companies performing advanced prompt engineering practices
    will likely end up curating prompt libraries and repositories for these purposes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，我们使用版本控制系统来跟踪和管理我们应用程序代码的更新。我们可以将这些原则应用于提示工程，使用版本控制系统和模板来帮助我们有效地开发、重用和共享提示。执行高级提示工程实践的公司可能会最终为这些目的整理提示库和存储库。
- en: '**Prompt templates** can be used to standardize the structures of effective
    prompts so that we don’t need to keep using trial and error and reinventing the
    wheel to create prompts that are best suited for specific use cases. For example,
    imagine that we work in the marketing department and we run a monthly report to
    measure the success of our marketing campaigns. We may want to use an LLM to summarize
    the reports, and there are likely specific pieces of information that certain
    team members want to review each time. We could create the following prompt template
    to formulate those requirements:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示模板**可以用来标准化有效提示的结构，这样我们就不需要继续使用试错和重新发明轮子来创建最适合特定用例的提示。例如，想象我们在市场营销部门工作，我们运行月度报告来衡量我们营销活动的成功。我们可能希望使用LLM来总结报告，并且很可能有一些特定信息是某些团队成员每次都希望审查的。我们可以创建以下提示模板来制定这些要求：'
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, the marketing team members simply need to use this prompt template and
    fill in their desired values for each of the placeholders in the template. As
    is the case with individual prompts, we could maintain different versions of our
    prompt templates and evaluate how each version performs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，市场营销团队成员只需使用这个提示模板，并在模板中的每个占位符处填写他们希望使用的值。与个人提示一样，我们可以维护我们提示模板的不同版本，并评估每个版本的表现。
- en: We will likely see many more prompt engineering and prompt management approaches
    being developed in the coming years. In addition to this, we can even apply **machine
    learning** (**ML**) techniques to find or suggest the best prompts for our use
    cases, either by asking an LLM to suggest the best prompts or by using traditional
    ML optimization approaches such as classification and regression to find or suggest
    prompts that will likely lead to the best outcome for a given use case. Expect
    to continue seeing interesting approaches emerging in this field.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会在未来几年看到更多提示工程和提示管理方法的开发。除此之外，我们甚至可以将**机器学习**（**ML**）技术应用于寻找或建议最适合我们用例的最佳提示，无论是通过让LLM建议最佳提示，还是通过使用传统的ML优化方法，如分类和回归，来寻找或建议可能导致给定用例最佳结果的提示。预计在这个领域将继续出现有趣的方法。
- en: Going beyond prompt engineering and management, the next section describes larger-scale
    LLM tuning techniques, which can mainly be categorized under the umbrella of **transfer**
    **learning** (**TL**).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 超越提示工程和管理，下一节将描述更大规模的LLM调优技术，这些技术主要可以归类在**迁移学习**（**TL**）的范畴下。
- en: TL
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TL
- en: In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), I mentioned at a high level
    that LLMs usually go through an unsupervised pre-training phase followed by a
    supervised tuning phase. This section describes in more detail some of the supervised
    training techniques that are used to fine-tune LLMs. Let’s begin with a definition
    of TL.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第15章*](B18143_15.xhtml#_idTextAnchor371)中，我简要提到LLMs通常经过一个无监督预训练阶段，随后是一个监督调优阶段。本节将更详细地描述一些用于微调LLMs的监督训练技术。让我们从TL的定义开始。
- en: Definition
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: TL is an ML approach that uses a model that has already been trained on a certain
    task (or set of tasks) as a starting point for a new model that will perform a
    similar task (or set of tasks) with different but somewhat related parameters
    or data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: TL是一种机器学习方法，它使用一个已经在某个任务（或一系列任务）上训练好的模型作为新模型的起点，这个新模型将执行类似任务（或一系列任务），但参数或数据不同但有一定关联。
- en: An example of TL would be to take a model that was pre-trained on generic image
    recognition tasks and then fine-tune it to identify objects that are specifically
    relevant to driving scenarios by incrementally training it on datasets containing
    road scenes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TL的一个例子是，取一个在通用图像识别任务上预训练的模型，然后通过逐步在包含道路场景的数据集上训练，将其微调以识别与驾驶场景特别相关的对象。
- en: TL approaches can be seen as a kind of spectrum, where some TL techniques and
    use cases only require updating a small portion of the model weights, while others
    involve much more extensive updates. Different points on this spectrum represent
    a trade-off between customizability and computational expense. For example, updating
    a lot of model weights provides more customizability but is more computationally
    expensive, while updating a small number of model weights offers less customizability
    but is also less computationally expensive.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: TL方法可以被看作是一种光谱，其中一些TL技术和用例只需要更新模型权重的一小部分，而其他则涉及更广泛的更新。光谱上的不同点代表了定制性和计算成本之间的权衡。例如，更新大量模型权重提供了更多的定制性，但计算成本更高，而更新少量模型权重提供了较少的定制性，但计算成本也更低。
- en: Let’s begin our discussion at one extreme of the spectrum, in which we will
    update all or a large portion of the original model’s weights, which we refer
    to as **full fine-tuning**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从光谱的一端开始讨论，在这一端我们将更新原始模型的所有或大部分权重，我们称之为**完全微调**。
- en: Full fine-tuning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全微调
- en: In the case of fully fine-tuning LLMs, we could begin with a model that has
    been pre-trained on an enormous body of data and has learned a broad understanding
    of the concepts on which it was trained. We then introduce the model to a new
    dataset specific to the task at hand (for example, understanding the rules of
    the road). This dataset is usually smaller and more focused than the data used
    in the initial pre-training phase. During the fine-tuning process, the weights
    across all layers of the model are updated to minimize the loss in relation to
    our new task.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全微调LLMs的情况下，我们可以从一个在大量数据上预训练并学习了对训练概念广泛理解的模型开始。然后，我们将该模型引入一个针对当前任务特定的新数据集（例如，理解交通规则）。这个数据集通常比初始预训练阶段使用的数据集更小、更专注。在微调过程中，模型所有层的权重都会更新，以最小化与我们新任务相关的损失。
- en: A couple of things to bear in mind are that full fine-tuning can require a lot
    of computational resources, and there’s also a risk that the model might forget
    some of the useful representations it learned during pre-training (sometimes referred
    to as **catastrophic forgetting**), especially if the new task is considerably
    different from the pre-trained task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意，全微调可能需要大量的计算资源，而且还有风险，即模型可能会忘记它在预训练期间学习的一些有用的表示（有时被称为**灾难性遗忘**），尤其是如果新任务与预训练任务有相当大的不同。
- en: Remember that LLMs are large—in fact, they’re huge! With that in mind, many
    companies may find full fine-tuning infeasible due to the sheer amount of data,
    computational resources, and expense it often requires.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，LLMs很大——实际上，它们非常大！考虑到这一点，许多公司可能会因为数据量、计算资源以及通常所需的费用而发现全微调不可行。
- en: Rather than tuning all of the weights in an enormous LLM, research has found
    that sometimes, we can get effective improvements on specific tasks by only changing
    some weights. In such cases, we can “freeze” some of the model’s weights (or parameters)
    to ensure they do not get updated while allowing others to be updated. These approaches
    can be more efficient because they require fewer parameters to be updated and
    are therefore referred to as **parameter-efficient fine-tuning** (**PEFT**), which
    I will describe in more detail in the following subsections, starting with **adapter
    tuning**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是调整巨大LLM中的所有权重，研究已经发现，有时我们只需更改一些权重，就可以在某些特定任务上获得有效的改进。在这种情况下，我们可以“冻结”模型的一些权重（或参数），以确保它们在更新时不会改变，同时允许其他权重更新。这些方法可以更高效，因为它们需要更新的参数更少，因此被称为**参数高效微调**（PEFT），我将在以下小节中更详细地描述，从**适配器调整**开始。
- en: Adapter tuning
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 适配器调整
- en: In the case of adapter tuning, the original model layers remain unchanged, but
    we add **adapter layers** or **adapter modules**, which are small **neural networks**
    (**NNs**) inserted between the layers of a pre-trained model, consisting of just
    a few learnable parameters. As input data is fed into the model, it flows through
    the original pre-trained layers and the newly inserted adapter modules, and the
    adapters slightly alter the data processing flow, which introduces additional
    transformation steps in some of the layers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在适配器调整的情况下，原始模型层保持不变，但我们添加**适配器层**或**适配器模块**，这些是插入到预训练模型层之间的小型**神经网络**（NNs），仅包含几个可学习的参数。当输入数据被输入到模型中时，它通过原始预训练层和新插入的适配器模块流动，适配器略微改变了数据处理流程，这导致某些层中引入了额外的转换步骤。
- en: The loss calculation steps that are performed after the data passes through
    the network are the same steps we learned about in our chapter on NNs, and the
    calculated loss is used to compute gradients for the model parameters. However,
    unlike traditional full-model fine-tuning, the gradients are only applied to update
    the weights of the adapter modules, while the weights of the pre-trained model
    itself remain frozen and unchanged from their initial values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据通过网络后执行的计算损失步骤与我们关于NNs章节中学到的步骤相同，计算出的损失用于计算模型参数的梯度。然而，与传统全模型微调不同，梯度仅应用于更新适配器模块的权重，而预训练模型的权重本身保持冻结，并保持其初始值不变。
- en: Another popular PEFT technique is **low-rank adaptation** (**LoRA**), which
    I’ll describe next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的PEFT技术是**低秩调整**（LoRA），我将在下面进行描述。
- en: LoRA
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA
- en: LoRA is based on the premise that not all parameters in a NN are equally important
    for transferring learned knowledge to a new task, and that, by identifying and
    changing only a small subset of the model’s parameters, it’s possible to adapt
    the model for a specific task without completely retraining the model. Instead
    of modifying the original weight matrices directly, LoRA creates low-rank matrices
    that can be thought of as a simplified or compressed version of the full matrix
    that captures the most important properties with fewer parameters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA基于这样一个前提，即一个神经网络中的所有参数在将学习到的知识转移到新任务时并不同等重要，并且通过仅识别和改变模型参数的小子集，可以适应特定任务而无需完全重新训练模型。LoRA不是直接修改原始权重矩阵，而是创建低秩矩阵，可以将其视为全矩阵的简化或压缩版本，它以更少的参数捕获最重要的属性。
- en: This is what makes LoRA more parameter-efficient. As with adapter tuning, during
    backpropagation, only the parameters of the low-rank matrices are updated, and
    the original model parameters remain unchanged. Since the low-rank matrices have
    fewer parameters than the full matrices they represent, the training and updating
    process can be much more efficient.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LoRA更参数高效的原因。与适配器微调类似，在反向传播过程中，只有低秩矩阵的参数被更新，原始模型参数保持不变。由于低秩矩阵比它们所代表的完整矩阵具有更少的参数，因此训练和更新过程可以更加高效。
- en: In addition to improving a model’s performance on specific tasks, there are
    also more subtle performance improvement practices that are just as important,
    such as aligning with human values and expectations. Let’s explore that concept
    in more detail next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提高模型在特定任务上的性能外，还有一些更微妙但同样重要的性能改进实践，例如与人类价值观和期望保持一致。让我们接下来更详细地探讨这个概念。
- en: Aligning with human values and expectations
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与人类价值观和期望保持一致
- en: Have you ever met somebody who is highly intelligent and talented, but has poor
    communication skills? Such a person might perform amazingly when it comes to troubleshooting
    and fixing technology issues, for example, but they may not be a suitable person
    to lead a customer meeting. They may say things that could be considered rude
    or inappropriate, or perhaps just a bit odd, due to their poor communication skills.
    This is the analogy I like to use to explain the concept of tuning GenAI models
    to align with human values and expectations because such values and expectations
    can often be more subtle than scientific and, therefore, require tailored approaches.
    For example, in addition to a model’s outputs being accurate, human expectations
    could require the model’s outputs to be friendly, safe, and unbiased, among other
    subtle, nuanced qualities. I’ll refer to this concept from here onward as **alignment**,
    and in this section I’ll describe two approaches that are commonly used for human
    value alignment today, starting with **reinforcement learning from human** **feedback**
    (**RLHF**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有遇到过一些非常聪明和有才华的人，但沟通技巧却很差？例如，这样的人在解决和修复技术问题时可能表现得非常出色，但他们可能不适合主持客户会议。他们可能会说出一些可能被认为是不礼貌或不恰当的话，或者可能只是因为沟通技巧差而显得有些奇怪。这就是我喜欢用来解释调整GenAI模型以与人类价值观和期望保持一致的概念的类比，因为这样的价值观和期望往往比科学更微妙，因此需要定制的方法。例如，除了模型的输出要准确外，人类的期望可能还要求模型的输出要友好、安全、无偏见，以及其他微妙、细腻的品质。从现在起，我将把这个概念称为**一致性**，在本节中，我将描述两种今天常用于人类价值观一致性的方法，首先是**从人类反馈中进行强化学习**（**RLHF**）。
- en: RLHF
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RLHF
- en: We explored the concept of **reinforcement learning** (**RL**) in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015) of this book, and I explained that in RL,
    the model learns from rewards or penalties based on interactions with its environment
    as it pursues a specific goal. RLHF is one of those examples in which the name
    of the technology is highly descriptive and accurately captures what the technology
    entails. As the name suggests, RLHF is an extension of RL, in which feedback to
    the model is provided by humans.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的[*第一章*](B18143_01.xhtml#_idTextAnchor015)中探讨了**强化学习**（**RL**）的概念，并解释说，在RL中，模型在追求特定目标的过程中，通过与环境的交互从奖励或惩罚中学习。RLHF是那些技术名称高度描述性且准确捕捉技术内涵的例子之一。正如其名所示，RLHF是RL的扩展，其中模型通过人类提供反馈。
- en: In the case of RLHF, we start with an LLM that has already been pre-trained
    on a large dataset. The model generates multiple possible responses to a prompt,
    and humans evaluate the responses based on various preferences. The human feedback
    data is then used to train a separate model called the **reward model**, which
    learns to predict the kinds of responses humans are likely to prefer, based on
    the feedback given. This process is intended to capture human preferences in a
    quantifiable way.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF的情况下，我们从一个已经在大量数据集上预训练的LLM开始。该模型会对一个提示生成多个可能的响应，人类根据各种偏好评估这些响应。然后，人类反馈数据被用来训练一个名为**奖励模型**的独立模型，该模型学习根据反馈预测人类可能更喜欢的响应类型。这个过程旨在以可衡量的方式捕捉人类偏好。
- en: The LLM then uses this feedback (like a reward signal) to update its parameters
    through RL techniques, making it more likely to generate responses that humans
    find desirable in the future.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: LLM随后使用这种反馈（如奖励信号）通过RL技术更新其参数，使其更有可能在将来生成人类认为理想的响应。
- en: Another technique that can be used to align with human values and expectations
    is **Direct Preference Optimization** (**DPO**). Let’s discuss that next.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以用来与人类价值观和期望保持一致的技术是**直接偏好优化**（**DPO**）。让我们接下来讨论这个话题。
- en: DPO
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DPO
- en: DPO also uses human feedback to improve a model’s performance in aligning with
    human values and expectations. In the case of DPO, again, the model may provide
    multiple responses to a prompt, and a human can pick which response they prefer
    (similar to RLHF). However, DPO does not involve training a separate reward model.
    Instead, it uses pairwise comparisons as the optimization signal, and it directly
    optimizes policies based on user preferences rather than predefined reward functions,
    which is especially useful in cases where it’s difficult to define an explicit
    reward function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DPO也使用人类反馈来提高模型在符合人类价值观和期望方面的性能。在DPO的情况下，模型可能会对一个提示提供多个响应，而人类可以选择他们更喜欢的响应（类似于RLHF）。然而，DPO不涉及训练一个单独的奖励模型。相反，它使用成对比较作为优化信号，并直接根据用户偏好优化策略，而不是预定义的奖励函数，这在难以定义明确的奖励函数的情况下特别有用。
- en: It’s important to note that, while RLHF and DPO are valuable and important techniques,
    there are some challenges that human interaction inherently brings into scope.
    For example, any process that requires human interaction can be difficult to scale.
    This means that it can be difficult to gather large amounts of data via feedback
    from humans. Also, humans can make mistakes or introduce conscious or unconscious
    bias into the results. These are some factors you would need to monitor and mitigate
    if you implement an RLHF or DPO solution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然强化学习与人类反馈（RLHF）和直接偏好优化（DPO）是很有价值和重要的技术，但人类交互本身会带来一些挑战。例如，任何需要人类交互的过程都可能难以扩展。这意味着通过人类的反馈收集大量数据可能会很困难。此外，人类可能会犯错误，或者将主观或无意识的偏见引入结果中。如果你实施RLHF或DPO解决方案，这些是一些你需要监控和缓解的因素。
- en: Other LLM tuning techniques continue to emerge in addition to the tuning approaches
    I’ve covered here, and a lot of research is being invested in this field. This
    is another space in which you can expect to continue seeing groundbreaking leaps
    forward in development.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我在这里介绍的对齐方法之外，还有其他大型语言模型（LLM）调优技术正在出现，并且在这个领域投入了大量的研究。这又是一个你可以期待继续看到在开发方面取得突破性进展的空间。
- en: Next, let’s dive deeper into the role of embeddings and vector databases in
    GenAI.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更深入地探讨嵌入和向量数据库在生成人工智能（GenAI）中的作用。
- en: Embeddings and vector databases
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入和向量数据库
- en: In [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371), we discussed the importance
    of embeddings and latent spaces, and I explained how they can be created in different
    ways. One way is when a generative model learns them intrinsically during its
    training process, and another is when we use specific types of models to create
    them explicitly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第15章*](B18143_15.xhtml#_idTextAnchor371)中，我们讨论了嵌入和潜在空间的重要性，并解释了它们可以通过不同的方式创建。一种方式是当生成模型在其训练过程中内在地学习它们时，另一种方式是我们使用特定类型的模型来明确地创建它们。
- en: I also touched on why we would want to explicitly create them since they can
    be processed more efficiently and are a more suitable format for ML use cases.
    In this context, when we create an embedding for something, we are simply creating
    a vector of numeric values to represent it (how we actually do that is a more
    advanced topic we will cover shortly).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我还简要提到了我们为什么要明确创建它们，因为它们可以更有效地处理，并且更适合用于机器学习用例的格式。在这种情况下，当我们为某物创建嵌入时，我们只是在创建一个代表它的数值向量（我们实际上是如何做到这一点的是一个更高级的话题，我们将在稍后讨论）。
- en: Another concept I briefly touched upon was the importance of relationships between
    embeddings in the vector space. For example, the proximity of embeddings to each
    other in the vector space can reflect the similarity between the concepts they
    represent. Let’s examine this relationship in more detail.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我还简要提到了嵌入在向量空间中关系的重要性。例如，嵌入在向量空间中的邻近性可以反映它们所代表的概念之间的相似性。让我们更详细地考察这种关系。
- en: Embeddings and similarity of concepts
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念的嵌入和相似性
- en: A well-constructed embedding contains everything needed to describe and explain
    the concept it represents—that is, the “meaning” of the concept. That may seem
    somewhat abstract to us because we don’t often think of everything that goes into
    the meaning of something. For example, when I utter the word “car,” a certain
    image might appear in your mind, and a lot of other information is immediately
    contextualized by that word. You know that you can drive a car; you know that
    they are usually relatively expensive; you know that they have wheels and windows,
    and they’re traditionally made from some kind of metal. You know lots of pieces
    of information about the concept of a car, and this helps you to understand what
    it is. Imagine that all of the information required to conjure up the idea of
    a car is stored as a vector in your mind. Now, imagine that you have never heard
    of a truck before, and I suddenly show you a picture of a truck. It has wheels
    and windows; it’s made from steel; it looks like something you could possibly
    drive. Although you’ve never seen anything like it before, you can understand
    that this new object is similar to a car. That’s because the pieces of information
    (that is, the vector of information) associated with it are very similar to that
    of a car.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个构建良好的嵌入包含描述和解释其所代表概念所需的一切——即，该概念的“意义”。这在我们看来可能有些抽象，因为我们不经常思考构成某物意义的一切。例如，当我提到“汽车”这个词时，你脑海中可能会浮现出一个特定的形象，并且许多其他信息也会立即由这个词所关联。你知道你可以驾驶汽车；你知道它们通常相对昂贵；你知道它们有轮子和窗户，而且它们传统上是由某种金属制成的。你对汽车这一概念的了解有很多，这有助于你理解它是什么。想象一下，所有构成汽车这一想法的信息都存储在你心中的一个向量中。现在，想象一下你以前从未听说过卡车，我突然给你展示一张卡车的图片。它有轮子和窗户；它是由钢制成的；它看起来像你可以驾驶的东西。尽管你以前从未见过类似的东西，但你能够理解这个新物体与汽车相似。这是因为与之相关的信息片段（即信息向量）与汽车非常相似。
- en: How do we make these kinds of associations? It feels somewhat intuitive, and
    most lay people (including myself) don’t understand how this all really happens
    in our brains. However, in the case of embeddings, it’s a lot easier to understand
    because our good old friend, mathematics (much loved by computers and ML models),
    comes to the rescue. As I mentioned already, we can mathematically compare different
    vectors by calculating the distances between them in their vector space, using
    well-established distance metrics such as Euclidean distance or cosine similarity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何建立这些类型的关联？这感觉有些直观，而且大多数普通人（包括我自己）都不理解这一切在我们大脑中是如何真正发生的。然而，在嵌入的情况下，理解要容易得多，因为我们的老朋友数学（计算机和机器学习模型都非常喜欢）来帮忙了。正如我之前提到的，我们可以通过在向量空间中计算它们之间的距离来数学地比较不同的向量，使用诸如欧几里得距离或余弦相似度等已建立的距离度量。
- en: 'If concepts are close in the vector space, they are likely close in meaning,
    in which case, we can say that they are **semantically similar**. For example,
    in a text-based vector database, the phrases “The cat sat on the mat” and “The
    feline rested on the rug” would have very similar vector embeddings, even though
    their exact words differ. A query for one of these vectors is likely to also identify
    the other one as a highly relevant result. A classic example of this concept is
    the representation of the words “man,” “woman,” “king,” and “queen,” as represented
    in *Figure 16**.1*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果概念在向量空间中彼此接近，它们在意义上也可能接近，在这种情况下，我们可以说它们是**语义相似的**。例如，在一个基于文本的向量数据库中，“The cat
    sat on the mat”和“The feline rested on the rug”这两个短语会有非常相似的向量嵌入，尽管它们的精确单词不同。对这些向量之一进行查询很可能也会将另一个识别为高度相关的结果。这个概念的一个经典例子是“man”、“woman”、“king”和“queen”这些词的表示，如*图16.1*所示：
- en: '![Figure 16.1: Embeddings and semantic similarity](img/B18143_16_1.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图16.1：嵌入和语义相似性](img/B18143_16_1.jpg)'
- en: 'Figure 16.1: Embeddings and semantic similarity'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：嵌入和语义相似性
- en: 'As depicted in *Figure 16**.1*, depending on how embeddings of items are captured
    in a vector space, we may see somewhat consistent projections in the vector representations
    and distances between vectors. The example shows that the concept of binary gender
    (or some kind of consistent semantic relationship) is captured in the words “man,”
    “woman,” “king,” and “queen,” and moreover, the distance and direction from “man”
    to “woman” is similar to that as from “king” to “queen.” Additionally, the distance
    and direction from “man” to “king” is similar to that from “woman” to “queen.”
    In this scenario, hypothetically, you could infer the value “queen” by performing
    the following mathematical operation in the vector space:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图16**.1*所示，根据项目嵌入在向量空间中的方式，我们可能在向量表示和向量之间的距离中看到某种一致性投影。示例显示，二元性别（或某种一致的语义关系）在“男人”、“女人”、“国王”和“王后”这些词中得到了体现，而且从“男人”到“女人”的距离和方向与从“国王”到“王后”的距离和方向相似。此外，从“男人”到“国王”的距离和方向与从“女人”到“王后”的距离和方向相似。在这种情况下，假设你可以在向量空间中执行以下数学运算来推断值“王后”：
- en: king – man + woman = queen
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 国王 – 男人 + 女人 = 王后
- en: That is, if you take the concept of a king, subtract the male gender, and add
    the female gender, you end up with the concept of a queen.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果你将“国王”的概念减去男性性别，并添加女性性别，你最终得到的是“王后”的概念。
- en: 'We can also apply this approach more broadly. For example, in a vector database
    storing image embeddings, a picture of a tangerine may lie in close proximity
    to a picture of a clementine. These objects have similar properties, such as size,
    shape, and color, and a vector representing a banana would likely not be as close
    to them as they are to each other because a banana does not share as many similarities.
    However, from a multimodal perspective, which we will describe later in this chapter,
    considering that they are all types of fruit, the banana vector may still be closer
    to them than a vector representing a car or an umbrella, for example. This concept
    is depicted in a simplified manner in *Figure 16**.2*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更广泛地应用这种方法。例如，在一个存储图像嵌入的向量数据库中，一个柚子的图片可能靠近一个金桔的图片。这些物体具有相似的性质，如大小、形状和颜色，而代表香蕉的向量可能不会像它们彼此之间那样接近，因为香蕉与它们共享的相似性较少。然而，从多模态的角度来看，我们将在本章后面描述，考虑到它们都是水果类型，香蕉向量可能仍然比代表汽车或雨伞的向量更接近它们。这个概念在*图16**.2*中以简化的方式表示：
- en: '![Figure 16.2: Elaborated example of embeddings and semantic similarity](img/B18143_16_2.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2：嵌入和语义相似性的详细示例](img/B18143_16_2.jpg)'
- en: 'Figure 16.2: Elaborated example of embeddings and semantic similarity'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：嵌入和语义相似性的详细示例
- en: Note that, in reality, the embedding space consists of many abstract dimensions
    and is not usually visualizable or directly interpretable to humans. Embeddings
    can be seen as a way to translate the messy complexity of the real world into
    a numerical language that ML models can understand.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在现实中，嵌入空间由许多抽象维度组成，通常无法可视化或直接对人类进行解释。嵌入可以被视为将现实世界的混乱复杂性转化为机器学习模型可以理解的数值语言的一种方式。
- en: If we create embeddings, we might want somewhere to store them and easily find
    them when needed. This is where vector databases come into the picture. Let’s
    dive into this in more detail and explain what vector databases are.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建嵌入，我们可能希望有一个地方来存储它们，并在需要时轻松找到它们。这就是向量数据库发挥作用的地方。让我们更详细地探讨这个问题，并解释什么是向量数据库。
- en: Vector databases
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库
- en: Vector databases are specialized databases designed to store and manage vector
    embeddings. Unlike traditional databases that focus on exact matches (for example,
    finding a customer with a specific ID), vector databases are more suitable for
    use cases such as **similarity search**, in which they use distance metrics to
    determine how close and, therefore, how similar two vector embeddings are in the
    latent space they occupy.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是专门设计用于存储和管理向量嵌入的数据库。与关注精确匹配的传统数据库（例如，查找具有特定ID的客户）不同，向量数据库更适合用于**相似性搜索**等用例，它们使用距离度量来确定在潜在空间中两个向量嵌入的接近程度和相似程度。
- en: This approach significantly changes how we can retrieve and analyze data. While
    traditional methods rely on keywords or predefined attributes, vector databases
    allow us to perform semantic search, in which we can query based on meaning, and
    this enables new applications. If I search for “trousers,” for example, the responses
    can also include jeans, slacks, and chinos because these are all semantically
    similar concepts. Most modern search engines, such as Google, for example, use
    sophisticated mechanisms such as semantic search (and a combination of other techniques),
    rather than simple keyword matching.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显著改变了我们检索和分析数据的方式。虽然传统方法依赖于关键词或预定义的属性，但向量数据库允许我们执行语义搜索，其中我们可以根据意义进行查询，这开辟了新的应用。例如，如果我搜索“裤子”，响应也可以包括牛仔裤、休闲裤和卡其裤，因为这些都是在语义上相似的概念。大多数现代搜索引擎，例如谷歌，例如，使用复杂的机制，如语义搜索（以及其他技术的组合），而不是简单的关键词匹配。
- en: Semantic search provides a much better customer experience, as well as a potential
    increase in revenue for the company implementing it. If you run a retail website,
    for example, bear in mind that customers will not buy a product if they cannot
    find it, and their business may go to your competitors instead. By implementing
    semantic search, we can understand the meaning and the intent of the user and,
    therefore, present the most relevant products to them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索为用户提供更好的客户体验，同时也可能为公司带来潜在的收入增长。例如，如果你经营一个零售网站，请记住，如果客户找不到产品，他们就不会购买，他们的业务可能会转向你的竞争对手。通过实施语义搜索，我们可以理解用户的含义和意图，因此可以向他们展示最相关的产品。
- en: These new search capabilities are also important in the context of GenAI for
    reasons I will explain shortly. First, however, let’s dive into more detail on
    how vector databases work.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新的搜索能力在GenAI的背景下也非常重要，原因我将在稍后解释。首先，让我们更深入地探讨向量数据库是如何工作的。
- en: How vector databases work
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库是如何工作的
- en: As a precursor to explaining how vector databases work, let’s briefly talk about
    how databases work in general. The main purpose of any database is to store and
    organize data in a way that makes it easy to find as quickly as possible. Most
    databases make use of indexes for this purpose, which I’ll describe next.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释向量数据库是如何工作的之前，让我们简要地谈谈数据库通常是如何工作的。任何数据库的主要目的是以使其尽可能快速找到的方式存储和组织数据。大多数数据库为了这个目的使用索引，我将在下面描述。
- en: Indexing and neighbor search
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引和邻近搜索
- en: To understand the role of indexes, imagine I handed you a book and asked you
    to find information in the book about “anthropomorphism” as quickly as possible.
    Without any mechanisms to assist you, you would have to read through every word
    on every page until you found that word. This, of course, would be a very inefficient
    process, and it would likely take you a long time to find the information unless
    it happened to appear near the beginning of the book. This is referred to as a
    **full table scan** in the database world, which we usually want to avoid, if
    possible. This is where indexes come into play—if the book I handed you contains
    an index, you could look in the index to see which pages contain references to
    “anthropomorphism” and then go directly to those pages, cutting out a lot of inefficient
    searching.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解索引的作用，想象我给你一本书，并要求你尽可能快地在这本书中找到关于“拟人化”的信息。如果没有任何辅助机制，你就必须逐页阅读每个单词，直到找到那个词。这当然是一个非常低效的过程，除非信息恰好出现在书的开始附近，否则你很可能需要很长时间才能找到信息。这在数据库世界中被称为**全表扫描**，我们通常希望尽可能避免这种情况。这就是索引发挥作用的地方——如果我所给你的书包含索引，你可以在索引中查找包含“拟人化”引用的哪些页面，然后直接前往这些页面，从而避免大量的低效搜索。
- en: Relational databases typically use tree-like (for example, B-trees) or hashing-based
    indexes for rapid lookups, but vector database indexes can be more complex due
    to the complexities of the embedding space and how each vector is represented
    in potentially high-dimensional space. Such indexes can resemble graph-like structures
    that map relationships between embeddings to enable proximity-based searches.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库通常使用树形（例如，B树）或基于哈希的索引进行快速查找，但向量数据库的索引可能更复杂，因为嵌入空间和每个向量在可能的高维空间中的表示都很复杂。这样的索引可以类似于图结构，将嵌入之间的关系映射出来，以实现基于邻近度的搜索。
- en: The equivalent of a full table scan in the vector database world is called a
    **brute-force** search, which borrows its name from cybersecurity and relates
    to the practice of trying every possible combination of inputs to find the desired
    result. This is a common way in which bad actors try to guess a person’s password—they
    try every possible combination of characters (within certain parameters). A longer
    password makes it difficult for them to guess it via brute force because each
    additional character exponentially increases the number of potential combinations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量数据库世界中，与全表扫描相当的操作被称为**暴力搜索**，这个名字借用了网络安全领域的术语，并涉及到尝试所有可能的输入组合以找到期望结果的做法。这是恶意行为者尝试猜测某人密码的常见方式——他们在一定参数内尝试所有可能的字符组合。密码越长，他们通过暴力猜测就越困难，因为每个额外的字符都会指数级增加可能的组合数量。
- en: In the case of vector databases, our queries generally try to find items near
    the queried item in the vector space. If we have a lot of vectors in the database,
    then brute force would be impractical, especially in high-dimensional spaces where
    the **curse of dimensionality** (**CoD**) makes exact searches computationally
    expensive. Fortunately, brute force is usually unnecessary because it seeks to
    precisely find the vector that is closest to the vector being queried (referred
    to as the **exact nearest neighbor**), but we don’t always need the exact nearest
    neighbor. For example, use cases such as recommendation engines, which recommend
    items that are similar to a particular item, generally just need to find vectors
    that are in the close vicinity of the vector being queried, but it doesn’t have
    to be the exact nearest neighbor of the vector being queried. This is referred
    to as an **approximate nearest neighbor** (**ANN**) search, and it’s extremely
    useful in the context of vector databases because it allows a trade-off between
    the accuracy of the search results and the speed of the query – that is, for these
    kinds of use cases, it’s better to quickly get results that are close enough to
    the queried vector rather than spending a lot of time finding the exact best result.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量数据库的情况下，我们的查询通常试图在向量空间中找到与查询项附近的项。如果我们数据库中有大量的向量，那么暴力搜索将是不切实际的，尤其是在高维空间中，**维度诅咒**（**CoD**）使得精确搜索的计算成本很高。幸运的是，通常不需要暴力搜索，因为它寻求精确地找到与查询向量最接近的向量（称为**精确最近邻**），但我们并不总是需要精确的最近邻。例如，像推荐引擎这样的用例，它推荐与特定项目相似的项，通常只需要找到与查询向量在附近区域的向量，但不必是查询向量的精确最近邻。这被称为**近似最近邻**（**ANN**）搜索，在向量数据库的上下文中非常有用，因为它允许在搜索结果的准确性和查询速度之间进行权衡——也就是说，对于这类用例，快速得到足够接近查询向量的结果比花费大量时间找到最佳结果更好。
- en: Note – the CoD and vector proximity
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 - CoD和向量邻近度
- en: In high-dimensional spaces, points become increasingly spread out, and as the
    number of dimensions increases, the distance between the nearest and farthest
    points becomes diluted, which makes it harder to distinguish between truly similar
    and dissimilar items based on distance metrics.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中，点变得越来越分散，随着维数的增加，最近点和最远点之间的距离变得稀释，这使得基于距离度量的真正相似和不同项的区分变得更加困难。
- en: Some vector databases implement hybrid approaches that combine ANN with other
    indexing or search strategies, such as hierarchical indexing structures, multi-index
    hashing, and tree-based methods to improve accuracy, reduce search latency, or
    optimize resource usage. Graph-based methods such as **Navigable Small World**
    (**NSW**) graphs, **Hierarchical Navigable Small World** (**HNSW**) graphs, and
    others create a graph where nodes represent vectors, and queries then traverse
    the graph to find the nearest neighbors. Some vector databases also use partitioning
    or clustering algorithms to divide the dataset into smaller, more manageable chunks
    or clusters, and indexing can then be performed within these partitions or clusters,
    often using a combination of methods to first identify the relevant partition
    and then perform an ANN search within it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一些向量数据库实现了混合方法，将 ANN 与其他索引或搜索策略相结合，例如分层索引结构、多索引哈希和基于树的方法，以提高准确性、减少搜索延迟或优化资源使用。基于图的方法，如
    **可导航小世界**（**NSW**）图、**分层可导航小世界**（**HNSW**）图等，创建了一个图，其中节点代表向量，查询随后遍历图以找到最近的邻居。一些向量数据库还使用分区或聚类算法将数据集划分为更小、更易于管理的块或簇，然后在这些分区或簇内进行索引，通常使用多种方法首先识别相关的分区，然后在其中执行
    ANN 搜索。
- en: Also, we can use approaches such as product quantization or scalar quantization
    to compress vectors by mapping them to a finite set of reference vectors, which
    reduces the dimensionality and storage requirements of the vectors before indexing
    and enables faster searches by approximating distances in the compressed space.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用诸如乘积量化或标量量化等方法，通过将向量映射到有限个参考向量集来压缩向量，这减少了向量在索引前的维度和存储需求，并通过在压缩空间中近似距离来加速搜索。
- en: In [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408), we’ll walk through the
    various vector database offerings in Google Cloud, but for now, let’s move on
    to discuss how we can create embeddings in the first place.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第17章*](B18143_17.xhtml#_idTextAnchor408) 中，我们将介绍 Google Cloud 中提供的各种向量数据库产品，但到目前为止，让我们继续讨论我们如何首先创建嵌入。
- en: Creating embeddings
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建嵌入
- en: We’ve already discussed **autoencoders** (**AEs**) in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    and we learned how they can be used to create latent space representations of
    their inputs. There are many more ways in which we can create embeddings, and
    I’ll describe some of the more popular methods in this section, starting with
    one of the most famous word embedding models, Word2Vec.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [*第15章*](B18143_15.xhtml#_idTextAnchor371) 中讨论了 **自编码器**（**AEs**），我们学习了它们如何被用来创建输入的潜在空间表示。我们可以以许多其他方式创建嵌入，我将在本节中描述一些更受欢迎的方法，从最著名的词嵌入模型
    Word2Vec 开始。
- en: Word2Vec
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Word2Vec (short for “word to vector”) is a group of algorithms invented by Google
    (Mikolov et al., 2013) to learn representations of words as vectors. It’s often
    considered to be the grandfather of word embedding approaches, and although it
    wasn’t the first word embedding technique to be invented, it promoted the idea
    of representing words as dense vectors that capture the semantic meaning and relationships
    between words in a high-dimensional space. It works by building a vocabulary of
    unique words and learning a vector representation for each word, where words with
    similar meanings have similar vectors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec（简称“词向量化”）是由谷歌（Mikolov等人，2013年）发明的一组算法，用于学习将单词表示为向量的表示。它通常被认为是词嵌入方法的鼻祖，尽管它不是第一个被发明的词嵌入技术，但它推广了将单词表示为密集向量，这些向量能够捕捉到高维空间中单词的语义意义和关系。它通过构建一个独特的单词词汇表，并为每个单词学习一个向量表示来实现，其中具有相似意义的单词具有相似的向量。
- en: The two main approaches used in Word2Vec are the **Continuous Bag of Words**
    (**CBOW**) model, which predicts a target word based on its surrounding words,
    and **skip-gram**, which predicts surrounding words based on a given target word.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 中使用的两种主要方法是 **连续词袋**（**CBOW**）模型，它根据周围的单词预测目标词，以及 **跳字模型**（**skip-gram**），它根据给定的目标词预测周围的单词。
- en: While Word2Vec has been a popular and useful approach, newer, transformer-based
    approaches have emerged that provide more advanced capabilities. Let’s take a
    look at those next.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Word2Vec 已经是一种流行且有用的方法，但基于新近的、基于转换器的技术已经出现，提供了更高级的功能。让我们接下来看看那些方法。
- en: Transformers
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换器
- en: We’ve already introduced transformers in previous chapters, and in this section,
    I will briefly describe transformer-based approaches to create embeddings. While
    there are many options to choose from, I’ll focus on the famous **Bidirectional
    Encoder Representations from Transformers** (**BERT**) and its derivatives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前的章节中介绍了变压器，在本节中，我将简要描述基于变压器的创建嵌入的方法。虽然有很多选项可以选择，但我将重点关注著名的**双向编码器表示从变压器**（**BERT**）及其衍生版本。
- en: BERT
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BERT
- en: Earlier in this book, I mentioned that, every now and then, there’s a significant
    step forward in AI/ML research, and it’s well established that the invention of
    the transformer architecture by Google in 2017 (Vaswani et al., 2017) was one
    of those significant leaps. BERT, which was invented by Google in 2018 (Devlin
    et al., 2018), was another significant step.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期，我提到，在AI/ML研究中，每隔一段时间就会有一个重大的进步，而谷歌在2017年（Vaswani等人，2017）发明变压器架构已被确立为那些重大飞跃之一。BERT是由谷歌在2018年（Devlin等人，2018）发明的，这是另一个重大的进步。
- en: As indicated in the name, BERT is based on the transformer architecture, and
    it is pre-trained on a large dataset of text and code. During its training, it
    learns to model complex patterns and relationships between words and understand
    nuances such as context, grammar, and how different parts of a sentence relate
    to each other.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，BERT基于变压器架构，并在大量文本和代码数据集上进行了预训练。在训练过程中，它学会了模拟单词之间的复杂模式和关系，并理解诸如上下文、语法以及句子不同部分之间如何相互关联等细微差别。
- en: The two main approaches used in BERT are **masked language modeling** (**MLM**),
    which predicts missing words within a sentence based on the surrounding context,
    and **next sentence prediction** (**NSP**), which tries to determine if two sentences
    are logically connected. These are examples of the pre-training phase described
    in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371). By combining these two approaches,
    BERT develops an understanding of language structure.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: BERT使用的两种主要方法是**掩码语言建模**（**MLM**），它根据周围上下文预测句子中的缺失单词，以及**下一句预测**（**NSP**），它试图确定两个句子是否在逻辑上相连。这些都是[*第15章*](B18143_15.xhtml#_idTextAnchor371)中描述的预训练阶段的例子。通过结合这两种方法，BERT发展了对语言结构的理解。
- en: At the heart of BERT are the transformer layers, which take the text we give
    it and produce word embeddings that capture meaning depending on the surrounding
    words, which is a major improvement over static embeddings such as Word2Vec.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的核心是变压器层，它接受我们给出的文本并产生词嵌入，这些嵌入根据周围的单词捕捉意义，这比静态嵌入（如Word2Vec）是一个重大的改进。
- en: BERT was such an important breakthrough that many variants have been created
    since its initial release, such as DistilBERT and ALBERT, which are smaller, distilled
    versions of BERT with fewer parameters (trading off some accuracy for computational
    efficiency), as well as domain-specific variants, such as SciBERT (trained on
    scientific publications) and FinBERT (fine-tuned for financial industry use cases).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是一个如此重要的突破，以至于自其最初发布以来已经创建了多种变体，如DistilBERT和ALBERT，它们是BERT的较小、精简版本，参数更少（以一些准确性换取计算效率），以及特定领域的变体，如SciBERT（在科学出版物上训练）和FinBERT（针对金融行业用例微调）。
- en: There are also more targeted models that build on top of transformer-based models
    such as BERT. For example, rather than focusing on individual words, **Sentence
    Transformers** use pooling strategies such as mean pooling or max pooling to create
    semantically meaningful embeddings of entire sentences.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些更针对特定领域的模型建立在基于变压器的模型之上，如BERT。例如，**句子变压器**不是关注单个单词，而是使用池化策略，如平均池化或最大池化，来创建整个句子的语义上有意义的嵌入。
- en: For image embeddings, while we’ve discussed **convolutional NNs** (**CNNs**)
    in previous chapters for use cases such as image classification, it’s important
    to note that CNNs can also be used in creating image embeddings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像嵌入，虽然我们在之前的章节中讨论了**卷积神经网络**（**CNNs**）在图像分类等用例中的应用，但需要注意的是，CNNs也可以用于创建图像嵌入。
- en: Now that we know about some options for creating embeddings, as well as options
    for storing embeddings in vector databases, let’s explore a relatively new pattern
    that is becoming increasingly popular in the industry and that combines these
    topics, referred to as RAG.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了创建嵌入的一些选项，以及将嵌入存储在向量数据库中的选项，让我们探索一个相对较新的模式，它在行业中越来越受欢迎，并且结合了这些主题，被称为RAG。
- en: RAG
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG
- en: While LLMs are clearly impressive and powerful technologies, they have some
    limitations. Firstly, LLMs are only as good as the data on which they were trained,
    and they can be expensive and time-consuming to train, so for most companies,
    it would not be feasible to retrain them with new information every day. For this
    reason, in many cases, updated versions of LLMs tend to be released every few
    months. This means that the information provided in their responses depends on
    their most recent training dataset. If you want to ask about something that happened
    yesterday, but the LLM was last updated a month ago, it simply will not have any
    information on that topic. This can be quite limiting in today’s fast-paced business
    world, where people and applications constantly need up-to-date knowledge at their
    fingertips.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LLMs（大型语言模型）无疑是令人印象深刻且强大的技术，但它们也有一些局限性。首先，LLMs 的表现取决于它们训练时所使用的数据，而且训练它们可能既昂贵又耗时，因此对于大多数公司来说，每天用新信息重新训练它们并不可行。因此，在许多情况下，LLMs
    的更新版本通常每隔几个月就会发布一次。这意味着它们提供的回答信息取决于它们最近的训练数据集。如果你想要询问昨天发生的事情，但 LLM 上次更新是一个月前，那么它将简单地没有关于这个主题的任何信息。这在当今快节奏的商业世界中可能相当受限，因为人们和应用程序需要随时掌握最新的知识。
- en: Secondly, unless you are one of the large corporations that have created the
    popular LLMs being used today, you most likely did not train the LLM yourself
    from scratch, so it has not been trained on your specific data. Imagine you are
    a retail company that wants to use an LLM to enable customers to find out about
    your products via a chat interface. Since the LLM was not trained specifically
    on your product catalog, it will not be familiar with the details of your products.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，除非你是那些创建了今天广泛使用的流行 LLM 的大型企业之一，否则你很可能没有从头开始自己训练 LLM，因此它没有针对你的特定数据进行训练。想象一下，你是一家零售公司，想使用
    LLM 通过聊天界面让客户了解你的产品。由于 LLM 没有针对你的产品目录进行专门训练，它将不熟悉你产品的细节。
- en: To combat these kinds of challenges, rather than relying solely on the data
    that was used to train an LLM, we can insert additional data into the context
    of the prompt being sent to the LLM. We already touched on this in the *Prompt
    engineering* section of this chapter, in which we provided additional information
    in the prompts in order to guide the LLM’s responses to more accurately match
    our required outputs. That approach works great for small amounts of data, but
    if we consider the aforementioned use case of getting the LLM to answer questions
    about a product catalog, we cannot include the entire product catalog in every
    prompt. This is where RAG can help.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们不仅可以依赖用于训练 LLM 的数据，还可以将额外的数据插入到发送给 LLM 的提示上下文中。我们已经在本章的 *提示工程* 部分提到了这一点，我们在提示中提供了额外的信息，以便引导
    LLM 的回答更准确地匹配我们所需的输出。这种方法对于少量数据非常有效，但如果考虑到上述 LLM 回答产品目录问题的用例，我们不可能在每次提示中都包含整个产品目录。这就是
    RAG（Retrieval-Augmented Generation）能发挥作用的地方。
- en: With RAG, we can *augment* the responses *generated* by LLMs by first *retrieving*
    relevant information from a data store and then including that information in
    the prompt being sent to the LLM. That data store can contain whatever information
    we want, such as the contents of our product catalog. Let’s take a look at this
    pattern in more detail.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RAG，我们可以在首先从数据存储中检索相关信息并将其包含在发送给 LLM 的提示中之后，*增强* LLM 生成的响应。这个数据存储可以包含我们想要的信息，例如我们产品目录的内容。让我们更详细地看看这个模式。
- en: How RAG works
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 的工作原理
- en: 'To set the context for explaining how RAG works, let’s level-set the discussion
    by reviewing how LLM interactions typically work without RAG. This is very straightforward
    and is depicted in *Figure 16**.3*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释 RAG 的工作原理，让我们通过回顾没有 RAG 的 LLM 交互通常是如何工作的来统一讨论的基准。这非常简单，如 *图 16.3* 所示：
- en: '![Figure 16.3: LLM prompt and response](img/B18143_16_3.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.3：LLM 提示和响应](img/B18143_16_3.jpg)'
- en: 'Figure 16.3: LLM prompt and response'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3：LLM 提示和响应
- en: 'As we can see, the typical LLM interaction simply consists of sending a prompt
    and getting a response. Note that LLMs can only understand numerical values, so
    behind the scenes, the textual prompt sent by the user is broken down into tokens,
    and the tokens are encoded into vectors that the LLM can understand. The reverse
    process is performed when sending the response to the user. Next, let’s look at
    how RAG changes the process, as depicted in *Figure 16**.4*:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，典型的LLM交互简单地说就是发送一个提示并得到一个响应。请注意，LLM只能理解数值，因此幕后，用户发送的文本提示被分解成标记，这些标记被编码成LLM可以理解的向量。当向用户发送响应时，执行反向过程。接下来，让我们看看RAG如何改变这个过程，如图16.4所示：
- en: '![Figure 16.4: RAG](img/B18143_16_4.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图16.4：RAG](img/B18143_16_4.jpg)'
- en: 'Figure 16.4: RAG'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：RAG
- en: '*Figure 16**.4* outlines how RAG works at a high level. The steps are as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4概述了RAG在高级别上是如何工作的。步骤如下：
- en: The user or client application sends a textual request or prompt.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户或客户端应用程序发送文本请求或提示。
- en: The contents of the prompt are converted to vectors (embeddings).
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提示内容转换为向量（嵌入）。
- en: During the “retrieval” part of the process, the prompt embeddings are used to
    find similar embeddings in the vector database. These embeddings represent the
    data that we want to use to augment the prompt (for example, embeddings that represent
    data from our product catalog).
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在过程的“检索”部分，使用提示嵌入在向量数据库中查找相似的嵌入。这些嵌入代表我们想要用来增强提示的数据（例如，代表我们产品目录数据的嵌入）。
- en: The embeddings from our vector database are combined with the embeddings of
    the user prompt and sent to the LLM.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的向量数据库中的嵌入与用户提示的嵌入相结合，并发送到LLM。
- en: The LLM uses the combined embeddings of the user prompt and the retrieved embeddings
    to augment its response, aiming to provide a response that is not only based on
    the data on which it was originally trained but that is also relevant in the context
    of the data retrieved from our vector database.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM使用用户提示和检索到的嵌入的组合来增强其响应，旨在提供不仅基于其最初训练的数据，而且在从我们的向量数据库检索到的数据上下文中也相关的响应。
- en: The response embeddings are decoded to provide a text response back to the user
    or client application.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将响应嵌入解码以向用户或客户端应用程序提供文本响应。
- en: We will implement RAG in a later chapter in this book. Next, let’s begin to
    explore the topic of multimodal models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后续章节中实现RAG。接下来，让我们开始探讨多模态模型的主题。
- en: Multimodal models
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态模型
- en: Given that LLMs, as the name suggests, focus on language, the primary modality
    for many of the popular LLMs you are likely interacting with at the time of writing
    this in early 2024 is likely to be text. However, the concept of GenAI also goes
    beyond text and expands into other modalities, such as pictures, sound, and video.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM（语言模型）顾名思义，专注于语言，因此在2024年初撰写本文时，你很可能交互的许多流行LLM的主要模态可能是文本。然而，GenAI的概念也超越了文本，扩展到其他模态，如图片、声音和视频。
- en: The term *multimodal* is becoming ever more prominent within the field of GenAI.
    In this section, we explore what that means, starting with a definition of “modality.”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: “多模态”一词在GenAI领域变得越来越突出。在本节中，我们将探讨这意味着什么，从“模态”的定义开始。
- en: Definition
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: 'The Oxford English Dictionary defines modality as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津高阶英汉双解大词典将模态定义为：
- en: '“*Those aspects of a thing which relate to its mode, or manner or state of
    being, as distinct from its substance or identity; the non-essential aspect or
    attributes of a concept or entity. Also: a particular quality or attribute denoting
    the mode or manner of being* *of something.*”'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: “*事物与它的模式、方式或状态有关的部分，与它的实质或身份不同；概念或实体的非本质方面或属性。也指表示某物存在模式或方式的特定品质或属性* *。”
- en: '*Oxford English Dictionary*, *s.v. “modality (n.), sense 1.a,”* *December*
    *2023*, [https://doi.org/10.1093/OED/1055677936](https://doi.org/10.1093/OED/1055677936).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: “*牛津高阶英汉双解大词典*，*s.v. “modality (n.), sense 1.a,”* *2023年12月*，[https://doi.org/10.1093/OED/1055677936](https://doi.org/10.1093/OED/1055677936)。”
- en: Considering this formal definition of modalities, unlike traditional models
    that focus on a single data type, multimodal models are designed to process information
    from multiple modalities or data types. This is another one of those important
    leaps forward in AI research because it opens up new use cases and applications
    for AI. Let’s dive into this in more detail.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种模态的正式定义，与只关注单一数据类型的传统模型不同，多模态模型旨在处理来自多个模态或数据类型的信息。这是AI研究中的另一个重要飞跃，因为它为AI开辟了新的用例和应用。让我们更详细地探讨这一点。
- en: Why multimodality matters
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么多模态很重要
- en: In the section on RAG in this chapter, I mentioned an example of a company using
    an LLM to help customers explore product details via a chat interface and how
    that would require interacting with data from the company’s product catalog. Diving
    into this example in more detail, consider that items in a product catalog can
    have many different types of information associated with them. This may consist
    of structured data with standardized fields such as product dimensions, color,
    price, and other attributes. It may be associated with semi-structured or unstructured
    data such as customer reviews as well as product images. Imagine how much of a
    more detailed understanding our model could gain regarding the items in our catalog
    if it could ingest and understand all of those data modalities. If the written
    product description was somehow missing details regarding the color of the product,
    the model could instead learn that information from pictures of the product. Also,
    in addition to written customer reviews, some websites allow customers to post
    videos in their reviews. The model could learn more information from those videos.
    Overall, this could provide a much richer end-user experience. This is just one
    example of the power of multimodality, and this concept becomes even more important
    when creating models that need to interact with the physical world, such as robots
    and self-driving vehicles.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章关于RAG的部分，我提到了一个公司使用LLM通过聊天界面帮助客户探索产品细节的例子，以及这需要与公司产品目录中的数据进行交互。更详细地探讨这个例子，考虑一下产品目录中的项目可以与许多不同类型的信息相关联。这可能包括具有标准化字段的结构化数据，如产品尺寸、颜色、价格和其他属性。它也可能与半结构化或非结构化数据相关联，如客户评论以及产品图片。想象一下，如果我们的模型能够摄取和理解所有这些数据模态，它将能够对我们的目录中的项目获得多么详细的理解。如果产品描述中缺少关于产品颜色的细节，模型可以从产品图片中学习这些信息。此外，除了书面客户评论外，一些网站允许客户在其评论中发布视频。模型可以从这些视频中学习更多信息。总的来说，这可以为最终用户提供更丰富的体验。这只是多模态力量的一例，当创建需要与物理世界交互的模型，如机器人和自动驾驶汽车时，这一概念变得更加重要。
- en: Multimodal implementations are also important in AI research because one of
    the main challenges in creating useful and powerful models is getting access to
    relevant data. Considering that there is a finite amount of text that has ever
    been created in the world, if text remained the only modality, it would eventually
    become a limiting factor in AI development. As we can all clearly see on various
    social media platforms and other websites, video, audio, and pictures have equaled
    or surpassed text as the primary modalities in user-generated content, and we
    can expect those trends to continue. We’ve all heard the expression, “A picture
    is worth a thousand words,” and this becomes ever more true (in addition to other
    modalities) when training and interacting with large generative models!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态实现对于AI研究也很重要，因为在创建有用且强大的模型时，一个主要挑战是获取相关数据。考虑到世界上已经创作了有限数量的文本，如果文本是唯一模态，它最终将成为AI发展的限制因素。正如我们都可以在各种社交媒体平台和其他网站上清楚地看到，视频、音频和图片已经等同于或超过了文本，成为用户生成内容中的主要模态，我们可以预期这些趋势会继续。我们都听说过这句话，“一图胜千言”，当训练和与大型生成模型交互时，这一点（以及其他模态）变得更加真实！
- en: Having expressed the importance and advantages of multimodal model implementations,
    I’ll balance the discussion by highlighting some of the related challenges next.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表达了多模态模型实现的重要性和优势后，我将通过强调一些相关挑战来平衡讨论。
- en: Multimodality challenges
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态挑战
- en: I’ll begin this section with a challenge that consistently emerges in many contexts
    that we’ve discussed throughout this book. Generally, creating more powerful models
    requires more computing resources. Multimodal approaches are no exception here,
    and training multimodal models often requires significant computational resources.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从本节开始，介绍在本书中讨论的许多情境中反复出现的一个挑战。一般来说，创建更强大的模型需要更多的计算资源。多模态方法也不例外，训练多模态模型通常需要大量的计算资源。
- en: Another challenge is the complexity—combining data from different modalities
    can be complex and require careful preprocessing, especially considering that
    the feature representations in different modalities usually have very different
    feature spaces, and some modalities have richer or more sparse data than others.
    For example, text is usually sequential, images are spatial, and audio and video
    have temporal properties, so aligning all of these for the model to build a unified
    understanding can be difficult.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是复杂性——结合来自不同模态的数据可能很复杂，需要仔细的预处理，特别是考虑到不同模态的特征表示通常具有非常不同的特征空间，一些模态的数据比其他模态更丰富或更稀疏。例如，文本通常是序列的，图像是空间的，而音频和视频具有时间属性，因此对于模型来说，将这些属性对齐以建立一个统一的理解可能很困难。
- en: In previous chapters, we looked at examples of data quality issues and the kinds
    of data preparation steps and pipelines required to address those issues. The
    tricky thing, however, is that data in different modalities has different kinds
    of potential quality issues, preparation steps, and pipelines that are required
    to get it ready for training models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了数据质量问题以及解决这些问题的数据准备步骤和管道。然而，棘手的是，不同模态的数据有不同的潜在质量问题、准备步骤和管道，这些是使数据准备好用于模型训练所必需的。
- en: Despite the challenges, multimodality is a rapidly developing space that will
    continue to get significant attention for the foreseeable future.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在挑战，但多模态是一个快速发展的领域，在可预见的未来将继续受到显著的关注。
- en: While this section focused mainly on training GenAI models (specifically, multimodal
    approaches), other parts of the model development life cycle also have unique
    approaches that must be factored in when dealing with GenAI models rather than
    traditional AI models. The next important topic to highlight is how we evaluate
    GenAI models and how this can differ in some cases from evaluating traditional
    AI models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节主要关注训练通用人工智能模型（特别是多模态方法），但在模型开发的生命周期中，其他部分也有独特的处理方法，当处理通用人工智能模型而不是传统人工智能模型时，必须考虑这些方法。接下来要强调的重要话题是如何评估通用人工智能模型，以及在某些情况下，这种评估可能与传统人工智能模型的评估有所不同。
- en: GenAI model evaluation
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用人工智能模型评估
- en: While evaluating traditional AI models usually involves measuring the model’s
    outputs against a ground truth dataset and calculating established, objective
    metrics such as accuracy, precision, recall, F1 score, **Mean Squared Error**
    (**MSE**), and others we’ve covered in this book, evaluating generative models
    is not always as straightforward.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然评估传统人工智能模型通常涉及将模型的输出与一个真实数据集进行比较，并计算已建立的、客观的指标，如准确率、精确率、召回率、F1分数、**均方误差**（**MSE**）以及本书中提到的其他指标，但评估生成模型并不总是那么直接。
- en: When evaluating a GenAI model, we might want to focus on different factors,
    such as the model’s ability to produce creative and human-like outputs while staying
    relevant to the task at hand. An extra challenge in this case is that evaluating
    these properties can be somewhat vague and subjective. For example, if I ask a
    generative model to write a poem or create a photorealistic picture of a cat in
    a meadow, the quality of that poem or the realness of the picture is not always
    easy to represent with a mathematically calculated number, although some formulaic
    measurements do exist, which I will describe in this section. However, let’s first
    discuss human evaluation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估一个通用人工智能模型时，我们可能希望关注不同的因素，例如模型在保持与当前任务相关的同时，产生具有创造性和类似人类输出的能力。在这种情况下，一个额外的挑战是评估这些属性可能有些模糊和主观。例如，如果要求一个生成模型写一首诗或创建一幅草地上的猫的逼真图片，那么这首诗的质量或图片的真实性并不总是容易用数学计算出的数字来表示，尽管存在一些公式化的测量方法，我将在本节中描述。然而，让我们首先讨论人工评估。
- en: Human evaluation
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工评估
- en: Perhaps the most straightforward but also slow, cumbersome, and potentially
    expensive approach is for human evaluators to assess generated outputs for quality,
    creativity, coherence, and how well they align with the task requirements. There
    are tools, frameworks, services, and entire companies dedicated to performing
    these kinds of evaluations. Some common challenges with this approach are that
    the evaluations are only as good as the instructions provided to the evaluators,
    so it’s necessary to clearly understand and explain how the responses should be
    evaluated, and the evaluations can be subjective, based on the personal interpretations
    of the evaluator.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最直接但同时也缓慢、繁琐且可能昂贵的做法是让人类评估者评估生成输出的质量、创造力、连贯性以及它们与任务要求的匹配程度。有一些工具、框架、服务和整个公司致力于执行这类评估。这种方法的一些常见挑战是，评估的质量仅与提供给评估者的指示一样好，因此有必要清楚地理解和解释如何评估响应，并且评估可能是主观的，基于评估者的个人解释。
- en: Next, I’ll describe some of the formalized methods that can be used in evaluating
    generative models, starting with metrics for textual language models such as **BiLingual
    Evaluation Understudy** (**BLEU**) and **Recall-Oriented Understudy for Gisting**
    **Evaluation** (**ROUGE**).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将描述一些可以用于评估生成模型的正式化方法，从评估文本语言模型的指标开始，如**双向评估辅助者**（**BLEU**）和**基于回忆的摘要评估辅助者**（**ROUGE**）。
- en: BLEU
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLEU
- en: BLEU is used mainly for evaluating machine translation use cases, and it works
    by measuring how similar the generated text is to human-written text, so when
    we’re using BLEU, we need a set of reference, human-written text examples, which
    are compared against the generated text. More specifically, BLEU measures something
    called **n-gram overlap**, which is the overlap in sequences of words between
    the generated text and the reference text.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU主要用于评估机器翻译用例，它通过衡量生成文本与人工编写的文本的相似性来工作，因此当我们使用BLEU时，我们需要一组参考、人工编写的文本示例，这些示例与生成的文本进行比较。更具体地说，BLEU衡量的是所谓的**n-gram重叠**，即生成文本和参考文本之间单词序列的重叠。
- en: ROUGE
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROUGE
- en: ROUGE also measures the overlap between generated text and human-written reference
    examples, but it focuses on summarization use cases by measuring the **recall**,
    which represents how well a generated summary captures important information from
    the reference summaries.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE同样衡量生成文本与人工编写的参考示例之间的重叠，但它通过衡量**召回率**来专注于总结用例，召回率表示生成的摘要从参考摘要中捕获重要信息的程度。
- en: In addition to overlap-based evaluations, we can also evaluate text generation
    performance with metrics such as **perplexity** and **negative log-likelihood**
    (**NLL**), which can be used to evaluate a model’s performance in predicting sequences
    of words, such as predicting the next word in a sentence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于重叠的评估之外，我们还可以使用诸如**困惑度**和**负对数似然**（**NLL**）等指标来评估文本生成性能，这些指标可以用来评估模型在预测单词序列方面的性能，例如预测句子中的下一个单词。
- en: Next, let’s consider some approaches for evaluating image generation models,
    such as the **Inception Score** (**IS**) and **Fréchet Inception** **Distance**
    (**FID**).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们考虑一些评估图像生成模型的方法，例如**Inception Score**（**IS**）和**Fréchet Inception Distance**（**FID**）。
- en: IS
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IS
- en: IS is used to measure the quality and **diversity** of generated images, in
    which a good image generation model should be able to generate a wide variety
    of images, not just variations of the same thing, and the images should look clear
    and realistic. We can calculate the IS by using a pre-trained image classification
    model to classify images generated by a generative model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: IS用于衡量生成图像的质量和**多样性**，一个好的图像生成模型应该能够生成各种各样的图像，而不仅仅是同一事物的变体，并且图像应该看起来清晰、逼真。我们可以通过使用预训练的图像分类模型来对生成模型生成的图像进行分类，从而计算IS。
- en: FID
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FID
- en: While the IS evaluates generated images independently, FID was designed to improve
    on it by measuring how similar a distribution of generated images is to a distribution
    of real images (it gets its name from using the Fréchet distance to measure the
    distance between two distributions).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然IS独立评估生成图像，但FID旨在通过衡量生成图像的分布与真实图像分布的相似性来改进它（它得名于使用Fréchet距离来衡量两个分布之间的距离）。
- en: For the purpose of this chapter, we mainly need to be aware that specific metrics
    exist for evaluating some generative models, but going into the mathematical detail
    of how these metrics are calculated would be more information than is required
    at this point. There are additional approaches and metrics beyond the ones we’ve
    covered in this section, but this section describes some popular ones to consider.
    Next, I’ll introduce a somewhat different approach to evaluating generative models,
    which is to use an **auto-rater**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，我们主要需要意识到存在一些用于评估某些生成模型的特定指标，但深入探讨这些指标是如何计算的数学细节，在这个阶段将提供比所需更多的信息。除了我们在本节中介绍的方法和指标之外，还有其他的方法和指标。接下来，我将介绍一种评估生成模型的不同方法，即使用**自动评分器**。
- en: Auto-raters and side-by-side evaluations
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动评分器和并行评估
- en: At a high level, an auto-rater is an ML model that rates or evaluates the outputs
    of models. There are different ways in which this can be done, and I will explain
    the concept in the context of Google Cloud Vertex AI.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，自动评分器是一个机器学习模型，用于评分或评估模型的输出。这可以通过不同的方式进行，我将在Google Cloud Vertex AI的背景下解释这个概念。
- en: Google Cloud Vertex AI recently launched a service called **Automatic side-by-side**
    (**AutoSxS**), which can be used to evaluate pre-generated predictions or GenAI
    models in the Vertex AI Model Registry. This means that, in addition to Vertex
    AI foundation models, we can also use it to evaluate third-party language models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI最近推出了一项名为**自动并行**（**AutoSxS**）的服务，可用于评估Vertex AI模型注册表中的预生成预测或生成AI模型。这意味着，除了Vertex
    AI基础模型之外，我们还可以使用它来评估第三方语言模型。
- en: In order to compare the results of two models against each other, it uses an
    auto-rater to decide which model gives the better response to a prompt. The same
    prompt is sent to both models, and the auto-rater evaluates the responses from
    each model against various criteria, such as relevance, comprehensiveness, the
    model’s ability to follow instructions, and whether the responses are grounded
    in established facts.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较两个模型的结果，它使用自动评分器来决定哪个模型对提示给出了更好的响应。相同的提示被发送到两个模型，自动评分器评估每个模型的响应，并针对各种标准进行评估，例如相关性、全面性、模型遵循指令的能力以及响应是否基于既定事实。
- en: As I mentioned at the beginning of this section, evaluating generative models
    often requires some subjectivity and more flexibility than the simple objective
    evaluations of traditional ML models. The various criteria supported by Google
    Cloud Vertex AI AutoSxS provide such additional flexibility.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在本节开头所提到的，评估生成模型通常需要一定的主观性和比传统机器学习模型简单客观评估更多的灵活性。Google Cloud Vertex AI AutoSxS支持的各项标准提供了这种额外的灵活性。
- en: Now that we have a high-level understanding of how the evaluation of GenAI models
    differs in some ways from that of traditional ML models, let’s move on to introduce
    another important topic in GenAI—LangChain.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对生成AI模型的评估在某些方面与传统机器学习模型的评估有何不同有了高层次的理解，让我们继续介绍生成AI中的另一个重要主题——LangChain。
- en: LangChain
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain
- en: By sending a prompt to an LLM, we can achieve amazing feats and get useful information.
    However, we may want to build applications that implement more complex logic than
    we could achieve in a single prompt, and these applications may require interacting
    with multiple systems in addition to an LLM.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向LLM发送提示，我们可以实现惊人的成就并获得有用的信息。然而，我们可能希望构建比单个提示所能实现的更复杂的逻辑的应用程序，并且这些应用程序可能需要与LLM之外的多个系统进行交互。
- en: LangChain is a popular framework for developing applications using LLMs, and
    it enables us to combine multiple steps into a chain, in which each step implements
    some logic, such as reading from a data store, sending a prompt to an LLM, taking
    the outputs from the LLM, and using them in subsequent steps.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 是一个流行的框架，用于使用大型语言模型（LLM）开发应用程序，它使我们能够将多个步骤组合成一个链，其中每个步骤实现一些逻辑，例如从数据存储中读取、向LLM发送提示、获取LLM的输出并在后续步骤中使用它们。
- en: One of LangChain’s advantages is that it uses a modular approach, so we can
    build complex workflows by combining smaller, simpler modules of logic. It provides
    tools for useful tasks such as creating prompt templates and managing context
    for interactive integrations with LLMs, and we can easily find integrations for
    accessing information from sources such as Google Search, knowledge bases, or
    custom data stores.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain的一个优点是它采用模块化方法，因此我们可以通过组合较小的、更简单的逻辑模块来构建复杂的流程。它提供了创建提示模板和管理与LLM交互集成上下文的工具，并且我们可以轻松地找到访问来自Google搜索、知识库或自定义数据存储等信息源的集成。
- en: We will use LangChain later in this book to orchestrate a number of different
    steps in a workflow. For now, let’s summarize what we covered in this chapter
    before we move on to the next one.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分，我们将使用LangChain来编排工作流程中的多个不同步骤。现在，在我们继续下一章之前，让我们总结一下本章所涵盖的内容。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter on advanced GenAI concepts and use cases, we started by diving
    into techniques for tuning and optimizing LLMs. We learned how prompt engineering
    practices can affect model outputs, and how tuning approaches such as full fine-tuning,
    adapter tuning, and LoRA enable pre-trained models to be adapted for specific
    domains or tasks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章关于高级通用人工智能（GenAI）概念和用例的章节中，我们首先深入探讨了调整和优化大型语言模型（LLMs）的技术。我们学习了提示工程实践如何影响模型输出，以及全微调、适配器调整和LoRA等调整方法如何使预训练模型能够适应特定领域或任务。
- en: Next, we dived into embeddings and vector databases, including how they represent
    the meanings of concepts, and enable similarity-based searches. We looked into
    specific embedding models such as Word2Vec and transformer-based encodings.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入探讨了嵌入和向量数据库，包括它们如何表示概念的意义，并启用基于相似性的搜索。我们研究了特定的嵌入模型，如Word2Vec和基于transformer的编码。
- en: We then moved on to describe how RAG can help us to combine information from
    custom data stores into prompts being sent to an LLM, thereby enabling the LLM
    to modify its responses in alignment with the contents of our data stores.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续描述了如何使用RAG（Retrieval-Augmented Generation）将来自自定义数据存储的信息结合到发送给LLM的提示中，从而使得LLM能够根据我们数据存储的内容调整其响应。
- en: After that, we discussed multimodal models and how they can open up additional
    use cases beyond textual language. We then moved on to discuss how the evaluation
    of GenAI models differs in some ways from that of traditional ML models, and we
    introduced some new approaches and metrics for evaluating GenAI models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们讨论了多模态模型以及它们如何打开超越文本语言的额外用例。然后，我们继续讨论了GenAI模型的评估与传统机器学习（ML）模型评估在某些方面的不同，并介绍了一些新的评估GenAI模型的方法和指标。
- en: Finally, we introduced the important topic of LangChain and how it can help
    us build applications that implement complex logic by chaining simple modules
    or building blocks together.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了LangChain这个重要主题，以及它如何帮助我们通过连接简单的模块或构建块来构建实现复杂逻辑的应用程序。
- en: In the next chapter, we will learn about the various GenAI offerings and implementations
    on Google Cloud.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于Google Cloud上提供的各种GenAI服务和实现。
