<html><head></head><body>
		<div id="_idContainer330">
			<h1 id="_idParaDest-215"><em class="italic"><a id="_idTextAnchor248"/>Appendix</em>: Anomaly Detection Tips</h1>
			<p>As we wind down the content for this book, it occurred to us that there's still a plethora of good, bite-sized explanations, examples, and pieces of advice that didn't quite fit into sections of the other chapters. It therefore made sense to give them a home all to themselves here in the <a href="B17040_14_Epub_AM.xhtml#_idTextAnchor248"><em class="italic">Appendix</em></a>. Enjoy this potpourri of tips, tricks, and advice!</p>
			<p>The following topics will be covered here in the <a href="B17040_14_Epub_AM.xhtml#_idTextAnchor248"><em class="italic">Appendix</em></a>:</p>
			<ul>
				<li>Understanding influencers in split versus non-split jobs</li>
				<li>Using one-sided functions to your advantage</li>
				<li>Ignoring time periods</li>
				<li>Using custom rules and filters to your advantage</li>
				<li>Anomaly detection job throughput considerations</li>
				<li>Avoiding the over-engineering of a use case</li>
				<li>Using anomaly detection on runtime fields</li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor249"/>Technical requirements</h1>
			<p>The information in this chapter will use the Elastic Stack as it exists in v7.12. </p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor250"/>Understanding influencers in split versus non-split jobs</h1>
			<p>You might <a id="_idIndexMarker960"/>question whether or not it is necessary to split<a id="_idIndexMarker961"/> the analysis by a field, or merely hope that the use of influencers will give the desired effect of identifying the offending entity. </p>
			<p>Let's remind ourselves of the difference between the purpose of influencers and the purpose of splitting a job. An entity<a id="_idIndexMarker962"/> is identified by an anomaly detection job as an influencer if it has contributed significantly to the existence of the anomaly. This notion of deciding influential entities is completely independent of whether or not the job is split. An entity can be deemed influential on an anomaly only if an anomaly happens in the first place. If there is no anomaly detected, there is no need to figure out whether there is an influencer. However, the job may or may not find that something is anomalous, depending on <a id="_idIndexMarker963"/>whether or not the job is split into multiple <a id="_idIndexMarker964"/>time series. When splitting the job, you are modeling (creating a separate analysis) for each entity of the field chosen for the split.</p>
			<p>Let's look at one of the<a id="_idIndexMarker965"/> Elastic ML development team's favorite <a id="_idIndexMarker966"/>demo datasets, called <strong class="source-inline">farequote</strong> (available in the GitHub repository for this book as a file entitled <strong class="source-inline">farequote-2021.csv</strong> and easily uploaded via Elastic ML's file upload in the <strong class="bold">Data Visualizer</strong>). This dataset originated from a real customer, who ran a travel portal application. The application's access log recorded the number of times a piece of middleware was called when it reached out to a third-party airline for a quote of airline fares. The JSON documents look like the following:</p>
			<p class="source-code">   {</p>
			<p class="source-code">       "@timestamp": "2021-02-11T23:59:54.000Z",</p>
			<p class="source-code">       "responsetime": 251.573,</p>
			<p class="source-code">       "airline": "FFT"</p>
			<p class="source-code">}</p>
			<p>The number of events per unit of time corresponds to the number of requests being made, and the <strong class="source-inline">responsetime</strong> field is the response time of that individual request to the fare quoting web service of that airline.</p>
			<p>Let's take a look at the following cases:</p>
			<ul>
				<li><strong class="bold">Case 1</strong>: An analysis of count over time, not split on airline, but using airline as an influencer. You could accomplish this using the Multi-metric wizard configured as follows:</li>
			</ul>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="image/B17040_14_1.jpg" alt="Figure A.1 – A job with no split, but an influencer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.1 – A job with no split, but an influencer</p>
			<p>After the <a id="_idIndexMarker967"/>analysis runs, the spike seen in the middle <a id="_idIndexMarker968"/>of the data (as shown in the job configuration preview screen in <em class="italic">Figure A.1</em>) is indeed flagged as anomalous with a modest score of 27, and the airline of AAL was flagged as an influencer:</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/B17040_14_2.jpg" alt="Figure A.2 – Result of count job with no split, but an influencer found&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.2 – Result of count job with no split, but an influencer found</p>
			<p>Let's <a id="_idIndexMarker969"/>compare this result to what we see <a id="_idIndexMarker970"/>in the next case.</p>
			<ul>
				<li><strong class="bold">Case 2</strong>: An analysis of count over time, split on <strong class="source-inline">airline</strong>, and using <strong class="source-inline">airline</strong> as an influencer.<p>If we repeat the configuration in <em class="italic">Figure A.1</em>, but this time choose to split on <strong class="source-inline">airline</strong> (thereby setting <strong class="source-inline">partition_field_name:airline</strong>), we will certainly see that airline AAL is still the most unusual, and that the anomaly score for it is much higher than in Case 1:</p></li>
			</ul>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<img src="image/B17040_14_3.jpg" alt="Figure A.3 – Result of count job with a split, and an influencer found&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.3 – Result of count job with a split, and an influencer found</p>
			<p>In other words, the anomalous behavior of AAL is much more prominent when the job was split to model each airline separately. Otherwise, AAL's anomalous behavior was being masked somewhat when all the document counts were mixed. This<a id="_idIndexMarker971"/> is even more prominent when<a id="_idIndexMarker972"/> we show the difference of splitting versus not splitting when we look at an analysis of the <strong class="source-inline">responsetime</strong> field in the next two cases.</p>
			<ul>
				<li><strong class="bold">Case 3</strong>: An analysis of the mean of the <strong class="source-inline">responsetime</strong> field, split on <strong class="source-inline">airline</strong>. <p>Here, we see that AAL is also the most unusual airline with respect to an analysis of the <strong class="source-inline">responsetime</strong> field:</p></li>
			</ul>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/B17040_14_4.jpg" alt="Figure A.4 – Result of response time job with a split, and an influencer found&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.4 – Result of response time job with a split, and an influencer found</p>
			<p>Let's now<a id="_idIndexMarker973"/> compare this<a id="_idIndexMarker974"/> result to the next case, where we won't split the job.</p>
			<ul>
				<li><strong class="bold">Cast 4:</strong> An analysis of the mean of the <strong class="source-inline">responsetime</strong> field, no split, but still using <strong class="source-inline">airline</strong> as an influencer.<p>In this case, the results are as follows:</p></li>
			</ul>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="image/B17040_14_5.jpg" alt="Figure A.5 – Result of response time job without a split, and an influencer found&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.5 – Result of response time job without a split, and an influencer found</p>
			<p>You can<a id="_idIndexMarker975"/> see that the airline that we know is the most <a id="_idIndexMarker976"/>unusual (AAL) is no longer found. In this case, all of the airline's response times are getting averaged together each bucket span, because the job is not split. Now, the most prominent anomaly (even though it is a relatively minor variation above normal) is shown and is deemed to be influenced by <strong class="source-inline">airline=NKS</strong>. However, this may be misleading. You see, <strong class="source-inline">airline=NKS</strong> has a very stable response time during this period, but note that its normal operating range is much higher than the rest of the group:</p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="image/B17040_14_6.jpg" alt="Figure A.6 – Average response times of each airline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.6 – Average response times of each airline</p>
			<p>As such, the<a id="_idIndexMarker977"/> contribution of NKS to the total aggregate <a id="_idIndexMarker978"/>response times of all airlines is more significant than the others. So, of course, ML identifies NKS as the most prominent influencer.</p>
			<p>And there you have it: the moral here is that you should be thoughtful if you are simply relying on influencers to find unusual entities within a dataset of multiple entities. It might be more sensible to individually model each entity independently!</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor251"/>Using one-sided functions to your advantage</h1>
			<p>Many people realize the usefulness <a id="_idIndexMarker979"/>of one-sided functions in ML, such as <strong class="source-inline">low_count</strong> and <strong class="source-inline">high_mean</strong>, to allow for the detection of anomalies only on the high side or on the low side. This is useful when you only care about a drop in revenue or a spike in response time.</p>
			<p>However, when you care about deviations in both directions, you are often inclined to use just the regular function (such as <strong class="source-inline">count</strong> or <strong class="source-inline">mean</strong>). However, on some datasets, it is more optimal to use both the high and low versions of the function as two separate detectors. Why is this the case and under what conditions, you might ask?</p>
			<p>The condition where this makes sense is when the dynamic range of the possible deviations is asymmetrical. In other words, the magnitude of potential spikes in the data is far, far bigger than the magnitude of the potential drops, possibly because the count or sum of something cannot be less than zero. Let's look at the following screenshot:</p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="image/B17040_14_7.jpg" alt="Figure A.7 – An analysis using the two-sided &quot;sum&quot; function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.7 – An analysis using the two-sided "sum" function</p>
			<p>Here, the two-sided <strong class="source-inline">sum</strong> function properly identifies the large spike with a critical anomaly on the left, but the lack of expected double bumps in the middle is identified with only warning anomalies. Again, this is because, with a double-sided function, the normalization process ranks all anomalies together. The magnitude (and therefore the unlikeliness) of the spike is far bigger than the lack of data around 18:00, so the anomaly scores are assigned relatively.</p>
			<p>However, if the dataset was analyzed with two separate detectors, using an advanced job, that is, <strong class="source-inline">low_sum(num_trx)</strong> and <strong class="source-inline">high_sum(num_trx)</strong>, then the results would look<a id="_idIndexMarker980"/> very different. Here's the result of the high side:</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="image/B17040_14_8.jpg" alt="Figure A.8 – An analysis using the one-sided &quot;high_sum&quot; function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.8 – An analysis using the one-sided "high_sum" function</p>
			<p>And here's the result of the low side:</p>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<img src="image/B17040_14_9.jpg" alt="Figure A.9 – An analysis using the one-sided &quot;low_sum&quot; function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.9 – An analysis using the one-sided "low_sum" function</p>
			<p>Notice that the anomalies in the middle are now scored much higher (in this case, with a maximum score of 47 yellow).</p>
			<p>So now, when the two one-sided detectors are run together in the same job, you've optimized the dynamic range of each detector (since they have their own normalization table)!</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor252"/>Ignoring time periods</h1>
			<p>Often, people<a id="_idIndexMarker981"/> ask how they can get ML to ignore the fact that a certain event has occurred. Perhaps it was an expected maintenance window, or perhaps something was broken within the data ingest pipeline and data was lost for a few moments. There are a few ways that you can get ML to ignore time periods, and for distinction, we'll separate them into two groups:</p>
			<ul>
				<li>A known, upcoming window of time</li>
				<li>An unexpected window of time that is discovered only after the fact</li>
			</ul>
			<p>To illustrate things, we'll use a single-metric count job (from <em class="italic">Figure A.1</em>) on the <strong class="source-inline">farequote</strong> dataset that has an anomaly on the date of February 9th:</p>
			<div>
				<div id="_idContainer319" class="IMG---Figure">
					<img src="image/B17040_14_10.jpg" alt="Figure A.10 – An analysis on the farequote dataset with an anomaly we'd like to ignore&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.10 – An analysis on the farequote dataset with an anomaly we'd like to ignore</p>
			<p>Now, let's explore the ways we can ignore the anomaly on February 9th using different situations.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor253"/>Ignoring an upcoming (known) window of time</h2>
			<p>Two methods can be<a id="_idIndexMarker982"/> used to ignore an upcoming window of time, as shown in the following subsections. One involves creating a special calendar event, and the other manipulates the time that the data feed runs for.</p>
			<h3>Creating a calendar event</h3>
			<p>You can easily <a id="_idIndexMarker983"/>create an event by clicking on <strong class="bold">Settings</strong> and then <strong class="bold">Create</strong> under the <strong class="bold">Calendar</strong> section. Here, I've created a calendar entry for February 9th:</p>
			<div>
				<div id="_idContainer320" class="IMG---Figure">
					<img src="image/B17040_14_11.jpg" alt="Figure A.11 – Creating a calendar event to ignore a specific time period&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.11 – Creating a calendar event to ignore a specific time period</p>
			<p>If a new job was created (and in this case, belonged to the <strong class="bold">farequote_jobs</strong> group so that it obeyed this calendar), then if the job were to be run over the data, the entire day of February 9th would be completely ignored:</p>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<img src="image/B17040_14_12.jpg" alt="Figure A.12 – A time period being ignored via a calendar event&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.12 – A time period being ignored via a calendar event</p>
			<p>As you can<a id="_idIndexMarker984"/> see, the entire day was masked, including the time of the anomalous spike.</p>
			<h3>Stopping and starting the data feed to ignore the desired timeframe</h3>
			<p>By simply<a id="_idIndexMarker985"/> stopping and restarting the data feed of the anomaly detection job at the appropriate times, you can create a gap in the analysis. Here, the data feed was stopped at midnight on February 9th and restarted at midnight on February 10th:</p>
			<div>
				<div id="_idContainer322" class="IMG---Figure">
					<img src="image/B17040_14_13.jpg" alt="Figure A.13 – A time period being ignored via manipulation of the data feed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.13 – A time period being ignored via manipulation of the data feed</p>
			<p>It was like <a id="_idIndexMarker986"/>February 9th never happened! Now, let's discuss what you can do to ignore a window of time after the fact.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor254"/>Ignoring an unexpected window of time, after the fact</h2>
			<p>To go <em class="italic">back in time</em> and <em class="italic">forget</em> that a window of time happened, we can use two methods. The first <a id="_idIndexMarker987"/>involves the simple cloning and re-running of historical data, and the second involves the use of model snapshots.</p>
			<h3>Cloning the job and re-running the historical data</h3>
			<p>Similar to the<a id="_idIndexMarker988"/> previous section as we saw, resulting in <em class="italic">Figure A.13</em>, we could create a new cloned job and just have the data feed avoid<a id="_idIndexMarker989"/> the window of time you wish to ignore. Stop it at the beginning of the window and resume it at the end of the window. This method works just fine if rebuilding the model from existing (still available) historical data isn't that burdensome. However, if you have really mature models that encapsulate data behaviors gleaned from data that you no longer have access to (because it aged out and was dropped from your cluster), then you will instead need to use the model snapshot technique discussed next.</p>
			<h3>Reverting a job to a prior model snapshot</h3>
			<p>When cloning and <a id="_idIndexMarker990"/>retraining the job on existing historical data isn't desired or practical, you can effectively remove a window of time by using the fact that a model snapshot is taken periodically by the running job. By default, snapshots are captured approximately every 3 to 4 hours. You can change this interval (<strong class="source-inline">background_persist_interval</strong>) when you create or update a job.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Retention of these snapshots is controlled by a few other parameters (such as <strong class="source-inline">daily_model_snapshot_retention_after_days</strong> and <strong class="source-inline">model_snapshot_retention_days</strong>). Consult the Anomaly Detection API documentation at <a href="https://www.elastic.co/guide/en/machine-learning/current/ml-api-quickref.html">https://www.elastic.co/guide/en/machine-learning/current/ml-api-quickref.html</a>.</p>
			<p>The basic procedure for reverting an anomaly detection job to a prior snapshot is as follows:</p>
			<ol>
				<li>Stop the job's data feed if it is running.</li>
				<li>Find the most recent model snapshot that was taken just before the window of time you wish to erase by using the <strong class="bold">Model snapshots </strong>tab of the <strong class="bold">Job</strong> <strong class="bold">Management</strong> page in Kibana:<div id="_idContainer323" class="IMG---Figure"><img src="image/B17040_14_14.jpg" alt="Figure A.14 – Model snapshots UI in Job Management&#13;&#10;"/></div><p class="figure-caption">Figure A.14 – Model snapshots UI in Job Management</p><p>Alternatively, you can use the <strong class="source-inline">get</strong> <strong class="source-inline">snapshots</strong> API call as documented at <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html</a>.</p></li>
				<li>Revert the<a id="_idIndexMarker991"/> job to that snapshot by clicking the revert icon (<img src="image/B17040_14_15.png" alt=""/>) or, if using the API, use the <strong class="source-inline">_revert</strong> command. In the Kibana UI, you will see options on how to delete the data and replay an analysis of the historical data after the snapshot time, including the ability to mask out the problematic time period using a <strong class="bold">Calendar</strong> event:<div id="_idContainer325" class="IMG---Figure"><img src="image/B17040_14_16.jpg" alt="Figure A.15 – Reverting to a prior model snapshot with the ability to mask a period of time&#13;&#10;"/></div><p class="figure-caption">Figure A.15 – Reverting to a prior model snapshot with the ability to mask a period of time</p></li>
				<li>Continue <a id="_idIndexMarker992"/>running the data feed in real time following the ignored time period, if desired.</li>
			</ol>
			<p>With all of these helpful options, you can easily determine the right approach to ignoring a time interval and keeping your anomaly detection jobs from being polluted by problematic operational issues or undesired events.</p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor255"/>Using custom rules and filters to your advantage</h1>
			<p>While the anomaly <a id="_idIndexMarker993"/>detection jobs are incredibly useful, they are also agnostic to the<a id="_idIndexMarker994"/> domain and to the relevance of the raw data. In other words, the unsupervised machine learning algorithms do not know that a tenfold increase in CPU utilization (from 1% to 10%, for example) may not be that interesting to the proper operation of an application even though it may be statistically anomalous/unlikely in the scenario. Likewise, the anomaly detection jobs treat every entity analyzed equally, but the user might want to disavow results for a certain IP address or user ID, since the user knows that anomalies found for these entities are not desired or useful. The usage of custom rules and filters allows the user to inject domain knowledge into the anomaly detection job configuration, thereby having a fair amount of control as to what gets deemed or marked anomalous – or even if entities get considered part of the modeling process in the first place.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor256"/>Creating custom rules</h2>
			<p>To define a custom rule, you<a id="_idIndexMarker995"/> can either accomplish it at job creation time (but only if using the Create job API) or after the job has revealed some anomalies by using the <strong class="bold">Configure rules</strong> menu option from the <strong class="bold">actions</strong> menu in the Anomaly Explorer UI:</p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="image/B17040_14_17.jpg" alt="Figure A.16 – The Configure rules menu item in the Anomaly Explorer UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.16 – The Configure rules menu item in the Anomaly Explorer UI</p>
			<p>When defining a rule, it is mostly self-explanatory. Here, we may decide that despite our response time anomaly of 282.025 ms (shown in <em class="italic">Figure A.16</em>), this is not that interesting and that we wish to ignore anomalies if the response time is still below 1 second (1,000 ms). We can define the rule as shown here:</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B17040_14_18.jpg" alt="Figure A.17 – The Create rule UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.17 – The Create rule UI</p>
			<p>There are additional <a id="_idIndexMarker996"/>options to exclude the value from the modeling and also to limit the scope of the rule to a certain filter list to have the rule only apply to particular entities (for example, only servers that are in a certain location). Filter lists can be defined under <strong class="bold">Settings</strong> and then <strong class="bold">Filter lists</strong> in the UI.</p>
			<p>Note that the rule definition applies to future analysis (from the point of rule definition, forward in time) and does not apply to past anomalies. To have the rule apply to past anomalies, you would have to clone the existing job (once the rule was defined) and then re-run the analysis on the historical raw data. </p>
			<p>So, with rules and filters, the user has a lot of control as to what ultimately gets reported (and alerted) upon as anomalous. This allows a pretty major paradigm shift over the traditional approach of a bottom-up alert creation philosophy that has existed in IT operations for <a id="_idIndexMarker997"/>decades. An alternative approach is described in the next subsection.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor257"/>Benefiting from custom rules for a "top-down" alerting philosophy</h2>
			<p>If we asked, "what percentage of the data that you collect is being paid attention to?", often, a realistic answer is likely &lt;10%, and maybe even &lt;1%. The reason why this is the case is that the traditional approach to making data proactive is to start from scratch and then build up <a id="_idIndexMarker998"/>thresholds or rules-based alerts over time. This can be a daunting and/or tedious task that requires upfront knowledge (or at least a guess) as to what the expected behavior of each time series should be. Then, once the alerts have been configured, there can be an extended tuning process that balances alert sensitivity with annoying false positives. Additionally, there could also be metrics whose unusual behaviors could never be caught with a static threshold.</p>
			<p>Combine this challenge with scale; if I have 10 metrics per server and 100 servers, there are 1,000 individual metrics. Creating individual alerts for each of these is impractical.</p>
			<p>However, a single anomaly detection job could be created against this data in less than 1 minute. Elastic ML's self-learning on historical data, which also takes very little time, will minimize false positives by adapting to the natural characteristics of each time series independently. However, if anomaly detection reveals things we don't care to know about, we can simply exclude them with custom rules.</p>
			<p>This top-down approach (coverage for everything and then begin to exclude what you don't want) is faster and provides broader proactive coverage of the data than the bottom-up approach (creating threshold alerts from scratch).</p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor258"/>Anomaly detection job throughput considerations</h1>
			<p>Elastic ML is <a id="_idIndexMarker999"/>awesome and is no doubt very fast and scalable, but there will still be a practical upper bound of events/second processed to any anomaly detection job, depending on a couple of different factors:</p>
			<ul>
				<li>The speed at which data can be delivered to the algorithms (that is, query performance)</li>
				<li>The speed at which the algorithms can chew through the data, given the desired analysis</li>
			</ul>
			<p>For the latter, much of the<a id="_idIndexMarker1000"/> performance is based upon the following:</p>
			<ul>
				<li>The function(s) chosen for the analysis, that is, <strong class="source-inline">count</strong> is faster than <strong class="source-inline">lat_long</strong></li>
				<li>The <strong class="source-inline">bucket_span</strong> value chosen (longer bucket spans are faster than smaller bucket spans because more buckets analyzed per unit of time compound the per-bucket processing overhead, which is writing results and so on)</li>
			</ul>
			<p>However, if you have a defined analysis set up and can't change it for other reasons, then there's not that much you can do unless you get creative and split the data up into multiple jobs. This is because the ML jobs (at least for now) are currently tied to a single CPU for the analysis bit (running the C++ process called autodetect). So, splitting the data into a few separate ML jobs to at least take advantage of multiple CPUs might be an option. But, before that, let's focus on the former, the query's performance, as there are a variety of possibilities here:</p>
			<ul>
				<li>Avoid doing a cross-cluster search to limit data transmission across the network. </li>
				<li>Tweak data feed parameters to optimize performance.</li>
				<li>Use Elasticsearch query aggregations to distribute the task of distilling the data to a smaller set of ML algorithms.</li>
			</ul>
			<p>The first one is sort of obvious. You're only going to improve performance if you move the analysis closer to the raw data.</p>
			<p>The second one may take some experimentation. There are parameters, such as <strong class="source-inline">scroll_size</strong>, which control the size of each scroll. The default is 1,000, and for decent-sized clusters, this could be safely increased to 10,000. Run some tests at different scroll sizes and see how it affects query and cluster performance.</p>
			<p>The last one should make the biggest impact on performance, in my opinion, but obviously, it is a little tricky and error-prone to get the ES aggregation correct for it to work properly with ML, but it's not so bad. Refer to the documentation at <a href="https://www.elastic.co/guide/en/machine-learning/current/ml-configuring-aggregation.html">https://www.elastic.co/guide/en/machine-learning/current/ml-configuring-aggregation.html</a> for more information. The downside of using aggregations with ML, in general, is that you lose<a id="_idIndexMarker1001"/> access to the other fields in the data that might be good as influencers.</p>
			<p>All in all, these are a few things to consider when optimizing the ML job's performance.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor259"/>Avoiding the over-engineering of a use case</h1>
			<p>I once worked with a user <a id="_idIndexMarker1002"/>where we discussed different use cases for anomaly detection. In particular, this customer was building a hosted security operations <a id="_idIndexMarker1003"/>center as part of their <strong class="bold">managed security service provider </strong>(<strong class="bold">MSSP</strong>) business, so they were keen to think about use cases in which ML could help.</p>
			<p>A high-level theme to their use cases was to look at a user's behavior and find unexpected behavior. One example that was discussed was login activity from unusual/rare locations such as <em class="italic">Bob just logged in from Ukraine, but he doesn't normally log in from there</em>.</p>
			<p>In the process of thinking the implementation through, there was talk of them having multiple clients, each of which had multiple users. Therefore, they were thinking of ways to split/partition the data so that they could execute <strong class="source-inline">rare by country</strong> for each and every user of every client.</p>
			<p>I asked them to take a step back and said, "Is it worthy of an anomaly if anyone logs in from Ukraine, not just Bob?" to which the answer was "Yes."</p>
			<p>So, in this case, there is no point in splitting the analysis out per user; perhaps just keep the partitioning at the client level and simply lump all of the user's locations from each client into a single pool of observed countries. This is actually a better scenario; there's more overall data, and as we know, the <strong class="source-inline">rare</strong> function works best when there is lots of routine data to contrast a novel observation against. </p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor260"/>Using anomaly detection on runtime fields</h1>
			<p>In some cases, it <a id="_idIndexMarker1004"/>might be necessary to analyze the <a id="_idIndexMarker1005"/>value of a field that doesn't exist in the index mappings but can be calculated dynamically from other field values. This capability to dynamically define field values has existed for quite some time in Elasticsearch as <strong class="bold">script fields</strong>, but <a id="_idIndexMarker1006"/>starting in v7.11, script fields are replaced by an updated concept<a id="_idIndexMarker1007"/> known as <strong class="bold">runtime fields</strong>. In short, runtime fields are treated like first-class citizens in the Elasticsearch mapping (if defined there) and will eventually allow the user to promote a runtime field into an indexed field.</p>
			<p>Users can define runtime fields in the mapping or only in the search request. It is good to note that at the time of writing, there is no support for definitions of runtime fields in the data feed of an anomaly detection job. However, if the runtime fields are defined in the mappings, then the anomaly detection job can leverage them seamlessly.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information<a id="_idIndexMarker1008"/> on runtime fields, please consult the Elastic documentation at <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/runtime.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/runtime.html</a>.</p>
			<p>While the full details of runtime fields are beyond the scope of this book, it is important to know that anomaly detection jobs can leverage these dynamic fields just as if they were normal fields. Let's look at an interesting, albeit contrived, example.</p>
			<p>Suppose that we return to the <strong class="source-inline">farequote</strong> example shown in <em class="italic">Figure A.1</em>, where, for the sake of this argument, we declare that February 9th is a special day of some type for <strong class="source-inline">airline:AAL</strong> – perhaps the rough equivalent of Black Friday or Cyber Monday, or even just a day where we know things will be slightly off from normal by a known amount. We will contrive a scenario in which we know that AAL will experience predictably higher response times that could be 20% slower than normal (meaning measurements of <strong class="source-inline">responsetime</strong> should be 20% higher, in milliseconds). We don't want to make February 9th a calendar event for Elastic ML to avoid completely, and we don't want to stop looking at the data for AAL. Rather, we just want to temper the response time measures down by 20% so as not to upset our normal modeling and/or alerting. We can <a id="_idIndexMarker1009"/>accomplish this via runtime<a id="_idIndexMarker1010"/> fields:</p>
			<ol>
				<li value="1">The first thing to do is to define a new runtime field in the mapping for the index, called <strong class="source-inline">responsetime_adjusted</strong>:<p class="source-code">PUT farequote/_mapping</p><p class="source-code">{</p><p class="source-code">  "runtime": {</p><p class="source-code">    "responsetime_adjusted": {</p><p class="source-code">      "type": "double",</p><p class="source-code">      "script": {</p><p class="source-code">        "source": "emit(params._source.responsetime * 1.0)"</p><p class="source-code">      }</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p><p>This field (for now) will be exactly the same as the <strong class="source-inline">responsetime</strong> field for all airlines, simply accomplished by multiplying the field value by the constant of <strong class="source-inline">1.0</strong>. </p></li>
				<li>Next, we will configure a job to use a <strong class="source-inline">high_mean</strong> detector on this new <strong class="source-inline">responsetime_adjusted</strong> field, where we will also split the analysis on the <strong class="source-inline">airline</strong> field:<div id="_idContainer328" class="IMG---Figure"><img src="image/B17040_14_19.jpg" alt="Figure A.18 – Configuring a job to analyze a runtime field&#13;&#10;"/></div><p class="figure-caption">Figure A.18 – Configuring a job to analyze a runtime field</p></li>
				<li>We will run the data feed up until midnight on February 9th but stop the analysis there. To <a id="_idIndexMarker1011"/>then temper the <a id="_idIndexMarker1012"/>response time for AAL's data by 20% (but leave other airlines' data alone), we will execute the following command:<p class="source-code">PUT farequote/_mapping</p><p class="source-code">{</p><p class="source-code">  "runtime": {</p><p class="source-code">    "responsetime_adjusted": {</p><p class="source-code">      "type": "double",</p><p class="source-code">      "script": {</p><p class="source-code">        "source": "if(doc['airline'].value.equals('AAL')) {emit(params._source.responsetime * 0.8)} else {emit(params._source.responsetime * 1.0)}"</p><p class="source-code">      }</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p></li>
				<li>Next, we will continue the job's data feed to analyze the special day (February 9th, but stop at midnight at the beginning of February 10th).</li>
				<li>Once February 9th's data has been analyzed, we will return the response time for AAL's data back to normal by re-invoking the command in <em class="italic">Step 1</em>.</li>
				<li>We'll allow the job to continue analyzing the rest of the data, as normal.<p>The end result is <a id="_idIndexMarker1013"/>that we were able to<a id="_idIndexMarker1014"/> successfully suppress the values of AAL's response times by 20% (as evidenced by the depressed values between the two annotations), but we were still able to pick up a significant anomaly despite our special treatment for AAL:</p></li>
			</ol>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/B17040_14_20.jpg" alt="Figure A.19 – Results of the job that analyzed a dynamically changed runtime field&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure A.19 – Results of the job that analyzed a dynamically changed runtime field</p>
			<p>This technique <a id="_idIndexMarker1015"/>could be useful in bringing any <a id="_idIndexMarker1016"/>number of dynamic modifications to the data on the fly for enhanced analysis or to support the analysis in aspects of the data that may not be available in the default field mappings of the index.</p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor261"/>Summary</h1>
			<p>Elastic ML is a powerful, flexible, yet easy-to-use feature that gives the power of data science to non-data scientists so that they can gain insight into massive amounts of data. Throughout this entire book, there are many different ways in which users can take advantage of technology to solve real-world challenges in IT. We hope that you will take the knowledge that you have gained in this book and implement some great use cases of your own. Don't worry about solving all possible problems on day 1 – start small, get some tangible wins, and grow your usage as you gain more confidence. Success will breed success!</p>
		</div>
	</body></html>