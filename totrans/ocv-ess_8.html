<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Topics"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Advanced Topics</h1></div></div></div><p class="calibre7">This chapter covers the less commonly used topics, such as machine learning with multiple classes and GPU-based optimizations. Both the topics are seeing a growth in interest and practical applications, so they deserve a complete chapter. We consider them advanced only as long as additional knowledge is required about machine learning / statistical classification and parallelization. We will start by explaining some of the most well-known classifiers such as KNN, SVM, and Random Forests, all of which are available in the <code class="email">ml</code> module and show how they work with different database formats and multiple classes. Finally, a set of classes and functions to utilize GPU-based computational resources will be described.</p></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Topics">
<div class="book" title="Machine learning"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec60" class="calibre1"/>Machine learning</h1></div></div></div><p class="calibre7">Machine learning <a id="id502" class="calibre1"/>deals with techniques that allow computers to learn and make decisions by themselves. A central concept in machine learning is the <a id="id503" class="calibre1"/>classifier. A classifier learns from the examples in a dataset, where the label of each sample is known. Usually, we have two datasets at hand: training and test. The classifier builds a model using the training set. This trained classifier is expected to predict the label of new unseen samples, so we finally use the test set to validate it and assess label recognition rates.</p><p class="calibre7">In this section, we explain the different classes and functions that OpenCV provides for classification, and simple examples of their use. Machine learning classes and functions for statistical classification, regression, and clustering of data are all included in the <code class="email">ml</code> module.</p></div></div>
<div class="book" title="The KNN classifier"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec61" class="calibre1"/>The KNN classifier</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre8">K-nearest neighbors</strong></span> (<span class="strong"><strong class="calibre8">KNN</strong></span>) is one<a id="id504" class="calibre1"/> of the simplest classifiers. It is a supervised classification method, which learns from available cases and classifies new cases by a minimum distance. K is the number of neighbors to be analyzed in the decision. The new data point to classify (the query) is projected to the same space as the learning points, and its class is given by the most frequent class among its KNN from the training set.</p><p class="calibre7">The <a id="id505" class="calibre1"/>following <code class="email">KNNClassifier</code> code is an example of using the KNN algorithm to classify each image pixel to the nearest color: black (0, 0, 0), white (255, 255, 255), blue (255, 0, 0), green (0, 255, 0), or red (0, 0, 255):</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
<span class="strong"><strong class="calibre8">#include &lt;opencv2/ml/ml.hpp&gt;</strong></span>

using namespace std;
using namespace cv;

int main(int argc, char *argv[]){

//Create Mat for the training set and classes
<span class="strong"><strong class="calibre8">    Mat classes(5, 1, CV_32FC1);</strong></span>
<span class="strong"><strong class="calibre8">    Mat colors(5, 3, CV_32FC1);</strong></span>

    //Training set (primary colors)
    colors.at&lt;float&gt;(0,0)=0, colors.at&lt;float&gt;(0,1)=0, colors.at&lt;float&gt;(0,2)=0;
    colors.at&lt;float&gt;(1,0)=255, colors.at&lt;float&gt;(1,1)=255, colors.at&lt;float&gt;(1,2)=255;
    colors.at&lt;float&gt;(2,0)=255, colors.at&lt;float&gt;(2,1)=0, colors.at&lt;float&gt;(2,2)=0;
    colors.at&lt;float&gt;(3,0)=0, colors.at&lt;float&gt;(3,1)=255, colors.at&lt;float&gt;(3,2)=0;
    colors.at&lt;float&gt;(4,0)=0, colors.at&lt;float&gt;(4,1)=0, colors.at&lt;float&gt;(4,2)=255;

    //Set classes to each training sample
    classes.at&lt;float&gt;(0,0)=1;
    classes.at&lt;float&gt;(1,0)=2;
    classes.at&lt;float&gt;(2,0)=3;
    classes.at&lt;float&gt;(3,0)=4;
    classes.at&lt;float&gt;(4,0)=5;

    //KNN classifier (k=1)
<span class="strong"><strong class="calibre8">    CvKNearest classifier;</strong></span>
<span class="strong"><strong class="calibre8">    classifier.train(colors,classes,Mat(),false,1,false);</strong></span>

    //Load original image
    Mat src=imread("baboon.jpg",1);
    imshow("baboon",src);

    //Create result image
    Mat dst(src.rows , src.cols, CV_8UC3);

    Mat results;
<span class="strong"><strong class="calibre8">Mat newPoint(1,3,CV_32FC1);</strong></span>

    //Response for each pixel and store the result in the result image
    float prediction=0;
    for(int y = 0; y &lt; src.rows; ++y){
      for(int x = 0; x &lt; src.cols; ++x){
         newPoint.at&lt;float&gt;(0,0)= src.at&lt;Vec3b&gt;(y, x)[0];
         newPoint.at&lt;float&gt;(0,1) = src.at&lt;Vec3b&gt;(y, x)[1];
         newPoint.at&lt;float&gt;(0,2) = src.at&lt;Vec3b&gt;(y, x)[2];
<span class="strong"><strong class="calibre8">          prediction=classifier.find_nearest(newPoint,1,&amp;results, 0, 0);</strong></span>
         dst.at&lt;Vec3b&gt;(y, x)[0]= colors.at&lt;float&gt;(prediction-1,0);
         dst.at&lt;Vec3b&gt;(y, x)[1]= colors.at&lt;float&gt;(prediction-1,1);
         dst.at&lt;Vec3b&gt;(y, x)[2]= colors.at&lt;float&gt;(prediction-1,2);
      }
    }

    //Show result image
    cv::imshow("result KNN",dst);
    cv::waitKey(0);
    return 0;
}</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note38" class="calibre1"/>Note</h3><p class="calibre7">Remember that OpenCV uses a BGR color scheme.</p></div><p class="calibre7">OpenCV provides the <a id="id506" class="calibre1"/>KNN algorithm through the <code class="email">CvKNearest</code> class. The training information is added to the KNN classifier through the <code class="email">bool CvKNearest::train(const Mat&amp; trainData, const Mat&amp; responses, const Mat&amp; sampleIdx, bool isRegression, int maxK, bool updateBase)</code> function. The example creates a training set with five samples, (<code class="email">Mat colors(5, 3, CV_32FC1)</code>), which represent each class (color) (<code class="email">Mat classes(5, 1, CV_32FC1)</code>); these are the first two input parameters. The <code class="email">isRegression</code> is parameter is a Boolean value that defines whether we want to perform a classification or a regression. The <code class="email">maxK</code> value indicates the maximum number of neighbors that will be used in the test phase. </p><p class="calibre7">Finally, <code class="email">updateBaseparameter</code> allows us to indicate whether we want to train a new classifier with the data or use it to update the previous training data. Then, the code sample performs the test phase with each pixel of the original image using the <code class="email">float CvKNearest::find_nearest(const Mat&amp; samples, int k, Mat* results=0, const float** neighbors=0, Mat* neighborResponses=0, Mat* dist=0)</code> function. The function tests the input sample, selecting the KNN, and finally predicts the class value for this sample.</p><p class="calibre7">In the following screenshot, we <a id="id507" class="calibre1"/>can see the code output and the difference between the original and the result images after this KNN classification:</p><div class="mediaobject"><img src="../images/00054.jpeg" alt="The KNN classifier" class="calibre9"/><div class="caption"><p class="calibre13">KNN classification using the primary colors as classes (left: the original image, right: the result image)</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="The Random Forest classifier"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec62" class="calibre1"/>The Random Forest classifier</h1></div></div></div><p class="calibre7">Random Forests <a id="id508" class="calibre1"/>are a general class of ensemble building methods that use a decision tree as the base classifier. The Random Forest classifier is a variation of the <a id="id509" class="calibre1"/>Bagging classifier (Bootstrap Aggregating). The Bagging algorithm is a method of classification that generates weak individual classifiers using bootstrap. Each classifier is trained on a random redistribution of the training set so that many of the original examples may be repeated in each classification. </p><p class="calibre7">The principal difference between Bagging and Random Forest is that Bagging uses all the features in each tree node and Random Forest selects a random subset of the features. The suitable number of randomized features corresponds to the square root of the total number of features. For prediction, a new sample is pushed down the tree and it is assigned the class of the terminal (or leaf) node in the tree. This method is iterated over all the trees, and finally, the average vote of all the tree predictions is considered as the prediction result. The following diagram shows the Random Forest algorithm:</p><div class="mediaobject"><img src="../images/00055.jpeg" alt="The Random Forest classifier" class="calibre9"/><div class="caption"><p class="calibre13">The RF classifier</p></div></div><p class="calibre10"> </p><p class="calibre7">Random Forests are <a id="id510" class="calibre1"/>currently one of the best classifiers available, both in recognition power and efficiency. In our example <code class="email">RFClassifier</code>, we use the OpenCV Random Forest classifier and also the OpenCV <code class="email">CvMLData</code> class. A large amount of information is typically handled in machine learning problems, and for this reason, it is convenient to use a <code class="email">.cvs</code> file. The <code class="email">CvMLData</code> class is used to load the training set information from such a file as follows:</p><div class="informalexample"><pre class="programlisting">//… (omitted for simplicity)

int main(int argc, char *argv[]){

<span class="strong"><strong class="calibre8">    CvMLData mlData;</strong></span>
<span class="strong"><strong class="calibre8">    mlData.read_csv("iris.csv");</strong></span>
<span class="strong"><strong class="calibre8">    mlData.set_response_idx(4);</strong></span>
    //Select 75% samples as training set and 25% as test set
<span class="strong"><strong class="calibre8">    CvTrainTestSplit cvtts(0.75f, true);</strong></span>
    //Split the iris dataset
<span class="strong"><strong class="calibre8">    mlData.set_train_test_split(&amp;cvtts);</strong></span>

    //Get training set
<span class="strong"><strong class="calibre8">    Mat trainsindex= mlData.get_train_sample_idx();</strong></span>
    cout&lt;&lt;"Number of samples in the training <span class="strong"><strong class="calibre8">      </strong></span>set:"&lt;&lt;trainsindex.cols&lt;&lt;endl;
    //Get test set
<span class="strong"><strong class="calibre8">    Mat testindex=mlData.get_test_sample_idx();</strong></span>
    cout&lt;&lt;"Number of samples in the test set:"&lt;&lt;testindex.cols&lt;&lt;endl;
    cout&lt;&lt;endl;

    //Random Forest parameters
<span class="strong"><strong class="calibre8">    CvRTParams params = CvRTParams(3, 1, 0, false, 2, 0, false, 0, 100, 0, CV_TERMCRIT_ITER | CV_TERMCRIT_EPS);</strong></span>

<span class="strong"><strong class="calibre8">    CvRTrees classifierRF;</strong></span>
    //Taining phase
<span class="strong"><strong class="calibre8">    classifierRF.train(&amp;mlData,params);</strong></span>
    std::vector&lt;float&gt; train_responses, test_responses;

    //Calculate train error
    cout&lt;&lt;"Error on train samples:"&lt;&lt;endl;
    cout&lt;&lt;<span class="strong"><strong class="calibre8">(float)classifierRF.calc_error( &amp;mlData, CV_TRAIN_ERROR,&amp;train_responses)</strong></span>&lt;&lt;endl;

    //Print train responses
    cout&lt;&lt;"Train responses:"&lt;&lt;endl;
    for(int i=0;i&lt;(int)train_responses.size();i++)
        cout&lt;&lt;i+1&lt;&lt;":"&lt;&lt;(float)train_responses.at(i)&lt;&lt;"  ";
    cout&lt;&lt;endl&lt;&lt;endl;

    //Calculate test error
    cout&lt;&lt;"Error on test samples:"&lt;&lt;endl;
    cout&lt;&lt;<span class="strong"><strong class="calibre8">(float)classifierRF.calc_error( &amp;mlData, CV_TEST_ERROR,&amp;test_responses)</strong></span>&lt;&lt;endl;

    //Print test responses
    cout&lt;&lt;"Test responses:"&lt;&lt;endl;
    for(int i=0;i&lt;(int)test_responses.size();i++)
        cout&lt;&lt;i+1&lt;&lt;":"&lt;&lt;(float)test_responses.at(i)&lt;&lt;"  ";
    cout&lt;&lt;endl&lt;&lt;endl;

    return 0;
}</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre7">The dataset has been provided by the <a id="id511" class="calibre1"/>UC Irvine Machine Learning Repository, available at <a class="calibre1" href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>. For this code sample, the Iris dataset was used.</p></div><p class="calibre7">As we <a id="id512" class="calibre1"/>mentioned previously, the <code class="email">CvMLData</code> class allows you to load the dataset from a <code class="email">.csv</code> file using the <code class="email">read_csv</code> function and indicates the class column by the <code class="email">set_response_idx</code> function. In this case, we use this dataset to perform the training and test phases. It is possible to split the dataset into two disjoint sets for training and test. For this, we use the <code class="email">CvTrainTestSplit</code> struct and the <code class="email">void CvMLData::set_train_test_split(const CvTrainTestSplit* spl)</code> function. In the <code class="email">CvTrainTestSplit</code> struct, we indicate the percentage of samples to be used as the training set (0.75 percent in our case) and whether we want to mix the indices of the training and test samples from the dataset. The <code class="email">set_train_test_split</code> function performs the split. Then, we can store each set in <code class="email">Mat</code> with the <code class="email">get_train_sample_idx()</code> and <code class="email">get_test_sample_idx()</code>functions.</p><p class="calibre7">The Random Forest classifier is created using the <code class="email">CvRTrees</code> class, and its parameters are defined by the <code class="email">CvRTParams::CvRTParams(int max_depth, int min_sample_count, float regression_accuracy, bool use_surrogates, int max_categories, const float* priors, bool calc_var_importance, int nactive_vars, int max_num_of_trees_in_the_forest, float forest_accuracy, int termcrit_type)</code> constructor. Some of the most important input parameters refer to the maximum depth of the trees (<code class="email">max_depth</code>)—in our sample, it has a value of 3—the number of randomized features in each node (<code class="email">nactive_vars</code>), and the maximum number of trees in the forest (<code class="email">max_num_of_trees_in_the_forest</code>). If we set the <code class="email">nactive_vars</code> parameter to 0, the number of randomized features will be the square root of the total number of features.</p><p class="calibre7">Finally, once the classifier is trained with the <code class="email">train</code> function, we can obtain the percentage of misclassified samples using the <code class="email">float CvRTrees::calc_error(CvMLData* data, int type, std::vector&lt;float&gt;* resp=0 )</code> method. The parameter type allows you to select the source of the error: <code class="email">CV_TRAIN_ERROR</code> (an error in the training samples) or <code class="email">CV_TEST_ERROR</code> (an error in the test samples).</p><p class="calibre7">The following screenshot <a id="id513" class="calibre1"/>shows the training and test errors and the classifier responses in both the sets:</p><div class="mediaobject"><img src="../images/00056.jpeg" alt="The Random Forest classifier" class="calibre9"/><div class="caption"><p class="calibre13">The RF classifier sample results</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="SVM for classification"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec63" class="calibre1"/>SVM for classification</h1></div></div></div><p class="calibre7">The <span class="strong"><strong class="calibre8">Support Vector Machine</strong></span> (<span class="strong"><strong class="calibre8">SVM</strong></span>) classifier finds a discriminant function by maximizing the geometrical margin between the classes. Thus, the <a id="id514" class="calibre1"/>space is mapped in such a way that the classes are as widely separated as possible. SVM minimizes both the training error and the geometrical margin. Nowadays, this classifier is one of the best classifiers available and has been applied to many real-world problems. The following <code class="email">SVMClassifier</code> sample code performs a classification using the SVM classifier and a dataset of 66 image objects. The dataset is divided into four classes: a training shoe (class 1), a cuddly toy (class 2), a plastic cup (class 3), and a bow (class 4). The following screenshot shows the examples of the four classes. A total of 56 images and 10 images were used for the training and the test sets, respectively. Images in the training set take the following name structure: <code class="email">[1-14].png</code> corresponds to class 1, <code class="email">[15-28].png</code> to class 2, <code class="email">[29-42].png</code> to class 3, and <code class="email">[43-56].png</code> to class 4. On the other hand, images in the test set are characterized by the word unknown followed by a number, for example, <code class="email">unknown1.png</code>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip12" class="calibre1"/>Tip</h3><p class="calibre7">The images of the four classes have been extracted from the <span class="strong"><strong class="calibre8">Amsterdam Library of Object Images</strong></span> (<span class="strong"><strong class="calibre8">ALOI</strong></span>) available<a id="id515" class="calibre1"/> at <a class="calibre1" href="http://aloi.science.uva.nl/">http://aloi.science.uva.nl/</a>.</p></div><div class="mediaobject"><img src="../images/00057.jpeg" alt="SVM for classification" class="calibre9"/><div class="caption"><p class="calibre13">Classes selected for the SVM classification example</p></div></div><p class="calibre10"> </p><p class="calibre7">The <code class="email">SVMClassifier</code> sample <a id="id516" class="calibre1"/>code is as follows:</p><div class="informalexample"><pre class="programlisting">//… (omitted for simplicity)
#include &lt;opencv2/features2d/features2d.hpp&gt;
#include &lt;opencv2/nonfree/features2d.hpp&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[]){

    Mat groups;
    Mat samples;
    vector&lt;KeyPoint&gt; keypoints1;
    //ORB feature detector with 15 interest points
    OrbFeatureDetector detector(15, 1.2f, 2, 31,0, 2, ORB::HARRIS_SCORE, 31);
    Mat descriptors, descriptors2;
    //SURF feature descriptor
    SurfDescriptorExtractor extractor;

    //Training samples
    for(int i=1; i&lt;=56; i++){
        stringstream nn;
        nn &lt;&lt;i&lt;&lt;".png";
        //Read the image to be trained
        Mat img=imread(nn.str());
        cvtColor(img, img, COLOR_BGR2GRAY);
        //Detect interest points
        detector.detect(img, keypoints1);
        //Compute SURF descriptors
        extractor.compute(img, keypoints1, descriptors);
        //Organize and save information in one row
        samples.push_back(descriptors.reshape(1,1));
        keypoints1.clear();
    }

    //Set the labels of each sample
    for(int j=1; j&lt;=56; j++){
        if(j&lt;=14)  groups.push_back(1);
        else if(j&gt;14 &amp;&amp; j&lt;=28)  groups.push_back(2);
             else if(j&gt;28 &amp;&amp; j&lt;=42)  groups.push_back(3);
                  else groups.push_back(4);
    }

    //Indicate SVM parameters
<span class="strong"><strong class="calibre8">    CvSVMParams params=CvSVMParams(CvSVM::C_SVC, CvSVM::LINEAR, 0, 1, 0, 1, 0, 0, 0, cvTermCriteria(CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 100, FLT_EPSILON));</strong></span>

    //Create SVM classifier
<span class="strong"><strong class="calibre8">    CvSVM classifierSVM;</strong></span>

    //Train classifier
<span class="strong"><strong class="calibre8">    classifierSVM.train(samples, groups, Mat(), Mat(), params );</strong></span>

    //Test samples
    for(int i=1; i&lt;=10; i++){
        stringstream nn;
        nn &lt;&lt;"unknown"&lt;&lt;i&lt;&lt;".png";
        //Read the image to be tested
        Mat unknown=imread(nn.str());
        cvtColor(unknown, unknown, COLOR_BGR2GRAY);
        //Detect interest points
        detector.detect(unknown, keypoints1);
        //Compute descriptors
        extractor.compute(unknown, keypoints1, descriptors2);
        //Test sample
<span class="strong"><strong class="calibre8">        float result=classifierSVM.predict(descriptors2.reshape(1,1));</strong></span>
        //Print result
        cout&lt;&lt;nn.str()&lt;&lt;": class "&lt;&lt;result&lt;&lt;endl;
    }
    return 0;
}</pre></div><p class="calibre7">The explanation of the code is given as follows. In this example, images are represented by their descriptors (see <a class="calibre1" title="Chapter 5. Focusing on the Interesting 2D Features" href="part0042_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre12">Focusing on the Interesting 2D Features</em></span>). For each image in the training set, its interest points are detected using an <span class="strong"><strong class="calibre8">Oriented FAST and Rotated BRIEF</strong></span> (<span class="strong"><strong class="calibre8">ORB</strong></span>)<a id="id517" class="calibre1"/> detector (<code class="email">OrbFeatureDetector</code>) and its descriptors are computed using the <span class="strong"><strong class="calibre8">Speeded Up Robust Features</strong></span> (<span class="strong"><strong class="calibre8">SURF</strong></span>)<a id="id518" class="calibre1"/> descriptor (<code class="email">SurfDescriptorExtractor</code>).</p><p class="calibre7">An SVM classifier is<a id="id519" class="calibre1"/> created using the <code class="email">CvSVM</code> class and its parameters are set using the <code class="email">CvSVMParams::CvSVMParams(int svm_type, int kernel_type, double degree, double gamma, double coef0, double Cvalue, double nu, double p, CvMat* class_weights, CvTermCriteria term_crit)</code> constructor. The interesting parameters in this constructor are the type of SVM (<code class="email">svm_type</code>) and the type of kernel (<code class="email">kernel_type</code>). The first specified parameter takes, in our case, the <code class="email">CvSVM::C_SVC</code> value because an n-classification (n <span class="strong"><img src="../images/00058.jpeg" alt="SVM for classification" class="calibre27"/></span> 2) with an imperfect separation of the classes is needed. It also uses a C penalty value for atypical values. C acts, therefore, as a regularizer. The <code class="email">kernel_type</code> parameter indicates the type of SVM kernel. The kernel represents the basis function required to separate the cases. For the SVM classifier, OpenCV includes the following kernels:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">CvSVM::LINEAR</code>: The linear kernel</li><li class="listitem"><code class="email">CvSVM::POLY</code>: The polynomial kernel</li><li class="listitem"><code class="email">CvSVM::RBF</code>: The radial basis function</li><li class="listitem"><code class="email">CvSVM::SIGMOID</code>: The sigmoid kernel</li></ul></div><p class="calibre7">Then, the<a id="id520" class="calibre1"/> classifier builds an optimal linear discriminating function using the training set (with the <code class="email">train</code> function). Now, it is prepared to classify new unlabeled samples. The test set is used for this purpose. Note that we also have to calculate the ORB detector and the SURF descriptors for each image in the test set. The result is as shown in the following screenshot, where all the classes have been classified correctly:</p><div class="mediaobject"><img src="../images/00059.jpeg" alt="SVM for classification" class="calibre9"/><div class="caption"><p class="calibre13">The classification result using SVM</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="What about GPUs?"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec64" class="calibre1"/>What about GPUs?</h1></div></div></div><p class="calibre7">CPUs seem to have<a id="id521" class="calibre1"/> reached their speed and thermal power limits. It has become complex and expensive to build a computer with several processors. Here is where GPUs come into play. <span class="strong"><strong class="calibre8">General-Purpose Computing on Graphics Processing Units</strong></span> (<span class="strong"><strong class="calibre8">GPGPU</strong></span>) is a new <a id="id522" class="calibre1"/>programming paradigm that uses the GPU to perform computations and enables the faster execution of programs and a reduction of power consumption. They include hundreds of general-purpose computing processors that can do much more than render graphics, especially if they are used in tasks that can be parallelized, which is the case with computer vision algorithms.</p><p class="calibre7">OpenCV includes <a id="id523" class="calibre1"/>support for the OpenCL and CUDA architectures, with the latter having more implemented algorithms and a better optimization. This is the reason why we are introducing the CUDA GPU module in this chapter.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Setting up OpenCV with CUDA"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec65" class="calibre1"/>Setting up OpenCV with CUDA</h1></div></div></div><p class="calibre7">The installation guide <a id="id524" class="calibre1"/>presented in <a class="calibre1" title="Chapter 1. Getting Started" href="part0014_split_000.html#page">Chapter 1</a>, <span class="strong"><em class="calibre12">Getting Started</em></span>, needs a<a id="id525" class="calibre1"/> few additional steps in order to include the GPU module. We assume that the computer in which OpenCV is going to be installed already has the software detailed in that guide.</p><p class="calibre7">There are new requirements to be satisfied in order to compile OpenCV with CUDA on Windows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre8">CUDA-capable GPU</strong></span>: This is the <a id="id526" class="calibre1"/>main requirement. Note that CUDA is developed by NVIDIA and, consequently, it is only compatible with NVIDIA graphic cards. Besides, the model of the card has to be listed at <a class="calibre1" href="http://developer.nvidia.com/cuda-gpus">http://developer.nvidia.com/cuda-gpus</a>. The so-called <span class="strong"><strong class="calibre8">Compute Capability</strong></span> (<span class="strong"><strong class="calibre8">CC</strong></span>) can also be checked on this website as it will be needed later.</li><li class="listitem"><span class="strong"><strong class="calibre8">Microsoft Visual Studio</strong></span>: CUDA<a id="id527" class="calibre1"/> is compatible only with this Microsoft compiler. It is possible to install the Visual Studio Express edition, which is free. Note that Visual Studio 2013 is still not compatible with CUDA at the time of writing, so we are using Visual Studio 2012 in this book.</li><li class="listitem"><span class="strong"><strong class="calibre8">NVIDIA CUDA Toolkit</strong></span>: This <a id="id528" class="calibre1"/>includes a compiler for GPUs, libraries, tools, and documentation. This <a id="id529" class="calibre1"/>toolkit is available at <a class="calibre1" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>.</li><li class="listitem"><span class="strong"><strong class="calibre8">Qt library for Visual C++ compiler</strong></span>: In <a class="calibre1" title="Chapter 1. Getting Started" href="part0014_split_000.html#page">Chapter 1</a>, <span class="strong"><em class="calibre12">Getting Started</em></span>, the MinGW binaries of the<a id="id530" class="calibre1"/> Qt library were installed, but they are not compatible with the Visual C++ compiler. A compatible version can be downloaded using the package manager by means of the <code class="email">MaintenanceTool</code> application located in <code class="email">C:\Qt</code>. A good choice is the <code class="email">msvc2012</code> 32-bit component, as can be seen in the following screenshot. It is also necessary to update the <code class="email">Path</code> environment with the new location (for example, in our local system, it is <code class="email">C:\Qt\5.2.1\msvc2012\bin</code>). The Qt library is included in the compilation to take<a id="id531" class="calibre1"/> advantage of its user interface<a id="id532" class="calibre1"/> features.<div class="mediaobject"><img src="../images/00060.jpeg" alt="Setting up OpenCV with CUDA" class="calibre9"/><div class="caption"><p class="calibre13">Downloading a new version of the Qt libraries</p></div></div><p class="calibre14"> </p></li></ul></div></div>

<div class="book" title="Setting up OpenCV with CUDA">
<div class="book" title="Configuring the OpenCV build"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec27" class="calibre1"/>Configuring the OpenCV build</h2></div></div></div><p class="calibre7">The build configuration <a id="id533" class="calibre1"/>with CMake differs in some points from the typical <a id="id534" class="calibre1"/>one explained in the first chapter. These differences are explained as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">When you select the generator for the project, you have to choose the Visual Studio compiler version that corresponds to the installed environment in the machine. In our case, Visual Studio 11 is the correct compiler, as it corresponds to the version of the compiler included in Visual Studio 2012. The following screenshot shows this selection.</li><li class="listitem">In the selection of build options, we have to focus on the CUDA-related ones. If the installation of the CUDA toolkit was correct, CMake should automatically detect its location and activate the <code class="email">WITH_CUDA</code> option. In addition, the installation path of the toolkit is shown through <code class="email">CUDA_TOOLKIT_ROOT_DIR</code>. Another interesting option is <code class="email">CUDA_ARCH_BIN</code> because the compilation time can be significantly reduced if we just select the corresponding version of our GPU; otherwise, it will compile the code for all the architectures. As mentioned previously, the version can be checked at <a class="calibre1" href="http://developer.nvidia.com/cuda-gpus">http://developer.nvidia.com/cuda-gpus</a>. The following screenshot shows the options set in our build configuration:<div class="mediaobject"><img src="../images/00061.jpeg" alt="Configuring the OpenCV build" class="calibre9"/><div class="caption"><p class="calibre13">The CMake build configuration</p></div></div><p class="calibre14"> </p></li></ul></div></div></div>

<div class="book" title="Setting up OpenCV with CUDA">
<div class="book" title="Building and installing the library"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec28" class="calibre1"/>Building and installing the library</h2></div></div></div><p class="calibre7">CMake<a id="id535" class="calibre1"/> generates<a id="id536" class="calibre1"/> several Visual Studio projects in the target<a id="id537" class="calibre1"/> directory, <code class="email">ALL_BUILD</code> being the essential one. Once it is opened in Visual Studio, we can choose the build configuration (Debug or Release) as well as the architecture (Win32 or Win64). The compilation starts by pressing <span class="strong"><em class="calibre12">F7</em></span> or by clicking on <span class="strong"><strong class="calibre8">Build Solution</strong></span>. After the compilation has finished, it is recommended that you open and build the <code class="email">INSTALL</code> project as it generates an install directory with all the necessary files.</p><p class="calibre7">Finally, the <code class="email">Path</code> system needs to be updated with the location of the newly generated binaries. It is important to remove the previous location from the <code class="email">Path</code> variable and have only one version of the binaries in it.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note39" class="calibre1"/>Note</h3><p class="calibre7">Qt Creator<a id="id538" class="calibre1"/> should now find two compilers and two Qt versions: one for Visual C++ and one for MingGW. We have to choose the correct kit depending on the developed application when creating a new project. It is also possible to change the configuration of an existing project as kits are manageable.</p></div></div></div>

<div class="book" title="Setting up OpenCV with CUDA">
<div class="book" title="A quick recipe for setting up OpenCV with CUDA"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec29" class="calibre1"/>A quick recipe for setting up OpenCV with CUDA</h2></div></div></div><p class="calibre7">The installation <a id="id539" class="calibre1"/>process <a id="id540" class="calibre1"/>can be summarized in the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install Microsoft Visual Studio Express 2012.</li><li class="listitem" value="2">Download and install the NVIDIA CUDA Toolkit<a id="id541" class="calibre1"/> (available at <a class="calibre1" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>).</li><li class="listitem" value="3">Add the binaries for the Visual C++ compiler to the Qt installation and update the <code class="email">Path</code> system with the new location (for example, <code class="email">C:\Qt\5.2.1\msvc2012\bin</code>).</li><li class="listitem" value="4">Configure the OpenCV build with CMake. Set the <code class="email">WITH_CUDA</code>, <code class="email">CUDA_ARCH_BIN</code>, <code class="email">WITH_QT</code>, and <code class="email">BUILD_EXAMPLES</code> options.</li><li class="listitem" value="5">Open the <code class="email">ALL_BUILD</code> Visual Studio project and build it. Do the same operation with the <code class="email">INSTALL</code> project.</li><li class="listitem" value="6">Modify the <code class="email">Path</code> environment variable to update the OpenCV bin directory (for example, <code class="email">C:\opencv-buildCudaQt\install\x86\vc11\bin</code>).</li></ol><div class="calibre16"/></div></div></div>
<div class="book" title="Our first GPU-based program"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec66" class="calibre1"/>Our first GPU-based program</h1></div></div></div><p class="calibre7">In this section, we <a id="id542" class="calibre1"/>show two versions of the same program: one version uses the CPU to perform computations, and the other version uses the GPU. These two examples are called <code class="email">edgesCPU</code> and <code class="email">edgesGPU</code>, respectively, and allow us to point out the differences when using the <code class="email">GPU</code> module in OpenCV.</p><p class="calibre7">The <code class="email">edgesCPU</code> example is presented in the first place:</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp" 
using namespace cv;

int main(int argc, char** argv){
if ( argc &lt; 2 ){
        std::cout &lt;&lt; "Usage: ./edgesGPU &lt;image&gt;" &lt;&lt; std::endl;
        return -1;
    }
    Mat orig = imread(argv[1]);
    Mat gray, dst;

    bilateralFilter(orig,dst,-1,50,7);
    cvtColor(dst,gray,COLOR_BGR2GRAY);
    Canny(gray,gray,7,20);

    imshow("Canny Filter", gray);
    waitKey(0);

    return 0;
}</pre></div><p class="calibre7">Now the <code class="email">edgesGPU</code> <a id="id543" class="calibre1"/>example is shown as follows:</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
<span class="strong"><strong class="calibre8">#include &lt;opencv2/gpu/gpu.hpp&gt;</strong></span>
using namespace cv;

int main( int argc, char** argv){
  if ( argc &lt; 2 ){
        std::cout &lt;&lt; "Usage: ./edgesGPU &lt;image&gt;" &lt;&lt; std::endl;
        return -1;
    }
    Mat orig = imread(argv[1]);
<span class="strong"><strong class="calibre8">    gpu::GpuMat g_orig, g_gray, g_dst;</strong></span>
    //Transfer the image data to the GPU
<span class="strong"><strong class="calibre8">    g_orig.upload(orig);</strong></span>

<span class="strong"><strong class="calibre8">    gpu::bilateralFilter(g_orig,g_dst,-1,50,7);</strong></span>
<span class="strong"><strong class="calibre8">    gpu::cvtColor(g_dst,g_gray,COLOR_BGR2GRAY);</strong></span>
<span class="strong"><strong class="calibre8">    gpu::Canny(g_gray,g_gray,7,20);</strong></span>

    Mat dst;
    //Copy the image back to the CPU memory
<span class="strong"><strong class="calibre8">    g_gray.download(dst);</strong></span>
    imshow("Canny Filter", dst);
    waitKey(0);

    return 0;
}</pre></div><p class="calibre7">The <a id="id544" class="calibre1"/>explanation of the code is given as follows. There are several differences in the previous examples, although they ultimately obtain the same result, as shown in the following screenshot. A new header file is added as the new data type and different implementations of the algorithms are used. <code class="email">#include &lt;opencv2/gpu/gpu.hpp&gt;</code> contains the <code class="email">GpuMat</code> data type, which is the basic container that stores images in the GPU memory. It also includes the specific GPU versions of the filter algorithms used in the second example.</p><p class="calibre7">An important consideration is that we need to transfer the images between the CPU and the GPU. This is achieved with the <code class="email">g_orig.upload(orig)</code> and <code class="email">g_gray.download(dst)</code> methods. Once the image is uploaded to the GPU, we can apply different operations to it that are executed on the GPU. In order to distinguish the version of the algorithm that needs to run, the <code class="email">gpu</code> namespace is used as in  <code class="email">gpu::bilateralFilter</code>, <code class="email">gpu::cvtColor</code>, and <code class="email">gpu::Canny</code>. After the filters have been applied, the image is copied to the CPU memory again and displayed.</p><p class="calibre7">Regarding performance, the CPU version runs in 297 milliseconds, whereas the GPU version runs in just 18 milliseconds. In other words, the GPU version runs 16.5x faster.</p><div class="mediaobject"><img src="../images/00062.jpeg" alt="Our first GPU-based program" class="calibre9"/><div class="caption"><p class="calibre13">The output of the edgesCPU and edgesGPU examples</p></div></div><p class="calibre10"> </p></div>

<div id="page" style="height:0pt"/><div class="book" title="Going real time"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec67" class="calibre1"/>Going real time</h1></div></div></div><p class="calibre7">One of the main advantages of using the<a id="id545" class="calibre1"/> GPU to perform computations in images is that they are much faster. This increase in speed allows you to run heavy computational algorithms in real-time applications, such as stereo vision, pedestrian detection, or dense optical flow. In the next <code class="email">matchTemplateGPU</code> example, we show an application that matches a template in a video sequence:</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/features2d/features2d.hpp"
#include "opencv2/gpu/gpu.hpp"
#include "opencv2/nonfree/gpu.hpp"

using namespace std;
using namespace cv;

int main( int argc, char** argv )
{
    Mat img_template_cpu = imread( argv[1],IMREAD_GRAYSCALE);
    gpu::GpuMat img_template;
    img_template.upload(img_template_cpu);

    //Detect keypoints and compute descriptors of the template
<span class="strong"><strong class="calibre8">    gpu::SURF_GPU surf;</strong></span>
    gpu::GpuMat keypoints_template, descriptors_template;

    <span class="strong"><strong class="calibre8">surf(img_template,gpu::GpuMat(),keypoints_template, </strong></span>      <span class="strong"><strong class="calibre8">descriptors_template);</strong></span>

    //Matcher variables
<span class="strong"><strong class="calibre8">    gpu::BFMatcher_GPU matcher(NORM_L2);   </strong></span>

    //VideoCapture from the webcam
    gpu::GpuMat img_frame;
    gpu::GpuMat img_frame_gray;
    Mat img_frame_aux;
    VideoCapture cap;
    cap.open(0);
    if (!cap.isOpened()){
        cerr &lt;&lt; "cannot open camera" &lt;&lt; endl;
        return -1;
    }
    int nFrames = 0;
    uint64 totalTime = 0;
    //main loop
    for(;;){
<span class="strong"><strong class="calibre8">        int64 start = getTickCount();</strong></span>
        cap &gt;&gt; img_frame_aux;
        if (img_frame_aux.empty())
            break;
        img_frame.upload(img_frame_aux);
        cvtColor(img_frame,img_frame_gray, CV_BGR2GRAY);

        //Step 1: Detect keypoints and compute descriptors
        gpu::GpuMat keypoints_frame, descriptors_frame;
<span class="strong"><strong class="calibre8">        surf(img_frame_gray,gpu::GpuMat(),keypoints_frame, descriptors_frame);</strong></span>

        //Step 2: Match descriptors
        vector&lt;vector&lt;DMatch&gt;&gt;matches;<span class="strong"><strong class="calibre8">        matcher.knnMatch(descriptors_template,descriptors_frame,matches,2);</strong></span>

        //Step 3: Filter results
        vector&lt;DMatch&gt; good_matches;
        float ratioT = 0.7;
        for(int i = 0; i &lt; (int) matches.size(); i++)
        {
            if((matches[i][0].distance &lt; ratioT*(matches[i][1].distance)) &amp;&amp; ((int) matches[i].size()&lt;=2 &amp;&amp; (int) matches[i].size()&gt;0))
            {
                good_matches.push_back(matches[i][0]);
            }
        }
        // Step 4: Download results
        vector&lt;KeyPoint&gt; keypoints1, keypoints2;
        vector&lt;float&gt; descriptors1, descriptors2;
<span class="strong"><strong class="calibre8">        surf.downloadKeypoints(keypoints_template, keypoints1);</strong></span>
<span class="strong"><strong class="calibre8">        surf.downloadKeypoints(keypoints_frame, keypoints2);</strong></span>
<span class="strong"><strong class="calibre8">        surf.downloadDescriptors(descriptors_template, descriptors1);</strong></span>
<span class="strong"><strong class="calibre8">        surf.downloadDescriptors(descriptors_frame, descriptors2);</strong></span>

        //Draw the results
        Mat img_result_matches;
        drawMatches(img_template_cpu, keypoints1, img_frame_aux, keypoints2, good_matches, img_result_matches);
        imshow("Matching a template", img_result_matches);

<span class="strong"><strong class="calibre8">        int64 time_elapsed = getTickCount() - start;</strong></span>
<span class="strong"><strong class="calibre8">        double fps = getTickFrequency() / time_elapsed;</strong></span>
        totalTime += time_elapsed;
        nFrames++;
        cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt;endl;

        int key = waitKey(30);
        if (key == 27)
            break;;
    }
<span class="strong"><strong class="calibre8">    double meanFps = getTickFrequency() / (totalTime / nFrames);</strong></span>
    cout &lt;&lt; "Mean FPS: " &lt;&lt; meanFps &lt;&lt; endl;

    return 0;
}</pre></div><p class="calibre7">The explanation of the <a id="id546" class="calibre1"/>code is given as follows. As detailed in <a class="calibre1" title="Chapter 5. Focusing on the Interesting 2D Features" href="part0042_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre12">Focusing on the Interesting 2D Features</em></span>, features can be used to find the correspondence between two images. The template image, which is searched afterwards within every frame, is processed in the first place using the GPU version of SURF (<code class="email">gpu::SURF_GPU surf;</code>) to detect interest points and extract descriptors. This is accomplished by running <code class="email">surf(img_template,gpu::GpuMat(),keypoints_template, descriptors_template);</code>. The same process is performed for every frame taken from the video sequence. In order to match the descriptors of both images, a GPU version of the BruteForce matcher is also created with <code class="email">gpu::BFMatcher_GPU matcher(NORM_L2);</code>. An extra step is needed due to the fact that interest points and descriptors are stored in the GPU memory, and they need to be downloaded before we can show them. That's why <code class="email">surf.downloadKeypoints(keypoints, keypoints);</code> and <code class="email">surf.downloadDescriptors(descriptors, descriptors);</code> are executed. The following screenshot shows the example running:</p><div class="mediaobject"><img src="../images/00063.jpeg" alt="Going real time" class="calibre9"/><div class="caption"><p class="calibre13">Template matching using a webcam</p></div></div><p class="calibre10"> </p></div>

<div class="book" title="Going real time">
<div class="book" title="Performance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec30" class="calibre1"/>Performance</h2></div></div></div><p class="calibre7">The principal motivation for choosing GPU programming<a id="id547" class="calibre1"/> is performance. Therefore, this example includes time measurements to compare the speedups obtained with respect to the CPU version. Specifically, time is saved at the beginning of the main loop of the program by means of the <code class="email">getTickCount()</code> method. At the end of this loop, the same method is used as well as <code class="email">getTickFrequency</code>, which helps to calculate the FPS of the current frame. The time elapsed in each frame is accumulated, and at the end of the program, the mean is computed. The previous example has an average latency of 15 FPS, whereas the same example using CPU data types and algorithms achieves a mere 0.5 FPS. Both examples have been tested on the same hardware: a PC equipped with an i5-4570 processor and an NVIDIA GeForce GTX 750 graphics card. Obviously, a speed increment of 30x is significant, especially when we just need to change a few lines of code.</p></div></div>
<div class="book" title="Summary"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec68" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">In this chapter, we have covered two advanced modules of OpenCV: machine learning and GPU. Machine learning has the capability to learn computers to make decisions. For this, a classifier is trained and validated. This chapter provides three classification samples: KNN classifier, Random Forest using a <code class="email">.cvs</code> database, and SVM using an image database. The chapter also addresses the use of OpenCV with CUDA. GPUs have a growing role in intensive tasks because they can offload the CPU and run parallel tasks such as those encountered in computer vision algorithms. Several GPU examples have been provided: GPU module installation, a basic first GPU program, and real-time template matching.</p></div>
<div class="book" title="What else?"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec69" class="calibre1"/>What else?</h1></div></div></div><p class="calibre7">The GPU module now covers most of the functionalities of OpenCV; so, it is recommended that you explore the library and check which algorithms are available. In addition, the <code class="email">performance_gpu</code> program can be found at <code class="email">[opencv_build]/install/x86/vc11/samples/gpu]</code>, which shows the speedups of many OpenCV algorithms when using the GPU version.</p></div></body></html>