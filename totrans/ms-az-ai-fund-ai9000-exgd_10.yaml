- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Identify Features of Generative AI Solutions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定生成式人工智能解决方案的特点
- en: At last! This might be the most anticipated chapter of this book!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 终于！这可能是本书最令人期待的一章！
- en: Unless you’ve been hiding under a rock for the last year and a half, you’ve
    probably heard of **Generative AI** (sometimes called **GenAI**). It’s the technology
    that enables services such as ChatGPT to have natural-sounding conversations and
    produce semi-original content (we’ll get into that a little bit later in this
    chapter).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你在过去一年半的时间里一直躲在岩石下，否则你可能已经听说过**生成式人工智能**（有时称为**GenAI**）。这是使ChatGPT等服务能够进行自然对话并产生半原创内容的技术（我们将在本章稍后对此进行一些探讨）。
- en: Generative AI is exploding right now, so there’s no better time to become familiar
    with its uses and applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能目前正处于爆炸式增长阶段，因此现在是熟悉其用途和应用的最佳时机。
- en: 'The objectives and skills we’ll cover in this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖的目标和技能如下：
- en: What is Generative AI?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是生成式人工智能？
- en: Identify features of Generative AI models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定生成式人工智能模型的特点
- en: Identify common scenarios for Generative AI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定生成式人工智能的常见场景
- en: Identify Responsible AI considerations for Generative AI
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定生成式人工智能的责任人工智能考虑因素
- en: By the end of this chapter, you should be able to describe the various features
    of Generative AI, as well as articulate the importance of Responsible AI principles
    in conjunction with Generative AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该能够描述生成式人工智能的各种特性，以及阐述在生成式人工智能结合下责任人工智能原则的重要性。
- en: Let’s go!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is Generative AI?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是生成式人工智能？
- en: Back in [*Chapter 1*](B22207_01.xhtml#_idTextAnchor016), *Identify Features
    of Common AI Workloads*, we introduced some broad concepts around Generative AI.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B22207_01.xhtml#_idTextAnchor016)中，*确定常见人工智能工作负载的特点*，我们介绍了关于生成式人工智能的一些广泛概念。
- en: Generative AI represents one of the most exciting advancements in the field
    of AI, marking a significant shift from traditional AI systems, which are primarily
    designed to recognize patterns or make predictions based on input data (largely,
    statistical analysis and prediction). Instead, Generative AI focuses on creating
    new data instances that resemble the training data, not just in form but also
    in function. Generative AI is also useful in helping interpret data and can be
    used to identify patterns in content more quickly than traditional machine learning
    models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能代表了人工智能领域最令人兴奋的进步之一，标志着从传统人工智能系统的一个重大转变，传统人工智能系统主要是为了识别模式或根据输入数据（主要是统计分析预测）进行预测而设计的。相反，生成式人工智能专注于创建与训练数据相似的新数据实例，不仅在形式上，而且在功能上。生成式人工智能还有助于解释数据，并且可以比传统机器学习模型更快地识别内容中的模式。
- en: Generative AI applications leverage **large language models** (**LLMs**) for
    various **natural language processing** (**NLP**) tasks, such as sentiment analysis,
    text summarization, semantic similarity comparison between texts, and generating
    new text content. Despite the complexity of their mathematical foundations, understanding
    the architecture of LLMs can provide insights into their operational mechanisms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能应用利用**大型语言模型**（**LLMs**）进行各种**自然语言处理**（**NLP**）任务，例如情感分析、文本摘要、文本之间的语义相似性比较以及生成新的文本内容。尽管它们的数学基础复杂，但理解LLMs的架构可以提供对其操作机制的见解。
- en: Identify Features of Generative AI models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定生成式人工智能模型的特点
- en: In this section, we’ll dive a little more deeply into the features of generative
    AI models, including the foundation components that enable Generative AI capabilities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更深入地探讨生成式人工智能模型的特点，包括使生成式人工智能能力得以实现的基础组件。
- en: 'Generative AI models possess several distinct features that enable them to
    generate new content, predict outcomes, and learn from data in ways that mimic
    human creativity and intelligence. Here are some of the key features of Generative
    AI models:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能模型具有几个独特的特点，使它们能够生成新的内容、预测结果，并从数据中学习，其方式模仿人类的创造力和智慧。以下是生成式人工智能模型的一些关键特点：
- en: '**Content generation**: One of the hallmark features of Generative AI is its
    ability to create new data instances that resemble the original data. This includes
    generating text, images, audio, and video that are similar to, but not exact replicas
    of, the training data.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容生成**：生成式人工智能的一个标志性特点是它能够创建与原始数据相似的新数据实例。这包括生成与训练数据相似但不是精确复制品的文本、图像、音频和视频。'
- en: '**Learning data distributions**: Generative AI models are designed to understand
    and learn the underlying distribution of the data they are trained on. This allows
    them to produce outputs that are consistent with the real-world phenomena represented
    by the training data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习数据分布**：生成式AI模型旨在理解和学习它们训练所基于的数据的潜在分布。这使得它们能够产生与训练数据所代表的现实世界现象一致的输出。'
- en: '**Handling ambiguity and creativity**: These models can handle ambiguous inputs
    and produce diverse outputs, showcasing a form of artificial creativity that’s
    frequently managed through a feature called **temperature**. For instance, when
    asked to generate images of animals, a generative AI model can produce various
    images of different animals in different settings (some of which may be non-existent
    in real life). Similarly, you can instruct a generative AI model to render its
    output in the style of an author or artist (such as *commentary in the style of
    Mark Twain*, *a painting of a cat in the style of Vincent van Goh*, or *lyrics
    that match the tone and tempo of Whitney Houston’s ‘I Wanna Dance* *With Somebody’*).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理模糊性和创造性**：这些模型可以处理模糊的输入并产生多样化的输出，展示了通过名为**温度**的功能经常管理的一种形式的人工创造性。例如，当被要求生成动物图像时，一个生成式AI模型可以产生不同动物在不同环境中的各种图像（其中一些可能在实际生活中并不存在）。同样，你可以指示一个生成式AI模型以作者或艺术家的风格呈现其输出（例如，*以马克·吐温的风格进行的评论*，*梵高风格的猫的画作*，或*与惠特尼·休斯顿的《我想和某人跳舞》的节奏和调调相匹配的歌词*）。'
- en: '**Adaptability**: Generative AI models can be adapted to various domains and
    tasks, such as creating realistic human voices, designing new molecular structures
    for drugs, or generating code based on natural language descriptions.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：生成式AI模型可以适应各种领域和任务，例如创建逼真的人类声音、为药物设计新的分子结构或根据自然语言描述生成代码。'
- en: '**Unsupervised learning**: Many generative AI models can learn from data without
    explicit labels or annotations, which is known as unsupervised learning. This
    is particularly powerful for exploring large datasets where manual labeling is
    impractical.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：许多生成式AI模型可以在没有明确标签或注释的数据中学习，这被称为无监督学习。这对于探索大型数据集特别强大，因为手动标记是不切实际的。'
- en: '**Interpretability and control**: Advanced generative AI models offer mechanisms
    to control and interpret the generation process, allowing users to specify certain
    attributes or guide the model toward desired outcomes.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性和控制**：高级生成式AI模型提供了控制和解释生成过程的机制，使用户能够指定某些属性或引导模型向期望的结果发展。'
- en: '**Personalization**: Generative AI can tailor content to individual preferences
    or requirements, making it highly relevant for personalized recommendations, customized
    content creation, and targeted marketing strategies.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化**：生成式AI可以根据个人偏好或需求定制内容，这使得它在个性化推荐、定制内容创作和针对性营销策略方面高度相关。'
- en: '**Anomaly detection and data augmentation**: Despite being called *generative
    AI*, these models can identify unusual patterns in data (anomaly detection) and
    generate additional data points for training (data augmentation), enhancing the
    robustness and performance of other machine learning models.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测和数据增强**：尽管被称为*生成式AI*，但这些模型可以识别数据中的异常模式（异常检测）并为训练生成额外的数据点（数据增强），从而增强其他机器学习模型的鲁棒性和性能。'
- en: '**Multi-modality**: Some generative AI models are multi-modal, meaning they
    can understand and generate content across different forms of data, such as converting
    text descriptions into images or translating between different languages.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态**：一些生成式AI模型是多模态的，这意味着它们可以理解和生成不同形式的数据内容，例如将文本描述转换为图像或在不同语言之间进行翻译。'
- en: '**Iterative improvement**: Generative models can refine their outputs through
    iterative processes, where initial results are progressively improved based on
    feedback or additional input, leading to higher quality and more precise outputs.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代改进**：生成模型可以通过迭代过程改进其输出，其中初始结果根据反馈或额外输入逐步改进，从而产生更高质量和更精确的输出。'
- en: Now that you’ve seen some of the things LLMs and generative AI models can do,
    let’s look specifically at what makes generative AI models such as ChatGPT or
    **Bidirectional Encoder Representations from Transformers** (**BERT**) so unique.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了一些LLMs和生成式AI模型能做的事情，让我们具体看看是什么使得生成式AI模型，如ChatGPT或**双向编码器表示从Transformer**（**BERT**）如此独特。
- en: What’s a transformer model and how does it work?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Transformer模型以及它是如何工作的？
- en: Over the years, machine learning models dedicated to NLP have significantly
    evolved, leading to the advent of advanced LLMs based on transformer architecture.
    This architecture enhances previous techniques that are used for vocabulary modeling
    in NLP tasks, especially in terms of language generation. Transformers are trained
    on extensive text corpora (hence the terminology *LLM*), allowing them to understand
    semantic relationships between words and predict logical text sequences. With
    a comprehensive vocabulary, these models can produce responses nearly indistinguishable
    from those of actual people.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，专注于NLP的机器学习模型已经显著发展，导致了基于变压器架构的高级LLM的出现。这种架构增强了之前用于NLP任务中词汇建模的技术，特别是在语言生成方面。变压器在大量的文本语料库上训练（因此有*LLM*这一术语），使它们能够理解单词之间的语义关系并预测逻辑文本序列。拥有全面的词汇，这些模型可以产生几乎无法与实际人类区分的响应。
- en: 'The transformer model architecture is fundamentally composed of two main parts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型架构在本质上由两个主要部分组成：
- en: '**Encoder block**: This component is responsible for creating semantic representations
    of words in the training vocabulary, capturing the context and meaning of each
    word within a given sequence'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器块**：该组件负责创建训练词汇中单词的语义表示，捕捉给定序列中每个单词的上下文和含义'
- en: '**Decoder block**: This part focuses on generating new sequences of language
    based on the semantic representations prepared by the encoder'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器块**：这部分主要关注基于编码器准备好的语义表示生成新的语言序列'
- en: Different implementations of transformer architecture may emphasize different
    components. For instance, Google’s **BERT** model, which is designed to enhance
    search engine results, primarily utilizes the encoder block. Conversely, OpenAI’s
    **Generative Pretrained Transformer** (**GPT**) model, aimed at generating human-like
    text, relies mainly on the decoder block.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构的不同实现可能强调不同的组件。例如，谷歌的**BERT**模型，旨在增强搜索引擎结果，主要利用编码器块。相反，OpenAI的**生成预训练变压器**（**GPT**）模型，旨在生成类似人类的文本，主要依赖于解码器块。
- en: While delving into all details of transformer models might be complex, understanding
    these fundamental elements offers a glimpse into how they underpin generative
    AI capabilities, enabling the creation of sophisticated and coherent language
    outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深入研究变压器模型的全部细节可能很复杂，但理解这些基本元素可以一窥它们如何支撑生成式AI的能力，使创建复杂且连贯的语言输出成为可能。
- en: While several processes and functions go into how these generative AI models
    work, they usually share some common concepts. We’ll cover these in the following
    sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些生成式AI模型的工作方式涉及多个过程和功能，但它们通常共享一些共同的概念。我们将在以下章节中介绍这些概念。
- en: Tokenization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: '**Tokenization** is a crucial preprocessing step in the workflow of LLMs and
    Generative AI where text data is broken down into smaller units called **tokens**.
    These tokens can be words, subwords, or even characters, depending on the model’s
    design and the granularity needed for the task. The process of tokenization allows
    models to efficiently process and understand the input text by analyzing it piece
    by piece, laying the foundation for further NLP tasks, such as language generation,
    translation, or sentiment analysis.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**是LLM和生成式AI工作流程中的关键预处理步骤，其中文本数据被分解成称为**标记**的更小单元。这些标记可以是单词、子词，甚至字符，具体取决于模型的设计和任务所需的粒度。分词过程允许模型通过逐个分析文本来高效地处理和理解输入文本，为后续的NLP任务，如语言生成、翻译或情感分析，奠定基础。'
- en: For LLMs and Generative AI, tokenization not only simplifies the complexity
    of the input text but also helps in capturing the context and semantics of the
    language. By breaking down the text into manageable units or chunks, the model
    can learn the relationships and patterns within the language, which is essential
    for generating coherent and contextually relevant text outputs that mimic human
    output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM和生成式AI，分词不仅简化了输入文本的复杂性，还有助于捕捉语言的语言和语义。通过将文本分解成可管理的单元或块，模型可以学习语言内部的关系和模式，这对于生成连贯且与上下文相关的文本输出，模仿人类输出至关重要。
- en: 'Take, for example, the following sentence:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下句子为例：
- en: '*Tokenization is essential* *for NLP.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词对于NLP至关重要* *。'
- en: In a simple word-based tokenization approach, this might be broken down into
    “Tokenization”, “is”, “essential”, “for”, “NLP”, and “.” as tokens. Each token
    then serves as an input for the LLM, which processes these tokens to understand
    the sentence’s structure and meaning. In more advanced models, such as those using
    subword tokenization, the word *Tokenization* might be further split into smaller
    tokens such as *Token* and *ization* to capture more granular linguistic features
    and handle unknown words or neologisms more effectively.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的基于单词的分词方法中，这可能会被分解为“Tokenization”，“is”，“essential”，“for”，“NLP”和“.”作为标记。每个标记然后作为LLM的输入，LLM处理这些标记以理解句子的结构和意义。在更高级的模型中，例如使用子词分词的模型，单词“Tokenization”可能会进一步分解为更小的标记，如“Token”和“ization”，以捕捉更细粒度的语言特征并更有效地处理未知单词或新词。
- en: This tokenized input enables LLMs to perform a wide range of Generative AI tasks,
    from completing sentences in a way that mimics human writing to translating sentences
    into different languages while preserving their original meaning. Through tokenization,
    LLMs can effectively navigate the complexities of human language, making it a
    foundational step in the world of NLP and Generative AI.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词输入使LLMs能够执行广泛的生成式AI任务，从以模仿人类写作的方式完成句子到将句子翻译成不同的语言同时保留其原始意义。通过分词，LLMs可以有效地导航人类语言的复杂性，使其成为NLP和生成式AI世界中的基础步骤。
- en: Embeddings
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Embeddings
- en: In the context of LLMs and Generative AI, **embeddings** are high-dimensional
    vectors that are used to represent the tokens that are obtained from the tokenization
    process. These vectors capture the semantic and syntactic features of the words,
    allowing the model to understand the relationships between different words and
    their context within the text. The process of creating embeddings involves mapping
    each unique token to a point in a geometric space, where the distance and direction
    between points reflect the linguistic and contextual relationships between the
    words.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs和生成式AI的背景下，**embeddings**是用于表示从分词过程中获得的标记的高维向量。这些向量捕捉了单词的语义和句法特征，使模型能够理解不同单词及其在文本中的关系。创建embeddings的过程涉及将每个唯一的标记映射到几何空间中的一个点，其中点之间的距离和方向反映了单词之间的语言和上下文关系。
- en: The process of generating embeddings allows LLMs to capture complex relationships
    between tokens, such as similarity, difference, and contextuality. For instance,
    words that appear in similar contexts tend to have closer embeddings in the vector
    space, which helps the model in tasks such as word prediction, sentence generation,
    and semantic analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成embeddings的过程允许LLMs捕捉标记之间的复杂关系，例如相似性、差异性和上下文性。例如，在相似上下文中出现的单词往往在向量空间中具有更接近的embeddings，这有助于模型在单词预测、句子生成和语义分析等任务中。
- en: For example, taking our previously tokenized sentence, *Tokenization is essential
    for NLP*, each word (token) would be converted into an embedding – a **vector**
    (or coordinates) of real numbers. These embeddings would be used by the LLM to
    understand the sentence’s meaning and context. For instance, the model might learn
    that *Tokenization* and *NLP* are closely related concepts in the field of NLP,
    and their embeddings would be positioned closer in the vector space compared to
    unrelated words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以我们之前分词的句子“Tokenization is essential for NLP”为例，每个单词（标记）将被转换为embeddings –
    一个实数**向量**（或坐标）。这些embeddings将被LLM用于理解句子的意义和上下文。例如，模型可能会学习到“Tokenization”和“NLP”在NLP领域是密切相关的概念，它们的embeddings在向量空间中的位置会比无关的单词更接近。
- en: 'Let’s say the tokens were converted into the following vectors:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设标记被转换为以下向量：
- en: '| **Token ID** | **Token Value** | **X** | **Y** | **Z** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **Token ID** | **Token Value** | **X** | **Y** | **Z** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Tokenization | 8 | 7 | 9 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Tokenization | 8 | 7 | 9 |'
- en: '| 2 | Is | -2 | 4 | 3 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Is | -2 | 4 | 3 |'
- en: '| 3 | Essential | -5 | -5 | -9 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Essential | -5 | -5 | -9 |'
- en: '| 4 | For | -7 | -7 | 0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | For | -7 | -7 | 0 |'
- en: '| 5 | NLP | 8 | 7 | 10 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 5 | NLP | 8 | 7 | 10 |'
- en: Table 10.1 – Tokens represented by coordinates
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 – 由坐标表示的标记
- en: 'In *Table 10.1*, each token exists in one dimension or plane (*X*, *Y*, and
    *Z*). You can think of embeddings as representing the tokens in a three-dimensional
    graph:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在*表10.1*中，每个标记存在于一个维度或平面上（*X*，*Y*和*Z*）。你可以将embeddings视为在三维图上表示标记：
- en: '![Figure 10.1 – Tokens plotted on a 3D graph](img/B22207_10_01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 在3D图上绘制的标记](img/B22207_10_01.jpg)'
- en: Figure 10.1 – Tokens plotted on a 3D graph
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 在3D图上绘制的标记
- en: These embeddings are learned while the language model is being trained on a
    large corpus of text. The model learns to place semantically similar tokens closer
    together in the embedding space. For example, *Tokenization* and *NLP* might be
    closer to each other than to *is* because they are related to language processing
    concepts, while *is* is a more general verb.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入是在语言模型在大量文本语料库上训练时学习的。模型学会在嵌入空间中将语义上相似的标记放置得更近。例如，*分词*和*NLP*可能比*is*更接近，因为它们与语言处理概念相关，而*is*是一个更通用的动词。
- en: Attention
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力
- en: The concept of **attention** in generative AI and LLMs represents a significant
    advancement in how models process and understand sequences of data, such as text.
    Attention mechanisms allow models to focus on different parts of the input data
    when performing a task, much like how human attention focuses on specific aspects
    of what we see or hear to derive meaning or make decisions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI和LLM中，**注意力**的概念代表了模型处理和理解数据序列（如文本）方式的一个重大进步。注意力机制允许模型在执行任务时关注输入数据的不同部分，就像人类的注意力集中在我们所看到或听到的特定方面以推导意义或做出决定一样。
- en: The attention mechanism dynamically weighs the importance of different tokens
    in a sentence when generating an output. This means the model can pay more attention
    to relevant words and less to others, depending on the task at hand, such as translation,
    question-answering, or text generation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在生成输出时动态地权衡句子中不同标记的重要性。这意味着模型可以根据手头的任务（如翻译、问答或文本生成）更多地关注相关单词，而对其他单词的关注较少。
- en: Let’s go back to our example sentence, *Tokenization is essential for NLP*.
    When processing this sentence, an LLM with an attention mechanism might focus
    more on the words *Tokenization* and *NLP* because they are key terms that define
    the context and subject matter. The model recognizes that *essential* is important
    as it describes the relationship between *Tokenization* and *NLP* but might pay
    less attention to *is* and *for* as these serve more grammatical functions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的例子句子，*分词对于NLP至关重要*。在处理这个句子时，具有注意力机制的LLM可能会更多地关注*分词*和*NLP*这两个词，因为它们是定义上下文和主题的关键术语。模型认识到*至关重要*很重要，因为它描述了*分词*和*NLP*之间的关系，但可能对*is*和*for*的关注较少，因为它们更多地承担语法功能。
- en: The attention mechanism has been a cornerstone of transformer-based models,
    enabling breakthroughs in NLP applications by providing a more nuanced and flexible
    way to handle sequences of data. This has led to the development of highly effective
    models that are capable of understanding and generating human-like text.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是基于transformer的模型的基础，通过提供一种更细腻和灵活的方式来处理数据序列，从而在NLP应用中实现了突破。这导致了高度有效的模型的发展，这些模型能够理解和生成类似人类的文本。
- en: What’s an attention score?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是注意力分数？
- en: '**Attention scores** are used when identifying the relative importance or weight
    of tokens in attention layers or attention mechanisms. The weight or importance
    of a token influences its relevance when making predictions. In models using **multi-head**
    attention, the inputs are transformed multiple times and multiple attention scores
    are computed, capturing different relationships in the data.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力分数**在识别注意力层或注意力机制中标记的相对重要性或权重时使用。标记的权重或重要性会影响其在预测中的相关性。在采用**多头**注意力的模型中，输入被多次转换，并计算多个注意力分数，捕捉数据中的不同关系。'
- en: While *Figure 10**.1* helps visualize the concept of embeddings, in the real
    world, each token is represented as a vector with hundreds or thousands of dimensions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*图10.1*有助于可视化嵌入的概念，但在现实世界中，每个标记都表示为一个具有数百或数千维的向量。
- en: 'The overall process looks something like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程看起来可能像这样：
- en: The token embeddings (the token and its numeric vector or coordinates) are fed,
    in sequence, to the attention layer.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记嵌入（标记及其数值向量或坐标）按顺序输入到注意力层。
- en: The decoder begins predicting the next token and vector in the sequence.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器开始预测序列中的下一个标记和向量。
- en: The attention layer evaluates the sequence and assigns a weight to the tokens.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力层评估序列并为标记分配权重。
- en: The weights are then used to calculate a new vector and an **attention score**
    for the next token. In systems with **multi-head** attention, the attention layer
    uses different elements in the embeddings to calculate multiple alternative tokens.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用这些权重来计算新的向量和下一个标记的**注意力分数**。在具有**多头**注意力的系统中，注意力层使用嵌入中的不同元素来计算多个替代标记。
- en: The neural network uses the attention scores to predict the most likely next
    token from its entire vocabulary (acquired through the training process).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络使用注意力分数来预测其整个词汇表中最可能出现的下一个标记（通过训练过程获得）。
- en: The predicted output is added to the sequence, which, in turn, is used as the
    next input, starting the process over again at *Step 1*.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测的输出被添加到序列中，反过来，这个序列又作为下一个输入，从*步骤1*重新开始这个过程。
- en: Just like other machine learning styles, generative AI relies on a training
    process where it is provided content. The predicted token values, based on the
    attention scores, and vectors are compared to the actual values of the next vector,
    and the loss is calculated. Like other automated machine learning models, weights
    are dynamically adjusted to reduce the loss, allowing the model to more accurately
    generate its predictions (which, in the case of Generative AI, is the response
    to the prompt).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他机器学习风格一样，生成式人工智能依赖于一个训练过程，在这个过程中，它被提供内容。基于注意力分数的预测标记值和向量与下一个向量的实际值进行比较，并计算损失。与其他自动机器学习模型一样，权重会动态调整以减少损失，从而使模型能够更准确地生成其预测（在生成式人工智能的情况下，是对于提示的响应）。
- en: How does generative AI put all this together?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式人工智能是如何将这些内容整合在一起的？
- en: Generative AI harnesses deep learning techniques, particularly **generative
    adversarial networks** (**GANs**) and **variational autoencoders**, to produce
    content that is not only novel but also realistic and contextually relevant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能利用深度学习技术，特别是**生成对抗网络**（GANs）和**变分自编码器**，来生成既新颖又真实且与上下文相关的内容。
- en: What’s a GAN?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是GAN？
- en: 'GANs on the concepts of neural networks. A GAN consists of two competing neural
    network models: a **generator** and a **discriminator**. The generator’s role
    is to create data that is similar to data in a training set, while the discriminator’s
    role is to distinguish between genuine data from the training set and fake or
    artificial data produced by the generator. During training, these two networks
    engage in a kind of tug-of-war; the generator continuously improves its ability
    to produce realistic data, while the discriminator improves its ability to detect
    the generated data. This process continues until the generator produces data so
    convincingly real that the discriminator can no longer distinguish it from actual
    data. This adversarial process enables GANs to generate high-quality, realistic
    data, mimicking the distribution of the original dataset.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GAN基于神经网络的概念。一个GAN由两个相互竞争的神经网络模型组成：一个**生成器**和一个**判别器**。生成器的角色是创建与训练集中数据相似的数据，而判别器的角色是区分来自训练集的真实数据和由生成器产生的虚假或人工数据。在训练过程中，这两个网络进行一种拔河比赛；生成器不断改进其生成逼真数据的能力，而判别器则改进其检测生成数据的能力。这个过程一直持续到生成器产生的数据如此逼真，以至于判别器无法再将其与实际数据区分开来。这种对抗过程使GAN能够生成高质量、逼真的数据，模仿原始数据集的分布。
- en: At the heart of generative AI is the ability to understand and replicate the
    complexities of human creativity. By analyzing vast amounts of data – whether
    it’s text, images, sounds, or videos – generative AI algorithms learn the underlying
    patterns, styles, and structures. They then use this understanding to generate
    new content that can be indistinguishable from content created by humans. This
    capability opens up a myriad of applications, from composing music and writing
    stories to creating realistic visuals and simulating virtual environments.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能的核心能力在于理解和复制人类创造力的复杂性。通过分析大量数据——无论是文本、图像、声音还是视频——生成式人工智能算法学习其背后的模式、风格和结构。然后，它们利用这种理解来生成新的内容，这些内容可以与人类创造的内容难以区分。这种能力为音乐创作、故事撰写、创建逼真视觉效果和模拟虚拟环境等众多应用打开了大门。
- en: Further exploration
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步探索
- en: You can experiment with some popular generative AI services right now, such
    as OpenAI’s GPT-4 ([https://chat.openai.com](https://chat.openai.com)) and Midjourney
    ([https://www.midjourney.com](https://www.midjourney.com)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在就可以尝试一些流行的生成式人工智能服务，例如OpenAI的GPT-4 ([https://chat.openai.com](https://chat.openai.com))
    和Midjourney ([https://www.midjourney.com](https://www.midjourney.com))。
- en: Azure generative AI is Microsoft’s foray into this revolutionary technology,
    providing tools and services that leverage the Azure AI ecosystem. This platform
    enables developers and businesses to integrate generative AI capabilities into
    their applications, creating a bridge between human creativity and machine efficiency.
    With Azure AI, users can harness the power of generative models to produce high-quality,
    innovative content across various domains, significantly reducing the time and
    effort traditionally required for content creation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Azure生成式AI是微软进入这一革命性技术的尝试，提供利用Azure AI生态系统的工具和服务。这个平台使开发者和企业能够将生成式AI功能集成到他们的应用程序中，在人类创造力和机器效率之间架起一座桥梁。借助Azure
    AI，用户可以利用生成模型的力量，在各个领域产生高质量、创新的内容，显著减少传统内容创作所需的时间和精力。
- en: One of the standout features of Azure generative AI is its capacity for generating
    realistic images. Utilizing GANs and other state-of-the-art techniques, the platform
    can produce images that closely mimic real-life scenarios. These capabilities
    find applications in numerous industries, such as augmenting datasets for more
    effective machine learning training, generating product images for eCommerce platforms,
    and crafting detailed graphics for gaming and virtual reality experiences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Azure生成式AI的一个显著特点是生成逼真图像的能力。利用GAN和其他最先进的技术，该平台可以生成与真实场景非常接近的图像。这些能力在多个行业中都有应用，例如增强数据集以更有效地进行机器学习训练、为电子商务平台生成产品图像以及为游戏和虚拟现实体验制作详细图形。
- en: How does a GAN create images?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是如何创建图像的？
- en: Whether it’s processing text or images, a GAN employs neural networks in both
    the generator and discriminator roles. Let’s say you trained the generator with
    several images of cats. The generator’s job is to produce an image of a cat by
    using the source material as inspiration (if you will), and then inserting some
    random content (noise) that may help the picture look like a cat. However, it
    won’t be actual cat image data. Then, operating as a binary classifier, the discriminator
    takes those images from the generator and determines if they look like a cat or
    not.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是处理文本还是图像，GAN在生成器和判别器角色中都使用神经网络。假设你用几只猫的图像训练了生成器。生成器的任务是利用源材料作为灵感（如果可以这样说的話），然后插入一些可能有助于使图片看起来像猫的随机内容（噪声）。然而，这不会是实际的猫图像数据。然后，作为二元分类器，判别器从生成器那里获取这些图像，并确定它们是否看起来像猫。
- en: In addition to visual content, Azure generative AI excels in text generation.
    By training on extensive text datasets, the platform can produce written content
    that mirrors human writing styles. This is particularly useful for generating
    narrative content, automating customer service responses, or creating informative
    text for websites and applications. The technology ensures that the generated
    text is coherent, contextually appropriate, and varied in style, further mimicking
    human language patterns.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了视觉内容，Azure生成式AI在文本生成方面表现出色。通过在大量的文本数据集上进行训练，该平台可以生成模仿人类写作风格的书面内容。这对于生成叙事内容、自动化客户服务响应或为网站和应用程序创建信息性文本特别有用。这项技术确保生成的文本在风格上连贯、上下文适当且多样化，进一步模仿人类语言模式。
- en: Generative AI, particularly as it’s implemented in Azure AI, represents a significant
    leap forward in how we approach content creation and data synthesis. By automating
    the creative process, it offers the potential to revolutionize industries, streamline
    workflows, and unleash new levels of creativity and innovation. As this technology
    continues to evolve, it will undoubtedly become an integral part of the digital
    transformation journey for many organizations worldwide, redefining what is possible
    with artificial intelligence.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI，尤其是在Azure AI中实现的形式，在内容创作和数据合成方面取得了重大进步。通过自动化创作过程，它有可能彻底改变行业，简化工作流程，并释放新的创造力和创新水平。随着这项技术的不断发展，它无疑将成为许多全球组织数字化转型旅程中不可或缺的一部分，重新定义人工智能的可行性。
- en: Identify common scenarios for generative AI
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别生成式AI的常见场景
- en: Generative AI, with its ability to create new content, has applications spanning
    numerous fields. Here are some common scenarios where generative AI is making
    a significant impact, along with examples for each.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI，凭借其创建新内容的能力，在众多领域都有应用。以下是一些生成式AI产生重大影响的常见场景，以及每个场景的示例。
- en: Image generation
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像生成
- en: 'Multimodal generative AI can create new images from textual descriptions, such
    as generating photorealistic images of objects or scenes that don’t exist, using
    models such as OpenAI’s DALL-E or Midjourney:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态生成式AI可以从文本描述中创建新的图像，例如使用OpenAI的DALL-E或Midjourney等模型生成不存在物体的或场景的逼真图像：
- en: '![Figure 10.2 – Generating an image with GPT4](img/B22207_10_02.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 使用GPT4生成图像](img/B22207_10_02.jpg)'
- en: Figure 10.2 – Generating an image with GPT4
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 使用GPT4生成图像
- en: Text generation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本生成
- en: 'Generative AI can be used to produce coherent and contextually relevant text
    for articles or stories by utilizing models such as GPT-3 from OpenAI:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用OpenAI的GPT-3等模型，生成式AI可以用于生成用于文章或故事的连贯且上下文相关的文本：
- en: '![Figure 10.3 – Text generation using GPT-3.5](img/B22207_10_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 使用GPT-3.5进行文本生成](img/B22207_10_03.jpg)'
- en: Figure 10.3 – Text generation using GPT-3.5
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 使用GPT-3.5进行文本生成
- en: Music creation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音乐创作
- en: 'Generative AI can compose new music pieces or songs in various genres by learning
    from a vast dataset of music, exemplified by projects such as OpenAI’s Jukebox:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习大量音乐数据集，生成式AI可以创作各种流派的新音乐作品或歌曲，例如OpenAI的Jukebox项目所示：
- en: '![Figure 10.4 – Songs generated with OpenAI Jukebox](img/B22207_10_04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 使用OpenAI Jukebox生成的歌曲](img/B22207_10_04.jpg)'
- en: Figure 10.4 – Songs generated with OpenAI Jukebox
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 使用OpenAI Jukebox生成的歌曲
- en: Synthetic data generation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成数据生成
- en: Obtaining personal data for model training can be very difficult. Generative
    AI can help fill the gap by creating realistic but artificial datasets that mimic
    the statistical properties of sensitive real-world data, enabling the development
    of machine learning models without compromising privacy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 获取用于模型训练的个人数据可能非常困难。生成式AI可以通过创建模仿敏感真实世界数据统计特性的真实但人工数据集来填补这一空白，从而在不损害隐私的情况下开发机器学习模型。
- en: Code generation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码生成
- en: 'Generative AI has begun growing in terms of development and can generate syntactically
    correct code (in many cases), as shown in *Figure 10**.5*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI在开发方面开始增长，可以生成语法正确的代码（在许多情况下），如图*10**.5*所示：
- en: '![Figure 10.5 – JSON content generated by GPT-3.5](img/B22207_10_05.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – GPT-3.5生成的JSON内容](img/B22207_10_05.jpg)'
- en: Figure 10.5 – JSON content generated by GPT-3.5
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – GPT-3.5生成的JSON内容
- en: Another growing area is using Generative AI to review code, make suggestions,
    or recommend improvements.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个增长领域是使用生成式AI来审查代码、提出建议或推荐改进。
- en: Voice generation and transformation
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音生成和转换
- en: Generative AI can be used to convert text into lifelike speech in various languages
    and accents for applications such as audiobooks or virtual assistants via technologies
    such as Google’s WaveNet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI可以用于将文本转换为各种语言和口音的逼真语音，用于有声读物或虚拟助手等应用，例如通过Google的WaveNet等技术。
- en: Drug discovery and chemical synthesis
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 药物发现和化学合成
- en: It can accelerate the discovery of new pharmaceuticals by predicting molecular
    structures that could lead to effective drugs, as seen in the work of startups
    such as Atomwise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以通过预测可能导致有效药物的分子的结构来加速新药发现的发现，如Atomwise等初创公司的工作所示。
- en: Personalized content and recommendation systems
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个性化内容和推荐系统
- en: Generative AI can tailor digital experiences to individual users by generating
    personalized content or product recommendations on platforms such as Netflix or
    Amazon.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI可以通过在Netflix或Amazon等平台上生成个性化内容或产品推荐来为个人用户定制数字体验。
- en: Maintenance analysis
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护分析
- en: By reviewing the **Internet of Things** (**IoT**) and other sensor data, generative
    AI can detect patterns that indicate potential failure points or manufacturing
    device maintenance requirements.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过审查**物联网**（**IoT**）和其他传感器数据，生成式AI可以检测出潜在故障点或制造设备维护需求的模式。
- en: Copilots
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协作者
- en: The advent of LLMs has given rise to **copilots**, novel tools that are designed
    to assist users with common tasks through generative AI models integrated into
    various applications. These copilots, built on a unified architecture, enable
    developers to create tailored solutions for specific business needs, appearing
    as features such as chat screens alongside user files, utilizing the content created
    or searched within the product to generate relevant results.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的出现，**协作助手**应运而生，这些新型工具旨在通过集成到各种应用中的生成式AI模型帮助用户完成常见任务。这些协作助手基于统一架构，使开发者能够为特定业务需求创建定制解决方案，以聊天界面等特征的形式出现在用户文件旁边，利用产品中创建或搜索的内容生成相关结果。
- en: The development process involves training LLMs with extensive data and utilizing
    services such as Azure OpenAI Service for accessing pretrained models, which can
    be deployed as-is or fine-tuned with custom data for specific applications. Copilots
    offer significant enhancements to productivity and creativity, aiding in tasks
    ranging from drafting documents to strategic planning, marking a transformative
    shift in how tasks are approached and executed in the digital workspace. One of
    the most prominent copilots currently is **Microsoft Copilot**, integrated into
    the Microsoft 365 suite of applications.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 开发过程涉及使用大量数据训练LLM，并利用Azure OpenAI服务之类的服务来访问预训练模型，这些模型可以原样部署或使用定制数据针对特定应用进行微调。协作伙伴提供了对生产力和创造力的显著提升，帮助完成从起草文件到战略规划的各种任务，标志着在数字工作空间中处理和执行任务方式的变革性转变。目前最突出的协作伙伴之一是**微软协作伙伴**，它集成在微软365应用程序套件中。
- en: Generative AI’s capabilities extend beyond these examples, touching on areas
    such as fashion design, where AI generates new clothing styles, and gaming, where
    AI creates dynamic, evolving environments. As technology advances, the potential
    applications of generative AI continue to expand, promising to revolutionize industries
    by offering novel solutions to complex problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的能力不仅限于这些例子，还触及了时尚设计等领域，AI可以生成新的服装风格，以及游戏领域，AI可以创建动态、不断发展的环境。随着技术的进步，生成式AI的潜在应用继续扩展，通过为复杂问题提供新颖的解决方案，有望彻底改变行业。
- en: Deepfake creation and detection
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度伪造的创建和检测
- en: While the scenarios we’ve looked at so far have been positive use cases for
    generative AI, a new type of tooling is emerging that has a large negative potential.
    This technology, commonly called deepfakes, revolves around generating or altering
    video and audio recordings to realistically depict content that was not originally
    said or performed by the individuals involved. From political propaganda to revenge
    pornography, generative AI’s role in creating deepfakes has already raised many
    ethical concerns (and in some cases, resulted in court battles ranging from child
    custody disputes to automobile collisions).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管到目前为止我们所探讨的场景都是生成式AI的积极用例，但一种新型工具正在出现，它具有很大的负面影响潜力。这种技术通常被称为深度伪造，它围绕生成或修改视频和音频记录，以真实地描绘出涉及的个人原本并未说过或表演的内容。从政治宣传到报复色情，生成式AI在创建深度伪造中的作用已经引发了众多伦理担忧（在某些情况下，甚至导致了从儿童监护权争议到汽车碰撞的法庭之战）。
- en: 'Deepfakes rely on deep learning techniques and can be seen with tools such
    as Jiggy, MyHeritage, and DeepFaceLab, as shown in *Figure 10**.6*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造依赖于深度学习技术，可以使用Jiggy、MyHeritage和DeepFaceLab等工具进行识别，如*图10.6*所示。6：
- en: '![Figure 10.6 – MyHeritage customizes portraits with a historical flair](img/B22207_10_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6 – MyHeritage以历史风格定制肖像](img/B22207_10_06.jpg)'
- en: Figure 10.6 – MyHeritage customizes portraits with a historical flair
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.6 – MyHeritage以历史风格定制肖像*'
- en: Conversely, similar technology is used to detect such manipulations to ensure
    authenticity and combat misinformation via platforms such as Sentinel (not to
    be confused with Microsoft Sentinel), WeVerify, and Microsoft Video Authenticator.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，类似的技术被用于检测此类操纵以确保真实性，并通过Sentinel（不要与微软Sentinel混淆）、WeVerify和微软视频验证器等平台来对抗虚假信息。
- en: Quality control
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量控制
- en: Generative AI has potential in the quality control space as well as it can analyze
    vast quantities of data and detect anomalies to predict potential defects. By
    being connected to a steady stream of production data, generative AI can be used
    to predict failure points and defects by identifying correlations between different
    types of anomalous activity and poor-quality product outputs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI在质量控制领域也有潜力，因为它可以分析大量数据并检测异常以预测潜在缺陷。通过连接到稳定的生产数据流，生成式AI可以通过识别不同类型异常活动与低质量产品输出之间的相关性来预测故障点和缺陷。
- en: In terms of machine learning and human interaction technologies, generative
    AI showcases some of the most exciting and significant capabilities. However,
    with that come some of the most significant risks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和人机交互技术方面，生成式AI展示了最激动人心和最显著的能力。然而，随之而来的是一些最重大的风险。
- en: Identify Responsible AI considerations for generative AI
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定生成式AI的负责任AI考虑因素
- en: 'Microsoft has created a framework for responsible AI and generative AI solutions
    comprised of four stages:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 微软为负责任的AI和生成式AI解决方案创建了一个框架，包括四个阶段：
- en: Identify potential harms that could be related to your planned solution
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定与您计划中的解决方案可能相关的潜在危害
- en: Measure the presence of those identified harms in the solution’s output
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量解决方案输出中那些已识别危害的存在
- en: Mitigate the harms at multiple levels to minimize their expression and impact
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个层面上减轻危害，以最大限度地减少其表达和影响
- en: Operate the solution responsibly
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任地运营解决方案
- en: Let’s look at each of those four areas.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一审视这四个领域。
- en: Identify
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别
- en: The first stage in implementing a responsible generative AI solution is to identify
    potential harms that may result from your solution.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 实施负责任的生成式AI解决方案的第一步是确定可能由您的解决方案导致的潜在危害。
- en: Identifying potential harms or risks
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别潜在危害或风险
- en: 'You must identify possible risks associated with your generative AI project,
    which vary based on the services, models, and data you employ. Here are some common
    risks:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须确定与您的生成式AI项目相关的可能风险，这些风险取决于您使用的服务、模型和数据。以下是一些常见风险：
- en: Generating offensive or biased content
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成冒犯性或偏见的内容
- en: Spreading misinformation
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传播错误信息
- en: Promoting harmful behavior
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进有害行为
- en: To understand the limitations and typical behaviors of your models and services,
    refer to their documentation, such as the transparency notes provided by Azure
    OpenAI Service, or specific model documentation, such as OpenAI’s system card
    for GPT-4.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解您模型和服务的局限性和典型行为，请参考它们的文档，例如Azure OpenAI服务提供的透明度说明，或特定模型的文档，例如OpenAI为GPT-4提供的系统卡片。
- en: Further reading
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: You can use specially crafted resources such as Microsoft’s *Responsible AI
    Impact Assessment Guide* ([https://aka.ms/RAIImpactAssessmentGuidePDF](https://aka.ms/RAIImpactAssessmentGuidePDF))
    and the *Responsible AI Impact Assessment template* ([https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFk](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFk))
    to outline and evaluate these potential risks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用专门定制的资源，如微软的*负责任AI影响评估指南*([https://aka.ms/RAIImpactAssessmentGuidePDF](https://aka.ms/RAIImpactAssessmentGuidePDF))和*负责任AI影响评估模板*([https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFk](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFk))来概述和评估这些潜在风险。
- en: Risk prioritization
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险优先级
- en: Here, you must evaluate and rank each identified risk based on its likelihood
    and potential impact. This step is crucial for focusing efforts on mitigating
    the most significant risks first. Consider both the intended application of your
    AI solution and the possibilities for misuse, and then rank the potential risks
    based on factors such as impact and likelihood.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您必须根据其可能性和潜在影响评估和排名每个已识别的风险。这一步骤对于集中精力首先减轻最重大的风险至关重要。考虑您AI解决方案的预期应用和误用的可能性，然后根据影响和可能性等因素对潜在风险进行排名。
- en: 'Imagine that you’re developing a copilot to provide diet and exercise recommendations
    based on the user’s current physical health condition, metrics, and goals. Here
    are some potential harms:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您正在开发一个共乘机，根据用户的当前健康状况、指标和目标提供饮食和锻炼建议。以下是一些潜在的危害：
- en: Recommending an ineffective exercise or meal plan
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐无效的锻炼或饮食计划
- en: Recommending an exercise routine that results in serious physical injury
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐可能导致严重身体伤害的锻炼计划
- en: While recommending an ineffective meal plan doesn’t meet the stated goal of
    the copilot, recommending an exercise that puts someone at risk for serious physical
    injury or death has a might higher potential negative impact and should likely
    be addressed first.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然推荐无效的饮食计划不符合共乘机的既定目标，但推荐可能导致严重身体伤害或死亡的锻炼具有更高的潜在负面影响，因此可能需要首先解决。
- en: Engage with your development team and possibly legal or policy experts to accurately
    prioritize these risks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与您的开发团队以及可能的法律或政策专家合作，准确地对这些风险进行优先排序。
- en: Testing for risks
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险测试
- en: With a prioritized list of risks, conduct targeted tests to confirm their presence
    and understand their triggers. This could involve **red team** testing (sometimes
    called **red teaming**), where specialists attempt to find and exploit vulnerabilities
    within your AI solution.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在优先级列表的基础上，进行有针对性的测试以确认其存在并了解其触发因素。这可能涉及**红队测试**（有时称为**红队行动**），其中专家试图在您的AI解决方案中寻找并利用漏洞。
- en: For example, in a diet and exercise copilot scenario, tests might involve inputting
    that the user has asthma and congestive heart failure. Documenting the outcomes
    of these tests helps gauge the actual likelihood of harmful outcomes and may uncover
    additional risks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个饮食和锻炼辅助场景中，测试可能包括输入用户患有哮喘和充血性心力衰竭。记录这些测试的结果有助于评估有害结果的实际可能性，并可能揭示额外的风险。
- en: Documentation and communication
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文档和沟通
- en: Document the confirmed risks and communicate this information to all relevant
    stakeholders. Keep an updated and detailed list of potential and confirmed risks
    as your solution evolves. This documentation is essential for transparency and
    informs ongoing efforts to mitigate harm in your Generative AI solutions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 记录已确认的风险，并将此信息传达给所有相关利益相关者。随着你的解决方案的发展，保持一个更新且详细的潜在和已确认风险清单。这份文档对于透明度至关重要，并告知持续努力以缓解你的生成式AI解决方案中的危害。
- en: Measure
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量
- en: Once you have a list of potentially harmful effects prioritized, the next phase
    is to evaluate your solution’s actual output against these risks. Start by establishing
    a baseline to understand the extent of harm your solution could cause in different
    scenarios and use this as a reference point to assess improvements as you refine
    your solution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将潜在的有害影响按优先级排序，下一步就是评估你的解决方案的实际输出与这些风险之间的对比。首先，建立一个基线以了解你的解决方案在不同场景下可能造成的损害程度，并以此作为参考点来评估随着你改进解决方案时的改进情况。
- en: 'To effectively measure your system for possible negative impacts, follow these
    three steps:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地衡量你的系统可能产生的负面影响，请遵循以下三个步骤：
- en: Develop a varied set of test queries that could trigger the identified potential
    harms. For instance, if there’s a risk that the system might provide instructions
    for creating harmful substances, prepare queries that might lead to such responses,
    such as asking for ways to create harmful substances from common household items.
    In the diet and exercise copilot example, this might include preparing questions
    scenarios where you would provide the system information about at-risk health
    conditions and then ask for an exercise plan that would put the person in physical
    jeopardy.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一组多样化的测试查询，这些查询可能触发已识别的潜在危害。例如，如果存在系统可能提供制造有害物质指令的风险，准备可能导致此类响应的查询，例如询问如何从常见的家庭用品中制造有害物质。在饮食和锻炼辅助的例子中，这可能包括准备你提供系统关于风险健康状况的信息的情景，然后询问可能导致身体危险的锻炼计划。
- en: Run these queries through your system and collect the responses.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些查询运行通过你的系统并收集响应。
- en: Assess the responses based on clear criteria to determine their potential harm.
    This could mean simply classifying them as “harmful” or “safe,” or you might establish
    a spectrum of harm severity. It’s crucial to apply consistent, predefined standards
    when evaluating the output to ensure accurate categorization of potential risks.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据明确的准则评估响应，以确定其潜在危害。这可能意味着简单地将其分类为“有害”或“安全”，或者你可能建立一个危害严重程度的范围。在评估输出时应用一致、预定义的标准至关重要，以确保潜在风险的准确分类。
- en: The results of the testing process should then be shared with the project or
    solution stakeholders.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 测试过程的结果应随后与项目或解决方案的利益相关者共享。
- en: Mitigate
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解
- en: Once you’ve established a baseline and a method for assessing potentially risky
    or harmful outputs from your solution, you can implement measures to minimize
    these risks. Subsequently, reassess the updated system and evaluate the reduced
    levels of harm compared to the original baseline. Mitigating potential harms in
    a generative AI solution requires a multi-layered strategy, where various mitigation
    techniques are applied across different layers of the system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你建立了基线和评估你的解决方案可能存在风险或有害输出的方法，你可以实施措施来最小化这些风险。随后，重新评估更新后的系统，并评估与原始基线相比减少的危害水平。在生成式AI解决方案中缓解潜在的危害需要多层次的策略，其中各种缓解技术在系统的不同层中应用。
- en: '*Figure 10**.7* shows the four-layered approach that Microsoft proposes:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10**.7*显示了微软提出的四层方法：'
- en: '![Figure 10.7 – Reviewing the layered approach to harm mitigation](img/B22207_10_07.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7 – 审查危害缓解的分层方法](img/B22207_10_07.jpg)'
- en: Figure 10.7 – Reviewing the layered approach to harm mitigation
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 审查危害缓解的分层方法
- en: Let’s take a closer look at each of these layers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看每一层。
- en: Model
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: 'The **model** layer is the base AI model (or models) at the foundation of your
    solution, such as GPT-3 or GPT-4\. At the model layer, mitigating potential harms
    involves doing the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**层是您解决方案基础上的基础AI模型（或模型），如GPT-3或GPT-4。在模型层，缓解潜在危害包括以下措施：'
- en: Choosing a model that’s suitable for the intended use of the solution. For instance,
    while GPT-4 is highly capable, a less complex model could suffice for tasks requiring
    only specific text input classification, reducing the risk of generating harmful
    content.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合解决方案预期用途的模型。例如，虽然GPT-4功能强大，但对于只需要特定文本输入分类的任务，一个更简单的模型可能就足够了，从而降低生成有害内容的风险。
- en: Fine-tuning the chosen foundational model with custom training data to ensure
    the generated responses are tailored and relevant to the specific scenario of
    your solution.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义训练数据微调所选的基础模型，以确保生成的响应针对性强且与您解决方案的具体场景相关。
- en: Once you have mitigated potential issues at the model layer, it’s time to move
    on to the next layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在模型层缓解了潜在问题，就到了转向下一层的时候了。
- en: Safety system
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全系统
- en: 'The **safety system** layer encompasses measures at the platform level that
    are designed to reduce risks, including configurations and features integrated
    into services such as Azure OpenAI Service. This layer can offer these types of
    features:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**安全系统**层包括平台层面的措施，旨在降低风险，包括集成到Azure OpenAI服务等服务中的配置和功能。这一层可以提供以下类型的特性：'
- en: Content filters, which evaluate and categorize content and interactions based
    on their potential harm across four levels (safe to high) and categories (hate
    speech, sexual content, violence, and self-harm), preventing inappropriate prompts
    and responses
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容过滤器，根据其潜在危害（从安全到高）和类别（仇恨言论、色情内容、暴力和自残）对内容和交互进行评估和分类，防止不适当的提示和响应
- en: Algorithms for detecting misuse, such as identifying unusually high or automated
    requests indicative of bot activity and implementing alert systems for quick responses
    to any signs of abuse or harmful actions within the system
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测滥用（如识别异常高或自动请求，表明机器人活动）的算法，并实施快速响应系统内任何滥用或有害行为的警报系统
- en: When actions activate the alerts in the safety systems (such as alerts for unusual
    or bot-type activity), it’s important to act quickly to prevent the system from
    being compromised.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当操作激活安全系统中的警报（例如针对异常或机器人类型活动的警报）时，迅速采取行动以防止系统被破坏至关重要。
- en: Metaprompt and grounding
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元提示和接地
- en: 'Mitigations at the **metaprompt and grounding** layer focus on the prompts
    that are sent to the solution. Potential harm mitigation strategies might include
    the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在**元提示和接地**层进行的缓解措施集中在发送到解决方案的提示上。潜在的损害缓解策略可能包括以下内容：
- en: Specifying **metaprompts** (a special prompt that provides guidance on how to
    handle prompts submitted by the end users) or other prompt engineering tactics
    to further enforce or refine the system’s behavior and outputs. For example, you
    might use a metaprompt to tell the model to ignore specific harmful words or phrases.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定**元提示**（一种提供如何处理最终用户提交的提示的指导的特殊提示）或其他提示工程策略，以进一步强化或细化系统的行为和输出。例如，您可能使用元提示来告诉模型忽略特定的有害词汇或短语。
- en: Implementing a **retrieval-augmented generation** (**RAG**) system to pull data
    from trusted data sources to integrate into the prompt.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施一个**检索增强生成**（**RAG**）系统，从可信数据源中提取数据以集成到提示中。
- en: What’s RAG?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是RAG？
- en: 'RAG is a method in generative AI that combines the power of LLMs with information
    retrieval systems to enhance the generation of text. This approach involves two
    key components:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是生成式AI中的一种方法，它结合了LLMs的力量和信息检索系统，以增强文本生成。这种方法涉及两个关键组件：
- en: '**•** **Retrieval component**: Before generating text, the model queries a
    database, knowledge base, or a large corpus of documents to retrieve relevant
    information based on the input prompt. This step is akin to looking up reference
    materials before writing on a topic, ensuring the information included in the
    generated content is grounded in factual and relevant sources.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**•** **检索组件**：在生成文本之前，模型查询数据库、知识库或大量文档的大集合，根据输入提示检索相关信息。这一步骤类似于在撰写某个主题之前查找参考资料，确保生成内容中的信息基于事实和相关的来源。'
- en: '**•** **Generation component**: This part involves a generative model, such
    as a transformer-based language model, which uses the retrieved information along
    with the original prompt to produce the final output. The model integrates the
    context and details from the retrieved documents into the generated text, enhancing
    the accuracy, relevance, and factual grounding of the content.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**•** **生成组件**：这部分涉及生成模型，例如基于转换器的语言模型，它使用检索到的信息以及原始提示来生成最终输出。该模型将检索到的文档中的上下文和细节整合到生成的文本中，增强了内容的准确性、相关性和事实基础。'
- en: User experience
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户体验
- en: The **user experience** layer of a generative AI solution relates not only to
    the application interface users engage with but also to the supporting documentation
    provided to users and stakeholders. By customizing the application’s user interface
    to limit inputs to certain subjects or types and implementing input and output
    validation, the risk of generating potentially harmful responses can be reduced
    (as can error conditions resulting from incomplete or improperly formatted input).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI解决方案的**用户体验**层不仅与用户交互的应用程序界面有关，还与提供给用户和利益相关者的支持性文档有关。通过定制应用程序的用户界面以限制输入到某些主题或类型，并实施输入和输出验证，可以降低生成潜在有害响应的风险（以及由于输入不完整或不正确格式化而产生的错误条件）。
- en: Additionally, it’s vital for the documentation and descriptive materials about
    the generative AI solution to communicate the system’s capabilities, limitations,
    and the foundational models it utilizes. This transparency is crucial for informing
    users of any potential risks that may not be fully mitigated by existing safeguards.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，关于生成式AI解决方案的文档和描述性材料传达系统的能力、限制以及所使用的基座模型至关重要。这种透明度对于告知用户任何可能的风险（这些风险可能无法通过现有的安全措施完全缓解）至关重要。
- en: Operate
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运营
- en: After you have identified potential harms, developed methods to measure their
    existence, and implemented mitigations to reduce their appearance in your solution,
    you can release your solution.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在你确定了潜在的危害、开发了衡量其存在的方法并实施了缓解措施以减少其在你的解决方案中出现的可能性之后，你可以发布你的解决方案。
- en: 'As with any solution, you should apply systematic testing, as well as submit
    to your organization’s relevant governance, compliance, or legal processes. Many
    organizations require several review gates to ensure products comply with a variety
    of requirements, including the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何解决方案一样，你应该进行系统测试，并提交给你的组织的相关治理、合规或法律流程。许多组织要求进行多个审查关卡，以确保产品符合各种要求，包括以下内容：
- en: Legal
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法律
- en: Marketing and branding
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场营销和品牌建设
- en: Privacy
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私
- en: Accessibility
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可访问性
- en: Security
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性
- en: 'Once any internal reviews have been completed, it’s time to release the solution!
    Many frameworks cover various facets of daily operations for software and services
    (whether they’re developed internally or purchased). These guidelines aren’t limited
    to AI-based solutions, but rather should be applied to any deployed products and
    services:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成任何内部审查，就到了发布解决方案的时候了！许多框架涵盖了软件和服务（无论它们是内部开发还是购买的）日常运营的各个方面。这些指南不仅限于基于AI的解决方案，而应该应用于任何部署的产品和服务：
- en: Phased delivery or pilot plans to slowly onboard users to the solution
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分阶段交付或试点计划，以缓慢地将用户引入解决方案
- en: Incident response plans
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件响应计划
- en: Communication plans for emergencies, planned upgrades, or outages
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应急、计划升级或故障的沟通计划
- en: Upgrade plans
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级计划
- en: Rollback plans for reverting applications and services to a previously known
    good state
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚计划，以将应用程序和服务恢复到之前已知的好状态
- en: Abuse protection measures, such as user identity requirements and network security
    rules
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滥用保护措施，例如用户身份要求和网络安全规则
- en: Monitoring for availability and outages
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控可用性和故障
- en: Feedback mechanisms for reporting inaccurate or inappropriate data
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈机制，用于报告不准确或不适当的数据
- en: Telemetry to measure system performance and gather end user experience metrics
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量系统性能和收集最终用户体验指标的遥测
- en: Implementing these operational tasks and guidelines should ensure you have a
    robust solution.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些运营任务和指南应确保你有一个稳健的解决方案。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discussed the features of generative AI and provided you with a
    deeper understanding of how generative AI works, including the concepts of tokenization,
    embeddings, and attention, as well as demonstrating many current real-world usage
    scenarios for generative AI.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了生成式AI的特点，并为你提供了对生成式AI如何工作的更深入理解，包括分词、嵌入和注意力的概念，以及演示了许多生成式AI的当前实际应用场景。
- en: In addition, this chapter covered techniques for applying responsible AI considerations
    to AI solutions that you develop or implement.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章还涵盖了将负责任的AI考虑因素应用于你开发或实施的AI解决方案的技术。
- en: In the next chapter, we’ll explore the Azure OpenAI service.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索Azure OpenAI服务。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 – 第10章复习题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项帮助你通过认证考试的重要技能。这就是为什么在学习的早期阶段就培养这些技能至关重要。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 复习题旨在随着你学习并复习每一章的内容，逐步提高你的应试技巧，同时同时检查你对章节中关键概念的理解。你将在每个章节的末尾找到这些习题。
- en: Before You Proceed
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前
- en: If you don't have a Packt Library subscription or you haven't purchased this
    book from the Packt store, you will need to unlock the online resources to access
    the exam readiness drills. Unlocking is free and needs to be done only once. To
    learn how to do that, head over to the chapter titled [*Chapter 12*](B22207_12.xhtml#_idTextAnchor228)*,
    Accessing the* *Online Resources*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有Packt图书馆订阅或没有从Packt商店购买这本书，你需要解锁在线资源以访问考试准备练习。解锁是免费的，只需进行一次。要了解如何操作，请参阅名为[*第12章*](B22207_12.xhtml#_idTextAnchor228)*的章节，*访问在线资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的复习题，请执行以下步骤：
- en: Click the link – [https://packt.link/AI-900_CH10](https://packt.link/AI-900_CH10).
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/AI-900_CH10](https://packt.link/AI-900_CH10)。
- en: 'Alternatively, you can scan the following QR code (*Figure 10**.8*):'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以扫描以下二维码(*图10.8*)：
- en: '![Figure 10.8 – QR code that opens Chapter Review Questions for logged-in users](img/B22207_10_08.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – 为登录用户打开第10章复习题的二维码](img/B22207_10_08.jpg)'
- en: Figure 10.8 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 为登录用户打开第10章复习题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 10**.9*:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，你将看到一个类似于*图10.9*所示的页面：
- en: '![Figure 10.9 – Chapter Review Questions for Chapter 10](img/B22207_10_09.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – 第10章的复习题](img/B22207_10_09.jpg)'
- en: Figure 10.9 – Chapter Review Questions for Chapter 10
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 第10章的复习题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备就绪后，开始以下练习，多次重新尝试测验。
- en: Exam Readiness Drill
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不要担心时间限制。
- en: ATTEMPT 1
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试，目标至少达到**40%**。查看你答错的答案，并再次阅读章节中相关的部分，以修复你的学习差距。
- en: ATTEMPT 2
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次尝试，目标至少达到**60%**。查看你答错的答案，并再次阅读章节中相关的部分，以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次尝试，目标至少达到**75%**。一旦得分达到75%或更高，你就可以开始练习时间管理。
- en: Tip
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要超过**三次**尝试才能达到75%。这没关系。只需复习章节中的相关部分，直到达到那里。
- en: Working On Timing
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习时间管理
- en: 'Your aim is to keep the score the same while trying to answer these questions
    as quickly as possible. Here’s an example of how your next attempts should look
    like:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是保持分数不变，同时尽可能快地回答这些问题。以下是你下一次尝试的示例：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **分数** | **用时** |'
- en: '| --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 尝试5次 | 77% | 21分30秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 尝试6次 | 78% | 18分34秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 尝试7次 | 76% | 14分44秒 |'
- en: Table 10.2 – Sample timing practice drills on the online platform
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.2 – 在线平台上的样本时间练习题
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，每次尝试时为自己设定自己的时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的分数应保持在**75%**以上，而完成所需的时间应“减少”。重复尝试，直到你觉得自己能够自信地应对时间压力。
