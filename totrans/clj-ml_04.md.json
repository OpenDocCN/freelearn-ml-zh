["```py\n;; truth table for XOR logic gate\n(def sample-data [[[0 0] [0]]\n                  [[0 1] [1]]\n                  [[1 0] [1]]\n                  [[1 1] [0]]])\n```", "```py\n(defprotocol NeuralNetwork\n  (run        [network inputs])\n  (run-binary [network inputs])\n  (train-ann  [network samples]))\n```", "```py\n(defn rand-list\n  \"Create a list of random doubles between \n  -epsilon and +epsilon.\"\n  [len epsilon]\n  (map (fn [x] (- (rand (* 2 epsilon)) epsilon))\n         (range 0 len)))\n\n(defn random-initial-weights\n  \"Generate random initial weight matrices for given layers.\n  layers must be a vector of the sizes of the layers.\"\n  [layers epsilon]\n  (for [i (range 0 (dec (length layers)))]\n    (let [cols (inc (get layers i))\n          rows (get layers (inc i))]\n      (matrix (rand-list (* rows cols) epsilon) cols))))\n```", "```py\n(defn sigmoid\n  \"Apply the sigmoid function 1/(1+exp(-z)) to all \n  elements in the matrix z.\"\n  [z]\n  (div 1 (plus 1 (exp (minus z)))))\n```", "```py\n(defn bind-bias\n  \"Add the bias input to a vector of inputs.\"\n  [v]\n  (bind-rows [1] v))\n```", "```py\n(defn matrix-mult\n  \"Multiply two matrices and ensure the result is also a matrix.\"\n  [a b]\n  (let [result (mmult a b)]\n    (if (matrix? result)\n      result\n      (matrix [result]))))\n\n(defn forward-propagate-layer\n  \"Calculate activations for layer l+1 given weight matrix \n  of the synapse between layer l and l+1 and layer l activations.\"\n  [weights activations]\n  (sigmoid (matrix-mult weights activations)))\n\n(defn forward-propagate\n  \"Propagate activation values through a network's\n  weight matrix and return output layer activation values.\"\n  [weights input-activations]\n  (reduce #(forward-propagate-layer %2 (bind-bias %1))\n          input-activations weights))\n```", "```py\n(defn forward-propagate-all-activations\n  \"Propagate activation values through the network \n  and return all activation values for all nodes.\"\n  [weights input-activations]\n  (loop [all-weights     weights\n         activations     (bind-bias input-activations)\n         all-activations [activations]]\n    (let [[weights\n           & all-weights']  all-weights\n           last-iter?       (empty? all-weights')\n           out-activations  (forward-propagate-layer\n                             weights activations)\n           activations'     (if last-iter? out-activations\n                                (bind-bias out-activations))\n           all-activations' (conj all-activations activations')]\n      (if last-iter? all-activations'\n          (recur all-weights' activations' all-activations')))))\n```", "```py\n(defn back-propagate-layer\n  \"Back propagate deltas (from layer l+1) and \n  return layer l deltas.\"\n  [deltas weights layer-activations]\n  (mult (matrix-mult (trans weights) deltas)\n        (mult layer-activations (minus 1 layer-activations))))\n```", "```py\n(defn calc-deltas\n  \"Calculate hidden deltas for back propagation.\n  Returns all deltas including output-deltas.\"\n  [weights activations output-deltas]\n  (let [hidden-weights     (reverse (rest weights))\n        hidden-activations (rest (reverse (rest activations)))]\n    (loop [deltas          output-deltas\n           all-weights     hidden-weights\n           all-activations hidden-activations\n           all-deltas      (list output-deltas)]\n      (if (empty? all-weights) all-deltas\n        (let [[weights\n               & all-weights']      all-weights\n               [activations\n                & all-activations'] all-activations\n              deltas'        (back-propagate-layer\n                               deltas weights activations)\n              all-deltas'    (cons (rest deltas') \n                                    all-deltas)]\n          (recur deltas' all-weights' \n                 all-activations' all-deltas'))))))\n```", "```py\n(defn calc-gradients\n  \"Calculate gradients from deltas and activations.\"\n  [deltas activations]\n  (map #(mmult %1 (trans %2)) deltas activations))\n```", "```py\n(defn calc-error\n  \"Calculate deltas and squared error for given weights.\"\n  [weights [input expected-output]]\n  (let [activations    (forward-propagate-all-activations \n                        weights (matrix input))\n        output         (last activations)\n        output-deltas  (minus output expected-output)\n        all-deltas     (calc-deltas \n                        weights activations output-deltas)\n        gradients      (calc-gradients all-deltas activations)]\n    (list gradients\n          (sum (pow output-deltas 2)))))\n```", "```py\n(defn new-gradient-matrix\n  \"Create accumulator matrix of gradients with the\n  same structure as the given weight matrix\n  with all elements set to 0.\"\n  [weight-matrix]\n  (let [[rows cols] (dim weight-matrix)]\n    (matrix 0 rows cols)))\n```", "```py\n(defn calc-gradients-and-error' [weights samples]\n  (loop [gradients   (map new-gradient-matrix weights)\n         total-error 1\n         samples     samples]\n    (let [[sample\n           & samples']     samples\n           [new-gradients\n            squared-error] (calc-error weights sample)\n            gradients'     (map plus new-gradients gradients)\n            total-error'   (+ total-error squared-error)]\n      (if (empty? samples')\n        (list gradients' total-error')\n        (recur gradients' total-error' samples')))))\n\n(defn calc-gradients-and-error\n  \"Calculate gradients and MSE for sample\n  set and weight matrix.\"\n  [weights samples]\n  (let [num-samples   (length samples)\n        [gradients\n         total-error] (calc-gradients-and-error'\n                       weights samples)]\n    (list\n      (map #(div % num-samples) gradients)    ; gradients\n      (/ total-error num-samples))))          ; MSE\n```", "```py\n(defn gradient-descent-complete?\n  \"Returns true if gradient descent is complete.\"\n  [network iter mse]\n  (let [options (:options network)]\n    (or (>= iter (:max-iters options))\n        (< mse (:desired-error options)))))\n```", "```py\n(defn apply-weight-changes\n  \"Applies changes to corresponding weights.\"\n  [weights changes]\n  (map plus weights changes))\n\n(defn gradient-descent\n  \"Perform gradient descent to adjust network weights.\"\n  [step-fn init-state network samples]\n  (loop [network network\n         state init-state\n         iter 0]\n    (let [iter     (inc iter)\n          weights  (:weights network)\n          [gradients\n           mse]    (calc-gradients-and-error weights samples)]\n      (if (gradient-descent-complete? network iter mse)\n        network\n        (let [[changes state] (step-fn network gradients state)\n              new-weights     (apply-weight-changes \n                               weights changes)\n              network         (assoc network \n                              :weights new-weights)]\n          (recur network state iter))))))\n```", "```py\n(defn calc-weight-changes\n  \"Calculate weight changes:\n  changes = learning rate * gradients + \n            learning momentum * deltas.\"\n  [gradients deltas learning-rate learning-momentum]\n  (map #(plus (mult learning-rate %1)\n              (mult learning-momentum %2))\n       gradients deltas))\n\n(defn bprop-step-fn [network gradients deltas]\n  (let [options             (:options network)\n        learning-rate       (:learning-rate options)\n        learning-momentum   (:learning-momentum options)\n        changes             (calc-weight-changes\n                             gradients deltas\n                             learning-rate learning-momentum)]\n    [(map minus changes) changes]))\n\n(defn gradient-descent-bprop [network samples]\n  (let [gradients (map new-gradient-matrix (:weights network))]\n    (gradient-descent bprop-step-fn gradients\n                      network samples)))\n```", "```py\n(defn round-output\n  \"Round outputs to nearest integer.\"\n  [output]\n  (mapv #(Math/round ^Double %) output))\n\n(defrecord MultiLayerPerceptron [options]\n  NeuralNetwork\n\n  ;; Calculates the output values for the given inputs.\n  (run [network inputs]\n    (let [weights (:weights network)\n          input-activations (matrix inputs)]\n      (forward-propagate weights input-activations)))\n\n  ;; Rounds the output values to binary values for\n  ;; the given inputs.\n  (run-binary [network inputs]\n    (round-output (run network inputs)))\n\n  ;; Trains a multilayer perceptron ANN from sample data.\n  (train-ann [network samples]\n    (let [options         (:options network)\n          hidden-neurons  (:hidden-neurons options)\n          epsilon         (:weight-epsilon options)\n          [first-in\n           first-out]     (first samples)\n          num-inputs      (length first-in)\n          num-outputs     (length first-out)\n          sample-matrix   (map #(list (matrix (first %)) \n                                      (matrix (second %)))\n                               samples)\n          layer-sizes     (conj (vec (cons num-inputs \n                                           hidden-neurons))\n                                num-outputs)\n          new-weights     (random-initial-weights \n                           layer-sizes epsilon)\n          network         (assoc network :weights new-weights)]\n      (gradient-descent-bprop network sample-matrix))))\n```", "```py\n(def default-options\n  {:max-iters 100\n   :desired-error 0.20\n   :hidden-neurons [3]\n   :learning-rate 0.3\n   :learning-momentum 0.01\n   :weight-epsilon 50})\n\n(defn train [samples]\n  (let [network (MultiLayerPerceptron. default-options)]\n    (train-ann network samples)))\n```", "```py\nuser> (def MLP (train sample-data))\n#'user/MLP\n```", "```py\nuser> (run-binary MLP  [0 1])\n[1]\nuser> (run-binary MLP  [1 0])\n[1]\n```", "```py\nuser> (run-binary MLP  [0 0])\n[0]\nuser> (run-binary MLP  [1 1]) ;; incorrect output generated\n[1]\n```", "```py\n[org.encog/encog-core \"3.1.0\"]\n[enclog \"0.6.3\"]\n```", "```py\n(ns my-namespace\n  (:use [enclog nnets training]))\n```", "```py\n(def mlp (network (neural-pattern :feed-forward)\n                  :activation :sigmoid\n                  :input      2\n                  :output     1\n                  :hidden     [3]))\n```", "```py\n(defn train-network [network data trainer-algo]\n  (let [trainer (trainer trainer-algo\n                         :network network\n                         :training-set data)]\n    (train trainer 0.01 1000 []))) ;; 0.01 is the expected error\n```", "```py\n(def dataset\n  (let [xor-input [[0.0 0.0] [1.0 0.0] [0.0 1.0] [1.0 1.0]]\n        xor-ideal [[0.0]     [1.0]     [1.0]     [0.0]]]\n        (data :basic-dataset xor-input xor-ideal)))\n```", "```py\nuser> (def MLP (train-network mlp dataset :back-prop))\nIteration # 1 Error: 26.461526% Target-Error: 1.000000%\nIteration # 2 Error: 25.198031% Target-Error: 1.000000%\nIteration # 3 Error: 25.122343% Target-Error: 1.000000%\nIteration # 4 Error: 25.179218% Target-Error: 1.000000%\n...\n...\nIteration # 999 Error: 3.182540% Target-Error: 1.000000%\nIteration # 1,000 Error: 3.166906% Target-Error: 1.000000%\n#'user/MLP\n```", "```py\n(defn run-network [network input]\n  (let [input-data (data :basic input)\n        output     (.compute network input-data)\n        output-vec (.getData output)]\n    (round-output output-vec)))\n```", "```py\nuser> (run-network MLP [1 1])\n[0]\nuser> (run-network MLP [1 0])\n[1]\nuser> (run-network MLP [0 1])\n[1]\nuser> (run-network MLP [0 0])\n[0]\n```", "```py\n(def elman-network (network (neural-pattern :elman)\n                             :activation :sigmoid\n                             :input      2\n                             :output     1\n                             :hidden     [3]))\n```", "```py\nuser> (def EN (train-network elman-network dataset \n                             :resilient-prop))\nIteration # 1 Error: 26.461526% Target-Error: 1.000000%\nIteration # 2 Error: 25.198031% Target-Error: 1.000000%\nIteration # 3 Error: 25.122343% Target-Error: 1.000000%\nIteration # 4 Error: 25.179218% Target-Error: 1.000000%\n...\n...\nIteration # 99 Error: 0.979165% Target-Error: 1.000000%\n#'user/EN\n```", "```py\n(def som (network (neural-pattern :som) :input 4 :output 2))\n\n(defn train-som [data]\n  (let [trainer (trainer :basic-som :network som\n                         :training-set data\n                         :learning-rate 0.7\n                         :neighborhood-fn \n          (neighborhood-F :single))]\n    (train trainer Double/NEGATIVE_INFINITY 10 [])))\n```", "```py\n(defn train-and-run-som []\n  (let [input [[-1.0, -1.0, 1.0, 1.0 ]\n               [1.0, 1.0, -1.0, -1.0]]\n        input-data (data :basic-dataset input nil) ;no ideal data\n        SOM        (train-som input-data)\n        d1         (data :basic (first input))\n        d2         (data :basic (second input))]\n    (println \"Pattern 1 class:\" (.classify SOM d1))\n    (println \"Pattern 2 class:\" (.classify SOM d2))\n    SOM))\n```", "```py\nuser> (train-and-run-som)\nIteration # 1 Error: 2.137686% Target-Error: NaN\nIteration # 2 Error: 0.641306% Target-Error: NaN\nIteration # 3 Error: 0.192392% Target-Error: NaN\n...\n...\nIteration # 9 Error: 0.000140% Target-Error: NaN\nIteration # 10 Error: 0.000042% Target-Error: NaN\nPattern 1 class: 1\nPattern 2 class: 0\n#<SOM org.encog.neural.som.SOM@19a0818>\n```"]