<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Scala for Learning Classification</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we saw how to develop a predictive model for analyzing insurance severity claims as a regression analysis problem. We applied very simple linear regression, as well as <strong>generalized linear regression</strong> (<strong>GLR</strong>).</p>
<p>In this chapter, we'll learn about another supervised learning task, called classification. We'll use widely used algorithms such as logistic regression, <strong>Naive Bayes</strong> (<strong>NB</strong>), and <strong>Support Vector Machines</strong> (<strong>SVMs</strong>) to analyze and predict whether a customer is likely to cancel the subscription of their telecommunication contract or not.</p>
<p>In particular, we will cover the following topics:</p>
<ul>
<li>Introduction to classification</li>
<li>Learning classification with a real-life example</li>
<li>Logistic regression for churn prediction</li>
<li>SVM for churn prediction</li>
<li>NB for prediction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.</p>
<p>The code files of this chapters can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03</a></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2ZKVrxH" target="_blank">http://bit.ly/2ZKVrxH</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of classification</h1>
                </header>
            
            <article>
                
<p><span class="ILfuVd">As a supervised learning task, classification is the problem of identifying which set of observations (sample) belongs to what based on one or more independent variables. This learning process is based on a training set containing observations (or instances) about the class or label of membership.</span> Typically, classification problems are when we are training a model to predict quantitative (but discrete) targets, such as <em>s</em>pam detection, churn prediction, sentiment analysis, cancer type prediction, and so on.</p>
<p>Suppose we want to develop a predictive model, which will predict whether a student is competent enough to get admission into computer science based on his/her competency in TOEFL and GRE. Also, suppose we have some historical data in the following range/format:</p>
<ul>
<li><strong>TOEFL</strong>: Between 0 and 100</li>
<li><strong>GRE</strong>: Between 0 and 100</li>
<li><strong>Admission</strong>: 1 for admitted, 0 if not admitted</li>
</ul>
<p>Now, to understand whether we can use such simple data to make predictions, let's create a scatter plot by putting all the records with <strong>Admitted</strong> and <strong>Rejected</strong> as the dependent variables and <strong>TOEFL</strong> and <strong>GRE</strong> as the independent variables, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-546 image-border" src="assets/004d677c-7313-4a40-bf44-90ee1afbcda1.png" style="width:24.83em;height:20.67em;"/></p>
<p>By looking at the data points (imagine that the diagonal line in the graph is not there), we can reasonably develop a linear model to separate most of the data points. Now, if we draw a straight line between two classes of data, those almost get separated. Such a line (green, in our case) is called the decision boundary. So, if the decision boundary <span><span>has</span></span> reasonably separate maximal data points, it can be used for making predictions on unseen data, and we can also say that the data point above the line we predicted is competent for admission, and below the line we predict that the students are not competent enough.</p>
<p>Although this example is for a basic head-start into regression analysis, separating missions of data points is not very easy. Thus, to calculate where to draw the line for separating such a huge number of data points, we can use logistic regression or other classification algorithms that we will discuss in upcoming sections. We'll also see that drawing an ordinary straight line might not be the right one, and therefore we often have to draw curved lines.</p>
<p>If we look at the admission-related data plot carefully, maybe a straight line is not the best way of separating each data point—a curved line would be better, as shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-547 image-border" src="assets/8513e773-de23-4536-b7f1-7daa4b32d5e0.png" style="width:29.42em;height:24.83em;"/></p>
<p>However, to get a curved decision boundary, we have to change not only the function (called the decision boundary function) that's responsible from being linear to some high-order polynomial, but also the data to be a second-degree polynomial. </p>
<p class="mce-root"/>
<p>This means that we have to model our problem as a logistic regression model. That is, we need to change the data from <span class="MathJax"><span class="math"><span><span class="mrow"><em><span class="mo">{</span><span class="mi">GRE</span><span class="mo">, </span><span class="mi">TOEFL</span></em><span class="mo"><em>}</em> format to</span></span></span></span></span> a quadratic function format, <span class="MathJax"><span class="math"><span><span class="mrow"><em><span class="mo">{</span><span class="mi">GRE</span><span class="mo">,</span> <span class="mi">GRE^2</span><span class="mo">, TOEFL,</span> <span class="mi">TOEFL^2</span><span class="mo">,</span> <span class="mi">GRE</span><span class="mo">∗</span><span class="mi">TOEFL</span></em><span class="mo"><em>}</em>.</span></span></span></span></span> <span class="MathJax"><span class="math"><span><span class="mrow"><span class="mo">However, doing so in a hand-crafted way is cumbersome and will not be possible for large datasets.</span></span></span></span></span><span class="ILfuVd"> </span>Fortunately, Spark MLlib has numerous algorithms implemented for modeling such problems and for solving other classification problems, including the following:</p>
<ul>
<li><strong>Logistic regression</strong> (<strong>LR</strong>)</li>
<li>SVM</li>
<li>NB</li>
<li><strong>Multilayer perceptron</strong> (<strong>MLP</strong>)</li>
<li><strong>Decision tree</strong> (<strong>DT</strong>)</li>
<li><strong>Random forest</strong> (<strong>RF</strong>)</li>
<li><strong>Gradient boosted trees</strong> (<strong>GBT</strong>)</li>
</ul>
<p>For a classification problem, actual (that is, true) labels (that is, class) and the predicted label (that is, class) exist for the samples that are used to train or test a classifier; this can be assigned to one of the following categories:</p>
<ul>
<li><strong>True positive (TP)</strong>: The true label is positive and the prediction made by the classifier is also positive</li>
<li><strong>True negative (TN)</strong>: The true label is negative and the prediction made by the classifier is also negative</li>
<li><strong>False positive (FP)</strong>: The true label is negative but the prediction made by the classifier is positive</li>
<li><strong>False negative (FN)</strong>: The true label is positive but the prediction made by the classifier is negative</li>
</ul>
<p>These metrics (TP, FP, TN, and FN) are the building blocks of the evaluation metrics for most of the classifiers we listed previously. However, the pure accuracy that is often used for identifying how many predictions were correct is not generally a good metric, so other metrics such as precision, recall, F1 score, AUC, and <strong>Matthew's correlation coefficient</strong> (<strong>MCC</strong>) are used:</p>
<ul>
<li><em>Accuracy</em> is the fraction of samples that the classifier correctly predicted (both positive and negative), divided by the total number of samples:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f75a674d-2f5c-4d02-b65d-6d52bfd3b3fb.png" style="width:14.25em;height:2.08em;"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><em>Precision</em> is the number of samples correctly predicted that belong to a positive class (true positives), divided by the total number of samples actually belonging to the positive class:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/acf55427-ec4f-415e-974f-eb47fd364dc4.png" style="width:9.75em;height:2.17em;"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><em>Recall</em> is the number of samples that were correctly predicted to belong to a negative class, divided by the total number of elements actually belonging to the negative class:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2c2e3301-f478-4212-8034-c27a95f8855e.png" style="width:9.00em;height:2.33em;"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign">F1 score is the harmonic mean of precision and recall. Since the F1 score is a balance between recall and precision, it can be considered as an alternative to accuracy: </li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/21df9705-5b82-4f30-8758-c7c0bfa23a7f.png" style="width:12.67em;height:2.33em;"/></p>
<p><strong>Receiver Operating Characteristics</strong> (<strong>ROC</strong>) is a curve that's drawn by plotting <strong>FPR</strong> (to <em>x</em>-axis) and <strong>TPR</strong> (to <em>y</em>-axis) for different threshold values. So, for different thresholds for your classifier, we calculate the <strong>TPR</strong> and <strong>FPR</strong>, draw the <strong>ROC</strong> curve, and calculate the area under the <strong>ROC</strong> curve (also known as <strong>AUC</strong>). This can be visualized as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-548 image-border" src="assets/ed60cca4-db3c-4239-a13f-054b24e18629.png" style="width:16.25em;height:13.42em;"/></p>
<p>MCC is regarded as a balanced measure of a binary classifier, even for a dataset that has very imbalanced classes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e1e62338-9246-4eb7-8cb6-9f51f6b31057.png" style="width:24.83em;height:2.67em;"/></p>
<p>Let's discuss a more real-life example of a classification problem, which is churn analysis. Customer churn is the loss of clients or customers in any business, which is becoming a prime concern in different area of business, such as banks, internet service providers, insurance companies, and so on<em>.</em> Customer dissatisfaction and better offers from the competitor are the primary reasons behind this<em>.</em> In the telecommunications industry, when many subscribers switch to another service provider, the company not only loses those customers and revenue—<span>this </span>also creates a bad impression for other, regular customers, or people who were planning to start using their service.</p>
<p>Eventually, the full cost of customer churn includes both the lost revenue and the telemarketing costs involved with replacing those customers with new ones. However, these types of loss can cause a huge loss for a business. Remember the time when Nokia was the dominator in the cell phone market? All of a sudden, Apple announced iPhone 3G, which was a revolution in the smartphone era. Then, around 10% to 12% customers discontinued using Nokia and switched to iPhone. Although Nokia also tried to release a smartphone later on, they could not compete with Apple. </p>
<p>In short, churn prediction is essential for businesses as it helps you detect different kinds of customers who are likely to cancel a subscription, product, or service. In short, the idea is to predict whether an existing customer will unsubscribe from an existing service or not, that is, a binary classification problem. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing predictive models for churn</h1>
                </header>
            
            <article>
                
<p>Accurate identification of churn possibility can minimize customer defection if you first identify which customers are likely to cancel a subscription to an existing service, and offering a special offer or plan to those customers. When it comes to employee churn prediction and developing a predictive model, where the process is heavily data-driven, machine learning can be used to understand a customer's behavior. This is done by analyzing the following:</p>
<ul>
<li>Demographic data, such as age, marital status, and job status</li>
<li>Sentiment analysis based on their social media data</li>
<li>Behavior analysis using their browsing clickstream logs</li>
<li>Calling-circle data and support call center statistics</li>
</ul>
<p>An automated churn analytics pipeline can be developed by following three steps:</p>
<ol>
<li>First, identify typical tasks to analyze the churn, which will depend on company policy</li>
<li>Then, collect and analyze data and develop a predictive model</li>
<li>Finally, deploy the model in a production-ready environment</li>
</ol>
<p>Eventually, telecom companies will be able to predict and enhance customer experience, prevent churn, and tailor marketing campaigns. In practice, such an analysis will be helpful to retain the customers who are most likely to leave. This means that we don't need to worry about the customers who are likely to stay.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p>We can use the Orange Telecom's Churn Dataset to develop a predictive model, which will predict which customers would like to cancel their subscription to existing services. The dataset is well studied, comprehensive, and used for developing small prototypes. It contains both churn-80 and churn-20 datasets, which can be downloaded from the following links:</p>
<ul>
<li>churn-80: <a href="https://bml-data.s3.amazonaws.com/churn-bigml-80.csv">https://bml-data.s3.amazonaws.com/churn-bigml-80.csv</a></li>
<li>churn-20: <a href="https://bml-data.s3.amazonaws.com/churn-bigml-20.csv">https://bml-data.s3.amazonaws.com/churn-bigml-20.csv</a></li>
</ul>
<p>Since both datasets came from the same distribution, which has the same structure, we will use the churn-80 dataset for the training and 10-fold cross-validation. Then, churn-20 will be used to evaluate the trained model. Both datasets have a similar structure, and therefore have the following schema:</p>
<ul>
<li><strong>State</strong>: <kbd>String</kbd></li>
<li><strong>Account length</strong>: <kbd>Integer</kbd></li>
<li><strong>Area code</strong>: <kbd>Integer</kbd></li>
<li><strong>International plan</strong>: <kbd>String</kbd></li>
<li><strong>Voicemail plan</strong>: <kbd>String</kbd></li>
<li><strong>Number email messages</strong>: <kbd>Integer</kbd></li>
<li><strong>Total day minutes</strong>: <kbd>Double</kbd></li>
<li><strong>Total day calls</strong>: <kbd>Integer</kbd></li>
<li><strong>Total day charge</strong>: <kbd>Double</kbd></li>
<li><strong>Total evening minutes</strong>: <kbd>Double</kbd></li>
<li><strong>Total evening calls</strong>: <kbd>Integer</kbd></li>
<li><strong>Total evening charge</strong>: <kbd>Double</kbd></li>
<li><strong>Total night minutes</strong>: <kbd>Double</kbd></li>
<li><strong>Total night calls</strong>: <kbd>Integer</kbd></li>
<li><strong>Total night charge</strong>: <kbd>Double</kbd></li>
<li><strong>Total international minutes</strong>: <kbd>Double</kbd></li>
<li><strong>Total international calls</strong>: <kbd>Integer</kbd></li>
<li><strong>Total international charge</strong>: <kbd>Double</kbd></li>
<li><strong>Customer service calls</strong>: <kbd>Integer</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory analysis and feature engineering</h1>
                </header>
            
            <article>
                
<p>First, we specify exactly the same schema (that is, a custom schema) before loading the data as a Spark DataFrame, as follows:</p>
<pre class="mce-root"><strong>val</strong> schema = <strong>StructType</strong>(Array(<br/>    StructField("state_code", StringType, true),<br/>    StructField("account_length", IntegerType, true),<br/>    StructField("area_code", StringType, true),<br/>    StructField("international_plan", StringType, true),<br/>    StructField("voice_mail_plan", StringType, true),<br/>    StructField("num_voice_mail", DoubleType, true),<br/>    StructField("total_day_mins", DoubleType, true),<br/>    StructField("total_day_calls", DoubleType, true),<br/>    StructField("total_day_charge", DoubleType, true),<br/>    StructField("total_evening_mins", DoubleType, true),<br/>    StructField("total_evening_calls", DoubleType, true),<br/>    StructField("total_evening_charge", DoubleType, true),<br/>    StructField("total_night_mins", DoubleType, true),<br/>    StructField("total_night_calls", DoubleType, true),<br/>    StructField("total_night_charge", DoubleType, true),<br/>    StructField("total_international_mins", DoubleType, true),<br/>    StructField("total_international_calls", DoubleType, true),<br/>    StructField("total_international_charge", DoubleType, true),<br/>    StructField("total_international_num_calls", DoubleType, true),<br/>    StructField("churn", StringType, true)))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Then, we have to create a Scala case class with all the fields specified and align the preceding schema (variable names are self-explanatory):</p>
<pre><strong>case</strong> <strong>class</strong> CustomerAccount(state_code: String, account_length: Integer, area_code: String, <br/>                 international_plan: String, voice_mail_plan: String, num_voice_mail: Double, <br/>                 total_day_mins: Double, total_day_calls: Double, total_day_charge: Double, <br/>                 total_evening_mins: Double, total_evening_calls: Double, total_evening_charge: Double, <br/>                 total_night_mins: Double, total_night_calls: Double, total_night_charge: Double,  <br/>                 total_international_mins: Double, total_international_calls: Double, <br/>                 total_international_charge: Double, total_international_num_calls: Double, churn: String)</pre>
<p>Let's create a Spark session and import the <kbd>implicit._</kbd> package, which allows us to specify a DataFrame operation, as follows:</p>
<pre><strong>import</strong> spark.implicits._</pre>
<p>Now, let's create the training set. We read the CSV file with Spark's recommended format, <kbd>com.databricks.spark.csv</kbd>. We do not need any explicit schema inference; hence, we are making the <kbd>inferSchema</kbd> <kbd>false</kbd>, but we are using our own schema, which we created previously. Then, we load the data file from our desired location, and finally specify our data source so that our DataFrame looks exactly the same as what we specified:</p>
<pre class="mce-root"><strong>val</strong> trainSet: Dataset[CustomerAccount] = spark.read.<br/>    option("inferSchema", "false")<br/>    .format("com.databricks.spark.csv")<br/>    .schema(schema)<br/>    .load("data/churn-bigml-80.csv")<br/>    .as[CustomerAccount]<br/>trainSet.printSchema()</pre>
<p>As we can see in the following screenshot, the schema of the Spark DataFrame has been correctly identified. However, some of the features are non-numeric but categorical. However, as expected by ML algorithms, all the features have to be numeric (that is, <kbd>integer</kbd> or <kbd>double</kbd> format):<br/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-549 image-border" src="assets/0a954d3d-1355-4db4-b697-46555926af92.png" style="width:28.42em;height:20.75em;"/></p>
<p>Excellent! It looks exactly the same as the data structure. Now, let's look at some sample data by using the <kbd>show()</kbd> method, as follows:</p>
<pre>trainSet.show()</pre>
<p>The output of the preceding line of code shows the first 20 samples of the DataFrame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-550 image-border" src="assets/d567b228-865e-4c27-9d1f-99b719fb9c49.png" style="width:163.33em;height:58.33em;"/></p>
<p class="mce-root"/>
<p>In the preceding screenshot, column names are made shorter for visibility. We can also see related statistics of the training set by using the <kbd>describe()</kbd> method:</p>
<pre><strong>val</strong> statsDF = trainSet.describe()  <br/>statsDF.show()</pre>
<p>The following summary statistics not only give us some idea on the distribution with mean and standard deviation of the data, but also some descriptive statistics, such as number samples (that is, count), minimum value, and maximum value for each feature in the DataFrame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-551 image-border" src="assets/9d3a2a16-f183-424e-85d9-6faca2aeb658.png" style="width:141.67em;height:29.83em;"/></p>
<p>If this dataset can fit into RAM, we can cache it for quick and repeated access by using the <kbd>cache()</kbd> method from Spark:</p>
<pre>trainSet.cache()</pre>
<p>Let's look at some useful properties such as variable correlation with <kbd>churn</kbd>. For example, let's see how the <kbd>churn</kbd> is related with total number of international calls:</p>
<pre>trainSet.groupBy("churn").sum("total_international_num_calls").show()</pre>
<p class="mce-root">As we can see from the following output, customers who make more international calls are less likely (that is, <kbd>False</kbd>) to change operator: <span class="packt_screen"><br/></span></p>
<pre><strong>+-----+----------------------------------+</strong><br/><strong> |churn|sum(total_international_num_calls)|</strong><br/><strong> +-----+----------------------------------+</strong><br/><strong> |False|                            3310.0|</strong><br/><strong> |True |                             856.0|</strong><br/><strong> +-----+----------------------------------+</strong></pre>
<p>Let's see how the <kbd>churn</kbd> is related to total international call charges:</p>
<pre>trainSet.groupBy("churn").sum("total_international_charge").show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">As we can see from the following output, customers who make more international calls (as shown earlier) are charged more, but are still less likely (that is, <kbd>False</kbd>) to change operator:<span class="packt_screen"><br/></span></p>
<pre><span class="packt_screen"> </span><strong>+-----+-------------------------------+</strong><br/><strong> |churn|sum(total_international_charge)|</strong><br/><strong> +-----+-------------------------------+</strong><br/><strong> |False|              6236.499999999996|</strong><br/><strong> | True|                        1133.63|</strong><br/><strong> +-----+-------------------------------+</strong></pre>
<p>Now that we also need to have the test set prepared to evaluate the model, let's prepare the same set, similar to the train set, as follows:</p>
<pre class="mce-root"><strong>val</strong> testSet: Dataset[CustomerAccount] = spark.read<br/>      .option("inferSchema", "false")<br/>      .format("com.databricks.spark.csv")<br/>      .schema(schema)<br/>      .load("data/churn-bigml-20.csv")<br/>      .as[CustomerAccount]</pre>
<p>Now, let's cache them for faster and quick access for further manipulation:</p>
<pre>testSet.cache()</pre>
<p>Let's look at some of the related properties of the training set to understand how suitable it is for our purposes. First, let's create a temp view for persistence for this session. Nevertheless, we can create a catalog as an interface that can be used to create, drop, alter, or query underlying databases, tables, functions, and so on:</p>
<pre>trainSet.createOrReplaceTempView("UserAccount") <br/>spark.catalog.cacheTable("UserAccount")</pre>
<p>We can now group the data by the <kbd>churn</kbd> label and count the number of instances in each group, as follows:</p>
<pre>trainSet.groupBy("churn").count.show()</pre>
<p class="mce-root">The preceding line should show that only <kbd>388</kbd> customers are likely to switch to another operator. However, <kbd>2278</kbd> customers still have their current operator as their preferred one: </p>
<pre class="mce-root"><strong> +-----+-----+</strong><br/><strong> |churn|count|</strong><br/><strong> +-----+-----+</strong><br/><strong> |False| 2278|</strong><br/><strong> | True| 388 |</strong><br/><strong> +-----+-----+</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>So, we have roughly seven times more <kbd>False</kbd> churn samples than <kbd>True</kbd> churn samples. Since the target is to retain the customers who are most likely to leave, we will prepare our training set so that it ensures that the predictive ML model is sensitive to the <kbd>True</kbd> churn samples.</p>
<p>Also, since the training set is highly unbalanced, we should downsample the <kbd>False</kbd> <kbd>churn</kbd> class to a fraction of 388/2278, which gives us <kbd>0.1675</kbd>:</p>
<pre><strong>val</strong> fractions = Map("False" -&gt; 0.1675, "True" -&gt; 1.0)</pre>
<p>This way, we are also mapping only <kbd>True</kbd> churn samples. Now, let's create a new DataFrame for the training set containing the samples from the downsample one only using the <kbd>sampleBy()</kbd> method:</p>
<pre><strong>val</strong> churnDF = trainSet.stat.sampleBy("churn", fractions, 12345L)</pre>
<p>The third parameter is the seed that's used for reproducibility purposes. Let's take a look at this:</p>
<pre>churnDF.groupBy("churn").count.show()</pre>
<p class="mce-root">Now, we can see that the classes are almost balanced:</p>
<pre class="mce-root"><strong><span class="packt_screen"> </span>+-----+-----+</strong><br/><strong> |churn|count|</strong><br/><strong> +-----+-----+</strong><br/><strong> |False|  390|</strong><br/><strong> | True|  388|</strong><br/><strong> +-----+-----+</strong></pre>
<p>Now, let's see how the variables are related to each other. Let's see how the day, night, evening, and international voice calls contribute to the <kbd>churn</kbd> class:</p>
<pre>spark.sql()("<strong>SELECT</strong> churn, <strong>SUM</strong>(total_day_charge) as TDC, <br/>                                    <strong>SUM</strong>(total_evening_charge) as TEC, <strong>SUM</strong>(total_night_charge) as TNC, <br/>                                    <strong>SUM</strong>(total_international_charge) as TIC, <br/>                                    <strong>SUM</strong>(total_day_charge) + <strong>SUM</strong>(total_evening_charge) +  <br/>                                    <strong>SUM</strong>(total_night_charge) + <strong>SUM</strong>(total_international_charge) <br/>                    as Total_charge <strong>FROM</strong> UserAccount <strong>GROUP</strong> <strong>BY</strong> churn <br/>                    <strong>ORDER BY</strong> Total_charge <strong>DESC</strong>").show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>This, however, doesn't give any clear correlation because customers who are likely to stay make more day, night, evening, and international voice calls than the other customers who want to leave:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-552 image-border" src="assets/270ed293-4589-43ff-9b37-c742898c2e8d.png" style="width:137.75em;height:19.58em;"/></p>
<p>Now, let's see how many minutes of voice calls on day, night, evening, and international voice calls have contributed to the preceding <kbd>Total_charge</kbd> for the <kbd>churn</kbd> class:</p>
<pre>spark.sql()("<strong>SELECT</strong> churn, <strong>SUM</strong>(total_day_mins) + <br/>                    <strong>SUM</strong>(total_evening_mins) + <br/>                    <strong>SUM</strong>(total_night_mins) + <br/>                    <strong>SUM</strong>(total_international_mins) as Total_minutes <br/>                    <strong>FROM</strong> UserAccount <strong>GROUP BY</strong> churn")<br/>                .show()</pre>
<p class="mce-root">From the preceding two tables, it is clear that the total day minutes and total day charge are a highly correlated feature in this training set, which is not beneficial for our ML model training. Therefore, it would be better to remove them altogether. Let's drop one column of each pair of correlated fields, along with the <kbd>state_code</kbd> and <kbd>area_code</kbd> columns too since those won't be used:</p>
<pre class="mce-root"><strong>val</strong> trainDF = churnDF<br/>    .<strong>drop</strong>("state_code")<br/>    .<strong>drop</strong>("area_code")<br/>    .<strong>drop</strong>("voice_mail_plan")<br/>    .<strong>drop</strong>("total_day_charge")<br/>    .<strong>drop</strong>("total_evening_charge")</pre>
<p>Excellent! Finally, we have our training DataFrame, which can be used for better, predictive modeling. Let's take a look at some of the columns of the resulting DataFrame:</p>
<pre>trainDF.select("account_length", "international_plan", "num_voice_mail", <br/>                "total_day_calls","total_international_num_calls", "churn")<br/>             .show(10)</pre>
<p>However, we are not done yet—the current DataFrame cannot be fed to the model. This is known as an estimator. As we described earlier, our data needs to be converted into a Spark DataFrame format consisting of labels (in <kbd>Double</kbd>) and features (in <kbd>Vector</kbd>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-563 image-border" src="assets/596a64fd-2b61-4725-95a9-d7d111d75fe7.png" style="width:156.42em;height:50.00em;"/></p>
<p>Now, we need to create a pipeline to pass the data through by chaining several transformers and estimators. The pipeline then works as a feature extractor. More specifically, we have prepared two <kbd>StringIndexer</kbd> transformers and a <kbd>VectorAssembler</kbd>.</p>
<p>The first <kbd>StringIndexer</kbd> converts the <kbd>String</kbd> categorical feature <kbd>international_plan</kbd> and labels into number indices. The second <kbd>StringIndexer</kbd> converts the categorical label (that is, <kbd>churn</kbd>) into numeric format. This way, indexing categorical features allows decision trees and random forest-like classifiers to treat categorical features appropriately, thus improving performance:</p>
<pre class="mce-root"><strong>val</strong> ipindexer = <strong>new StringIndexer()</strong><br/>      .setInputCol("international_plan")<br/>      .setOutputCol("iplanIndex")<br/><br/><strong>val</strong> labelindexer = <strong>new StringIndexer()</strong><br/>      .setInputCol("churn")<br/>      .setOutputCol("label")</pre>
<p>Now, we need to extract the most important features that contribute to the classification. Since we have dropped some columns already, the resulting column set consists of the following fields:</p>
<ul>
<li>Label → churn: <kbd>True</kbd> or <kbd>False</kbd></li>
<li>Features → {<kbd>account_length</kbd>, <kbd>iplanIndex</kbd>, <kbd>num_voice_mail</kbd>, <kbd>total_day_mins</kbd>, <kbd>total_day_calls</kbd>, <kbd>total_evening_mins</kbd>, <kbd>total_evening_calls</kbd>, <kbd>total_night_mins</kbd>, <kbd>total_night_calls</kbd>, <kbd>total_international_mins</kbd>, <kbd>total_international_calls</kbd>, <kbd>total_international_num_calls</kbd>}</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since we have already converted categorical labels into numeric ones using <kbd>StringIndexer</kbd>, the next task is to extract the features:</p>
<pre><strong>val</strong> featureCols = <strong>Array</strong>("account_length", "iplanIndex", "num_voice_mail", <br/>                        "total_day_mins", "total_day_calls", "total_evening_mins", <br/>                        "total_evening_calls", "total_night_mins", "total_night_calls", <br/>                        "total_international_mins", "total_international_calls", <br/>                        "total_international_num_calls")<strong><br/></strong></pre>
<p>Now, let's transform the features into feature vectors using <kbd>VectorAssembler()</kbd>, which takes all the <kbd>featureCols</kbd> and combines/transforms them into a single column called features:</p>
<pre class="mce-root"><strong>val</strong> assembler = <strong>new VectorAssembler</strong>()<br/>      .setInputCols(featureCols)<br/>      .setOutputCol("features")</pre>
<p>Now that we have the real training set, which consists of label and feature vectors ready, the next task is to create an estimator—the third element of a pipeline. We will start with a very simple but powerful LR classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LR for churn prediction</h1>
                </header>
            
            <article>
                
<p>LR is an algorithm for classification, which predicts a binary response. It is similar to linear regression, which we described in <a href="f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml" target="_blank">Chapter 2</a>, <em>Scala for Regression Analysis</em>, except that it does not predict continuous values—<span>it predicts </span>discrete classes. The loss function is the sigmoid function<span> (or logistic function)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/29b314c7-58a8-45bc-bb3f-1d5aca687aed.png" style="width:16.00em;height:1.42em;"/></p>
<p>Similar to linear regression, <span>the intuition behind the cost function is to penalize models that have large errors between the real response and the predicted response:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-554 image-border" src="assets/9008c370-be7a-40c3-b97d-e0a9ec94501e.png" style="width:30.08em;height:19.50em;"/></p>
<p>For a given new data point, <strong>x</strong><em>,</em> the LR model makes predictions using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8193a7b2-cee1-4ccc-86f3-365b682b6739.png" style="width:7.17em;height:2.67em;"/></p>
<p>In the preceding equation, the logistic function is applied to the regression to get the probabilities of it belonging in either class, where <em>z = w<sup>T</sup> x</em> and if <em>f(w<sup>T</sup> x) &gt; 0.5</em>, the outcome is positive; otherwise, it is negative. This means that the <span>threshold for the classification line is assumed to be at <em>0.5</em></span>.<span><br/></span></p>
<p>Now that we know how the LR algorithm works, let's start using Spark ML-based LR estimator development, which will predict whether a customer is likely to get churn or not. First, we need to define some hyperparameters to train a LR-based pipeline:</p>
<pre class="mce-root"><strong>val</strong> numFolds = 10<br/><strong>val</strong> MaxIter: Seq[Int] = Seq(100)<br/><strong>val</strong> RegParam: Seq[Double] = Seq(1.0) // L2 regularization param set 1.0 with L1 reg. to reduce overfitting<br/><strong>val</strong> Tol: Seq[Double] = Seq(1e-8)// for convergence tolerance for iterative algorithms<br/><strong>val</strong> ElasticNetParam: Seq[Double] = Seq(0.0001) //Combination of L1 &amp; L2</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><kbd>RegParam</kbd> is a scalar that helps us adjust the strength of the constraints: a small value implies a soft margin, while a large value implies a hard margin. The <kbd>Tol</kbd> parameter is used for the convergence tolerance for iterative algorithms, such as LR or linear SVM. Once we have the hyperparameters defined and initialized, our next task is to instantiate a LR estimator, as follows:</p>
<pre class="mce-root"><strong>val</strong> lr = new LogisticRegression()<br/>         .setLabelCol("label")<br/>         .setFeaturesCol("features")</pre>
<p>Now, let's build a pipeline estimator using the <kbd>Pipeline()</kbd> method to chain three transformers (the <kbd>ipindexer</kbd>, <kbd>labelindexer</kbd>, and <kbd>assembler</kbd> vectors) and the LR estimator (that is, <kbd>lr</kbd>) in a single pipeline—that is, each of them acts as a stage:</p>
<pre class="mce-root"><strong>val</strong> pipeline = <strong>new Pipeline()</strong><br/>            .setStages(Array(PipelineConstruction.ipindexer,<br/>                  PipelineConstruction.labelindexer,<br/>                        PipelineConstruction.assembler, lr))</pre>
<div class="packt_infobox">
<p>A Spark ML pipeline can have the following components:</p>
<ul>
<li><strong>DataFrame</strong>: To hold original data and intermediate transformed ones.</li>
</ul>
<ul>
<li><strong>Transformer</strong>: Used to transform one DataFrame into another by adding additional feature columns.</li>
<li><strong>Estimator</strong>: An estimator is an ML model, such as linear regression.</li>
<li><strong>Pipeline</strong>: Used to chain the preceding components, DataFrame, transformer, and estimator together.</li>
<li><strong>Parameter</strong>: An ML algorithm has many knobs to tweak. These are called hyperparameters, and the values learned by an ML algorithm to fit the data are called parameters.</li>
</ul>
</div>
<p>In order to perform such a grid search over the hyperparameter space, we need to define it first. Here, the functional programming properties of Scala are quite handy because we just add function pointers and the respective parameters to be evaluated to the parameter grid. Here, a cross-validation evaluator will search through LR's max iteration, regularization param, tolerance, and elastic net for the best model:</p>
<pre class="mce-root"><strong>val</strong> paramGrid = <strong>new ParamGridBuilder()</strong><br/>      .addGrid(lr.maxIter, MaxIter)<br/>      .addGrid(lr.regParam, RegParam)<br/>      .addGrid(lr.tol, Tol)<br/>      .addGrid(lr.elasticNetParam, ElasticNetParam)<br/>      .build()</pre>
<div>
<p>Note that the hyperparameters form an <em>n</em>-dimensional space, where <em>n</em> is the number of hyperparameters. Every point in this space is one particular hyperparameter configuration, which is a hyperparameter vector. Of course, we can't explore every point in this space, so what we basically do is a grid search over an (ideally evenly distributed) subset in that space. We then need to define a <kbd>BinaryClassificationEvaluator</kbd> evaluator, since this is a binary classification problem:</p>
</div>
<pre class="mce-root"><strong>val</strong> evaluator = <strong>new BinaryClassificationEvaluator()</strong><br/>                  .setLabelCol("label")<br/>                  .setRawPredictionCol("prediction")</pre>
<p>We use a <kbd>CrossValidator</kbd> by using <kbd>ParamGridBuilder</kbd> to iterate through the max iteration, regression param, tolerance, and elastic net parameters of LR with 10-fold cross-validation: </p>
<pre class="mce-root"><strong>val</strong> crossval = <strong>new CrossValidator()</strong><br/>      .setEstimator(pipeline)<br/>      .setEvaluator(evaluator)<br/>      .setEstimatorParamMaps(paramGrid)<br/>      .setNumFolds(numFolds)</pre>
<p>The preceding code is meant to perform cross-validation. The validator itself uses the <kbd>BinaryClassificationEvaluator</kbd> estimator to evaluate the training in the progressive grid space on each fold and make sure that no overfitting occurs.</p>
<p>Although there is so much stuff going on behind the scenes, the interface of our <kbd>CrossValidator</kbd> object stays slim and well known as <kbd>CrossValidator</kbd> also extends from the estimator and supports the <kbd>fit</kbd> method. This means that, after calling the <kbd>fit</kbd> method, the complete predefined pipeline, including all feature preprocessing and the LR classifier, is executed multiple times—each time with a different hyperparameter vector:</p>
<pre><strong>val</strong> cvModel = crossval.fit(Preprocessing.trainDF)</pre>
<p>Now, it's time to evaluate the LR model using the test dataset. First, we need to transform the test set, similar to the training set we described previously:</p>
<pre class="mce-root"><strong>val</strong> predDF= cvModel.transform(Preprocessing.testSet)<br/><strong>val</strong> result = predDF.select("label", "prediction", "probability")<br/><strong>val</strong> resutDF = result.withColumnRenamed("prediction", "Predicted_label")<br/>resutDF.show(10)</pre>
<p>The preceding code block shows the <kbd>Predicted_label</kbd> and the raw <kbd>probability</kbd> that were generated by the model. Additionally, it shows the actual labels. As we can see, for some instances, the model predicted correctly, but for other instances, it got confused:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-555 image-border" src="assets/4046ceff-8ab9-4903-8201-afe6a5b67485.png" style="width:29.33em;height:20.00em;"/></p>
<p>The prediction probabilities can also be very useful in ranking customers according to their likeliness of imperfection. This way, a limited number of resources can be utilized in telecommunication business that can be focused on the most valuable customers. However, by looking at the preceding prediction DataFrame, it is difficult to guess the classification accuracy. However, in the second step, the evaluator evaluates itself using <kbd>BinaryClassificationEvaluator</kbd>, as follows:</p>
<pre><strong>val</strong> accuracy = evaluator.evaluate(predDF)<br/>println("Classification accuracy: " + accuracy)</pre>
<p>This should show around 77% classification accuracy from our binary classification model: </p>
<pre><strong>Classification accuracy: 0.7679333824070667</strong></pre>
<p>We compute another performance metric called area under the precision-recall curve and the area under the ROC curve. For this, we can construct an RDD containing the raw scores on the test set:</p>
<pre class="mce-root"><strong>val</strong> predictionAndLabels = predDF<br/>      .select("prediction", "label")<br/>      .rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1)<br/>      .asInstanceOf[Double]))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, the preceding RDD can be used to compute the aforementioned performance metrics:</p>
<pre><strong>val</strong> metrics = <strong>new</strong> BinaryClassificationMetrics(predictionAndLabels)<br/>println("Area under the precision-recall curve: " + metrics.areaUnderPR)<br/>println("Area under the receiver operating characteristic (ROC) curve : " + metrics.areaUnderROC)</pre>
<p>In this case, the evaluation returns 77% accuracy, but only 58% precision: </p>
<pre><strong>Area under the precision-recall curve: 0.5770932703444629</strong><br/><strong>Area under the receiver operating characteristic (ROC) curve: 0.7679333824070667</strong></pre>
<p>In the following code, we are calculating some more metrics. False and true positive and negative predictions are also useful to evaluate the model's performance. Then, we print the results to see the metrics, as follows:</p>
<pre><strong>val</strong> tVSpDF = predDF.select("label", "prediction") // True vs predicted labels<br/><strong>val</strong> TC = predDF.count() //Total count<br/><br/><strong>val</strong> tp = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> tn = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> fp = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/><strong>val</strong> fn = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/>    <br/>println("True positive rate: " + tp *100 + "%")<br/>println("False positive rate: " + fp * 100 + "%")<br/>println("True negative rate: " + tn * 100 + "%")<br/>println("False negative rate: " + fn * 100 + "%")<strong><br/></strong></pre>
<p class="mce-root">The preceding code segment shows the true positive, false positive, true negative, and false negative rates, which we will use to compute the MCC score later on:</p>
<pre class="mce-root"><strong>True positive rate: 66.71664167916042%</strong><br/><strong>False positive rate: 19.04047976011994%</strong><br/><strong>True negative rate: 10.944527736131935%</strong><br/><strong>False negative rate: 3.2983508245877062%</strong></pre>
<p class="mce-root"/>
<p class="mce-root">Finally, we also compute the MCC score, as follows:</p>
<pre><strong>val</strong> MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))<br/>println("Matthews correlation coefficient: " + MCC)</pre>
<p class="mce-root">The preceding line gives a Matthews correlation coefficient of <kbd>0.41676531680973805</kbd>. This is a positive value, which gives us some sign of a robust model. However, we have not received good accuracy yet, so let's move on and try other classifiers, such as NB. This time, we will use the liner NB implementation from the Apache Spark ML package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NB for churn prediction</h1>
                </header>
            
            <article>
                
<p>The NB classifier is based on Bayes' theorem, with the following assumptions:</p>
<ul>
<li>Independence between every pair of features</li>
<li>Feature values are non-negative, such as counts</li>
</ul>
<p>For example, if cancer is related to age, this can be used to assess the probability that a patient might have cancer<em>.</em> Bayes' theorem is stated mathematically as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-556 image-border" src="assets/ff5bdc71-63cf-4e09-b271-1b58ec82a648.png" style="width:16.83em;height:6.00em;"/></p>
<p>In the preceding equation, <em>A</em> and <em>B</em> are events with <em>P (B) ≠ 0</em>. The other terms can be described as follows:</p>
<ul>
<li><em>P</em> (<em>A</em> | <em>B</em>) is called the posterior or the conditional probability of observing event <em>A</em>, given that <em>B</em> is true</li>
<li><em>P</em> (<em>B</em>| <em>A</em>) is the likelihood of event <em>B</em> given that <em>A</em> is true</li>
<li><em>P(A)</em> is the prior and <em>P(B)</em> is the prior probability, also called marginal likelihood or marginal probability</li>
</ul>
<p class="mce-root"/>
<p>Gaussian NB is a generalized version of NB that's used for classification, which is based on the binomial distribution of data. For example, our churn prediction problem can be formulated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-557 image-border" src="assets/27623093-baba-475e-9920-c07b85deede3.png" style="width:28.25em;height:11.75em;"/></p>
<p>The preceding list can be adopted to solve our problem as follows:</p>
<ul class="postList">
<li class="graf graf--li graf-after--figure"><em>P(class|data)</em> is the posterior probability of the <em>class</em> to be predicted by modelling with an independent variable (<em>data</em>)</li>
<li><em>P(data|class)</em> is the likelihood or the probability of the predictor, given <em>class</em></li>
<li class="graf graf--li graf-after--li"><em>P(class)</em> is the prior probability of <em>class</em> and <em>P(data)</em> of the predictor or marginal likelihood</li>
</ul>
<p>The well-known Harvard study on happiness shows that only 10% of happy people are rich. Although you might think that this statistic is very compelling, you might be somewhat interested in knowing the percentage of rich people who are also really happy. Bayes' theorem helps you out with calculating this reserving statistic using two additional clues:</p>
<ul>
<li>The percentage of people overall who are happy—that is, <em>P(A)</em></li>
<li>The percentage of people overall who are rich—that is, <em>P(B)</em></li>
</ul>
<p>The key idea behind Bayes' theorem is reversing the statistic by considering the overall rates. Suppose the following pieces of information are available prior:</p>
<ul>
<li>40% of people are happy =&gt; <em>P(A)</em> </li>
<li>5% of people are rich =&gt; <em>P(B)</em></li>
</ul>
<p>Now, let's consider that the Harvard study is correct—that is, <em>P(B|A) = 10%</em>. Now that we know the fraction of rich people who are happy, <em>P(A|B)</em> can be calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>P(A|B) = {P(A)* P(B|A)} / P(B) = (40%*10%)/5% = 80%</em></p>
<p>Consequently, a majority of people are also happy! Nice. To make this clearer, let's assume that the population of the whole world is 5,000, for simplicity. According to our calculation, two facts exist:</p>
<ul>
<li><strong>Fact 1</strong>: This tells us there are 500 people are happy, and the Harvard study tells us that 50 of these happy people are also rich</li>
<li><strong>Fact 2</strong>: There are 60 rich people altogether, so the fraction of them who are happy is 50/60 ~ 83%</li>
</ul>
<p>This proves Bayes theorem and its effectiveness. To use NB, we need to instantiate an NB estimator, as follows:</p>
<pre class="mce-root"><strong>val</strong> nb = <strong>new NaiveBayes</strong>()<br/>      .setLabelCol("label")<br/>      .setFeaturesCol("features")</pre>
<p>Now that we have transformers and an estimator ready, the next task is to chain in a single pipeline—that is, each of them acts as a stage:</p>
<pre class="mce-root"><strong>val</strong> pipeline = <strong>new Pipeline</strong>()<br/>      .setStages(Array(PipelineConstruction.ipindexer,<br/>        PipelineConstruction.labelindexer,<br/>        PipelineConstruction.assembler,nb))</pre>
<p>Let's define the <kbd>paramGrid</kbd> to perform such a grid search over the hyperparameter space. Then the cross-validator will search for the best model through the NB's <kbd>smoothing</kbd> parameter. Unlike LR or SVM, there is no hyperparameter in the NB algorithm:</p>
<pre class="mce-root"><strong>    val</strong> paramGrid = <strong>new ParamGridBuilder</strong>()<br/>      .addGrid(nb.smoothing, Array(1.0, 0.1, 1e-2, 1e-4))// default value is 1.0<br/>      .build()</pre>
<div class="packt_tip packt_infobox">Additive smoothing, or Laplace smoothing, is a technique that's used to smooth categorical data.</div>
<p>Let's define a <kbd>BinaryClassificationEvaluator</kbd> evaluator to evaluate the model:</p>
<pre class="mce-root"><strong>val</strong> evaluator = <strong>new BinaryClassificationEvaluator</strong>()<br/>                  .setLabelCol("label")<br/>                  .setRawPredictionCol("prediction")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We use a <kbd>CrossValidator</kbd> to perform 10-fold cross-validation for best model selection:</p>
<pre class="mce-root"><strong>val</strong> crossval = <strong>new CrossValidator</strong>()<br/>      .setEstimator(pipeline)<br/>      .setEvaluator(evaluator)<br/>      .setEstimatorParamMaps(paramGrid)<br/>      .setNumFolds(numFolds)</pre>
<p>Let's call the <kbd>fit()</kbd> method so that the complete predefined <kbd>pipeline</kbd>, including all feature preprocessing and the LR classifier, is executed multiple times—each time with a different hyperparameter vector:</p>
<pre><strong>val</strong> cvModel = crossval.fit(Preprocessing.trainDF)</pre>
<p>Now, it's time to evaluate the predictive power of the SVM model on the test dataset. First, we need to transform the test set with the model pipeline, which will map the features according to the same mechanism we described in the preceding feature engineering step:</p>
<pre><strong>val</strong> predDF = cvModel.transform(Preprocessing.testSet)</pre>
<p>However, by looking at the preceding prediction DataFrame, it is difficult to guess the classification accuracy. However, in the second step, the evaluator evaluates itself using <kbd>BinaryClassificationEvaluator</kbd>, as follows:</p>
<pre><strong>val</strong> accuracy = evaluator.evaluate(predDF)<br/>println("Classification accuracy: " + accuracy)</pre>
<p>The preceding line of code should show 75% classification accuracy for our binary classification model: </p>
<pre><strong>Classification accuracy: 0.600772911299227</strong></pre>
<p>Like we did previously, we construct an RDD containing the raw scores on the test set:</p>
<pre><strong>val</strong> predictionAndLabels = predDF.select("prediction", "label")<br/>      .rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1)<br/>        .asInstanceOf[Double]))</pre>
<p>Now, the preceding RDD can be used to compute the aforementioned performance metrics:</p>
<pre><strong>val</strong> metrics = <strong>new</strong> BinaryClassificationMetrics(predictionAndLabels)<br/>println("Area under the precision-recall curve: " + metrics.areaUnderPR)<br/>println("Area under the receiver operating characteristic (ROC) curve : " + metrics.areaUnderROC)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this case, the evaluation returns 75% accuracy but only 55% precision: </p>
<pre><strong>Area under the precision-recall curve: 0.44398397740763046</strong><br/><strong>Area under the receiver operating characteristic (ROC) curve: 0.600772911299227</strong></pre>
<p>In the following code, again, we calculate some more metrics. False and true positive and negative predictions are also useful to evaluate the model's performance:</p>
<pre><strong>val</strong> tVSpDF = predDF.select("label", "prediction") // True vs predicted labels<br/><strong>val</strong> TC = predDF.count() //Total count<br/><br/><strong>val</strong> tp = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> tn = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> fp = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/><strong>val</strong> fn = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/>    <br/>println("True positive rate: " + tp *100 + "%")<br/>println("False positive rate: " + fp * 100 + "%")<br/>println("True negative rate: " + tn * 100 + "%")<br/>println("False negative rate: " + fn * 100 + "%")<strong> </strong></pre>
<p class="mce-root">The preceding code segment shows the true positive, false positive, true negative, and false negative rates, which we will use to compute the MCC score later on:</p>
<pre class="mce-root"><strong>True positive rate: 66.71664167916042%</strong><br/><strong>False positive rate: 19.04047976011994%</strong><br/><strong>True negative rate: 10.944527736131935%</strong><br/><strong>False negative rate: 3.2983508245877062%</strong></pre>
<p class="mce-root">Finally, we also compute the MCC score, as follows:</p>
<pre><strong>val</strong> MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))<br/>println("Matthews correlation coefficient: " + MCC)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding line gives a Matthews correlation coefficient of <kbd>0.14114315409796457</kbd> and this time, we experienced even worse performance in terms of accuracy and MCC scores. So, it's worth trying this with another classifier, such as SVM. We will use the linear SVM implementation from the Spark ML package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SVM for churn prediction</h1>
                </header>
            
            <article>
                
<p>SVM is also a population algorithm for classification. SVM is based on the concept of decision planes, which defines the decision boundaries we discussed at the beginning of this chapter. The following diagram shows how the SVM algorithm works:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-558 image-border" src="assets/b56682ec-0673-457c-abc0-bb7d2e504bbf.png" style="width:25.00em;height:14.58em;"/></p>
<p>SVM uses kernel function, which finds the linear hyperplane that separates classes with the maximum margin. The following diagram shows how the data points (that is, support vectors) belonging to two different classes (red versus blue) are separated using the decision boundary based on the maximum margin:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-559 image-border" src="assets/bb7dd951-c399-42d4-bddb-f43ae3fd151c.png" style="width:25.83em;height:12.67em;"/></p>
<p>The preceding support vector classifier can be represented as a dot product mathematically, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-560 image-border" src="assets/d6931a67-3fe2-4fb8-90b5-61ff9a5c4db2.png" style="width:23.08em;height:8.00em;"/></p>
<p>If the data to be separated is very high-dimensional, the kernel trick uses the kernel function to transform the data into a higher-dimensional feature space so that they can be linearly separable for classification. Mathematically, the kernel trick is to replace the dot product with the kernel, which will allow for non-linear decision boundaries and computational efficiency:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-561 image-border" src="assets/295208ef-c737-4e7c-9c4a-31742453c623.png" style="width:13.50em;height:2.92em;"/></p>
<p>Now that we already know about SVMs, let's start using the Spark-based implementation of SVM. First, we need to define some hyperparameters to train an LR-based pipeline:</p>
<pre class="mce-root"><strong>val</strong> numFolds = 10<br/><strong>val</strong> MaxIter: Seq[Int] = Seq(100)<br/><strong>val</strong> RegParam: Seq[Double] = Seq(1.0) // L2 regularization param, set 0.10 with L1 regularization<br/><strong>val</strong> Tol: Seq[Double] = Seq(1e-8)<br/><strong>val</strong> ElasticNetParam: Seq[Double] = Seq(1.0) // Combination of L1 and L2</pre>
<p>Once we have the hyperparameters defined and initialized, the next task is to instantiate an SVM estimator, as follows:</p>
<pre><strong>val</strong> svm = <strong>new</strong> LinearSVC()</pre>
<p>Now that we have transformers and an estimator ready, the next task is to chain in a single pipeline—that is, each of them acts as a stage:</p>
<pre class="mce-root"><strong>val</strong> pipeline = <strong>new Pipeline()</strong><br/>      .setStages(Array(PipelineConstruction.ipindexer,<br/>        PipelineConstruction.labelindexer,<br/>        PipelineConstruction.assembler,svm))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's define the <kbd>paramGrid</kbd> to perform such a grid search over the hyperparameter space. This searches through SVM's max iteration, regularization param, tolerance, and elastic net for the best model:</p>
<pre class="mce-root"><strong>val</strong> paramGrid = <strong>new ParamGridBuilder()</strong><br/>      .addGrid(svm.maxIter, MaxIter)<br/>      .addGrid(svm.regParam, RegParam)<br/>      .addGrid(svm.tol, Tol)<br/>      .addGrid(svm.elasticNetParam, ElasticNetParam)<br/>      .build()</pre>
<p>Let's define a <kbd>BinaryClassificationEvaluator</kbd> evaluator to evaluate the model:</p>
<pre class="mce-root"><strong>val</strong> evaluator = <strong>new BinaryClassificationEvaluator</strong>()<br/>                  .setLabelCol("label")<br/>                  .setRawPredictionCol("prediction")</pre>
<p>We use a <kbd>CrossValidator</kbd> to perform 10-fold cross-validation for best model selection: </p>
<pre class="mce-root"><strong>val</strong> crossval = <strong>new CrossValidator</strong>()<br/>      .setEstimator(pipeline)<br/>      .setEvaluator(evaluator)<br/>      .setEstimatorParamMaps(paramGrid)<br/>      .setNumFolds(numFolds)</pre>
<p>Now, let's call the fit method so that the complete predefined <kbd>pipeline</kbd>, including all feature preprocessing and the LR classifier, is executed multiple times—each time with a different hyperparameter vector:</p>
<pre><strong>val</strong> cvModel = crossval.fit(Preprocessing.trainDF)</pre>
<p>Now, it's time to evaluate the predictive power of the SVM model on the test dataset:</p>
<pre><strong>val</strong> predDF= cvModel.transform(Preprocessing.testSet)<br/>predDF.show(10)</pre>
<p>The preceding code block shows the predicted label and the raw probability generated by the model. Additionally, it shows the actual labels.</p>
<p>As we can see, for some instances, the model predicted correctly, but for some other instances, it got confused:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-562 image-border" src="assets/525773ae-dfca-4d02-ac7d-5e9ccfd6a72e.png" style="width:23.42em;height:16.50em;"/></p>
<p>However, by looking at the preceding prediction DataFrame, it is difficult to guess the classification accuracy. However, in the second step, the evaluator evaluates itself using <kbd>BinaryClassificationEvaluator</kbd>, as follows:</p>
<pre><strong>val</strong> accuracy = evaluator.evaluate(predDF)<br/>println("Classification accuracy: " + accuracy)</pre>
<p>Therefore, we get about 75% classification accuracy from our binary classification model:</p>
<pre><strong>Classification accuracy: 0.7530180345969819</strong></pre>
<p>Now, we construct an RDD containing the raw scores on the test set, which will be used to compute performance metrics such as area under the precision-recall curve (AUC) and are under the received operating characteristic curve (ROC):</p>
<pre class="mce-root"><strong>val</strong> predictionAndLabels = predDF<br/>      .select("prediction", "label")<br/>      .rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1)<br/>      .asInstanceOf[Double]))</pre>
<p>Now, the preceding RDD can be used to compute the aforementioned performance metrics:</p>
<pre><strong>val</strong> metrics = <strong>new</strong> <strong>BinaryClassificationMetrics</strong>(predictionAndLabels) <br/>println("Area under the precision-recall curve: " + metrics.areaUnderPR)<br/>println("Area under the receiver operating characteristic (ROC) curve : " + metrics.areaUnderROC)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>In this case, the evaluation returns 75% accuracy, but only 55% precision: </p>
<pre><strong>Area under the precision-recall curve: 0.5595712265324828</strong><br/><strong>Area under the receiver operating characteristic (ROC) curve: 0.7530180345969819</strong></pre>
<p>We can also calculate some more metrics; for example, false and true positive and negative predictions are also useful to evaluate the model's performance:</p>
<pre><strong>val</strong> tVSpDF = predDF.select("label", "prediction") // True vs predicted labels<br/><strong>val</strong> TC = predDF.count() //Total count<br/><br/><strong>val</strong> tp = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> tn = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter($"label" === $"prediction")<br/>            .count() / TC.toDouble<br/><br/><strong>val</strong> fp = tVSpDF.filter($"prediction" === 1.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/><strong>val</strong> fn = tVSpDF.filter($"prediction" === 0.0)<br/>            .filter(not($"label" === $"prediction"))<br/>            .count() / TC.toDouble<br/>    <br/>println("True positive rate: " + tp *100 + "%")<br/>println("False positive rate: " + fp * 100 + "%")<br/>println("True negative rate: " + tn * 100 + "%")<br/>println("False negative rate: " + fn * 100 + "%")<strong> </strong><strong> </strong></pre>
<p class="mce-root">The preceding code segment shows the true positive, false positive, true negative, and false negative rates, which we will use to compute the MCC score later on:</p>
<pre class="mce-root"><strong>True positive rate: 66.71664167916042%</strong><br/><strong>False positive rate: 19.04047976011994%</strong><br/><strong>True negative rate: 10.944527736131935%</strong><br/><strong>False negative rate: 3.2983508245877062%</strong></pre>
<p class="mce-root">Finally, we also compute the MCC score, as follows:</p>
<pre><strong>val</strong> MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))<br/>println("Matthews correlation coefficient: " + MCC)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This gave me a Matthews correlation coefficient of <kbd>0.3888239300421191</kbd>. Although we have tried to use as many as three classification algorithms, we still haven't received good accuracy. Considering that SVM managed to give us an accuracy of 76%, this is still considered to be low. Moreover, there is no option for most suitable feature selection, which helps us train our model with the most appropriate features. To improve classification accuracy, we will need to use tree-based approaches, such as DT, RF, and GBT, which are expected to provide more powerful responses. We will do this in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned about different classical classification algorithms, such as LR, SVM, and NB. Using these algorithms, we predicted whether a customer is likely to cancel their telecommunications subscription or not. We've also discussed what types of data are required to build a successful churn predictive model.</p>
<p>Tree-based and tree ensemble classifiers are really useful and robust, and are widely used for solving both classification and regression tasks. In the next chapter, we will look into developing such classifiers and regressors using tree-based and ensemble techniques such as DT, RF, and GBT, for both classification and regression.</p>


            </article>

            
        </section>
    </body></html>