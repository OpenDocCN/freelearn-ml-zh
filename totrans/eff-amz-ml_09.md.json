["```py\n{\n \"groups\": {},\n \"assignments\": {},\n \"outputs\": [\n     \"ALL_BINARY\",\n     \"ALL_TEXT\"\n ]\n}\n\n```", "```py\nimport boto3\nclient = boto3.client('machinelearning')\nclient.predict(\n   MLModelId = \"ml-ZHqxUjPNQTq\",\n   Record = { \"SentimentText\": \"Hello world, this is a great day\" },\n   PredictEndpoint = \"https://realtime.machinelearning.us-east-1.amazonaws.com\"\n)\n\n```", "```py\nimport twitter\nclass ATweets():\n    def __init__(self, raw_query):\n         self.twitter_api = twitter.Api(consumer_key='your own key',\n              consumer_secret= 'your own key',\n              access_token_key='your own key',\n              access_token_secret='your own key')\n         self.raw_query = raw_query\n\n    # capture the tweets: see http://python-twitter.readthedocs.io/en/latest/twitter.html \n    def capture(self):\n        statuses = self.twitter_api.GetSearch(\n             raw_query = self.raw_query,\n             lang = 'en',\n             count=100, result_type='recent', include_entities=True\n        )\n     return statuses\n\n```", "```py\ntw = ATweets(raw_query) \nstatuses = tw.capture()\n\n```", "```py\nfrom tweets import ATweets\nimport json\nimport boto3\n\n# The kinesis firehose delivery stream we send data to\nstream_name = \"veggieTweets\" \n\n# Initialize the firehose client\nfirehose = boto3.client('firehose')\n\n# Our own homemade list of vegetables, feel free to add seasoning\nvegetables = ['artichoke','asparagus', 'avocado', 'brocolli','cabbage', 'carrot', 'cauliflower','celery', 'chickpea', 'corn','cucumber', 'eggplant','endive', 'garlic', 'green beans', 'kale', 'leek', 'lentils', 'lettuce','mushroom','okra', 'onion','parsnip', 'potato','pumpkin', 'radish','turnip', 'quinoa', 'rice', 'spinach', 'squash' , 'tomato', 'yams', 'zuchinni']\n\n# Loop over all vegetables\nfor veggie in vegetables:\n # for a given veggie define the query and capture the tweets\n raw_query = 'f=tweets&vertical=default&l=en&q={0}&src=typd'.format(veggie)\n # capture the tweets\n tw = ATweets(raw_query)\n statuses = tw.capture()\n # and for each tweet, cleanup, add other data and send to firehose\n for status in statuses:\n # remove commas and line returns from tweets\n clean_tweet = ''.join([s for s in st.text if s not in [',', 'n']])\n # and build the record to be sent as a comma separated string followed by a line return\n record = ','.join([str(st.id), st.user.screen_name,veggie, clean_tweet]) + 'n'\n # send the record to firehose\n response=firehose.put_record(DeliveryStreamName = stream_name, Record={'Data': record} )\n\n```", "```py\n(st.lang=='en') & (st.retweeted_status is None) & (len(st.text) > 10):\n\n```", "```py\n$ watch -n 600 python producer.py\n\n```", "```py\n848616357398753280,laurenredhead,artichoke,Artichoke gelatin dogs. Just one of many delicious computer-algorithm generated recipe titles:https://t.co/mgI8HtTGfs\n\n```", "```py\n$ psql --host=vegetables.cenllwot8v9r.us-east-1.redshift.amazonaws.com --port=5439 --username=alexperrier --password --dbname=vegetablesdb\n\n```", "```py\nCREATE TABLE IF NOT EXISTS tweets (\n id BIGINT primary key,\n screen_name varchar(255),\n veggie varchar(255),\n text varchar(65535)\n);\n\n```", "```py\nCOPY tweets (id,screen_name, veggie,tb_polarity, text) FROM 's3://aml.packt/<manifest>' CREDENTIALS 'aws_iam_role=arn:aws:iam::<aws-account-id>:role/<role-name>' MANIFEST delimiter ',';\n\n```", "```py\nfrom __future__ import print_function\n\nimport base64\nimport boto3\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nml_client = boto3.client('machinelearning')\n\nprint('Loading function')\n\ndef lambda_handler(event, context):\n  output = []\n  for record in event['records']:\n     payload = base64.b64decode(record['data'])\n     payload = payload.split(',')\n     tweet = payload.pop(4)\n\n     predicted_label, predicted_score = get_sentiment(tweet)\n\n     payload.append(str(predicted_label) )\n     payload.append(str(predicted_score) )\n     payload.append(tweet)\n     payload = ','.join(payload)\n\n     output_record = {\n       'recordId': record['recordId'],\n       'result': 'Ok',\n       'data': base64.b64encode(payload)\n     }\n     output.append(output_record)\n     return {'records': output}\n\n```", "```py\ndef get_sentiment(tweet):\n\n   response = ml_client.predict(\n       MLModelId = \"ml-ZHqxUjPNQTq\",\n       Record = { \"SentimentText\": tweet },\n       PredictEndpoint = \"https://realtime.machinelearning.us-east-1.amazonaws.com\"\n   )\n   predicted_label = response['Prediction']['predictedLabel']\n   predicted_score = response['Prediction']['predictedScores'][predicted_label]\n\n   return predicted_label, predicted_score\n\n```", "```py\ndrop table if exists tweets;\nCREATE TABLE IF NOT EXISTS tweets (\n id BIGINT primary key,\n screen_name varchar(255),\n veggie varchar(255),\n ml_label int,\n ml_score float,\n text varchar(65535)\n);\n\n```", "```py\nunload ('select * from tweets') to 's3://aml.packt/data/veggies/results/' iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole';\n\n```", "```py\n# Download\n$ aws s3 cp s3://aml.packt/data/veggies/results/0000_part_00 data/\n$ aws s3 cp s3://aml.packt/data/veggies/results/0001_part_00 data/\n# Combine\n$ cp data/0000_part_00 data/veggie_tweets.tmp\n$ cat data/0001_part_00 >> data/veggie_tweets.tmp\n\n```", "```py\n$ sed 's/|/,/g' data/veggie_tweets.tmp > data/veggie_tweets.csv\n\n```", "```py\nimport pandas as pd\ndf = pd.read_csv('data/veggie_tweets.csv')\n\n```", "```py\nfrom textblob import TextBlob\nprint(TextBlob(text).sentiment)\n\n```", "```py\nfrom textblob import TextBlob\ndf['tb_polarity'] = 0\nfor i, row in df.iterrows():\n    df.loc[i, 'tb_polarity'] = TextBlob(row['text']).sentiment.polarity\n\n```", "```py\n' '.join([token for token tk in tweet.split(' ') if 'https://t.co/' not in tk])\n\n```", "```py\ndf['new_column'] = df[existing_column].apply( \n                      lambda existing_column : {\n                         some operation or function on existing_column \n                      } \n                   )\n\n```", "```py\ndf['no_urls'] = df['text'].apply(\n                   lambda tweet : ' '.join(\n                       [tk for  tk in tweet.split(' ') if 'https://t.co/' not in tk]\n                   )\n                )\n\n```", "```py\ndf.drop_duplicates(subset= ['no_urls'], inplace = True)\n\n```"]