- en: Working with Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理大数据
- en: The amount of data is increasing at an exponential rate. Today's systems are
    generating and recording information on customer behavior, distributed systems,
    network analysis, sensors, and many, many more sources. While the current big
    trend of mobile data is pushing the current growth, the next big thing—**the Internet
    of Things (IoT)**—is going to further increase the rate of growth.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量正在以指数级增长。今天的系统正在生成和记录有关客户行为、分布式系统、网络分析、传感器以及许多许多其他来源的信息。虽然当前移动数据的大趋势正在推动当前的增长，但下一个大趋势——**物联网（IoT）**——将进一步增加增长速度。
- en: What this means for data mining is a new way of thinking. Complex algorithms
    with high runtimes need to be improved or discarded, while simpler algorithms
    that can deal with more samples are becoming more popular to use. As an example,
    while support vector machines are great classifiers, some variants are difficult
    to use on very large datasets. In contrast, simpler algorithms such as logistic
    regression can manage more easily in these scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这对数据挖掘意味着一种新的思维方式。需要高运行时间的复杂算法需要改进或丢弃，而能够处理更多样本的简单算法越来越受欢迎。例如，虽然支持向量机是出色的分类器，但某些变体在处理非常大的数据集时难以使用。相比之下，像逻辑回归这样的简单算法在这些场景中更容易管理。
- en: This complexity versus distribution issue is just one of the reasons why deep
    neural networks (DNNs) have become so popular. You can create very complex models
    using DNNs, but you can also *distribute* the workload for training them across
    many computers quite easily.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性与分布问题只是深度神经网络（DNNs）变得如此受欢迎的原因之一。您可以使用DNN创建非常复杂的模型，但也可以非常容易地将训练这些模型的负载分布到多台计算机上。
- en: 'In this chapter, we will investigate the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下内容：
- en: Big data challenges and applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据挑战与应用
- en: The MapReduce paradigm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce范式
- en: Hadoop MapReduce
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: mrjob, a Python library to run MapReduce programs on Amazon's AWS infrastructure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mrjob，一个在亚马逊AWS基础设施上运行MapReduce程序的Python库
- en: Big data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据
- en: 'What makes big data different? Most big data proponents talk about the four
    Vs of big data:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据有什么不同之处？大多数大数据倡导者谈论大数据的四个V：
- en: '**Volume**: The amount of data that we generate and store is growing at an
    increasing rate, and predictions of the future generally only suggest further
    increases. Today''s multi-gigabyte-sized hard drives will turn into exabyte-sized
    drives in a few years, and network throughput traffic will be increasing as well.
    The signal-to-noise ratio can be quite difficult, with important data being lost
    in the mountain of non-important data.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体积**：我们生成和存储的数据量正在以越来越快的速度增长，对未来的一般预测通常只会建议进一步的增长。今天的多吉字节大小的硬盘驱动器将在几年内变成艾字节大小的驱动器，网络吞吐量流量也将增加。信号与噪声比可能相当困难，重要数据可能会在非重要数据的山丘中丢失。'
- en: '**Velocity**: While related to volume, the velocity of data is increasing too.
    Modern cars have hundreds of sensors that stream data into their computers, and
    information from these sensors needs to be analyzed at a sub-second level to operate
    the car. It isn''t just a case of finding answers in the volume of data; those
    answers often need to come quickly. In some cases, we also simply do not have
    enough disk space to store data, meaning we also need to make decisions on what
    data to keep for later analysis.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：虽然与体积相关，但数据速度也在增加。现代汽车拥有数百个传感器，将数据流式传输到它们的计算机中，而这些传感器上的信息需要以亚秒级进行分析，以便操作汽车。这不仅仅是找到数据量中的答案；这些答案通常需要迅速得出。在某些情况下，我们甚至没有足够的磁盘空间来存储数据，这意味着我们还需要决定保留哪些数据以供后续分析。'
- en: '**Variety**: Nice datasets with clearly defined columns are only a small fraction of
    the datasets that we have these days. Consider a social media post that may have
    text, photos, user mentions, likes, comments, videos, geographic information,
    and other fields. Simply ignoring parts of this data that don''t fit your model
    will lead to a loss of information, but integrating that information itself can
    be very difficult.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：具有明确定义列的优质数据集只是我们今天拥有的数据集的一小部分。考虑一下社交媒体帖子，它可能包含文本、照片、用户提及、点赞、评论、视频、地理信息和其他字段。简单地忽略不适合您模型的数据部分会导致信息丢失，但整合这些信息本身可能非常困难。'
- en: '**Veracity**: With this increase in the amount of data, it can be hard to determine
    whether the data is being correctly collected—whether it is outdated, noisy, contains
    outliers—or generally whether it is useful at all. Being able to trust the data
    is hard when a human can''t reliably verify it. External datasets are being increasingly
    merged into internal ones too, giving rise to more troubles relating to the veracity
    of data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：随着数据量的增加，很难确定数据是否被正确收集——是否过时、嘈杂、包含异常值——或者总的来说是否有用。当人类无法可靠地验证数据时，能够信任数据是很困难的。外部数据集越来越多地被合并到内部数据集中，这也引发了更多与数据真实性相关的问题。'
- en: These main four Vs (others have proposed additional Vs) outline why big data
    is different from just *lots of data*. At these scales, the engineering problem
    of working with data is often more difficult—let alone the analysis. While there
    are lots of snake oil salesmen that overstate a particular product's ability to
    analyze big data, it is hard to deny the engineering challenges and the potential
    of big data analytics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个主要V（其他人还提出了额外的V）概述了为什么大数据与仅仅是**大量数据**不同。在这些规模下，与数据打交道的工程问题通常更加困难——更不用说分析。尽管有很多夸大其词的推销员声称某个产品的分析大数据能力，但很难否认工程挑战和大数据分析潜力。
- en: The algorithms we have used so far in the book load the dataset into memory
    and then work on the in-memory version. This gives a large benefit in terms of
    speed of computation (because using computer memory is faster than using hard
    drives), as it is much faster to compute on in-memory data than having to load
    a sample before we use it. In addition, in-memory data allows us to iterate over
    the data many times, thereby improving our machine learning model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中迄今为止使用的算法是将数据集加载到内存中，然后在该内存版本上工作。这在计算速度方面带来了很大的好处（因为使用计算机内存比使用硬盘驱动器快），因为对内存中的数据进行计算比在用它之前加载样本要快得多。此外，内存数据允许我们多次迭代数据，从而提高我们的机器学习模型。
- en: In big data, we can't load our data into memory. In many ways, this is a good
    definition for whether a problem is big data or not—if the data can fit in the
    memory on your computer, you aren't dealing with a big data problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据中，我们无法将数据加载到内存中。在许多方面，这是判断一个问题是否是大数据问题的一个很好的定义——如果数据可以适合你电脑的内存，那么你就不在处理大数据问题。
- en: When looking at the data you create, such as log data from your company's internal
    applications, it might be tempting to simply throw it all into a file, unstructured,
    and use big-data concepts later to analyze it. It is best not to do this; instead,
    you should use structured formats for your own datasets. The reason is that the
    four Vs we just outlined are actually *problems* that need to be solved to perform
    data analysis, not goals to strive for!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看你创建的数据时，例如公司内部应用程序的日志数据，你可能会有直接将所有数据无结构地放入文件，然后使用大数据概念来分析它们的冲动。这样做最好是不；相反，你应该使用结构化格式来处理自己的数据集。原因是我们刚刚概述的四个V实际上是需要解决以执行数据分析的**问题**，而不是需要努力追求的目标！
- en: Applications of big data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的应用
- en: There are many use cases for big data, in public and private sectors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据在公共和私营部门有许多用例。
- en: The most common experience people have using a big-data-based system is in Internet
    search, such as Google. To run these systems, a search needs to be carried out
    over billions of websites in a fraction of a second. Doing a basic text-based
    search would be inadequate to deal with such a problem. Simply storing the text
    of all those websites is a large problem. In order to deal with queries, new data
    structures and data mining methods need to be created and implemented specifically
    for this application.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人们使用基于大数据的系统最常见的体验是在互联网搜索中，例如Google。为了运行这些系统，需要在不到一秒钟的时间内对数十亿个网站进行搜索。进行基于文本的基本搜索不足以处理这样的问题。简单地存储所有这些网站的文本就是一个大问题。为了处理查询，需要创建和实施专门针对此应用的新数据结构和数据挖掘方法。
- en: Big data is also used in many other scientific experiments such as the Large
    Hadron Collider, part of which is pictured next. It stretches over 27 kilometers
    and contains 150 million sensors monitoring hundreds of millions of particle collisions
    per second. The data from this experiment is massive, with 25 petabytes created
    daily, after a filtering process (if filtering were not used, there would be 150
    million petabytes per year). Analysis on data this big has led to amazing insights
    about our universe, but it has been a significant engineering and analytics challenge.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据也被用于许多其他科学实验，例如大型强子对撞机，其中一部分在下面的图片中展示。它跨越27公里，包含1.5亿个传感器，每秒监测数亿个粒子碰撞。这个实验的数据量巨大，每天产生25拍字节的数据，经过过滤过程（如果没有使用过滤，每年将有1.5亿拍字节的数据）。对如此大量数据的分析导致了关于我们宇宙的惊人见解，但这已经是一个重大的工程和数据分析挑战。
- en: '![](img/B06162_12_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_12_01.jpg)'
- en: Governments are increasingly using big data too, to track populations, businesses,
    and other aspects related to their country. Tracking millions of people and billions
    of interactions (such as business transactions or health spending) leads to a
    need for big data analytics in many government organizations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 政府也越来越多地使用大数据，以追踪人口、企业和与其国家相关的其他方面。追踪数百万人和数十亿次的互动（如商业交易或健康支出）导致许多政府机构需要大数据分析。
- en: Traffic management is a particular focus of many governments around the world,
    who are tracking traffic using millions of sensors to determine which roads are
    the most congested and predicting the impact of new roads on traffic levels. These
    management systems will link with data from autonomous cars in the near future,
    leading to even more data about traffic conditions in real time. Cities that make
    use of this data will find that their traffic flows more freely.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 交通管理是全球许多政府关注的重点，他们通过数百万个传感器追踪交通情况，以确定哪些道路最拥堵，并预测新道路对交通水平的影响。这些管理系统将在不久的将来与自动驾驶汽车的数据相连，从而获得更多关于实时交通状况的数据。利用这些数据的城市会发现他们的交通流动更加顺畅。
- en: Large retail organizations are using big data to improve customer experience
    and reduce costs. This involves predicting customer demand in order to have the
    correct level of inventory, upselling customers with products they may like to
    purchase, and tracking transactions to look for trends, patterns, and potential
    frauds. Companies that automatically create great predictions can have higher
    sales at lower costs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大型零售组织正在使用大数据来改善客户体验并降低成本。这包括预测客户需求，以便拥有正确的库存水平，向客户推荐他们可能喜欢购买的产品，并跟踪交易以寻找趋势、模式和潜在的欺诈行为。能够自动创建优秀预测的公司可以在较低的成本下实现更高的销售额。
- en: Other large businesses are also leveraging big data to automate aspects of their
    business and improve their offering. This includes leveraging analytics to predict
    future trends in their sector and tracking external competitors. Large businesses
    also use analytics to manage their own employees—tracking employees to look for
    signs that an employee may leave the company, in order to intervene before they
    do.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其他大型企业也在利用大数据来自动化其业务的一些方面并改善其产品。这包括利用分析来预测其行业未来的趋势和跟踪外部竞争对手。大型企业还使用分析来管理自己的员工——追踪员工以寻找他们可能离职的迹象，以便在他们离职前进行干预。
- en: The information security sector is also leveraging big data in order to look
    for malware infections in large networks, by monitoring network traffic. This
    can include looking for odd traffic patterns, evidence of malware spreading, and
    other oddities. Advanced Persistent Threats (APTs) is another problem, where a
    motivated attacker will hide their code within a large network to steal information
    or cause damage over a long period of time. Finding APTs is often a case of forensically
    examining many computers, a task which simply takes too long for a human to effectively
    perform themselves. Analytics helps automate and analyze these forensic images
    to find infections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 信息安全领域也在利用大数据来寻找大型网络中的恶意软件感染，通过监控网络流量来实现。这可能包括寻找异常流量模式、恶意软件传播的证据和其他异常情况。高级持续性威胁（APT）也是一个问题，其中一名有动机的攻击者将他们的代码隐藏在一个大型网络中，以在长时间内窃取信息或造成损害。寻找APT通常需要法医检查多台计算机，这项任务对于人类来说耗时过长，无法有效自行完成。分析有助于自动化和分析这些法医图像以找到感染。
- en: Big data is being used in an increasing number of sectors and applications,
    and this trend is likely to only continue.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据正在越来越多地应用于各个领域和应用程序，这一趋势很可能会持续下去。
- en: MapReduce
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce
- en: There are a number of concepts to perform data mining and general computation
    on big data. One of the most popular is the MapReduce model, which can be used
    for general computation on arbitrarily large datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据上进行数据挖掘和通用计算有许多概念。其中最受欢迎的是MapReduce模型，它可以用于任意大数据集的通用计算。
- en: MapReduce originates from Google, where it was developed with distributed computing
    in mind. It also introduces fault tolerance and scalability improvements. The
    *original* research for MapReduce was published in 2004, and since then there
    have been thousands of projects, implementations, and applications using it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce起源于Google，它是在考虑分布式计算的情况下开发的。它还引入了容错性和可扩展性的改进。MapReduce的*原始*研究于2004年发表，从那时起，已有成千上万个项目、实现和应用使用它。
- en: While the concept is similar to many previous concepts, MapReduce has become
    a staple in big data analytics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个概念与许多先前的概念相似，但MapReduce已经成为大数据分析的一个基本工具。
- en: There are two major stages in a MapReduce job.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业有两个主要阶段。
- en: 'The first is Map, by which we take a function and a list of items, and apply
    that function to each item. Put another way, we take each item as the input to
    the function and store the result of that function call:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个是Map，通过它我们取一个函数和一个项目列表，并将该函数应用到每个项目上。换句话说，我们将每个项目作为函数的输入，并存储该函数调用的结果：
- en: '![](img/B06162_12_02-e1493116032553.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_12_02-e1493116032553.jpg)'
- en: 'The second step is Reduce, where we take the results from the map step and
    combine them using a function. For statistics, this could be as simple as adding
    all the numbers together. The reduce function in this scenario is an add function,
    which would take the previous sum, and add the new result:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是Reduce，我们将Map步骤的结果组合起来，使用一个函数。对于统计，这可以简单地将所有数字相加。在这个场景中，reduce函数是一个加法函数，它将添加新的结果到之前的总和：
- en: '![](img/B06162_12_03-e1493116048271.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_12_03-e1493116048271.jpg)'
- en: After these two steps, we will have transformed our data and reduced it to a
    final result.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个步骤之后，我们将已经转换了我们的数据，并将其减少到最终结果。
- en: MapReduce jobs can have many iterations, some of which are only Map jobs, some
    only Reduce jobs and some iterations with both a Map and Reduce step. Let us now
    have a look at some more tangible examples, first using built-in python functions
    and then using a specific tool for MapReduce jobs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业可以有多个迭代，其中一些只是Map作业，一些只是Reduce作业，还有一些迭代既有Map步骤又有Reduce步骤。现在让我们看看一些更具体的例子，首先使用内置的Python函数，然后使用特定的MapReduce工具。
- en: The intuition behind MapReduce
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce背后的直觉
- en: 'MapReduce has two main steps: the `Map` step and the `Reduce` step. These are
    built on the functional programming concepts of mapping a function to a list and
    reducing the result. To explain the concept, we will develop code that will iterate
    over a list of lists and produce the sum of all numbers in those lists.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce有两个主要步骤：`Map`步骤和`Reduce`步骤。这些步骤建立在函数式编程的概念之上，即映射一个函数到一个列表，并减少结果。为了解释这个概念，我们将开发一个代码，它将遍历一个列表的列表，并产生那些列表中所有数字的总和。
- en: There are also `shuffle` and `combine` steps in the MapReduce paradigm, which
    we will see later.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce范式中，也存在`shuffle`和`combine`步骤，我们稍后会看到。
- en: To start with, the Map step takes a function and applies it to each element
    in a list. The returned result is a list of the same size, with the results of
    the function applied to each element.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Map步骤取一个函数并将其应用到列表中的每个元素。返回的结果是一个大小相同的列表，其中包含将函数应用到每个元素的结果。
- en: 'To open a new Jupyter Notebook, start by creating a list of lists with numbers
    in each sublist:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开一个新的Jupyter Notebook，首先创建一个包含每个子列表中数字的列表列表：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we can perform a `map` using the sum function. This step will apply the
    sum function to each element of *a*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用求和函数执行一个`map`操作。这一步骤将求和函数应用到*a*的每个元素上：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While `sums` is a generator (the actual value isn''t computed until we ask
    for it), the preceding step is approximately equal to the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`sums`是一个生成器（实际值只有在请求时才会计算），但前一个步骤大约等于以下代码：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `reduce` step is a little more complicated. It involves applying a function
    to each element of the returned result, to some starting value. We start with
    an initial value and then apply a given function to that initial value and the
    first value. We then apply the given function to the result and the next value,
    and so on
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce`步骤稍微复杂一些。它涉及到将一个函数应用到返回结果中的每个元素，以及某个起始值。我们从初始值开始，然后应用给定的函数到初始值和第一个值。然后我们将给定的函数应用到结果和下一个值，依此类推。'
- en: We start by creating a function that takes two numbers and adds them together.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个函数，该函数接受两个数字并将它们相加。
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then perform the reduce. The signature of `reduce` is: `reduce(function,
    sequence, initial)`, where the function is applied at each step to the sequence.
    In the first step, the initial value is used as the first value rather than the
    first element of the list:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行`reduce`。`reduce`的签名是：`reduce(function, sequence, initial)`，其中函数在每一步应用于序列。在第一步中，初始值用作第一个值，而不是列表的第一个元素：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result, 25, is the sum of each of the values in the sums list and is consequently
    the sum of each of the elements in the original array.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果25是求和列表中每个值的总和，因此也是原始数组中每个元素的总和。
- en: 'The preceding code is similar to the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码类似于以下代码：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this simple example, our code would be greatly simplified if it didn't use
    the MapReduce paradigm, but the real gains come from distributing the computation.
    For instance, if we have a million sublists and each of those sublists contains
    a million elements, we can distribute this computation over many computers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，如果我们不使用MapReduce范式，我们的代码将会大大简化，但真正的收益来自于计算的分布。例如，如果我们有一个包含一百万个子列表的集合，每个子列表包含一百万个元素，我们可以在多台计算机上分布这个计算。
- en: In order to do this, we distribute the `map` step by segmenting out data. For
    each of the elements in our list, we send it, along with a description of our
    function, to a computer. This computer then returns the result to our main computer
    (the master).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们通过分割数据来分布`map`步骤。对于我们列表中的每个元素，我们将其以及函数的描述发送到一台计算机。然后，这台计算机将结果返回到我们的主计算机（即主节点）。
- en: The master then sends the result to a computer for the `reduce` step. In our
    example of a million sublists, we would send a million jobs to different computers
    (the same computer may be reused after it completes our first job). The returned
    result would be just a single list of a million numbers, which we then compute
    the sum of.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点然后将结果发送到一台计算机进行`reduce`步骤。在我们的例子中，一百万个子列表，我们会将一百万个任务发送到不同的计算机（同一台计算机在完成我们的第一个任务后可能被重复使用）。返回的结果将只是一个包含一百万个数字的单个列表，然后我们计算这些数字的总和。
- en: The result is that no computer ever needed to store more than a million numbers,
    despite our original data having a trillion numbers in it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，没有计算机需要存储超过一百万个数字，尽管我们的原始数据中包含了一万亿个数字。
- en: A word count example
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词计数示例
- en: Any *actual* implementation of MapReduce is a little more complex than just
    using a `map` and `reduce` step. Both steps are invoked using keys, which allows
    for the separation of data and tracking of values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 任何实际的MapReduce实现都比仅仅使用`map`和`reduce`步骤要复杂一些。这两个步骤都是通过键来调用的，这允许数据分离和值跟踪。
- en: The map function takes a key-value pair and returns a list of *key/value* pairs.
    The keys for the input and output don't necessarily relate to each other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 映射函数接受一个键值对，并返回一个键值对列表。输入和输出的键不一定相互关联。
- en: 'For example, for a MapReduce program that performs a word count, the input
    key might be a sample document''s ID value, while the output key would be a given
    word. The input value would be the text of the document and the output value would
    be the frequency of each word.  We split the document to get the words, and then
    yield each of the word, count pairs. The word here is the key, with the count
    being the value in MapReduce terms:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一个执行单词计数的MapReduce程序，输入键可能是一个样本文档的ID值，而输出键将是给定的单词。输入值将是文档的文本，输出值将是每个单词的频率。我们拆分文档以获取单词，然后输出每个单词及其计数的对。这里的单词是键，在MapReduce术语中，计数是值：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Have a really, really big dataset? You can just do `yield (word, 1)` when you
    come across a new word and then combine the ones in the shuffle step rather than
    count within the map step. Where you place it depends on your dataset size, per-document
    size, network capacity, and a whole range of factors. Big data is a big engineering
    problem and to get the maximum performance out of a system, you'll need to model
    how data will flow throughout the algorithm.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个非常大的数据集？当你遇到一个新单词时，只需执行`yield (word, 1)`，然后在洗牌步骤中合并它们，而不是在映射步骤中进行计数。你放置的位置取决于你的数据集大小、每份文档的大小、网络容量以及一系列因素。大数据是一个巨大的工程问题，为了从系统中获得最佳性能，你需要模拟数据在整个算法中的流动。
- en: 'By using the word as the key, we can then perform a shuffle step, which groups
    all the values for each key:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用单词作为键，我们可以执行一个洗牌步骤，该步骤将每个键的所有值分组：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final step is the reduce step, which takes a key-value pair (the value,
    in this case, is always a list) and produces a key-value pair as a result. In
    our example, the key is the word, the input list is the list of counts produced
    in the shuffle step, and the output value is the sum of the counts:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是 reduce 步骤，它接受一个键值对（在这种情况下，值始终是一个列表）并产生一个键值对作为结果。在我们的例子中，键是单词，输入列表是洗牌步骤产生的计数列表，输出值是计数的总和：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To see this in action, we can use the 20 newsgroups dataset, which is provided
    in scikit-learn. This dataset is not big data, but we can see the concepts in
    action here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一过程在实际中的应用，我们可以使用 scikit-learn 提供的 20 个新闻组数据集。这个数据集不是大数据，但我们可以在这里看到概念的实际应用：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then apply our map step. We use enumerate here to automatically generate
    document IDs for us. While they aren''t important in this application, these keys
    are important in other applications:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用我们的 map 步骤。我们在这里使用 enumerate 来自动为我们生成文档 ID。虽然它们在这个应用中并不重要，但这些键在其他应用中很重要：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The actual result here is just a generator; no actual counts have been produced.
    That said, it is a generator that emits (word, count) pairs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实际的结果只是一个生成器；没有产生实际的计数。尽管如此，它是一个发出（单词，计数）对的生成器。
- en: 'Next, we perform the shuffle step to sort these word counts:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行洗牌步骤来对这些单词计数进行排序：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This, in essence, is a MapReduce job; however, it is only running on a single
    thread, meaning we aren't getting any benefit from the MapReduce data format.
    In the next section, we will start using Hadoop, an open source provider of MapReduce,
    to start getting the benefits of this type of paradigm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上是一个 MapReduce 作业；然而，它只在一个线程上运行，这意味着我们没有从 MapReduce 数据格式中获得任何好处。在下一节中，我们将开始使用
    Hadoop，一个开源的 MapReduce 提供商，以开始获得这种类型范式的好处。
- en: Hadoop MapReduce
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: Hadoop is a set of open source tools from Apache that includes an implementation
    of MapReduce. In many cases, it is the de-facto implementation used by many. The
    project is managed by the Apache group (who are responsible for the famous web
    server of the same name).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 是 Apache 提供的一系列开源工具，其中包括 MapReduce 的一个实现。在许多情况下，它是许多人所使用的默认实现。该项目由 Apache
    团队管理（他们负责同名的著名网络服务器）。
- en: 'The Hadoop ecosystem is quite complex, with a large number of tools. The main
    component we will use is Hadoop MapReduce. Other tools for working with big data
    that are included in Hadoop are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 生态系统相当复杂，包含大量工具。我们将使用的主要组件是 Hadoop MapReduce。Hadoop 中包含的其他用于处理大数据的工具如下：
- en: '**Hadoop Distributed File System (HDFS)**: This is a file system that can store
    files over many computers, with the goal of being robust against hardware failure
    while providing high bandwidth.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 分布式文件系统 (HDFS)**：这是一个可以在多台计算机上存储文件的文件系统，旨在在提供高带宽的同时，对硬件故障具有鲁棒性。'
- en: '**YARN**: This is a method for scheduling applications and managing clusters
    of computers.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YARN**：这是一种用于调度应用程序和管理计算机集群的方法。'
- en: '**Pig:** This is a higher level programming language for MapReduce. Hadoop
    MapReduce is implemented in Java, and Pig sits on top of the Java implementation,
    allowing you to write programs in other languages—including Python.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pig**：这是 MapReduce 的一种高级编程语言。Hadoop MapReduce 是用 Java 实现的，而 Pig 则建立在 Java
    实现之上，允许你用其他语言编写程序——包括 Python。'
- en: '**Hive:** This is for managing data warehouses and performing queries.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hive**：这是用于管理数据仓库和执行查询的。'
- en: '**HBase**: This is an implementation of Google''s BigTable, a distributed database.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HBase**：这是 Google BigTable 的一个实现，一个分布式数据库。'
- en: These tools all solve different issues that come up when doing big data experiments,
    including data analytics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具都解决了在进行大数据实验时出现的问题，包括数据分析。
- en: There are also non-Hadoop-based implementations of MapReduce, as well as other
    projects with similar goals. In addition, many cloud providers have MapReduce-based
    systems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于 Hadoop 的 MapReduce 实现，还有其他具有类似目标的项目。此外，许多云服务提供商也拥有基于 MapReduce 的系统。
- en: Applying MapReduce
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用 MapReduce
- en: In this application, we will look at predicting the gender of a writer based
    on their use of different words. We will use a Naive Bayes method for this, trained
    in MapReduce. The final model doesn't need MapReduce, although we can use the
    Map step to do so—that is, run the prediction model on each document in a list.
    This is a common Map operation for data mining in MapReduce, with the reduce step
    simply organizing the list of predictions so they can be tracked back to the original
    document.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用中，我们将通过分析作者使用不同词汇来预测作者的性别。我们将使用朴素贝叶斯方法进行预测，并在MapReduce中进行训练。最终的模型不需要MapReduce，尽管我们可以使用Map步骤来这样做——即在列表中的每个文档上运行预测模型。这是MapReduce中数据挖掘的常见Map操作，而reduce步骤只是简单地组织预测列表，以便可以追溯到原始文档。
- en: We will be using Amazon's infrastructure to run our application, allowing us
    to leverage their computing resources.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用亚马逊的基础设施来运行我们的应用，这样我们可以利用他们的计算资源。
- en: Getting the data
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The data we are going to use is a set of blog posts that are labeled for age,
    gender, industry (that is, work) and, funnily enough, star sign. This data was
    collected from [http://blogger.com](http://blogger.com) in August 2004 and has
    over 140 million words in more than 600,000 posts. Each blog is *probably* written
    by just one person, with some work put into verifying this (although, we can never
    be really sure). Posts are also matched with the date of posting, making this
    a very rich dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的数据是一组标记了年龄、性别、行业（即工作）以及有趣的是，星座的博客文章。这些数据是在2004年8月从[http://blogger.com](http://blogger.com)收集的，包含超过600,000篇博客文章，总字数超过1.4亿。每篇博客文章*可能*是由一个人撰写的，我们为此做了一些验证工作（尽管，我们永远无法完全确定）。文章还与发布日期相匹配，这使得这是一个非常丰富的数据集。
- en: To get the data, go to [http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)
    and click on Download Corpus. From there, unzip the file to a directory on your
    computer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据，请访问[http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)并点击下载语料库。从那里，将文件解压缩到您的计算机上的一个目录中。
- en: 'The dataset is organized with a single blog to a file, with the filename giving
    the classes. For instance, one of the filenames is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是以单个博客文章对应一个文件的方式组织的，文件名给出类别。例如，以下是一个文件名：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The filename is separated by periods, and the fields are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 文件名由点分隔，字段如下：
- en: '**Blogger ID**: This a simple ID value to organize the identities.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**博客ID**：这是一个简单的ID值，用于组织身份。'
- en: '**Gender**: This is either male or female, and all the blogs are identified
    as one of these two options (no other options are included in this dataset).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：这是男性或女性，所有博客都被标识为这两种选项之一（此数据集中不包含其他选项）。'
- en: '**Age**: The exact ages are given, but some gaps are deliberately present.
    Ages present are in the (inclusive) ranges of 13-17, 23-27, and 33-48\. The reason
    for the gaps is to allow for splitting the blogs into age ranges with gaps, as
    it would be quite difficult to separate an 18-year old''s writing from a 19-year-old,
    and it is possible that the age itself is a little outdated itself and would need
    to be updated to 19 anyway.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年龄**：给出了确切的年龄，但故意存在一些差距。存在的年龄范围是（包含）13-17岁、23-27岁和33-48岁。存在差距的原因是允许将博客分成有差距的年龄范围，因为将18岁年轻人的写作与19岁年轻人的写作区分开来会相当困难，而且年龄本身可能已经过时，可能需要更新到19岁。'
- en: '**Industry**: In one of 40 different industries including science, engineering,
    arts, and real estate. Also, included is indUnk, for an unknown industry.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业**：在包括科学、工程、艺术和房地产在内的40个不同行业中。还包括indUnk，代表未知行业。'
- en: '**Star Sign**: This is one of the 12 astrological star signs.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**星座**：这是12个占星术星座之一。'
- en: All values are self-reported, meaning there may be errors or inconsistencies
    with labeling, but are assumed to be mostly reliable—people had the option of
    not setting values if they wanted to preserve their privacy in those ways.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所有值都是自行报告的，这意味着可能存在错误或不一致性，但假设它们大多是可靠的——人们有选择不设置值以保护他们隐私的方式。
- en: A single file is in a pseudo-XML format, containing a `<Blog>` tag and then
    a sequence of `<post>` tags. Each of the `<post>` tag is proceeded by a `<date>`
    tag as well. While we can parse this as XML, it is much simpler to parse it on
    a line-by-line basis as the files are not exactly well-formed XML, with some errors
    (mostly encoding problems). To read the posts in the file, we can use a loop to
    iterate over the lines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 单个文件采用伪XML格式，包含一个`<Blog>`标签，然后是一系列`<post>`标签。每个`<post>`标签之前都有一个`<date>`标签。虽然我们可以将其解析为XML，但由于文件并非完全符合良好的XML格式，存在一些错误（主要是编码问题），因此逐行解析要简单得多。为了读取文件中的帖子，我们可以使用循环遍历每一行。
- en: 'We set a test filename so we can see this in action:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置一个测试文件名，以便我们可以看到这个动作：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'First, we create a list that will let us store each of the posts:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个列表，以便我们可以存储每个帖子：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we open the file to read:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打开文件进行读取：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If we aren't in a current post, we simply ignore the line.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不在当前帖子中，我们只需忽略该行。
- en: 'We can then grab the text of each post:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以然后获取每个帖子的文本：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also find out how many posts this author created:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以找出这位作者创建了多少帖子：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Naive Bayes prediction
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯预测
- en: We are now going to implement the Naive Bayes algorithm using mrjob, allowing
    it to process our dataset. Technically our version will be a reduced version of
    most Naive Bayes' implementations, without many of the features that you would
    expect like smoothing small values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用mrjob实现朴素贝叶斯算法，使其能够处理我们的数据集。技术上，我们的版本将是大多数朴素贝叶斯实现的一个简化版本，没有许多你可能会期望的功能，比如平滑小值。
- en: The mrjob package
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mrjob包
- en: The **mrjob** package allows us to create MapReduce jobs that can easily be
    computed on Amazon's infrastructure. While mrjob sounds like a sedulous addition
    to the Mr. Men series of children's books, it stands for **Map Reduce Job**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**mrjob**包允许我们创建可以在Amazon基础设施上轻松计算的MapReduce作业。虽然mrjob听起来像是儿童书籍《Mr. Men》系列的忠实补充，但它代表**Map
    Reduce Job**。'
- en: 'You can install mrjob using the following: `pip install ``mrjob`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令安装mrjob：`pip install ``mrjob`
- en: 'I had to install the filechunkio package separately using `conda install -c
    conda-forge filechunkio`, but this will depend on your system setup. There are
    other Anaconda channels for installing mrjob, check them with:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我不得不单独使用`conda install -c conda-forge filechunkio`安装filechunkio包，但这将取决于您的系统设置。还有其他Anaconda通道可以安装mrjob，您可以使用以下命令检查它们：
- en: '`anaconda search -t conda mrjob`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`anaconda search -t conda mrjob`'
- en: In essence, mrjob provides the standard functionality that most MapReduce jobs
    need. Its most amazing feature is that you can write the same code, test on your
    local machine (without heavy infrastructure like Hadoop), and then push to Amazon's
    EMR service or another Hadoop server.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，mrjob提供了大多数MapReduce作业需要的标准功能。它最令人惊奇的功能是，您可以在本地机器上编写相同的代码，进行测试（无需像Hadoop这样的重基础设施），然后推送到Amazon的EMR服务或另一个Hadoop服务器。
- en: This makes testing the code significantly easier, although it can't magically
    make a big problem small— any local testing uses a subset of the dataset, rather
    than the whole, big dataset. Instead, mrjob gives you a framework that you can
    test with a small problem and have more confidence that the solution will scale
    to a larger problem, distributed on different systems.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得测试代码变得容易得多，尽管它不能神奇地将大问题变小——任何本地测试都使用数据集的一个子集，而不是整个大数据集。相反，mrjob提供了一个框架，您可以使用小问题进行测试，并更有信心解决方案可以扩展到更大的问题，分布式在不同的系统上。
- en: Extracting the blog posts
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取博客帖子
- en: We are first going to create a MapReduce program that will extract each of the
    posts from each blog file and store them as separate entries. As we are interested
    in the gender of the author of the posts, we will extract that too and store it
    with the post.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将创建一个MapReduce程序，它将从每个博客文件中提取每个帖子，并将它们作为单独的条目存储。由于我们对帖子的作者性别感兴趣，我们还将提取它并与帖子一起存储。
- en: We can't do this in a Jupyter Notebook, so instead open a Python IDE for development.
    If you don't have a Python IDE you can use a text editor. I recommend PyCharm,
    although it has a larger learning curve and it is probably a bit heavy for just
    this chapter's code.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在Jupyter Notebook中这样做，所以相反，打开Python IDE进行开发。如果您没有Python IDE，可以使用文本编辑器。我推荐PyCharm，尽管它有一个较大的学习曲线，而且可能对于本章的代码来说有点重。
- en: At the very least, I recommend using an IDE that has syntax highlighting and
    basic completion of variable names (that last one helps find typos in your code
    easily.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，我建议使用具有语法高亮和基本变量名补全功能的IDE（最后一个功能有助于轻松查找代码中的错误）。
- en: If you still can't find an IDE you like, you can write the code in an IPython
    Notebook and then click on File| Download As| Python. Save this file to a directory
    and run it as we outlined in [Chapter 11](lrn-dtmn-py-2e_ch02.html), *Classifying
    Objects in Images using Deep Learning*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然找不到喜欢的IDE，你可以在IPython Notebook中编写代码，然后点击文件|另存为| Python。将此文件保存到目录中，然后按照我们在第11章中概述的方式运行它，*使用深度学习在图像中分类对象*。
- en: 'To do this, we will need the `os` and `re` libraries as we will be obtaining
    environment variables and we will also use a regular expression for word separation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要`os`和`re`库，因为我们将会获取环境变量，我们还将使用正则表达式进行单词分隔：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then import the MRJob class, which we will inherit from our MapReduce job:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们导入MRJob类，我们将从这个MapReduce作业中继承：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then create a new class that subclasses MRJob. We will use a similar loop,
    as before, to extract blog posts from the file. The mapping function we will define
    next will work off each line, meaning we have to track different posts outside
    of the mapping function. For this reason, we make `post_start` and post class
    variables, rather than variables inside the function. We then define our mapper
    function—this takes a line from a file as input and yields blog posts. The lines
    are guaranteed to be ordered from the same per-job file. This allows us to use
    the above class variables to record current post data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个新的类，该类是MRJob的子类。我们将使用与之前类似的循环来从文件中提取博客帖子。我们将定义的映射函数将针对每一行工作，这意味着我们必须在映射函数外部跟踪不同的帖子。因此，我们将`post_start`和`post`作为类变量，而不是函数内部的变量。然后我们定义我们的映射函数——这个函数接收文件中的一行作为输入并生成博客帖子。这些行保证是从同一个作业文件中按顺序排列的。这允许我们使用上面的类变量来记录当前帖子数据：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Rather than storing the posts in a list, as we did earlier, we yield them. This
    allows mrjob to track the output. We yield both the gender and the post so that
    we can keep a record of which gender each record matches. The rest of this function
    is defined in the same way as our loop above.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前将帖子存储在列表中的做法不同，我们现在将它们生成。这允许mrjob跟踪输出。我们生成性别和帖子，以便我们可以记录每个记录匹配的性别。这个函数的其余部分与上面的循环定义方式相同。
- en: 'Finally, outside the function and class, we set the script to run this MapReduce
    job when it is called from the command line:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在函数和类外部，我们将脚本设置为在从命令行调用时运行这个MapReduce作业：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we can run this MapReduce job using the following shell command.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下shell命令运行这个MapReduce作业。
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Just a reminder that you don't need to enter the $ on the above line - this
    just indicates this is a command run from the command line, and not in a Jupyter
    Notebook.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 只是一个提醒，你不需要在上面的行中输入$ - 这只是表示这是一个从命令行运行的命令，而不是在Jupyter Notebook中。
- en: The first parameter, `<your_data_folder>/blogs/51*` (just remember to change
    `<your_data_folder>` to the full path to your data folder), obtains a sample of
    the data (all files starting with 51, which is only 11 documents). We then set
    the output directory to a new folder, which we put in the data folder, and specify
    not to output the streamed data. Without the last option, the output data is shown
    to the command line when we run it—which isn't very helpful to us and slows down
    the computer quite a lot.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数`<your_data_folder>/blogs/51*`（只需记住将`<your_data_folder>`更改为数据文件夹的完整路径），获取数据样本（所有以51开头的文件，这只有11个文档）。然后我们将输出目录设置为数据文件夹中的一个新文件夹，并将其放在数据文件夹中，并指定不输出流数据。如果没有最后一个选项，当运行时，输出数据会显示在命令行上——这对我们来说并不很有帮助，并且会大大减慢计算机的速度。
- en: Run the script, and quite quickly each of the blog posts will be extracted and
    stored in our output folder. This script only ran on a single thread on the local
    computer so we didn't get a speedup at all, but we know the code runs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本，很快每个博客帖子都会被提取并存储在我们的输出文件夹中。这个脚本只在本地计算机的单个线程上运行，所以我们根本没得到任何加速，但我们知道代码是运行的。
- en: We can now look in the output folder for the results. A bunch of files are created
    and each file contains each blog post on a separate line, preceded by the gender
    of the author of the blog.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在输出文件夹中查看结果。创建了一堆文件，每个文件都包含一个单独的博客帖子，并在博客作者的性别之前。
- en: Training Naive Bayes
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯
- en: Now that we have extracted the blog posts, we can train our Naive Bayes model
    on them. The intuition is that we record the probability of a word being written
    by a particular gender, and record these values in our model. To classify a new
    sample, we would multiply the probabilities and find the most likely gender.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取了博客文章，我们可以在它们上训练我们的朴素贝叶斯模型。直觉是记录特定性别撰写单词的概率，并将这些值记录在我们的模型中。要分类一个新样本，我们将乘以概率并找到最可能的性别。
- en: 'The aim of this code is to output a file that lists each word in the corpus,
    along with the frequencies of that word for each gender. The output file will
    look something like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本代码的目的是输出一个文件，列出语料库中的每个单词，以及每个性别中该单词的频率。输出文件将类似于以下内容：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first value is the word and the second is a dictionary mapping the genders
    to the frequency of that word in that gender's writings.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个值是单词，第二个是一个将性别映射到该性别写作中该单词频率的字典。
- en: 'Open a new file in your Python IDE or text editor. We will again need the `os`
    and `re` libraries, as well as `NumPy` and `MRJob` from `mrjob`. We also need
    `itemgetter`, as we will be sorting a dictionary:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的Python IDE或文本编辑器中打开一个新文件。我们仍然需要`os`和`re`库，以及来自`mrjob`的`NumPy`和`MRJob`。我们还需要`itemgetter`，因为我们将会对一个字典进行排序：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will also need `MRStep`, which outlines a step in a MapReduce job. Our previous
    job only had a single step, which is defined as a mapping function and then as
    a reducing function. This job will have multiple steps where we Map, Reduce, and
    then Map and Reduce again. The intuition is the same as the pipelines we used
    in earlier chapters, where the output of one step is the input to the next step:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要`MRStep`，它概述了MapReduce作业中的一个步骤。我们之前的作业只有一个步骤，它被定义为一个映射函数然后是一个减少函数。这个作业将会有多个步骤，我们将进行映射、减少，然后再进行映射和减少。直觉与我们在前面的章节中使用的管道相同，其中一步的输出是下一步的输入：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then create our word search regular expression and compile it, allowing
    us to find word boundaries. This type of regular expression is much more powerful
    than the simple split we used in some previous chapters, but if you are looking
    for a more accurate word splitter, I recommend using NLTK or Spacey as we did
    in [Chapter 6](lrn-dtmn-py-2e_ch06.html)<q>, Social Media Insight using Naive
    Bayes</q>:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建我们的单词搜索正则表达式并编译它，这样我们就可以找到单词边界。这种正则表达式比我们在一些前面的章节中使用的简单分割更强大，但如果您正在寻找一个更精确的单词分割器，我建议使用NLTK或Spacey，就像我们在[第6章](lrn-dtmn-py-2e_ch06.html)<q>“使用朴素贝叶斯进行社交媒体洞察”</q>中所做的那样：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We define a new class for our training. I''ll first provide the whole class
    as one code block, and then we will come back to each section to review what it
    does:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们的训练定义了一个新类。我首先会提供一个完整的代码块，然后我们将回到每个部分来审查它所做的工作：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s have a look at the sections of this code, one step at a time:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地查看这段代码的各个部分：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We define the steps of our MapReduce job. There are two steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了我们的MapReduce作业的步骤。有两个步骤：
- en: 'The first step will extract the word occurrence probabilities. The second step
    will compare the two genders and output the probabilities for each to our output
    file. In each MRStep, we define the mapper and reducer functions, which are class
    functions in this NaiveBayesTrainer class (we will write those functions next):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将提取单词出现概率。第二步将比较两个性别，并将每个性别的概率输出到我们的输出文件。在每一个MRStep中，我们定义mapper和reducer函数，这些函数是NaiveBayesTrainer类中的类函数（我们将在下一节中编写这些函数）：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The first function is the mapper function for the first step. The goal of this
    function is to take each blog post, get all the words in that post, and then note
    the occurrence. We want the frequencies of the words, so we will return `1 / len(all_words)`,
    which allows us to later sum the values for frequencies. The computation here
    isn't exactly correct—we need to also normalize for the number of documents. In
    this dataset, however, the class sizes are the same, so we can conveniently ignore
    this with little impact on our final version.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数是第一个步骤的mapper函数。这个函数的目标是取每一篇博客文章，获取该文章中的所有单词，并记录出现次数。我们想要单词的频率，所以我们将返回`1
    / len(all_words)`，这样我们就可以稍后对频率值求和。这里的计算并不完全正确——我们需要对文档数量进行归一化。然而，在这个数据集中，类的大小是相同的，因此我们可以方便地忽略这一点，对最终版本的影响很小。
- en: 'We also output the gender of the post''s author, as we will need that later:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还输出了帖子作者的性别，因为我们稍后会需要这个信息：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We used `eval` in the preceding code to simplify the parsing of the blog posts
    from the file, for this example. This is not recommended. Instead, use a format
    such as JSON to properly store and parse the data from the files. A malicious
    user with access to the dataset can insert code into these tokens and have that
    code run on your server.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`eval`来简化从文件中解析博客文章，为了这个例子。这不被推荐。相反，使用JSON等格式来正确存储和解析文件中的数据。一个恶意用户如果可以访问数据集，可以将代码插入这些令牌，并让该代码在您的服务器上运行。
- en: In the reducer for the first step, we sum the frequencies for each gender and
    word pair. We also change the key to be the word, rather than the combination,
    as this allows us to search by word when we use the final trained model (although,
    we still need to output the gender for later use);
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步的reducer中，我们为每个性别和单词对求和频率。我们还更改了键，使其成为单词，而不是组合，这样当我们使用最终训练好的模型进行搜索时，我们可以按单词进行搜索（尽管，我们仍然需要输出性别以供后续使用）；
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The final step doesn't need a mapper function, which is why we didn't add one.
    The data will pass straight through as a type of identity mapper. The reducer,
    however, will combine frequencies for each gender under the given word and then
    output the word and frequency dictionary.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步不需要mapper函数，这就是为什么我们没有添加一个。数据将直接作为一种类型的身份mapper通过。然而，reducer将组合给定单词下的每个性别的频率，然后输出单词和频率字典。
- en: 'This gives us the information we needed for our Naive Bayes implementation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了朴素贝叶斯实现所需的信息：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we set the code to run this model when the file is run as a script.
    We will need to add this code to the file:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将代码设置为在文件作为脚本运行时运行此模型。我们需要将以下代码添加到文件中：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can then run this script. The input to this script is the output of the previous
    post-extractor script (we can actually have them as different steps in the same
    MapReduce job if you are so inclined);
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以运行这个脚本。该脚本的输入是前一个后提取脚本（如果你愿意，实际上可以将它们作为同一个MapReduce作业中的不同步骤）的输出；
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output directory is a folder that will store a file containing the output
    from this MapReduce job, which will be the probabilities we need to run our Naive
    Bayes classifier.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出目录是一个文件夹，将存储此MapReduce作业的输出文件，这将是我们运行朴素贝叶斯分类器所需的概率。
- en: Putting it all together
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: We can now actually run the Naive Bayes classifier using these probabilities.
    We will do this in a Jupyter Notebook, although this processing itself can be
    transferred to a mrjob package to be performed at scale.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实际运行这些概率的朴素贝叶斯分类器。我们将使用Jupyter Notebook来完成这项工作，尽管这项处理本身可以被转移到mrjob包中，以进行大规模处理；
- en: 'First, take a look at the `models` folder that was specified in the last MapReduce
    job. If the output was more than one file, we can merge the files by just appending
    them to each other using a command line function from within the `models` directory:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查看最后MapReduce作业中指定的`models`文件夹。如果输出文件多于一个，我们可以通过在`models`目录中使用命令行函数将它们合并在一起：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If you do this, you'll need to update the following code with `model.txt` as
    the model filename.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样做，你需要将以下代码中的`model.txt`作为模型文件名进行更新。
- en: 'Back to our Notebook, we first import some standard imports we need for our
    script:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的Notebook，我们首先导入一些标准导入，这些导入对于我们的脚本来说是必需的：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We again redefine our word search regular expression—if you were doing this
    in a real application, I recommend centralizing the functionality. It is important
    that words are extracted in the same way for both training and testing:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次重新定义我们的单词搜索正则表达式——如果你在实际应用中这样做，我建议集中化功能。对于训练和测试，单词的提取方式必须相同：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Next, we create the function that loads our model from a given filename. The
    model parameters will take the form of a dictionary of dictionaries, where the
    first key is a word, and the inner dictionary maps each gender to a probability.
    We use `defaultdicts`, which will return zero if a value isn't present;
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数，用于从给定的文件名中加载我们的模型。模型参数将采用字典的字典形式，其中第一个键是一个单词，内部字典将每个性别映射到一个概率。我们使用`defaultdicts`，如果不存在值，则返回零；
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The line is split into two sections, separated by whitespace. The first is the
    word itself and the second is a dictionary of probabilities. For each, we run
    `eval` on them to get the actual value, which was stored using `repr` in the previous
    code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这行被分成两个部分，由空格分隔。第一部分是单词本身，第二部分是概率字典。对于每一部分，我们运行`eval`以获取实际值，这些值在之前的代码中使用`repr`存储。
- en: Next, we load our actual model. You may need to change the model filename—it
    will be in the output dir of the last MapReduce job;
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载我们的实际模型。你可能需要更改模型文件名——它将在最后一个MapReduce作业的输出目录中；
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As an example, we can see the difference in usage of the word *i* (all words
    are turned into lowercase in the MapReduce jobs) between males and females:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到男性和女性在单词 *i*（在MapReduce作业中所有单词都转换为小写）的使用上的差异：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we create a function that can use this model for prediction. We won''t
    use the scikit-learn interface for this example, and just create a function instead.
    Our function takes the model and a document as the parameters and returns the
    most likely gender:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个可以用于预测的函数。在这个例子中，我们不会使用scikit-learn接口，而是创建一个函数。我们的函数接受模型和文档作为参数，并返回最可能的性别：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It is important to note that we used `np.log` to compute the probabilities.
    Probabilities in Naive Bayes models are often quite small. Multiplying small values,
    which is necessary for many statistical values, can lead to an underflow error
    where the computer's precision isn't good enough and just makes the whole value
    0\. In this case, it would cause the likelihoods for both genders to be zero,
    leading to incorrect predictions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们使用了 `np.log` 来计算概率。朴素贝叶斯模型中的概率通常非常小。对于许多统计值来说，乘以小的数值可能会导致下溢错误，即计算机的精度不够，整个值变成0。在这种情况下，这会导致男女两性的似然值都变为零，从而导致预测错误。
- en: To get around this, we use log probabilities. For two values a and b, *log(a× b)*
    is equal to *log(a) + log(b)*. The log of a small probability is a negative value,
    but a relatively large one. For instance, log(0.00001) is about -11.5\. This means
    that rather than multiplying actual probabilities and risking an underflow error,
    we can sum the log probabilities and compare the values in the same way (higher
    numbers still indicate a higher likelihood).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用对数概率。对于两个值a和b，*log(a× b)* 等于 *log(a) + log(b)*。小概率的对数是一个负值，但相对较大。例如，log(0.00001)
    大约是 -11.5。这意味着，我们不必乘以实际概率并冒着下溢错误的风险，而是可以相加对数概率，并以相同的方式比较值（数值仍然表示更高的似然性）。
- en: If you want to obtain probabilities back from the log probabilities, make sure
    to undo the log operation by using e to the power of the value you are interested
    in. To revert -11.5 into the probability, take e^(-11.5), which equals 0.00001
    (approximately).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从对数概率中获取概率，确保使用e的幂来撤销对数操作。要将-11.5还原为概率，取e^(-11.5)，这等于0.00001（大约）。
- en: One problem with using log probabilities is that they don't handle zero values
    well (although, neither does multiplying by zero probabilities). This is due to
    the fact that log(0) is undefined. In some implementations of Naive Bayes, a 1
    is added to all counts to get rid of this, but there are other ways to address
    this. This is a simple form of smoothing of the values. In our code, we just return
    a very small value if the word hasn't been seen for our given gender.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数概率的一个问题是它们处理零值不好（尽管，乘以零概率也不会处理得好）。这是因为log(0)是未定义的。在某些朴素贝叶斯实现中，通过将所有计数加1来解决这个问题，但还有其他方法可以解决这个问题。这是对值的一种简单平滑形式。在我们的代码中，如果单词对于给定的性别没有出现过，我们只返回一个非常小的值。
- en: Adding one to all counts above is a form of smoothing. Another option is to
    initialise to a very small value, such as 10^(-16) - as long as its not exactly
    0!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有计数上添加一个是一种平滑方法。另一种选择是初始化到一个非常小的值，例如10^(-16)——只要它不是正好是0！
- en: 'Back to our prediction function, we can test this by copying a post from our
    dataset:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的预测函数，我们可以通过复制数据集中的一个帖子来测试它：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We then predict with the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用以下代码进行预测：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The resulting prediction, *male*, is correct for this example. Of course, we
    never test a model on a single sample. We used the file starting with 51 for training
    this model. It wasn't many samples, so we can't expect too high of an accuracy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 结果预测为 *男性*，对于这个例子来说是正确的。当然，我们从不只在单个样本上测试模型。我们使用了以51开头的文件来训练这个模型。样本数量不多，所以我们不能期望很高的准确率。
- en: The first thing we should do is train on more samples. We will test on any file
    that starts with a 6 or 7 and train on the rest of the files.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该做的是在更多样本上进行训练。我们将测试以6或7开头的任何文件，并在其余文件上进行训练。
- en: In the command line and in your data folder (`cd <your_data_folder>`), where
    the blogs folder exists, create a copy of the blogs folder into a new folder.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行和你的数据文件夹（`cd <your_data_folder>`），其中存在博客文件夹，将博客文件夹复制到一个新文件夹中。
- en: 'Make a folder for our training set:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们的训练集创建一个文件夹：
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Move any file starting with a 6 or 7 into the test set, from the train set:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 将任何以6或7开头的文件从训练集中移动到测试集：
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, make a folder for our test set:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为我们的测试集创建一个文件夹：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Move any file starting with a 6 or 7 into the test set, from the train set:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将任何以6或7开头的文件从训练集中移动到测试集：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We will rerun the blog extraction on all files in the training set. However,
    this is a large computation that is better suited to cloud infrastructure than
    our system. For this reason, we will now move the parsing job to Amazon's infrastructure.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练集中的所有文件上重新运行博客提取。然而，这是一个更适合云基础设施而不是我们系统的大型计算。因此，我们现在将解析作业移动到亚马逊的基础设施。
- en: 'Run the following on the command line, as you did before. The only difference
    is that we train on a different folder of input files. Before you run the following
    code, delete all files in the blog posts and models folders:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行上运行以下命令，就像之前做的那样。唯一的区别是我们将在不同的输入文件文件夹中进行训练。在运行以下代码之前，请删除博客文章和模型文件夹中的所有文件：
- en: '[PRE48]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next up comes the training of our Naive Bayes model.  The code here will take
    quite a bit longer to run. Many, many hours. You may want to skip running this
    locally, unless you have a really powerful system! If you do want to skip, head
    to the next section.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练我们的朴素贝叶斯模型。这里的代码运行时间会相当长。很多很多小时。除非你有一个非常强大的系统，否则你可能想跳过本地运行这一步！如果你想跳过，请转到下一节。
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We will test on any blog file in our test set. To get the files, we need to
    extract them. We will use the `extract_posts.py` MapReduce job, but store the
    files in a separate folder:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在测试集中的任何博客文件上进行测试。为了获取这些文件，我们需要提取它们。我们将使用`extract_posts.py` MapReduce作业，但将文件存储在单独的文件夹中：
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Back in the Jupyter Notebook, we list all the outputted testing files:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Jupyter笔记本中，我们列出所有输出的测试文件：
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For each of these files, we extract the gender and document and then call the
    predict function. We do this in a generator, as there are a lot of documents,
    and we don''t want to use too much memory. The generator yields the actual gender
    and the predicted gender:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些文件中的每一个，我们提取性别和文档，然后调用预测函数。我们这样做是在生成器中，因为有大量的文档，我们不希望使用太多的内存。生成器产生实际性别和预测性别：
- en: '[PRE52]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We then record the predictions and actual genders across our entire dataset.
    Our predictions here are either male or female. In order to use the `f1_score`
    function from scikit-learn, we need to turn these into ones and zeroes. In order
    to do that, we record a 0 if the gender is male and 1 if it is female. To do this,
    we use a Boolean test, seeing if the gender is female. We then convert these Boolean
    values to `int` using NumPy:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们记录了整个数据集上的预测和实际性别。我们的预测结果是男性或女性。为了使用scikit-learn中的`f1_score`函数，我们需要将这些结果转换为1和0。为了做到这一点，如果性别是男性，我们记录一个0，如果是女性，则记录一个1。为此，我们使用布尔测试，查看性别是否为女性。然后，我们使用NumPy将这些布尔值转换为`int`：
- en: '[PRE53]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we test the quality of this result using the F1 score in scikit-learn:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用scikit-learn中的F1分数来测试这个结果的质量：
- en: '[PRE54]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The result of 0.78 is quite reasonable. We can probably improve this by using
    more data, but to do that, we need to move to a more powerful infrastructure that
    can handle it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 0.78的结果相当合理。我们可能通过使用更多数据来提高这个结果，但为了做到这一点，我们需要迁移到一个更强大的基础设施，它可以处理这些数据。
- en: Training on Amazon's EMR infrastructure
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在亚马逊的EMR基础设施上训练
- en: We are going to use Amazon's **Elastic Map Reduce** (**EMR**) infrastructure
    to run our parsing and model building jobs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用亚马逊的**弹性映射减少**（**EMR**）基础设施来运行我们的解析和模型构建作业。
- en: In order to do that, we first need to create a bucket in Amazon's storage cloud.
    To do this, open the Amazon S3 console in your web browser by going to [http://console.aws.amazon.com/s3](http://console.aws.amazon.com/s3)
    and click on Create Bucket. Remember the name of the bucket, as we will need it
    later.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们首先需要在亚马逊的存储云中创建一个存储桶。为此，通过访问[http://console.aws.amazon.com/s3](http://console.aws.amazon.com/s3)并在你的网络浏览器中打开Amazon
    S3控制台，然后点击创建存储桶。记住存储桶的名称，因为我们稍后会用到它。
- en: Right-click on the new bucket and select Properties. Then, change the permissions,
    granting everyone full access. This is not a good security practice in general,
    and I recommend that you change the access permissions after you complete this
    chapter. You can use advanced permissions in Amazon's services to give your script
    access and also protect against third parties viewing your data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 右键单击新存储桶并选择属性。然后，更改权限，授予所有人完全访问权限。这通常不是一个好的安全实践，我建议您在完成本章后更改访问权限。您可以使用亚马逊服务的高级权限来授予您的脚本访问权限，并防止第三方查看您的数据。
- en: Left-click the bucket to open it and click on Create Folder. Name the folder
    blogs_train. We are going to upload our training data to this folder for processing
    on the cloud.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 左键单击存储桶以打开它，然后单击创建文件夹。将文件夹命名为blogs_train。我们将把我们的训练数据上传到这个文件夹以在云上进行处理。
- en: On your computer, we are going to use Amazon's AWS CLI, a command-line interface
    for processing on Amazon's cloud.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的电脑上，我们将使用亚马逊的AWS CLI，这是一个用于处理亚马逊云的命令行界面。
- en: 'To install it, use the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装它，请使用以下命令：
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Follow the instructions at [http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html)
    to set the credentials for this program.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html)中的说明来设置此程序的凭证。
- en: 'We now want to upload our data to our new bucket. First, we want to create
    our dataset, which is all the blogs not starting with a 6 or 7\. There are more
    graceful ways to do this copy, but none are cross-platform enough to recommend.
    Instead, simply copy all the files and then delete the ones that start with a
    6 or 7, from the training dataset:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想将我们的数据上传到我们的新存储桶。首先，我们想要创建我们的数据集，即所有不以6或7开头的博客。有更优雅的方式来执行此复制，但没有足够跨平台的来推荐。相反，只需简单地复制所有文件，然后从训练数据集中删除以6或7开头的文件：
- en: '[PRE56]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Next, upload the data to your Amazon S3 bucket. Note that this will take some
    time and use quite a lot of upload data (several hundred megabytes). For those
    with slower internet connections, it may be worth doing this at a location with
    a faster connection;
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将数据上传到您的亚马逊S3存储桶。请注意，这将花费一些时间并使用相当多的上传数据（数百万字节）。对于互联网连接较慢的用户，在更快连接的地方进行此操作可能值得考虑；
- en: '[PRE57]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We are going to connect to Amazon's EMR (Elastic Map Reduce) using mrjob—it
    handles the whole thing for us; it only needs our credentials to do so. Follow
    the instructions at [https://pythonhosted.org/mrjob/guides/emr-quickstart.html](https://pythonhosted.org/mrjob/guides/emr-quickstart.html)
    to setup mrjob with your Amazon credentials.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用mrjob连接到亚马逊的EMR（弹性映射减少），它为我们处理整个流程；它只需要我们的凭证即可完成。请按照[https://pythonhosted.org/mrjob/guides/emr-quickstart.html](https://pythonhosted.org/mrjob/guides/emr-quickstart.html)中的说明，使用您的亚马逊凭证设置mrjob。
- en: After this is done, we alter our mrjob run, only slightly, to run on Amazon
    EMR. We just tell mrjob to use emr using the -r switch and then set our s3 containers
    as the input and output directories. Even though this will be run on Amazon's
    infrastructure, it will still take quite a long time to run, as the default settings
    for mrjob use a single, low powered computer.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们仅对mrjob运行进行轻微修改，以便在亚马逊EMR上运行。我们只需告诉mrjob使用-r开关来使用emr，然后设置我们的s3容器作为输入和输出目录。尽管这将运行在亚马逊的基础设施上，但它仍然需要相当长的时间来运行，因为mrjob的默认设置使用的是单个低功耗计算机。
- en: '[PRE58]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: You will be charged for the usage of both S3 and EMR. This will only be a few
    dollars, but keep this in mind if you are going to keep running the jobs or doing
    other jobs on bigger datasets. I ran a very large number of jobs and was charged
    about $20 all up. Running just these few should be less than $4\. However, you
    can check your balance and set up pricing alerts, by going to [https://console.aws.amazon.com/billing/home](https://console.aws.amazon.com/billing/home)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您将因使用S3和EMR而付费。这只会花费几美元，但如果您打算继续运行作业或在更大的数据集上执行其他作业，请记住这一点。我运行了大量的作业，总共花费了大约20美元。仅运行这些作业应该不到4美元。然而，您可以通过访问[https://console.aws.amazon.com/billing/home](https://console.aws.amazon.com/billing/home)来检查您的余额并设置价格警报。
- en: It isn't necessary for the blogposts_train and model folders to exist—they will
    be created by EMR. In fact, if they exist, you will get an error. If you are rerunning
    this, just change the names of these folders to something new, but remember to
    change both commands to the same names (that is, the output directory of the first
    command is the input directory of the second command).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: blogposts_train 和 model 文件夹的存在不是必需的——它们将由 EMR 创建。实际上，如果它们存在，您将收到错误。如果您正在重新运行此操作，只需将这些文件夹的名称更改为新的名称，但请记住将两个命令都更改为相同的名称（即第一个命令的输出目录是第二个命令的输入目录）。
- en: If you are getting impatient, you can always stop the first job after a while
    and just use the training data gathered so far. I recommend leaving the job for
    an absolute minimum of 15 minutes and probably at least an hour. You can't stop
    the second job and get good results though; the second job will probably take
    about two to three times as long as the first job did.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感到不耐烦，您可以在一段时间后停止第一个作业，只需使用到目前为止收集的训练数据。我建议至少让作业运行 15 分钟，可能至少一个小时。但是，您不能停止第二个作业并得到良好的结果；第二个作业可能需要第一个作业的两到三倍时间。
- en: 'If you have the ability to purchase more advanced hardware, mrjob supports
    the creation of clusters on Amazon''s infrastructure and also the ability to use
    more powerful computing hardware. You can run a job on a cluster of machines by
    specifying the type and number at the command line. For instance, to use 16 c1.medium
    computers to extract the text, run the following command:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有能力购买更高级的硬件，mrjob 支持在 Amazon 的基础设施上创建集群，并且还可以使用更强大的计算硬件。您可以通过指定命令行中的类型和数量在机器集群上运行作业。例如，要使用
    16 台 c1.medium 电脑提取文本，请运行以下命令：
- en: '[PRE59]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In addition, you can create clusters separately and reattach jobs to those clusters.
    See mrjob's documentation at [https://pythonhosted.org/mrjob/guides/emr-advanced.html](https://pythonhosted.org/mrjob/guides/emr-advanced.html)
    for more information on this process. Keep in mind that more advanced options
    become an interaction between advanced features of mrjob and advanced features
    of Amazon's AWS infrastrucutre, meaning you will need to investigate both technologies
    to get high-powered processing. Keep in mind that if you run more instances of
    more powerful hardware, you will be charged more in turn.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以单独创建集群并将作业重新附加到这些集群上。有关此过程的更多信息，请参阅 mrjob 的文档[https://pythonhosted.org/mrjob/guides/emr-advanced.html](https://pythonhosted.org/mrjob/guides/emr-advanced.html)。请记住，更高级的选项成为
    mrjob 的高级功能和 Amazon 的 AWS 基础设施的高级功能之间的交互，这意味着您需要研究这两种技术以获得强大的处理能力。请记住，如果您运行更多更强大的硬件实例，您将相应地支付更多费用。
- en: 'You can now go back to the s3 console and download the output model from your
    bucket. Saving it locally, we can go back to our Jupyter Notebook and use the
    new model. We reenter the code here—only the differences are highlighted, just
    to update to our new model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以回到 s3 控制台，从您的存储桶中下载输出模型。将其保存在本地后，我们可以回到我们的 Jupyter Notebook 并使用新的模型。我们在这里重新输入代码——只有差异被突出显示，只是为了更新到我们的新模型：
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The result is better with the extra data, at 0.81.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 增加额外数据后，结果更好，为 0.81。
- en: If everything went as planned, you may want to remove the bucket from Amazon
    S3—you will be charged for the storage.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按计划进行，您可能想从 Amazon S3 中删除存储桶——您将为此存储付费。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at running jobs on big data. By most standards, our
    dataset is actually quite small—only a few hundred megabytes. Many industrial
    datasets are much bigger, so extra processing power is needed to perform the computation.
    In addition, the algorithms we used can be optimized for different tasks to further
    increase the scalability.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在大型数据上运行作业。根据大多数标准，我们的数据集实际上相当小——只有几百兆字节。许多工业数据集要大得多，因此需要额外的处理能力来执行计算。此外，我们使用的算法可以针对不同的任务进行优化，以进一步提高可扩展性。
- en: Our approach extracted word frequencies from blog posts, in order to predict
    the gender of the author of a document. We extracted the blogs and word frequencies
    using MapReduce-based projects in mrjob. With those extracted, we can then perform
    a Naive Bayes-esque computation to predict the gender of a new document.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法从博客文章中提取单词频率，以预测文档作者的性别。我们使用基于 mrjob 的 MapReduce 项目提取博客和单词频率。有了这些提取的数据，我们就可以执行类似朴素贝叶斯（Naive
    Bayes）的计算来预测新文档的性别。
- en: We only scratched the surface of what you can do with MapReduce, and we did
    not even use it to its full potential for this application. To take the lessons
    further, convert the prediction function to a MapReduce job. That is, you train
    the model on MapReduce to obtain a model, and you run the model with MapReduce
    to get a list of predictions. Extend this by also doing your evaluation in MapReduce,
    with the final result coming back as simply the F1-score!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了MapReduce可以做到的事情的表面，而且我们甚至没有充分利用它来完成这个应用。为了进一步学习，将预测函数转换为MapReduce作业。也就是说，您在MapReduce上训练模型以获得一个模型，然后使用MapReduce运行模型以获得预测列表。通过在MapReduce中进行评估来扩展这一点，最终结果简单地以F1分数的形式返回！
- en: We can use the mrjob library to test locally and then automatically set up and
    use Amazon's EMR cloud infrastructure. You can use other cloud infrastructure
    or even a custom built Amazon EMR cluster to run these MapReduce jobs, but there
    is a bit more tinkering needed to get them running.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用mrjob库在本地进行测试，然后自动设置并使用亚马逊的EMR云基础设施。您可以使用其他云基础设施，甚至是一个定制的亚马逊EMR集群来运行这些MapReduce作业，但需要做一些额外的调整才能使它们运行。
