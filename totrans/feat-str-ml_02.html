<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Chapter 1: An Overview of the Machine Learning Life Cycle</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) is a <a id="_idIndexMarker000"/>subfield of computer science that involves studying and exploring computer algorithms that can learn the structure of data using statistical analysis. The dataset that's used for learning is called training <a id="_idIndexMarker001"/>data. The output of training is<a id="_idIndexMarker002"/> called a model, which can then be used to run predictions against a new dataset that the model hasn't seen before. There are two broad categories of machine learning: <strong class="bold">supervised learning</strong> and <strong class="bold">unsupervised learning</strong>. In supervised learning, the<a id="_idIndexMarker003"/> training dataset is labeled (the dataset will have a target column). The algorithm intends to learn how to predict the target <a id="_idIndexMarker004"/>column based on other columns (features) in the dataset. Predicting house prices, stock market changes, and customer churn are some supervised learning examples. In unsupervised learning, on the other hand, the data is not labeled (the dataset will not have a target column). In this, the algorithm intends to recognize the common patterns in the dataset. One of the methods of generating labels for an unlabeled dataset is using unsupervised learning algorithms. Anomaly detection<a id="_idIndexMarker005"/> is one of the use cases for unsupervised learning. </p>
			<p>The idea of the first mathematical model<a id="_idIndexMarker006"/> for machine learning was presented in 1943 by Walter Pitts and Warren McCulloch (<em class="italic">The History of Machine Learning: How Did It All Start?</em> – <a href="https://labelyourdata.com/articles/history-of-machine-learning-how-did-it-all-start">https://labelyourdata.com/articles/history-of-machine-learning-how-did-it-all-start</a>). Later, in the 1950s, Arthur Samuel developed a program for playing championship-level computer checkers. Since then, we have come a long way in ML. I would highly recommend reading this article if you haven't. </p>
			<p>Today, as we try to teach real-time decision-making to systems and devices, ML engineer and data scientist positions are the hottest jobs on the market. It is predicted that the global machine learning market will grow from $8.3 billion in 2019 to $117.9 billion by 2027. As shown in the <a id="_idIndexMarker007"/>following diagram, it's a unique skill set that overlaps with multiple domains:</p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B18024_01_01.jpg" alt="Figure 1.1 – ML/data science skill sets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – ML/data science skill sets</p>
			<p>In 2007 and 2008, the DevOps<a id="_idIndexMarker008"/> movement revolutionized the way software was developed and operationalized. It reduced the time to production for software:</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B18024_01_02.jpg" alt="Figure 1.2 – DevOps &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – DevOps </p>
			<p>Similarly, to take a model from experimentation to operationalization, we need a set of standardized processes that makes this process seamless. Well, the answer to that is <strong class="bold">machine learning operations</strong> (<strong class="bold">MLOps</strong>). Many <a id="_idIndexMarker009"/>experts in the industry have come across a set of patterns that would reduce the time to production of ML models. 2021 is the year of MLOps – there are a lot of new start-ups that are trying to cater to the ML needs of the firms that are still behind in the ML journey. We can assume that this will expand over time and only get better, just like any other process. As we grow with it, there will be a lot of discoveries and ways of working, best practices, and more will evolve. In this book, we will talk about one of the common tools that's used to standardize ML and its best practices: the feature store.</p>
			<p>Before we discuss what a feature store is and how to use it, we need to understand the ML life cycle and its common oversights. I want to dedicate this chapter to learning about the different stages of the ML life cycle. As part of this chapter, we will take up an ML model-building exercise. We won't dive deep into the ML model itself, such as its algorithms or how to do feature engineering; instead, we will focus on the stages an ML model would typically go through, as well as the difficulties involved in model building versus model operationalization. We will also discuss the stages that are time-consuming and repetitive. The goal of this chapter is to understand the overall ML life cycle and the issues involved in operationalizing models. This will set the stage for later chapters, where we will discuss feature management, the role of a feature store in ML, and how the feature store solves some of the issues we will discuss in this chapter.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The ML life cycle in practice</li>
				<li>An ideal world versus the real world</li>
				<li>The most time-consuming stages of ML</li>
			</ul>
			<p>Without further ado, let's get our hands dirty with an ML model.</p>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Technical requirements</h1>
			<p>To follow the code examples in this book, you need to be familiar with Python and any notebook environment, which could be a local setup such as Jupyter or an online notebook environment such as Google Colab or Kaggle. We will be using the Python3 interpreter and PIP3 to manage the virtual environment. You can download the code examples for this chapter from the following GitHub link: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter01">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter01</a>.</p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>The ML life cycle in practice</h1>
			<p>As Jeff Daniel's character in HBO's <em class="italic">The Newsroom</em> once said, the first step in solving any problem is recognizing there is one. Let's follow this knowledge and see if it works for us. </p>
			<p>In this section, we'll pick a problem statement and execute the<a id="_idIndexMarker010"/> ML life cycle step by step. Once completed, we'll retrospect and identify any issues. The following diagram shows the different stages of ML:</p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B18024_01_03.jpg" alt="Figure 1.3 – The ML life cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – The ML life cycle</p>
			<p>Let's take a look at our problem statement.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>Problem statement (plan and create)</h2>
			<p>For<a id="_idIndexMarker011"/> this exercise, let's assume that you own a retail business and would like to improve customer experience. First and foremost, you want to find your customer segments and customer <strong class="bold">lifetime value (LTV)</strong>. If you have worked in the domain, you probably know different ways to solve this problem. I will follow a medium blog series called <em class="italic">Know Your Metrics – Learn what and how to track with Python</em> by Barış Karaman (https://towardsdatascience.com/data-driven-growth-with-python-part-1-know-your-metrics-812781e66a5b). You can go through the article <a id="_idIndexMarker012"/>for more details. Feel free to try it out for yourself. The dataset is available here: <a href="https://www.kaggle.com/vijayuv/onlineretail">https://www.kaggle.com/vijayuv/onlineretail</a>.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Data (preparation and cleaning)</h2>
			<p>First, let's <a id="_idIndexMarker013"/>install the <strong class="source-inline">pandas</strong> package:</p>
			<pre class="source-code">!pip install pandas</pre>
			<p>Let's make the dataset available to our notebook environment. To do that, download the dataset to your local system, then perform either of the following steps, depending on your setup:</p>
			<ul>
				<li><strong class="bold">Local Jupyter</strong>: Copy the <a id="_idIndexMarker014"/>absolute path of the <strong class="source-inline">.csv</strong> file and give it as input to the <strong class="source-inline">pd.read_csv</strong> method.</li>
				<li><strong class="bold">Google Colab</strong>: Upload<a id="_idIndexMarker015"/> the dataset by clicking on the folder icon and then the upload icon from the left navigation menu.</li>
			</ul>
			<p>Let's preview the dataset:</p>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">retail_data = pd.read_csv('/content/OnlineRetail.csv', </pre>
			<pre class="source-code">                          encoding= 'unicode_escape')</pre>
			<pre class="source-code">retail_data.sample(5)</pre>
			<p>The output of the preceding code block is as follows:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B18024_01_04.jpg" alt="Figure 1.4 – Dataset preview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Dataset preview</p>
			<p>As you can see, the dataset includes customer transaction data. The dataset consists of eight columns, apart<a id="_idIndexMarker016"/> from the index column, which is unlabeled:</p>
			<ul>
				<li><strong class="source-inline">InvoiceNo</strong>: A unique order ID; the data is of the <strong class="source-inline">integer</strong> type</li>
				<li><strong class="source-inline">StockCode</strong>: The unique ID of the product; the data is of the <strong class="source-inline">string</strong> type </li>
				<li><strong class="source-inline">Description</strong>: The product's description; the data is of the <strong class="source-inline">string</strong> type</li>
				<li><strong class="source-inline">Quantity</strong>: The number of units of the product that have been ordered</li>
				<li><strong class="source-inline">InvoiceDate</strong>: The date when the invoice was generated</li>
				<li><strong class="source-inline">UnitPrice</strong>: The cost of the product per unit</li>
				<li><strong class="source-inline">CustomerID</strong>: The unique ID of the customer who ordered the product</li>
				<li><strong class="source-inline">Country</strong>: The country where the product was ordered</li>
			</ul>
			<p>Once you have the dataset, before jumping into feature engineering and model building, data scientists usually perform some exploratory analysis. The idea here is to check if the dataset you have is sufficient to solve the problem, identify missing gaps, check if there is any correlation in the dataset, and more.</p>
			<p>For the exercise, we'll calculate the monthly revenue and look at its seasonality. The following code block extracts year and month (<strong class="source-inline">yyyymm</strong>) information from the <strong class="source-inline">InvoiceDate</strong> column, calculates the <strong class="source-inline">revenue</strong> property of each transaction by multiplying the <strong class="source-inline">UnitPrice</strong> and <strong class="source-inline">Quantity</strong> columns, and aggregates the revenue based on the extracted year-month (<strong class="source-inline">yyyymm</strong>) column.</p>
			<p>Let's continue from the preceding code statement:</p>
			<pre class="source-code">##Convert 'InvoiceDate' to of type datetime</pre>
			<pre class="source-code">retail_data['InvoiceDate'] = pd.to_datetime(</pre>
			<pre class="source-code">    retail_data['InvoiceDate'], errors = 'coerce')</pre>
			<pre class="source-code">##Extract year and month information from 'InvoiceDate'</pre>
			<pre class="source-code">retail_data['yyyymm']=retail_data['InvoiceDate'].dt.strftime('%Y%m')</pre>
			<pre class="source-code">##Calculate revenue generated per order</pre>
			<pre class="source-code">retail_data['revenue'] = retail_data['UnitPrice'] * retail_data['Quantity']</pre>
			<pre class="source-code">## Calculate monthly revenue by aggregating the revenue on year month column  </pre>
			<pre class="source-code">revenue_df = retail_data.groupby(['yyyymm'])['revenue'].sum().reset_index()</pre>
			<pre class="source-code">revenue_df.head()</pre>
			<p>The <a id="_idIndexMarker017"/>preceding code will output the following DataFrame:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B18024_01_05.jpg" alt="Figure 1.5 – Revenue DataFrame&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Revenue DataFrame</p>
			<p>Let's visualize the <strong class="source-inline">revenue</strong> DataFrame. I will be using a library called <strong class="source-inline">plotly</strong>. The following command will install <strong class="source-inline">plotly</strong> in your notebook environment:</p>
			<pre class="source-code">!pip install plotly</pre>
			<p>Let's plot a bar graph from the <strong class="source-inline">revenue</strong> DataFrame with the <strong class="source-inline">yyyymm</strong> column on the <em class="italic">x</em> axis and <strong class="source-inline">revenue</strong> on the <em class="italic">y</em> axis:</p>
			<pre class="source-code">import plotly.express as px</pre>
			<pre class="source-code">##Sort rows on year-month column</pre>
			<pre class="source-code">revenue_df.sort_values( by=['yyyymm'], inplace=True)</pre>
			<pre class="source-code">## plot a bar graph with year-month on x-axis and revenue on y-axis, update x-axis is of type category.</pre>
			<pre class="source-code">fig = px.bar(revenue_df, x="yyyymm", y="revenue", </pre>
			<pre class="source-code">             title="Monthly Revenue") </pre>
			<pre class="source-code">fig.update_xaxes(type='category')</pre>
			<pre class="source-code">fig.show()</pre>
			<p>The<a id="_idIndexMarker018"/> preceding codes sort the revenue DataFrame on the <strong class="source-inline">yyyymm</strong> column and plot a bar graph of <strong class="source-inline">revenue</strong> against the year-month (<strong class="source-inline">yyyymm</strong>) column, as shown in the following screenshot. As you can see, September, October, and November are high revenue months. It would have been good to validate our assumption against a few years of data, but unfortunately, we don't have that. Before we move on to model development, let's look at one more metric – the monthly active customers – and see if it's co-related to monthly revenue:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B18024_01_06.jpg" alt="Figure 1.6 – Monthly revenue&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Monthly revenue</p>
			<p>Continuing in the same notebook, the following commands will calculate the monthly active customers by aggregating a count of unique <strong class="source-inline">CustomerID</strong> on the year-month (<strong class="source-inline">yyyymm</strong>) column:</p>
			<pre class="source-code">active_customer_df = retail_data.groupby(['yyyymm'])['CustomerID'].nunique().reset_index()</pre>
			<pre class="source-code">active_customer_df.columns = ['yyyymm', </pre>
			<pre class="source-code">                              'No of Active customers']</pre>
			<pre class="source-code">active_customer_df.head()</pre>
			<p>The preceding code will produce the following output: </p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B18024_01_07.jpg" alt="Figure 1.7 – Monthly active customers DataFrame&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Monthly active customers DataFrame</p>
			<p>Let's plot<a id="_idIndexMarker019"/> the preceding DataFrame in the same way that we did for monthly revenue:</p>
			<pre class="source-code">## Plot bar graph from revenue data frame with yyyymm column on x-axis and No. of active customers on the y-axis.</pre>
			<pre class="source-code">fig = px.bar(active_customer_df, x="yyyymm", </pre>
			<pre class="source-code">             y="No of Active customers", </pre>
			<pre class="source-code">             title="Monthly Active customers") </pre>
			<pre class="source-code">fig.update_xaxes(type='category')</pre>
			<pre class="source-code">fig.show()</pre>
			<p>The preceding command plots a bar graph of <strong class="source-inline">No of Active customers</strong> against the year-month (<strong class="source-inline">yyyymm</strong>) column. As shown in the following screenshot, <strong class="source-inline">Monthly Active customers</strong> is positively related to the monthly revenue shown in the preceding screenshot:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B18024_01_08.jpg" alt="Figure 1.8 – Monthly active customers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – Monthly active customers</p>
			<p>In the next section, we'll build a customer LTV model.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Model</h2>
			<p>Now that we have<a id="_idIndexMarker020"/> finished exploring the data, let's build the LTV model. <strong class="bold">Customer lifetime value </strong>(<strong class="bold">CLTV</strong>) is defined as <em class="italic">the net profitability associated with a customer's life cycle with the company. Simply put, CLV/LTV is a projection for what each customer is worth to a business</em> (reference: <a href="https://www.toolbox.com/marketing/customer-experience/articles/what-is-customer-lifetime-value-clv/">https://www.toolbox.com/marketing/customer-experience/articles/what-is-customer-lifetime-value-clv/</a>). There are different ways to predict lifetime value. One could be predicting the value of a customer, which is a regression problem, while another way could be predicting the customer group, which is a classification problem. In this exercise, we will use the latter approach. </p>
			<p>For this exercise, we will segment customers into the following groups:</p>
			<ul>
				<li><strong class="bold">Low LTV</strong>: Less active or low revenue customers</li>
				<li><strong class="bold">Mid-LTV</strong>: Fairly active and moderate revenue customers</li>
				<li><strong class="bold">High LTV</strong>: High revenue customers – the segment that we don't want to lose</li>
			</ul>
			<p>We will be using 3 months worth of data to calculate the <strong class="bold">recency</strong> (<strong class="bold">R</strong>), <strong class="bold">frequency</strong> (<strong class="bold">F</strong>), and <strong class="bold">monetary</strong> (<strong class="bold">M</strong>) metrics of the customers to generate features. Once we have these features, we will use 6 months worth of data to calculate the revenue of every customer and generate LTV cluster labels (low LTV, mid-LTV, and high LTV). The generated labels and features will then be used to train an XGBoost model that can be used to predict the group of new customers.</p>
			<h3>Feature engineering</h3>
			<p>Let's continue our work in<a id="_idIndexMarker021"/> the same notebook, calculate the R, F, and M values for the customers, and group our customers based on a value that's been calculated from the individual R, F, and M scores:</p>
			<ul>
				<li><strong class="bold">Recency (R)</strong>: The recency metric represents how many days have passed since the customer made their last purchase. </li>
				<li><strong class="bold">Frequency (F)</strong>: As the term suggests, F represents how many times the customer made a purchase.</li>
				<li><strong class="bold">Monetary (M)</strong>: How much revenue a particular customer brought in. </li>
			</ul>
			<p>Since the spending and purchase patterns of customers differ based on demographic location, we will only consider the data that belongs to the United Kingdom for this exercise. Let's read the <strong class="source-inline">OnlineRetails.csv</strong> file and filter out the data that doesn't belong to the United Kingdom:</p>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">from datetime import datetime, timedelta, date</pre>
			<pre class="source-code">from sklearn.cluster import KMeans</pre>
			<pre class="source-code">##Read the data and filter out data that belongs to country other than UK</pre>
			<pre class="source-code">retail_data = pd.read_csv('/content/OnlineRetail.csv', </pre>
			<pre class="source-code">                           encoding= 'unicode_escape')</pre>
			<pre class="source-code">retail_data['InvoiceDate'] = pd.to_datetime(</pre>
			<pre class="source-code">    retail_data['InvoiceDate'], errors = 'coerce')</pre>
			<pre class="source-code">uk_data = retail_data.query("Country=='United Kingdom'").reset_index(drop=True)</pre>
			<p>In the following code block, we will create two different DataFrames. The first one (<strong class="source-inline">uk_data_3m</strong>) will be for <strong class="source-inline">InvoiceDate</strong> between <strong class="source-inline">2011-03-01</strong> and <strong class="source-inline">2011-06-01</strong>. This DataFrame will be used to generate the RFM features. The second DataFrame (<strong class="source-inline">uk_data_6m</strong>) will be for <strong class="source-inline">InvoiceDate</strong> between <strong class="source-inline">2011-06-01</strong> and <strong class="source-inline">2011-12-01</strong>. This DataFrame will be used to generate the target column for model training. In this exercise, the target column is LTV groups/clusters. Since we are calculating the customer LTV group, a larger time interval would give a better grouping. Hence, we will be using 6 months worth of data to generate the LTV group labels:</p>
			<pre class="source-code">## Create 3months and 6 months data frames</pre>
			<pre class="source-code">t1 = pd.Timestamp("2011-06-01 00:00:00.054000")</pre>
			<pre class="source-code">t2 = pd.Timestamp("2011-03-01 00:00:00.054000")</pre>
			<pre class="source-code">t3 = pd.Timestamp("2011-12-01 00:00:00.054000")</pre>
			<pre class="source-code">uk_data_3m = uk_data[(uk_data.InvoiceDate &lt; t1) &amp; (uk_data.InvoiceDate &gt;= t2)].reset_index(drop=True)</pre>
			<pre class="source-code">uk_data_6m = uk_data[(uk_data.InvoiceDate &gt;= t1) &amp; (uk_data.InvoiceDate &lt; t3)].reset_index(drop=True)</pre>
			<p>Now that we have two different DataFrames, let's calculate the RFM values<a id="_idIndexMarker022"/> using the <strong class="source-inline">uk_data_3m</strong> DataFrame. The following code block calculates the <strong class="source-inline">revenue</strong> column by multiplying <strong class="source-inline">UnitPrice</strong> with <strong class="source-inline">Quantity</strong>. To calculate the RFM values, the code block performs three aggregations on <strong class="source-inline">CustomerID</strong>:</p>
			<ul>
				<li>To calculate <strong class="bold">R</strong>, <strong class="source-inline">max_date</strong> in the DataFrame must be calculated and for every customer, we must calculate <strong class="source-inline">R = max_date – x.max()</strong>, where <strong class="source-inline">x.max()</strong> calculates the latest <strong class="source-inline">InvoiceDate</strong> of a specific <strong class="source-inline">CustomerID</strong>. </li>
				<li>To calculate <strong class="bold">F</strong>, <strong class="source-inline">count</strong> the number of invoices for a specific <strong class="source-inline">CustomerID</strong>.</li>
				<li>To calculate <strong class="bold">M</strong>, find the <strong class="source-inline">sum</strong> value of <strong class="source-inline">revenue</strong> for a specific <strong class="source-inline">CustomerID</strong>.</li>
			</ul>
			<p>The following code snippet performs this logic:</p>
			<pre class="source-code">## Calculate RFM values.</pre>
			<pre class="source-code">uk_data_3m['revenue'] = uk_data_3m['UnitPrice'] * uk_data_3m['Quantity']</pre>
			<pre class="source-code"># Calculating the max invoice date in data (Adding additional day to avoid 0 recency value)</pre>
			<pre class="source-code">max_date = uk_data_3m['InvoiceDate'].max() + timedelta(days=1)</pre>
			<pre class="source-code">rfm_data = uk_data_3m.groupby(['CustomerID']).agg({</pre>
			<pre class="source-code">        'InvoiceDate': lambda x: (max_date - x.max()).days,</pre>
			<pre class="source-code">        'InvoiceNo': 'count',</pre>
			<pre class="source-code">        'revenue': 'sum'})</pre>
			<pre class="source-code">rfm_data.rename(columns={'InvoiceDate': 'Recency',</pre>
			<pre class="source-code">                         'InvoiceNo': 'Frequency',</pre>
			<pre class="source-code">                         'revenue': 'MonetaryValue'}, </pre>
			<pre class="source-code">                         inplace=True)</pre>
			<p>Here, we have calculated the<a id="_idIndexMarker023"/> R, F, and M values for the customers. Next, we need to divide customers into the R, F, and M groups. This grouping defines where a customer stands concerning the other customers in terms of the R, F, and M metrics. To calculate the R, F, and M groups, we will divide the customers into equal-sized groups based on their R, F, and M values, respectively. These were<a id="_idIndexMarker024"/> calculated in the previous code block. To achieve this, we will use a method called <strong class="source-inline">pd.qcut</strong> (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html</a>) on the DataFrame. Alternatively, you can use any <em class="italic">clustering</em> methods to divide customers into different groups. We will add the R, F, and M groups' values together to generate a single value called <strong class="source-inline">RFMScore</strong> that will range from 0 to 9. </p>
			<p>In this exercise, the customers will be divided into four <a id="_idIndexMarker025"/>groups. The <em class="italic">elbow method </em>(<a href="https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6">https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6</a>) can be used to calculate the optimal number of groups for any dataset. The preceding link also contains information about alternative methods you can use to calculate the optimal number of groups, so feel free to try it out. I will leave that as an exercise for you. </p>
			<p>The following code block calculates <strong class="source-inline">RFMScore</strong>:</p>
			<pre class="source-code">## Calculate RFM groups of customers </pre>
			<pre class="source-code">r_grp = pd.qcut(rfm_data['Recency'], q=4, </pre>
			<pre class="source-code">                labels=range(3,-1,-1))</pre>
			<pre class="source-code">f_grp = pd.qcut(rfm_data['Frequency'], q=4, </pre>
			<pre class="source-code">                labels=range(0,4))</pre>
			<pre class="source-code">m_grp = pd.qcut(rfm_data['MonetaryValue'], q=4, </pre>
			<pre class="source-code">                labels=range(0,4))</pre>
			<pre class="source-code">rfm_data = rfm_data.assign(R=r_grp.values).assign(F=f_grp.values).assign(M=m_grp.values)</pre>
			<pre class="source-code">rfm_data['R'] = rfm_data['R'].astype(int)</pre>
			<pre class="source-code">rfm_data['F'] = rfm_data['F'].astype(int)</pre>
			<pre class="source-code">rfm_data['M'] = rfm_data['M'].astype(int)</pre>
			<pre class="source-code">rfm_data['RFMScore'] = rfm_data['R'] + rfm_data['F'] + rfm_data['M']</pre>
			<pre class="source-code">rfm_data.groupby('RFMScore')['Recency','Frequency','MonetaryValue'].mean()</pre>
			<p>The <a id="_idIndexMarker026"/>preceding code will generate the following output:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B18024_01_09.jpg" alt="Figure 1.9 – RFM score summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – RFM score summary</p>
			<p>This summary data gives us a rough idea of how <strong class="source-inline">RFMScore</strong> is directly proportional to the <strong class="source-inline">Recency</strong>, <strong class="source-inline">Frequency</strong>, and <strong class="source-inline">MonetaryValue</strong> metrics. For example, the group with <strong class="source-inline">RFMScore=0</strong> has the highest mean recency (the last purchase day of this group is the farthest in past), the lowest mean frequency, and the lowest mean monetary value. On the other hand, the group with <strong class="source-inline">RFMScore=9</strong> has the lowest mean recency, highest mean frequency, and highest mean monetary value.</p>
			<p>With that, we <a id="_idIndexMarker027"/>understand <strong class="source-inline">RFMScore</strong> is positively related to the value a customer brings to the business. So, let's segment customers as follows:</p>
			<ul>
				<li>0-3 =&gt; Low value</li>
				<li>4-6 =&gt; Mid value</li>
				<li>7-9 =&gt; High value</li>
			</ul>
			<p>The following code labels customers as having either a low, mid, or high value:</p>
			<pre class="source-code"># segment customers.</pre>
			<pre class="source-code">rfm_data['Segment'] = 'Low-Value'</pre>
			<pre class="source-code">rfm_data.loc[rfm_data['RFMScore']&gt;4,'Segment'] = 'Mid-Value' </pre>
			<pre class="source-code">rfm_data.loc[rfm_data['RFMScore']&gt;6,'Segment'] = 'High-Value' </pre>
			<pre class="source-code">rfm_data = rfm_data.reset_index()</pre>
			<h3>Customer LTV</h3>
			<p>Now that we have RFM<a id="_idIndexMarker028"/> features ready for the customers in the DataFrame that contains 3 months worth of data, let's use 6 months worth of data (<strong class="source-inline">uk_data_6m)</strong> to calculate the revenue of the customers, as we did previously, and merge the RFM features with the newly created revenue DataFrame:</p>
			<pre class="source-code"># Calculate revenue using the six month dataframe.</pre>
			<pre class="source-code">uk_data_6m['revenue'] = uk_data_6m['UnitPrice'] * uk_data_6m['Quantity']</pre>
			<pre class="source-code">revenue_6m = uk_data_6m.groupby(['CustomerID']).agg({</pre>
			<pre class="source-code">        'revenue': 'sum'})</pre>
			<pre class="source-code">revenue_6m.rename(columns={'revenue': 'Revenue_6m'}, </pre>
			<pre class="source-code">                  inplace=True)</pre>
			<pre class="source-code">revenue_6m = revenue_6m.reset_index()</pre>
			<pre class="source-code">revenue_6m = revenue_6m.dropna()</pre>
			<pre class="source-code"># Merge the 6m revenue data frame with RFM data.</pre>
			<pre class="source-code">merged_data = pd.merge(rfm_data, revenue_6m, how="left")</pre>
			<pre class="source-code">merged_data.fillna(0)</pre>
			<p>Feel free to<a id="_idIndexMarker029"/> plot <strong class="source-inline">revenue_6m</strong> against <strong class="source-inline">RFMScore</strong>. You will see a positive correlation between the two.</p>
			<p>In the flowing code block, we are using the <strong class="source-inline">revenue_6m</strong> columns, which is the <em class="italic">lifetime value of a customer</em>, and creating three groups called <em class="italic">Low LTV</em>, <em class="italic">Mid LTV</em>, and <em class="italic">High LTV</em> using K-means clustering. Again, you can verify the optimal number of clusters using the <em class="italic">elbow method</em> mentioned previously:</p>
			<pre class="source-code"># Create LTV cluster groups</pre>
			<pre class="source-code">merged_data = merged_data[merged_data['Revenue_6m']&lt;merged_data['Revenue_6m'].quantile(0.99)]</pre>
			<pre class="source-code">kmeans = KMeans(n_clusters=3)</pre>
			<pre class="source-code">kmeans.fit(merged_data[['Revenue_6m']])</pre>
			<pre class="source-code">merged_data['LTVCluster'] = kmeans.predict(merged_data[['Revenue_6m']])</pre>
			<pre class="source-code">merged_data.groupby('LTVCluster')['Revenue_6m'].describe()</pre>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B18024_01_10.jpg" alt="Figure 1.10 – LTV cluster summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – LTV cluster summary</p>
			<p>As you can see, the<a id="_idIndexMarker030"/> cluster with label 1 contains the group of customers whose lifetime value is very high since the mean revenue of the group is $14,123.309, whereas there are only 21 such customers. The cluster with label 0 contains the group of customers whose lifetime value is low since the mean revenue of the group is only $828.67, whereas there are 1,170 such customers. This grouping gives us an idea of which customers should always be kept happy. </p>
			<h3>The feature set and model</h3>
			<p>Let's build an <a id="_idIndexMarker031"/>XGBoost model using the features we have calculated so far so that the model can predict the LTV group of the customers, given the input features. The following is the final feature set that will be used as input for the model:</p>
			<pre class="source-code">feature_data = pd.get_dummies(merged_data)</pre>
			<pre class="source-code">feature_data.head(5)</pre>
			<p>The preceding code block produces the following DataFrame. This includes the feature set that will be used to train the model:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B18024_01_11.jpg" alt="Figure 1.11 – Feature set for model training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Feature set for model training</p>
			<p>Now, let's use this feature set to train the <strong class="source-inline">Xgboost</strong> model. The prediction label (<strong class="source-inline">y</strong>) is the <strong class="source-inline">LTVCluster</strong> column; the rest of the dataset except for the <strong class="source-inline">Revenue_6m</strong> and <strong class="source-inline">CustomerID</strong> columns are the <strong class="source-inline">X</strong> value. <strong class="source-inline">Revenue_6m</strong> will be dropped from the feature set as the <strong class="source-inline">LTVCluster</strong> column (<strong class="source-inline">y</strong>) is calculated using <strong class="source-inline">Revenue_6m</strong>. For the new customer, we can calculate other features without needing at least 6 months worth of data and also predict their <strong class="source-inline">LTVCluster(y)</strong>. </p>
			<p>The following <a id="_idIndexMarker032"/>code will train the <strong class="source-inline">Xgboost</strong> model:</p>
			<pre class="source-code">from sklearn.metrics import classification_report, confusion_matrix</pre>
			<pre class="source-code">import xgboost as xgb</pre>
			<pre class="source-code">from sklearn.model_selection import KFold, cross_val_score, train_test_split</pre>
			<pre class="source-code">#Splitting data into train and test data set.</pre>
			<pre class="source-code">X = feature_data.drop(['CustomerID', 'LTVCluster',</pre>
			<pre class="source-code">                       'Revenue_6m'], axis=1)</pre>
			<pre class="source-code">y = feature_data['LTVCluster']</pre>
			<pre class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)</pre>
			<pre class="source-code">xgb_classifier = xgb.XGBClassifier(max_depth=5, objective='multi:softprob')</pre>
			<pre class="source-code">xgb_model = xgb_classifier.fit(X_train, y_train)</pre>
			<pre class="source-code">y_pred = xgb_model.predict(X_test)</pre>
			<pre class="source-code">print(classification_report(y_test, y_pred))</pre>
			<p>The preceding code block will output the following classification results:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B18024_01_12.jpg" alt="Figure 1.12 – Classification report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – Classification report</p>
			<p>Now, let's assume that we are happy with the model and want to take it to the next level – that is, to production. </p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Package, release, and monitor</h2>
			<p>So far, we have<a id="_idIndexMarker033"/> spent a lot of time looking at data analysis, exploration, cleaning, and model building since that is what a data scientist should concentrate on. But once all that work has been done, can the model be deployed without any additional work? The answer is no. We are still far away from deployment. We must do the following things before we can deploy the model:</p>
			<ul>
				<li>We must create a scheduled data pipeline that performs data cleaning and feature engineering.</li>
				<li>We need a way to fetch features during prediction. If it's an online/transactional model, there should be a way to fetch features at low latency. Since customers' R, F, and M values change frequently, let's say that we want to run two different campaigns for mid-value and high-value segments on the website. There will be a need to score customers in near-real time.</li>
				<li>Find a way to reproduce the model using the historical data.</li>
				<li>Perform<a id="_idIndexMarker034"/> model packaging and versioning. </li>
				<li>Find a way to AB test the model.</li>
				<li>Find a way to monitor model and data drift.</li>
			</ul>
			<p>As we don't have any of these ready, let's stop here and look back at what we have done, if there is a way to do this better, and see if there are any common oversights.</p>
			<p>In the next section, we'll look at <em class="italic">what we think we have built (ideal world) versus what we have built (real world)</em>.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>An ideal world versus the real world</h1>
			<p>Now that we have spent <a id="_idIndexMarker035"/>a good amount of time building this beautiful data product that can help the business treat customers differently based on the value they bring to the table, let's look at what we expect from this versus what it can do. </p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Reusability and sharing</h2>
			<p>Reusability is <a id="_idIndexMarker036"/>one of the common problems in the IT industry. We have this great data for a product in front of us, the graphs we built during exploration, and the features we generated for our model. These can be reused by other data scientists, analysts, and data engineers. With the state it is in currently, can it be reused? The answer is maybe. Data scientists can share the notebook itself, can create a presentation, and so on. But there is no way for somebody to discover if they are looking for, say, customer segmentation or RFM features, which could be very useful in other models. So, if another <a id="_idIndexMarker037"/>data scientist or ML engineer is building a model that needs the same features, the only option they are left with is to reinvent the same wheel. The new model may be built with the same, more accurate, or less accurate RFM features based on how the data scientist generates it. However, it could be a case where the development of the second model could have been accelerated if there was a better way to discover and reuse the work. Also, as the saying goes, <em class="italic">two heads are better than one</em>. A collaboration would have benefitted both the data scientist and the business. </p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Everything in a notebook</h2>
			<p>Data science<a id="_idIndexMarker038"/> is a unique skill that is different from software engineering. Though some of the data scientists might have a software engineer background, the needs of the role itself may push them away from software engineering skills. As the data scientists spend more time in the data exploration and model building phases, the <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>) may not be sufficient as the amount of data they are dealing with is huge. The data processing phase will run for days if we have to explore, do feature engineering, and do model building on our personal Mac or PC. Also, they need to have the flexibility to use different programming languages such as Python, Scala, R, SQL, and others to add commands dynamically during analysis. That is one of the reasons why there are so many notebook platform providers, including Jupyter, Databricks, and SageMaker.</p>
			<p>Since data product/model <a id="_idIndexMarker039"/>development is different from traditional software development, it is always impossible to ship the experimental code to production without any additional work. Most data scientists start their work in a notebook and build everything in the same way as we did in the previous section. A few standard practices and tools such as feature store will not only help them break down the model building process into multiple production-ready notebooks but can also help them avoid re-processing data, debugging issues, and code reuse.</p>
			<p>Now that we understand the reality of ML development, let's briefly go through the most time-consuming stages of ML.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>The most time-consuming stages of ML</h1>
			<p>In the first section of this chapter, we went through the different stages of the ML life cycle. Let's look at some of the stages in more detail and consider their level of complexity and the time we should spend on each of them. </p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Figuring out the dataset</h2>
			<p>Once we have a <a id="_idIndexMarker040"/>problem statement, the next step is to figure out the dataset we need to solve the problem. In the example we followed, we knew where the dataset was and it was given. However, in the real world, it is not that simple. Since each organization has its own way of data warehousing, it may be simple or take forever to find the data you need. Most organizations run data catalog services such as Amundsen, Atlan, and Azure Data Catalog to make their dataset easily discoverable. But again, the tools are as good as the way they are used or the people using them. So, the point I'm making here is that it's always easy to find the data you are looking for. Apart from this, considering the access control for the data, even if you figure out the dataset that's needed for the problem, it is highly likely that you may not have access to it unless you have used it before. Figuring out access will be another major roadblock. </p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Data exploration and feature engineering</h2>
			<p>Data exploration: once you figure out<a id="_idIndexMarker041"/> the dataset, the next biggest task is to <em class="italic">figure out the dataset again!</em> You read that right – for a data scientist, the next biggest<a id="_idIndexMarker042"/> task is to make sure that the dataset they've picked is the right dataset to solve the problem. This would involve data cleaning, augmenting missing data, transforming data, plotting different graphs, finding a correlation, finding out data skew, and more. The best part is that if the data scientists find that something is not right, they will go back to the previous step, which is to look for more datasets, try them out again, and go back. </p>
			<p>Feature engineering<a id="_idIndexMarker043"/> is not easy either; domain knowledge becomes key to<a id="_idIndexMarker044"/> building the feature set to train the model. If you are a data scientist who has been working on the pricing and promotion models for the past few years, you would know what dataset and features would result in a better model than a data scientist who has been working on customer value models for the past few years and vice versa. Let's try out an exercise and see if feature engineering is easy or not and if domain knowledge plays a key role. Have a look at the following screenshot and see if you can recognize the animals:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18024_01_13.jpg" alt="Figure 1.13 – A person holding a dog and a cat&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – A person holding a dog and a cat</p>
			<p>I'm sure you know what <a id="_idIndexMarker045"/>these animals are, but let's take a step back and see how we correctly identified the animals. When we looked at the figure, our subconscious did feature engineering. It could have picked features such as <em class="italic">it has a couple of ears</em>, a <em class="italic">couple of eyes</em>, <em class="italic">a nose</em>, <em class="italic">a head</em>, and <em class="italic">a tail</em>. Instead, it picked much more sophisticated features, such as <em class="italic">the shape of its face</em>, <em class="italic">the shape of its eyes</em>, <em class="italic">the shape of its nose</em>, and <em class="italic">the color and texture of its fur</em>. If it had picked the first set of features, both animals would have turned out to be the same, which is an example of bad feature engineering and a bad model. Since it chose the latter, we identified it as different animals. Again, this is an example of good feature engineering and a good model. </p>
			<p>But another question we need to answer would be, when did we develop expertise in animal identification? Well, maybe it's from our kindergarten teachers. We all remember some version of the first 100 animals that we learned about from our teachers, parents, brothers, and sisters. We didn't get all of them right at first but eventually, we did. We gained expertise over time. </p>
			<p>Now, what if, instead of a picture of a cat and a dog, it was a picture of two snakes and our job was to identify which one of them is venomous and which is not. Though all of us could identify them as snakes, almost none of us would be able to identify which one is venomous and which is not. Unless the person has been a snake charmer before. </p>
			<p>Hence, domain expertise becomes crucial in feature engineering. Just like the data exploration stage, if we are not happy with the features, we are back to square one, which involves looking for more data and better features. </p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Modeling to production and monitoring</h2>
			<p>Once we've figured out the aforementioned stage, taking the model to production is very time-consuming unless the right infrastructure is ready and waiting. For a model to run in production, it needs a processing platform that will run the data cleaning and feature engineering<a id="_idIndexMarker046"/> code. It also needs an orchestration framework to run the feature engineering pipeline in a scheduled or event-based way. We also need a way to store and retrieve features securely at low latency in some cases. If the model is transactional, the model must be packaged so that it can be accessed by the consumers securely, maybe as a REST endpoint. Also, the deployed model should be scalable to serve the incoming traffic. </p>
			<p>Model and data monitoring are crucial aspects too. As model performance directly affects the business, you must know what metrics would determine that the model needs to be retrained in advance. Other than model monitoring, the dataset also needs to be monitored for skews. For example, in an e-commerce business, traffic patterns and purchase patterns <a id="_idIndexMarker047"/>may change frequently based on seasonality, trends, and other factors. Identifying these changes early will affect the business positively. Hence, data and feature monitoring are key in taking the model to production. </p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Summary</h1>
			<p>In this chapter, we discussed the different stages in the ML life cycle. We picked a problem statement, performed data exploration, plotted a few graphs, did feature engineering and customer segmentation, and built a customer lifetime value model. We looked at the oversights and discussed the most time-consuming stages of ML. I wanted to get you onto the same page as I am and set a good foundation for the rest of this book. </p>
			<p>In the next chapter, we will set the stage for the need for a feature store and how it could improve the ML process. We will also discuss the need to bring features into production and some of the traditional ways of doing so. </p>
		</div>
		<div id="_idContainer020" class="Basic-Text-Frame">
			<p class="hidden">     Chapter 1: An Overview of the Machine Learning Life Cycle</p>
		</div>
		<div id="_idContainer021" class="Basic-Text-Frame">
			<p class="hidden">Chapter 1: An Overview of the Machine Learning Life Cycle    </p>
		</div>
	</body></html>