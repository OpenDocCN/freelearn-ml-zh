<html><head></head><body>
  <div id="_idContainer330" class="Basic-Text-Frame">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-163" class="chapterTitle">Modeling for Computer Vision</h1>
    <p class="normal">Computer vision tasks are among the most popular problems in practical applications of machine learning; they were the gateway into deep learning for many Kagglers, including yours truly (Konrad, that is). Over the last few years, there has been tremendous progress in the field and new SOTA libraries continue to be released. In this chapter, we will give you an overview of the most popular competition types in computer vision:</p>
    <ul>
      <li class="bulletList">Image classification</li>
      <li class="bulletList">Object detection</li>
      <li class="bulletList">Image segmentation</li>
    </ul>
    <p class="normal">We will begin with a short section on image augmentation, a group of task-agnostic techniques that can be applied to different problems to increase the generalization capability of our models.</p>
    <h1 id="_idParaDest-164" class="heading-1">Augmentation strategies</h1>
    <p class="normal">While deep <a id="_idIndexMarker835"/>learning techniques have been extremely successful in computer vision tasks like image recognition, segmentation, or object detection, the underlying algorithms are typically extremely data-intensive: they require large amounts of data to avoid overfitting. However, not all domains of interest satisfy that requirement, which is where <strong class="keyWord">data augmentation</strong><strong class="keyWord"><a id="_idIndexMarker836"/></strong><strong class="keyWord"> </strong>comes in. This is the name for a group of image processing techniques that create modified versions of images, thus enhancing the size and quality of training datasets, leading to better performance of deep learning models. The augmented data will typically represent a more comprehensive set of possible data points, thereby minimizing the distance between the training and validation set, as well as any future test sets.</p>
    <p class="normal">In this section, we will review<a id="_idIndexMarker837"/> some of the more common augmentation techniques, along with choices for their software implementations. The most frequently<a id="_idIndexMarker838"/> used transformations include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Flipping</strong>: Flipping the image (along the horizontal or vertical axis)</li>
      <li class="bulletList"><strong class="keyWord">Rotation</strong>: Rotating the image by a given angle (clockwise or anti-clockwise)</li>
      <li class="bulletList"><strong class="keyWord">Cropping</strong>: A random subsection of the image is selected</li>
      <li class="bulletList"><strong class="keyWord">Brightness</strong>: Modifying the brightness of the image</li>
      <li class="bulletList"><strong class="keyWord">Scaling</strong>: The image is increased or decreased to a higher (outward) or lower (inward) size</li>
    </ul>
    <p class="normal">Below, we demonstrate<a id="_idIndexMarker839"/> how those transformations work in practice using the image of an American acting legend and comedian, Betty White: </p>
    <figure class="mediaobject"><img src="../Images/B17574_10_01.png" alt="Obraz zawierający osoba, niebieski  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.1: Betty White image</p>
    <p class="normal">We can flip the image along the vertical or horizontal axes:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_02.png" alt="Obraz zawierający osoba, kobieta, niebieski  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.2: Betty White image – flipped vertically (left) and horizontally (right)</p>
    <p class="normal">Rotations are fairly <a id="_idIndexMarker840"/>self-explanatory; notice the automatic padding of the image in the background:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_03.png" alt="Obraz zawierający osoba  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.3: Betty White image – rotated clockwise</p>
    <p class="normal">We can also crop <a id="_idIndexMarker841"/>an image to the region of interest:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_04.png" alt="Obraz zawierający osoba, zamknąć, oczy, wpatrywanie się  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.4: Betty White image – cropped</p>
    <p class="normal">On a high level, we can say that augmentations can be applied in one of two ways:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Offline</strong>: These <a id="_idIndexMarker842"/>are usually applied for smaller datasets (fewer images or smaller sizes, although the definition of “small” depends on the available hardware). The idea is to generate modified versions of the original images as a preprocessing step for your dataset, and then use those alongside the “original” ones.</li>
      <li class="bulletList"><strong class="keyWord">Online</strong>: These <a id="_idIndexMarker843"/>are used for bigger datasets. The augmented images are not saved on disk; the augmentations are applied in mini-batches and fed to the model.</li>
    </ul>
    <p class="normal">In the next few sections, we will give you an overview of two of the most common methods for augmenting your image dataset: the built-in Keras functionality and the <code class="inlineCode">albumentations</code> package. There are several other options available out there (<code class="inlineCode">skimage</code>, OpenCV, <code class="inlineCode">imgaug</code>, Augmentor, SOLT), but we will focus on the most popular ones.</p>
    <div class="note">
      <p class="normal">The methods discussed in this chapter focus on image analysis powered by GPU. The use <a id="_idIndexMarker844"/>of <strong class="keyWord">tensor processing units</strong> (<strong class="keyWord">TPUs</strong>) is an emerging, but still somewhat niche, application. Readers interested in image augmentation in combination with TPU-powered analysis are encouraged to check out the excellent <a id="_idIndexMarker845"/>work of <em class="italic">Chris Deotte</em> (<strong class="keyWord">@cdeotte</strong>):</p>
      <p class="normal"><a href="https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords"><span class="url">https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords</span></a></p>
      <p class="normal">Chris is a quadruple Kaggle Grandmaster and a fantastic educator through the Notebooks he creates and discussions he participates in; overall, a person definitely worth following for any Kaggler, irrespective of your level of experience.</p>
    </div>
    <p class="normal">We will be using<a id="_idIndexMarker846"/> data from the <em class="italic">Cassava Leaf Disease Classification</em> competition (<a href="https://www.kaggle.com/c/cassava-leaf-disease-classification"><span class="url">https://www.kaggle.com/c/cassava-leaf-disease-classification</span></a>). As usual, we begin with the groundwork: first, loading the necessary packages:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> scipy <span class="hljs-keyword">as</span> sp
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">from</span> skimage.io <span class="hljs-keyword">import</span> imshow, imread, imsave
<span class="hljs-comment"># imgaug</span>
<span class="hljs-keyword">import</span> imageio
<span class="hljs-keyword">import</span> imgaug <span class="hljs-keyword">as</span> ia
<span class="hljs-keyword">import</span> imgaug.augmenters <span class="hljs-keyword">as</span> iaa
<span class="hljs-comment"># Albumentations</span>
<span class="hljs-keyword">import</span> albumentations <span class="hljs-keyword">as</span> A
<span class="hljs-comment"># Keras</span>
<span class="hljs-comment"># from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img</span>
<span class="hljs-comment"># Visualization</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> matplotlib.image <span class="hljs-keyword">as</span> mpimg
%matplotlib inline
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> HTML, Image
<span class="hljs-comment"># Warnings</span>
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
</code></pre>
    <p class="normal">Next, we define some helper functions that will streamline the presentation later. We need a way to load images into arrays:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">load_image</span>(<span class="hljs-params">image_id</span>):
    file_path = image_id 
    image = imread(Image_Data_Path + file_path)
    <span class="hljs-keyword">return</span> image
</code></pre>
    <p class="normal">We would like to display multiple images in a gallery style, so we create a function that takes as input an array containing the images along with the desired number of columns, and outputs the array reshaped into a grid with a given number of columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">gallery</span>(<span class="hljs-params">array, ncols=</span><span class="hljs-number">3</span>):
    nindex, height, width, intensity = array.shape
    nrows = nindex//ncols
    <span class="hljs-keyword">assert</span> nindex == nrows*ncols
    result = (array.reshape(nrows, ncols, height, width, intensity)
              .swapaxes(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)
              .reshape(height*nrows, width*ncols, intensity))
    <span class="hljs-keyword">return</span> result
</code></pre>
    <p class="normal">With the boilerplate<a id="_idIndexMarker847"/> taken care of, we can load the images for augmentation:</p>
    <pre class="programlisting code"><code class="hljs-code">data_dir = <span class="hljs-string">'../input/cassava-leaf-disease-classification/'</span>
Image_Data_Path = data_dir + <span class="hljs-string">'/train_images/'</span>
train_data = pd.read_csv(data_dir + <span class="hljs-string">'/train.csv'</span>)
<span class="hljs-comment"># We load and store the first 10 images in memory for faster access</span>
train_images = train_data[<span class="hljs-string">"image_id"</span>][:<span class="hljs-number">10</span>].apply(load_image)
</code></pre>
    <p class="normal">Let’s load a single image so we know what our reference is:</p>
    <pre class="programlisting code"><code class="hljs-code">curr_img = train_images[<span class="hljs-number">7</span>]
plt.figure(figsize = (<span class="hljs-number">15</span>,<span class="hljs-number">15</span>))
plt.imshow(curr_img)
plt.axis(<span class="hljs-string">'off'</span>)
</code></pre>
    <p class="normal">Here it is:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_05.png" alt="Obraz zawierający podłoże, roślina, zewnętrzne, zielony  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.5: Reference image</p>
    <p class="normal">In the following sections, we <a id="_idIndexMarker848"/>will demonstrate how to generate augmented images from this reference image using both built-in Keras functionality and the <code class="inlineCode">albumentations</code> library.</p>
    <h2 id="_idParaDest-165" class="heading-2">Keras built-in augmentations</h2>
    <p class="normal">The<a id="_idIndexMarker849"/> Keras library has a built-in functionality for augmentations. While not as extensive as dedicated packages, it has the advantage of easy integration with your code. We do not need a separate code block for defining the augmentation transformations but can incorporate them inside <code class="inlineCode">ImageDataGenerator</code>, a functionality we are likely to be using anyway.</p>
    <p class="normal">The first Keras approach we examine is based upon the <code class="inlineCode">ImageDataGenerator</code> class. As the name suggests, it can be used to generate batches of image data with real-time data augmentations.</p>
    <h3 id="_idParaDest-166" class="heading-3">ImageDataGenerator approach</h3>
    <p class="normal">We begin <a id="_idIndexMarker850"/>by instantiating <a id="_idIndexMarker851"/>an object of class <code class="inlineCode">ImageDataGenerator</code> in the following manner: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator,
array_to_img, img_to_array, load_img 
datagen = ImageDataGenerator( 
        rotation_range = <span class="hljs-number">40</span>, 
        shear_range = <span class="hljs-number">0.2</span>, 
        zoom_range = <span class="hljs-number">0.2</span>, 
        horizontal_flip = <span class="hljs-literal">True</span>, 
        brightness_range = (<span class="hljs-number">0.5</span>, <span class="hljs-number">1.5</span>)) 
curr_img_array = img_to_array(curr_img)
curr_img_array = curr_img_array.reshape((<span class="hljs-number">1</span>,) + curr_img_array.shape)
</code></pre>
    <p class="normal">We define the desired augmentations as arguments to <code class="inlineCode">ImageDataGenerator</code>. The official documentation does not seem to address the topic, but practical results indicate that the augmentations are applied in the order in which they are defined as arguments.</p>
    <div class="note">
      <p class="normal">In the above example, we utilize only a limited subset of possible options; for a full list, the reader is encouraged to consult the official documentation: <a href="https://keras.io/api/preprocessing/image/"><span class="url">https://keras.io/api/preprocessing/image/</span></a>.</p>
    </div>
    <p class="normal">Next, we iterate through the images with the <code class="inlineCode">.flow</code> method of the <code class="inlineCode">ImageDataGenerator</code> object. The class provides three different functions to load the image dataset in memory and generate batches of augmented data:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">flow</code></li>
      <li class="bulletList"><code class="inlineCode">flow_from_directory</code></li>
      <li class="bulletList"><code class="inlineCode">flow_from_dataframe</code></li>
    </ul>
    <p class="normal">They all achieve the same objective, but differ in the way the locations of the files are specified. In our example, the images are already in memory, so we can iterate using the simplest method:</p>
    <pre class="programlisting code"><code class="hljs-code">i = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> datagen.flow(
    curr_img_array,
    batch_size=<span class="hljs-number">1</span>,
    save_to_dir=<span class="hljs-string">'.'</span>,
    save_prefix=<span class="hljs-string">'Augmented_image'</span>,
    save_format=<span class="hljs-string">'jpeg'</span>):
    i += <span class="hljs-number">1</span>
    <span class="hljs-comment"># Hard-coded stop - without it, the generator enters an infinite loop</span>
    <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">9</span>: 
        <span class="hljs-keyword">break</span>  
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker852"/>examine the augmented<a id="_idIndexMarker853"/> images using the helper functions we defined earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">aug_images = []
<span class="hljs-keyword">for</span> img_path <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">"*.jpeg"</span>):
    aug_images.append(mpimg.imread(img_path))
plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))
plt.axis(<span class="hljs-string">'</span><span class="hljs-string">off'</span>)
plt.imshow(gallery(np.array(aug_images[<span class="hljs-number">0</span>:<span class="hljs-number">9</span>]), ncols = <span class="hljs-number">3</span>))
plt.title(<span class="hljs-string">'Augmentation examples'</span>)
</code></pre>
    <p class="normal">Here is the result:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_06.png" alt="Obraz zawierający roślina  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.6: A collection of augmented images</p>
    <p class="normal">Augmentations <a id="_idIndexMarker854"/>are a very useful tool, but using them efficiently requires a judgment call. First, it is obviously a good <a id="_idIndexMarker855"/>idea to visualize them to get a feeling for the impact on the data. On the one hand, we want to introduce some variation in the data to increase the generalization of our model; on the other, if we change the images too radically, the input data will be less informative and the model performance is likely to suffer. In addition, the choice of which augmentations to use can also be problem-specific, as we can see by comparing different competitions.</p>
    <p class="normal">If you look at <em class="italic">Figure 10.6</em> above (the reference image from the <em class="italic">Cassava Leaf Disease Classification</em> competition), the leaves on which we are supposed to identify the disease can be of different sizes, pointing at different angles, and so on, due both to the shapes of the plants and differences in how the images are taken. This means transformations such as vertical or horizontal flips, cropping, and rotations all make sense in this context.</p>
    <p class="normal">By <a id="_idIndexMarker856"/>contrast, we can look at a sample image from the <em class="italic">Severstal: Steel Defect Detection</em> competition (<a href="https://www.kaggle.com/c/severstal-steel-defect-detection"><span class="url">https://www.kaggle.com/c/severstal-steel-defect-detection</span></a>). In this competition, participants had to localize and classify defects on a steel sheet. All the images had the<a id="_idIndexMarker857"/> same size and orientation, which means that rotations or crops would have produced unrealistic images, adding to the noise and having an adverse impact on the generalization capabilities of an algorithm.</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_07.png" alt="Obraz zawierający tekst, półka, zrzut ekranu  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.7: Sample images from the Severstal competition</p>
    <h3 id="_idParaDest-167" class="heading-3">Preprocessing layers</h3>
    <p class="normal">An alternative approach<a id="_idIndexMarker858"/> to data augmentation as a preprocessing step in a native Keras manner is to use the <code class="inlineCode">preprocessing</code> layers API. The functionality is remarkably flexible: these pipelines can be used either in combination<a id="_idIndexMarker859"/> with Keras models or independently, in a manner similar to <code class="inlineCode">ImageDataGenerator</code>.</p>
    <p class="normal">Below we show briefly how a preprocessing layer can be set up. First, the imports:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers.experimental <span class="hljs-keyword">import</span> preprocessing
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers
</code></pre>
    <p class="normal">We load a pretrained model in the standard Keras manner:</p>
    <pre class="programlisting code"><code class="hljs-code">pretrained_base = tf.keras.models.load_model(
    <span class="hljs-string">'../input/cv-course-models/cv-course-models/vgg16-pretrained-base'</span>,
)
pretrained_base.trainable = <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">The preprocessing layers can be used in the same way as other layers are used inside the <code class="inlineCode">Sequential</code> constructor; the only requirement is that they need to be specified before any others, at the beginning of our model definition:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential([
    <span class="hljs-comment"># Preprocessing layers</span>
    preprocessing.RandomFlip(<span class="hljs-string">'horizontal'</span>), <span class="hljs-comment"># Flip left-to-right</span>
    preprocessing.RandomContrast(<span class="hljs-number">0.5</span>), <span class="hljs-comment"># Contrast change by up to 50%</span>
    <span class="hljs-comment"># Base model</span>
    pretrained_base,
    <span class="hljs-comment"># model head definition </span>
    layers.Flatten(),
    layers.Dense(<span class="hljs-number">6</span>, activation=<span class="hljs-string">'relu'</span>),
    layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>),
])
</code></pre>
    <h2 id="_idParaDest-168" class="heading-2">albumentations</h2>
    <p class="normal">The <code class="inlineCode">albumentations</code> package<a id="_idIndexMarker860"/> is a fast image augmentation library that is built as a wrapper of sorts around other libraries. </p>
    <div class="note">
      <p class="normal">The package is the result of intensive coding in quite a few Kaggle competitions (see <a href="https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3"><span class="url">https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3</span></a>), and claims among its core developers and contributors quite a few notable Kagglers, including <em class="italic">Eugene Khvedchenya</em> (<a href="https://www.kaggle.com/bloodaxe"><span class="url">https://www.kaggle.com/bloodaxe</span></a>), <em class="italic">Vladimir Iglovikov</em> (<a href="https://www.kaggle.com/iglovikov"><span class="url">https://www.kaggle.com/iglovikov</span></a>), <em class="italic">Alex Parinov</em> (<a href="https://www.kaggle.com/creafz"><span class="url">https://www.kaggle.com/creafz</span></a>), and <em class="italic">ZFTurbo</em> (<a href="https://www.kaggle.com/zfturbo"><span class="url">https://www.kaggle.com/zfturbo</span></a>). </p>
      <p class="normal">The full documentation can be found at <a href="https://albumentations.readthedocs.io/en/latest/"><span class="url">https://albumentations.readthedocs.io/en/latest/</span></a>.</p>
    </div>
    <p class="normal">Below we list the<a id="_idIndexMarker861"/> important characteristics: </p>
    <ul>
      <li class="bulletList">A unified API for different data types</li>
      <li class="bulletList">Support for all common computer vision tasks</li>
      <li class="bulletList">Integration both with TensorFlow and PyTorch</li>
    </ul>
    <p class="normal">Using <code class="inlineCode">albumentations</code> functionality<a id="_idIndexMarker862"/> to transform an image is straightforward. We begin by initializing the required transformations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> albumentations <span class="hljs-keyword">as</span> A
horizontal_flip = A.HorizontalFlip(p=<span class="hljs-number">1</span>)
rotate = A.ShiftScaleRotate(p=<span class="hljs-number">1</span>)
gaus_noise = A.GaussNoise() 
bright_contrast = A.RandomBrightnessContrast(p=<span class="hljs-number">1</span>) 
gamma = A.RandomGamma(p=<span class="hljs-number">1</span>) 
blur = A.Blur()
</code></pre>
    <p class="normal">Next, we apply the transformations to our reference image:</p>
    <pre class="programlisting code"><code class="hljs-code">img_flip = horizontal_flip(image = curr_img)
img_gaus = gaus_noise(image = curr_img)
img_rotate = rotate(image = curr_img)
img_bc = bright_contrast(image = curr_img)
img_gamma = gamma(image = curr_img)
img_blur = blur(image = curr_img)
</code></pre>
    <p class="normal">We can access the augmented images with the <code class="inlineCode">'image'</code> key and visualize the results:</p>
    <pre class="programlisting code"><code class="hljs-code">img_list = [img_flip[<span class="hljs-string">'image'</span>],img_gaus[<span class="hljs-string">'image'</span>], img_rotate[<span class="hljs-string">'image'</span>],
            img_bc[<span class="hljs-string">'image'</span>], img_gamma[<span class="hljs-string">'image'</span>], img_blur[<span class="hljs-string">'image'</span>]]
plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))
plt.axis(<span class="hljs-string">'off'</span>)
plt.imshow(gallery(np.array(img_list), ncols = <span class="hljs-number">3</span>))
plt.title(<span class="hljs-string">'Augmentation examples'</span>)
</code></pre>
    <p class="normal">Here are our<a id="_idIndexMarker863"/> results:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_08.png" alt="Obraz zawierający roślina, zewnętrzne, ogród, trawa  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.8: Image augmented using the albumentations library</p>
    <p class="normal">Having discussed augmentation as a crucial preprocessing step in approaching a computer vision problem, we are now in a position to apply this knowledge in the following sections, beginning with a very common task: image classification.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Chris_Deotte.png" alt=""/>
      </div>
      <p class="intervieweeName">Chris Deotte</p>
      <p class="normal"><a href="https://www.kaggle.com/cdeotte"><span class="url">https://www.kaggle.com/cdeotte</span></a></p>
      <p class="normal">Before we <a id="_idIndexMarker864"/>proceed, let’s look at a brief conversation we had with Chris Deotte, who we’ve mentioned quite a few times in this book (including earlier in this chapter), and for good reason. He is a quadruple Kaggle Grandmaster and Senior Data Scientist &amp; Researcher at NVIDIA, who joined Kaggle in 2019.</p>

      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">I enjoy competitions with fascinating data and competitions that require building creative novel models. My specialty is analyzing trained models to determine their strengths and weaknesses. Afterward, I enjoy improving the models and/or developing post-processing to boost CV LB. </em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">I begin each competition by performing EDA (exploratory data analysis), creating a local validation, building some simple models, and submitting to Kaggle for leaderboard scores. This fosters an intuition of what needs to be done in order to build an accurate and competitive model.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal">Kaggle’s Shopee – Price Match Guarantee<em class="italic"> was a challenging competition that required both image models and natural language models. A key insight was extracting embeddings from the two types of models and then determining how to use both image and language information together to find product matches.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Yes. Kaggle helped me become a senior data scientist at NVIDIA by improving my skills and boosting my resume’s marketability.</em></p>
      <p class="normal"><em class="italic">Many employers peruse the work on Kaggle to find employees with specific skills to help solve their specific projects. In this way, I have been solicited about many job opportunities.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">In my opinion, inexperienced Kagglers often overlook the importance of local validation. Seeing your name on the leaderboard is exciting. And it’s easy to focus on improving our leaderboard scores instead of our cross-validation scores. </em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">Many times, I have made the mistake of trusting my leaderboard score over my cross-validation score and selecting the wrong final submission.</em></p>

      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">Absolutely. Feature engineering and quick experimentation are important when optimizing tabular data models. In order to accelerate the cycle of experimentation and validation, using NVIDIA RAPIDS cuDF and cuML on GPU are essential.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition? </p>
      <p class="normal"><em class="italic">The most important thing is to have fun and learn. Don’t worry about your final placement. If you focus on learning and having fun, then over time your final placements will become better and better.</em></p>
      <p class="interviewHeader">Do you use other competition platforms? How do they compare to Kaggle?</p>
      <p class="normal"><em class="italic">Yes, I have entered competitions outside of Kaggle. Individual companies like Booking.com or Twitter.com will occasionally host a competition. These competitions are fun and involve high-quality, real-life data.</em></p>
    </div>
    <h1 id="_idParaDest-169" class="heading-1">Classification</h1>
    <p class="normal">In this section, we <a id="_idIndexMarker865"/>will demonstrate an end-to-end pipeline that can be used as a template for handling image classification problems. We will walk through the necessary steps, from data preparation, to model setup and estimation, to results visualization. Apart from being informative (and cool), this last step can also be very useful if you need to examine your code in-depth to get a better understanding of the performance.</p>
    <p class="normal">We will continue using the data from the <em class="italic">Cassava Leaf Disease Classification</em> contest (<a href="https://www.kaggle.com/c/cassava-leaf-disease-classification"><span class="url">https://www.kaggle.com/c/cassava-leaf-disease-classification</span></a>).</p>
    <p class="normal">As usual, we begin by loading the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> models, layers
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> image
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator
<span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
<span class="hljs-keyword">from</span> tensorflow.keras.applications <span class="hljs-keyword">import</span> EfficientNetB0
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">import</span> os, cv2, json
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
</code></pre>
    <p class="normal">It is usually a good idea to define a few helper functions; it makes for code that is easier to both read and debug. If you are approaching a general image classification problem, a good starting <a id="_idIndexMarker866"/>point can be provided by a model from the <strong class="keyWord">EfficientNet</strong> family, introduced in 2019 in a paper from the Google Research Brain Team (<a href="https://arxiv.org/abs/1905.11946"><span class="url">https://arxiv.org/abs/1905.11946</span></a>). The basic idea is to balance network depth, width, and resolution to enable more efficient scaling across all dimensions and subsequently better performance. For our solution, we will use the simplest member of the family, <strong class="keyWord">EfficientNet B0</strong>, which <a id="_idIndexMarker867"/>is a mobile-sized network with 11 million trainable parameters.</p>
    <div class="packt_tip">
      <p class="normal">For a properly detailed explanation of the EfficientNet networks, you are encouraged to explore <a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html"><span class="url">https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html</span></a> as a starting point.</p>
    </div>
    <p class="normal">We construct our model with B0 as the basis, followed by a pooling layer for improved translation invariance and a dense layer with an activation function suitable for our multiclass classification problem:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> CFG:    
    <span class="hljs-comment"># config</span>
    WORK_DIR = <span class="hljs-string">'../input/cassava-leaf-disease-classification'</span>
    BATCH_SIZE = <span class="hljs-number">8</span>
    EPOCHS = <span class="hljs-number">5</span>
    TARGET_SIZE = <span class="hljs-number">512</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">create_model</span>():
    conv_base = EfficientNetB0(include_top = <span class="hljs-literal">False</span>, weights = <span class="hljs-literal">None</span>,
                               input_shape = (CFG.TARGET_SIZE,
                               CFG.TARGET_SIZE, <span class="hljs-number">3</span>))
    model = conv_base.output
    model = layers.GlobalAveragePooling2D()(model)
    model = layers.Dense(<span class="hljs-number">5</span>, activation = <span class="hljs-string">"softmax"</span>)(model)
    model = models.Model(conv_base.<span class="hljs-built_in">input</span>, model)
    model.<span class="hljs-built_in">compile</span>(optimizer = Adam(lr = <span class="hljs-number">0.001</span>),
                  loss = <span class="hljs-string">"sparse_categorical_crossentropy"</span>,
                  metrics = [<span class="hljs-string">"</span><span class="hljs-string">acc"</span>])
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">Some brief remarks on the parameters we pass to the <code class="inlineCode">EfficientNetB0</code> function:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">include_top</code> parameter allows you to decide whether to include the final dense layers. As we want to use the pre-trained model as a feature extractor, a default strategy would be to skip them and then define the head ourselves.</li>
      <li class="bulletList"><code class="inlineCode">weights</code> can be set to <code class="inlineCode">None</code> if we want to train the model from scratch, or to <code class="inlineCode">'imagenet'</code>or <code class="inlineCode">'noisy-student'</code> if we instead prefer to utilize the weights pre-trained on large image collections.</li>
    </ul>
    <p class="normal">The helper function <a id="_idIndexMarker868"/>below allows us to visualize the activation layer, so we can examine the network performance from a visual angle. This is frequently helpful in developing an intuition in a field notorious for its opacity:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">activation_layer_vis</span>(<span class="hljs-params">img, activation_layer = </span><span class="hljs-number">0</span><span class="hljs-params">, layers = </span><span class="hljs-number">10</span>):
    layer_outputs = [layer.output <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.layers[:layers]]
    activation_model = models.Model(inputs = model.<span class="hljs-built_in">input</span>,
                                    outputs = layer_outputs)
    activations = activation_model.predict(img)
    
    rows = <span class="hljs-built_in">int</span>(activations[activation_layer].shape[<span class="hljs-number">3</span>] / <span class="hljs-number">3</span>)
    cols = <span class="hljs-built_in">int</span>(activations[activation_layer].shape[<span class="hljs-number">3</span>] / rows)
    fig, axes = plt.subplots(rows, cols, figsize = (<span class="hljs-number">15</span>, <span class="hljs-number">15</span> * cols))
    axes = axes.flatten()
    
    <span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(activations[activation_layer].shape[<span class="hljs-number">3</span>]), axes):
        ax.matshow(activations[activation_layer][<span class="hljs-number">0</span>, :, :, i],
                   cmap = <span class="hljs-string">'</span><span class="hljs-string">viridis'</span>)
        ax.axis(<span class="hljs-string">'off'</span>)
    plt.tight_layout()
    plt.show()
</code></pre>
    <p class="normal">We generate the activations by creating predictions for a given model based on a “restricted” model, in other words, using the entire architecture up until the penultimate layer; this is the code up to the <code class="inlineCode">activations</code> variable. The rest of the function ensures we show the right layout of activations, corresponding to the shape of the filter in the appropriate convolution layer.</p>
    <p class="normal">Next, we process the labels and set up the validation scheme; there is no special structure in the data (for example, a time dimension or overlap across classes), so we can use a simple random split:</p>
    <pre class="programlisting code"><code class="hljs-code">train_labels = pd.read_csv(os.path.join(CFG.WORK_DIR, <span class="hljs-string">"train.csv"</span>))
STEPS_PER_EPOCH = <span class="hljs-built_in">len</span>(train_labels)*<span class="hljs-number">0.8</span> / CFG.BATCH_SIZE
VALIDATION_STEPS = <span class="hljs-built_in">len</span>(train_labels)*<span class="hljs-number">0.2</span> / CFG.BATCH_SIZE
</code></pre>
    <div class="note">
      <p class="normal">For a refresher on more elaborate validation schemes, refer to <em class="chapterRef">Chapter 6</em>, <em class="italic">Designing Good Validation</em>.</p>
    </div>
    <p class="normal">We are now able to set up the data generators, which are necessary for our TF-based algorithm to loop through the image data.</p>
    <p class="normal">First, we <a id="_idIndexMarker869"/>instantiate two <code class="inlineCode">ImageDataGenerator</code> objects; this is when we incorporate the image augmentations. For the purpose of this demonstration, we will go with the Keras built-in ones. After that, we create the generator using a <code class="inlineCode">flow_from_dataframe()</code> method, which is used to generate batches of tensor image data with real-time data augmentation:</p>
    <pre class="programlisting code"><code class="hljs-code">train_labels.label = train_labels.label.astype(<span class="hljs-string">'str'</span>)
train_datagen = ImageDataGenerator(
    validation_split = <span class="hljs-number">0.2</span>, preprocessing_function = <span class="hljs-literal">None</span>,
        rotation_range = <span class="hljs-number">45</span>, zoom_range = <span class="hljs-number">0.2</span>,
        horizontal_flip = <span class="hljs-literal">True</span>, vertical_flip = <span class="hljs-literal">True</span>,
        fill_mode = <span class="hljs-string">'nearest'</span>, shear_range = <span class="hljs-number">0.1</span>,
        height_shift_range = <span class="hljs-number">0.1</span>, width_shift_range = <span class="hljs-number">0.1</span>)
train_generator = train_datagen.flow_from_dataframe(
    train_labels, 
    directory = os.path.join(CFG.WORK_DIR, <span class="hljs-string">"train_images"</span>),
    subset = <span class="hljs-string">"training"</span>, 
    x_col = <span class="hljs-string">"image_id"</span>,y_col = <span class="hljs-string">"label"</span>, 
    target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),
    batch_size = CFG.BATCH_SIZE, 
    class_mode = <span class="hljs-string">"sparse"</span>)
validation_datagen = ImageDataGenerator(validation_split = <span class="hljs-number">0.2</span>)
validation_generator = validation_datagen.flow_from_dataframe(
        train_labels,
        directory = os.path.join(CFG.WORK_DIR, <span class="hljs-string">"train_images"</span>),
        subset = <span class="hljs-string">"validation"</span>, 
        x_col = <span class="hljs-string">"image_id"</span>,y_col = <span class="hljs-string">"label"</span>, 
        target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),
        batch_size = CFG.BATCH_SIZE, class_mode = <span class="hljs-string">"sparse"</span>)
</code></pre>
    <p class="normal">With the data structures specified, we can create the model:</p>
    <pre class="programlisting code"><code class="hljs-code">model = create_model()
model.summary()
</code></pre>
    <p class="normal">Once our model is created, we can quickly examine a summary. This is mostly useful for sanity checks, because unless you have a photographic memory, chances are you are not going to <a id="_idIndexMarker870"/>remember the layer composition batches of a sophisticated model like EffNetB0. In practice, you can use the summary to check whether the dimensions of output filters are correct or whether the parameter counts (trainable on non-trainable) are in line with expectations. For the sake of compactness, we only demonstrate the first few lines of the output below; inspecting the architecture diagram for B0 will give you an idea of how long the complete output would be.</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "functional_1"
__________________________________________________________________________
Layer (type)                  Output Shape         Param # Connected to
==========================================================================
input_1 (InputLayer)          [(None, 512, 512, 3) 0
__________________________________________________________________________
rescaling (Rescaling)         (None, 512, 512, 3)  0       input_1[0][0]
__________________________________________________________________________
normalization (Normalization) (None, 512, 512, 3)  7       rescaling[0][0]
___________________________________________________________________________
stem_conv_pad (ZeroPadding2D) (None, 513, 513, 3)  0       normalization[0][0]
___________________________________________________________________________
stem_conv (Conv2D)              (None, 256, 256, 32) 864    stem_conv_pad[0][0]
___________________________________________________________________________
stem_bn (BatchNormalization)    (None, 256, 256, 32) 128    stem_conv[0][0]
___________________________________________________________________________
stem_activation (Activation)    (None, 256, 256, 32) 0      stem_bn[0][0]
___________________________________________________________________________
block1a_dwconv (DepthwiseConv2D (None, 256, 256, 32) 288    stem_activation[0][0]
___________________________________________________________________________
block1a_bn (BatchNormalization) (None, 256, 256, 32) 128    block1a_dwconv[0][0]
___________________________________________________________________________
</code></pre>
    <p class="normal">With the above <a id="_idIndexMarker871"/>steps taken care of, we can proceed to fitting the model. In this step, we can also very conveniently define callbacks. The first one is <code class="inlineCode">ModelCheckpoint</code>: </p>
    <pre class="programlisting code"><code class="hljs-code">model_save = ModelCheckpoint(<span class="hljs-string">'./EffNetB0_512_8_best_weights.h5'</span>, 
                             save_best_only = <span class="hljs-literal">True</span>, 
                             save_weights_only = <span class="hljs-literal">True</span>,
                             monitor = <span class="hljs-string">'val_loss'</span>, 
                             mode = <span class="hljs-string">'min'</span>, verbose = <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The checkpoint uses a few parameters worth elaborating on:</p>
    <ul>
      <li class="bulletList">We can preserve the best set of model weights by setting <code class="inlineCode">save_best_only = True</code>.</li>
      <li class="bulletList">We reduce the size of the model by only keeping the weights, instead of the complete set of optimizer state.</li>
      <li class="bulletList">We decide on which model is optimal by locating a minimum for validation loss.</li>
    </ul>
    <p class="normal">Next, we use one of the popular methods for <a id="_idIndexMarker872"/>preventing overfitting, <strong class="keyWord">early stopping</strong>. We monitor the performance of the model on the holdout set and stop the algorithm if the metric stops improving for a given number of epochs, in this case <code class="inlineCode">5</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">early_stop = EarlyStopping(monitor = <span class="hljs-string">'val_loss'</span>, min_delta = <span class="hljs-number">0.001</span>,
                           patience = <span class="hljs-number">5</span>, mode = <span class="hljs-string">'min'</span>,
                           verbose = <span class="hljs-number">1</span>, restore_best_weights = <span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The <code class="inlineCode">ReduceLROnPlateau</code> callback monitors the loss on the holdout set and if no improvement is seen for a <code class="inlineCode">patience</code> number of epochs, the learning rate is reduced, in this case by a factor of 0.3. While not a universal solution, it can frequently help with convergence:</p>
    <pre class="programlisting code"><code class="hljs-code">reduce_lr = ReduceLROnPlateau(monitor = <span class="hljs-string">'val_loss'</span>, factor = <span class="hljs-number">0.3</span>, 
                              patience = <span class="hljs-number">2</span>, min_delta = <span class="hljs-number">0.001</span>, 
                              mode = <span class="hljs-string">'min'</span>, verbose = <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We are now ready to fit the model:</p>
    <pre class="programlisting code"><code class="hljs-code">history = model.fit(
    train_generator,
    steps_per_epoch = STEPS_PER_EPOCH,
    epochs = CFG.EPOCHS,
    validation_data = validation_generator,
    validation_steps = VALIDATION_STEPS,
    callbacks = [model_save, early_stop, reduce_lr]
)
</code></pre>
    <p class="normal">We will briefly explain the<a id="_idIndexMarker873"/> two parameters we have not encountered before:</p>
    <ul>
      <li class="bulletList">The training generator yields <code class="inlineCode">steps_per_epoch</code> batches per training epoch.</li>
      <li class="bulletList">When the epoch is finished, the validation generator produces <code class="inlineCode">validation_steps</code> batches.</li>
    </ul>
    <p class="normal">An example output after calling <code class="inlineCode">model.fit()</code> is given below:</p>
    <pre class="programlisting con"><code class="hljs-con">Epoch 00001: val_loss improved from inf to 0.57514, saving model to ./EffNetB0_512_8_best_weights.h5
</code></pre>
    <p class="normal">Once a model is fitted, we can examine the activations on a sample image using the helper function we wrote at the start. While this is not necessary for successful model execution, it can help determine what sort of features our model is extracting before applying the classification layer at the top:</p>
    <pre class="programlisting code"><code class="hljs-code">activation_layer_vis(img_tensor, <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here is what we might see:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_09.png" alt="Obraz zawierający tekst, warzywo, kolorowy  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.9: Sample activations from a fitted model</p>
    <p class="normal">We can generate <a id="_idIndexMarker874"/>the predictions with <code class="inlineCode">model.predict()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">ss = pd.read_csv(os.path.join(CFG.WORK_DIR, <span class="hljs-string">"sample_submission.csv"</span>))
preds = []
<span class="hljs-keyword">for</span> image_id <span class="hljs-keyword">in</span> ss.image_id:
    image = Image.<span class="hljs-built_in">open</span>(os.path.join(CFG.WORK_DIR,  <span class="hljs-string">"test_images"</span>,
                                    image_id))
    image = image.resize((CFG.TARGET_SIZE, CFG.TARGET_SIZE))
    image = np.expand_dims(image, axis = <span class="hljs-number">0</span>)
    preds.append(np.argmax(model.predict(image)))
ss[<span class="hljs-string">'label'</span>] = preds
</code></pre>
    <p class="normal">We build the predictions by iterating through the list of images. For each of them, we reshape the image to the required dimensions and pick the channel with the strongest signal (the model predicts class probabilities, of which we pick the largest one with <code class="inlineCode">argmax</code>). The final predictions are class numbers, in line with the metric utilized in the competition.</p>
    <p class="normal">We have now demonstrated a minimal end-to-end pipeline for image classification. Numerous improvements are, of course, possible – for instance, more augmentations, bigger architecture, callback customization – but the basic underlying template should provide you with a good starting point going forward.</p>
    <p class="normal">We move on now to a second popular problem in computer vision: object detection.</p>
    <h1 id="_idParaDest-170" class="heading-1">Object detection</h1>
    <p class="normal"><strong class="keyWord">Object detection</strong> is <a id="_idIndexMarker875"/>a computer vision/image processing task where we need to identify instances of semantic objects of a certain class in an image or video. In classification problems like those discussed in the previous section, we simply need to assign a class to each image, whereas in object detection tasks, we want to draw a <strong class="keyWord">bounding box</strong> around <a id="_idIndexMarker876"/>an object of interest to locate it within an image.</p>
    <p class="normal">In this section, we<a id="_idIndexMarker877"/> will use data from the <em class="italic">Global Wheat Detection</em> competition (<a href="https://www.kaggle.com/c/global-wheat-detection"><span class="url">https://www.kaggle.com/c/global-wheat-detection</span></a>). In this competition, participants had to detect wheat heads, which are spikes atop plants containing grain. Detection of these in plant images is used to estimate the size and density of wheat heads across crop varieties. We will demonstrate how to train a model for solving this using <strong class="keyWord">Yolov5</strong>, a <a id="_idIndexMarker878"/>well-established model in object detection, and state-of-the-art until late 2021 when it was (based on preliminary results) surpassed by the YoloX architecture. Yolov5 gave rise to extremely competitive results in the competition and although it was eventually disallowed by the organizers due to licensing issues, it is very well suited for the purpose of this demonstration. </p>
    <figure class="mediaobject"><img src="../Images/B17574_10_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.10: Sample image visualizations of detected wheat heads</p>
    <p class="normal">An important point worth mentioning before we begin is the different formats for bounding box annotations; there are different (but mathematically equivalent) ways of describing the coordinates of a rectangle. </p>
    <p class="normal">The most common types are coco, voc-pascal, and yolo. The differences between them are clear from the figure below:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.11: Annotation formats for bounding boxes</p>
    <p class="normal">One more part <a id="_idIndexMarker879"/>we need to define is the grid structure: Yolo detects objects by placing a grid over an image and checking for the presence of an object of interest (wheat head, in our case) in any of the cells. The bounding boxes are reshaped to be offset within the relevant cells of the image and the <em class="italic">(x, y, w, h)</em> parameters are scaled to the unit interval:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.12: Yolo annotation positioning</p>
    <p class="normal">We start by <a id="_idIndexMarker880"/>loading the annotations for our training data:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.read_csv(<span class="hljs-string">'../input/global-wheat-detection/train.csv'</span>)
df.head(<span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">Let’s inspect a few:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_13.png" alt="Obraz zawierający stół  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.13: Training data with annotations</p>
    <p class="normal">We extract the actual coordinates of the bounding boxes from the <code class="inlineCode">bbox</code> column:</p>
    <pre class="programlisting code"><code class="hljs-code">bboxs = np.stack(df[<span class="hljs-string">'bbox'</span>].apply(<span class="hljs-keyword">lambda</span> x: np.fromstring(x[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>],
                                  sep=<span class="hljs-string">','</span>)))
bboxs
</code></pre>
    <p class="normal">Let’s look at the array:</p>
    <pre class="programlisting con"><code class="hljs-con">array([[834., 222.,  56.,  36.],
       [226., 548., 130.,  58.],
       [377., 504.,  74., 160.],
       ...,
       [134., 228., 141.,  71.],
       [430.,  13., 184.,  79.],
       [875., 740.,  94.,  61.]])
</code></pre>
    <p class="normal">The next step is to extract the coordinates in Yolo format into separate columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i, column <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>([<span class="hljs-string">'x'</span>, <span class="hljs-string">'y'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'h'</span>]):
    df[column] = bboxs[:,i]
df.drop(columns=[<span class="hljs-string">'bbox'</span>], inplace=<span class="hljs-literal">True</span>)
df[<span class="hljs-string">'x_center'</span>] = df[<span class="hljs-string">'x'</span>] + df[<span class="hljs-string">'</span><span class="hljs-string">w'</span>]/<span class="hljs-number">2</span>
df[<span class="hljs-string">'y_center'</span>] = df[<span class="hljs-string">'y'</span>] + df[<span class="hljs-string">'h'</span>]/<span class="hljs-number">2</span>
df[<span class="hljs-string">'classes'</span>] = <span class="hljs-number">0</span>
df = df[[<span class="hljs-string">'image_id'</span>,<span class="hljs-string">'x'</span>, <span class="hljs-string">'</span><span class="hljs-string">y'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'h'</span>,<span class="hljs-string">'x_center'</span>,<span class="hljs-string">'y_center'</span>,<span class="hljs-string">'classes'</span>]]
df.head(<span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">The implementation <a id="_idIndexMarker881"/>from Ultralytics has some requirements on the structure of the dataset, specifically, where the annotations are stored and the folders for training/validation data. </p>
    <p class="normal">The creation of the folders in the code below is fairly straightforward, but a more inquisitive reader is encouraged to consult the official documentation (<a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"><span class="url">https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data</span></a>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># stratify on source</span>
source = <span class="hljs-string">'train'</span>
<span class="hljs-comment"># Pick a single fold for demonstration's sake</span>
fold = <span class="hljs-number">0</span> 
val_index = <span class="hljs-built_in">set</span>(df[df[<span class="hljs-string">'fold'</span>] == fold][<span class="hljs-string">'image_id'</span>])
<span class="hljs-comment"># Loop through the bounding boxes per image</span>
<span class="hljs-keyword">for</span> name,mini <span class="hljs-keyword">in</span> tqdm(df.groupby(<span class="hljs-string">'image_id'</span>)):
    <span class="hljs-comment"># Where to save the files</span>
    <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> val_index:
        path2save = <span class="hljs-string">'valid/'</span>
    <span class="hljs-keyword">else</span>:
        path2save = <span class="hljs-string">'train/'</span>   
    <span class="hljs-comment"># Storage path for labels</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">'convertor/fold{}/labels/'</span>.
                          <span class="hljs-built_in">format</span>(fold)+path2save):
        os.makedirs(<span class="hljs-string">'</span><span class="hljs-string">convertor/fold{}/labels/'</span>.<span class="hljs-built_in">format</span>(fold)+path2save)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'convertor/fold{}/labels/'</span>.<span class="hljs-built_in">format</span>(fold)+path2save+name+<span class="hljs-string">".</span>
<span class="hljs-string">              txt"</span>, <span class="hljs-string">'w+'</span>) <span class="hljs-keyword">as</span> f:
   <span class="hljs-comment"># Normalize the coordinates in accordance with the Yolo format requirements</span>
        row = mini[[<span class="hljs-string">'</span><span class="hljs-string">classes'</span>,<span class="hljs-string">'x_center'</span>,<span class="hljs-string">'y_center'</span>,<span class="hljs-string">'w'</span>,<span class="hljs-string">'h'</span>]].
        astype(<span class="hljs-built_in">float</span>).values
        row = row/<span class="hljs-number">1024</span>
        row = row.astype(<span class="hljs-built_in">str</span>)
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(row)):
            text = <span class="hljs-string">' '</span>.join(row[j])
            f.write(text)
            f.write(<span class="hljs-string">"\n"</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">'convertor/fold{}/images/{}'</span>.
                          <span class="hljs-built_in">format</span>(fold,path2save)):
        os.makedirs(<span class="hljs-string">'convertor/fold{}/images/{}'</span>.<span class="hljs-built_in">format</span>(fold,path2save))
    <span class="hljs-comment"># No preprocessing needed for images =&gt; copy them as a batch</span>
    sh.copy(<span class="hljs-string">"../input/global-wheat-detection/{}/{}.jpg"</span>.
            <span class="hljs-built_in">format</span>(source,name),
            <span class="hljs-string">'convertor/fold{}/images/{}/{}.jpg'</span>.
            <span class="hljs-built_in">format</span>(fold,path2save,name))
</code></pre>
    <p class="normal">The next thing <a id="_idIndexMarker882"/>we do is install the Yolo package itself. If you are running this in a Kaggle Notebook or Colab, make sure to double-check GPU is enabled; Yolo installation will actually work without it, but you are likely to run into all sorts of timeouts and memory issues due to CPU versus GPU performance differences.</p>
    <pre class="programlisting code"><code class="hljs-code">!git clone https://github.com/ultralytics/yolov5  &amp;&amp; cd yolov5 &amp;&amp;
pip install -r requirements.txt  
</code></pre>
    <p class="normal">We omit the output, as it is rather extensive. The last bit of preparation needed is the YAML configuration file, where we specify the training and validation data locations and the number of classes. We are only interested in detecting wheat heads and not distinguishing between different types, so we have one class (its name is only provided for notational consistency and can be an arbitrary string in this instance):</p>
    <pre class="programlisting code"><code class="hljs-code">yaml_text = <span class="hljs-string">"""train: /kaggle/working/convertor/fold0/images/train/</span>
<span class="hljs-string">            val: /kaggle/working/convertor/fold0/images/valid/</span>
<span class="hljs-string">            nc: 1</span>
<span class="hljs-string">            names: ['wheat']"""</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"wheat.yaml"</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
    f.write(yaml_text)
%cat wheat.yaml
</code></pre>
    <p class="normal">With that, we can start training our model:</p>
    <pre class="programlisting code"><code class="hljs-code">!python ./yolov5/train.py --img <span class="hljs-number">512</span> --batch <span class="hljs-number">2</span> --epochs <span class="hljs-number">3</span> --workers <span class="hljs-number">2</span> --data wheat.yaml --cfg <span class="hljs-string">"./yolov5/models/yolov5s.yaml"</span> --name yolov5x_fold0 --cache
</code></pre>
    <p class="normal">Unless you are used to launching things from the command line, the incantation above is positively cryptic, so let’s discuss its composition in some detail:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">train.py</code> is the workhorse script for training a YoloV5 model, starting from pre-trained weights.</li>
      <li class="bulletList"><code class="inlineCode">--img 512</code> means we want the original images (which, as you can see, we did not preprocess in any way) to be rescaled to 512x512. For a competitive result, you should use a higher resolution, but this code was executed in a Kaggle Notebook, which has certain limitations on resources.</li>
      <li class="bulletList"><code class="inlineCode">--batch</code> refers to the batch size in the training process.</li>
      <li class="bulletList"><code class="inlineCode">--epochs 3</code> means we want to train the model for three epochs.</li>
      <li class="bulletList"><code class="inlineCode">--workers 2</code> specifies the number of workers in the data loader. Increasing this <a id="_idIndexMarker883"/>number might help performance, but there is a known bug in version 6.0 (the most recent one available in the Kaggle Docker image, as of the time of this writing) when the number of workers is too high, even on a machine where more might be available.</li>
      <li class="bulletList"><code class="inlineCode">--data wheat.yaml</code> is the file pointing to our data specification YAML file, defined above.</li>
      <li class="bulletList"><code class="inlineCode">--cfg "./yolov5/models/yolov5s.yaml"</code> specifies the model architecture and the corresponding set of weights to be used for initialization. You can use the ones provided with the installation (check the official documentation for details), or you can customize your own and keep them in the same <code class="inlineCode">.yaml</code> format.</li>
      <li class="bulletList"><code class="inlineCode">--name</code> specifies where the resulting model is to be stored.</li>
    </ul>
    <p class="normal">We break down the output of the training command below. First, the groundwork:</p>
    <pre class="programlisting con"><code class="hljs-con">Downloading the pretrained weights, setting up Weights&amp;Biases https://wandb.ai/site integration, GitHub sanity check.
Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...
wandb: (1) Create a W&amp;B account
wandb: (2) Use an existing W&amp;B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: (30 second timeout) 
wandb: W&amp;B disabled due to login timeout.
train: weights=yolov5/yolov5s.pt, cfg=./yolov5/models/yolov5s.yaml, data=wheat.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=2, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=yolov5/runs/train, name=yolov5x_fold0, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest
github: up to date with https://github.com/ultralytics/yolov5 <img src="../Images/B17574_10_002.png" alt=""/>
YOLOv5 <img src="../Images/B17574_10_003.png" alt=""/> v6.1-76-gc94736a torch 1.9.1 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)
hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
Weights &amp; Biases: run 'pip install wandb' to automatically track and visualize YOLOv5 <img src="../Images/B17574_10_003.png" alt=""/> runs (RECOMMENDED)
TensorBoard: Start with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/
Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5/yolov5s.pt...
100%|██████████████████████████████████████| 14.1M/14.1M [00:00&lt;00:00, 40.7MB/s]
</code></pre>
    <p class="normal">Then comes<a id="_idIndexMarker884"/> the model. We see a summary of the architecture, the optimizer setup, and the augmentations used:</p>
    <pre class="programlisting con"><code class="hljs-con">Overriding model.yaml nc=80 with nc=1
                 from  n    params  module                                  arguments
  0                -1  1    3520  models.common.Conv                      [3, 32, 6, 2, 2]
  1                -1  1    18560  models.common.Conv                      [32, 64, 3, 2]
  2                -1  1    18816  models.common.C3                        [64, 64, 1]
  3                -1  1    73984  models.common.Conv                      [64, 128, 3, 2]
  4                -1  2    115712  models.common.C3                        [128, 128, 2]
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  6                -1  3    625152  models.common.C3                        [256, 256, 3]
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]
 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
YOLOv5s summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.8 GFLOPs
Transferred 342/349 items from yolov5/yolov5s.pt
Scaled weight_decay = 0.0005
optimizer: SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias
albumentations: Blur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))
train: Scanning '/kaggle/working/convertor/fold0/labels/train' images and labels
train: New cache created: /kaggle/working/convertor/fold0/labels/train.cache
train: Caching images (0.0GB ram): 100%|██████████| 51/51 [00:00&lt;00:00, 76.00it/
val: Scanning '/kaggle/working/convertor/fold0/labels/valid' images and labels..
val: New cache created: /kaggle/working/convertor/fold0/labels/valid.cache
val: Caching images (2.6GB ram): 100%|██████████| 3322/3322 [00:47&lt;00:00, 70.51i
Plotting labels to yolov5/runs/train/yolov5x_fold0/labels.jpg... 
AutoAnchor: 6.00 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset <img src="../Images/B17574_10_002.png" alt=""/>
Image sizes 512 train, 512 val
Using 2 dataloader workers
</code></pre>
    <p class="normal">This is <a id="_idIndexMarker885"/>followed by the actual training log:</p>
    <pre class="programlisting con"><code class="hljs-con">Starting training for 3 epochs...
     Epoch   gpu_mem       box       obj       cls    labels  img_size
       0/2    0.371G    0.1196   0.05478         0        14       512: 100%|███
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@
                 all       3322     147409    0.00774     0.0523    0.00437   0.000952
     Epoch   gpu_mem       box       obj       cls    labels  img_size
       1/2    0.474G    0.1176   0.05625         0         5       512: 100%|███
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@
                 all       3322     147409    0.00914     0.0618    0.00493    0.00108
     Epoch   gpu_mem       box       obj       cls    labels  img_size
       2/2    0.474G    0.1146   0.06308         0        12       512: 100%|███
               Class     Images     Labels          P          R     mAP@.5 mAP@
                 all       3322     147409    0.00997     0.0674    0.00558    0.00123
3 epochs completed in 0.073 hours.
Optimizer stripped from yolov5/runs/train/yolov5x_fold0/weights/last.pt, 14.4MB
Optimizer stripped from yolov5/runs/train/yolov5x_fold0/weights/best.pt, 14.4MB
Validating yolov5/runs/train/yolov5x_fold0/weights/best.pt...
Fusing layers... 
YOLOv5s summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded
               Class     Images     Labels          P          R     mAP@.5 mAP@
                 all       3322     147409    0.00997     0.0673    0.00556    0.00122
Results saved to yolov5/runs/train/yolov5x_fold0
</code></pre>
    <p class="normal">The results from <a id="_idIndexMarker886"/>both training and validation stages can be examined; they are stored in the <code class="inlineCode">yolov5</code> folder under <code class="inlineCode">./yolov5/runs/train/yolov5x_fold0</code>:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.14: Validation data with annotations</p>
    <p class="normal">Once we have trained the model, we can use the weights from the best performing model (Yolov5 has a neat functionality of automatically keeping both the best and last epoch model, storing them as <code class="inlineCode">best.pt</code> and <code class="inlineCode">last.pt</code>) to generate predictions on the test data:</p>
    <pre class="programlisting code"><code class="hljs-code">!python ./yolov5/detect.py --weights ./yolov5/runs/train/yolov5x_fold0/weights/best.pt --img <span class="hljs-number">512</span> --conf <span class="hljs-number">0.1</span> --source /kaggle/input/global-wheat-detection/test --save-txt --save-conf --exist-ok
</code></pre>
    <p class="normal">We will discuss the <a id="_idIndexMarker887"/>parameters that are specific to the inference stage:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">--weights</code> points to the location of the best weights from our model trained above.</li>
      <li class="bulletList"><code class="inlineCode">--conf 0.1</code> specifies which candidate bounding boxes generated by the model should be kept. As usual, it is a compromise between precision and recall (too low a threshold gives a high number of false positives, while moving the threshold too high means we might not find any wheat heads at all).</li>
      <li class="bulletList"><code class="inlineCode">--source</code> is the location of the test data.</li>
    </ul>
    <p class="normal">The labels created for our test images can be inspected locally: </p>
    <pre class="programlisting code"><code class="hljs-code">!ls ./yolov5/runs/detect/exp/labels/
</code></pre>
    <p class="normal">This is what we might see:</p>
    <pre class="programlisting con"><code class="hljs-con">2fd875eaa.txt  53f253011.txt  aac893a91.txt  f5a1f0358.txt
348a992bb.txt  796707dd7.txt  cc3532ff6.txt
</code></pre>
    <p class="normal">Let’s look at an individual prediction: </p>
    <pre class="programlisting code"><code class="hljs-code">!cat 2fd875eaa.txt
</code></pre>
    <p class="normal">It has the following format:</p>
    <pre class="programlisting con"><code class="hljs-con">0 0.527832 0.580566 0.202148 0.838867 0.101574
0 0.894531 0.587891 0.210938 0.316406 0.113519
</code></pre>
    <p class="normal">This means that in image <code class="inlineCode">2fd875eaa</code>, our trained model detected two bounding boxes (their coordinates are entries 2-5 in the row), with confidence scores above 0.1 given at the end of the row.</p>
    <p class="normal">How do we go about combining the predictions into a submission in the required format? We start by defining a helper function that helps us convert the coordinates from the yolo format to coco (as required in this competition): it is a matter of simple rearrangement of the order and normalizing to the original range of values by multiplying the fractions by the image size:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">convert</span>(<span class="hljs-params">s</span>):
    x = <span class="hljs-built_in">int</span>(<span class="hljs-number">1024</span> * (s[<span class="hljs-number">1</span>] - s[<span class="hljs-number">3</span>]/<span class="hljs-number">2</span>))
    y = <span class="hljs-built_in">int</span>(<span class="hljs-number">1024</span> * (s[<span class="hljs-number">2</span>] - s[<span class="hljs-number">4</span>]/<span class="hljs-number">2</span>))
    w = <span class="hljs-built_in">int</span>(<span class="hljs-number">1024</span> * s[<span class="hljs-number">3</span>])
    h = <span class="hljs-built_in">int</span>(<span class="hljs-number">1024</span> * s[<span class="hljs-number">4</span>])
    
    <span class="hljs-keyword">return</span>(<span class="hljs-built_in">str</span>(s[<span class="hljs-number">5</span>]) + <span class="hljs-string">' '</span> + <span class="hljs-built_in">str</span>(x) + <span class="hljs-string">' '</span> + <span class="hljs-built_in">str</span>(y) + <span class="hljs-string">' '</span> + <span class="hljs-built_in">str</span>(w)
           + <span class="hljs-string">' '</span> + <span class="hljs-built_in">str</span>(h))
</code></pre>
    <p class="normal">We then proceed<a id="_idIndexMarker888"/> to generate a submission file: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">We loop over the files listed above.</li>
      <li class="numberedList">For each file, all rows are converted into strings in the required format (one row represents one bounding box detected).</li>
      <li class="numberedList">The rows are then concatenated into a single string corresponding to this file.</li>
    </ol>
    <p class="normal">The code is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'</span><span class="hljs-string">submission.csv'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> myfile:
    <span class="hljs-comment"># Prepare submission</span>
    wfolder = <span class="hljs-string">'./yolov5/runs/detect/exp/labels/'</span>
    <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.listdir(wfolder):
        fname = wfolder + f
        xdat = pd.read_csv(fname, sep = <span class="hljs-string">' '</span>, header = <span class="hljs-literal">None</span>)
        outline = f[:-<span class="hljs-number">4</span>] + <span class="hljs-string">'</span><span class="hljs-string"> '</span> + <span class="hljs-string">' '</span>.join(<span class="hljs-built_in">list</span>(xdat.apply(<span class="hljs-keyword">lambda</span> s:
                                     convert(s), axis = <span class="hljs-number">1</span>)))
        myfile.write(outline + <span class="hljs-string">'\n'</span>)
        
myfile.close()
</code></pre>
    <p class="normal">Let’s see what it looks like:</p>
    <pre class="programlisting code"><code class="hljs-code">!cat submission.csv
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">53f253011 0.100472 61 669 961 57 0.106223 0 125 234 183 0.1082 96 696 928 126 0.108863 515 393 86 161 0.11459 31 0 167 209 0.120246 517 466 89 147
aac893a91 0.108037 376 435 325 188
796707dd7 0.235373 684 128 234 113
cc3532ff6 0.100443 406 752 144 108 0.102479 405 87 4 89 0.107173 576 537 138 94 0.113459 256 498 179 211 0.114847 836 618 186 65 0.121121 154 544 248 115 0.125105 40 567 483 199
2fd875eaa 0.101398 439 163 204 860 0.112546 807 440 216 323
348a992bb 0.100572 0 10 440 298 0.101236 344 445 401 211
f5a1f0358 0.102549 398 424 295 96
</code></pre>
    <p class="normal">The generated <code class="inlineCode">submission.csv</code> file completes our pipeline.</p>
    <p class="normal">In this section, we <a id="_idIndexMarker889"/>have demonstrated how to use a YoloV5 to solve the problem of object detection: how to handle annotations in different formats, how to customize a model for a specific task, train it, and evaluate the results. </p>
    <p class="normal">Based on this knowledge, you should be able to start working with object detection problems.</p>
    <p class="normal">We now move on to the third popular class of computer vision tasks: semantic segmentation.</p>
    <h1 id="_idParaDest-171" class="heading-1">Semantic segmentation</h1>
    <p class="normal">The <a id="_idIndexMarker890"/>easiest way to think about <strong class="keyWord">segmentation</strong> is that it classifies each pixel in an image, assigning it to a corresponding class; combined, those pixels form areas of interest, such as regions with disease on an organ in medical images. By contrast, object detection (discussed in the previous section) classifies patches of an image into different object classes and creates bounding boxes around them.</p>
    <p class="normal">We will demonstrate the modeling approach using data from the <em class="italic">Sartorius – Cell Instance Segmentation</em> competition (<a href="https://www.kaggle.com/c/sartorius-cell-instance-segmentation"><span class="url">https://www.kaggle.com/c/sartorius-cell-instance-segmentation</span></a>). In this one, the participants were tasked to train models for instance segmentation of neural cells using a set of microscopy images.</p>
    <p class="normal">Our solution will be built <a id="_idIndexMarker891"/>around <strong class="keyWord">Detectron2</strong>, a library created by Facebook AI Research that supports multiple detection and segmentation algorithms.</p>
    <div class="note">
      <p class="normal">Detectron2 is a successor to the original Detectron library (<a href="https://github.com/facebookresearch/Detectron/"><span class="url">https://github.com/facebookresearch/Detectron/</span></a>) and the Mask R-CNN project (<a href="https://github.com/facebookresearch/maskrcnn-benchmark/"><span class="url">https://github.com/facebookresearch/maskrcnn-benchmark/</span></a>).</p>
    </div>
    <p class="normal">We begin by installing the extra packages: </p>
    <pre class="programlisting code"><code class="hljs-code">!pip install pycocotools
!pip install <span class="hljs-string">'git+https://github.com/facebookresearch/detectron2.git'</span>
</code></pre>
    <p class="normal">We install <code class="inlineCode">pycocotools</code> (<a href="https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools"><span class="url">https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools</span></a>), which we will need to format the annotations, and Detectron2 (<a href="https://github.com/facebookresearch/detectron2"><span class="url">https://github.com/facebookresearch/detectron2</span></a>), our workhorse in this task.</p>
    <p class="normal">Before we can<a id="_idIndexMarker892"/> train our model, we need a bit of preparation: the annotations <a id="_idIndexMarker893"/>need to be converted from the <strong class="keyWord">run-length encoding</strong> (<strong class="keyWord">RLE</strong>) format provided by the organizers to the COCO format required as input for Detectron2. The basic idea behind RLE is saving space: creating a segmentation means marking a group of pixels in a certain manner. Since an image can be thought of as an array, this area can be denoted by a series of straight lines (row- or column-wise). </p>
    <p class="normal">You can encode each of those lines by listing the indices, or by specifying a starting position and the length of the subsequent contiguous block. A visual example is given below:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_15.png" alt="Obraz zawierający stół  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.15: Visual representation of RLE</p>
    <p class="normal">Microsoft’s <strong class="keyWord">Common Objects in Context</strong> (<strong class="keyWord">COCO</strong>) format<a id="_idIndexMarker894"/> is a specific JSON structure dictating how labels and metadata are saved for an image dataset. Below, we demonstrate <a id="_idIndexMarker895"/>how to convert RLE to COCO<a id="_idIndexMarker896"/> and combine it with a <em class="italic">k</em>-fold validation split, so we get the required train/validation pair of JSON files for each fold.</p>
    <p class="normal">Let’s begin:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># from pycocotools.coco import COCO</span>
<span class="hljs-keyword">import</span> skimage.io <span class="hljs-keyword">as</span> io
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> json,itertools
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GroupKFold
<span class="hljs-comment"># Config</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">CFG</span>:
    data_path = <span class="hljs-string">'../input/sartorius-cell-instance-segmentation/'</span>
    nfolds = <span class="hljs-number">5</span>
</code></pre>
    <p class="normal">We need three functions to go from RLE to COCO. First, we need to convert from RLE to a binary mask:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># From https://www.kaggle.com/stainsby/fast-tested-rle</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">rle_decode</span>(<span class="hljs-params">mask_rle, shape</span>):
    <span class="hljs-string">'''</span>
<span class="hljs-string">    mask_rle: run-length as string formatted (start length)</span>
<span class="hljs-string">    shape: (height,width) of array to return </span>
<span class="hljs-string">    Returns numpy array, 1 - mask, 0 - background</span>
<span class="hljs-string">    '''</span>
    s = mask_rle.split()
    starts, lengths = [np.asarray(x, dtype=<span class="hljs-built_in">int</span>)
                       <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> (s[<span class="hljs-number">0</span>:][::<span class="hljs-number">2</span>], s[<span class="hljs-number">1</span>:][::<span class="hljs-number">2</span>])]
    starts -= <span class="hljs-number">1</span>
    ends = starts + lengths
    img = np.zeros(shape[<span class="hljs-number">0</span>]*shape[<span class="hljs-number">1</span>], dtype=np.uint8)
    <span class="hljs-keyword">for</span> lo, hi <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(starts, ends):
        img[lo:hi] = <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> img.reshape(shape)  <span class="hljs-comment"># Needed to align to RLE direction</span>
</code></pre>
    <p class="normal">The second one<a id="_idIndexMarker897"/> converts a binary mask to RLE:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># From https://newbedev.com/encode-numpy-array-using-uncompressed-rle-for-</span>
<span class="hljs-comment"># coco-dataset</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">binary_mask_to_rle</span>(<span class="hljs-params">binary_mask</span>):
    rle = {<span class="hljs-string">'counts'</span>: [], <span class="hljs-string">'size'</span>: <span class="hljs-built_in">list</span>(binary_mask.shape)}
    counts = rle.get(<span class="hljs-string">'</span><span class="hljs-string">counts'</span>)
    <span class="hljs-keyword">for</span> i, (value, elements) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(
            itertools.groupby(binary_mask.ravel(order=<span class="hljs-string">'F'</span>))):
        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> value == <span class="hljs-number">1</span>:
            counts.append(<span class="hljs-number">0</span>)
        counts.append(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(elements)))
    <span class="hljs-keyword">return</span> rle
</code></pre>
    <p class="normal">Finally, we <a id="_idIndexMarker898"/>combine the two in order to produce the COCO output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">coco_structure</span>(<span class="hljs-params">train_df</span>):
    cat_ids = {name: <span class="hljs-built_in">id</span>+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span>, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(
        train_df.cell_type.unique())}
    cats = [{<span class="hljs-string">'name'</span>: name, <span class="hljs-string">'id'</span>: <span class="hljs-built_in">id</span>} <span class="hljs-keyword">for</span> name, <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> cat_ids.items()]
    images = [{<span class="hljs-string">'id'</span>: <span class="hljs-built_in">id</span>, <span class="hljs-string">'width'</span>: row.width, <span class="hljs-string">'height'</span>: row.height,
               <span class="hljs-string">'file_name'</span>:<span class="hljs-string">f'train/</span><span class="hljs-subst">{</span><span class="hljs-built_in">id</span><span class="hljs-subst">}</span><span class="hljs-string">.png'</span>} <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span>,
               row <span class="hljs-keyword">in</span> train_df.groupby(<span class="hljs-string">'id'</span>).agg(<span class="hljs-string">'first'</span>).iterrows()]
    annotations = []
    <span class="hljs-keyword">for</span> idx, row <span class="hljs-keyword">in</span> tqdm(train_df.iterrows()):
        mk = rle_decode(row.annotation, (row.height, row.width))
        ys, xs = np.where(mk)
        x1, x2 = <span class="hljs-built_in">min</span>(xs), <span class="hljs-built_in">max</span>(xs)
        y1, y2 = <span class="hljs-built_in">min</span>(ys), <span class="hljs-built_in">max</span>(ys)
        enc =binary_mask_to_rle(mk)
        seg = {
            <span class="hljs-string">'segmentation'</span>:enc, 
            <span class="hljs-string">'bbox'</span>: [<span class="hljs-built_in">int</span>(x1), <span class="hljs-built_in">int</span>(y1), <span class="hljs-built_in">int</span>(x2-x1+<span class="hljs-number">1</span>), <span class="hljs-built_in">int</span>(y2-y1+<span class="hljs-number">1</span>)],
            <span class="hljs-string">'area'</span>: <span class="hljs-built_in">int</span>(np.<span class="hljs-built_in">sum</span>(mk)),
            <span class="hljs-string">'image_id'</span>:row.<span class="hljs-built_in">id</span>, 
            <span class="hljs-string">'category_id'</span>:cat_ids[row.cell_type], 
            <span class="hljs-string">'iscrowd'</span>:<span class="hljs-number">0</span>, 
            <span class="hljs-string">'id'</span>:idx
        }
        annotations.append(seg)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">'categories'</span>:cats, <span class="hljs-string">'images'</span>:images,<span class="hljs-string">'annotations'</span>:annotations}
</code></pre>
    <p class="normal">We split <a id="_idIndexMarker899"/>our data into non-overlapping folds: </p>
    <pre class="programlisting code"><code class="hljs-code">train_df = pd.read_csv(CFG.data_path + <span class="hljs-string">'</span><span class="hljs-string">train.csv'</span>)
gkf = GroupKFold(n_splits = CFG.nfolds)
train_df[<span class="hljs-string">"fold"</span>] = -<span class="hljs-number">1</span>
y = train_df.width.values
<span class="hljs-keyword">for</span> f, (t_, v_) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(gkf.split(X=train_df, y=y,
                             groups=train_df.<span class="hljs-built_in">id</span>.values)):
    train_df.loc[v_, <span class="hljs-string">"fold"</span>] = f
    
fold_id = train_df.fold.copy()
</code></pre>
    <p class="normal">We can now loop over the folds:</p>
    <pre class="programlisting code"><code class="hljs-code">all_ids = train_df.<span class="hljs-built_in">id</span>.unique()
<span class="hljs-comment"># For fold in range(CFG.nfolds):</span>
<span class="hljs-keyword">for</span> fold <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>):    
    train_sample = train_df.loc[fold_id != fold]
    root = coco_structure(train_sample)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'annotations_train_f' </span>+ <span class="hljs-built_in">str</span>(fold) + 
              <span class="hljs-string">'</span><span class="hljs-string">.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:
        json.dump(root, f, ensure_ascii=<span class="hljs-literal">True</span>, indent=<span class="hljs-number">4</span>)
        
    valid_sample = train_df.loc[fold_id == fold]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'fold '</span> + <span class="hljs-built_in">str</span>(fold) + <span class="hljs-string">': produced'</span>)
<span class="hljs-keyword">for</span> fold <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>):    
    train_sample = train_df.loc[fold_id == fold]
    root = coco_structure(train_sample)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'annotations_valid_f' </span>+ <span class="hljs-built_in">str</span>(fold) + 
              <span class="hljs-string">'.json'</span>, <span class="hljs-string">'</span><span class="hljs-string">w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:
        json.dump(root, f, ensure_ascii=<span class="hljs-literal">True</span>, indent=<span class="hljs-number">4</span>)
        
    valid_sample = train_df.loc[fold_id == fold]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'fold '</span> + <span class="hljs-built_in">str</span>(fold) + <span class="hljs-string">': produced'</span>)
</code></pre>
    <p class="normal">The reason <a id="_idIndexMarker900"/>why the loop has to be executed in pieces is the size limit of the Kaggle environment: the maximum size of Notebook output is limited to 20 GB, and 5 folds with 2 files (training/validation) for each fold meant a total of 10 JSON files, exceeding that limit.</p>
    <p class="normal">Such practical considerations are worth keeping in mind when running code in a Kaggle Notebook, although for such “preparatory” work, you can, of course, produce the results elsewhere, and upload them as Kaggle Datasets afterward.</p>
    <p class="normal">With the splits produced, we can move toward training a Detectron2 model for our dataset. As usual, we start by loading the necessary packages:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pycocotools.mask <span class="hljs-keyword">as</span> mask_util
<span class="hljs-keyword">import</span> detectron2
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-keyword">import</span> random, cv2, os
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># Import some common detectron2 utilities</span>
<span class="hljs-keyword">from</span> detectron2 <span class="hljs-keyword">import</span> model_zoo
<span class="hljs-keyword">from</span> detectron2.engine <span class="hljs-keyword">import</span> DefaultPredictor, DefaultTrainer
<span class="hljs-keyword">from</span> detectron2.config <span class="hljs-keyword">import</span> get_cfg
<span class="hljs-keyword">from</span> detectron2.utils.visualizer <span class="hljs-keyword">import</span> Visualizer, ColorMode
<span class="hljs-keyword">from</span> detectron2.data <span class="hljs-keyword">import</span> MetadataCatalog, DatasetCatalog
<span class="hljs-keyword">from</span> detectron2.data.datasets <span class="hljs-keyword">import</span> register_coco_instances
<span class="hljs-keyword">from</span> detectron2.utils.logger <span class="hljs-keyword">import</span> setup_logger
<span class="hljs-keyword">from</span> detectron2.evaluation.evaluator <span class="hljs-keyword">import</span> DatasetEvaluator
<span class="hljs-keyword">from</span> detectron2.engine <span class="hljs-keyword">import</span> BestCheckpointer
<span class="hljs-keyword">from</span> detectron2.checkpoint <span class="hljs-keyword">import</span> DetectionCheckpointer
setup_logger()
<span class="hljs-keyword">import</span> torch
</code></pre>
    <p class="normal">While the number of imports from Detectron2 can seem intimidating at first, their function will become clear as we progress with the task definition; we start by specifying paths to the input data folder, annotations folder, and a YAML file defining our preferred model architecture:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CFG</span>:
    wfold = <span class="hljs-number">4</span>
    data_folder = <span class="hljs-string">'../input/sartorius-cell-instance-segmentation/'</span>
    anno_folder = <span class="hljs-string">'../input/sartoriusannotations/'</span>
    model_arch = <span class="hljs-string">'mask_rcnn_R_50_FPN_3x.yaml'</span>
    nof_iters = <span class="hljs-number">10000</span> 
    seed = <span class="hljs-number">45</span>
</code></pre>
    <p class="normal">One point <a id="_idIndexMarker901"/>worth mentioning here is the iterations parameter (<code class="inlineCode">nof_iters</code> above). Usually, model training is parametrized in terms of the number of epochs, in other words, complete passes through the training data. Detectron2 is engineered differently: one iteration refers to one mini-batch and different mini-batch sizes are used in different parts of the model.</p>
    <p class="normal">In order to ensure the results are reproducible, we fix random seeds used by different components of the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">seed_everything</span>(<span class="hljs-params">seed</span>):
    random.seed(seed)
    os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>] = <span class="hljs-built_in">str</span>(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span>
seed_everything(CFG.seed)
</code></pre>
    <p class="normal">The competition metric was <a id="_idIndexMarker902"/>the mean average precision at different <strong class="keyWord">intersection over union</strong> (<strong class="keyWord">IoU</strong>) thresholds. As a refresher from <em class="chapterRef">Chapter 5</em>, <em class="italic">Competition Tasks and Metrics</em>, the IoU of a proposed set of object pixels and a set of true object pixels is calculated as:</p>
    <p class="center"><img src="../Images/B17574_10_001.png" alt=""/></p>
    <p class="normal">The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95, with increments of 0.05.</p>
    <p class="normal">At each threshold value, a precision value is calculated based on the number of <strong class="keyWord">true positives</strong> (<strong class="keyWord">TP</strong>), <strong class="keyWord">false negatives</strong> (<strong class="keyWord">FN</strong>), and <strong class="keyWord">false positives</strong> (<strong class="keyWord">FP</strong>) resulting from comparing the predicted object with all ground truth objects. Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.</p>
    <p class="normal">Below, we <a id="_idIndexMarker903"/>define the functions necessary to calculate the metric and use it directly inside the model as the objective function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Taken from https://www.kaggle.com/theoviel/competition-metric-map-iou</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">precision_at</span>(<span class="hljs-params">threshold, iou</span>):
    matches = iou &gt; threshold
    true_positives = np.<span class="hljs-built_in">sum</span>(matches, axis=<span class="hljs-number">1</span>) == <span class="hljs-number">1</span>  <span class="hljs-comment"># Correct objects</span>
    false_positives = np.<span class="hljs-built_in">sum</span>(matches, axis=<span class="hljs-number">0</span>) == <span class="hljs-number">0</span>  <span class="hljs-comment"># Missed objects</span>
    false_negatives = np.<span class="hljs-built_in">sum</span>(matches, axis=<span class="hljs-number">1</span>) == <span class="hljs-number">0</span>  <span class="hljs-comment"># Extra objects</span>
    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(true_positives), np.<span class="hljs-built_in">sum</span>(false_positives),
    np.<span class="hljs-built_in">sum</span>(false_negatives)
<span class="hljs-keyword">def</span> <span class="hljs-title">score</span>(<span class="hljs-params">pred, targ</span>):
    pred_masks = pred[<span class="hljs-string">'instances'</span>].pred_masks.cpu().numpy()
    enc_preds = [mask_util.encode(np.asarray(p, order=<span class="hljs-string">'F'</span>))
                 <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> pred_masks]
    enc_targs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-string">'segmentation'</span>], targ))
    ious = mask_util.iou(enc_preds, enc_targs, [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(enc_targs))
    prec = []
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.05</span>):
        tp, fp, fn = precision_at(t, ious)
        p = tp / (tp + fp + fn)
        prec.append(p)
    <span class="hljs-keyword">return</span> np.mean(prec)
</code></pre>
    <p class="normal">With the metric defined, we can use it in the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">MAPIOUEvaluator</span>(<span class="hljs-title">DatasetEvaluator</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, dataset_name</span>):
        dataset_dicts = DatasetCatalog.get(dataset_name)
        self.annotations_cache = {item[<span class="hljs-string">'image_id'</span>]:item[<span class="hljs-string">'annotations'</span>]
                                  <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dataset_dicts}
            
    <span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>):
        self.scores = []
    <span class="hljs-keyword">def</span> <span class="hljs-title">process</span>(<span class="hljs-params">self, inputs, outputs</span>):
        <span class="hljs-keyword">for</span> inp, out <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(inputs, outputs):
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(out[<span class="hljs-string">'instances'</span>]) == <span class="hljs-number">0</span>:
                self.scores.append(<span class="hljs-number">0</span>)    
            <span class="hljs-keyword">else</span>:
                targ = self.annotations_cache[inp[<span class="hljs-string">'image_id'</span>]]
                self.scores.append(score(out, targ))
    <span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> {<span class="hljs-string">"MaP IoU"</span>: np.mean(self.scores)}
</code></pre>
    <p class="normal">This gives us <a id="_idIndexMarker904"/>the basis for creating a <code class="inlineCode">Trainer</code> object, which is the workhorse of our solution built around Detectron2:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Trainer</span>(<span class="hljs-title">DefaultTrainer</span>):
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">build_evaluator</span>(<span class="hljs-params">cls, cfg, dataset_name, output_folder=</span><span class="hljs-literal">None</span>):
        <span class="hljs-keyword">return</span> MAPIOUEvaluator(dataset_name)
    <span class="hljs-keyword">def</span> <span class="hljs-title">build_hooks</span>(<span class="hljs-params">self</span>):
        <span class="hljs-comment"># copy of cfg</span>
        cfg = self.cfg.clone()
        <span class="hljs-comment"># build the original model hooks</span>
        hooks = <span class="hljs-built_in">super</span>().build_hooks()
        <span class="hljs-comment"># add the best checkpointer hook</span>
        hooks.insert(-<span class="hljs-number">1</span>, BestCheckpointer(cfg.TEST.EVAL_PERIOD, 
                                         DetectionCheckpointer(self.model,
                                         cfg.OUTPUT_DIR),
                                         <span class="hljs-string">"MaP IoU"</span>,
                                         <span class="hljs-string">"max"</span>,
                                         ))
        <span class="hljs-keyword">return</span> hooks
</code></pre>
    <p class="normal">We now proceed to load the training/validation data in Detectron2 style:</p>
    <pre class="programlisting code"><code class="hljs-code">dataDir=Path(CFG.data_folder)
register_coco_instances(<span class="hljs-string">'sartorius_train'</span>,{}, CFG.anno_folder + 
                        <span class="hljs-string">'annotations_train_f' </span>+ <span class="hljs-built_in">str</span>(CFG.wfold) + 
                        <span class="hljs-string">'.json'</span>, dataDir)
register_coco_instances(<span class="hljs-string">'sartorius_val'</span>,{}, CFG.anno_folder + 
                        <span class="hljs-string">'annotations_valid_f' </span>+ <span class="hljs-built_in">str</span>(CFG.wfold) + 
                        <span class="hljs-string">'.json'</span>, dataDir)
metadata = MetadataCatalog.get(<span class="hljs-string">'sartorius_train'</span>)
train_ds = DatasetCatalog.get(<span class="hljs-string">'sartorius_train'</span>)
</code></pre>
    <p class="normal">Before we instantiate a Detectron2 model, we need to take care of configuring it. Most of the values can be left at default values (at least, in a first pass); if you decide to tinker a bit more, start with <code class="inlineCode">BATCH_SIZE_PER_IMAGE</code> (for increased generalization performance) and <code class="inlineCode">SCORE_THRESH_TEST</code> (to limit false negatives):</p>
    <pre class="programlisting code"><code class="hljs-code">cfg = get_cfg()
cfg.INPUT.MASK_FORMAT=<span class="hljs-string">'bitmask'</span>
cfg.merge_from_file(model_zoo.get_config_file(<span class="hljs-string">'COCO-InstanceSegmentation/'</span> +
                    CFG.model_arch))
cfg.DATASETS.TRAIN = (<span class="hljs-string">"sartorius_train"</span>,)
cfg.DATASETS.TEST = (<span class="hljs-string">"sartorius_val"</span>,)
cfg.DATALOADER.NUM_WORKERS = <span class="hljs-number">2</span>
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(<span class="hljs-string">'COCO-InstanceSegmentation/'</span>
<span class="hljs-string">                  </span>  + CFG.model_arch)
cfg.SOLVER.IMS_PER_BATCH = <span class="hljs-number">2</span>
cfg.SOLVER.BASE_LR = <span class="hljs-number">0.001</span>
cfg.SOLVER.MAX_ITER = CFG.nof_iters
cfg.SOLVER.STEPS = []
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = <span class="hljs-number">512</span>
cfg.MODEL.ROI_HEADS.NUM_CLASSES = <span class="hljs-number">3</span>  
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = <span class="hljs-number">.4</span>
cfg.TEST.EVAL_PERIOD = <span class="hljs-built_in">len</span>(DatasetCatalog.get(<span class="hljs-string">'sartorius_train'</span>)) 
                           // cfg.SOLVER.IMS_PER_BATCH  
</code></pre>
    <p class="normal">Training a <a id="_idIndexMarker905"/>model is straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code">os.makedirs(cfg.OUTPUT_DIR, exist_ok=<span class="hljs-literal">True</span>)
trainer = Trainer(cfg) 
trainer.resume_or_load(resume=<span class="hljs-literal">False</span>)
trainer.train()
</code></pre>
    <p class="normal">You will notice that the output during training is rich in information about the progress of the procedure:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_16.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.16: Training output from Detectron2</p>
    <p class="normal">Once the model is <a id="_idIndexMarker906"/>trained, we can save the weights and use them for inference (potentially in a separate Notebook – see the discussion earlier in this chapter) and submission preparation. We start by adding new parameters that allow us to regularize the prediction, setting confidence thresholds and minimal mask sizes:</p>
    <pre class="programlisting code"><code class="hljs-code">THRESHOLDS = [<span class="hljs-number">.18</span>, <span class="hljs-number">.35</span>, <span class="hljs-number">.58</span>]
MIN_PIXELS = [<span class="hljs-number">75</span>, <span class="hljs-number">150</span>, <span class="hljs-number">75</span>]
</code></pre>
    <p class="normal">We need a helper function for encoding a single mask into RLE format:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">rle_encode</span>(<span class="hljs-params">img</span>):
    <span class="hljs-string">'''</span>
<span class="hljs-string">    img: numpy array, 1 - mask, 0 - background</span>
<span class="hljs-string">    Returns run length as string formatted</span>
<span class="hljs-string">    '''</span>
    pixels = img.flatten()
    pixels = np.concatenate([[<span class="hljs-number">0</span>], pixels, [<span class="hljs-number">0</span>]])
    runs = np.where(pixels[<span class="hljs-number">1</span>:] != pixels[:-<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>
    runs[<span class="hljs-number">1</span>::<span class="hljs-number">2</span>] -= runs[::<span class="hljs-number">2</span>]
    <span class="hljs-keyword">return</span> <span class="hljs-string">' '</span>.join(<span class="hljs-built_in">str</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runs)
</code></pre>
    <p class="normal">Below is the <a id="_idIndexMarker907"/>main function for producing all masks per image, filtering out the dubious ones (with confidence scores below <code class="inlineCode">THRESHOLDS</code>) with small areas (containing fewer pixels than <code class="inlineCode">MIN_PIXELS</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_masks</span>(<span class="hljs-params">fn, predictor</span>):
    im = cv2.imread(<span class="hljs-built_in">str</span>(fn))
    pred = predictor(im)
    pred_class = torch.mode(pred[<span class="hljs-string">'instances'</span>].pred_classes)[<span class="hljs-number">0</span>]
    take = pred[<span class="hljs-string">'instances'</span>].scores &gt;= THRESHOLDS[pred_class]
    pred_masks = pred[<span class="hljs-string">'instances'</span>].pred_masks[take]
    pred_masks = pred_masks.cpu().numpy()
    res = []
    used = np.zeros(im.shape[:<span class="hljs-number">2</span>], dtype=<span class="hljs-built_in">int</span>) 
    <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> pred_masks:
        mask = mask * (<span class="hljs-number">1</span>-used)
<span class="hljs-comment">        # Skip predictions with small area</span>
        <span class="hljs-keyword">if</span> mask.<span class="hljs-built_in">sum</span>() &gt;= MIN_PIXELS[pred_class]:
            used += mask
            res.append(rle_encode(mask))
    <span class="hljs-keyword">return</span> res
</code></pre>
    <p class="normal">We then prepare the lists where image IDs and masks will be stored:</p>
    <pre class="programlisting code"><code class="hljs-code">dataDir=Path(CFG.data_folder)
ids, masks=[],[]
test_names = (dataDir/<span class="hljs-string">'test'</span>).ls()
</code></pre>
    <p class="normal">Competitions with large image sets – like the ones discussed in this section – often require training models for longer than 9 hours, which is the time limit imposed in Code competitions (see <a href="https://www.kaggle.com/docs/competitions"><span class="url">https://www.kaggle.com/docs/competitions</span></a>). This means that training a model and running inference within the same Notebook becomes impossible. A typical workaround is to run a training Notebook/script first as a standalone Notebook in Kaggle, Google Colab, GCP, or locally. The output of this first Notebook (the trained weights) is used as input to the second one, in other words, to define the model used for predictions.</p>
    <p class="normal">We proceed in that manner by loading the weights of our trained model:</p>
    <pre class="programlisting code"><code class="hljs-code">cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(<span class="hljs-string">"COCO-InstanceSegmentation/"</span>+
                    CFG.arch+<span class="hljs-string">".yaml"</span>))
cfg.INPUT.MASK_FORMAT = <span class="hljs-string">'</span><span class="hljs-string">bitmask'</span>
cfg.MODEL.ROI_HEADS.NUM_CLASSES = <span class="hljs-number">3</span> 
cfg.MODEL.WEIGHTS = CFG.model_folder + <span class="hljs-string">'model_best_f' </span>+ 
                    <span class="hljs-built_in">str</span>(CFG.wfold)+<span class="hljs-string">'.pth'</span> 
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = <span class="hljs-number">0.5</span>
cfg.TEST.DETECTIONS_PER_IMAGE = <span class="hljs-number">1000</span>
predictor = DefaultPredictor(cfg)
</code></pre>
    <p class="normal">We can<a id="_idIndexMarker908"/> visualize some of the predictions:</p>
    <pre class="programlisting code"><code class="hljs-code">encoded_masks = get_masks(test_names[<span class="hljs-number">0</span>], predictor)
_, axs = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>, figsize = (<span class="hljs-number">40</span>, <span class="hljs-number">15</span>))
axs[<span class="hljs-number">1</span>].imshow(cv2.imread(<span class="hljs-built_in">str</span>(test_names[<span class="hljs-number">0</span>])))
<span class="hljs-keyword">for</span> enc <span class="hljs-keyword">in</span> encoded_masks:
    dec = rle_decode(enc)
axs[<span class="hljs-number">0</span>].imshow(np.ma.masked_where(dec == <span class="hljs-number">0</span>, dec))
</code></pre>
    <p class="normal">Here is an example:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.17: Visualizing a sample prediction from Detectron2 alongside the source image</p>
    <p class="normal">With the helper functions defined above, producing the masks in RLE format for submission is straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> fn <span class="hljs-keyword">in</span> test_names:
    encoded_masks = get_masks(fn, predictor)
    <span class="hljs-keyword">for</span> enc <span class="hljs-keyword">in</span> encoded_masks:
        ids.append(fn.stem)
        masks.append(enc)
pd.DataFrame({<span class="hljs-string">'id'</span>:ids, <span class="hljs-string">'predicted'</span>:masks}).to_csv(<span class="hljs-string">'submission.csv'</span>, 
                                                   index=<span class="hljs-literal">False</span>)
pd.read_csv(<span class="hljs-string">'submission.csv'</span>).head()
</code></pre>
    <p class="normal">Here are the <a id="_idIndexMarker909"/>first few rows of the final submission:</p>
    <figure class="mediaobject"><img src="../Images/B17574_10_18.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 10.18: Formatted submission from a trained Detectron2 model</p>
    <p class="normal">We have reached the end of the section. The pipeline above demonstrates how to set up a semantic segmentation model and train it. We have used a small number of iterations, but in order to achieve competitive results, longer training is necessary.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Laura_Fink.png" alt=""/>
      </div>
      <p class="intervieweeName">Laura Fink</p>
      <p class="normal"><a href="https://www.kaggle.com/allunia"><span class="url">https://www.kaggle.com/allunia</span></a></p>
      <p class="normal">To wrap up this <a id="_idIndexMarker910"/>chapter, let’s see what Kaggler Laura Fink has to say about her time on the platform. As well as being a Notebooks Grandmaster and producing many masterful Notebooks, she is also Head of Data Science at MicroMata.</p>

      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">My favorite competitions are those that want to yield something good to humanity. I especially like all healthcare-related challenges. Nonetheless, each competition feels like an adventure for me with its own puzzles to be solved. I really enjoy learning new skills and exploring new kinds of datasets or problems. Consequently, I’m not focused on specific techniques but rather on learning something new. I think I’m known for my strengths in exploratory data analysis (EDA).</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">When entering a competition, I start by reading the problem statement and the data description. After browsing through the forum and public Notebooks for collecting ideas, I usually start by developing my own solutions. In the initial phase, I spend some time on EDA to search for hidden groups and get some intuition. This helps quite a lot in setting up a proper validation strategy, which I believe is the foundation of all remaining steps. Then, I start to iterate through different parts of the machine learning pipeline like feature engineering or preprocessing, improving the model architecture, asking questions about the data collection, searching for leakages, doing more EDA, or building ensembles. I try to improve my solution in a greedy fashion. Kaggle competitions are very dynamic and one needs to try out diverse ideas and different solutions to survive in the end.</em></p>
      <p class="normal"><em class="italic">This is definitely different from my day-to-day work, where the focus is more on gaining insights from data and finding simple but effective solutions to improve business processes. Here, the task is often more complex than the models used. The problem to be solved has to be defined very clearly, which means that one has to discuss with experts of different backgrounds which goals should be reached, which processes are involved, and how the data needs to be collected or fused. Compared to Kaggle competitions, my daily work needs much more communication than machine learning skills.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">The </em>G2Net Gravitational Wave Detection<em class="italic"> competition was one of my favorites. The goal was to detect simulated gravitational wave signals that were hidden in noise originating from detector components and terrestrial forces. An important insight during this competition was that you should have a critical look at standard ways to analyze data and try out your own ideas. In the papers I read, the data was prepared mainly by using the Fourier or Constant-Q transform after whitening the data and applying a bandpass filter. </em></p>

      <p class="normal"><em class="italic">It came out very quickly that whitening was not helpful, as it used spline interpolation of the Power Spectral Density, which was itself very noisy. Fitting polynomials to small subsets of noisy data adds another source of errors because of overfitting.</em></p>
      <p class="normal"><em class="italic">After dropping the whitening, I tried out different hyperparameters of the Constant-Q transform, which turned out to be the leading method in the forum and public</em> <em class="italic">Notebooks for a long time. As there were two sources of gravitational waves that can be covered by different ranges of Q-values, I tried out an ensemble of models that differed in these hyperparameters. This turned out to be helpful in improving my score, but then I reached a limit. The Constant-Q transform applies a series of filters to time series and transforms them into the frequency domain. I started to ask myself if there was a method that does these filtering tasks in a better, more flexible way. It was at the same time that the idea of using 1 dim CNNs came up in the community, and I loved it. We all know that filters of 2 dim CNNs are able to detect edges, lines, and textures given image data. The same could be done with “classical” filters like the Laplace or Sobel filter. For this reason, I asked myself: can’t we use the 1dCNN to learn the most important filters on its own, instead of applying transformations that are already fixed somehow? </em></p>
      <p class="normal"><em class="italic">I was not able to get my 1 dim CNN solution to work, but it turned out that many top teams managed it well. The G2Net competition was one of my favorites even though I missed out on the goal of winning a medal. However, the knowledge I gained along the way and the lesson I learned about so-called standard approaches were very valuable.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">I started my first job after university as a Java software developer even though I already had my first contact with machine learning during my master’s thesis. I was interested in doing more data analytics, but at that time, there were almost no data science jobs, or they were not named this way. When I heard about Kaggle the first time, I was trapped right from the start. Since then, I often found myself on Kaggle during the evenings to have some fun. It was not my intent to change my position at that time, but then a research project came up that needed machine learning skills. I was able to show that I was a suitable candidate for this project because of the knowledge I gained by participating on Kaggle. This turned out to be the entry point for my data science career.</em></p>
      <p class="normal"><em class="italic">Kaggle has always been a great place for me to try out ideas, learn new methods and tools, and gain practical experience. The skills I obtained this way have been quite helpful for data science projects at work. It’s like a boost of knowledge, as Kaggle provides a sandbox for you to try out different ideas and to be creative without risk. Failing in a competition means that there was at least one lesson to learn, but failing in a project can have a huge negative impact on yourself and other people.</em></p>

      <p class="normal"><em class="italic">Besides taking part in competitions, another great way to build up your portfolio is to write Notebooks. In doing so, you can show the world how you approach problems and how to communicate insights and conclusions. The latter is very important when you have to work with management, clients, and experts from different backgrounds.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">I think many beginners that enter competitions are seduced by the public leaderboard and build their models without having a good validation strategy. While measuring their success on the leaderboard, they are likely overfitting to the public test data. After the end of the competition, their models are not able to generalize to the unseen private test data, and they often fall down hundreds of places. I still remember how frustrated I was during the </em>Mercedes-Benz Greener Manufacturing<em class="italic"> competition as I was not able to climb up the public leaderboard. But when the final standings came out, it was a big surprise how many people were shuffled up and down the leaderboard. Since then, I always have in mind that a proper validation scheme is very important for managing the challenges of under- and overfitting.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">My biggest mistake so far was to spend too much time and effort on the details of my solution at the beginning of a competition. Indeed, it’s much better to iterate fast through diverse and different ideas after building a proper validation strategy. That way, it’s easier and faster to find promising directions for improvements and the danger of getting stuck somewhere is much smaller.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">There are a lot of common tools and libraries you can learn and practice when becoming active in the Kaggle community and I can only recommend them all. It’s important to stay flexible and to learn about their advantages and disadvantages. This way, your solutions don’t depend on your tools, but rather on your ideas and creativity.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Data science is not about building models, but rather about understanding the data and the way it was collected. Many competitions I have entered so far showed leakages or had hidden groups in the test data that one could find with exploratory data analysis.</em></p>
    </div>
    <h1 id="_idParaDest-172" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we gave you an overview of the most important topics related to computer vision from a Kaggle competition angle. We introduced augmentations, an important class of techniques used for extending the generalization capabilities of an algorithm, and followed by demonstrating end-to-end pipelines for three of the most frequent problems: image classification, object detection, and semantic segmentation.</p>
    <p class="normal">In the next chapter, we switch our attention to natural language processing, another extremely broad and popular category of problems.</p>
    <h1 id="_idParaDest-173" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>