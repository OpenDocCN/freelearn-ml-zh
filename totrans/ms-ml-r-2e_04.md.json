["```py\n    > library(ElemStatLearn) #contains the data\n    > library(car) #package to calculate Variance Inflation Factor\n    > library(corrplot) #correlation plots\n    > library(leaps) #best subsets regression\n    > library(glmnet) #allows ridge regression, LASSO and elastic net\n    > library(caret) #parameter tuning\n\n```", "```py\n    > data(prostate)\n    > str(prostate)\n    'data.frame':97 obs. of  10 variables:\n     $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...\n     $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...\n     $ age    : int  50 58 74 58 62 50 64 58 47 63 ...\n     $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...\n     $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...\n     $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...\n     $ gleason: int  6 6 7 6 6 6 6 6 6 6 ...\n     $ pgg45  : int  0 0 20 0 0 0 0 0 0 0 ...\n     $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...\n     $ train  : logi  TRUE TRUE TRUE TRUE TRUE TRUE ...\n\n```", "```py\n    > plot(prostate)\n\n```", "```py\n    > plot(prostate$gleason)\n\n```", "```py\n    > table(prostate$gleason)\n    6  7  8  9 \n    35 56  1  5 \n\n```", "```py\n    > boxplot(prostate$lpsa ~ prostate$gleason, xlab = \"Gleason Score\", \n      ylab = \"Log of PSA\")\n\n```", "```py\n    > prostate$gleason <- ifelse(prostate$gleason == 6, 0, 1)\n\n```", "```py\n    > table(prostate$gleason)\n    0  1 \n    35 62\n\n```", "```py\n    > p.cor = cor(prostate)\n    > corrplot.mixed(p.cor)\n\n```", "```py\n    > train <- subset(prostate, train == TRUE)[, 1:9]\n    > str(train)\n    'data.frame':67 obs. of  9 variables:\n     $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...\n     $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...\n     $ age    : int  50 58 74 58 62 50 58 65 63 63 ...\n     $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...\n     $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...\n     $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...\n     $ gleason: num  0 0 1 0 0 0 0 0 0 1 ...\n     $ pgg45  : int  0 0 20 0 0 0 0 0 0 30 ...\n     $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...\n    > test <- subset(prostate, train == FALSE)[, 1:9]\n    > str(test)\n    'data.frame':30 obs. of  9 variables:\n     $ lcavol : num  0.737 -0.777 0.223 1.206 2.059 ...\n     $ lweight: num  3.47 3.54 3.24 3.44 3.5 ...\n     $ age    : int  64 47 63 57 60 69 68 67 65 54 ...\n     $ lbph   : num  0.615 -1.386 -1.386 -1.386 1.475 ...\n     $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...\n     $ lcp    : num  -1.386 -1.386 -1.386 -0.431 1.348 ...\n     $ gleason: num  0 0 0 1 1 0 0 1 0 0 ...\n     $ pgg45  : int  0 0 0 5 20 0 0 20 0 0 ...\n     $ lpsa   : num  0.765 1.047 1.047 1.399 1.658 ...\n\n```", "```py\n    > subfit <- regsubsets(lpsa ~ ., data = train)\n\n```", "```py\n    > b.sum <- summary(subfit)\n    > which.min(b.sum$bic)\n      [1] 3\n\n```", "```py\n    > plot(b.sum$bic, type = \"l\", xlab = \"# of Features\", ylab = \"BIC\", \n      main = \"BIC score by Feature Inclusion\")\n\n```", "```py\n    > plot(subfit, scale = \"bic\", main = \"Best Subset Features\")\n\n```", "```py\n    > ols <- lm(lpsa ~ lcavol + lweight + gleason, data = train)\n    > plot(ols$fitted.values, train$lpsa, xlab = \"Predicted\", ylab = \n      \"Actual\", main = \"Predicted vs Actual\")\n\n```", "```py\n    > pred.subfit <- predict(ols, newdata = test)\n    > plot(pred.subfit, test$lpsa , xlab = \"Predicted\", ylab = \n      \"Actual\", main = \"Predicted vs Actual\")\n\n```", "```py\n    > resid.subfit <- test$lpsa - pred.subfit\n    > mean(resid.subfit^2)\n    [1] 0.5084126\n\n```", "```py\n    > x <- as.matrix(train[, 1:8])\n    > y <- train[, 9]\n\n```", "```py\n    > ridge <- glmnet(x, y, family = \"gaussian\", alpha = 0)\n\n```", "```py\n    > print(ridge)\n    Call:  glmnet(x = x, y = y, family = \"gaussian\", alpha = 0) \n           Df      %Dev    Lambda\n      [1,]  8 3.801e-36 878.90000\n      [2,]  8 5.591e-03 800.80000\n      [3,]  8 6.132e-03 729.70000\n      [4,]  8 6.725e-03 664.80000\n      [5,]  8 7.374e-03 605.80000\n      ...........................\n     [91,]  8 6.859e-01   0.20300\n     [92,]  8 6.877e-01   0.18500\n     [93,]  8 6.894e-01   0.16860\n     [94,]  8 6.909e-01   0.15360\n     [95,]  8 6.923e-01   0.13990\n     [96,]  8 6.935e-01   0.12750\n     [97,]  8 6.946e-01   0.11620\n     [98,]  8 6.955e-01   0.10590\n     [99,]  8 6.964e-01   0.09646\n    [100,]  8 6.971e-01   0.08789\n\n```", "```py\n    > plot(ridge, label = TRUE)\n\n```", "```py\n    > plot(ridge, xvar = \"lambda\", label = TRUE)\n\n```", "```py\n    > ridge.coef <- coef(ridge, s = 0.1, exact = TRUE)\n    > ridge.coef\n    9 x 1 sparse Matrix of class \"dgCMatrix\"\n                          1\n    (Intercept)  0.13062197\n    lcavol       0.45721270\n    lweight      0.64579061\n    age         -0.01735672\n    lbph         0.12249920\n    svi          0.63664815\n    lcp         -0.10463486\n    gleason      0.34612690\n    pgg45        0.00428580\n\n```", "```py\n    > plot(ridge, xvar = \"dev\", label = TRUE)\n\n```", "```py\n    > newx <- as.matrix(test[, 1:8])\n\n```", "```py\n    > ridge.y <- predict(ridge, newx = newx, type = \"response\", s = \n      0.1)\n    > plot(ridge.y, test$lpsa, xlab = \"Predicted\", ylab = \"Actual\",main \n      = \"Ridge Regression\")\n\n```", "```py\n    > ridge.resid <- ridge.y - test$lpsa\n    > mean(ridge.resid^2)\n    [1] 0.4789913\n\n```", "```py\n    > lasso <- glmnet(x, y, family = \"gaussian\", alpha = 1)\n    > print(lasso)\n    Call: glmnet(x = x, y = y, family = \"gaussian\", alpha = 1) \n    Df %Dev Lambda\n    [1,] 0 0.00000 0.878900\n    [2,] 1 0.09126 0.800800\n    [3,] 1 0.16700 0.729700\n    [4,] 1 0.22990 0.664800\n    [5,] 1 0.28220 0.605800\n    ........................\n    [60,] 8 0.70170 0.003632\n    [61,] 8 0.70170 0.003309\n    [62,] 8 0.70170 0.003015\n    [63,] 8 0.70170 0.002747\n    [64,] 8 0.70180 0.002503\n    [65,] 8 0.70180 0.002281\n    [66,] 8 0.70180 0.002078\n    [67,] 8 0.70180 0.001893\n    [68,] 8 0.70180 0.001725\n    [69,] 8 0.70180 0.001572\n\n```", "```py\n    [31,] 7 0.67240 0.053930\n    [32,] 7 0.67460 0.049140\n    [33,] 7 0.67650 0.044770\n    [34,] 8 0.67970 0.040790\n    [35,] 8 0.68340 0.037170\n\n```", "```py\n    > plot(lasso, xvar = \"lambda\", label = TRUE)\n\n```", "```py\n    > lasso.coef <- coef(lasso, s = 0.045, exact = TRUE)\n    > lasso.coef\n    9 x 1 sparse Matrix of class \"dgCMatrix\"\n                            1\n    (Intercept) -0.1305852115\n    lcavol       0.4479676523\n    lweight      0.5910362316\n    age         -0.0073156274\n    lbph         0.0974129976\n    svi          0.4746795823\n    lcp          . \n    gleason      0.2968395802\n    pgg45        0.0009790322\n\n```", "```py\n    > lasso.y <- predict(lasso, newx = newx, type = \"response\", s =  \n      0.045)\n    > plot(lasso.y, test$lpsa, xlab = \"Predicted\", ylab = \"Actual\", \n      main = \"LASSO\")\n\n```", "```py\n    > lasso.resid <- lasso.y - test$lpsa\n    > mean(lasso.resid^2)\n    [1] 0.4437209\n\n```", "```py\n    > grid <- expand.grid(.alpha = seq(0, 1, by = .2), .lambda = \n      seq(0.00, 0.2,  by = 0.02))\n\n```", "```py\n    > table(grid)\n          .lambda\n    .alpha 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2\n       0   1    1    1    1    1   1    1    1    1    1   1\n       0.2 1    1    1    1    1   1    1    1    1    1   1\n       0.4 1    1    1    1    1   1    1    1    1    1   1\n       0.6 1    1    1    1    1   1    1    1    1    1   1\n       0.8 1    1    1    1    1   1    1    1    1    1   1\n       1   1    1    1    1    1   1    1    1    1    1   1\n\n```", "```py\n    > control <- trainControl(method = \"LOOCV\")\n\n```", "```py\n    > enet.train <- train(lpsa ~ ., data = train, method = \"glmnet\", \n      trControl = control, tuneGrid = grid)\n\n```", "```py\n    > enet.train\n    glmnet \n    67 samples\n     8 predictor\n    No pre-processing\n    Resampling: \n    Summary of sample sizes: 66, 66, 66, 66, 66, 66, ... \n    Resampling results across tuning parameters:\n      alpha  lambda  RMSE   Rsquared\n    0.0    0.00    0.750  0.609 \n    0.0    0.02    0.750  0.609 \n    0.0    0.04    0.750  0.609 \n    0.0    0.06    0.750  0.609 \n    0.0    0.08    0.750  0.609 \n    0.0    0.10    0.751  0.608 \n       .........................\n    1.0    0.14    0.800  0.564 \n    1.0    0.16    0.809  0.558 \n    1.0    0.18    0.819  0.552 \n    1.0    0.20    0.826  0.549 \n\n```", "```py\n    > enet <- glmnet(x, y, family = \"gaussian\", alpha = 0, lambda = \n      .08)\n    > enet.coef <- coef(enet, s = .08, exact = TRUE)\n    > enet.coef\n    9 x 1 sparse Matrix of class \"dgCMatrix\"\n                           1\n    (Intercept)  0.137811097\n    lcavol       0.470960525\n    lweight      0.652088157\n    age         -0.018257308\n    lbph         0.123608113\n    svi          0.648209192\n    lcp         -0.118214386\n    gleason      0.345480799\n    pgg45        0.004478267\n    > enet.y <- predict(enet, newx=newx, type=\"response\", s=.08)\n    > plot(enet.y, test$lpsa, xlab=\"Predicted\", ylab=\"Actual\", \n      main=\"Elastic Net\")\n\n```", "```py\n    > enet.resid <- enet.y - test$lpsa\n    > mean(enet.resid^2)\n    [1] 0.4795019\n\n```", "```py\n    > set.seed(317)  \n    > lasso.cv = cv.glmnet(x, y, nfolds = 3)\n    > plot(lasso.cv)\n\n```", "```py\n    > lasso.cv$lambda.min #minimum\n    [1] 0.0133582\n    > lasso.cv$lambda.1se #one standard error away\n    [1] 0.124579\n\n```", "```py\n    > coef(lasso.cv, s = \"lambda.1se\")\n    9 x 1 sparse Matrix of class \"dgCMatrix\"\n     1\n    (Intercept) -0.13543760\n    lcavol 0.43892533\n    lweight 0.49550944\n    age . \n    lbph 0.04343678\n    svi 0.34985691\n    lcp . \n    gleason 0.21225934\n    pgg45 . \n\n    > lasso.y.cv = predict(lasso.cv, newx=newx, type = \"response\",\n    s = \"lambda.1se\")\n\n    > lasso.cv.resid = lasso.y.cv - test$lpsa\n\n    > mean(lasso.cv.resid^2)\n    [1] 0.4465453\n\n```", "```py\n > library(MASS)\n > biopsy$ID = NULL\n > names(biopsy) = c(\"thick\", \"u.size\", \"u.shape\", \"adhsn\",\n \"s.size\", \"nucl\", \"chrom\", \"n.nuc\", \"mit\", \"class\")\n > biopsy.v2 <- na.omit(biopsy) > set.seed(123) \n > ind <- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, \n      0.3))\n > train <- biopsy.v2[ind==1, ] \n > test <- biopsy.v2[ind==2, ] \n\n```", "```py\n > x <- as.matrix(train[, 1:9])\n > y <- train[, 10]\n\n```", "```py\n > set.seed(3)\n > fitCV <- cv.glmnet(x, y, family = \"binomial\",\n type.measure = \"auc\",\n nfolds = 5)\n\n```", "```py\n > plot(fitCV)\n\n```", "```py\n > fitCV$lambda.1se\n    [1] 0.1876892\n    > coef(fitCV, s = \"lambda.1se\")\n    10 x 1 sparse Matrix of class \"dgCMatrix\"\n     1\n    (Intercept) -1.84478214\n    thick 0.01892397\n    u.size 0.10102690\n    u.shape 0.08264828\n    adhsn . \n    s.size . \n    nucl 0.13891750\n    chrom . \n    n.nuc . \n    mit .\n\n```", "```py\n > library(InformationValue)\n    > predCV <- predict(fitCV, newx = as.matrix(test[, 1:9]),\n        s = \"lambda.1se\",\n        type = \"response\")\n    actuals <- ifelse(test$class == \"malignant\", 1, 0)\n    misClassError(actuals, predCV)\n    [1] 0.0622\n    > plotROC(actuals, predCV)\n\n```", "```py\n > predCV.min <- predict(fitCV, newx = as.matrix(test[, 1:9]),\n s = \"lambda.min\",\n type = \"response\") > misClassError(actuals, predCV.min)\n [1] 0.0239\n\n```"]