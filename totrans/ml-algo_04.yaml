- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear models are the simplest parametric methods and always deserve the right
    attention, because many problems, even intrinsically non-linear ones, can be easily
    solved with these models. As discussed previously, a regression is a prediction
    where the target is continuous and its applications are several, so it's important
    to understand how a linear model can fit the data, what its strengths and weaknesses
    are, and when it's preferable to pick an alternative. In the last part of the
    chapter, we're going to discuss an interesting method to work efficiently with
    non-linear data using the same models.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a dataset of real-values vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c11132d3-2d0a-4829-bf9a-1f0683bc1b54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each input vector is associated with a real value y[i]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d55285c-b14f-4500-90ec-b71cc653fc91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A linear model is based on the assumption that it''s possible to approximate
    the output values through a regression process based on the rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b1cdea2-df22-4389-92b9-4555648cc184.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the strong assumption is that our dataset and all other unknown
    points lie on a hyperplane and the maximum error is proportional to both the training
    quality and the adaptability of the original dataset. One of the most common problems
    arises when the dataset is clearly non-linear and other models have to be considered
    (such as neural networks or kernel support vector machines).
  prefs: []
  type: TYPE_NORMAL
- en: A bidimensional example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a small dataset built by adding some uniform noise to the points
    belonging to a segment bounded between -6 and 6\. The original equation is: *y
    = x + 2 + n*, where n is a noise term.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, there''s a plot with a candidate regression function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b861a0a-9b14-4014-b4d7-d96af0867025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we''re working on a plane, the regressor we''re looking for is a function
    of only two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b0783e1-6cda-43b0-819e-e068cf5de528.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to fit our model, we must find the best parameters and to do that
    we choose an ordinary least squares approach. The loss function to minimize is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2978dc42-6e7b-443e-baf2-834395af5fa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With an analytic approach, in order to find the global minimum, we must impose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0805d7ec-87ec-4aaa-9351-c83d29be9651.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So (for simplicity, it accepts a vector containing both variables):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And the gradient can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimization problem can now be solved using SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the regression denoised our dataset, rebuilding the original equation:
    *y = x + 2*.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with scikit-learn and higher dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn offers the class `LinearRegression`, which works with n-dimensional
    spaces. For this purpose, we''re going to use the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It has 506 samples with 13 input features and one output. In the following
    figure, there'' a collection of the plots of the first 12 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34b5f0a3-0aba-4aff-a89a-d92a089bb68b.png)'
  prefs: []
  type: TYPE_IMG
- en: When working with datasets, it's useful to have a tabular view to manipulate
    data. pandas is a perfect framework for this task, and even though it's beyond
    the scope of this book, I suggest you create a data frame with the command `pandas.DataFrame(boston.data,
    columns=boston.feature_names)` and use Jupyter to visualize it. For further information,
    refer to Heydt M., *Learning pandas - Python Data Discovery and Analysis Made
    Easy*, Packt.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different scales and outliers (which can be removed using the methods
    studied in the previous chapters), so it''s better to ask the model to normalize
    the data before processing it. Moreover, for testing purposes, we split the original
    dataset into training (90%) and test (10%) sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When the original data set isn''t large enough, splitting it into training
    and test sets may reduce the number of samples that can be used for fitting the
    model. k-fold cross-validation can help in solving this problem with a different
    strategy. The whole dataset is split into k folds using always k-1 folds for training
    and the remaining one to validate the model. K iterations will be performed, using
    always a different validation fold. In the following figure, there''s an example
    with 3 folds/iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/818fee35-73a6-482c-9733-cf5b1b2570bf.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, the final score can be determined as average of all values and
    all samples are selected for training k-1 times.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the accuracy of a regression, scikit-learn provides the internal method
    `score(X, y)` which evaluates the model on test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So the overall accuracy is about 77%, which is an acceptable result considering
    the non-linearity of the original dataset, but it can be also influenced by the
    subdivision made by `train_test_split`(like in our case). Instead, for k-fold
    cross-validation, we can use the function `cross_val_score()`, which works with
    all the classifiers. The scoring parameter is very important because it determines
    which metric will be adopted for tests. As `LinearRegression` works with ordinary
    least squares, we preferred the negative mean squared error, which is a cumulative
    measure that must be evaluated according to the actual values (it's not relative).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Another very important metric used in regressions is called the **coefficient
    of determination** or *R²*. It measures the amount of variance on the prediction
    which is explained by the dataset. We define **residuals**, the following quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1e61431-300d-4745-b697-7fde495ae484.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, it is the difference between the sample and the prediction.
    So the *R²* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b4dfc9-5c5c-4f67-bdf8-e4aa967ad90f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our purposes, *R²* values close to 1 mean an almost perfect regression,
    while values close to 0 (or negative) imply a bad model. Using this metric is
    quite easy with cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Regressor analytic expression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to have an analytical expression of our model (a hyperplane), `LinearRegression`
    offers two instance variables, `intercept_` and `coef_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As for any other model, a prediction can be obtained through the method `predict(X)`.
    As an experiment, we can try to add some Gaussian noise to our training data and
    predict the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s obvious that the model is not performing in an ideal way and there are
    many possible reasons, the foremost being nonlinearities and the presence of outliers.
    However, in general, a linear regression model is not a perfectly robust solution.
    In Hastie T., Tibshirani R., Friedman J., *The Elements of Statistical Learning:
    Data Mining, Inference, and, Prediction*, Springer, you can find a very detailed
    discussion about its strengths and weaknesses. However, in this context, a common
    threat is represented by collinearities that lead to low-rank *X* matrix. This
    determines an ill-conditioned matrix that is particularly sensitive to noise,
    causing the explosion of some parameters as well. The following methods have been
    studied in order to mitigate this risk and provide more robust solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge, Lasso, and ElasticNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ridge** regression imposes an additional shrinkage penalty to the ordinary
    least squares loss function to limit its squared *L2* norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97b9c114-646c-42e9-bbfc-de728b2df441.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, *X* is a matrix containing all samples as columns and the term *w* represents
    the weight vector. The additional term (through the coefficient alpha—if large
    it implies a stronger regularization and smaller values) forces the loss function
    to disallow an infinite growth of *w*, which can be caused by multicollinearity
    or ill-conditioning. In the following figure, there''s a representation of what
    happens when a Ridge penalty is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b9f8364-acdf-4dce-b9ec-22231b2ef2f1.png)'
  prefs: []
  type: TYPE_IMG
- en: The gray surface represents the loss function (here, for simplicity, we're working
    with only two weights), while the circle center **O** is the boundary imposed
    by the Ridge condition. The minimum will have smaller *w* values and potential
    explosions are avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, we''re going to compare `LinearRegression` and `Ridge`with
    a cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, finding the right value for alpha (Ridge coefficient) is not so
    immediate. scikit-learn provides the class `RidgeCV`, which allows performing
    an automatic grid search (among a set and returning the best estimation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A Lasso regressor imposes a penalty on the *L1* norm of *w* to determine a
    potentially higher number of null coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52b950db-dba6-47b3-97f5-f7fa04526062.png)'
  prefs: []
  type: TYPE_IMG
- en: The sparsity is a consequence of the penalty term (the mathematical proof is
    non-trivial and will be omitted).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35ad9c79-5d86-4ef4-83b2-a86d468b4677.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, there are vertices where a component is non-null while all the
    other weights are zero. The probability of an intersection with a vertex is proportional
    to the dimensionality of *w* and, therefore, it's normal to discover a rather
    sparse model after training a Lasso regressor.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, the diabetes dataset is used to fit a Lasso model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Also for Lasso, there''s the possibility of running a grid search for the best
    alpha parameter. The class, in this case, is `LassoCV` and its internal dynamics
    are similar to what was already seen for Ridge. Lasso can also perform efficiently
    on the sparse data generated through the `scipy.sparse` class, allowing for training
    bigger models without the need for partial fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When working with a huge amount of data, some models cannot fit completely in
    memory, so it's impossible to train them. scikit-learn offers some models, such
    as **stochastic gradient descent** (**SGD**), which work in a way quite similar
    to `LinearRegression` with `Ridge`/`Lasso`; however, they also implement the method
    `partial_fit()`, which also allows continuous training through Python generators.
    See [http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd](http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd),
    for further details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last alternative is **ElasticNet**, which combines both Lasso and Ridge
    into a single model with two penalty factors: one proportional to *L1* norm and
    the other to *L2* norm. In this way, the resulting model will be sparse like a
    pure Lasso, but with the same regularization ability as provided by Ridge. The
    resulting loss function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08b661fa-86da-4f1c-a9df-7f6648a4da0e.png)'
  prefs: []
  type: TYPE_IMG
- en: The `ElasticNet` class provides an implementation where the alpha parameter
    works in conjunction with `l1_ratio` (beta in the formula**)**. The main peculiarity
    of `ElasticNet` is avoiding a selective exclusion of correlated features, thanks
    to the balanced action of the *L1* and *L2* norms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, there''s an example using both the `ElasticNet` and
    `ElasticNetCV` classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Robust regression with random sample consensus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common problem with linear regressions is caused by the presence of outliers.
    An ordinary least square approach will take them into account and the result (in
    terms of coefficients) will be therefore biased. In the following figure, there''s
    an example of such a behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/758d76d5-e5cb-4d23-a38b-f0de93d9bce3.png)'
  prefs: []
  type: TYPE_IMG
- en: The less sloped line represents an acceptable regression which discards the
    outliers, while the other one is influenced by them. An interesting approach to
    avoid this problem is offered by **random sample consensus** (**RANSAC**), which
    works with every regressor by subsequent iterations, after splitting the dataset
    into inliers and outliers. The model is trained only with valid samples (evaluated
    internally or through the callable `is_data_valid()`) and all samples are re-evaluated
    to verify if they're still inliers or they have become outliers. The process ends
    after a fixed number of iterations or when the desired score is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In the following snippet, there's an example of simple linear regression applied
    to the dataset shown in the previous figure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As imagined, the slope is high due to the presence of outliers. The resulting
    regressor is *y = 5.5 + 2.5x* (slightly less sloped than what was shown in the
    figure). Now we''re going to use RANSAC with the same linear regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the regressor is about *y = 2 + x* (which is the original clean
    dataset without outliers).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to have further information, I suggest visiting the page [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html).
    For other robust regression techniques, visit: [http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors](http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors).
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomial regression is a technique based on a trick that allows using linear
    models even when the dataset has strong non-linearities. The idea is to add some
    extra variables computed from the existing ones and using (in this case) only
    polynomial combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47b8dff1-28fb-46b7-abbc-b976d7678923.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, with two variables, it''s possible to extend to a second-degree
    problem by transforming the initial vector (whose dimension is equal to *m*) into
    another one with higher dimensionality (whose dimension is *k* > *m*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64a917dd-dd97-4221-b9a3-57c62b96b9a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the model remains externally linear, but it can capture internal
    non-linearities. To show how scikit-learn implements this technique, let''s consider
    the dataset shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/287dbe08-4340-4462-90d5-0067ccc84196.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is clearly a non-linear dataset, and any linear regression based only
    on the original two-dimensional points cannot capture the dynamics. Just to try,
    we can train a simple model (testing it on the same dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Performances are poor, as expected. However, looking at the figure, we might
    suppose that a quadratic regression could easily solve this problem. scikit-learn
    provides the class `PolynomialFeatures`, which transforms an original set into
    an expanded one according to the parameter `degree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the old *x[1]* coordinate has been replaced by a triplet, which
    also contains the quadratic and mixed terms. At this point, a linear regression
    model can be trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The score is quite higher and the only price we have paid is an increase in
    terms of features. In general, this is feasible; however, if the number grows
    over an accepted threshold, it's useful to try a dimensionality reduction or,
    as an extreme solution, to move to a non-linear model (such as SVM-Kernel). Usually,
    a good approach is using the class `SelectFromModel` to let scikit-learn select
    the best features based on their importance. In fact, when the number of features
    increases, the probability that all of them have the same importance gets lower.
    This is the result of mutual correlation or of the co-presence of major and minor
    trends, which act like noise and don't have the strength to alter perceptibility
    the hyperplane slope. Moreover, when using a polynomial expansion, some weak features (that
    cannot be used for a linear separation) are substituted by their functions and
    so the actual number of strong features decreases.
  prefs: []
  type: TYPE_NORMAL
- en: In the following snippet, there's an example with the previous Boston dataset.
    The `threshold` parameter is used to set a minimum importance level. If missing,
    the class will try to maximize the efficiency by removing the highest possible
    number of features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After selecting only the best features (with the threshold set to 10), the
    score remains the same, with a consistent dimensionality reduction (only 8 features
    are considered important for the prediction). If, after any other processing step,
    it''s necessary to return to the original dataset, it''s possible to use the inverse
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Isotonic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are situations when we need to find a regressor for a dataset of non-decreasing
    points which can present low-level oscillations (such as noise). A linear regression
    can easily achieve a very high score (considering that the slope is about constant),
    but it works like a denoiser, producing a line that can''t capture the internal
    dynamics we''d like to model. For these situations, scikit-learn offers the class
    `IsotonicRegression`, which produces a piecewise interpolating function minimizing
    the functional:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8b669d-6385-4b37-b4dd-c1dca9d761a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example (with a toy dataset) is provided next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a plot of the dataset. As everyone can see, it can be easily modeled
    by a linear regressor, but without a high non-linear function, it is very difficult
    to capture the slight (and local) modifications in the slope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/f992fdee-7f5b-4403-acd3-8ce8a3de7b28.png)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class `IsotonicRegression` needs to know *y[min]* and *y[max]* (which correspond
    to the variables *y*[*0* ]and *y*[*n* ]in the loss function). In this case, we
    impose -6 and 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is provided through three instance variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The last one, `(ir.f_)`, is an interpolating function which can be evaluated
    in the domain [*x[min]*, *x[max]*]. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of this function (the green line), together with the original data set,
    is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b30ebc1-8ec9-489a-961b-877ba107af1d.png)'
  prefs: []
  type: TYPE_IMG
- en: For further information about interpolation with SciPy, visit [https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hastie T., Tibshirani R., Friedman J., *The Elements of Statistical Learning:
    Data Mining, Inference, and, Prediction*, Springer'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have introduced the important concepts of linear models
    and have described how linear regression works. In particular, we focused on the
    basic model and its main variants: Lasso, Ridge, and ElasticNet. They don''t modify
    the internal dynamics but work as normalizers for the weights, in order to avoid
    common problems when the dataset contains unscaled samples. These penalties have
    specific peculiarities. While Lasso promotes sparsity, Ridge tries to find a minimum
    with the constraints that the weights must lay on a circle centered at the origin
    (whose radius is parametrized to increase/decrease the normalization strength).
    ElasticNet is a mix of both these techniques and it tries to find a minimum where
    the weights are small enough and a certain degree of sparsity is achieved.'
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed advanced techniques such as RANSAC, which allows coping with
    outliers in a very robust way, and polynomial regression, which is a very smart
    way to include virtual non-linear features into our model and continue working
    with them with the same linear approach. In this way, it's possible to create
    another dataset, containing the original columns together with polynomial combinations
    of them. This new dataset can be used to train a linear regression model, and
    then it's possible to select only those features that contributed towards achieving
    good performances. The last method we saw was isotonic regression, which is particularly
    useful when the function to interpolate is always not decreasing. Moreover it
    can capture the small oscillations that would be flattened by a generic linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to discuss some linear models for classifications.
    In particular, we'll focus our attention on the logistic regression and stochastic
    gradient descent algorithms. Moreover, we're going to introduce some useful metrics
    to evaluate the accuracy of a classification system, and a powerful technique
    to automatically find the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
