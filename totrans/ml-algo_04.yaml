- en: Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear models are the simplest parametric methods and always deserve the right
    attention, because many problems, even intrinsically non-linear ones, can be easily
    solved with these models. As discussed previously, a regression is a prediction
    where the target is continuous and its applications are several, so it's important
    to understand how a linear model can fit the data, what its strengths and weaknesses
    are, and when it's preferable to pick an alternative. In the last part of the
    chapter, we're going to discuss an interesting method to work efficiently with
    non-linear data using the same models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型是最简单的参数化方法，始终值得适当的关注，因为许多问题，甚至本质上是非线性的，都可以用这些模型轻松解决。正如之前讨论的那样，回归是一种预测，其中目标值是连续的，其应用范围很广，因此了解线性模型如何拟合数据，其优势和劣势是什么，以及在什么情况下选择替代方案更可取，是很重要的。在章节的最后部分，我们将讨论一种有趣的方法，使用相同的模型有效地处理非线性数据。
- en: Linear models
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'Consider a dataset of real-values vectors:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个真实值向量的数据集：
- en: '![](img/c11132d3-2d0a-4829-bf9a-1f0683bc1b54.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c11132d3-2d0a-4829-bf9a-1f0683bc1b54.png)'
- en: 'Each input vector is associated with a real value y[i]:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入向量都与一个实数值 y[i] 相关联：
- en: '![](img/8d55285c-b14f-4500-90ec-b71cc653fc91.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8d55285c-b14f-4500-90ec-b71cc653fc91.png)'
- en: 'A linear model is based on the assumption that it''s possible to approximate
    the output values through a regression process based on the rule:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型基于这样的假设：可以通过基于以下规则的回归过程来近似输出值：
- en: '![](img/7b1cdea2-df22-4389-92b9-4555648cc184.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7b1cdea2-df22-4389-92b9-4555648cc184.png)'
- en: In other words, the strong assumption is that our dataset and all other unknown
    points lie on a hyperplane and the maximum error is proportional to both the training
    quality and the adaptability of the original dataset. One of the most common problems
    arises when the dataset is clearly non-linear and other models have to be considered
    (such as neural networks or kernel support vector machines).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，强烈的假设是，我们的数据集和所有其他未知点都位于一个超平面上，最大误差与训练质量和原始数据集的适应性成正比。当数据集明显是非线性的，并且必须考虑其他模型（如神经网络或核支持向量机）时，最常见的问题之一就会出现。
- en: A bidimensional example
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个二维示例
- en: 'Let''s consider a small dataset built by adding some uniform noise to the points
    belonging to a segment bounded between -6 and 6\. The original equation is: *y
    = x + 2 + n*, where n is a noise term.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个由向 -6 和 6 之间的一段添加一些均匀噪声的点构建的小数据集。原始方程是：*y = x + 2 + n*，其中 n 是噪声项。
- en: 'In the following figure, there''s a plot with a candidate regression function:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，有一个候选回归函数的图表：
- en: '![](img/0b861a0a-9b14-4014-b4d7-d96af0867025.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b861a0a-9b14-4014-b4d7-d96af0867025.png)'
- en: 'As we''re working on a plane, the regressor we''re looking for is a function
    of only two parameters:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在平面上工作，我们寻找的回归器仅是两个参数的函数：
- en: '![](img/0b0783e1-6cda-43b0-819e-e068cf5de528.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b0783e1-6cda-43b0-819e-e068cf5de528.png)'
- en: 'In order to fit our model, we must find the best parameters and to do that
    we choose an ordinary least squares approach. The loss function to minimize is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合我们的模型，我们必须找到最佳参数，为此我们选择普通最小二乘法。要最小化的损失函数是：
- en: '![](img/2978dc42-6e7b-443e-baf2-834395af5fa1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2978dc42-6e7b-443e-baf2-834395af5fa1.png)'
- en: 'With an analytic approach, in order to find the global minimum, we must impose:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解析方法，为了找到全局最小值，我们必须施加：
- en: '![](img/0805d7ec-87ec-4aaa-9351-c83d29be9651.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0805d7ec-87ec-4aaa-9351-c83d29be9651.png)'
- en: 'So (for simplicity, it accepts a vector containing both variables):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此（为了简单起见，它接受包含两个变量的向量的向量）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And the gradient can be defined as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度可以定义为：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The optimization problem can now be solved using SciPy:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用 SciPy 解决优化问题：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As expected, the regression denoised our dataset, rebuilding the original equation:
    *y = x + 2*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，回归去噪了我们的数据集，重建了原始方程：*y = x + 2*。
- en: Linear regression with scikit-learn and higher dimensionality
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 和更高维度的线性回归
- en: 'scikit-learn offers the class `LinearRegression`, which works with n-dimensional
    spaces. For this purpose, we''re going to use the Boston dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了 `LinearRegression` 类，它适用于 n 维空间。为此，我们将使用波士顿数据集：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It has 506 samples with 13 input features and one output. In the following
    figure, there'' a collection of the plots of the first 12 features:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它有 506 个样本，13 个输入特征和一个输出。在下面的图中，有一组前 12 个特征的图表：
- en: '![](img/34b5f0a3-0aba-4aff-a89a-d92a089bb68b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34b5f0a3-0aba-4aff-a89a-d92a089bb68b.png)'
- en: When working with datasets, it's useful to have a tabular view to manipulate
    data. pandas is a perfect framework for this task, and even though it's beyond
    the scope of this book, I suggest you create a data frame with the command `pandas.DataFrame(boston.data,
    columns=boston.feature_names)` and use Jupyter to visualize it. For further information,
    refer to Heydt M., *Learning pandas - Python Data Discovery and Analysis Made
    Easy*, Packt.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据集时，有一个表格视图来操作数据是非常有用的。pandas是这个任务的完美框架，尽管这超出了本书的范围，但我建议你使用命令`pandas.DataFrame(boston.data,
    columns=boston.feature_names)`创建一个数据框，并使用Jupyter来可视化它。有关更多信息，请参阅Heydt M.的《Learning
    pandas - Python Data Discovery and Analysis Made Easy》，Packt出版社。
- en: 'There are different scales and outliers (which can be removed using the methods
    studied in the previous chapters), so it''s better to ask the model to normalize
    the data before processing it. Moreover, for testing purposes, we split the original
    dataset into training (90%) and test (10%) sets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同的尺度异常值（可以使用前几章研究的方法去除），因此最好让模型在处理数据之前对数据进行归一化。此外，为了测试目的，我们将原始数据集分为训练集（90%）和测试集（10%）：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When the original data set isn''t large enough, splitting it into training
    and test sets may reduce the number of samples that can be used for fitting the
    model. k-fold cross-validation can help in solving this problem with a different
    strategy. The whole dataset is split into k folds using always k-1 folds for training
    and the remaining one to validate the model. K iterations will be performed, using
    always a different validation fold. In the following figure, there''s an example
    with 3 folds/iterations:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当原始数据集不够大时，将其分为训练集和测试集可能会减少可用于拟合模型的样本数量。k折交叉验证可以通过不同的策略帮助解决这个问题。整个数据集被分为k个部分，始终使用k-1个部分进行训练，剩余的一个用于验证模型。将执行k次迭代，每次使用不同的验证部分。在以下图中，有一个3个部分/迭代的示例：
- en: '![](img/818fee35-73a6-482c-9733-cf5b1b2570bf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/818fee35-73a6-482c-9733-cf5b1b2570bf.png)'
- en: In this way, the final score can be determined as average of all values and
    all samples are selected for training k-1 times.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，最终得分可以确定为所有值和所有样本的平均值，并且所有样本都会被用于训练k-1次。
- en: 'To check the accuracy of a regression, scikit-learn provides the internal method
    `score(X, y)` which evaluates the model on test data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查回归的准确性，scikit-learn提供了内部方法`score(X, y)`，它评估模型在测试数据上的表现：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So the overall accuracy is about 77%, which is an acceptable result considering
    the non-linearity of the original dataset, but it can be also influenced by the
    subdivision made by `train_test_split`(like in our case). Instead, for k-fold
    cross-validation, we can use the function `cross_val_score()`, which works with
    all the classifiers. The scoring parameter is very important because it determines
    which metric will be adopted for tests. As `LinearRegression` works with ordinary
    least squares, we preferred the negative mean squared error, which is a cumulative
    measure that must be evaluated according to the actual values (it's not relative).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整体准确率约为77%，考虑到原始数据集的非线性，这是一个可接受的结果，但它也可能受到`train_test_split`所做的细分（如我们的情况）的影响。相反，对于k折交叉验证，我们可以使用`cross_val_score()`函数，它适用于所有分类器。评分参数非常重要，因为它决定了将采用哪个指标进行测试。由于`LinearRegression`使用普通最小二乘法，我们更喜欢负均方误差，这是一个累积度量，必须根据实际值进行评估（它不是相对的）。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Another very important metric used in regressions is called the **coefficient
    of determination** or *R²*. It measures the amount of variance on the prediction
    which is explained by the dataset. We define **residuals**, the following quantity:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中使用的另一个非常重要的指标被称为**确定系数**或*R²*。它衡量了由数据集解释的预测中的方差量。我们定义**残差**，以下数量：
- en: '![](img/b1e61431-300d-4745-b697-7fde495ae484.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1e61431-300d-4745-b697-7fde495ae484.png)'
- en: 'In other words, it is the difference between the sample and the prediction.
    So the *R²* is defined as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它是样本与预测之间的差异。因此，*R²*被定义为以下：
- en: '![](img/79b4dfc9-5c5c-4f67-bdf8-e4aa967ad90f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79b4dfc9-5c5c-4f67-bdf8-e4aa967ad90f.png)'
- en: 'For our purposes, *R²* values close to 1 mean an almost perfect regression,
    while values close to 0 (or negative) imply a bad model. Using this metric is
    quite easy with cross-validation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，接近1的*R²*值意味着几乎完美的回归，而接近0（或负数）的值意味着模型不好。使用这个指标进行交叉验证相当简单：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Regressor analytic expression
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归分析表达式
- en: 'If we want to have an analytical expression of our model (a hyperplane), `LinearRegression`
    offers two instance variables, `intercept_` and `coef_`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要得到我们模型（一个超平面）的解析表达式，`LinearRegression` 提供了两个实例变量，`intercept_` 和 `coef_`：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As for any other model, a prediction can be obtained through the method `predict(X)`.
    As an experiment, we can try to add some Gaussian noise to our training data and
    predict the value:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何其他模型，可以通过 `predict(X)` 方法获得预测。作为一个实验，我们可以尝试向我们的训练数据添加一些高斯噪声并预测值：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It''s obvious that the model is not performing in an ideal way and there are
    many possible reasons, the foremost being nonlinearities and the presence of outliers.
    However, in general, a linear regression model is not a perfectly robust solution.
    In Hastie T., Tibshirani R., Friedman J., *The Elements of Statistical Learning:
    Data Mining, Inference, and, Prediction*, Springer, you can find a very detailed
    discussion about its strengths and weaknesses. However, in this context, a common
    threat is represented by collinearities that lead to low-rank *X* matrix. This
    determines an ill-conditioned matrix that is particularly sensitive to noise,
    causing the explosion of some parameters as well. The following methods have been
    studied in order to mitigate this risk and provide more robust solutions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '很明显，模型的表现并不理想，有许多可能的原因，最显著的是非线性性和异常值的存在。然而，一般来说，线性回归模型并不是一个完美的鲁棒解决方案。在 Hastie
    T.，Tibshirani R.，Friedman J. 的 *The Elements of Statistical Learning: Data Mining,
    Inference, and, Prediction*，Springer 一书中，你可以找到关于其优势和劣势的非常详细的讨论。然而，在这个上下文中，一个常见的威胁是由共线性引起的，这会导致低秩
    *X* 矩阵。这决定了病态矩阵，它对噪声特别敏感，导致某些参数的爆炸。为了减轻这种风险并提供更鲁棒的解决方案，已经研究了以下方法。'
- en: Ridge, Lasso, and ElasticNet
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 岭回归、Lasso 和 ElasticNet
- en: '**Ridge** regression imposes an additional shrinkage penalty to the ordinary
    least squares loss function to limit its squared *L2* norm:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**岭回归**在普通最小二乘损失函数上施加额外的收缩惩罚，以限制其平方 *L2* 范数：'
- en: '![](img/97b9c114-646c-42e9-bbfc-de728b2df441.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/97b9c114-646c-42e9-bbfc-de728b2df441.png)'
- en: 'In this case, *X* is a matrix containing all samples as columns and the term *w* represents
    the weight vector. The additional term (through the coefficient alpha—if large
    it implies a stronger regularization and smaller values) forces the loss function
    to disallow an infinite growth of *w*, which can be caused by multicollinearity
    or ill-conditioning. In the following figure, there''s a representation of what
    happens when a Ridge penalty is applied:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*X* 是一个包含所有样本作为列的矩阵，而项 *w* 代表权重向量。通过系数 alpha 的附加项（如果很大，则意味着更强的正则化和更小的值），迫使损失函数不允许
    *w* 无限增长，这可能是由于多重共线性或病态性引起的。在下面的图中，展示了应用岭惩罚时发生的情况：
- en: '![](img/8b9f8364-acdf-4dce-b9ec-22231b2ef2f1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b9f8364-acdf-4dce-b9ec-22231b2ef2f1.png)'
- en: The gray surface represents the loss function (here, for simplicity, we're working
    with only two weights), while the circle center **O** is the boundary imposed
    by the Ridge condition. The minimum will have smaller *w* values and potential
    explosions are avoided.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 灰色表面表示损失函数（这里，为了简单起见，我们只使用两个权重），而圆心 **O** 是由岭条件强加的边界。最小值将具有较小的 *w* 值，并避免了潜在的爆炸。
- en: 'In the following snippet, we''re going to compare `LinearRegression` and `Ridge`with
    a cross-validation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们将使用交叉验证比较 `LinearRegression` 和 `Ridge`：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Sometimes, finding the right value for alpha (Ridge coefficient) is not so
    immediate. scikit-learn provides the class `RidgeCV`, which allows performing
    an automatic grid search (among a set and returning the best estimation):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，找到 alpha（岭系数）的正确值并不那么直接。scikit-learn 提供了 `RidgeCV` 类，允许执行自动网格搜索（在一系列值中返回最佳估计）：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A Lasso regressor imposes a penalty on the *L1* norm of *w* to determine a
    potentially higher number of null coefficients:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 回归器对 *w* 的 *L1* 范数施加惩罚，以确定一个可能更高的零系数数量：
- en: '![](img/52b950db-dba6-47b3-97f5-f7fa04526062.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/52b950db-dba6-47b3-97f5-f7fa04526062.png)'
- en: The sparsity is a consequence of the penalty term (the mathematical proof is
    non-trivial and will be omitted).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性是惩罚项的结果（数学证明非平凡，将省略）。
- en: '![](img/35ad9c79-5d86-4ef4-83b2-a86d468b4677.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/35ad9c79-5d86-4ef4-83b2-a86d468b4677.png)'
- en: In this case, there are vertices where a component is non-null while all the
    other weights are zero. The probability of an intersection with a vertex is proportional
    to the dimensionality of *w* and, therefore, it's normal to discover a rather
    sparse model after training a Lasso regressor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，存在一些顶点，其中某个组件非空，而所有其他权重为零。与顶点相交的概率与权重向量 *w* 的维度成正比，因此，在训练 Lasso 回归器后，发现一个相当稀疏的模型是正常的。
- en: 'In the following snippet, the diabetes dataset is used to fit a Lasso model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，使用了糖尿病数据集来拟合 Lasso 模型：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Also for Lasso, there''s the possibility of running a grid search for the best
    alpha parameter. The class, in this case, is `LassoCV` and its internal dynamics
    are similar to what was already seen for Ridge. Lasso can also perform efficiently
    on the sparse data generated through the `scipy.sparse` class, allowing for training
    bigger models without the need for partial fitting:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Lasso，也有运行网格搜索以找到最佳 alpha 参数的可能性。在这种情况下，类是 `LassoCV`，其内部动态与之前看到的 Ridge 类似。Lasso
    还可以在通过 `scipy.sparse` 类生成的稀疏数据上高效运行，从而允许在不进行部分拟合的情况下训练更大的模型：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When working with a huge amount of data, some models cannot fit completely in
    memory, so it's impossible to train them. scikit-learn offers some models, such
    as **stochastic gradient descent** (**SGD**), which work in a way quite similar
    to `LinearRegression` with `Ridge`/`Lasso`; however, they also implement the method
    `partial_fit()`, which also allows continuous training through Python generators.
    See [http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd](http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd),
    for further details.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量数据时，一些模型无法完全装入内存，因此无法训练它们。scikit-learn 提供了一些模型，例如 **随机梯度下降**（**SGD**），其工作方式与
    `LinearRegression` 结合 `Ridge`/`Lasso` 非常相似；然而，它们还实现了 `partial_fit()` 方法，这也允许通过
    Python 生成器进行连续训练。有关更多详细信息，请参阅 [http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd](http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd)。
- en: 'The last alternative is **ElasticNet**, which combines both Lasso and Ridge
    into a single model with two penalty factors: one proportional to *L1* norm and
    the other to *L2* norm. In this way, the resulting model will be sparse like a
    pure Lasso, but with the same regularization ability as provided by Ridge. The
    resulting loss function is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种选择是 **ElasticNet**，它将 Lasso 和 Ridge 结合成一个具有两个惩罚因子的单一模型：一个与 *L1* 范数成正比，另一个与
    *L2* 范数成正比。这样，得到的模型将像纯 Lasso 一样稀疏，但具有 Ridge 提供的相同的正则化能力。结果损失函数为：
- en: '![](img/08b661fa-86da-4f1c-a9df-7f6648a4da0e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08b661fa-86da-4f1c-a9df-7f6648a4da0e.png)'
- en: The `ElasticNet` class provides an implementation where the alpha parameter
    works in conjunction with `l1_ratio` (beta in the formula**)**. The main peculiarity
    of `ElasticNet` is avoiding a selective exclusion of correlated features, thanks
    to the balanced action of the *L1* and *L2* norms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`ElasticNet` 类提供了一个实现，其中 alpha 参数与 `l1_ratio`（公式中的 beta）一起工作。`ElasticNet` 的主要特性是避免由于
    *L1* 和 *L2* 范数的平衡作用而导致的特征选择性排除。'
- en: 'In the following snippet, there''s an example using both the `ElasticNet` and
    `ElasticNetCV` classes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，使用了 `ElasticNet` 和 `ElasticNetCV` 类的示例：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Robust regression with random sample consensus
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有随机样本一致性的鲁棒回归
- en: 'A common problem with linear regressions is caused by the presence of outliers.
    An ordinary least square approach will take them into account and the result (in
    terms of coefficients) will be therefore biased. In the following figure, there''s
    an example of such a behavior:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的一个常见问题是由于异常值的存在。普通最小二乘法将考虑它们，因此结果（就系数而言）将因此有偏。以下图示了这种行为的一个例子：
- en: '![](img/758d76d5-e5cb-4d23-a38b-f0de93d9bce3.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/758d76d5-e5cb-4d23-a38b-f0de93d9bce3.png)'
- en: The less sloped line represents an acceptable regression which discards the
    outliers, while the other one is influenced by them. An interesting approach to
    avoid this problem is offered by **random sample consensus** (**RANSAC**), which
    works with every regressor by subsequent iterations, after splitting the dataset
    into inliers and outliers. The model is trained only with valid samples (evaluated
    internally or through the callable `is_data_valid()`) and all samples are re-evaluated
    to verify if they're still inliers or they have become outliers. The process ends
    after a fixed number of iterations or when the desired score is achieved.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率较小的线代表一个可接受的回归，它忽略了异常值，而另一条线则受到异常值的影响。为了避免这个问题，**随机样本一致性**（**RANSAC**）提供了一种有趣的方法，它通过迭代与每个回归器一起工作，在将数据集分为内点和异常值之后。模型仅使用有效样本（通过内部评估或通过可调用的
    `is_data_valid()`）进行训练，并且所有样本都会重新评估以验证它们是否仍然是内点或已变成异常值。过程在固定次数的迭代后结束，或者当达到所需的分数时。
- en: In the following snippet, there's an example of simple linear regression applied
    to the dataset shown in the previous figure.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，有一个简单线性回归应用于前一个图中所示数据集的例子。
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As imagined, the slope is high due to the presence of outliers. The resulting
    regressor is *y = 5.5 + 2.5x* (slightly less sloped than what was shown in the
    figure). Now we''re going to use RANSAC with the same linear regressor:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，由于异常值的存在，斜率很高。得到的回归器是 *y = 5.5 + 2.5x*（比图中所显示的斜率略小）。现在我们将使用RANSAC与相同的线性回归器：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this case, the regressor is about *y = 2 + x* (which is the original clean
    dataset without outliers).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，回归器大约是 *y = 2 + x*（这是原始的无异常值清洁数据集）。
- en: If you want to have further information, I suggest visiting the page [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html).
    For other robust regression techniques, visit: [http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors](http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多信息，我建议访问页面 [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html)。对于其他鲁棒回归技术，请访问：[http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors](http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors)。
- en: Polynomial regression
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: 'Polynomial regression is a technique based on a trick that allows using linear
    models even when the dataset has strong non-linearities. The idea is to add some
    extra variables computed from the existing ones and using (in this case) only
    polynomial combinations:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是一种基于技巧的技术，即使在数据集具有强烈的非线性时也能使用线性模型。其思路是添加一些从现有变量计算得出的额外变量，并仅使用（在这种情况下）多项式组合：
- en: '![](img/47b8dff1-28fb-46b7-abbc-b976d7678923.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/47b8dff1-28fb-46b7-abbc-b976d7678923.png)'
- en: 'For example, with two variables, it''s possible to extend to a second-degree
    problem by transforming the initial vector (whose dimension is equal to *m*) into
    another one with higher dimensionality (whose dimension is *k* > *m*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于两个变量，可以通过将初始向量（其维度等于 *m*）转换为另一个具有更高维度的向量（其维度为 *k* > *m*）来扩展到二次问题：
- en: '![](img/64a917dd-dd97-4221-b9a3-57c62b96b9a6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/64a917dd-dd97-4221-b9a3-57c62b96b9a6.png)'
- en: 'In this case, the model remains externally linear, but it can capture internal
    non-linearities. To show how scikit-learn implements this technique, let''s consider
    the dataset shown in the following figure:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型在形式上保持线性，但它可以捕捉内部非线性。为了展示scikit-learn如何实现这一技术，让我们考虑以下图中的数据集：
- en: '![](img/287dbe08-4340-4462-90d5-0067ccc84196.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/287dbe08-4340-4462-90d5-0067ccc84196.png)'
- en: 'This is clearly a non-linear dataset, and any linear regression based only
    on the original two-dimensional points cannot capture the dynamics. Just to try,
    we can train a simple model (testing it on the same dataset):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个非线性数据集，仅基于原始二维点的线性回归无法捕捉其动态。为了尝试，我们可以在同一数据集上训练一个简单的模型（对其进行测试）：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Performances are poor, as expected. However, looking at the figure, we might
    suppose that a quadratic regression could easily solve this problem. scikit-learn
    provides the class `PolynomialFeatures`, which transforms an original set into
    an expanded one according to the parameter `degree`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 性能如预期的那样较差。然而，从图中看，我们可能会认为二次回归可以轻易解决这个问题。scikit-learn提供了`PolynomialFeatures`类，该类根据`degree`参数将原始集转换为一个扩展集：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As expected, the old *x[1]* coordinate has been replaced by a triplet, which
    also contains the quadratic and mixed terms. At this point, a linear regression
    model can be trained:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，旧的*x[1]*坐标已被一个三元组所取代，其中也包含了二次和混合项。在此阶段，可以训练一个线性回归模型：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The score is quite higher and the only price we have paid is an increase in
    terms of features. In general, this is feasible; however, if the number grows
    over an accepted threshold, it's useful to try a dimensionality reduction or,
    as an extreme solution, to move to a non-linear model (such as SVM-Kernel). Usually,
    a good approach is using the class `SelectFromModel` to let scikit-learn select
    the best features based on their importance. In fact, when the number of features
    increases, the probability that all of them have the same importance gets lower.
    This is the result of mutual correlation or of the co-presence of major and minor
    trends, which act like noise and don't have the strength to alter perceptibility
    the hyperplane slope. Moreover, when using a polynomial expansion, some weak features (that
    cannot be used for a linear separation) are substituted by their functions and
    so the actual number of strong features decreases.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 得分相当高，我们付出的唯一代价是特征数量的增加。一般来说，这是可行的；然而，如果数量超过一个可接受的阈值，尝试维度降低或作为一个极端解决方案，转向非线性模型（如SVM核）是有用的。通常，一个好的方法是使用`SelectFromModel`类，让scikit-learn根据它们的重要性选择最佳特征。实际上，当特征数量增加时，所有特征都具有相同重要性的概率会降低。这是由于相互相关性或主要和次要趋势的共同存在，它们像噪声一样没有足够的力量改变超平面斜率的可感知性。此外，当使用多项式展开时，一些弱特征（不能用于线性分离）被它们的函数所替代，因此实际上的强特征数量减少了。
- en: In the following snippet, there's an example with the previous Boston dataset.
    The `threshold` parameter is used to set a minimum importance level. If missing,
    the class will try to maximize the efficiency by removing the highest possible
    number of features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，有一个使用先前波士顿数据集的示例。`threshold`参数用于设置最小重要性级别。如果缺失，分类器将尝试通过移除尽可能多的特征来最大化效率。
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After selecting only the best features (with the threshold set to 10), the
    score remains the same, with a consistent dimensionality reduction (only 8 features
    are considered important for the prediction). If, after any other processing step,
    it''s necessary to return to the original dataset, it''s possible to use the inverse
    transformation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅选择最佳特征（阈值设置为10）后，得分保持不变，维度降低保持一致（只有8个特征被认为对预测很重要）。如果在任何其他处理步骤之后需要返回到原始数据集，可以使用逆变换：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Isotonic regression
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等调回归
- en: 'There are situations when we need to find a regressor for a dataset of non-decreasing
    points which can present low-level oscillations (such as noise). A linear regression
    can easily achieve a very high score (considering that the slope is about constant),
    but it works like a denoiser, producing a line that can''t capture the internal
    dynamics we''d like to model. For these situations, scikit-learn offers the class
    `IsotonicRegression`, which produces a piecewise interpolating function minimizing
    the functional:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要为非递减点的数据集找到一个回归器，这些点可能呈现低级振荡（如噪声）。线性回归可以轻易实现一个非常高的得分（考虑到斜率大约是恒定的），但它像一个降噪器，产生一条无法捕捉我们想要建模的内部动态的线。对于这些情况，scikit-learn提供了`IsotonicRegression`类，该类产生一个分段插值函数，最小化函数：
- en: '![](img/9f8b669d-6385-4b37-b4dd-c1dca9d761a0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9f8b669d-6385-4b37-b4dd-c1dca9d761a0.png)'
- en: 'An example (with a toy dataset) is provided next:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例（使用一个玩具数据集）将提供：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Following is a plot of the dataset. As everyone can see, it can be easily modeled
    by a linear regressor, but without a high non-linear function, it is very difficult
    to capture the slight (and local) modifications in the slope:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是数据集的图。正如大家所看到的，它可以很容易地被线性回归器建模，但没有一个高非线性函数，很难捕捉到斜率上的微小（和局部的）变化：
- en: '[![](img/f992fdee-7f5b-4403-acd3-8ce8a3de7b28.png)]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/f992fdee-7f5b-4403-acd3-8ce8a3de7b28.png)'
- en: 'The class `IsotonicRegression` needs to know *y[min]* and *y[max]* (which correspond
    to the variables *y*[*0* ]and *y*[*n* ]in the loss function). In this case, we
    impose -6 and 10:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`IsotonicRegression`类需要知道*y[min]*和*y[max]*（它们对应于损失函数中的*y[*0*]和*y[*n*]变量）。在这种情况下，我们设定为-6和10：'
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result is provided through three instance variables:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果通过三个实例变量提供：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The last one, `(ir.f_)`, is an interpolating function which can be evaluated
    in the domain [*x[min]*, *x[max]*]. For example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个，`(ir.f_)`，是一个插值函数，可以在[*x[min]*, *x[max]*]域内评估。例如：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A plot of this function (the green line), together with the original data set,
    is shown in the following figure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了该函数（绿色线）与原始数据集的图：
- en: '![](img/0b30ebc1-8ec9-489a-961b-877ba107af1d.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b30ebc1-8ec9-489a-961b-877ba107af1d.png)'
- en: For further information about interpolation with SciPy, visit [https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于SciPy插值的信息，请访问 [https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html](https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html)。
- en: References
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Hastie T., Tibshirani R., Friedman J., *The Elements of Statistical Learning:
    Data Mining, Inference, and, Prediction*, Springer'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Hastie T.，Tibshirani R.，Friedman J.，《统计学习的要素：数据挖掘、推理和预测》，Springer
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have introduced the important concepts of linear models
    and have described how linear regression works. In particular, we focused on the
    basic model and its main variants: Lasso, Ridge, and ElasticNet. They don''t modify
    the internal dynamics but work as normalizers for the weights, in order to avoid
    common problems when the dataset contains unscaled samples. These penalties have
    specific peculiarities. While Lasso promotes sparsity, Ridge tries to find a minimum
    with the constraints that the weights must lay on a circle centered at the origin
    (whose radius is parametrized to increase/decrease the normalization strength).
    ElasticNet is a mix of both these techniques and it tries to find a minimum where
    the weights are small enough and a certain degree of sparsity is achieved.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了线性模型的重要概念，并描述了线性回归的工作原理。特别是，我们关注了基本模型及其主要变体：Lasso、Ridge和ElasticNet。它们不修改内部动态，但作为权重的正规化器，以避免当数据集包含未缩放的样本时的常见问题。这些惩罚具有特定的特性。Lasso促进稀疏性，Ridge试图在权重必须位于以原点为中心的圆上（其半径被参数化以增加/减少正规化强度）的约束下找到最小值。ElasticNet是这两种技术的混合，它试图找到权重足够小且达到一定稀疏度的最小值。
- en: We also discussed advanced techniques such as RANSAC, which allows coping with
    outliers in a very robust way, and polynomial regression, which is a very smart
    way to include virtual non-linear features into our model and continue working
    with them with the same linear approach. In this way, it's possible to create
    another dataset, containing the original columns together with polynomial combinations
    of them. This new dataset can be used to train a linear regression model, and
    then it's possible to select only those features that contributed towards achieving
    good performances. The last method we saw was isotonic regression, which is particularly
    useful when the function to interpolate is always not decreasing. Moreover it
    can capture the small oscillations that would be flattened by a generic linear
    regression.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了诸如RANSAC等高级技术，它以非常鲁棒的方式处理异常值，以及多项式回归，这是一种将虚拟非线性特征包含到我们的模型中并继续以相同线性方法处理它们的非常智能的方法。通过这种方式，可以创建另一个数据集，包含原始列及其多项式组合。这个新数据集可以用来训练线性回归模型，然后可以选择那些有助于实现良好性能的特征。我们看到的最后一种方法是等距回归，当插值函数始终不是递减时特别有用。此外，它还可以捕捉到由通用线性回归平滑的小振荡。
- en: In the next chapter, we're going to discuss some linear models for classifications.
    In particular, we'll focus our attention on the logistic regression and stochastic
    gradient descent algorithms. Moreover, we're going to introduce some useful metrics
    to evaluate the accuracy of a classification system, and a powerful technique
    to automatically find the best hyperparameters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些用于分类的线性模型。特别是，我们将关注逻辑回归和随机梯度下降算法。此外，我们将介绍一些有用的度量来评估分类系统的准确性，以及一种强大的技术来自动找到最佳超参数。
