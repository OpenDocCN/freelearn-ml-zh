<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer100" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-106"><a id="_idTextAnchor168" class="calibre6 pcalibre pcalibre1"/>5</h1>
<h1 id="_idParaDest-107" class="calibre5"><a id="_idTextAnchor169" class="calibre6 pcalibre pcalibre1"/>Building Custom ML Models on Google Cloud</h1>
<p class="calibre3">In the last chapter, we implemented AI/ML workloads by letting Google do all of the work for us. Now is the point at which we’re going to elevate our knowledge and skills to an expert level by building our own models from scratch on <span>Google Cloud.</span></p>
<p class="calibre3">We will use popular software libraries that are commonly used in data science projects, and we will start implementing some of the concepts we discussed in previous chapters, such as <strong class="bold">unsupervised ML</strong> (<strong class="bold">UML</strong>) and <strong class="bold">supervised ML</strong> (<strong class="bold">SML</strong>), including clustering, regression, <span>and classification.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Background information – <span>libraries</span></li>
<li class="calibre8">UML with scikit-learn on <span>Vertex AI</span></li>
<li class="calibre8">Implementing a regression model with scikit-learn on <span>Vertex AI</span></li>
<li class="calibre8">Implementing a classification model with XGBoost on <span>Vertex AI</span></li>
</ul>
<h1 id="_idParaDest-108" class="calibre5"><a id="_idTextAnchor170" class="calibre6 pcalibre pcalibre1"/>Background information – libraries</h1>
<p class="calibre3">Before we dive<a id="_idIndexMarker510" class="calibre6 pcalibre pcalibre1"/> in and start building our own models, let’s take a moment to discuss some of the software libraries we will use in <span>this chapter.</span></p>
<p class="callout-heading">Definition</p>
<p class="callout">A software library is a collection of code and data that includes useful functions and tools for specific types of programming tasks. When common types of programming tasks are identified in a given industry, such as data manipulation or implementing complex mathematical equations, then usually somebody will eventually create a library that contains the code and other resources required to perform those tasks. The library can then easily be used by others to achieve those same tasks and potentially extended to add more functionality over time, rather than everybody needing to write the code to perform those common tasks over and over again. Without libraries, programmers would have to build everything from scratch all of the time and would waste a lot of time on rudimentary programming tasks. In this chapter, we will use libraries such as scikit-learn, Matplotlib, and XGBoost. Later in this book, we will use other libraries such as TensorFlow and PyTorch, and we will describe those libraries in more detail in their <span>respective chapters.</span></p>
<h2 id="_idParaDest-109" class="calibre9"><a id="_idTextAnchor171" class="calibre6 pcalibre pcalibre1"/>scikit-learn</h2>
<p class="calibre3">scikit-learn, also<a id="_idIndexMarker511" class="calibre6 pcalibre pcalibre1"/> referred to as <strong class="source-inline">sklearn</strong>, is an open source Python<a id="_idIndexMarker512" class="calibre6 pcalibre pcalibre1"/> library that has lots of useful data science tools and practice datasets built in. It’s like a “Swiss Army knife” for data science, and it’s a popular starting point for budding data scientists to begin working on ML projects because it’s relatively easy to start using, as you will see in <span>this chapter.</span></p>
<h2 id="_idParaDest-110" class="calibre9"><a id="_idTextAnchor172" class="calibre6 pcalibre pcalibre1"/>Matplotlib</h2>
<p class="calibre3">In <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, we discussed <a id="_idIndexMarker513" class="calibre6 pcalibre pcalibre1"/>typical stages that exist in almost all data science<a id="_idIndexMarker514" class="calibre6 pcalibre pcalibre1"/> projects, and how one of those stages focuses on data exploration. We also discussed that “visualization” is typically an important part of the data exploration phase, in which data scientists and engineers use tools to create visual representations of the characteristics of their datasets. These visual representations, such as graphs, can help data scientists and engineers build a better understanding of the datasets, as well as how those datasets are affected by experiments and transformations that are performed during data science projects. Data scientists and engineers also often want to build visualizations that represent other aspects of their data science project activities, such as increases or decreases in metrics that help determine how a model is performing. In the words of Matplotlib’s developers, “<em class="italic">Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.</em>” As such, Matplotlib is a commonly used library for creating visual representations <span>of data.</span></p>
<h2 id="_idParaDest-111" class="calibre9"><a id="_idTextAnchor173" class="calibre6 pcalibre pcalibre1"/>pandas</h2>
<p class="calibre3">pandas is a Python<a id="_idIndexMarker515" class="calibre6 pcalibre pcalibre1"/> library for data manipulation and analysis. It is<a id="_idIndexMarker516" class="calibre6 pcalibre pcalibre1"/> used for both data exploration and for performing transformations on data. For example, with pandas, we could read data in from a file or other data source, preview a subset of the data to understand what it contains, and perform statistical analysis on the dataset. Then, we could also use pandas to make changes to the data to prepare it for training an <span>ML model.</span></p>
<h2 id="_idParaDest-112" class="calibre9"><a id="_idTextAnchor174" class="calibre6 pcalibre pcalibre1"/>XGBoost</h2>
<p class="calibre3">In <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, we discussed <a id="_idIndexMarker517" class="calibre6 pcalibre pcalibre1"/>how the concept of Gradient Descent is <a id="_idIndexMarker518" class="calibre6 pcalibre pcalibre1"/>commonly used in the ML model training process. XGBoost is a popular ML library that is based on the concept <a id="_idIndexMarker519" class="calibre6 pcalibre pcalibre1"/>of <strong class="bold">Gradient Boosting</strong>, which uses an <strong class="bold">ensemble</strong> approach that combines many small models (often referred<a id="_idIndexMarker520" class="calibre6 pcalibre pcalibre1"/> to as <strong class="bold">weak learners</strong>) to create a better overall prediction model. In the case of Gradient Boosting, each iteration in the training process trains a small model. And, as is the case in almost every model training process, the resulting model will make some incorrect predictions. The next iteration in the Gradient Boosting process then trains a model on those <strong class="bold">residual errors</strong> made by the previous model. This helps to “boost” the training process in each subsequent iteration, with the intention of creating a stronger prediction model overall. XGBoost, which stands for Extreme Gradient Boosting, overcomes the sequential limitations of previous Gradient Boosting algorithms, and it can train thousands of weak learners in parallel. We will describe how XGBoost works in more detail later in <span>this chapter.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In general, the concept of “ensemble” model training refers to the combination of many weak models to create a better or “stronger” overall prediction mechanism, often referred to as a “meta-model.” Boosting is just one example of an ensemble approach. Other examples include “Bagging” (or “Bootstrap Aggregation”), which trains many weak learners on subsets of the data, and “Stacking,” which can be used to combine models trained with completely different algorithms. The intent, in each case, is to build a more useful prediction mechanism than could be achieved by any of the <span>models individually.</span></p>
<p class="callout">Depending on the ensemble implementation, the predictions of each model in the ensemble could be combined in different ways, such as being summed or averaged, or in classification use cases, a voting mechanism may be implemented in order to determine<a id="_idIndexMarker521" class="calibre6 pcalibre pcalibre1"/> the <a id="_idIndexMarker522" class="calibre6 pcalibre pcalibre1"/>resulting <span>best prediction.</span></p>
<h1 id="_idParaDest-113" class="calibre5"><a id="_idTextAnchor175" class="calibre6 pcalibre pcalibre1"/>Prerequisites for this chapter</h1>
<p class="calibre3">Just like in the previous chapter, we will perform some initial activities that are required before we can start to perform the primary activities in <span>this chapter.</span></p>
<p class="calibre3">Google Cloud makes it even easier to get started with the data science libraries we described in the previous sections because we can use Vertex AI Workbench notebooks that already have these libraries installed. If you wanted to use these libraries outside of Vertex AI, you would need to install <span>them yourself.</span></p>
<p class="calibre3">Considering that we’re going to use Vertex AI notebooks, and we will not need to explicitly install the libraries, we will not include details on how to do that in this book. If you would like to install the<a id="_idIndexMarker523" class="calibre6 pcalibre pcalibre1"/> libraries in another environment, including <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>) or <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), you<a id="_idIndexMarker524" class="calibre6 pcalibre pcalibre1"/> can find installation instructions on the respective websites for each of those libraries, <span>such as:</span></p>
<ul class="calibre16">
<li class="calibre8"><a href="https://scikit-learn.org/stable/install.xhtml" class="calibre6 pcalibre pcalibre1"><span>https://scikit-learn.org/stable/install.xhtml</span></a></li>
<li class="calibre8"><a href="https://matplotlib.org/stable/users/installing/index.xhtml" class="calibre6 pcalibre pcalibre1"><span>https://matplotlib.org/stable/users/installing/index.xhtml</span></a></li>
<li class="calibre8"><a href="https://pandas.pydata.org/docs/getting_started/install.xhtml" class="calibre6 pcalibre pcalibre1"><span>https://pandas.pydata.org/docs/getting_started/install.xhtml</span></a></li>
<li class="calibre8"><a href="https://xgboost.readthedocs.io/en/stable/install.xhtml" class="calibre6 pcalibre pcalibre1"><span>https://xgboost.readthedocs.io/en/stable/install.xhtml</span></a></li>
</ul>
<p class="calibre3">Next, we will discuss Vertex AI Workbench notebooks in a bit more detail, and how to create <span>a notebook.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Later in this chapter, you will see pieces of code that import these libraries. This is not the same as installing. The libraries are already installed in our Vertex AI Workbench notebooks, but we just need to import them into our notebook context in order to <span>use them.</span></p>
<h2 id="_idParaDest-114" class="calibre9"><a id="_idTextAnchor176" class="calibre6 pcalibre pcalibre1"/>Vertex AI Workbench</h2>
<p class="calibre3">Vertex AI Workbench<a id="_idIndexMarker525" class="calibre6 pcalibre pcalibre1"/> is a development environment in Google Cloud that enables you to manage all of your AI/ML development needs within Vertex AI. It is based on Jupyter notebooks, which provide an interactive interface for writing and running code. This makes notebooks extremely versatile because you can codify interactions not only with the broader Vertex AI ecosystem but also with other Google Cloud <span>service APIs.</span></p>
<h3 class="calibre11">Vertex AI Workbench notebooks</h3>
<p class="calibre3">There are three types of<a id="_idIndexMarker526" class="calibre6 pcalibre pcalibre1"/> notebooks you can use within Vertex <span>AI Workbench:</span></p>
<ul class="calibre16">
<li class="calibre8">Managed notebooks, which, as the name suggests, are managed for you by Google. These are a good default option for many use cases because they have a lot of useful tools built in and are ready to go on Google Cloud. They run in a JupyterLab environment, which is a web-based interactive development environment <span>for notebooks.</span></li>
<li class="calibre8">User-managed notebooks, which, as the name suggests, are managed by you. These are suitable if we require significant customization of our environment. They are customizable instances of Google Cloud’s Deep Learning VMs, which are <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) images<a id="_idIndexMarker527" class="calibre6 pcalibre pcalibre1"/> that include tools for<a id="_idIndexMarker528" class="calibre6 pcalibre pcalibre1"/> implementing <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) workloads. However, these notebooks can be used for more than just DL <span>use cases.</span></li>
<li class="calibre8">Vertex AI Workbench <strong class="bold">instances</strong>, which are the newest option released by Google Cloud in late 2023. These can be seen as a hybrid of the previous two options, and will likely become the primary option over time. At the time of writing this in September 2023, this option is only available in preview mode, so we will stick with <em class="italic">options 1</em> and <em class="italic">2</em> <span>for now.</span></li>
</ul>
<p class="calibre3">Google also provides an offering called Colab, which is a very popular service that allows you to run free notebooks over the public internet. In late 2023, Google Cloud also released an option named Colab Enterprise, which enables customers to use Colab within their own Google Cloud environment. At the time of writing <a id="_idIndexMarker529" class="calibre6 pcalibre pcalibre1"/>this, Colab Enterprise is also only available in <span>preview mode.</span></p>
<p class="calibre3">Managed notebooks and user-managed notebooks are being deprecated, so in this book we will primarily use the newest option: Vertex AI Workbench instances. Let's go ahead and create <span>one now.</span></p>
<h2 id="_idParaDest-115" class="calibre9"><a id="_idTextAnchor177" class="calibre6 pcalibre pcalibre1"/>Creating a Vertex AI Workbench instance</h2>
<p class="calibre3">To <a id="_idIndexMarker530" class="calibre6 pcalibre pcalibre1"/>create a <a id="_idIndexMarker531" class="calibre6 pcalibre pcalibre1"/>Vertex AI Workbench instance, perform the <span>following steps:</span></p>
<p class="callout-heading">Note </p>
<p class="callout">For configuration parameters that are not specifically called out in these instructions, leave those parameters at their <span>default values</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the Google Cloud services menu → <strong class="bold">Vertex AI</strong> → <span><strong class="bold">Workbench</strong></span><span>.</span></li>
<li class="calibre8">Select the <strong class="bold">Instances</strong> tab and click <span><strong class="bold">Create New</strong></span><span>.</span></li>
<li class="calibre8">In the side panel that appears (see <span><em class="italic">Figure 5</em></span><em class="italic">.1</em> for reference), enter a name for your notebook. Alternatively, you can use the default name that’s automatically generated in that <span>input field:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer084">
<img alt="Figure 5.1: The New instance dialogue" src="image/B18143_05_001.jpg" class="calibre89"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.1: The New instance dialogue</p>
<ol class="calibre7">
<li value="4" class="calibre8">Select <a id="_idIndexMarker532" class="calibre6 pcalibre pcalibre1"/>your <span>preferred </span><span><a id="_idIndexMarker533" class="calibre6 pcalibre pcalibre1"/></span><span>region.</span></li>
<li class="calibre8">Select the option to attach a GPU (at the time of writing, you should select the box that says <strong class="bold">Attach 1 NVIDIA T4 GPU</strong>). See <span><em class="italic">Figure 5</em></span><em class="italic">.1</em> <span>for reference.</span></li>
<li class="calibre8">Leave the networking configuration details at their default values <span>for now.</span></li>
<li class="calibre8">At the bottom of the side panel, click <span><strong class="bold">Advanced Options</strong></span><span>.</span></li>
<li class="calibre8">In the <a id="_idIndexMarker534" class="calibre6 pcalibre pcalibre1"/>next screen that appears (see <span><em class="italic">Figure 5</em></span><em class="italic">.2</em> for reference), ensure that the <a id="_idIndexMarker535" class="calibre6 pcalibre pcalibre1"/>option to <strong class="bold">Enable Dataproc</strong> is selected and click <strong class="bold">Continue</strong> (you may need to scroll down to see the <span><strong class="bold">Continue</strong></span><span> button):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer085">
<img alt="Figure 5.2: The New instance dialogue (continued)" src="image/B18143_05_002.jpg" class="calibre90"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.2: The New instance dialogue (continued)</p>
<ol class="calibre7">
<li value="9" class="calibre8">On the next screen that appears (that is, the environment configuration screen), select <strong class="bold">Use the latest version</strong> and <span>click </span><span><strong class="bold">Continue</strong></span><span>.</span></li>
<li class="calibre8">On the next screen that appears, you can use all of the default values (unless you have preferences to change anything) and <span>click </span><span><strong class="bold">Continue</strong></span><span>.</span></li>
<li class="calibre8">You can click the <strong class="bold">Continue</strong> button two more times (that is, to accept the default values on the next two screens, unless you have preferences to change anything) until you reach the <strong class="bold">Networking</strong> <span>configuration screen.</span></li>
<li class="calibre8">On the <strong class="bold">Networking configuration</strong> screen, ensure that the <strong class="bold">Assign external IP address</strong> option <span>is selected.</span></li>
<li class="calibre8">Unless <a id="_idIndexMarker536" class="calibre6 pcalibre pcalibre1"/>you have any specific networking configuration needs, you can simply use <a id="_idIndexMarker537" class="calibre6 pcalibre pcalibre1"/>the default network in <span>your project.</span></li>
<li class="calibre8"><span>Click </span><span><strong class="bold">Continue</strong></span><span>.</span></li>
<li class="calibre8">On the next screen that appears (that is, the <strong class="bold">IAM &amp; Security</strong> screen), ensure that all the options in the <strong class="bold">Security options</strong> section <span>are selected.</span></li>
<li class="calibre8">Select <strong class="bold">CREATE</strong> (at the bottom of the screen) and wait a few minutes for the notebook to <span>be created.</span><p class="calibre3">When the notebook is up and running, a green checkmark will appear to the left of the <span>notebook’s name.</span></p></li>
<li class="calibre8">Finally, click <span><strong class="bold">Open JupyterLab</strong></span><span>.</span></li>
</ol>
<p class="calibre3">At this point, I’d like<a id="_idIndexMarker538" class="calibre6 pcalibre pcalibre1"/> to highlight <a id="_idIndexMarker539" class="calibre6 pcalibre pcalibre1"/>an important set of integrations and features that Google Cloud has added to the JupyterLab interface in Vertex <span>AI Notebooks.</span></p>
<div class="calibre10"/><h2 id="_idParaDest-116" class="calibre9"><a id="_idTextAnchor178" class="calibre6 pcalibre pcalibre1"/>Vertex AI Notebook JupyterLab integrations</h2>
<p class="calibre3">In the <a id="_idIndexMarker540" class="calibre6 pcalibre pcalibre1"/>JupyterLab interface in your Vertex <a id="_idIndexMarker541" class="calibre6 pcalibre pcalibre1"/>AI Notebook instance, you will notice a set of icons on the far left-hand side of the screen, as shown in <span><em class="italic">Figure 5</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer086">
<img alt="Figure 5.3: Google Cloud JupyterLab integrations" src="image/B18143_05_003.jpg" class="calibre91"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.3: Google Cloud JupyterLab integrations</p>
<p class="calibre3">These icons represent integrations that we can use directly within our JupyterLab Notebook instances in Vertex AI. For example, if we click on the BigQuery icon, we can see <a id="_idIndexMarker542" class="calibre6 pcalibre pcalibre1"/>our BigQuery datasets, and we can even use the integrated SQL Editor to run SQL queries on our BigQuery datasets directly <a id="_idIndexMarker543" class="calibre6 pcalibre pcalibre1"/>from the JupyterLab <span>Notebook interface.</span></p>
<p class="calibre3">I recommend clicking on the various icons to learn more about what you can do with them. Other useful features include the ability to integrate with Google Cloud Storage and an option that enables us to schedule the execution of our notebooks. The latter is a very useful feature for workloads that need to be automatically repeated periodically, which is a common need in data science (for example, re-training, evaluating, and deploying a model on new data <span>every day).</span></p>
<p class="calibre3">Now that the notebook instance has been created, we will clone our GitHub repository so that we can access our notebook code and follow along with the activities in <span>this chapter.</span></p>
<h2 id="_idParaDest-117" class="calibre9"><a id="_idTextAnchor179" class="calibre6 pcalibre pcalibre1"/>Cloning the GitHub repository</h2>
<p class="calibre3">Cloning our GitHub <a id="_idIndexMarker544" class="calibre6 pcalibre pcalibre1"/>repository is the easiest way to quickly import all of the resources for the hands-on activities in this chapter into your notebook instance in Google Cloud Vertex AI. To clone our repository into your notebook, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Click on the <strong class="bold">Git</strong> symbol in the menu on the left of the screen. The symbol will look like the one shown in <span><em class="italic">Figure 5</em></span><span><em class="italic">.4</em></span><span>:</span></li>
</ol>
<p class="calibre3"><img alt="Figure 5.4: Git symbol" src="image/B18143_05_004.png" class="calibre92"/></p>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.4: Git symbol</p>
<ol class="calibre7">
<li value="2" class="calibre8">Select <span><strong class="bold">Clone Repository</strong></span><span>.</span></li>
<li class="calibre8">Enter our repository <span>URL: </span><a href="https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects" class="calibre6 pcalibre pcalibre1"><span>https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects</span></a><span>.</span></li>
<li class="calibre8">If any options are displayed, leave them at their <span>default values.</span></li>
<li class="calibre8"><span>Select </span><span><strong class="bold">Clone</strong></span><span>.</span></li>
<li class="calibre8">You should see a new folder appear in your notebook, <span>named </span><span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Now, we’re ready to start using our notebook instance! Let’s move on to the next section, in which we will perform some <span>unsupervised training.</span></p>
<h1 id="_idParaDest-118" class="calibre5"><a id="_idTextAnchor180" class="calibre6 pcalibre pcalibre1"/>UML with scikit-learn on Vertex AI</h1>
<p class="calibre3">In this section, we <a id="_idIndexMarker545" class="calibre6 pcalibre pcalibre1"/>will start using our<a id="_idIndexMarker546" class="calibre6 pcalibre pcalibre1"/> Vertex AI Workbench notebook to train models. We will begin with a relatively simple use case in which we will create an unsupervised model to discover clustering patterns in our data. Before we dive into the code, we will first take a minute to discuss the clustering algorithm we will use in this section, which is <span>called K-means.</span></p>
<h2 id="_idParaDest-119" class="calibre9"><a id="_idTextAnchor181" class="calibre6 pcalibre pcalibre1"/>K-means</h2>
<p class="calibre3">You may remember<a id="_idIndexMarker547" class="calibre6 pcalibre pcalibre1"/> that we discussed <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>) mechanisms such as clustering in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. Remember <a id="_idIndexMarker548" class="calibre6 pcalibre pcalibre1"/>that in clustering, data points are grouped together based on similarities between features or characteristics that are observed by the model. <span><em class="italic">Figure 5</em></span><em class="italic">.5</em> provides a visual representation of this concept, showing the input data on the left and the resulting data clusters on <span>the right:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer088">
<img alt="Figure 5.5: Clustering" src="image/B18143_05_005.jpg" class="calibre93"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.5: Clustering</p>
<p class="calibre3">K-means is an example of a clustering algorithm, and it is categorized as a centroid-based clustering algorithm. What this means is that it chooses <a id="_idIndexMarker549" class="calibre6 pcalibre pcalibre1"/>a <strong class="bold">centroid</strong>, which is a point that represents the center of each of our clusters. The members of each cluster are data points from our dataset, and their membership in each cluster will depend on a mathematical evaluation of how close or far they are from each centroid. This proximity or distance from each centroid is generally calculated in terms of the Euclidean distance, represented by <span><em class="italic">Equation 5.1</em></span><span>:</span></p>
<p class="calibre3"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="102" src="image/11.png" width="430" class="calibre94"/></p>
<p class="img---caption" lang="en-US" xml:lang="en-US">Equation 5.1: Euclidean distance</p>
<p class="calibre3">Don’t worry—we will discuss this in more detail. The graphs shown in <span><em class="italic">Figure 5</em></span><em class="italic">.5 </em>represent what’s called the feature space, and they show where each of our data points exists in the feature space. In the case of the aforementioned graphs, they represent a two-dimensional feature space, and each data point has an <em class="italic">x</em> and <em class="italic">y</em> coordinate that represents where the data point exists in the feature space. That is to say, the <em class="italic">x</em> and <em class="italic">y</em> coordinates are the features of each data point. If we take any two points on the graph, the Euclidean distance is simply the distance in a straight line between those two points, which is calculated by putting the <em class="italic">x</em> and <em class="italic">y</em> coordinates (that is, the features of each data point) into the equation represented in <span><em class="italic">Equation 5.1</em></span><span>.</span></p>
<p class="calibre3">Data points can have more than <em class="italic">x</em> and <em class="italic">y</em> coordinates as their features, and the concept applies in higher-dimensional feature <span>spaces also.</span></p>
<p class="calibre3">Let’s take a more concrete example than just <em class="italic">x</em> and <em class="italic">y</em> coordinates. Our dataset could consist of information regarding a retail company’s customers, and the features of each customer could include their age, the city they live in, and what they purchased in their most recent visit to the company’s store. Clustering algorithms such as K-means may then group those<a id="_idIndexMarker550" class="calibre6 pcalibre pcalibre1"/> customers by finding similarities between their features and the centroids in <span>each group.</span></p>
<p class="calibre3">One of the first questions that often comes up when discussing K-means is: How are centroids chosen? For example, how does the algorithm know how many centroids to use, and where to place them in the feature space? Let’s address the number of centroids first. This is specified as a “hyperparameter” to the algorithm. This means that you can tell the algorithm how many centroids to use (and therefore how many clusters to form). In fact, this is reflected in the name, “K-means,” where K represents the number of centroids or clusters. You would generally try different numbers of centroids until you find a value that maximizes the amount of information gained and minimizes the amount of variance, in each cluster. A mechanism that is often used to find the optimal value for K is referred to as “the elbow method.” Using the elbow method, we run K-means clustering on the dataset for a range of different values of K (such as 1-10). Then, for each value of K, the average score is computed for all clusters. The default computed score is <a id="_idIndexMarker551" class="calibre6 pcalibre pcalibre1"/>called the <strong class="bold">distortion</strong>. The distortion represents the sum of the squared distances from each point to the centroid in their assigned cluster, which again relates back to <em class="italic">Equation 5.1</em>. If you graph the value of K against the distortion score for each run, you will usually notice the distortion score going down each time you increase the value of K. The distortion score will usually go down sharply in the beginning and eventually start to go down in smaller increments. When adding new clusters (that is, increasing the value of K) stops significantly reducing the distortion score, then you can usually consider the optimal value of K to have <span>been found.</span></p>
<p class="calibre3">Now, let’s discuss how the algorithm knows where to place the centroids in the feature space. It begins by placing them at random locations in the feature space and randomly assigning data points to each centroid. It then repeats the following steps until no new data points are added to each cluster, at which point the optimal cluster positioning is believed to have <span>been calculated:</span></p>
<ol class="calibre7">
<li class="calibre8">Calculate which data points are closest to the centroid and assign them to <span>that centroid.</span></li>
<li class="calibre8">Calculate the average (mean) between <span>those points.</span></li>
<li class="calibre8">Move <span>the centroid.</span></li>
</ol>
<p class="calibre3">Now that we’ve <a id="_idIndexMarker552" class="calibre6 pcalibre pcalibre1"/>covered the theory of how K-means clustering works, let’s move on to the fun part, which is to actually implement <span>the algorithm.</span></p>
<h2 id="_idParaDest-120" class="calibre9"><a id="_idTextAnchor182" class="calibre6 pcalibre pcalibre1"/>Implementing a UML workload in Vertex AI</h2>
<p class="calibre3">We’re going to <a id="_idIndexMarker553" class="calibre6 pcalibre pcalibre1"/>use the K-means algorithm within <a id="_idIndexMarker554" class="calibre6 pcalibre pcalibre1"/>scikit-learn to start implementing our UML workload in Vertex AI. One of the quintessential clustering examples using K-means in scikit-learn is to use what’s referred to as the iris dataset, to find cluster patterns in the data. The iris dataset, as the name implies, contains data on various <span>iris flowers.</span></p>
<p class="callout-heading">Business case</p>
<p class="callout">Our company, <em class="italic">BigFlowers.com</em>, recently acquired a smaller flower company, and in doing so, we acquired all of their digital assets, including the datasets that contain their flower inventory. Unfortunately, they did not do a good job of documenting their datasets, so we don’t have a good idea of what’s in <span>their inventory.</span></p>
<p class="callout">We’ve been tasked with finding out as much as we can about the contents of the flower inventory, so we’re going to use some data analytics tools and ML models to learn more about the dataset. One of the first things we’re going to do is try to find any patterns such as logical groupings, which could help us to understand if there are distinct categories of objects in the dataset and additional information such as how many distinct <span>categories exist.</span></p>
<p class="callout"><em class="italic">Note</em>: At the time of writing this, the company <em class="italic">BigFlowers.com</em> does not exist, and our exercises here refer to a fictitious company for example <span>purposes only.</span></p>
<p class="calibre3">To get started with this task, navigate into the <strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong> folder in the Vertex AI notebook you created in the previous section. Then, navigate into the <strong class="source-inline">Chapter-05</strong> folder, double click on the <strong class="source-inline">Chapter-5.ipynb</strong> file (if prompted, select the <strong class="source-inline">Python</strong> kernel), and perform the <span>following steps.</span></p>
<ol class="calibre7">
<li class="calibre8">The first thing we need to do in our notebook is to import the resources that we need, such as the scikit-learn K-means class, and the pandas and Matplotlib libraries. We <a id="_idIndexMarker555" class="calibre6 pcalibre pcalibre1"/>will<a id="_idIndexMarker556" class="calibre6 pcalibre pcalibre1"/> also import a function to load the iris dataset from scikit-learn. To perform these actions, enter the following code into the interactive prompt in our notebook (or use the notebook you cloned from GitHub) The following code in the notebook performs <span>those steps:</span><pre class="source-code">
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
import pandas as pd # for exploring our data
import matplotlib.pyplot as plt # for plotting our clusters
from mpl_toolkits.mplot3d import Axes3D # Specifically for creating a 3-D graph</pre></li> <li class="calibre8">To execute the code, hold the <em class="italic">Shift</em> key on your keyboard, and press the <em class="italic">Enter</em> key. Considering that the code is simply importing libraries, you will not see any output displayed back to the screen unless an <span>error occurs.</span></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">From here onward, when we are performing activities in Vertex AI Workbench notebooks, we will just provide the code samples for each step, and it will imply that you need to enter that code into the next available empty cell in the notebook (unless you are using the cloned notebook that already contains the code) and execute the cell, just as you did with the previous piece <span>of code.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">On the left side of each cell in a Jupyter notebook, there is a square bracket symbol that looks like this: <strong class="source-inline1">[ ]</strong></p>
<p class="callout">This can be used to understand the status of the cell, and the indicators are <span>as follows:</span></p>
<p class="callout"><strong class="source-inline1">[ ]</strong> (Empty): The cell has not yet <span>been executed.</span></p>
<p class="callout"><strong class="source-inline1">[*]</strong> (Asterisk): The cell is <span>currently executing.</span></p>
<p class="callout"><strong class="source-inline1">[1]</strong> (Any number): the cell has <span>completed executing.</span></p>
<ol class="calibre7">
<li value="3" class="calibre8">Next, we<a id="_idIndexMarker557" class="calibre6 pcalibre pcalibre1"/> will <a id="_idIndexMarker558" class="calibre6 pcalibre pcalibre1"/>read the iris <span>dataset in:</span><pre class="source-code">
# Load the iris dataset:
iris = load_iris()
# Assign the data to a variable so we can start to use it:
iris_data = iris.data</pre></li> <li class="calibre8">Let’s use pandas to get some information about <span>our dataset:</span><pre class="source-code">
# Convert the dataset to a pandas data frame for analysis:
iris_df = pd.DataFrame(iris_data)
# Use the info() function to get some information about the dataset
iris_df.info()</pre><p class="calibre3">The output should look like what is shown in <span><em class="italic">Figure 5</em></span><span><em class="italic">.6</em></span><span>:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer090">
<img alt="Figure 5.6: pandas.DataFrame.info() output" src="image/B18143_05_006.jpg" class="calibre95"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.6: pandas.DataFrame.info() output</p>
<ol class="calibre7">
<li value="5" class="calibre8">The <strong class="source-inline">info()</strong> function output shows us what kinds of data our dataset contains. In this case, we can see that it contains 150 rows (indexed from 0 to 149) and 4 columns (indexed from 0 to 3), and the data values in each cell are floating-point numbers. Each <a id="_idIndexMarker559" class="calibre6 pcalibre pcalibre1"/>row in our dataset is a <a id="_idIndexMarker560" class="calibre6 pcalibre pcalibre1"/>data point, which represents a particular iris flower, and each column is a feature in our dataset. If we look at the description for the iris dataset in the scikit-learn documentation, which can be found at <a href="https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset" class="calibre6 pcalibre pcalibre1">https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset</a>, we can see that the <span>features are:</span><ol class="calibre77"><li class="alphabets">Sepal length <span>in cm</span></li><li class="alphabets">Sepal width <span>in cm</span></li><li class="alphabets">Petal length <span>in cm</span></li><li class="alphabets">Petal width <span>in cm</span></li></ol><p class="calibre3">From this, we can understand that the floating-point numbers in each cell in our dataset are measurements of those four aspects of <span>each flower.</span></p></li>
<li class="calibre8">We can also preview a subset of the data to see the actual values in each cell, <span>like so:</span><pre class="source-code">
iris_df.head()</pre><p class="calibre3">The output<a id="_idIndexMarker561" class="calibre6 pcalibre pcalibre1"/> should<a id="_idIndexMarker562" class="calibre6 pcalibre pcalibre1"/> look like what is shown in <span><em class="italic">Figure 5</em></span><span><em class="italic">.7</em></span><span>:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer091">
<img alt="Figure 5.7: pandas.DataFrame.head() output" src="image/B18143_05_007.jpg" class="calibre96"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.7: pandas.DataFrame.head() output</p>
<ol class="calibre7">
<li value="7" class="calibre8">Now, we’re going to use K-means to group similar data points in the dataset together, based on their features. First, we’re creating an instance of a K-means model, and we’re specifying that it will have three clusters because in this case, the dataset documentation told us that there are three different categories of iris in our dataset. If we didn’t already know how many clusters we needed to use, then we could try different numbers of clusters and use the elbow method to find the <span>best number:</span><pre class="source-code">
kmeans_model = KMeans(n_clusters=3)</pre></li> <li class="calibre8">At this point, we have defined our model, but it has not yet learned anything from our data. Next, we instruct the model to “fit” to our dataset. The term <em class="italic">fit</em> is used by many algorithms to refer to the training process because, during training, this is exactly what the algorithm is trying to do; it is trying to create a model that fits as accurately as possible (without overfitting) to the <span>given dataset:</span><pre class="source-code">
kmeans_model.fit(iris_data)</pre></li> <li class="calibre8">When our model has completed the training process, we can now get it to cluster our data. We will feed our original dataset into the model, and according to the patterns it learned during training, it will place the data points into each of the clusters <span>it defined:</span><pre class="source-code">
kmeans_model.predict(iris_data)</pre></li> <li class="calibre8">Finally, we store the model’s labels in a variable so that we can visualize the clusters in the <span>next cell:</span><pre class="source-code">
labels = kmeans_model.labels_</pre></li> <li class="calibre8">Next, we will <a id="_idIndexMarker563" class="calibre6 pcalibre pcalibre1"/>visualize<a id="_idIndexMarker564" class="calibre6 pcalibre pcalibre1"/> the clusters that K-means <span>has created:</span><pre class="source-code">
# Create a figure object:
fig = plt.figure()
# Define the axes (note: the auto_add_to_figure option will default to False from mpl3.5 onwards):
axes = Axes3D(fig, auto_add_to_figure=False)
# Add the axes to the figure:
fig.add_axes(axes)
# Create the scatter plot to graph the outputs from our K-means model:
axes.scatter(iris_data[:, 2], iris_data[:, 3], iris_data[:, 1],
    c=labels.astype(float))
# Set the labels for the X, Y, and Z axes:
axes.set_xlabel("Petal length")
axes.set_ylabel("Petal width")
axes.set_zlabel("Sepal width")</pre></li> <li class="calibre8">The resulting graph should look similar to the graph depicted in <span><em class="italic">Figure 5</em></span><em class="italic">.8</em>. Notice how we can see three distinct clusters of data points in the graph, where each distinct cluster<a id="_idIndexMarker565" class="calibre6 pcalibre pcalibre1"/> is <a id="_idIndexMarker566" class="calibre6 pcalibre pcalibre1"/>color-coded as either purple, green, <span>or yellow:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer092">
<img alt="Figure 5.8: K-means cluster graph" src="image/B18143_05_008.jpg" class="calibre97"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.8: K-means cluster graph</p>
<p class="calibre3">This gives us some useful information. We can see the three distinct categories or clusters of data points in our dataset, in which the data points share similar characteristics with other points in their same cluster but differ from the data points in other clusters. We get these insights without ever needing to label our dataset, which is quite useful, considering that labeling is a time-consuming and <span>error-prone task.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Although our dataset contains four features (also referred to as “dimensions”), humans can only see/visualize things in three dimensions. Therefore, we only used three of the four features to create our graph. The following line is where we defined the features to use in our graph, in which the numbers represent the features to <span>be used:</span></p>
<p class="callout"><strong class="source-inline1">axes.scatter(iris_data[:, 2], iris_data[:, 3], iris_data[:, </strong><span><strong class="source-inline1">1], c=labels.astype(float))</strong></span></p>
<p class="callout">You can try playing around with the graph by changing each of those numbers to anything between 0 and 3, as long as each entry is a unique value (that is, don’t repeat any of the numbers more than once in <span>the code).</span></p>
<p class="callout">Each time you make a change, you can execute the cell again to update <span>the graph.</span></p>
<p class="calibre3">Now that we’ve <a id="_idIndexMarker567" class="calibre6 pcalibre pcalibre1"/>seen <a id="_idIndexMarker568" class="calibre6 pcalibre pcalibre1"/>what it takes to implement a UML workload in Vertex AI, let’s move on and learn how to implement an SML workload in the <span>next section.</span></p>
<h1 id="_idParaDest-121" class="calibre5"><a id="_idTextAnchor183" class="calibre6 pcalibre pcalibre1"/>Implementing a regression model with scikit-learn on Vertex AI</h1>
<p class="calibre3">The first SML model<a id="_idIndexMarker569" class="calibre6 pcalibre pcalibre1"/> we’re going<a id="_idIndexMarker570" class="calibre6 pcalibre pcalibre1"/> to build in<a id="_idIndexMarker571" class="calibre6 pcalibre pcalibre1"/> our Vertex AI Workbench notebook is a linear regression model. You may remember that we described linear regression in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a><span>.</span></p>
<p class="callout-heading">Business case</p>
<p class="callout">Our boss at <em class="italic">BigFlowers.com</em> is running a fun competition at work. Employees are asked to predict the length of an iris flower’s petal when given some other measurements related to that flower, such as the sepal length, sepal width, and petal width. The person with the most accurate estimation will get a big prize, and employees are allowed to use technology to help them in their estimations, so we’re going to build an ML model to help us make <span>these predictions.</span></p>
<p class="calibre3">In the previous section, we used the K-means algorithm within scikit-learn. In this section, we will use the linear regression algorithm within scikit-learn. As such, we will need to import the <strong class="source-inline">LinearRegression</strong> class into our notebook context. We will also import a function that will<a id="_idIndexMarker572" class="calibre6 pcalibre pcalibre1"/> calculate the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) metric, which we described in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a> and which is commonly used to evaluate linear <span>regression models.</span></p>
<p class="calibre3">We can use the same iris dataset for our linear regression model, so we won’t need to repeat any of the previous dataset importing steps because it is already loaded into our notebook context. This is an important concept to note; we can use different types of models on the<a id="_idIndexMarker573" class="calibre6 pcalibre pcalibre1"/> same <a id="_idIndexMarker574" class="calibre6 pcalibre pcalibre1"/>data, depending on the<a id="_idIndexMarker575" class="calibre6 pcalibre pcalibre1"/> business use case and desired results. However, as we discussed in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, SML model training introduces some additional requirements regarding how the dataset is used, which we <span>describe next.</span></p>
<p class="callout-heading">Remember</p>
<p class="callout">In <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>) use cases, we need to define which column in the dataset is <a id="_idIndexMarker576" class="calibre6 pcalibre pcalibre1"/>designated as the “target” feature. This is the feature that we’re trying to predict based on the other features in the dataset. During training, some of the elements in this column are used as labels that represent known, correct answers from which the <span>model learns.</span></p>
<p class="callout">The values of the other features in the dataset are used as inputs. During the prediction process, the model uses the relationships it has learned between all of the input features and the target feature to predict the value of the target feature based on the values of the <span>input features.</span></p>
<p class="callout">Also, remember that in SL use cases, we generally split our dataset into subsets such as training, validation, and testing. The training dataset, as the name implies, is what the model is trained on. The testing dataset is how we evaluate the trained model (based on the metrics that we’ve defined for that model), and the validation set is usually used in <span>hyperparameter tuning.</span></p>
<p class="callout-heading"><em class="italic">Note</em>: </p>
<p class="callout">We will explore hyperparameter tuning in a later chapter. In our current chapter, we will train a single regression model and we will then test it directly, so we will not need to split out a validation subset of our data. Therefore, we will just split the dataset into subsets for training <span>and testing.</span></p>
<p class="calibre3">To start building our linear regression model, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Import the <strong class="source-inline">LinearRegression</strong> class from scikit-learn and import a function that will make it <a id="_idIndexMarker577" class="calibre6 pcalibre pcalibre1"/>easy <a id="_idIndexMarker578" class="calibre6 pcalibre pcalibre1"/>for <a id="_idIndexMarker579" class="calibre6 pcalibre pcalibre1"/>us to split our dataset into train and test datasets, as well as a function that will calculate the MSE metric that we will use later to evaluate our <span>model’s performance:</span><pre class="source-code">
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split</pre></li> <li class="calibre8">Define which column in our dataset we want to use as the target feature <span>to predict:</span><pre class="source-code">
target = iris_df[[2]]</pre></li> </ol>
<p class="callout-heading">Remember:</p>
<p class="callout">In the previous section of this chapter, we consulted the documentation for the iris dataset, and we saw that the columns in the dataset are as follows: sepal length in cm, sepal width in cm, petal length in cm, petal width in cm. These columns are indexed from 0 to 3. Considering that we want to predict the petal length, we have selected column 2 of our <strong class="source-inline1">iris_df</strong> dataframe as our <span>target column.</span></p>
<ol class="calibre7">
<li value="3" class="calibre8">Next, define our input features. In this case, we use all feature columns except column 2 (because we’ve defined that column 2 is <span>our target):</span><pre class="source-code">
input_feats = iris_df[[0, 1, 3]]</pre></li> <li class="calibre8">Split our dataset into separate subsets for training <span>and testing:</span><pre class="source-code">
input_train, input_test, target_train, target_test = \
    train_test_split(input_feats,target,test_size=0.2)</pre></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">Specifying a value of 0.2 for the <strong class="source-inline1">test_size</strong> variable means that 20% of the original dataset will be separated to create the test dataset. The remaining 80% then forms the training dataset. The <strong class="source-inline1">input_train</strong> and <strong class="source-inline1">input_test</strong> datasets contain the input features used during training and the input features that will be used to test the trained model, respectively. The <strong class="source-inline1">target_train</strong> and <strong class="source-inline1">target_test</strong> datasets contain the respective labels (that is, the “correct answers”) for the training and <span>test datasets.</span></p>
<ol class="calibre7">
<li value="5" class="calibre8">Now, let’s use<a id="_idIndexMarker580" class="calibre6 pcalibre pcalibre1"/> the<a id="_idIndexMarker581" class="calibre6 pcalibre pcalibre1"/> training <a id="_idIndexMarker582" class="calibre6 pcalibre pcalibre1"/>dataset to train our linear regression model, and then use the test dataset to generate predictions from <span>the model:</span><pre class="source-code">
# Create an instance of a LinearRegression model
lreg_model = LinearRegression()
# Train the model by fitting it to the training data
lreg_model.fit(input_train,target_train)
# Use the test set to generate predictions
target_predictions = lreg_model.predict(input_test)</pre></li> </ol>
<p class="callout-heading">Remember</p>
<p class="callout">To test the trained model, we send the <strong class="source-inline1">input_test</strong> features to it, and we ask it to predict what the corresponding target feature values should be, based on the values of the <strong class="source-inline1">input_test</strong> features. We can then compare the model’s predictions to the <strong class="source-inline1">target_test</strong> values (which are the known, correct answers) in order to see how close its predictions were to the correct, <span>expected values.</span></p>
<ol class="calibre7">
<li value="6" class="calibre8">Finally, it’s time to see what kinds of predictions our model has made based on the <span><strong class="source-inline">input_test</strong></span><span> data!</span><pre class="source-code">
pred_df = pd.DataFrame(target_predictions[0:5])
pred_df.head()</pre><p class="calibre3">The output should look similar to what is shown in <span><em class="italic">Figure 5</em></span><em class="italic">.9</em>, although the numbers may differ (the predictions are in the right-hand column, disregarding the <a id="_idIndexMarker583" class="calibre6 pcalibre pcalibre1"/>number 0 at <a id="_idIndexMarker584" class="calibre6 pcalibre pcalibre1"/>the top, which <a id="_idIndexMarker585" class="calibre6 pcalibre pcalibre1"/>is the <span>column header):</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer093">
<img alt="Figure 5.9: Linear regression model predictions" src="image/B18143_05_009.jpg" class="calibre98"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.9: Linear regression model predictions</p>
<ol class="calibre7">
<li value="7" class="calibre8">Now, let’s take a look at the corresponding known, correct values from the <span><strong class="source-inline">target_test</strong></span><span> dataset:</span><pre class="source-code">
target_test.head()</pre><p class="calibre3">The output should look similar to what is shown in <span><em class="italic">Figure 5</em></span><em class="italic">.10</em>, although the numbers may differ (the values are in the right-hand column, disregarding the number 2 at the top, which is the <span>column header):</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer094">
<img alt="Figure 5.10: The known, correct values from the target_test dataset" src="image/B18143_05_010.jpg" class="calibre99"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.10: The known, correct values from the target_test dataset</p>
<ol class="calibre7">
<li value="8" class="calibre8">As we can see, the numbers are pretty close in some cases. However, let’s use metrics to evaluate the model’s overall performance. To do this, we use the <strong class="source-inline">mean_squared_error()</strong> function to compare the predictions against the correct values from the <strong class="source-inline">target_test</strong> dataset and generate the MSE <span>metric value:</span><pre class="source-code">
mean_squared_error(target_test,target_predictions)</pre></li> <li class="calibre8">The value should be something <span>like 0.08871798773326277.</span><p class="calibre3">This is a pretty good value because the error is quite low, which means that our model is doing a good job of predicting the target values. At this point, I think we might win that <a id="_idIndexMarker586" class="calibre6 pcalibre pcalibre1"/>big <a id="_idIndexMarker587" class="calibre6 pcalibre pcalibre1"/>prize in the <a id="_idIndexMarker588" class="calibre6 pcalibre pcalibre1"/>competition to predict the length of the <span>iris petals!</span></p></li>
</ol>
<p class="callout-heading">Diving deeper into the various datasets mentioned in the previous section</p>
<p class="callout">The <strong class="source-inline1">train_test_split</strong> function in our hands-on activity created the following subsets from our <span>source dataset:</span></p>
<p class="callout"><strong class="source-inline1">input_train</strong>: The input features used <span>during training.</span></p>
<p class="callout"><strong class="source-inline1">input_test</strong>: The input features that will be used to test the <span>trained model.</span></p>
<p class="callout"><strong class="source-inline1">target_train</strong>: The target labels used during training. During the training process, the model uses these values as the known, correct answers that it is trying to predict. These are key to the training process because, during training, the model tries to learn relationships between the input features that will help it predict these answers as accurately <span>as possible.</span></p>
<p class="callout"><strong class="source-inline1">target_test</strong>: The target labels used during testing. These are known, correct answers from the original dataset that were separated out from the training set, so they were not included in the training process. Therefore, the model has never seen these values during training. We then use these values to test the performance of the <span>trained model.</span></p>
<p class="calibre3">Now that we’ve seen<a id="_idIndexMarker589" class="calibre6 pcalibre pcalibre1"/> linear<a id="_idIndexMarker590" class="calibre6 pcalibre pcalibre1"/> regression in<a id="_idIndexMarker591" class="calibre6 pcalibre pcalibre1"/> action, let’s move on to our first SL <span>classification task.</span></p>
<h1 id="_idParaDest-122" class="calibre5"><a id="_idTextAnchor184" class="calibre6 pcalibre pcalibre1"/>Implementing a classification model with XGBoost on Vertex AI</h1>
<p class="calibre3">By now, you’ve<a id="_idIndexMarker592" class="calibre6 pcalibre pcalibre1"/> started to <a id="_idIndexMarker593" class="calibre6 pcalibre pcalibre1"/>become familiar with<a id="_idIndexMarker594" class="calibre6 pcalibre pcalibre1"/> many of the popular libraries that are commonly used in data science projects. In this section, we will start using another very popular library, XGBoost, which can be used for either classification or regression <span>use cases.</span></p>
<p class="calibre3">While we briefly introduced XGBoost at the beginning of this chapter, we will dive further into how it works here, starting with the concept of <span>decision trees.</span></p>
<h2 id="_idParaDest-123" class="calibre9"><a id="_idTextAnchor185" class="calibre6 pcalibre pcalibre1"/>Decision trees</h2>
<p class="calibre3">When we discussed<a id="_idIndexMarker595" class="calibre6 pcalibre pcalibre1"/> the topic of Gradient Boosting earlier in this chapter, we mentioned that one of the components of Gradient Boosting is the concept of weak learners. Decision trees are one example of what could be used as a weak learner. Let’s start with a simple example of what a decision tree is. Refer to <span><em class="italic">Figure 5</em></span><em class="italic">.11</em>, which shows a decision tree that is used for estimating whether a bank customer is likely to purchase a house, based on their age group <span>and income:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer095">
<img alt="Figure 5.11: A decision tree" src="image/B18143_05_011.jpg" class="calibre100"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.11: A decision tree</p>
<p class="calibre3">A decision tree consists of a sequence of decisions. The process starts at what’s referred to as the <strong class="bold">root node</strong>, and at<a id="_idIndexMarker596" class="calibre6 pcalibre pcalibre1"/> each point in the tree, a decision is made, which then guides us on the next step to take. The sequence of decisions therefore results in a path through the tree, until we reach a final point that contains no further decisions. Such a point is referred to as<a id="_idIndexMarker597" class="calibre6 pcalibre pcalibre1"/> a <strong class="bold">leaf node</strong>. Following the steps in the tree in <span><em class="italic">Figure 5</em></span><em class="italic">.11</em>, it determines that a bank customer is likely to purchase a house if they are between 25 years old and 65 years old and earn more than $50,000 per year. All of the other factors in the decision tree indicate that they are otherwise unlikely to purchase <span>a house.</span></p>
<p class="calibre3">While the example in <span><em class="italic">Figure 5</em></span><em class="italic">.11</em> is quite simple, this kind of process can be used algorithmically in ML applications, whereby the decision at each point is based on some kind of threshold. For example, if the value is less than or greater than a certain threshold, then move on to evaluate feature <em class="italic">A</em>; otherwise, evaluate feature <em class="italic">B</em> or stop evaluating features. When using decision trees, one of the goals is to find which features in the dataset can help make the best decisions and how many decisions need to be made, which is referred to as<a id="_idIndexMarker598" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">tree depth</strong>, in order to get to the best <span>eventual prediction.</span></p>
<p class="calibre3">Two key concepts in the decision tree process <a id="_idIndexMarker599" class="calibre6 pcalibre pcalibre1"/>are <strong class="bold">entropy</strong> and <strong class="bold">information gain</strong>. Entropy, in this<a id="_idIndexMarker600" class="calibre6 pcalibre pcalibre1"/> context, is the measure of impurity in a given grouping of entities. For example, a grouping that perfectly captures identical entities in the same group would have zero entropy, whereas a grouping that has lots of variability between the entities would have high entropy. Information gain, in this context, can be seen as how much the entropy is decreased by each decision in our decision tree. In our bank-customer example in <span><em class="italic">Figure 5</em></span><em class="italic">.11</em>, the initial set of customers (before any decisions are made) consists of all of our customers, and this set would have a lot of entropy because it would include people of all ages and all kinds of economic situations and <span>other characteristics.</span></p>
<p class="calibre3">Now, as we try to figure out which customer features could help our algorithm decide whether they would be likely to purchase a house, we find a pattern that suggests that people within a certain age group (let’s say, between 25 and 65) and who earn over a certain amount of money per year (let’s say, more than $50,000) are more likely to purchase a house than<a id="_idIndexMarker601" class="calibre6 pcalibre pcalibre1"/> other customers. Therefore, in this case, a customer’s age and income are examples of features that help to maximize the information gain and reduce the entropy and would therefore be good decision point features in our decision tree. On the other hand, features such as their name or music preferences are unlikely to have any correlation with their probability of purchasing a house, so those features would not result in significant information gain in the decision points in our decision tree, and the decision tree model would likely learn to ignore those features <span>during training.</span></p>
<p class="calibre3">Decision tree algorithms can be quite effective for some use cases, although they come with some limitations such as being prone to overfitting. This is where the concept of ensembles comes into play because combining many trees together can make a more powerful prediction model—and one that is also a lot less likely to overfit—than any individual tree by itself. This could be done by using a Bagging approach, such as the Random Forest algorithm (see <span><em class="italic">Figure 5</em></span><em class="italic">.12</em> for reference), which trains each tree in the ensemble on a random sub-sample (with replacement) from the training feature space, or by using a Boosting approach, as we described in the case of <span>Gradient Boosting:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer096">
<img alt="Figure 5.12: Random Forest" src="image/B18143_05_12.jpg" class="calibre101"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.12: Random Forest</p>
<p class="calibre3">When we use Gradient Boosting with decision trees, we refer to this as <strong class="bold">gradient-boosted trees</strong>. One <a id="_idIndexMarker602" class="calibre6 pcalibre pcalibre1"/>of the inherent challenges with a simple gradient-boosted tree implementation is the sequential nature of the algorithm, whereby the errors of a tree in one training iteration need to be used in the next iteration to train the next tree in the ensemble. However, as we mentioned earlier in this chapter, XGBoost overcomes this limitation, and it can <a id="_idIndexMarker603" class="calibre6 pcalibre pcalibre1"/>train thousands of trees in parallel, which vastly speeds up the overall <span>training time.</span></p>
<p class="calibre3">With this background information in mind, let’s see how we can use XGBoost to build a classification model in Vertex AI. The business case here does not require its own dedicated callout section because it’s pretty straightforward: our model will need to predict the class for each iris, based on its petal and <span>sepal measurements.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The iris dataset contains details about irises that fit into one of <span>three classes:</span></p>
<p class="callout"> - <span><strong class="source-inline1">Iris-Setosa</strong></span></p>
<p class="callout"> - <span><strong class="source-inline1">Iris-Versicolour</strong></span></p>
<p class="callout"> - <span><strong class="source-inline1">Iris-Virginica</strong></span></p>
<p class="callout">In our linear regression example in the previous section of this chapter, we decided to use the <strong class="source-inline1">petal length</strong> feature as the target that we were trying to predict. However, in this section, we will try to predict the class (or category) of each iris flower based on all of the other features in the dataset. Because there are more than two classes, this will be a multi-class <span>classification task.</span></p>
<p class="callout">The class is a new feature of the iris dataset that we will introduce in this section, and we have not interacted with this feature yet. Next, we will talk about how to access <span>this feature.</span></p>
<p class="callout">The iris dataset in scikit-learn contains multiple objects. One of those objects is the <strong class="source-inline1">data</strong> object, and that’s what we’ve been using so far in <span>this chapter.</span></p>
<p class="callout">Another object in the<a id="_idIndexMarker604" class="calibre6 pcalibre pcalibre1"/> dataset is the <strong class="source-inline1">target</strong> object, which contains the <strong class="source-inline1">class</strong> feature column. The <strong class="source-inline1">class</strong> feature column represents the classes <span>as follows:</span></p>
<p class="callout"><strong class="source-inline1">0 = </strong><span><strong class="source-inline1">Iris-Setosa</strong></span></p>
<p class="callout"><strong class="source-inline1">1 = </strong><span><strong class="source-inline1">Iris-Versicolour</strong></span></p>
<p class="callout"><strong class="source-inline1">2 = </strong><span><strong class="source-inline1">Iris-Virginica</strong></span></p>
<p class="callout">Remember the following two lines of code from earlier in <span>this chapter:</span></p>
<p class="callout"><strong class="source-inline1">iris = </strong><span><strong class="source-inline1">load_iris()</strong></span></p>
<p class="callout"><strong class="source-inline1">iris_data = </strong><span><strong class="source-inline1">iris.data</strong></span></p>
<p class="callout">Those lines of code loaded the iris dataset in and specifically assigned the <strong class="source-inline1">data</strong> object from that dataset to the <strong class="source-inline1">iris_data</strong> variable. We did not reference the <strong class="source-inline1">target </strong>object at that time because we did not need to <span>do so.</span></p>
<p class="callout">We will start using the <strong class="source-inline1">target</strong> object in this section, and in order to do so, we will assign it to a variable <span>named </span><span><strong class="source-inline1">iris_classes</strong></span><span>.</span></p>
<p class="calibre3">To train the classification<a id="_idIndexMarker605" class="calibre6 pcalibre pcalibre1"/> model, perform the following steps in your Vertex AI <span>Workbench notebook:</span></p>
<ol class="calibre7">
<li class="calibre8">Just as with the libraries we used in the previous section, we will need to import the XGBoost library before we can start using it. More specifically, we will import the <strong class="source-inline">XGBClassifier</strong> class from the <span><strong class="source-inline">XGBoost</strong></span><span> library:</span><pre class="source-code">
from xgboost import XGBClassifier</pre></li> <li class="calibre8">Considering that we will use it for a classification use case, we will also import a function that will calculate a metric that can be used to evaluate a classification model, which is <span>called </span><span><strong class="source-inline">accuracy</strong></span><span>:</span><pre class="source-code">
from sklearn.metrics import accuracy_score</pre></li> <li class="calibre8">Assign the <strong class="source-inline">target</strong> object from the iris dataset to the <strong class="source-inline">iris_classes</strong> variable so that we can begin to <span>reference it:</span><pre class="source-code">
iris_classes = iris.target</pre></li> <li class="calibre8">Create our dataset splits for training and testing, just like we did in our regression example in the <span>previous section:</span><pre class="source-code">
xgb_input_train,xgb_input_test,xgb_target_train,xgb_target_test=
    train_test_split(iris_data, iris_classes, test_size=.2)</pre></li> <li class="calibre8">Create a model instance and specify <span>the hyperparameters:</span><pre class="source-code">
xgbc = XGBClassifier(n_estimators=2, max_depth=2, 
    learning_rate=1, objective='multi:softmax')</pre></li> </ol>
<p class="callout-heading">Diving deeper</p>
<p class="callout">The hyperparameters we’re specifying in the previous piece of code represent <span>the following:</span></p>
<p class="callout"><strong class="source-inline1">n_estimators</strong>: The number of decision trees to use in <span>the ensemble.</span></p>
<p class="callout"><strong class="source-inline1">max_depth</strong>: The maximum number of decision points (or maximum depth) for <span>each tree.</span></p>
<p class="callout"><strong class="source-inline1">learning_rate</strong>: The learning rate to use during the optimization of the <span>cost function.</span></p>
<p class="callout"><strong class="source-inline1">objective</strong>: The type of prediction we want our model to perform. In this case, we want it to use the <strong class="source-inline1">softmax</strong> function to perform <span>multi-class classification.</span></p>
<ol class="calibre7">
<li value="6" class="calibre8">Let’s train <span>our model:</span><pre class="source-code">
xgbc.fit(xgb_input_train, xgb_target_train)</pre><p class="calibre3">You will see an output similar to what is shown in <span><em class="italic">Figure 5</em></span><em class="italic">.13</em>, which summarizes the <a id="_idIndexMarker606" class="calibre6 pcalibre pcalibre1"/>values (including default values) of the <span>model’s parameters:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer097">
<img alt="Figure 5.13: XGBoost parameters" src="image/B18143_05_013.jpg" class="calibre102"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.13: XGBoost parameters</p>
<ol class="calibre7">
<li value="7" class="calibre8">Now, it’s time to get predictions from our model by using the <span><strong class="source-inline">xgb_input_test</strong></span><span> dataset:</span><pre class="source-code">
xgb_predictions = xgbc.predict(xgb_input_test)</pre></li> <li class="calibre8">Let’s see what it predicted for each item in the <span><strong class="source-inline">xgb_input_test</strong></span><span> dataset:</span><pre class="source-code">
xgb_predictions</pre><p class="calibre3">The output should be an array of predicted classes, which are represented by either 0, 1, or 2, similar to the output shown in <span><em class="italic">Figure 5</em></span><em class="italic">.14</em>, although the values you get <span>may differ:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer098">
<img alt="Figure 5.14: XGBoost iris classification predictions" src="image/B18143_05_014.jpg" class="calibre103"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.14: XGBoost iris classification predictions</p>
<ol class="calibre7">
<li value="9" class="calibre8">Did our model get it right? In order to find out, we can check what the known correct <span>answers are:</span><pre class="source-code">
xgb_target_test</pre><p class="calibre3">The output of this piece of code should also be an array of classes, which are represented by either 0, 1, or 2, similar to the output shown in <span><em class="italic">Figure 5</em></span><em class="italic">.15</em>, although the values you get <span>may differ:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer099">
<img alt="Figure 5.15: The known, correct answers" src="image/B18143_05_015.jpg" class="calibre104"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.15: The known, correct answers</p>
<ol class="calibre7">
<li value="10" class="calibre8">They look pretty<a id="_idIndexMarker607" class="calibre6 pcalibre pcalibre1"/> similar to me, but to be sure, let’s test the accuracy by comparing our predictions to the known, <span>correct answers:</span><pre class="source-code">
accuracy_score(xgb_target_test,xgb_predictions)</pre><p class="calibre3">The score will be presented as a floating-point number representing the accuracy percentage. If the result is 1.0, it means that our predictions were <span>100% accurate!</span></p></li> </ol>
<p class="callout-heading">Diving deeper</p>
<p class="callout">The accuracy metric quantifies the ratio of correct predictions from all of the  predictions performed by our model, as represented by the <span>following formula:</span></p>
<p class="callout"><em class="italic">Accuracy = Correct Predictions / Total Number </em><span><em class="italic">of Predictions</em></span></p>
<p class="callout">There are many other metrics for assessing a model’s prediction performance, and scikit-learn has built-in functions to calculate many of those metrics. As additional learning, we recommend you consult the scikit-learn metric documentation at the following <span>link: </span><a href="https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics" class="calibre6 pcalibre pcalibre1"><span>https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics</span></a><span>.</span></p>
<p class="calibre3">Great work! You have<a id="_idIndexMarker608" class="calibre6 pcalibre pcalibre1"/> officially trained multiple of your own models on Google Cloud Vertex AI! Let’s recap on what we’ve learned in <span>this chapter.</span></p>
<h1 id="_idParaDest-124" class="calibre5"><a id="_idTextAnchor186" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we took many of the ML concepts from <em class="italic">Chapters 1</em> and <em class="italic">2</em> and put them into practice. We used clustering to find patterns in our data in an unsupervised manner, and you specifically learned a lot more about how K-means is used for clustering and how <span>it works.</span></p>
<p class="calibre3">We then dived into SL, and you explored the linear regression class within scikit-learn and learned how to use metrics to measure the performance of a <span>regression model.</span></p>
<p class="calibre3">Next, you learned how to use XGBoost to build a classification model and classify items in the iris dataset based on <span>their features.</span></p>
<p class="calibre3">Not only did you put all of those important concepts into practice, but you also learned how to create and use Vertex AI <span>Workbench-managed notebooks.</span></p>
<p class="calibre3">Additionally, you learned other important concepts in the ML industry, such as how decision trees work, how Gradient Boosting works, and how XGBoost enhances that functionality to implement one of the most effective ML algorithms in <span>the industry.</span></p>
<p class="calibre3">That was a lot of stuff to learn in one chapter, and you should be proud that you are significantly elevating your skills and knowledge in areas that are in high demand in today’s <span>business world.</span></p>
<p class="calibre3">In the next chapter, you will learn another set of extremely important skills as we will focus on data analysis and data transformation for <span>ML workloads.</span></p>
</div>
</div></body></html>