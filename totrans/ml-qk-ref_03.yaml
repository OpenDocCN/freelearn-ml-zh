- en: Performance in Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned that no two models will give the same result. In other
    words, different combinations of data or algorithms will result in a different
    outcome. This outcome can be good for a particular combination and not so good
    for another combination. What if we have a model that tries to take these combinations
    into account and comes up with a generalized and better result? This is called
    an **ensemble model**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning about a number of concepts in regard to
    ensemble modeling, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ensemble learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, one machine learning model is not good enough for a certain scenario
    or use case as it might not give you the desired accuracy, recall, and precision.
    Hence, multiple learning models—or an ensemble of models captures the pattern
    of the data and gives better output.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s say we are trying to decide on a place where we would
    like to go in the summer. Typically, if we are planning for a trip, the suggestions
    for the place pours in from all corners. That is, these suggestions might come
    from our family, websites, friends, and travel agencies, and then we have to decide
    on the basis of a good experience that we had in the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Family**: Let''s say that whenever we have consulted a family member and
    listened to them, there has been a 60% chance that they were proven right and
    we ended up having a good experience on the trip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Friends**: Similarly, if we listen to our friends, they suggest places where
    we might have a good experience. In these instances, a good experience occurred
    in 50% of cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Travel websites**: Travel websites are another source where we can get loads
    of information regarding where to visit. If we choose to take their advice, there''s
    a 35% chance that they were right and we had a good experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Travel agencies**: Another piece of advice and information might flow from
    travel agencies if we go and check with them first. Based on our past experiences,
    we saw that they were right in 45% of cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, we have to accumulate all of the preceding inputs and make a decision
    since no source has been 100% correct so far. If we combine these results, the
    accuracy scenario will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: From this, we are able to see the impact of ensemble modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Primarily, there are three methods of building an ensemble model, that is,
    **Bagging**, **Boosting**, and **Stacking**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2ec24a4-2d97-43aa-8d0a-71c5a58a59ff.png)'
  prefs: []
  type: TYPE_IMG
- en: We will discuss each method one by one. However, before we get into this, we
    need to understand what bootstrapping is, which sets the basis for **Bagging**
    and **Boosting**.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bootstrapping is a statistical technique that''s used to draw an inference
    about the parameters of population based on the samples drawn from it with replacement
    and averaging these results out. In the event of sampling with replacement, samples
    are drawn one after another, and once one sample is drawn from the population,
    the population is replenished with the sampled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6e63b8-5481-4f46-8fe6-c3ecc23016dd.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, there is a dataset that has got multiples components
    (**A**, **B**, **C**, **D**, **E**, **F**, **G**, **H**, and **I**). To start,
    we need to draw three samples of the same size. Let's draw **Sample 1** randomly
    and say that the first element turned out to be **A**. However, before we draw
    the second element of **Sample 1**, **A** is returned to the dataset. A similar
    process takes place for the entire draw. This is called **Sampling with Replacement**.
    Hence, we have a chance of selecting the same item multiple times in a set. By
    following this process, we have drawn three samples, that is, **Sample 1**, **Sample
    2**, and **Sample 3**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we take a step further down, which is determining the statistics (various
    metrics) on **Sample 1**, **Sample 2**, and **Sample 3**, we find out a mean or
    an average of all the statistics to infer something about the dataset (population).
    This entire process is called **bootstrapping** and the drawn samples are termed
    bootstrapped samples. This can be defined with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inference about the Dataset(Population) = Average(sample 1,sample 2,............,sample
    N)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the preceding diagram carefully, there might be a scenario wherein
    a few elements of the dataset haven''t been picked or are not part of those three
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample 1**: (**A**, **E**, **H**, **C**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample 2**: (**F**, **G**, **A**, **C**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample 3**: (**E**, **H**, **G**, **F**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the elements that haven't been picked are **B**, **D**, and **I**.
    The samples that were not part of the drawn samples are called **out-of-bag**
    (**OOB**) samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a simple coding exercise to see how this can be done in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will be using the `sklearn` and `resample` functions. Let''s import
    the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a dataset that we will need to sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract a bootstrap sample with the help of the `resample` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use list comprehension to extract an OOB sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there is a repetition of 60 in the sampling. This is due to
    sampling with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to print the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By this end of this, we want to have a result that''s as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OOB = Dataset - Boot_Sample *'
  prefs: []
  type: TYPE_NORMAL
- en: '*=[10,20,30,40,50,60,70,80,90,100] - [60,90,100,60,10]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*=[20,30,40,50,70,80]*'
  prefs: []
  type: TYPE_NORMAL
- en: This is the same result we have got from the code.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bagging stands for bootstrap **aggregation**. Hence, it''s clear that the bagging
    concept stems from bootstrapping. It implies that bagging has got the elements
    of bootstrapping. It is a bootstrap ensemble method wherein multiple classifiers
    (typically from the same algorithm) are trained on the samples that are drawn
    randomly with replacements (bootstrap samples) from the training set/population.
    Aggregation of all the classifiers takes place in the form of average or by voting.
    It tries to reduce the affect of the overfitting issue in the model as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a942be0b-22b5-45c0-9fbf-007ec88a7e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three stages of bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrapping**:This is a statistical technique that''s used to generate
    random samples or bootstrap samples with replacement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model fitting**:In this stage, we build models on bootstrap samples. Typically,
    the same algorithm is used for building the models. However, there is no restriction
    on using different algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining models**:This step involves combining all the models and taking
    an average. For example, if we have applied a decision tree classifier, then the
    probability that''s coming out of every classifier is averaged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a supervised learning technique that works on the divide-and-conquer
    approach. It can be used to address both classification and regression. The population
    undergoes a split into two or more homogeneous samples based on the most significant
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say we have got a sample of people who applied for a loan
    from the bank. For this example, we will take the count as 50\. Here, we have
    got three attributes, that is, gender, income, and the number of other loans held
    by the person, to predict whether to give them a loan or not.
  prefs: []
  type: TYPE_NORMAL
- en: We need to segment the people based on gender, income, and the number of other
    loans they hold and find out the most significant factor. This tends to create
    the most homogeneous set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take income first and try to create the segment based on it. The total
    number of people who applied for the loan is 50\. Out of 50, the loan was awarded
    to 20 people. However, if we break this up by income, we can see that the breakup
    has been done by income <100,000 and >=100,000\. This doesn''t generate a homogeneous
    group. We can see that 40% of applicants (20) have been given a loan. Of the people
    whose income was less than 100,000, 30% of them managed to get the loan. Similarly,
    46.67 % of people whose income was greater than or equal to 100,000 managed to
    get the loan. The following diagram shows the tree splitting on the basis of income:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92c46e1e-2cb7-4b7e-8f24-a15ef8a9a299.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take up the number of loans now. Even this time around, we are not able
    to see the creation of a homogeneous group. The following diagram shows the tree
    splitting on the basis of the number of loans:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d36994c-4d82-4617-a5d3-4e8c4f27e2fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s get on with gender and see how it fares in terms of creating a homogeneous
    group. This turns out to be the homogeneous group. There were 15 who were female,
    out of which 53.3% got the loan. 34.3% of male also ended up getting the loan.
    The following diagram shows the tree splitting based on gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48b51511-931c-4a3f-a32a-9be7d3edf359.png)'
  prefs: []
  type: TYPE_IMG
- en: With the help of this, the most significant variable has been found. Now, we
    will dwell on how significant the variables are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do that, it''s imperative for us to understand the terminology and
    nomenclature associated with the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Root Node**: This stands for the whole population or dataset that undergoes
    a split into two or more homogeneous groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Node**: This is created when a node is divided into further subnodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaf Node**: When there is no possibility of nodes splitting any further,
    that node is termed a leaf node or terminal node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branch**: A subsection of the entire tree is called a **branch** or a **Sub-tree**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/83181071-c958-46b3-86ae-372d6b81f921.png)'
  prefs: []
  type: TYPE_IMG
- en: Tree splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various algorithms that help when it comes to tree splitting, all
    of which take us to the leaf node. The decision tree takes all of the features
    (variables) that are available into account and selects the feature that would
    result in the most pure or most homogeneous split. The algorithm that''s used
    to split the tree also depends on the target variable. Let''s go through this,
    step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini index**: This says that if we select two items at random from a population,
    they must be from the same class. The probability for this event would turn out
    to be 1 if the population is totally pure. It only performs binary splits. **Classification
    and regression trees** (**CARTs**) make use of this kind of split.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following formula is how you calculate the Gini index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adcdc6ab-e709-4f65-a616-2d00813d3e13.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p(t)* is the proportion of observations with a target variable with a
    value of *t.*
  prefs: []
  type: TYPE_NORMAL
- en: 'For the binary target variable, *t=1*, the max Gini index value is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 1 — (1/2)^2— (1/2)^2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 1–2*(1/2)^2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 1- 2*(1/4)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 1–0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: A Gini score gives an idea of how good a split is by how mixed the classes are
    in the two groups that were created the by the split. A perfect separation results
    in a Gini score of 0, whereas the worst case split results in 50/50 classes.
  prefs: []
  type: TYPE_NORMAL
- en: For a nominal variable with *k* level, the maximum value of the Gini index is *(1-
    1/k).*
  prefs: []
  type: TYPE_NORMAL
- en: '**Information gain**: Let''s delve into this and find out what it is. If we
    happened to have three scenarios, as shown in the following diagram, which can
    be described easily?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2ecceb2f-c502-4b47-9276-be973ecc1baf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since **Z** seem to be quite homogeneous and all of the values of it are similar,
    it is called a **pure set**. Hence, it requires less effort to explain it. However,
    **Y** would need more information to explain as it''s not pure. **X** turns out
    to be the impurest of them all. What it tries to convey is that randomness and
    disorganization adds to complexity and so it needs more information to explain.
    This degree of randomness is known as **entropy**. If the sample is completely
    homogeneous, then the entropy is *0*. If the sample is equally divided, its entropy
    will be *1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy = -p log[2]p - q log[2]q*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *p* means the probability of success and *q* means the probability of
    failure.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is also used with a categorical target variable. It picks the split
    that has the lowest entropy compared to the parent node.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we must calculate the entropy of parent node first. Then, we need to calculate
    entropy of each individual node that's been split and post that, including the
    weighted average of all subnodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduction in variance**: When it comes to the continuous target variable,
    reduction in variance is used. Here, we are using variance to decide the best
    split. The split with the lowest variance is picked as the criteria to split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variance = ![](img/b83a3621-9aeb-480a-9a06-8df93b13d3d0.png)
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/41238de0-7910-4753-b06d-e0aa03b19ed2.png) is the mean of all the
    values, *X,* is the real values, and *n* is the number of values.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of variance for each node is done first and then the weighted
    average of each node's variance makes us select the best node.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters of tree splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of parameters that we need to tune or be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Max_depth`: One of the most important parameters is `max_depth`. It captures
    the essence of how deep the tree can get. More depth in the tree means that it
    is able to extract more information from the features. However, sometimes, excessive
    depth might be a cause of worry as it tends to bring along overfitting as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: This represents the minimum number of samples required
    to split an internal node. This can vary between considering at least one sample
    at each node to considering all of the samples at each node. When we increase
    this parameter, the tree becomes more constrained as it has to consider more samples
    at each node. An increase in the value of `min_samples_split` tends to be underfitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: This is the minimum number of samples required to be at
    a leaf node. Increasing this value to the maximum might cause underfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This is maximum number of features to be considered for the
    best split. It might cause overfitting when there is an increase in the max number
    of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we are well equipped to understand the random forest algorithm. We're going
    to talk about that next.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The random forest algorithm works with the bagging technique. The number of
    trees are planted and grown in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: There are *N *observations in the training set. Samples out of *N* observations
    are taken at random and with replacement. These samples will act as a training
    set for different trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are *M* input features (variables), *m* features are drawn as a subset
    out of *M* and of course *m < M*. What this does is select *m* features at random
    at each node of the tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every tree is grown to the largest extent possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction takes place based on the aggregation of the results coming out of
    all the trees. In the case of classification, the method of aggregation is voting,
    whereas it is an average of all the results in the case of regression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/09fecda6-8be8-4b47-9b9c-51591cbcf909.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's work on a case study, since that will help us understand this concept
    more in detail. Let's work on breast cancer data.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data that is given in this case study is about patients who were detected
    with two kinds of breast cancer:'
  prefs: []
  type: TYPE_NORMAL
- en: Malignant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A number of features are given here that have characteristics in regard to
    the cell nuclei that have been computed from the **fine-needle aspiration** (**FNA**)
    of a breast mass. Based on these features, we need to predict whether the cancer
    is malignant or benign. Follow these steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the breast cancer data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9e6557f-a0cb-4eaa-9312-442ffb09c069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider `data.head()` here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/819947a5-3c40-47d5-8342-71e05f58cd5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We get the data diagnosis from the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e288080-bf9d-41ea-b5bc-b43963d466ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The data is described as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b619a949-107e-455d-840b-3027dbbf095e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4bd0bc77-162c-441f-b7cd-428b83e5e6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/83f6dc50-a460-45af-a92d-2b8ffc6995c3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding input gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d121f10-24e4-49d3-8832-e24d16f80548.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f63c06af-363d-4a1b-9699-bd534ae987c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a1c90824-2c4a-4261-85a7-1fc5076b3613.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d7aa01a1-4ef0-4fe8-8c92-90eed38bba16.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we can see that the performance accuracy on the testing data is
    `95.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the total predictions is `114`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e256f6d9-d551-4096-a773-babd33156f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we can see that the 10 k-fold cross validation mean score is `94.9661835749`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that the classification accuracy is `95.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/df04808f-0b0b-4ae4-ae11-5eaaea1d3c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ab332a99-7d5f-4df2-9ed3-4d8a540728df.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/959ed7fc-e995-4594-8772-215c3369f34e.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph is a **receiver operating characteristic** (**ROC**) metric,
    which is used to evaluate classifier output quality using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot shows the ROC response to our chosen features (`['compactness_mean',
    'perimeter_mean', 'radius_mean', 'texture_mean', 'concavity_mean', 'smoothness_mean']`)
    and the diagnosis-dependent variable that was created from k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: A ROC area of `0.99` is quite good.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to bagging, it can be applied to both classification and regression.
    However, there is another technique that is also part of the ensemble family:
    boosting. However, the underlying principle of these two are quite different.
    In bagging, each of the models runs independently and then the results are aggregated
    at the end. This is a parallel operation. Boosting acts in a different way, since
    it flows sequentially. Each model here runs and passes on the significant features
    to another model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4f677a3-842b-47dc-be84-527d26d94f49.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To explain gradient boosting, we will take the route of Ben Gorman, a great
    data scientist. He has been able to explain it in a mathematical yet simple way.
    Let''s say that we have got nine training examples wherein we are required to
    predict the age of a person based on three features, such as whether they like
    gardening, playing video games, or surfing the internet. The data for this is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bf3e5de-9aff-46ba-aaf6-830f589b6200.png)'
  prefs: []
  type: TYPE_IMG
- en: To build this model, the objective is to minimize the mean squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will build the model with a regression tree. To start with, if we want
    to have at least three samples at the training nodes, the first split of the tree
    might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/928c8ffa-6ef3-4e38-8c74-2e1a1fbde65f.png)'
  prefs: []
  type: TYPE_IMG
- en: This seems to be fine, but it's not including information such as whether they
    play video games or browse the internet. What if we plan to have two samples at
    the training nodes?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a16df436-cb99-466d-a364-c33c32f41c14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Through the preceding tree, we are able to get certain information from features,
    such as **SurfInternet** and **PlaysVideoGames**. Let''s figure out how residuals/errors
    come along:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53f43655-8f92-48e9-9912-97466e94b504.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will work on the residuals of the first model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa17a869-d555-461e-be73-e76c2dc09eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have built the model on residuals, we have to combine the previous
    model with the current one, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/948cc81a-328c-4744-9dde-1a5ba19ab78f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the residuals have come down and that the model is getting better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to formulate what we have done up until this point:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we built a model on the data *f[1](x) = y.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next thing we did was calculate the residuals and build the model on residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*h [1](x)=y- f[1](x)*'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to combine the model, that is, *f[2](x)= f[1](x) + h [1](x).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adding more models can correct the errors of the previous models. The preceding
    equation will turn out to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f3(x)= f2(x) + h2(x) *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation will finally look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f[m](x)= f[m][-1](x) + h[m][-1](x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '* h[m](x)= y- f[m](x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our task is to minimize the squared error, *f* will be initialized with
    the mean of the training target values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fa749feb-9490-4a4c-9790-fac085bc18f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we can find out *f*[*m+1*, ]just like before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*f[m](x)= f[m][-1](x) + h[m][-1](x)*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use gradient descent for our gradient boosting model. The objective
    function we want to minimize is *L*. Our starting point is *f[o](x)*. For iteration
    *m=1*, we compute the gradient of* L* with respect to* f[o](x)*. Then, we fit
    a weak learner to the gradient components. In the case of a regression tree, leaf
    nodes produce an **average gradient** among samples with similar features. For
    each leaf, we step in the direction of the average gradient. The result is *f[1]*and
    this can be repeated until we have *f[m]*.
  prefs: []
  type: TYPE_NORMAL
- en: We modified our gradient boosting algorithm so that it works with any differentiable
    loss function. Let's clean up the preceding ideas and reformulate our gradient
    boosting model once again.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters of gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different parameters to consider before applying gradient boosting
    for the breast cancer use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Min_samples_split`: The minimum number of samples required in a node to be
    considered for splitting is termed `min_samples_split`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Min_samples_leaf`: The minimum number of samples required at the terminal
    or leaf node is termed `min_samples_leaf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Max_depth`: This is the maximum number of nodes allowed from the root to the
    farthest leaf of a tree. Deeper trees can model more complex relationships, however,
    causing the model to overfit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Max_leaf_nodes`: The maximum number of nodes at the leaves in a tree. Since
    binary trees are created, a depth of `n` would produce a maximum of *2^(n )* leaves.
    Hence, either `max_depth` or `max_leaf_nodes` can be defined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will apply gradient boosting for the breast cancer use case. Here,
    we are loading the libraries that are required to build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We are now done with the various steps of data cleaning and exploration while
    performing random forest. Now, we will jump right into building the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will perform a grid search to find out the optimal parameters for
    the gradient boosting algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0781efe-1af7-400d-8397-3a9e5d860876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s find out the optimal parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/588de5cb-de5b-4e24-ba89-04eb0c654304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance accuracy on the testing data is `96.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of predictions is `114`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ee427abe-70f1-4061-95f2-045ab887cb9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s perform cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The 10 k-fold cross-validation mean score is `94.9420289855`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification accuracy is `96.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'By looking at the confusion matrix, we can see that this model is better than
    the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bc63967-fbe6-41e9-b577-ad52bb8bcdf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied ensemble learning and its different methods, namely
    bagging, boosting, and stacking. We even saw what is bootstrapping which is the
    root for ensemble learning methods such as bagging and boosting. We also learned
    about decision trees and its approach of divide and rule with example of people
    applying for loan. Then we covered tree splitting and the parameters to split
    a decision tree, moving on to the random forest algorithm. We worked on a case
    study of breast cancer using the concepts covered. We also discovered the difference
    between bagging and boosting and gradient boosting. We also discussed on parameters
    of gradient boosting to use it our example of breast cancer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about training neural networks.
  prefs: []
  type: TYPE_NORMAL
