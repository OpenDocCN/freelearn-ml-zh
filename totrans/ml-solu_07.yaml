- en: Chapter 7. Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be building the summarization application. We will
    specifically focus on the textual dataset. Our primary goal is to perform the
    summarization task on medical notes. Basically, the idea is to come up with a
    good solution to summarize medical transcription documents.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of summarization application helps doctors a great manner. You ask
    how? Let's take an example. Suppose a patient has 10 years of history with a certain
    disease, and after 10 years, he consults a new doctor for better results. On the
    first day, the patient needs to hand over their last 10 years of medical prescriptions
    to this new doctor. After that, the doctor will need to study all these documents.
    The doctor also relies on the conversation he had with the patient. By using medical
    notes and conversations with the patient, the doctor can find out the patient's
    health status. This is quite a lengthy method.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, what if we could generate a summary of the patient''s medical notes
    and provide these summarized documents to the doctor? It seems like a promising
    solution because this way, we can save the doctor''s time and efforts. Doctors
    can understand their patients'' issues in an efficient and accurate way. Patients
    can start getting treatment from their first meeting with the doctor. This is
    a win-win situation for both parties, and this kind of solution is what we are
    trying to build here. So, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the baseline approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the revised approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best approach: building a summarization application for Amazon reviews'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basics of summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be focusing on the basic concepts of summarization.
    In today's fast-growing information age, text summarization has become an important
    tool. It will be difficult for humans to generate a summary for large text documents.
    There are lots of documents available on the web today. So, we need a solution
    that can automatically generate a summary for documents efficiently, accurately,
    and intelligently. This task is referred to as automatic text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic text summarization is all about finding relevant information from
    the large text document in a small amount of time. Basically, there are two types
    of summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: Extractive summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstractive summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the types of summarization one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Extractive summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the extractive summarization method, we will be generating a summary of the
    document by selecting words, phrases, or sentences from the original document.
    We will be using concepts such as **Term-Frequency, Inverse-Document Frequency**
    (**TF-IDF**), Count vectorizers, Cosine similarity, and the ranking algorithm
    to generate this type of summary.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered concepts such as TF-IDF, Count vectorizers, and Cosine similarity
    in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Systems for E-Commerce*, section *Understanding TF-IDF*. We will
    look at the ranking mechanism when we implement the code for it in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Abstractive summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the abstractive summarization method, we will try and make the machine learn
    internal language representation so that it can generate more human-like summaries
    by paraphrasing.
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement this type of summarization, we will be using deep learning
    algorithms such as a sequence-to-sequence model with an attention mechanism. You
    will learn about the algorithm and concepts later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we already looked at an overview of the problem
    statement. Here, we will be delving into further details. We want to build an
    automatic text summarization application. We will be providing a medical transcription
    document as the input. Our goal is to generate the summary of this document. Note
    that here, we are going to provide a single document as the input, and as an output,
    we will be generating the summary of that single document. We want to generate
    an informative summary for the document. An informative summary is a type of summary
    where the summarization document is a substitute of the original document as far
    as the converging of information is concerned. This is because we are dealing
    with the medical domain.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we use extractive summarization methods in our approaches. We will
    be generating the extractive summary for a medical document. Later on in this
    chapter, we will be also developing a solution that can generate an abstractive
    summarization of Amazon reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to explore the dataset and look at the challenges we have faced
    in accessing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is divided into two parts. In the first part, we need to discuss
    the challenges we have faced in order to generate the dataset. In the later section,
    we will be discussing the attributes of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in obtaining the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we all know, the health domain is a highly regulated domain when it comes
    to obtaining the dataset. These are some of the challenges I want to highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: For summarization, ideally, we need to have a corpus that contains original
    text as well as a summary of that text. This is called parallel corpus. Unfortunately,
    there is no good, free parallel corpus available for medical document summarization.
    We need to obtain this kind of parallel dataset for the English language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some free datasets available, such as the MIMIC II and MIMIC III dataset,
    but they won't contain summaries of the medical transcription. We can access just
    the medical transcription from this dataset. Gaining access to this dataset is
    a lengthy and time-consuming process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to solve the preceding challenges, professionals, researchers, academics,
    and big tech companies need to come forward and make good quality, freely available
    datasets for the medical domain. Now let's look at how to get the medical transcription
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the medical transcription dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might wonder if we do not have a parallel dataset with us, then how will
    we build the summarization application? There is a workaround here. I have a sample
    medical transcription from the MIMIC – II dataset. We will be using them and generating
    an extractive summary of the documents. Apart from that, we will be referring
    to [www.mtsamples.com](http://www.mtsamples.com) in order to get an idea about
    the different kind of medical transcriptions we could possibly have. With the
    help of a minimum number of documents, we are going to build the summarization
    application. You can see what these medical transcriptions will look like in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the medical transcription dataset](img/B08394_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Sample medical transcription'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, in medical transcriptions, there are a couple of sections, and they
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chief complaint**: This section describes the main problem or disease that
    the patient is facing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hist****ory of patient''s illness**: This section has a detailed description
    of the patient''s medical status and their history of a similar disease or other
    kinds of diseases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Past medical history**: This section describes the name of the diseases that
    the patient had in past'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Past surgical history**: If the patient had any surgeries in the past, then
    the name of those surgeries is mentioned here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Family history**: If any family member has the same type of disease or a
    history of certain kinds of diseases in the family, then those are mentioned in
    this section'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medications**: This section describes the medicine names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Physical examination**: This section has all the descriptions related to
    physical examinations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assessment**: This section contains the details about the potential disease
    the patient may have after taking all preceding parameters into consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendations**: This section describes the recommended solution for the
    patient''s complaints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keywords**: This section has the keywords that can describe the entire document
    properly so the dataset can be used for the topic modeling task as well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of transcription is random in certain sections. Some transcriptions
    contain all the preceding sections, and some do not. So, the number of sections
    for this kind of document may vary a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at details related to the Amazon review dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Amazon's review dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Later on in this chapter, we will be using the Amazon review dataset in order
    to generate the abstractive summary. So, it is better if you understand basic
    data attributes for this dataset. First of all, you can download that dataset
    by using this link: [https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data](https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data).
    The name of the file you need to download is `Reviews.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look at the content of this dataset by referring to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Amazon''s review dataset](img/B08394_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Data records from Amazon''s review dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand each of the data attributes of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ID`: This attribute indicates the serial number for data records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ProductId`: This attribute indicates the unique ID for the particular product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserId`: This attribute indicates the unique user ID of the user who has shared
    their review for a particular product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ProfileName`: This data attribute is the user''s profile name. Using this
    profile name, the user will have submitted their review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HelpfulnessNumerator`: This attribute indicates how many other users found
    this review useful in a positive way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HelpfulnessDenominator`: This attribute indicates the total number of users
    who voted as to whether this review was useful or not useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Score`: This is the score for a particular product. Zero means the user didn''t
    like it, and five means the user liked it a lot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`: This attribute indicates the timestamp at which the review has been
    submitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Summary`: This attribute is quite useful as it indicates the summary for the
    entire review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Text`: This attribute is the long text review for any given product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have looked at both the datasets. Let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing the baseline approach for the summarization
    application. We will be using medical transcriptions to generate the summary.
    Here we will be using a small trial MIMIC-II dataset which contains a few sample
    medical documents and [www.mtsamples.com](http://www.mtsamples.com) for getting
    medical transcriptions. You can find the code by using this GitHub link: [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start building the baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will be performing the following steps in order to build the baseline
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Install python dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write code and generate summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing python dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will be using two python dependencies, which are really easy to use, in
    order to develop the summarization application. One is `PyTeaser`, and the second
    one is `Sumy`. You need to execute the following commands in order to install
    these two dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the `PyTeaser` library works only with `python 2.7`. Sumy can work
    with `python 2.7` and `python 3.3+`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's write the code.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the code and generating the summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Both the `PyTeaser` and Sumy libraries have great features. They take any weburl
    as the input and generate the summary for the given weburl. You can refer to the
    code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing the code and generating the summary](img/B08394_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Code snippet for generating summarization using PyTeaser'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we are passing the weburl of the sample medical transcription
    from [www.mtsamples.com](http://www.mtsamples.com). The `PyTeaser` library will
    generate the top five sentences of the document as the summary. To view the output,
    you can take a look at the the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing the code and generating the summary](img/B08394_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Summary for the medical transcription using PyTeaser'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s try out the `Sumy` library. You can refer to the code given in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing the code and generating the summary](img/B08394_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Code snippet for generating summarization using Sumy'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Sumy` library, we need to pass the weburl as the input, but there is
    one difference. As you can see in the preceding code, we have provided `SENTENCES_COUNT
    = 10`, which means our summary or output has 10 sentences. We can control the
    number of statements by using the `SENTENCES_COUNT` parameter. You can refer to
    the output given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing the code and generating the summary](img/B08394_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Summary for medical transcription using Sumy'
  prefs: []
  type: TYPE_NORMAL
- en: If you view and compare the output of the `Sumy` and `PyTeaser` libraries, then
    you could say that the `Sumy` library is performing really well compared to the
    `PyTeaser` library. As you can see, both these libraries obtain a basic summary
    of the given document. These libraries are using the ranking algorithm and the
    frequency of the words in order to obtain the summaries. We don't have control
    over their internal mechanisms. You might be wondering whether we can make our
    own summarization so that we can optimize the code as and when needed. The answer
    is yes; we can develop our code for this task. Before that, let's discuss the
    shortcomings of this approach, and then we will build our own code with the revised
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will be discussing the shortcomings of the baseline approach so that
    we can take care of these disadvantages in the next iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, we do not have full ownership over the code of these libraries.
    So, we cannot change or add functionalities easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have obtained a basic kind of summary, so we need to improve the result of
    the summary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the lack of a parallel corpus, we cannot build a solution that can
    generate an abstractive summary for medical documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are three main shortcomings of the baseline approach, and we need to solve
    them. In this chapter, we will be focusing on first and second shortcomings. For
    the third shortcoming, we cannot do much about it. So, we have to live with that
    shortcoming.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss how we will be optimizing this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing how to optimize the baseline approach.
    We will be implementing a simple summarization algorithm. The idea behind this
    algorithm is simple: This approach is also generating an extractive summary for
    the medical document. We need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to determine the frequencies of the words in the given document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The, we split the document into a series of sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to generate the summary, we select the sentences that have more frequent
    words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we reorder summarize sentences so that the generated output is aligned
    with the original document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding algorithm can solve our two shortcomings, although we may need
    help with the third one because right now, there is no availability of the dataset
    that can be used in the summarization task, especially in the medical domain.
    For this chapter, we have to live with this shortcoming (unfortunately, we don't
    have any other option), but don't worry. This doesn't mean we will not learn how
    to generate the abstractive summary. In order to learn how to generate abstractive
    summaries, we will be using the Amazon review dataset later on this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's implement the steps of the algorithms that we described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Building the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will be coding the algorithm that we discussed in the previous section.
    After implementing it, we will check how well or badly our algorithm is performing.
    This algorithm is easy to implement, so let''s begin with the code. You can find
    the code at this GitHub link: [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing the summarization algorithm step by
    step. These are the functions that we will be building here:'
  prefs: []
  type: TYPE_NORMAL
- en: The get_summarized function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reorder_sentences function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summarize function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin with the first one.
  prefs: []
  type: TYPE_NORMAL
- en: The get_summarized function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Basically, this function performs the summarization task. First, it will take
    the content of the document as input in the form of string. After that, this function
    generates the frequency of the words, so we need to tokenize the sentences into
    words. After that, we will be generating the top 100 most frequent words from
    the given document. For small of dataset, the top 100 most frequent words can
    describe the vocabulary of the given dataset really well so we are not considering
    more words. If you have large dataset, then you can consider the top 1,000 or
    top 10,000 most frequent words based on the size of the dataset. You can refer
    to the code given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The get_summarized function](img/B08394_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Code snippet for generating the most frequent words from the given
    input document'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s code the second step. We need to split the documents into sentences.
    We will convert the sentences into lowercase. We will use the NLTK sentence splitter
    here. You can refer to the code given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The get_summarized function](img/B08394_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Code snippet for generating sentences from the input document'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, we will iterate over the list of the most frequent words
    and find out the sentences that include a higher amount of frequent words. You
    can refer to the code shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The get_summarized function](img/B08394_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Code snippet for generating the sentence that has a higher amount
    of frequent words'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to rearrange the sentences so that the sentence order aligns with
    the original input document.
  prefs: []
  type: TYPE_NORMAL
- en: The reorder_sentences function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This function basically reorders the summarized sentence so that all the sentences
    align with the order of the sentences of the original document. We will take summarized
    sentences and sentences from the original document into consideration and perform
    the sorting operation. You can refer to the code given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The reorder_sentences function](img/B08394_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Code snippet for reordering the summarized sentences'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the final step.
  prefs: []
  type: TYPE_NORMAL
- en: The summarize function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This function basically generates the summary. This is the method that we can
    call from any other file. Here, we need to pass the input data and the number
    of sentences we need in the summarized content. You can refer to the code that
    is displayed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The summarize function](img/B08394_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Code snippet for defining the function that can be called outside
    of the class'
  prefs: []
  type: TYPE_NORMAL
- en: Generating the summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let''s look at a demonstration of this this code and generate the summary
    for the document. We will pass the textual content from [www.mtsamples.com](http://www.mtsamples.com)
    and then try to generate a summary of the content. You can refer to the code snippet
    given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the summary](img/B08394_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Code snippet to call the summarized function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the summary](img/B08394_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Output for the revised approach'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the output is more relevant than the baseline approach. We know
    the approach for the kind of steps we have been performing so far. This approach
    gives us clarity about how we can generate the extractive summary for the medical
    transcription. The good part of this approach is that we do not need any parallel
    summarization corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's discuss the shortcomings of the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing the shortcomings of the revised approach,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The revised approach does not have the ranking mechanism to rank the sentences
    based on their importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have considered word frequencies so far; we have not considered their importance
    with respect to the other words. Suppose word *a* appears a thousand times in
    a document. That doesn't mean it carries more importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's see how we can overcome these shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be discussing the steps that we should take in order
    to improve the revised approach. To obtain the best result for extractive summarization,
    we need to use TF-IDF and the sentence ranking mechanism to generate the summary.
    We have covered TF-IDF in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for E-Commerce*, in the *Generating
    features using TF-IDF* section. We will be building the ranking mechanism by using
    cosine similarity and LSA (Latent Semantic Analysis). We have already looked at
    cosine similarity in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for E-Commerce*. Let's explore the LSA
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The LSA algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The LSA algorithm is similar to the cosine similarity. We will generate the
    matrix by using the words present in the paragraphs of the document. The row of
    the matrix will represent the unique words present in each paragraph, and columns
    represent each paragraph. You can view the matrix representation for the LSA algorithm
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The LSA algorithm](img/B08394_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Matrix representation for the LSA algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: The basic assumption for the LSA algorithm is that words that are close in their
    meaning will occur in a similar piece of text. As you can see from the preceding
    example, if we say that the word pair (cat, is) occurs more frequently, it means
    that it carries higher semantic meaning than the (cat, mouse) word pair. This
    is the meaning of the assumption behind the algorithm. We generate the matrix
    that is given in the previous figure and then try to reduce the number of rows
    of the matrix by using the **single value decomposition** (**SVD**) method. SVD
    is basically a factorization of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can read more on SVD by using this link: [https://en.wikipedia.org/wiki/Singular-value_decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are reducing the number of rows (which means the number of words) while
    preserving the similarity structure among columns (which means paragraphs). In
    order to generate the similarity score between word pairs, we are using cosine
    similarity. This is more than enough to keep in mind in order to build the summarization
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's discuss the approach we are taking in order to build the best possible
    solution for generating an extractive summary for medical documents.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the best approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will perform the following steps in order to build the best approach:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we will take the content of the document in the form of a string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will parse the sentence, and after that, we will remove the stop words and
    special characters. We will be converting the abbreviations into their full forms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, we will generate the lemma of the words and their **Part-of-Speech**
    (**POS**) tags. Lemma is nothing but the root form of words and POS tags indicate
    whether the word is used as a verb, noun, adjective, or adverb. There are many
    POS tags available. You can find a list of POS tags at this site: [https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will generate the matrix of the TF-IDF vectors for the words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will generate the SVD matrix using the `SciPy` library for the given TF-IDF
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, using cosine-similarity, we can rank the sentences and generate the
    summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's look at the implementation of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the implementation of the best approach. We
    will also discuss the structure of the code. So, without wasting time, let''s
    begin with the implementation. You can find the code by using this GitHub link:
    [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps you need to take in order to implement the code are provided in the
    following list:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the structure of the project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding helper functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating the summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the first step.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the structure of the project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The structure of the project is quite important here. There will be four different
    files in which we will be writing code. You can see the structure of the project
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the structure of the project](img/B08394_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Structure of the project''s code files'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four code files. I will explain their usage one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Contractions.py`: This file contains an extensive list of all of the abbreviations,
    especially grammatical abbreviations. You can take a look at the list abbreviations
    in the following figure:![Understanding the structure of the project](img/B08394_07_16.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.16: List of abbreviations and their full forms'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Normalization.py`: This file contains various helper functions for the preprocessing
    step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Utils.py`: This file contains the helper function that is used to calculate
    TF-IDF and obtain the SVD matrix for the given TF-IDF matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Document_summarization.py`: This file uses the already defined helper function
    and generates a summary for the document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's see what kind of helper functions we have defined in each file.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding helper functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will discuss the helper function file-wise so you will get an idea as to
    which helper function is part of which file.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization.py
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This file contains many helper functions. I will explain each helper function
    based on the sequence of its usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`parse_document`: This function takes the content of the document as the input
    and tokenizes it sentence-wise. This means we are splitting the string sentence
    by sentence. We will consider only the Unicode string here. You can refer to the
    code snippet given in the following figure:![Normalization.py](img/B08394_07_17.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.17: Code snippet for parsing documents'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`remove_special_characters`: This function will remove the special characters
    from the strings. You can refer to the code snippet given in the following figure
    for a better idea:![Normalization.py](img/B08394_07_18.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.18: Code snippet for removing special characters from the string'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`remove_stopwords`: This function will remove the stop words from the sentences.
    You can refer to the code snippet given in the following figure:![Normalization.py](img/B08394_07_19.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.19: Code snippet for removing stop words'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`unescape_html`: This function removes HTML tags from the sentences. You can
    refer to the code snippet given in the following figure:![Normalization.py](img/B08394_07_20.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.20: Code snippet for removing HTML tags'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pos_tag_text`: This function tokenizes the sentences into words, after which
    it will provide POS tags to these words. You can refer to the code snippet given
    in the following figure:![Normalization.py](img/B08394_07_21.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.21: Code snippet for generating POS tags'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`lemmatize_text`: This function will tokenize the sentence into words and then
    generate the lemma of the words. You can refer to the code given in the following
    figure:![Normalization.py](img/B08394_07_22.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.22: Code snippet for generating the lemma of the words'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`expand_contractions`: This function looks at the abbreviations. If there is
    any abbreviation that is present in our list in the given sentence, then we will
    replace that abbreviation with its full form. You can refer to the code displayed
    in the following figure:![Normalization.py](img/B08394_07_23.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.23: Code snippet for replacing abbreviations with full forms'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`normalize_corpus`: This function calls all the preceding helper functions
    and generates the preprocessed sentences. You can refer to the code given in the
    following figure:![Normalization.py](img/B08394_07_24.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.24: Code snippet for generating preprocessed sentences'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now let's see what functions we have defined in the `utils.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: Utils.py
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this file, there are only two helper functions. They are described here.
  prefs: []
  type: TYPE_NORMAL
- en: '`build_feature_matrixs`: This function generates the TF-IDF vectors using the
    scikit-learn `Tfidfvectorizer` API. We are providing the preprocessed text as
    the input, and as the output, we have the matrix. This matrix contains the vectorized
    value of the given words. You can refer to the code snippet for this, which is
    provided in the following figure:![Utils.py](img/B08394_07_25.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.25: Code snippet for generating TF-IDF vectors'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`low_rank_svd`: This particular function uses the API from python''s `SciPy`
    library. It performs the SVD on the TF-IDF matrix, and after that, we obtain the
    cosine similarity score. Based on the score, we will rank the sentences. Here,
    we just define the function that can generate the SVD for the TF-IDF matrix. You
    can refer to the code snippet given in the following figure:![Utils.py](img/B08394_07_26.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.26: Code snippet for generating SVD'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now let's use all these helper functions in order to generate the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will look at the code that is given in the `document_summarization.py`
    file. There are two methods that are responsible for generating the summary for
    the given document. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`textrank_text_summarizer`: This method takes the preprocessed document as
    the input, and by using the `build_feature_matrix` helper function, we will generate
    the TF-IDF matrix. After that, we will generate the similarity score. Based on
    the similarity score, we will sort the sentences and provide them a rank. As an
    output, we will display these sorted sentences. Here, sentence sequence is aligned
    with the original document, so we don''t need to worry about that. You can take
    a look at the code snippet given in the following figure:![Generating the summary](img/B08394_07_27.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.27: Code snippet in order to generate the summary using the textrank_text_summarizer
    method'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`lsa_text_summarizer:` This function takes the preprocessed text as the input
    and generates the TF-IDF matrix. After that, the `low_rank_svd` method is applied
    on the matrix, and we get our factorized matrix. We will generate the similarity
    score using these factorized matrices. After sorting sentences based on this similarity
    score, we can generate the summary. You can refer to the code snippet displayed
    in the following figure:![Generating the summary](img/B08394_07_28.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.28: Code snippet for generating the summary using lsa_text_summarizer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will call these functions and generate the output. The code snippet for
    that is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the summary](img/B08394_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.29: Code snippet for generating the output summary'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at the output shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the summary](img/B08394_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.30: Output summary for document_1'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for another document is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the summary](img/B08394_07_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.31: Output summary for document_2'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, compared to the revised approach, we will get a much more relevant
    extractive type of summary for the given document. Now let's build the abstractive
    summarization application using Amazon's product review dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building the summarization application using Amazon reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are building this application so that you can learn how to use parallel
    corpus in order to generate the abstractive summary for the textual dataset. We
    have already explained basic stuff related to the dataset earlier in the chapter.
    Here, we will cover how to build an abstractive summarization application using
    the Deep Learning (DL) algorithm. You can refer to the code using this GitHub
    link: [https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb](https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also download the pre-trained model using this link: [https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg](https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this application, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will see the code for how we can load the dataset. Our
    dataset is in the CSV file format. We will be using pandas to read our dataset.
    You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_07_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.32: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be doing some basic analysis of the dataset. We will
    check whether any null entries are present. If there are, then we will remove
    them. You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the dataset](img/B08394_07_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.33: Code snippet for removing null data entries'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's prepare the dataset that can be used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are the steps that we will perform in order to prepare the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: We will replace the abbreviations that appeared in the text with their full
    forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will remove special characters, URLs, and HTML tags from the review data
    column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will remove stop words from the reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have performed all the preceding steps and generated the junk-free review.
    You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the dataset](img/B08394_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.34: Code snippet for performing preprocessing of the reviews'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, there are 132,884 unique words. You can find the size of the vocabulary
    when you run the code. These unique words are the vocabulary for this application,
    and we need to convert these words into a vector format. The vector format of
    the words is called word embeddings. You can use Word2vec, Numberbatch, or GloVe
    in order to generate word embeddings. Here, we will be using Numberbatch''s embedding
    pre-trained model in order to generate word embedding for this application. The
    Numberbatch''s pretrained model is more optimize and faster than GloVe so we are
    using Numberbatch''s model. You can refer to the code snippet given in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the dataset](img/B08394_07_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.35: Code snippet for generating word embedding using Numberbatch''s
    pre-trained model'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about word2vec, then you can refer to my previous
    book, *Python Natural Language Processing*, particularly [Chapter 6](ch06.xhtml
    "Chapter 6. Job Recommendation Engine"), *Advance Feature Engineering and NLP
    Algorithms*. The link is [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6).
  prefs: []
  type: TYPE_NORMAL
- en: Building the DL model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will be building the DL algorithm. We are using the seq2seq
    neural network. Basically, the seq2seq model is used to process the sequential
    data. Language or sentences are the sequence of words. In this algorithm, there
    is an encoder that accepts the word embedding and learns the language representation.
    The output of this layer is fed to the decoding layer. Here, we will also use
    the attention mechanism. The attention mechanism will focus on the most import
    part of the sentences. It will store the semantic representation of the sentences.
    For the attention mechanism, we will use the LSTM cell with the recurrent neural
    network architecture, which learns the complex semantic representation of the
    language and stores it in the LSTM network. When we generate the final output,
    we will be using the weight of the decoder cells as well as the weight of LSTM
    cells and will generate the final word embedding. Based on the word embedding,
    we will generate the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to achieve this, we need to build seq2seq using a **Recurrent Neural
    Network** (**RNN**) with the attention mechanism. You can refer to the code given
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the DL model](img/B08394_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.36: Code snippet for building the RNN encoding layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the DL model](img/B08394_07_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.37: Code snippet for building the RNN decoding layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet for building the seq2seq model is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the DL model](img/B08394_07_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.38: Code snippet for building seq2seq'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the DL model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Basically, we have built the neural network, and now it''s time to start the
    training. In this section, we will define the values for all hyperparameters,
    such as the learning rate, the batch size, and so on. You can refer to the code
    given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the DL model](img/B08394_07_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.39: Code snippet for training the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training, we will be tracking the loss function and using the gradient
    descent algorithm, and we will try to minimize the value of our loss function.
    You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the DL model](img/B08394_07_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.40: Code snippet for tracing the loss function'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have the trained the model on CPU for 6 to 8 hours, and we have the
    loss value 1.413\. You can train the model for more amount time as well. Now let's
    test the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the DL model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we load the trained model and generate the summary for a randomly
    selected review. You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the DL model](img/B08394_07_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.41: Code snippet for generating the summary for the given review'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the preceding code is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the DL model](img/B08394_07_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.42: Summary for the given review'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is great if we want to generate a one-line summary for the given
    textual data. In future, if we will have the parallel medical transcription dataset
    for the summarization task, then this approach will work well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built the summarization application for medical transcriptions.
    In the beginning, we listed the challenges in order to generate a good parallel
    corpus for the summarization task in the medical domain. After that, for our baseline
    approach, we used the already available Python libraries, such as `PyTeaser` and
    `Sumy`. In the revised approach, we used word frequencies to generate the summary
    of the medical document. In the best possible approach, we combined the word frequency-based
    approach and the ranking mechanism in order to generate a summary for medical
    notes.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we developed a solution, where we used Amazon's review dataset,
    which is the parallel corpus for the summarization task, and we built the deep
    learning-based model for summarization. I would recommend that researchers, community
    members, and everyone else come forward to build high-quality datasets that can
    be used for building some great data science applications for the health and medical
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building chatbots. Chatbots, or virtual assistants,
    have become a hot topic in the data science domain over the last couple of years.
    So, in the next chapter, we will take into consideration a movie dialog dataset
    and the Facebook `bAbI` dataset. With the help of these datasets and by using
    deep learning algorithms, we will build chatbots. So, if you want to learn how
    to build one for yourself, then keep reading!
  prefs: []
  type: TYPE_NORMAL
