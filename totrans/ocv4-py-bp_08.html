<html><head></head><body>
        

                            
                    <h1 class="header-title">Learning to Recognize Facial Emotions</h1>
                
            
            
                
<p class="mce-root CDPAlignLeft CDPAlign">We previously familiarized ourselves with the concepts of object detection and object recognition. In this chapter, we will develop an app that does both together. The app will be able to detect your own face in each captured frame of a webcam live stream, recognize your facial emotion, and label it on the <strong>Graphical User Interface</strong> (<strong>GUI</strong>).</p>
<p>The goal of this chapter is to develop an app that combines both <strong>face detection</strong> and <strong>face recognition</strong>, with a focus on recognizing emotional expressions for the detected face. After reading the chapter, you will be able to use both face detection and recognition in different applications of your own.</p>
<p>We will be covering the following topics in this chapter:</p>
<ul>
<li>Planning the app</li>
<li>Learning about face detection</li>
<li>Collecting data for machine learning tasks</li>
<li>Understanding facial emotion recognition </li>
<li>Putting it all together</li>
</ul>
<p>We will touch upon two classic algorithms that come bundled with OpenCV—<strong>Haar cascade classifiers</strong> and <strong>MLPs</strong>. While the former can be used to rapidly detect (or to locate, and to answer the question <em>Where</em>?) objects of various sizes and orientations in an image, the latter can be used to recognize them (or to identify, and answer the question <em>What</em>?).</p>
<p>Learning MLPs is also the first step toward learning one of the most trendy algorithms these days—<strong>deep neural networks</strong> (<strong>DNNs</strong>). We will use PCA to speed up and improve the accuracy of the algorithm when we have not got a huge amount of data, to improve the accuracy of our model.</p>
<p>We will collect our training data ourselves to show you how that process is done, in order for you to be able to train machine learning models for tasks that don't have data readily available. Unfortunately, not having the right data is still one of the biggest obstacles to the widespread adoption of machine learning these days.</p>
<p>Now, let's take a look at how to get started before we get our hands dirty.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting started</h1>
                
            
            
                
<p>You can find the code that we present in this chapter at our GitHub repository, at <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8</a>.</p>
<p>Other than that, you should download the <strong>Haar cascade</strong> files from the official OpenCV repository at <a href="https://github.com/opencv/opencv/blob/master/data/haarcascades/">https://github.com/opencv/opencv/blob/master/data/haarcascades/</a>, or copy them from the installation directory of your machine to the project repository.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the app</h1>
                
            
            
                
<p>The reliable recognition of faces and facial expressions is a challenging task for <strong>artificial intelligence</strong> (<strong>AI</strong>), yet humans are able to perform these kinds of tasks with ease. To make our task feasible, we will limit ourselves to the following limited emotional expressions:</p>
<ul>
<li>Neutral</li>
<li>Happy</li>
<li>Sad</li>
<li>Surprised</li>
<li>Angry</li>
<li>Disgusted</li>
</ul>
<p>Today's state-of-the-art models range all the way from 3D deformable face models fitting over <strong>convolutional neural networks</strong> (<strong>CNNs</strong>), to deep learning algorithms. Granted, these approaches are significantly more sophisticated than our approach.</p>
<p>Yet, an <strong>MLP</strong> is a classic algorithm that has helped transform the field of machine learning, so for educational purposes, we will stick to a set of algorithms that come bundled with OpenCV.</p>
<p>To arrive at such an app, we need to solve the following two challenges:</p>
<ul>
<li><strong>Face detection</strong>: We will use the popular Haar cascade classifier by Viola and Jones, for which OpenCV provides a whole range of pre-trained exemplars. We will make use of face cascades and eye cascades to reliably detect and align facial regions from frame to frame.</li>
<li><strong>Facial expression recognition</strong>: We will train an MLP to recognize the six different emotional expressions listed earlier, in every detected face. The success of this approach will crucially depend on the training set that we assemble, and the preprocessing that we choose to apply to each sample in the set.</li>
</ul>
<p style="padding-left: 60px">In order to improve the quality of our self-recorded training set, we will make sure that all data samples are aligned using <strong>affine transformations</strong>, and will reduce the dimensionality of the feature space by applying <strong>PCA</strong>. The resulting representation is sometimes also referred to as <strong>Eigenfaces</strong>.</p>
<p class="CDPAlignLeft CDPAlign">We will combine the algorithms mentioned earlier in a single end-to-end app that annotates a detected face with the corresponding facial expression label in each captured frame of a video live stream. The end result might look something like the following screenshot, capturing my sample reaction when the code first ran:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b4d5bb8e-eb59-466a-b7d4-50e341e42db1.png" style="width:29.58em;height:22.92em;"/></p>
<p>The final app will consist of the main script that integrates the process flow end to end—that is, from face detection to facial expression recognition, as well as some utility functions to help along the way.</p>
<p>Thus, the end product will require several components that are located in the <kbd>chapter8/</kbd> directory of the book's GitHub repository, listed as follows:</p>
<ul>
<li><kbd>chapter8.py</kbd>: This is the main script and entry point for the chapter, and we will use this for both data collection and demo. It will have the following layouts:
<ul>
<li><kbd>chapter8.FacialExpressionRecognizerLayout</kbd>: This is a custom layout based on <kbd>wx_gui.BaseLayout</kbd> that will detect a face in each video frame and predict the corresponding class label by using a pre-trained model.</li>
<li><kbd>chapter8.DataCollectorLayout</kbd>: This is a custom layout based on <kbd>wx_gui.BaseLayout</kbd> that will collect image frames, detect a face therein, assign a label using a user-selected facial expression label, and will save the frames into the <kbd>data/</kbd> directory.</li>
</ul>
</li>
<li><kbd>wx_gui.py</kbd>: This is a link to our <kbd>wxpython</kbd> GUI file that we developed in <a href="2e878463-75f1-40a5-b263-0c5aa9627328.xhtml">Chapter 1</a>, <em>Fun with Filters</em>.</li>
<li><kbd>detectors.FaceDetector</kbd>: This is a class that will encompass all the code for face detection based on Haar cascades. It will have the following two methods:<br/>
<ul>
<li><kbd>detect_face</kbd>: This method detects faces in a grayscale image. Optionally, the image is downscaled for better reliability. Upon successful detection, the method returns the extracted head region.</li>
<li><kbd>align_head</kbd>: This method preprocesses an extracted head region with affine transformations, such that the resulting face appears centered and upright.</li>
</ul>
</li>
<li><kbd>params/</kbd>: This is a directory that contains the default Haar cascades that we use for the book.</li>
<li><kbd>data/</kbd>: We will write all the code to store and process our custom data here. The code is split into the following files:<br/>
<ul>
<li><kbd>store.py</kbd>: This is a file where we put all the helper functions to write the data to disk and to load the data from the disk into computer memory.</li>
<li><kbd>process.py</kbd>: This is a file that will contain all the code to preprocess the data before saving. It will also contain the code to construct features from the raw data.</li>
</ul>
</li>
</ul>
<p class="mce-root">In the following sections, we will discuss these components in detail. First, let's look at the face detection algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about face detection</h1>
                
            
            
                
<p>OpenCV comes preinstalled with a range of sophisticated classifiers for general-purpose object detection. These all have very similar APIs and are easy to use, once you know what you are looking for. Perhaps the most commonly known detector is the <strong>cascade of Haar-based feature detectors</strong> for face detection, which was first introduced by Paul Viola and Michael Jones in their paper <em>Rapid Object Detection using a Boosted Cascade of Simple Features</em> in 2001.</p>
<p>A Haar-based feature detector is a machine learning algorithm that is trained on a lot of positive and negative labeled samples. What will we do in our application is take a pre-trained classifier that comes with OpenCV (you can find the link in the <em>Getting started </em>section). But first, let's take a closer look at how the classifier works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about Haar-based cascade classifiers</h1>
                
            
            
                
<p>Every book on OpenCV should at least mention the Viola-Jones face detector. Invented in 2001, this cascade classifier disrupted the field of computer vision, as it finally allowed real-time face detection and face recognition.</p>
<p>The classifier is based on <strong>Haar-like features</strong> (similar to <strong>Haar basis functions</strong>) that sum up the pixel intensities in small regions of an image, as well as capture the difference between adjacent image regions.</p>
<p>The following screenshot visualizes four rectangle features. The visualization works to calculate the value of the feature applied at a location. You should sum up all pixel values in the dark gray rectangle and subtract this value from the sum of all pixel values in the white rectangle:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/83c2064f-8325-4c2e-af90-ab2fdd8ded16.png" style="width:22.42em;height:15.50em;"/></p>
<p>In the previous screenshot, the top row shows two examples of an edge feature (that is, you can detect edges with them), either vertically oriented (top left) or oriented at a 45º angle (top right). The bottom row shows a line feature (bottom left) and a center-surround feature (bottom right).</p>
<p>Applying these filters at all possible locations allows the algorithm to capture certain details of human faces, such as the fact that eye regions are usually darker than the region surrounding the cheeks.</p>
<p>Thus, a common Haar feature would have a dark rectangle (representing the eye region) atop a bright rectangle (representing the cheek region). Combining this feature with a bank of rotated and slightly more complicated <strong>wavelets</strong>, Viola and Jones arrived at a powerful feature descriptor for human faces. In an additional act of brilliance, these guys came up with an efficient way to calculate these features, making it possible for the first time to detect faces in real time.</p>
<p>The final classifier is a weighted sum of small weaker classifiers, each of whose binary classifiers are based on a single feature described previously. The hardest part is to figure out which combinations of features are helpful for detecting different types of objects. Luckily, OpenCV contains a collection of such classifiers. Let's take a look at some of these in the following section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding pre-trained cascade classifiers</h1>
                
            
            
                
<p>Even better, this approach not only works for faces but also for eyes, mouths, full bodies, company logos; you name it. In the following table, a number of pre-trained classifiers are shown that can be found under the OpenCV install path in the <kbd>data</kbd> folder:</p>
<table style="border-collapse: collapse;width: 100%" class="table" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Cascade classifier types</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>XML filenames</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Face detector (default)</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><strong><kbd>haarcascade_frontalface_default.xml</kbd></strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Face detector (fast Haar)</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><kbd>haarcascade_frontalface_alt2.xml</kbd></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Eye detector</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><kbd>haarcascade_eye.xml</kbd></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Mouth detector</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><kbd>haarcascade_mcs_mouth.xml</kbd></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Nose detector</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><kbd>haarcascade_mcs_nose.xml</kbd></p>
</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p>Full body detector</p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><kbd>haarcascade_fullbody.xml</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In this chapter, we will only use <kbd>haarcascade_frontalface_default.xml</kbd> and  <kbd>haarcascade_eye.xml</kbd>.</p>
<p>If you are wearing glasses, make sure to use <kbd>haarcascade_eye_tree_eyeglasses.xml</kbd> for eye detection instead.</p>
<p>We will first look at how to use a cascade classifier.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using a pre-trained cascade classifier</h1>
                
            
            
                
<p>A cascade classifier can be loaded and applied to an image (grayscale) using the following code, where we first read the image, then convert it to grayscale, and finally detect all the faces using a cascade classifier:</p>
<pre>import cv2

gray_img = cv2.cvtColor(cv2.imread('example.png'), cv2.COLOR_RGB2GRAY)<br/><br/>cascade_clf = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/>faces = cascade_clf.detectMultiScale(gray_img,<br/>                                     scaleFactor=1.1,<br/>                                     minNeighbors=3,<br/>                                     flags=cv2.CASCADE_SCALE_IMAGE)</pre>
<p>From the previous code, the <kbd>detectMultiScale</kbd> function comes with a number of options: </p>
<ul>
<li><kbd>minFeatureSize</kbd> is the minimum face size to consider—for example, 20 x 20 pixels.</li>
<li><kbd>searchScaleFactor</kbd> is the amount by which we rescale the image (scale pyramid). For example, a value of <kbd>1.1</kbd> will gradually reduce the size of the input image by 10 %, making it more likely for a face (image) with a larger value to be found.</li>
<li><kbd>minNeighbors</kbd> is the number of neighbors that each candidate rectangle will have to retain. Typically, we choose <kbd>3</kbd> or <kbd>5</kbd>.</li>
<li><kbd>flags</kbd> is an options object used to tweak the algorithm—for example, whether to look for all faces or just the largest face (<kbd>cv2.cv.CASCADE_FIND_BIGGEST_OBJECT</kbd>).</li>
</ul>
<p>If detection is successful, the function will return a list of bounding boxes (<kbd>faces</kbd>) that contain the coordinates of the detected face regions, as follows:</p>
<pre>for (x, y, w, h) in faces: 
    # draw bounding box on frame 
    cv2.rectangle(frame, (x, y), (x + w, y + h), (100, 255, 0), <br/>                  thickness=2) </pre>
<p>In the previous code, we iterate through the returned faces and add a rectangle outline with a thickness of <kbd>2</kbd> pixels to each of the faces.</p>
<p>If your pre-trained face cascade does not detect anything, a common reason is usually that the path to the pre-trained cascade file could not be found. In this case, <kbd>CascadeClassifier</kbd> will fail silently. Thus, it is always a good idea to check whether the returned classifier <kbd>casc = cv2.CascadeClassifier(filename)</kbd> is empty, by checking <kbd>casc.empty()</kbd>.</p>
<p>This is what you should get if you run the code on the <kbd>Lenna.png</kbd> picture:</p>
<div><img src="img/e215013d-803e-4bd0-a29f-344637650c9e.png" style="width:47.50em;height:23.58em;"/></div>
<p>Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0</p>
<p class="mce-root">From the previous screenshot, on the left, you see the original image, and on the right is the image that was passed to OpenCV, and the rectangle outline of the detected face.</p>
<p class="mce-root">Now, let's try to wrap this detector into a class to make it usable for our application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the FaceDetector class</h1>
                
            
            
                
<p>All relevant face detection code for this chapter can be found as part of the <kbd>FaceDetector</kbd> class in the <kbd>detectors</kbd> module. Upon instantiation, this class loads two different cascade classifiers that are needed for preprocessing—namely, a <kbd>face_cascade</kbd> classifier and an <kbd>eye_cascade</kbd> classifier, as follows:</p>
<pre>import cv2 
import numpy as np 
 
 
class FaceDetector:<br/><br/>    def __init__(self, *,<br/>                 face_cascade='params/haarcascade_frontalface_default.xml',<br/>                 eye_cascade='params/haarcascade_lefteye_2splits.xml',<br/>                 scale_factor=4):</pre>
<p>Because our preprocessing requires a valid face cascade, we make sure that the file can be loaded. If not, we throw a <kbd>ValueError</kbd> exception, so the program will terminate and notify the user what went wrong, as shown in the following code block:</p>
<pre>        # load pre-trained cascades<br/>        self.face_clf = cv2.CascadeClassifier(face_cascade)<br/>        if self.face_clf.empty():<br/>            raise ValueError(f'Could not load face cascade <br/>            "{face_cascade}"')</pre>
<p>We do the same thing for the eye classifier as well, like this:</p>
<pre>        self.eye_clf = cv2.CascadeClassifier(eye_cascade)<br/>        if self.eye_clf.empty():<br/>            raise ValueError(<br/>                f'Could not load eye cascade "{eye_cascade}"')</pre>
<p>Face detection works best on low-resolution grayscale images. This is why we also store a scaling factor (<kbd>scale_factor</kbd>) so that we can operate on downscaled versions of the input image if necessary, like this:</p>
<pre>self.scale_factor = scale_factor </pre>
<p>Now that we have set up the class initialization, let's take a look at the algorithm that detects the faces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Detecting faces in grayscale images</h1>
                
            
            
                
<p>Now, we will put what we learned in the previous section into a method that will take an image and return the biggest face in the image. We are returning the biggest face to simplify things since we know that, in our application, there is going to be a single user sitting in front of the webcam. As a challenge, you could try to expand this to work with more than one face!</p>
<p>We call the method to detect the biggest face (<kbd>detect_face</kbd>). Let's go through it step by step:</p>
<ol>
<li class="mce-root">As in the last section, first, we convert the argument RGB image to grayscale and scale it by <kbd>scale_factor</kbd> by running the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">    def detect_face(self, rgb_img, *, outline=True):<br/>        frameCasc = cv2.cvtColor(cv2.resize(rgb_img, (0, 0),<br/>                                            fx=1.0 / <br/>                                            self.scale_factor,<br/>                                            fy=1.0 / <br/>                                            self.scale_factor),<br/>                                 cv2.COLOR_RGB2GRAY)</pre>
<ol start="2">
<li>Then, we detect the faces in the grayscale image, like this:</li>
</ol>
<pre style="padding-left: 60px">    faces = self.face_clf.detectMultiScale(<br/>            frameCasc,<br/>            scaleFactor=1.1,<br/>            minNeighbors=3,<br/>            flags=cv2.CASCADE_SCALE_IMAGE) * self.scale_factor</pre>
<ol start="3">
<li class="mce-root">We iterate over the detected faces and outline if the <kbd>outline=True</kbd> keyword argument was passed to <kbd>detect_face</kbd>. OpenCV returns us <kbd>x, y</kbd> coordinates of the top-left location and <kbd>w, h</kbd> width and height of the head. So, in order to construct the outline, we just calculate the bottom and right coordinates of the outline, and call the <kbd>cv2.rectangle</kbd> function, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">     for (x, y, w, h) in faces:<br/>            if outline:<br/>                cv2.rectangle(rgb_img, (x, y), (x + w, y + h), <br/>                              (100, 255, 0), thickness=2)</pre>
<ol start="4">
<li>We crop the head out of the original RGB image. This will be handy if we want to do more processing on the head (for example, recognize the facial expression). Run the following code:</li>
</ol>
<pre style="padding-left: 60px">        head = cv2.cvtColor(rgb_img[y:y + h, x:x + w],<br/>                            cv2.COLOR_RGB2GRAY)</pre>
<ol start="5">
<li>We return the following 4-tuple:
<ul>
<li>A Boolean value to check whether the detection was successful or not</li>
<li>The original image with the outline of the faces added (if requested)</li>
<li>A cropped image of the head to use as needed</li>
<li>Coordinates of the location of the head in the original image</li>
</ul>
</li>
<li>In the case of success, we return the following:</li>
</ol>
<pre style="padding-left: 60px">         return True, rgb_img, head, (x, y)</pre>
<p style="padding-left: 60px">In the case of failure, we return that no head was found, and return <kbd>None</kbd> for anything that is undetermined, like this:</p>
<pre style="padding-left: 60px">        return False, rgb_img, None, (None, None)</pre>
<p class="mce-root">Now, let's look at what happens after we detect the faces, to get them ready for machine learning algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preprocessing detected faces</h1>
                
            
            
                
<p>After a face has been detected, we might want to preprocess the extracted head region before applying classification to it. Although the face cascade is fairly accurate, for the recognition, it is important that all the faces are upright and centered on the image.</p>
<p>Here is what we want to accomplish:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d57ee527-7d22-4fef-bd90-e7df1f65b0a1.png" style="width:61.58em;height:21.58em;"/></p>
<p>Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0</p>
<p>As you can see from the preceding screenshot, as this is not a passport photo, the model has her head slightly tilted to the side while looking over her shoulder. The facial region, as extracted by the face cascade, is shown in the middle thumbnail in the preceding screenshot.</p>
<p>In order to compensate for the head orientation and position in the detected box, we aim to rotate, move, and scale the face so that all data samples will be perfectly aligned. This is the job of the <kbd>align_head</kbd> method in the <kbd>FaceDetector</kbd> class, shown in the following code block:</p>
<pre>    def align_head(self, head):<br/>        desired_eye_x = 0.25<br/>        desired_eye_y = 0.2<br/>        desired_img_width = desired_img_height = 200</pre>
<p>In the previous code, we have hardcoded some parameters that are used to align the heads. We want all eyes to be 25 % below the top of the final image and 20 % from the left and right edges, and this function is going to return a processed image of the head that has a fixed size of 200 x 200 pixels.</p>
<p>The first step of the process is to detect where the eyes are in the image, after which we will use their location to construct the necessary transformation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Detecting the eyes</h1>
                
            
            
                
<p>Fortunately, OpenCV comes with a few eye cascades that can detect both open and closed eyes, such as <kbd>haarcascade_eye.xml</kbd>. This allows us to calculate the angle between the line that connects the center of the two eyes and the horizon so that we can rotate the face accordingly.</p>
<p>In addition, adding eye detectors will reduce the risk of having false positives in our dataset, allowing us to add a data sample only if both the head and the eyes have been successfully detected.</p>
<p>After loading the eye cascade from the file in the <kbd>FaceDetector</kbd> constructor, it is applied to the input image (<kbd>head</kbd>), as follows:</p>
<pre>        try:<br/>            eye_centers = self.eye_centers(head)<br/>        except RuntimeError:<br/>            return False, head</pre>
<p>If we are unsuccessful and the cascade classifier couldn't find an eye, OpenCV will throw a <kbd>RuntimeError</kbd>. Here, we are catching it and returning a <kbd>(False, head)</kbd> tuple, indicating that we failed to align the head.</p>
<p>Next, we try to order the references to the eyes that the classifier has found. We set <kbd>left_eye</kbd> to be the eye with the lower first coordinate—that is, the one on the left, as follows:</p>
<pre>        if eye_centers[0][0] &lt; eye_centers[0][1]:<br/>            left_eye, right_eye = eye_centers<br/>        else:<br/>            right_eye, left_eye = eye_centers</pre>
<p>Now that we have the location of both of the eyes, we want to figure out what kind of transformation we want to make in order to put the eyes in the hardcoded positions—that is, 25% from the sides and 25% below the top of the image.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Transforming the face</h1>
                
            
            
                
<p>Transforming the face is a standard process that can be achieved by warping the image using <kbd>cv2.warpAffine</kbd> (recall <a href="905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml">Chapter 3</a>, <em>Finding Objects via Feature Matching and Perspective Transforms</em>). We will follow the next steps to achieve this transformation:</p>
<ol>
<li>First, we calculate the angle (in degrees) between the line that connects the two eyes and a horizontal line, as follows:</li>
</ol>
<pre style="padding-left: 60px">        eye_angle_deg = 180 / np.pi * np.arctan2(right_eye[1] <br/>                                                 - left_eye[1],<br/>                                                 right_eye[0] <br/>                                                 - left_eye[0])</pre>
<ol start="2">
<li>Then, we derive a scaling factor that will scale the distance between the two eyes to be exactly 50% of the image width, like this:</li>
</ol>
<pre style="padding-left: 60px">        eye_dist = np.linalg.norm(left_eye - right_eye)<br/>        eye_size_scale = (1.0 - desired_eye_x * 2) * <br/>        desired_img_width / eye_dist</pre>
<ol start="3">
<li>With the two parameters (<kbd>eye_angle_deg</kbd> and <kbd>eye_size_scale</kbd>) in hand, we can now come up with a suitable rotation matrix that will transform our image, as follows:</li>
</ol>
<pre style="padding-left: 60px">        eye_midpoint = (left_eye + right_eye) / 2<br/>        rot_mat = cv2.getRotationMatrix2D(tuple(eye_midpoint), <br/>                                          eye_angle_deg,<br/>                                          eye_size_scale)</pre>
<ol start="4">
<li>Next, we will make sure that the center of the eyes will be centered in the image, like this:</li>
</ol>
<pre style="padding-left: 60px">        rot_mat[0, 2] += desired_img_width * 0.5 - eye_midpoint[0]<br/>        rot_mat[1, 2] += desired_eye_y * desired_img_height - <br/>        eye_midpoint[1]</pre>
<ol start="5">
<li>Finally, we arrive at an upright scaled version of the facial region that looks like the third image (named as Training Image) in the previous screenshot (eye regions are highlighted only for the demonstration), as follows:</li>
</ol>
<pre style="padding-left: 60px">        res = cv2.warpAffine(head, rot_mat, (desired_img_width,<br/>                                             desired_img_width))<br/>        return True, res</pre>
<p>After this step, we know how to extract nicely aligned, cropped, and rotated images from unprocessed images. Now, it's time to take a look at how to use these images to identify facial expressions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Collecting data</h1>
                
            
            
                
<p>The facial-expression-recognition pipeline is encapsulated in <kbd>chapter8.py</kbd>. This file consists of an interactive GUI that operates in two modes (<strong>training</strong> and <strong>testing</strong>), as described earlier.</p>
<p>Our entire application is divided into parts, mentioned as follows:</p>
<ol>
<li>Running the application in the <kbd>collect</kbd> mode using the following command from the command line:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>$ python chapter8.py collect</strong></pre>
<p style="color: black;padding-left: 60px">The previous command will pop up a GUI in the data collection mode to assemble a training set,<br/>
training an MLP classifier on the training set via <kbd>python train_classifier.py</kbd>. Because this step can take a long time, the process takes place in its own script. After successful training, store the trained weights in a file, so that we can load the pre-trained MLP in the next step.</p>
<ol start="2">
<li>Then, again running the GUI in the <kbd>demo</kbd> mode as follows, we will be able to see how good the facial recognition is on the real data:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>$ python chapter8.py demo</strong></pre>
<p style="color: black;padding-left: 60px">In this mode, you will have a GUI to classify facial expressions on a live video stream in real time. This step involves loading several pre-trained cascade classifiers as well as our pre-trained MLP classifier. These classifiers will then be applied to every captured video frame.</p>
<p>Now, let's take a look at how you can build an application to collect training data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Assembling a training dataset</h1>
                
            
            
                
<p>Before we can train an MLP, we need to assemble a suitable training set. This is done because chances are that your face is not yet part of any dataset out there (the <strong>National Security Agency's</strong> (<strong>NSA's</strong>) private collection doesn't count), thus we will have to assemble our own. This is done most easily by returning to our GUI application from the previous chapters that can access a webcam, and operate on each frame of a video stream.</p>
<p>We are going to subclass the <kbd>wx_gui.BaseLayout</kbd> and tweak the <strong>user interface</strong> (<strong>UI</strong>) to our liking. We will have two classes for the two different modes.</p>
<p>The GUI will present the user with the option of recording one of the following six emotional expressions—namely, neutral, happy, sad, surprised, angry, and disgusted. Upon clicking a button, the app will take a snapshot of the detected facial region and add it to the data collection in a file.</p>
<p>These samples can then be loaded from the file and used to train a machine learning classifier in <kbd>train_classifier.py</kbd>, as described in <em>Step 2</em> (given earlier).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the application</h1>
                
            
            
                
<p>As we have seen in the previous chapters with a <strong>wxpython GUI</strong>, in order to run this app (<kbd>chapter8.py</kbd>), we need to set up a screen capture by using <kbd>cv2.VideoCapture</kbd>, and pass the handle to the <kbd>FaceLayout</kbd> class. We can do this by following the next steps:</p>
<ol>
<li>First, we create a <kbd>run_layout</kbd> function that will work with any <kbd>BaseLayout</kbd> subclass, as follows:</li>
</ol>
<pre style="padding-left: 60px">def run_layout(layout_cls, **kwargs):<br/>    # open webcam<br/>    capture = cv2.VideoCapture(0)<br/>    # opening the channel ourselves, if it failed to open.<br/>    if not(capture.isOpened()):<br/>        capture.open()<br/><br/>    capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)<br/>    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)<br/><br/>    # start graphical user interface<br/>    app = wx.App()<br/>    layout = layout_cls(capture, **kwargs)<br/>    layout.Center()<br/>    layout.Show()<br/>    app.MainLoop()</pre>
<p style="padding-left: 60px">As you can see, the code is very similar to the code from previous chapters that used <kbd>wxpython</kbd>. We open the webcam, set the resolution, initialize the layout, and start the main loop of the application. This type of optimization is good when you have to use the same function multiple times.</p>
<ol start="2">
<li>Next, we set up an argument parser that will figure out which of the two layouts needs to be run and run it with the appropriate arguments.</li>
</ol>
<p style="padding-left: 60px">To make use of the <kbd>run_layout</kbd> function in both modes, we add a command-line argument to our script using the <kbd>argparse</kbd> module, like this:</p>
<pre style="padding-left: 60px">if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument('mode', choices=['collect', 'demo'])<br/>    parser.add_argument('--classifier')<br/>    args = parser.parse_args()</pre>
<p style="padding-left: 60px">We have used the <kbd>argparse</kbd> module that comes with Python to set up an argument parser and add an argument with <kbd>collect</kbd> and <kbd>demo</kbd> options. We have also added an optional <kbd>--classifier</kbd> argument that we will use for <kbd>demo</kbd> mode only.</p>
<ol start="3">
<li>Now, we use all the arguments that the user passed, to call the <kbd>run_layout</kbd> function with appropriate arguments, as follows:</li>
</ol>
<pre style="padding-left: 60px">    if args.mode == 'collect':<br/>        run_layout(DataCollectorLayout, title='Collect Data')<br/>    elif args.mode == 'demo':<br/>        assert args.svm is not None, 'you have to provide --svm'<br/>        run_layout(FacialExpressionRecognizerLayout,<br/>                   title='Facial Expression Recognizer',<br/>                   classifier_path=args.classifier)</pre>
<p style="padding-left: 60px">As you can see in the previous code, we have set it up to pass an extra <kbd>classifier_path</kbd> argument when we are in the <kbd>demo</kbd> mode. We will see how it is being used when we talk about <kbd>FacialExpresssionRecognizerLayout</kbd> in the later sections of this chapter.</p>
<p>Now that we have established how to run our application, let's build the GUI elements.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the data collector GUI</h1>
                
            
            
                
<p>Analogous to some of the previous chapters, the GUI of the app is a customized version of the generic <kbd>BaseLayout</kbd>, as shown in the following code block:</p>
<pre>import wx<br/>from wx_gui import BaseLayout<br/><br/>class DataCollectorLayout(BaseLayout):</pre>
<p>We start building the GUI by calling the constructor of the parent class to make sure it's correctly initialized, like this:</p>
<pre>    def __init__(self, *args,<br/>                 training_data='data/cropped_faces.csv',<br/>                 **kwargs):<br/>        super().__init__(*args, **kwargs)</pre>
<p>Notice that we have added some extra arguments in the previous code. Those are for all extra attributes that our class has and that the parent class doesn't.</p>
<p>Next, before we go on to adding UI components, we also initialize a <kbd>FaceDetector</kbd> instance and a reference to the file to store data, as follows:</p>
<pre>        self.face_detector = FaceDetector(<br/>            face_cascade='params/haarcascade_frontalface_default.xml',<br/>            eye_cascade='params/haarcascade_eye.xml')<br/>        self.training_data = training_data</pre>
<p>Notice that we are using the hardcoded cascade XML files. Feel free to experiment with these as well.</p>
<p>Now, let's take a look at how we construct the UI using <kbd>wxpython</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Augmenting the basic layout</h1>
                
            
            
                
<p>The creation of the layout is again deferred to a method called <kbd>augment_layout</kbd>. We keep the layout as simple as possible. We create a panel for the acquired video frame and draw a row of buttons below it.</p>
<p>The idea is to then click one of the six radio buttons to indicate which facial expression you are trying to record, then place your head within the bounding box, and click the <kbd>Take Snapshot</kbd> button.</p>
<p>So, let's have a look at how to build the six buttons, and correctly place them on a <kbd>wx.Panel</kbd> object. The code for this is shown in the following block:</p>
<pre>    def augment_layout(self):<br/>        pnl2 = wx.Panel(self, -1)<br/>        self.neutral = wx.RadioButton(pnl2, -1, 'neutral', (10, 10),<br/>                                      style=wx.RB_GROUP)<br/>        self.happy = wx.RadioButton(pnl2, -1, 'happy')<br/>        self.sad = wx.RadioButton(pnl2, -1, 'sad')<br/>        self.surprised = wx.RadioButton(pnl2, -1, 'surprised')<br/>        self.angry = wx.RadioButton(pnl2, -1, 'angry')<br/>        self.disgusted = wx.RadioButton(pnl2, -1, 'disgusted')<br/>        hbox2 = wx.BoxSizer(wx.HORIZONTAL)<br/>        hbox2.Add(self.neutral, 1)<br/>        hbox2.Add(self.happy, 1)<br/>        hbox2.Add(self.sad, 1)<br/>        hbox2.Add(self.surprised, 1)<br/>        hbox2.Add(self.angry, 1)<br/>        hbox2.Add(self.disgusted, 1)<br/>        pnl2.SetSizer(hbox2)</pre>
<p>You can see that even if there is a lot of code, what we wrote is mostly repetitive. We create a <kbd>RadioButton</kbd> for each emotion and add the button to a <kbd>pnl2</kbd> panel.</p>
<p>The <kbd>Take Snapshot</kbd> button is placed below the radio buttons and will bind to the <kbd>_on_snapshot</kbd> method, as follows:</p>
<pre>        # create horizontal layout with single snapshot button<br/>        pnl3 = wx.Panel(self, -1)<br/>        self.snapshot = wx.Button(pnl3, -1, 'Take Snapshot')<br/>        self.Bind(wx.EVT_BUTTON, self._on_snapshot, self.snapshot)<br/>        hbox3 = wx.BoxSizer(wx.HORIZONTAL)<br/>        hbox3.Add(self.snapshot, 1)<br/>        pnl3.SetSizer(hbox3)</pre>
<p>As the comment suggests, we created a new panel and added a regular button with the <kbd>Take Snapshot</kbd> label. The important part is that we bind the click on the button to the <kbd>self._on_snapshot</kbd> method, which will process each captured image once we click on the <kbd>Take Snapshot</kbd> button.</p>
<p>The layout will look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/84c0313e-4ee5-4ad5-8475-5b67e47d6df6.png" style="width:29.25em;height:24.42em;"/></p>
<p>To make these changes take effect, the created panels need to be added to the list of existing panels, like this:</p>
<pre>        # arrange all horizontal layouts vertically<br/>        self.panels_vertical.Add(pnl2, flag=wx.EXPAND | wx.BOTTOM, <br/>                                 border=1)<br/>        self.panels_vertical.Add(pnl3, flag=wx.EXPAND | wx.BOTTOM, <br/>                                 border=1)</pre>
<p>The rest of the visualization pipeline is handled by the <kbd>BaseLayout</kbd> class.</p>
<p>Now, let's see how we add boundary boxes to the faces once they appear in the video capture, using the <kbd>process_frame</kbd> method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Processing the current frame</h1>
                
            
            
                
<p>The <kbd>process_frame</kbd> method is called on all the images, and we'd like to show a frame around a face when it appears in the video feed. This is illustrated in the following code block:</p>
<pre>    def process_frame(self, frame_rgb: np.ndarray) -&gt; np.ndarray:<br/>        _, frame, self.head, _ = self.face_detector.detect_face(frame_rgb)<br/>        return frame</pre>
<p>We have just called the  <kbd>FaceDetector.detect_face</kbd> method of the <kbd>self.face_detector</kbd> object we created in the constructor of the layout class. Remember from the previous section that it detects faces in a downscaled grayscale version of the current frame using Haar cascades.</p>
<p>So, we are adding a frame if we recognize a face; that's it. Now, let's look at how we store training images inside the <kbd>_on_snapshot</kbd> method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Storing the data</h1>
                
            
            
                
<p>We will store the data once the user clicks on the Take Snapshot button, and the <kbd>_on_snapshot</kbd> event listener method is called, as shown in the following code block:</p>
<pre>    def _on_snapshot(self, evt):<br/>        """Takes a snapshot of the current frame<br/><br/>           This method takes a snapshot of the current frame, preprocesses<br/>           it to extract the head region, and upon success adds the data<br/>           sample to the training set.<br/>        """</pre>
<p>Let's take a look at the code inside this method, as follows:</p>
<ol>
<li>First, we figure out the label of the image by finding out which of the radio buttons was selected, like this:</li>
</ol>
<pre style="padding-left: 60px">        if self.neutral.GetValue():<br/>            label = 'neutral'<br/>        elif self.happy.GetValue():<br/>            label = 'happy'<br/>        elif self.sad.GetValue():<br/>            label = 'sad'<br/>        elif self.surprised.GetValue():<br/>            label = 'surprised'<br/>        elif self.angry.GetValue():<br/>            label = 'angry'<br/>        elif self.disgusted.GetValue():<br/>            label = 'disgusted'</pre>
<p style="padding-left: 60px">As you can see, it's very straightforward, once we realize that each radio button has a <kbd>GetValue()</kbd> method that returns <kbd>True</kbd> only if it was selected.</p>
<ol start="2">
<li>Next, we need to look at the detected facial region of the current frame (stored in <kbd>self.head</kbd> by <kbd>detect_head</kbd>) and align it with all the other collected frames. That is, we want all the faces to be upright and the eyes to be aligned.</li>
</ol>
<p style="padding-left: 60px">Otherwise, if we do not align the data samples, we run the risk of having the classifier compare eyes to noses. Because this computation can be costly, we do not apply it on every frame in the <kbd>process_frame</kbd> method, but instead only upon taking a snapshot in the <kbd>_on_snapshot</kbd> method, as follows:</p>
<pre style="padding-left: 60px">        if self.head is None:<br/>            print("No face detected")<br/>        else:<br/>            success, aligned_head = <br/>            self.face_detector.align_head(self.head)</pre>
<p style="padding-left: 60px">Since this happened after <kbd>process_frame</kbd> was called, we already had access to <kbd>self.head</kbd>, which stored the image of the head present in the current frame.</p>
<ol start="3">
<li>Next, if we have successfully aligned the head (that is, if we have found the eyes), we will store the datum. Otherwise, we notify the user, using a <kbd>print</kbd> command to the Terminal, as follows:</li>
</ol>
<pre style="padding-left: 60px">            if success:<br/>                save_datum(self.training_data, label, aligned_head)<br/>                print(f"Saved {label} training datum.")<br/>            else:<br/>                print("Could not align head (eye detection <br/>                                             failed?)")</pre>
<p>Actual saving is done in the  <kbd>save_datum</kbd> function, which we have abstracted away since it is not part of the UI. Also, this is handy in case you want to add a different dataset to your file, as illustrated in the following code block:</p>
<pre>def save_datum(path, label, img):<br/>    with open(path, 'a', newline='') as outfile:<br/>        writer = csv.writer(outfile)<br/>        writer.writerow([label, img.tolist()])</pre>
<p>As you can see in the previous code, we use a <kbd>.csv</kbd> file to store the data, where each of the images is a <kbd>newline</kbd>. So, if you want to go back and delete an image (maybe you had forgotten to comb your hair), you just have to open the <kbd>.csv</kbd> file with a text editor and delete that line.</p>
<p>Now, let's move to more interesting parts, and find out how we are going to use the data we collect to be able to train a machine learning model to detect emotions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding facial emotion recognition</h1>
                
            
            
                
<p>In this section, we will train an MLP to recognize facial emotions in the pictures.</p>
<p>We have previously made the point that finding the features that best describe the data is often an essential part of the entire learning task. We have also looked at common preprocessing methods, such as <strong>mean subtraction</strong> and <strong>normalization</strong>.</p>
<p>Here, we will look at an additional method that has a long tradition in face recognition—that is, PCA. We are hoping that, even if we don't collect thousands of training pictures, PCA will help us get good results.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Processing the dataset</h1>
                
            
            
                
<p>Analogous to <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>L</em><em>earning to Recognize Traffic Signs</em>, we write a new dataset parser in <kbd>data/emotions.py</kbd> that will parse our self-assembled training set.</p>
<p>We define a <kbd>load_data</kbd> function that will load the training data and <kbd>return</kbd> a tuple of collected <kbd>data</kbd> and their corresponding labels, as follows:</p>
<pre>def load_collected_data(path):<br/>    data, targets = [], []<br/>    with open(path, 'r', newline='') as infile:<br/>        reader = csv.reader(infile)<br/>        for label, sample in reader:<br/>            targets.append(label)<br/>            data.append(json.loads(sample))<br/>    return data, targets</pre>
<p>This code, similar to all the processing codes, is self-contained and resides in the <kbd>data/process.py</kbd> file, similar to <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>.</p>
<p>Our featurization function in this chapter is going to be the <kbd>pca_featurize</kbd> function that will perform PCA on all samples. But unlike <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>, our featurization function takes into account the characteristics of the entire dataset, instead of operating on each of the images separately.</p>
<p>Now, instead of returning only the featurized data (as in <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>), it will return a tuple of training data, and all parameters necessary to apply the same function to the test data, as follows:</p>
<pre>def pca_featurize(data) -&gt; (np.ndarray, List)</pre>
<p class="mce-root">Now, let's figure out what is PCA, and why we need it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about PCA</h1>
                
            
            
                
<p>PCA is a dimensionality-reduction technique that is helpful whenever we are dealing with high-dimensional data. In a sense, you can think of an image as a point in a high-dimensional space. If we flatten a 2D image of height <kbd>m</kbd> and width <kbd>n</kbd> (by concatenating either all rows or all columns), we get a (feature) vector of length <em>m x n</em>. The value of the i<sup>th</sup> element in this vector is the grayscale value of the i<sup>th</sup> pixel in the image.</p>
<p>To describe every possible 2D grayscale image with these exact dimensions, we will need an <em>m x n</em>-dimensional vector space that contains <em>256<sup>m x n</sup></em> vectors. Wow!</p>
<p>An interesting question that comes to mind when considering these numbers is—<em>Could there be a smaller, more compact vector space (using less-than m x n features) that describes all these images equally well?</em> After all, we have previously realized that grayscale values are not the most informative measures of content.</p>
<p>This is where PCA comes into the picture. Consider a dataset from which we extracted exactly two features. These features could be the grayscale values of pixels at some <em>x</em> and <em>y</em> positions, but they could also be more complex than that. If we plot the dataset along these two feature axes, the data might be mapped within some multivariate Gaussian distribution, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/97e27eaa-2ed0-4baf-b382-71d3d3faa250.png" style="width:22.00em;height:16.08em;"/></p>
<p>What PCA does is <em>rotate</em> all data points until the data is mapped aligned with the two axes (the two inset vectors) that explain most of the <em>spread</em> of the data. PCA considers these two axes to be the most informative because, if you walk along with them, you can see most of the data points separated. In more technical terms, PCA aims to transform the data to a new coordinate system by means of an orthogonal linear transformation.</p>
<p>The new coordinate system is chosen such that if you project the data onto these new axes, the first coordinate (called the <strong>first principal component</strong>) observes the greatest variance. In the preceding screenshot, the small vectors drawn correspond to the eigenvectors of the covariance matrix, shifted so that their tails come to lie at the mean of the distribution.</p>
<p>If we had previously calculated a set of basis vectors (<kbd>top_vecs</kbd>) and mean (<kbd>center</kbd>), transforming the data would be straightforward, as stated in the previous paragraph—we subtract the center from each datum, then multiply those vectors by principal components, as follows:<br/></p>
<pre>def _pca_featurize(data, center, top_vecs):<br/>    return np.array([np.dot(top_vecs, np.array(datum).flatten() - center)<br/>                     for datum in data]).astype(np.float32)</pre>
<p>Notice that the previous code will work for any number of <kbd>top_vecs</kbd>; thus, if we only supply a <kbd>num_components</kbd> number of top vectors, it will reduce the dimensionality of the data to <kbd>num_components</kbd>.</p>
<p class="bookmark">Now, let's construct a <kbd>pca_featurize</kbd> function that takes only the data, and returns both the transformation and the list of arguments necessary to replicate the transformation—that is, <kbd>center</kbd> and <kbd>top_vecs</kbd>— so we can apply <kbd>_pcea_featurize</kbd> on the testing data as well, as follows:</p>
<pre>def pca_featurize(training_data) -&gt; (np.ndarray, List)</pre>
<p>Fortunately, someone else has already figured out how to do all this in Python. In OpenCV, performing PCA is as simple as calling <kbd>cv2.PCACompute</kbd>, but we have to pass correct arguments rather than re-format what we get from OpenCV. Here are the steps:</p>
<ol>
<li>First, we convert <kbd>training_data</kbd> into a NumPy 2D array, like this:</li>
</ol>
<pre style="padding-left: 60px">x_arr = np.array(training_data).reshape((len(training_data), -1)).astype(np.float32)</pre>
<ol start="2">
<li>Then, we call <kbd>cv2.PCACompute</kbd>, which computes the center of the data, and the principal components, as follows:</li>
</ol>
<pre style="padding-left: 60px">    mean, eigvecs = cv2.PCACompute(x_arr, mean=None)</pre>
<ol start="3">
<li>We can limit ourselves to the most informative components of <kbd>num_components</kbd> by running the following code:</li>
</ol>
<pre style="padding-left: 60px">    # Take only first num_components eigenvectors.<br/>    top_vecs = eigvecs[:num_components]</pre>
<p style="padding-left: 60px">The beauty of PCA is that the first principal component, by definition, explains most of the variance of the data. In other words, the first principal component is the most informative of the data. This means that we do not need to keep all of the components to get a good representation of the data.</p>
<ol start="4">
<li>We also convert <kbd>mean</kbd> to create a new <kbd>center</kbd> variable that is a 1D vector that represents the center of the data, as follows:</li>
</ol>
<pre style="padding-left: 60px">    center = mean.flatten()</pre>
<ol start="5">
<li>Finally, we return the training data processed by the <kbd>_pca_featurize</kbd> function, and the arguments necessary to pass to the <kbd>_pca_featurize</kbd> function, to replicate the same transformation so that the test data could be <em>featurized</em> in the exact same way as the train data, as follows:</li>
</ol>
<pre style="padding-left: 60px">    args = (center, top_vecs)<br/>    return _pca_featurize(training_data, *args), args</pre>
<p>Now we know how to clean and featurize our data, it's time to look at the training method we use to learn to recognize facial emotions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding MLPs</h1>
                
            
            
                
<p>MLPs have been around for a while. MLPs are <strong>artificial neural networks</strong> (<strong>ANNs</strong>) used to convert a set of input data into a set of output data.</p>
<p>At the heart of an MLP is a <strong>perceptron</strong>, which resembles (yet overly simplifies) a biological neuron. By combining a large number of perceptrons in multiple layers, the MLP is able to make nonlinear decisions about its input data. Furthermore, MLPs can be trained with <strong>backpropagation</strong>, which makes them very interesting for supervised learning.</p>
<p>The following section explains the concept of a perceptron.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding a perceptron</h1>
                
            
            
                
<p>A perceptron is a binary classifier that was invented in the 1950s by Frank Rosenblatt. A perceptron calculates a weighted sum of its inputs, and, if this sum exceeds a threshold, it outputs a <kbd>1</kbd>; else, it outputs a <kbd>0</kbd>.</p>
<p>In some sense, a perceptron is integrating evidence that its afferents signal the presence (or absence) of some object instance, and if this evidence is strong enough, the perceptron will be active (or silent). This is loosely connected to what researchers believe biological neurons are doing (or can be used to do) in the brain, hence the term <em>ANN</em>.</p>
<p>A sketch of a perceptron is depicted in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/68ef36cd-b7a3-4e7b-9f95-906df261fcea.png" style="width:23.33em;height:14.92em;"/></p>
<p>Here, a perceptron computes a weighted (<kbd>w<sub>i</sub></kbd>) sum of all its inputs (<kbd>x<sub>i</sub></kbd>), combined with a bias term (<kbd>b</kbd>). This input is fed into a nonlinear activation function (<kbd>θ</kbd>) that determines the output of the perceptron (<kbd>y</kbd>). In the original algorithm, the activation function was the <strong>Heaviside</strong> <strong>step function</strong>.</p>
<p>In modern implementations of ANNs, the activation function can be anything ranging from sigmoid to hyperbolic tangent functions. The Heaviside step function and the sigmoid function are plotted in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/74c33d6e-33a1-406b-af14-2d6562369ffa.png" style="width:38.08em;height:17.42em;"/></p>
<p>Depending on the activation function, these networks might be able to perform either classification or regression. Traditionally, one only talks of MLPs when nodes use the Heaviside step function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Knowing about deep architectures</h1>
                
            
            
                
<p>Once you have the perceptron figured out, it would make sense to combine multiple perceptrons to form a larger network. MLPs usually consist of at least three layers, where the first layer has a node (or neuron) for every input feature of the dataset, and the last layer has a node for every class label.</p>
<p>The layer in between the first and the last layer is called the hidden layer. An example of this feed-forward neural network is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/16e356e4-ba08-47b3-ad1e-b69f20123da3.png" style="width:20.83em;height:23.00em;"/></p>
<p>In a feed-forward neural network, some or all of the nodes in the input layer are connected to all the nodes in the hidden layer, and some or all of the nodes in the hidden layer are connected to some or all of the nodes in the output layer. You would usually choose the number of nodes in the input layer to be equal to the number of features in the dataset so that each node represents one feature.</p>
<p>Analogously, the number of nodes in the output layer is usually equal to the number of classes in the data, so that when an input sample of class <kbd>c</kbd> is presented, the <em>c<sup>th</sup></em> node in the output layer is active, and all others are silent.</p>
<p>It is also, of course, possible to have multiple hidden layers. Often, it is not clear beforehand what the optimal size of the network should be.</p>
<p>Typically, you will see the error rate on the training set decrease when you add more neurons to the network, as is depicted in the following screenshot (<em>thinner</em>, <em>red curve</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/971ee10c-412e-4c73-944e-54fdf9c61d3b.png" style="width:24.83em;height:16.33em;"/></p>
<p>This is because the expressive power or complexity (also referred to as the <strong>Vapnik-Chervonenkis</strong> or <strong>VC dimension</strong>) of the model increases with the increasing size of the neural network. However, the same cannot be said for the error rate on the test set shown in the preceding screenshot (<em>thicker</em>, <em>blue curve</em>).</p>
<p>Instead, you will find that, with increasing model complexity, the test error goes through its minimum, and adding more neurons to the network will not improve the performance on the test data anymore. Therefore, you would want to keep the size of the neural network to what is labeled the optimal range in the preceding screenshot, which is where the network achieves the best generalization performance.</p>
<p>You can think of it in this way—a model of weak complexity (on the far left of the plot) is probably too small to really understand the dataset that it is trying to learn, thus yielding large error rates on both the training and the test sets. This is commonly referred to as <strong>underfitting</strong>.</p>
<p>On the other hand, a model on the far right of the plot is probably so complex that it begins to memorize the specifics of each sample in the training data, without paying attention to the general attributes that make a sample stand apart from the others. Therefore, the model will fail when it has to predict data that it has never seen before, effectively yielding a large error rate on the test set. This is commonly referred to as <strong>overfitting</strong>.</p>
<p>Instead, what you want is to develop a model that neither is underfitting nor overfitting. Often, this can only be achieved by <em>trial and error</em>; that is, by considering the network size as a hyperparameter that needs to be tweaked and tuned, depending on the exact task to be performed.</p>
<p>An MLP learns by adjusting its weights so that when an input sample of class <kbd>c</kbd> is presented, the <em>c<sup>th</sup></em> node in the output layer is active and all the others are silent. MLPs are trained by means of the <strong>backpropagation</strong> method, which is an algorithm to calculate the partial derivative of a <strong>loss function</strong> with respect to any synaptic weight or neuron bias in the network. These partial derivatives can then be used to update the weights and biases in the network in order to reduce the overall loss step by step.</p>
<p>A loss function can be obtained by presenting training samples to the network and by observing the network's output. By observing which output nodes are active and which are dormant, we can calculate the relative error between the output of the last layer and the true labels we provided with the loss function.</p>
<p>We then make corrections to all the weights in the network so that the error decreases over time. It turns out that the error in the hidden layer depends on the output layer, and the error in the input layer depends on the error in both the hidden layer and the output layer. Thus, in a sense, the error backpropagates through the network. In OpenCV, backpropagation is used by specifying <kbd>cv2.ANN_MLP_TRAIN_PARAMS_BACKPROP</kbd> in the training parameters.</p>
<p>Gradient descent comes in two common flavors—that is, in <strong>stochastic gradient descent</strong> and <strong>batch learning</strong>. <br/></p>
<p class="mce-root">In stochastic gradient descent, we update the weights after each presentation of a training example, whereas, in batch learning, we present training examples in batches and update the weights only after each batch is presented. In both scenarios, we have to make sure that we adjust the weights only slightly per sample (by adjusting the <strong>learning rate</strong>) so that the network slowly converges to a stable solution over time.</p>
<p>Now, after learning the theory behind MLPs, let's get our hands dirty and code this up using OpenCV.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Crafting an MLP for facial expression recognition</h1>
                
            
            
                
<p>Analogous to <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>, we will use the machine learning class that OpenCV provides, which is <kbd>ml.ANN_MLP</kbd>. Here are the steps to create and configure an MLP in OpenCV:</p>
<ol>
<li>Instantiate an empty <kbd>ANN_MLP</kbd> object, like this:</li>
</ol>
<pre style="padding-left: 60px">    mlp = cv2.ml.ANN_MLP_create()</pre>
<ol start="2">
<li>Set the network architecture—the first layer equal to the dimensionality of the data, and the last layer equal to the output size of <kbd>6</kbd> required for the number of possible emotions, as follows:</li>
</ol>
<pre style="padding-left: 60px">    mlp.setLayerSizes(np.array([20, 10, 6], dtype=np.uint8)</pre>
<ol start="3">
<li>We set the training algorithm to backpropagation, and use the symmetric sigmoid function for activation, as we discussed in the previous sections, by running the following code:</li>
</ol>
<pre style="padding-left: 60px">    mlp.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1)<br/>    mlp.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM)</pre>
<ol start="4">
<li>Finally, we set the termination criteria to either after <kbd>30</kbd> iterations or when the loss reaches values smaller than <kbd>0.000001</kbd>, as follows, and we are ready to train the MLP:</li>
</ol>
<pre style="padding-left: 60px">    mlp.setTermCriteria((cv2.TERM_CRITERIA_COUNT | <br/>                         cv2.TERM_CRITERIA_EPS, 30, 0.000001 ))</pre>
<p>In order to train the MLP, we need training data. We would also like to have an idea of how well our classifier is doing, so we need to split the collected data into training and test sets.</p>
<p>The best way to split the data would be to make sure that we don't have near-identical images in the training and testing sets—for example, the user double-clicked on the Take Snapshot button, and we have two images that were taken milliseconds apart, thus are almost identical. Unfortunately, that is a tedious and manual process and out of the scope of this book.</p>
<p>We define the signature of the function, as follows. We want to get indices of an array of size <kbd>n</kbd> and we want to specify a ratio of the train data to all the data:</p>
<pre>def train_test_split(n, train_portion=0.8):</pre>
<p>Geared with the signature, let's go over the <kbd>train_test_split</kbd> function step by step, as follows:</p>
<ol>
<li>First, we create a list of <kbd>indices</kbd> and <kbd>shuffle</kbd> them, like this:</li>
</ol>
<pre style="padding-left: 60px">    indices = np.arange(n)<br/>    np.random.shuffle(indices)</pre>
<ol start="2">
<li>Then, we calculate the number of training points that need to be in the <kbd>N</kbd> training dataset, like this:</li>
</ol>
<pre style="padding-left: 60px">    N = int(n * train_portion)</pre>
<ol start="3">
<li>After that, we create a selector for the first <kbd>N</kbd> indices for the training data, and create a selector for the rest of <kbd>indices</kbd> to be used for the test data, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">    return indices[:N], indices[N:]</pre>
<p>Now we have a model class and a training data generator, let's take a look at how to train the MLP.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training the MLP</h1>
                
            
            
                
<p>OpenCV provides all the training and predicting methods, so we have to figure out how to format our data to fit the OpenCV requirements.</p>
<p>First, we split the data into train/test, and featurize the training data, as follows:</p>
<pre>    train, test = train_test_split(len(data), 0.8)<br/>    x_train, pca_args = pca_featurize(np.array(data)[train])</pre>
<p>Here, <kbd>pca_args</kbd> are the arguments that we will need to store if we wanted to featurize any future data (for example, live frames during the demo).</p>
<p>Because the <kbd>train</kbd> method of the <kbd>cv2.ANN_MLP</kbd> module does not allow integer-valued class labels, we need to first convert <kbd>y_train</kbd> into one-hot encoding, consisting only of 0s and 1s, which can then be fed to the <kbd>train</kbd> method, as follows:</p>
<pre>    encoded_targets, index_to_label = one_hot_encode(targets)<br/>    y_train = encoded_targets[train]<br/>    mlp.train(x_train, cv2.ml.ROW_SAMPLE, y_train)</pre>
<p>The one-hot encoding is taken care of in the <kbd>one_hot_encode</kbd> function in <kbd>train_classifiers.py</kbd>, in the following way:</p>
<ol>
<li>First, we determine how many points there are in the data, like this:</li>
</ol>
<pre style="padding-left: 60px">def one_hot_encode(all_labels) -&gt; (np.ndarray, Callable):<br/>    unique_lebels = list(sorted(set(all_labels)))</pre>
<ol start="2">
<li>Each <kbd>c</kbd> class label in <kbd>all_labels</kbd> needs to be converted into a (<kbd>len(unique_labels)</kbd>) long vector of 0s and 1s, where all entries are zeros except the <em>c<sup>th</sup></em>, which is a 1. We prepare this operation by allocating a vector of zeros, like this:</li>
</ol>
<pre style="padding-left: 60px">    y = np.zeros((len(all_labels), len(unique_lebels))).astype(np.float32)</pre>
<ol start="3">
<li>Then, we create dictionaries mapping indices of columns to labels, and vice versa, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">    index_to_label = dict(enumerate(unique_lebels))<br/>    label_to_index = {v: k for k, v in index_to_label.items()</pre>
<ol start="4">
<li>The vector elements at these indices then need to be set to <kbd>1</kbd>, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">    for i, label in enumerate(all_labels):<br/>        y[i, label_to_index[label]] = 1</pre>
<ol start="5">
<li>We also return <kbd>index_to_label</kbd> so we are able to recover the label from the prediction vector, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">    return y, index_to_label</pre>
<p>We now move on to the testing of the MLP that we just trained.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing the MLP</h1>
                
            
            
                
<p>Analogous to <a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>, we will evaluate the performance of our classifier in terms of accuracy, precision, and recall.</p>
<p>To reuse our previous code, we just need to calculate <kbd>y_hat</kbd> and pass <kbd>y_true</kbd> alongside it to the metric functions by doing the following:</p>
<ol>
<li>First, we featurize our test data using the <kbd>pca_args</kbd> we stored when we featurized the training data, and the <kbd>_pca_featurize</kbd> function, like this:</li>
</ol>
<pre style="padding-left: 60px">     x_test = _pca_featurize(np.array(data)[test], *pca_args)</pre>
<ol start="2">
<li>Then, we predict the new labels, like this:</li>
</ol>
<pre style="padding-left: 60px">_, predicted = mlp.predict(x_test)<br/>    y_hat = np.array([index_to_label[np.argmax(y)] for y <br/>    in predicte</pre>
<ol start="3">
<li>Finally, we extract the true test labels using indices we stored for testing, as follows:</li>
</ol>
<pre style="padding-left: 60px">    y_true = np.array(targets)[test]</pre>
<p>The only things that are left to pass to a function are both <kbd>y_hat</kbd> and <kbd>y_true</kbd>, to calculate the accuracy of our classifier.</p>
<p>It took me 84 pictures (10-15 of each emotion) to get to a training accuracy of <kbd>0.92</kbd> and have a good enough classifier to be able to show off my software to my friends. <em>Can you beat it?</em></p>
<p>Now, let's see how we run the training script, and save the output in a manner that the demo application will be able to use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the script</h1>
                
            
            
                
<p>The MLP classifier can be trained and tested by using the <kbd>train_classifier.py</kbd> script, which does the following:</p>
<ol>
<li>The script first sets up the command-line options of <kbd>--data</kbd> to the location of the saved data, and <kbd>--save</kbd> to the location of a directory where we want to save the trained model (this argument is optional), as follows:</li>
</ol>
<pre style="padding-left: 60px">if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument('--data', required=True)<br/>    parser.add_argument('--save', type=Path)<br/>    args = parser.parse_args()</pre>
<ol start="2">
<li>Then, we load the saved data, and follow the training procedure described in the previous section, as follows:</li>
</ol>
<pre style="padding-left: 60px">    data, targets = load_collected_data(args.data)<br/><br/>    mlp = cv2.ml.ANN_MLP_create()<br/>    ...<br/>    mlp.train(...</pre>
<ol start="3">
<li>Finally, we check if the user wants us to save the trained model, by running the following code:</li>
</ol>
<pre style="padding-left: 60px">    if args.save:<br/>        print('args.save')<br/>        x_all, pca_args = pca_featurize(np.array(data))<br/>        mlp.train(x_all, cv2.ml.ROW_SAMPLE, encoded_targets)<br/>        mlp.save(str(args.save / 'mlp.xml'))<br/>        pickle_dump(index_to_label, args.save / 'index_to_label')<br/>        pickle_dump(pca_args, args.save / 'pca_args')</pre>
<p>The previous code saves the trained model, the <kbd>index_to_label</kbd> dictionary so that we can display human-readable labels in the demo, and <kbd>pca_args</kbd> so that we can featurize live camera feed frames in the demo.</p>
<p>The saved <kbd>mlp.xml</kbd> file contains the network configuration and learned weights. OpenCV knows how to load it. So, let's see what the demo application looks like.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Putting it all together</h1>
                
            
            
                
<p>In order to run our app, we will need to execute the main function routine (<kbd>chapter8.py</kbd>) that loads the pre-trained cascade classifier and the pre-trained MLP and applies them to each frame of the webcam live stream.</p>
<p>However, this time, instead of collecting more training samples, we will start the program with a different option, shown here:</p>
<pre class="mce-root"> <strong>$ </strong><strong>python chapter8.py demo --classifier data/clf1</strong></pre>
<p class="mce-root">This will start the application with a new <kbd>FacialExpressionRecognizerLayout</kbd> layout, which is a subclass of <kbd>BasicLayout</kbd> without any extra UI elements. Let's go over the constructor first, as follows:</p>
<ol>
<li>It reads and initializes all the data that was stored by the training script, like this:</li>
</ol>
<pre style="padding-left: 60px">class FacialExpressionRecognizerLayout(BaseLayout):<br/>    def __init__(self, *args,<br/>                 clf_path=None,<br/>                 **kwargs):<br/>        super().__init__(*args, **kwargs)</pre>
<ol start="2">
<li>It loads the pre-trained classifier using <kbd>ANN_MLP_load</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 120px">self.clf = cv2.ml.ANN_MLP_load(str(clf_path / 'mlp.xml'))</pre>
<ol start="3">
<li>It loads the Python variables that we want to pass from training, like this:</li>
</ol>
<pre style="padding-left: 60px">        self.index_to_label = pickle_load(clf_path <br/>                                          / 'index_to_label')<br/>        self.pca_args = pickle_load(clf_path / 'pca_args')</pre>
<ol start="4">
<li>It initializes a <kbd>FaceDetector</kbd> class to be able to do face recognition, as follows:</li>
</ol>
<pre style="padding-left: 60px">        self.face_detector = FaceDetector(<br/>            face_cascade='params/<br/>            haarcascade_frontalface_default.xml',<br/>            eye_cascade='params/haarcascade_lefteye_2splits.xml')</pre>
<p>Once we have all the pieces from training in place, we can go ahead and put some code in place to add labels to faces. In this demo, we don't have any use of extra buttons; so, the only method we have to implement is <kbd>process_frame</kbd>, which first tries to detect a face in the live feed and place a label on top of it, We will proceed as follows:</p>
<ol>
<li>First, we try to detect if there is a face present in the video stream or not, by running the following code:</li>
</ol>
<pre style="padding-left: 60px">   def process_frame(self, frame_rgb: np.ndarray) -&gt; np.ndarray:<br/>        success, frame, self.head, (x, y) = <br/>        self.face_detector.detect_face(frame_rgb)</pre>
<ol start="2">
<li>If there is no face, we do nothing and <kbd>return</kbd> an unprocessed <kbd>frame</kbd>, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">        if not success:<br/>            return frame</pre>
<ol start="3">
<li>Once there is a face, we try to align the face (the same as when collecting the training data), like this:</li>
</ol>
<pre style="color: black;padding-left: 60px">        success, head = self.face_detector.align_head(self.head)<br/>        if not success:<br/>            return frame</pre>
<ol start="4">
<li>If we are successful, we featurize the head and predict the label using the MLP, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">        _, output = self.clf.predict(self.featruize_head(head))<br/>        label = self.index_to_label[np.argmax(output)]</pre>
<ol start="5">
<li>Finally, we put the text with the label on top of the face's bounding box and show that to the user, by running the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">        cv2.putText(frame, label, (x, y - 20),<br/>                    cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)<br/><br/>        return frame</pre>
<p>In the previous method, we made use of <kbd>featurize_head</kbd>, which is a convenient function to call <kbd>_pca_featurize</kbd>, as shown in the following code block:</p>
<pre>    def featurize_head(self, head):<br/>        return _pca_featurize(head[None], *self.pca_args)</pre>
<p>The end result looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3ded5d77-8f8a-4c9b-a8b6-9c27996f24e7.png" style="width:33.17em;height:24.83em;"/></p>
<p>Although the classifier has only been trained on (roughly) 100 training samples, it reliably detects my various facial expressions in every frame of the live stream, no matter how distorted my face seemed to be at the given moment.</p>
<p>This is a good indication that the neural network that was trained previously is neither underfitting nor overfitting the data since it is capable of predicting the correct class labels, even for new data samples.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter of the book has really rounded up our experience and made us combine a variety of our skills to arrive at an end-to-end app that consists of both object detection and object recognition. We became familiar with a range of pre-trained cascade classifiers that OpenCV has to offer, and we collected and created our very own training dataset, learned about MLPs, and trained them to recognize emotional expressions in faces (well, at least my face).</p>
<p>The classifier undoubtedly benefited from the fact that I was the only subject in the dataset, but, with all the knowledge and experience that you have gathered throughout this book, it is now time to overcome these limitations.</p>
<p>After learning the techniques in this chapter, you can start with something smaller, and train the classifier on images of you (indoors and outdoors, at night and day, during summer and winter). Or, you can take a look at <strong>Kaggle's Facial Expression Recognition Challenge</strong>, which has a lot of nice data you could play with.</p>
<p>If you are into machine learning, you might already know that there is a variety of accessible libraries out there, such as <strong>Pylearn</strong>, <strong>scikit-learn</strong>, and <strong>PyTorch</strong>.</p>
<p>In the next chapter, you will start your deep learning journey and will put your hands on deep CNNs. You will get acquainted with multiple deep learning concepts, and you will create and train your own classification and localization networks using transfer learning. To accomplish this, you will use one of the pre-trained classification CNNs available in <strong>Keras.</strong> Throughout the chapter, you will extensively use <strong>Keras</strong> and <strong>TensorFlow, </strong>which are a couple of the most popular deep learning frameworks at the time of writing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<ul>
<li><strong>Kaggle's Facial Expression Recognition Challenge</strong>: <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge</a>.</li>
<li><strong>Pylearn</strong>: <a href="https://github.com/lisa-lab/pylearn2">https://github.com/lisa-lab/pylearn2</a>.</li>
<li><strong>scikit-learn</strong>: <a href="http://scikit-learn.org">http://scikit-learn.org</a>.</li>
<li><strong>pycaffe</strong>: <a href="http://caffe.berkeleyvision.org">http://caffe.berkeleyvision.org</a>.</li>
<li><strong>Theano</strong>: <a href="http://deeplearning.net/software/theano">http://deeplearning.net/software/theano</a>.</li>
<li><strong>Torch</strong>: <a href="http://torch.ch">http://torch.ch</a>.</li>
<li><strong>UC Irvine Machine Learning Repository</strong>: <a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Attributions</h1>
                
            
            
                
<p><kbd>Lenna.png</kbd>—Image Lenna is available at <a href="http://www.flickr.com/photos/15489034@N00/3388463896">http://www.flickr.com/photos/15489034@N00/3388463896</a> by Conor Lawless under attribution CC 2.0 Generic.</p>


            

            
        
    </body></html>