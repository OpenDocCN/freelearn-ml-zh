- en: '*Chapter 7*: Hyperparameter Tuning via Scikit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn` is one of the Python packages that is used the most by data
    scientists. This package provides a range of `scikit-learn`, there are also other
    packages for the hyperparameter tuning task that are built on top of `scikit-learn`
    or mimic the interface of `scikit-learn`, such as `scikit-optimize` and `scikit-hyperband`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn about all of the important things to do with `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband`, along with how to utilize them to implement
    the hyperparameter tuning methods that we learned about in the previous chapters.
    We’ll start by walking through how to install each of the packages. Then, we’ll
    learn not only how to utilize those packages with their default configurations
    but also discuss the available configurations along with their usage. Additionally,
    we’ll discuss how the implementation of the hyperparameter tuning methods is related
    to the theory that we learned in previous chapters, as there might be some minor
    differences or adjustments made in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, equipped with the knowledge from previous chapters, you will also be
    able to understand what’s happening if there are errors or unexpected results
    and understand how to set up the method configuration to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing scikit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Coarse-to-Fine Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Successive Halving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Hyper Band
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gaussian Process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gradient Boosted Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will learn how to implement various hyperparameter tuning methods with `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband`. To ensure that you can reproduce the
    code examples in this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version 3.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `Pandas` package (version 1.3.4 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `NumPy` package (version 1.21.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `Scipy` package (version 1.7.3 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `Matplotlib` package (version 3.5.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `scikit-learn` package (version 1.0.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `scikit-optimize` package (version 0.9.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed `scikit-hyperband` package (directly cloned from the GitHub repository)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Scikit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn`, which is commonly called `sklearn` package is the consistency
    of its interface across many implemented classes.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, all of the implemented ML models, or `sklearn` have the same `fit()`
    and `predict()` methods for fitting the model on the training data and evaluating
    the fitted model on the test data, respectively. When working with data preprocessors,
    or `sklearn`, the typical method that every preprocessor has is the `fit()`, `transform()`,
    and `fit_transform()` methods for fitting the preprocessor, transforming new data
    with the fitted preprocessor, and fitting and directly transforming the data that
    is used to fit the preprocessor, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine
    Learning Models*, we learned how `sklearn` can be utilized to evaluate the performance
    of ML models through the concept of cross-validation, where the full data is split
    into several parts, such as train, validation, and test data. In *Chapters 3–6*,
    we always used the cross-validation score as our objective function. While we
    can manually perform hyperparameter tuning and calculate the cross-validation
    score based on the split data, `sklearn` provides dedicated classes for hyperparameter
    tuning that use the cross-validation score as the objective function during the
    tuning process. There are several hyperparameter tuning classes implemented in
    `sklearn`, such as `GridSearchCV`, `RandomizedSearchCV`, `HalvingGridSearchCV`,
    and `HalvingRandomSearchCV`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, all of the hyperparameter tuning classes implemented in `sklearn` have
    a consistent interface. We can use the `fit()` method to perform hyperparameter
    tuning on the given data where the cross-validation score is used as the objective
    function. Then, we can use the `best_params_` attribute to get the best set of
    hyperparameters, the `best_score_` attribute to get the average cross-validated
    score from the best set of hyperparameters, and the `cv_results_` attribute to
    get the details of the hyperparameter tuning process, including but not limited
    to the objective function score for each tested set of hyperparameters in each
    of the folds.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent data leakage when performing the data preprocessing steps (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*),
    `sklearn` also provides a `Pipeline` object that can be used along with the hyperparameter
    tuning classes. This `Pipeline` object will ensure that any data preprocessing
    steps are only fitted based on the train set during the cross-validation. Essentially,
    this object is just *a chain of several* `sklearn` *transformers and estimators*,
    which has the same `fit()` and `predict()` method, just like a usual `sklearn`
    estimator.
  prefs: []
  type: TYPE_NORMAL
- en: While `sklearn` can be utilized for many ML-related tasks, `scikit-optimize`,
    which is commonly called `sklearn` and can be utilized for implementing the `skopt`
    has a very similar interface to `sklearn`, so it will be very easy for you to
    get familiar with `skopt` once you are already familiar with `sklearn` itself.
    The main hyperparameter tuning class implemented in `skopt` is the `BayesSearchCV`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '`skopt` provides four implementations for the optimizer within the `BayesSearchCV`
    class, namely `sklearn` to be utilized as the optimizer. Note that, here, the
    optimizer refers to the `skopt` provides various implementations of the **acquisition
    function**, namely the **Expected Improvement** (**EI**), **Probability of Improvement**
    (**PI**), and **Lower Confidence Bound** (**LCB**) functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, the `scikit-hyperband` package. Additionally, this package
    is built on top of `sklearn` and is specifically designed for the HB implementation.
    The hyperparameter tuning class implemented in this package is `HyperbandSearchCV`.
    It also has a very similar interface to `sklearn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for `sklearn` and `skopt`, you can easily install them via `pip install`,
    just like you usually install other packages. As for `scikit-hyperband`, the author
    of the package didn’t put this on `sklearn`. Luckily, there’s a forked version
    ([https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband))
    of the original repo that works nicely with the newer version of `sklearn` (`1.0.1`
    or above). To install `scikit-hyperband`, please follow the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone [https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband)
    to your loca machinel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the cloned repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move the `hyperband` folder to your working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you are aware of the `scikit-learn`, `scikit-optimize`, and `scikit-hyperband`
    packages, in the following sections, we will learn how to utilize them to implement
    various hyperparameter tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To implement `for loop` that tests all of the possible hyperparameter values
    in the search space. However, by using `sklearn`’s implementation of Grid Search,
    `GridSearchCV`, we can have a cleaner code since we just need to call a single
    line of code to instantiate the class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through an example of how we can utilize `GridSearchCV` to perform
    Grid Search. Note that, in this example, we are performing hyperparameter tuning
    on an RF model. We will utilize `sklearn’s` implementation of RF, `RandomForestClassifier`.
    The dataset used in this example is the *Banking Dataset – Marketing Targets*
    provided on Kaggle ([https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)).
  prefs: []
  type: TYPE_NORMAL
- en: Original Data Source
  prefs: []
  type: TYPE_NORMAL
- en: This data was first published in *A Data-Driven Approach to Predict the Success
    of Bank Telemarketing*, by Sérgio Moro, Paulo Cortez, and Paulo Rita, Decision
    Support Systems, Elsevier, 62:22–31, June 2014 ([https://doi.org/10.1016/j.dss.2014.03.001](https://doi.org/10.1016/j.dss.2014.03.001)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a binary classification dataset with 16 features related to the marketing
    campaigns conducted by a bank institution. The target variable consists of two
    classes, *yes* or *no*, indicating whether the client of the bank has subscribed
    to a term deposit or not. Hence, the goal of training an ML model on this dataset
    is to identify whether a customer is potentially wanting to subscribe to the term
    deposit or not. For more details, you can refer to the description on the Kaggle
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two datasets provided, namely the `train.csv` dataset and the `test.csv`
    dataset. However, we will not use the provided `test.csv` dataset since it is
    sampled directly from the train data. We will manually split `train.csv` into
    two subsets, namely the train set and the test set, using the help of the `train_test_split`
    function from `sklearn` (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*). We will set the `test_size` parameter to
    `0.1`, meaning we will have `40,689` and `4,522` rows for the train set and the
    test set, respectively. The following code shows you how to load the data and
    perform the train set and the test set splitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Out of the 16 features provided in the data, there are 7 numerical features
    and 9 categorical features. As for the target class distribution, 12% of them
    are *yes* and 88% of them are *no*, for both train and test datasets. This means
    that we can’t use accuracy as our metric since we have an imbalanced class problem—a
    situation where we have a very skewed distribution of the target classes. Instead,
    in this example, we will use the F1-score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before performing Grid Search, let’s see how *RandomForestClassifier* with
    the default hyperparameter values work. Furthermore, let’s also try to train our
    model on only those seven numerical features for now. The following code shows
    you how to get only numerical features, train the model on those features in the
    train set, and finally, evaluate the model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `X_train_numerical` variable only stores numerical features from the train
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `X_test_numerical` variable only stores numerical features from the test
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model on train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.436` for the F1-Score when testing
    our trained RF model on the test set. Remember that this is the result of only
    using numerical features and the default hyperparameters of the `RandomForestClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before performing Grid Search, we have to define the hyperparameter space in
    a dictionary of list format, where the keys refer to the name of the hyperparameters
    and the lists consist of all the values we want to test for each hyperparameter.
    Let’s say we define the hyperparameter space for `RandomForestClassifier` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have defined the hyperparameter space, we can apply the `GridSearchCV`
    class to the train data, use the best set of hyperparameters to train a new model
    on the full train data, and then evaluate that final trained model on the test
    data, just as we learned in *Chapters 3–6*. The following code shows you how to
    do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `GridSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `GridSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Look how clean our code is by utilizing `sklearn`’s implementation of Grid Search
    instead of writing our code from scratch! Notice that we just need to pass `sklearn`’s
    estimator and the hyperparameter space dictionary to the `GridSearchCV` class,
    and the rest will be handled by `sklearn`. In this example, we also pass several
    additional parameters to the class, such as `scoring=’f1’`, `cv=5`, `n_jobs=-1`,
    and `refit=True`.
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, the `scoring` parameter governs the scoring strategy that
    we want to use to evaluate our model during the cross-validation. While our objective
    function will always be the cross-validation score, this parameter controls what
    type of score we want to use as our metric. In this example, we are using the
    F1-score as our metric. However, you can also pass a custom callable function
    as the scoring strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Available Scoring Strategies in Sklearn
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)
    for all of the implemented scoring strategies by `sklearn`, and refer to [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)
    if you want to implement your own custom scoring strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The `cv` parameter indicates how many folds of cross-validation you want to
    perform. The `n_jobs` parameter controls how many jobs you want to run in parallel.
    If you decide to use all of the processors, you can simply set `n_jobs=–1`, just
    as we did in the example.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we have the `refit` parameter. This Boolean parameter is
    responsible for deciding whether at the end of the hyperparameter tuning process
    we want to refit our model on the full train set using the best set of hyperparameters
    or not. In this example, we set `refit=True`, meaning that `sklearn` will automatically
    refit our RF model on the full train set using the best set of hyperparameters.
    It is very important to retrain our model on the full train set after performing
    hyperparameter tuning since we only utilize subsets of the train set during the
    hyperparameter tuning process. There are several other parameters that you can
    control when initiating a `GridSearchCV` class. For more details, you can refer
    to the official page of `sklearn` ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our example. By performing Grid Search in the predefined hyperparameter
    space, we are able to get an F1-score of `0.495` when evaluated on the test set.
    The best set of hyperparameters is `{‘class_weight’: ‘balanced’, ‘criterion’:
    ‘entropy’, ‘min_samples_split’: 0.01, ‘n_estimators’: 150}` with an objective
    function score of `0.493`. Note that we can get the best set of hyperparameters
    along with its objective function score via the `best_params_` and `best_score_`
    attributes, respectively. Not bad! We get around `0.06` of improvement in the
    F1-score. However, note that we are still only using numerical features.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will try to utilize not only numerical features but also categorical
    features from our data. To be able to utilize those categorical features, we need
    to perform the **categorical encoding** preprocessing step. Why? Because ML models
    are not able to understand non-numerical features. Therefore, we need to convert
    those non-numerical features into numerical ones so that the ML model is able
    to utilize those features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that when we want to perform any data preprocessing steps, we have
    to be very careful with it to prevent any data leakage problem where we might
    introduce part of our test data into the train data (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*). To prevent this problem, we can utilize
    the `Pipeline` object from `sklearn`. So, instead of passing an estimator to the
    `GridSearchCV` class, we can also pass a `Pipeline` object that consists of a
    chain of data preprocessors and an estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since, in this example, not all of our features are categorical and we only
    want to perform categorical encoding on those non-numerical features, we can utilize
    the `ColumnTransformer` class to specify which features we want to apply the categorical
    encoding step. Let’s say we also want to perform a normalization preprocessing
    step on the numerical features. We can also pass those numerical features to the
    `ColumnTransformer` class along with the normalization transformer. Then, it will
    automatically apply the normalization step to only those numerical features. The
    following code shows you how to create such a `Pipeline` object with `ColumnTransformer`,
    where we use `StandardScaler` for the normalization step and `OneHotEncoder` for
    the categorical encoding step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get list of numerical features and categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the preprocessor for numerical features and categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Delegate each preprocessor to the corresponding features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a pipeline of preprocessors and models. In this example, we named our
    pr-processing steps as *“preprocessor”* and the modeling step as *“model”*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the previous code blocks, the `ColumnTransformer` class is
    responsible for delegating each preprocessor to the corresponding features. Then,
    we can just reuse it for all of our preprocessing steps through a single preprocessor
    variable. Finally, we can create a pipeline consisting of the preprocessor variable
    and `RandomForestClassifier`. Note that within the `ColumnTransformer` class and
    the `Pipeline` class, we also have to provide the name of each preprocessor and
    step in the pipeline, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the pipeline, we can see how our model performs on
    the test set (without hyperparameter tuning) by utilizing all of the features
    and preprocessors defined in the pipeline. The following code shows how we can
    directly use the pipeline to perform the same `fit()` and `predict()` methods
    as we did earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.516` for the F1-score when testing
    our trained pipeline on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can start performing Grid Search over the pipeline, too. However,
    before we can do that, we need to redefine the hyperparameter space. We need to
    change the keys in the dictionary with the format of `<estimator_name_in_pipeline>__<hyperparameter_name>`.
    The following is the redefined version of our hyperparameter space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following code shows you how to perform Grid Search over the pipeline instead
    of the estimator itself. Essentially, the code is the same as the previous version.
    The only difference is that we are performing the Grid Search over the pipeline
    and on *all* of the features in the data, not just the numerical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initiate the `GridSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `GridSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.549` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.01, ‘model__n_estimators’:
    100}` with an objective function score of `0.549`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that we can also *create a pipeline within a pipeline*.
    For example, we can create a pipeline for `numeric_preprocessor` that consists
    of a chain of missing value imputation and normalization modules. The following
    code shows how we can create such a pipeline. The `SimpleImputer` class is the
    missing value imputation transformer from `sklearn` that can help us to perform
    mean, median, mode, or constant imputation strategies if there are any missing
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have learned how to implement Grid Search in `sklearn` through
    the `GridSearchCV` class, starting from defining the hyperparameter space, setting
    each important parameter of the `GridSearchCV` class, learning how to utilize
    the `Pipeline` and `ColumnTransformer` classes to prevent data leakage issues,
    and learning how to create a pipeline within the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to implement Random Search in `sklearn`
    via `RandomizedSearchCV`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing `sklearn` is very similar to implementing Grid Search. The main
    difference is that we have to provide the number of trials or iterations since
    Random Search will not try all of the possible combinations in the hyperparameter
    space. Additionally, we have to provide the accompanying distribution for each
    of the hyperparameters when defining the search space. In `sklearn`, Random Search
    is implemented in the `RandomizedSearchCV` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how we can implement Random Search in `sklearn`, let’s use the
    same example from the *Implementing Grid Search* section. Let’s directly try using
    all of the features available in the dataset. All of the pipeline creation processes
    are exactly the same, so we will directly jump into the process of how to define
    the hyperparameter space and the `RandomizedSearchCV` class. The following code
    shows you how to define the accompanying distribution for each of the hyperparameters
    in the space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the hyperparameter space is quite different from the one that
    we defined previously in the *Implementing Grid Search* section. Here, we are
    also specifying the distribution for each of the hyperparameters, where `randint`
    and `truncnorm` are utilized for the `n_estimators` and `min_samples_split` hyperparameters.
    As for `criterion` and `class_weight`, we are still using the same configuration
    as the previous search space. Note that *by not specifying any distribution* means
    we are *applying uniform distribution* to the hyperparameter, where all of the
    values will have the same probability to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the `randint` distribution is just a uniform distribution for discrete
    variables, while `truncnorm` stands for truncated normal distribution, which,
    as its name suggests, is a modified normal distribution bounded on a particular
    range. In this example, the range is bounded on a range from `a=0` and `b=0.5`,
    with a mean of `loc=0.005` and a standard deviation of `scale=0.01`.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution for Hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: There are many other available distributions that you can utilize. `sklearn`
    accepts all distributions that have the `rvs` method, as in the distribution implementation
    from `Scipy`. Essentially, this method is just a method to sample a value from
    the specified distribution. For more details, please refer to the official documentation
    page of `Scipy` ([https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions)).
  prefs: []
  type: TYPE_NORMAL
- en: 'When initiating the `RandomizedSearchCV` class, we also have to define the
    `n_iter` and `random_state` parameters, which refer to the number of iterations
    and the random seed, respectively. The following code shows you how to perform
    Random Search over the same pipeline defined in the *Implementing Grid Search*
    section. In contrast with the example in the *Implementing Grid Search* section,
    which only performs `120` iterations of Grid Search, here, we perform `200` iterations
    of random search since we set `n_iter=200`. Additionally, we have a bigger hyperparameter
    space since we increase the granularity of the `n_estimators` and `min_samples_split`
    hyperparameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `RandomizedSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `RandomizedSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.563` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005155815445940717,
    ‘model__n_estimators’: 187}` with an objective function score of `0.562`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Random Search in `sklearn`
    through the `RandomizedSearchCV` class, starting from defining the hyperparameter
    space to setting each important parameter of the `RandomizedSearchCV` class. In
    the next section, we will learn how to perform CFS with `sklearn`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Coarse-to-Fine Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sklearn` package, you can find the implemented custom class, `CoarseToFineSearchCV`,
    in the repo mentioned in the *Technical Requirements* section.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the same example and hyperparameter space as in the *Implementing
    Random Search* section, to see how `CoarseToFineSearchCV` works in practice. Note
    that this implementation of CFS only utilizes Random Search and uses the top *N*
    percentiles scheme to define the promising subspace in each iteration, similar
    to the example shown in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054).
    However, *you can edit the code based on your own preference* since CFS is a very
    simple method with customizable modules.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows you how to perform CFS with the `CoarseToFineSearchCV`
    class. It is worth noting that this class has very similar parameters to the `RandomizedSearchCV`
    class, with several additional parameters. The `random_iters` parameter controls
    the number of iterations for each random search trial, `top_n_percentile` controls
    the *N* value within the top N percentiles promising subspace definition (see
    [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)), `n_iter` defines the number
    of CFS iterations to be performed, and `continuous_hyperparams` stores the list
    of continuous hyperparameters in the predefined space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initiate the `CoarseToFineSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `CoarseToFineSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.561` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005867409821769845,
    ‘model__n_estimators’: 106}` with an objective function score of `0.560`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement CFS using a custom class on
    top of `sklearn` through the `CoarseToFineSearchCV` class. In the next section,
    we will learn how to perform SH with `sklearn`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Successive Halving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to CFS, `sklearn`, namely `HalvingGridSearchCV` and `HalvingRandomSearchCV`.
    As their names suggest, the former class is an implementation of SH that utilizes
    Grid Search in each of the SH iterations, while the latter utilizes Random Search.
  prefs: []
  type: TYPE_NORMAL
- en: By default, SH implementations in `sklearn` use the number of samples, or *n_samples*,
    as the definition of the budget or resource in SH. However, it is also possible
    to define a budget with other definitions. For example, we can use `n_estimators`
    in RF as the budget, instead of using the number of samples. It is worth noting
    that we cannot use `n_estimators`, or any other hyperparameters, to define the
    budget if it is part of the hyperparameter space.
  prefs: []
  type: TYPE_NORMAL
- en: Both `HalvingGridSearchCV` and `HalvingRandomSearchCV` have similar standard
    SH parameters to control how the SH iterations will work, such as the `factor`
    parameter, which refers to the multiplier factor for SH, `resource`, which refers
    to what definition of budget we want to use, `max_resources` refers to the maximum
    budget or resource, and `min_resources`, which refers to the minimum number of
    resources to be used at the first iteration. By default, the `max_resources` parameter
    is set to *auto*, meaning it will use the total number of samples that we have
    when `resource=’n_samples’`. On the other hand, `sklearn` implemented a heuristic
    to define the default value for the `min_resources` parameter, referred to as
    *smallest*. This heuristic will ensure that we have a small value of `min_resources`.
  prefs: []
  type: TYPE_NORMAL
- en: Specific for `HalvingRandomSearchCV`, there is also the `n_candidates` parameter
    that refers to the initial number of candidates to be evaluated at the first iteration.
    Note that this parameter is not available in `HalvingGridSearchCV` since it will
    automatically evaluate all of the hyperparameter candidates in the predefined
    space. It is worth noting that `sklearn` implemented a strategy, called *exhaust*,
    to define the default value of the `n_candidates` parameter. This strategy ensures
    that we evaluate enough candidates at the first iteration so that we can utilize
    as many resources as possible at the last SH iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Besides those standard SH parameters, both of the classes also have the `aggressive_elimination`
    parameter, which can be utilized when we have a low number of resources. If this
    Boolean parameter is set to `True`, `sklearn` will automatically rerun the first
    SH iteration several times until the number of candidates is small enough. The
    goal of this parameter is to ensure that we only evaluate a maximum of `factor`
    candidates in the last SH iteration. Note that this parameter is only implemented
    in `sklearn`, the original SH doesn’t introduce this strategy as part of the tuning
    method (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `GridSearchCV` and `RandomizedSearchCV`, `HalvingGridSearchCV` and
    `HalvingRandomSearchCV` also have the usual default `sklearn` parameters for hyperparameter
    tuning, such as `cv`, `scoring`, `refit`, `random_state`, and `n_jobs`.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Features of SH in sklearn
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that as per `version 1.0.2` of `sklearn`, the SH implementations
    are still in the experimental phase. This means that there might be changes in
    the implementation or interface of the classes without any depreciation cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how `HalvingRandomSearchCV` works with its default
    SH parameters. Note that we still use the same example and hyperparameter space
    as in the *Implementing Random Search* section. It is also worth noting that we
    only use the `HalvingRandomSearchCV` class in this example since `HalvingGridSearchCV`
    has a very similar interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `HalvingRandomSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `HalvingRandomSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.556` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.007286406330027324,
    ‘model__n_estimators’: 42}` with an objective function score of `0.565`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows you how to generate a figure that shows the tuning
    process in each SH iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the fitting history of each trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the fitting history for each trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The SH hyperparameter tuning process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – The SH hyperparameter tuning process
  prefs: []
  type: TYPE_NORMAL
- en: Based on *Figure 7.1*, we can see that we only utilized around 14,000 samples
    in the last iteration while we have around 40,000 samples in our training data.
    Indeed, this is not an ideal case since there are too many samples not being utilized
    in the last SH iteration. We can change the default value of the SH parameters
    set by `sklearn` to ensure that we utilize as many resources as possible at the
    last iteration, through the `min_resources` and `n_candidates` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement SH in `sklearn` through the
    `HalvingRandomSearchCV` and `HalvingGridSearchCV` classes. We have also learned
    all of the important parameters available for both classes. In the next section,
    we will learn how to perform HB with `scikit-hyperband`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Hyper Band
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The extension of Successive Halving, the `scikit-hyperband` package. This package
    is built on top of `sklearn`, which means it also provides a very similar interface
    for `GridSearchCV`, `RandomizedSearchCV`, `HalvingGridSearchCV`, and `HalvingRandomSearchCV`.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast with the default SH budget definition in the `sklearn` implementation,
    *Scikit-Hyperband defines the budget* as the number of estimators, *n_estimators*,
    in an ensemble of trees, or the number of iterations for estimators trained with
    stochastic gradient descent, such as the XGBoost algorithm. Additionally, we can
    use any other hyperparameters that exist in the estimator as the budget definition.
    However, `scikit-hyperband` *doesn’t allow us to use the number of samples as
    the budget definition*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the same example as in the *Implementing Successive Halving* section,
    but with a different hyperparameter space. Here, we use the number of estimators,
    *n_estimators*, as the resource, which means we have to take out this hyperparameter
    from our search space. Note that you also have to remove any other hyperparameters
    from the space when you use it as the resource definition, just like in the `sklearn`
    implementation of SH.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows you how `HyperbandSearchCV` works. The `resource_param`
    parameter refers to the hyperparameter that you want to use as the budget definition.
    The `eta` parameter is actually the same as the factor parameter in the `HalvingRandomSearchCV`
    or `HalvingGridSearchCV` classes, which refers to the multiplier factor for each
    SH run. The `min_iter` and `max_iter` parameters refer to the minimum and maximum
    resources for all brackets. Note that there’s no automatic strategy like in the
    `sklearn` implementation of SH for setting the value of the `min_iter` and `max_iter`
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining `HyperbandSearchCV` parameters are similar to any other `sklearn`
    implementation of the hyperparameter tuning methods. It is worth noting that the
    HB implementation used in this book is the modified version of the `scikit-hyperband`
    package. Please check the following folder in the book’s GitHub repo ([https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `HyperbandSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `HyperbandSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.569` in F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced’, ‘model__criterion’:
    ‘entropy’, ‘model__min_samples_split’: 0.0055643644642829684, ‘model__n_estimators’:
    33}` with an objective function score of `0.560`. Note that although we remove
    `model__n_estimators` from the search space, `HyperbandSearchCV` still outputs
    the best value for this hyperparameter by choosing from the best bracket.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement HB using the help of the `scikit-hyperband`
    package along with all of the important parameters available for the `HyperbandSearchCV`
    class. In the next section, we will learn how to perform Bayesian Optimization
    with `scikit-optimize`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gaussian Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`skopt` package. Similar to `scikit-hyperband`, this package is also built
    on top of the `sklearn` package, which means the interface for the implemented
    Bayesian Optimization tuning class, `BayesSearchCV`, is very similar to `GridSearchCV`,
    `RandomizedSearchCV`, `HalvingGridSearchCV`, `HalvingRandomSearchCV`, and `HyperbandSearchCV`.'
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike `sklearn` or `scikit-hyperband`, which works well directly with
    the distribution implemented in `scipy`, in `skopt`, we can only use the wrapper
    provided by the package when defining the hyperparameter space. The wrappers are
    defined within the `skopt.space.Dimension` instances and consist of three types
    of dimensions, such as `Real`, `Integer`, and `Categorical`. Within each of these
    dimension wrappers, `skopt` actually uses the same distribution from the `scipy`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the `Real` dimension only supports the `uniform` and `log-uniform`
    distributions and can take any real/numerical value as the input. As for the `Categorical`
    dimension, this wrapper can only take categorical values as the input, as implied
    by its name. It will automatically convert categorical values into integers or
    even real values, which means we can also utilize categorical hyperparameters
    for BOGP! Although we can do this, remember that BOGP only works best for the
    actual real variables (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*). Finally, we have the `Integer` dimension wrapper.
    By default, this wrapper only supports `uniform` and `log-uniform` distributions
    for integer formatting. The `uniform` distribution will utilize the `randint`
    distribution from `scipy`, while the `log-uniform` distribution is exactly the
    same as the one that is used in the Real wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that we can write our own wrapper for other distributions
    too; for example, the `truncnorm` distribution that we use in all of our earlier
    examples. In fact, you can find the custom `Real` wrapper that consists of the
    `truncnorm`, `uniform`, and `log-uniform` distributions in the repo mentioned
    in the *Technical Requirements* section. The following code shows you how we can
    define the hyperparameter space for `BayesSearchCV`. Note that we are still using
    the same example and hyperparameter space as the *Implementing Random Search*
    section. Here, `Integer` and `Categorical` are the original wrappers provided
    by `skopt`, while the `Real` wrapper is the custom wrapper that consists of the
    `truncnorm` distribution, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: All of the parameters of the `BayesSearchCV` class are very similar to the `GridSearchCV`,
    `RandomizedSearchCV`, `HalvingGridSearchCV`, `HalvingRandomSearchCV`, or `HyperbandSearchCV`.
    The only specific parameters for `BayesSearchCV` are the `n_iter` and `optimizer_kwargs`
    which refer to the total number of trials to be performed and the parameter that
    consists of all related parameters for the `Optimizer`, respectively. Here, the
    `Optimizer` is a class that represents each of the Bayesian Optimization steps,
    starting from initializing the initial points, fitting the surrogate model, sampling
    the next set of hyperparameters using the help of the acquisition function, and
    optimizing the acquisition function (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).
  prefs: []
  type: TYPE_NORMAL
- en: There are several parameters available that we can pass to the `optimizer_kwargs`
    dictionary. The `base_estimator` parameter refers to the type of surrogate model
    to be used. `skopt` has prepared several surrogate models with default setups,
    including the Gaussian Process or *GP*. The `n_initial_points` parameter refers
    to the number of random initial points before the actual Bayesian Optimization
    steps begin. The `initial_point_generator` parameter refers to the initialization
    method to be used. By default, `skopt` will initialize them randomly. However,
    you can also change the initialization method to *lhs*, *sobol*, *halton*, *hammersly*,
    or *grid*.
  prefs: []
  type: TYPE_NORMAL
- en: As for the type of acquisition function to be used, by default, `skopt` will
    use *gp_hedge*, which is an acquisition function that will automatically choose
    either one of the `acq_func` parameter to *LCB*, *EI*, and *PI*, respectively.
    As explained in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036), besides
    choosing what acquisition function needs to be used, we also have to define what
    kind of optimizer to be utilized for the acquisition function itself. There are
    two options for the acquisition function’s optimizer provided by `skopt`, namely
    random sampling (*sampling*) and *lbfgs*, or the type of second-order optimization
    strategy mentioned in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036). By
    default, `skopt` sets the `acq_optimizer` parameter to *auto*, which will choose
    automatically when to use the *sampling* or *lbfgs* optimization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can also pass the `acq_func_kwargs` parameter within the `optimizer_kwargs`
    parameter. We can pass all parameters related to the acquisition function to this
    `acq_func_kwargs` parameter; for example, the `xi` parameter that controls the
    exploration and exploitation behavior of the BOGP, as explained in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036).
    While the `xi` parameter is responsible for controlling the exploration versus
    exploitation trade-off for EI and PI acquisition functions, there is also another
    parameter called `kappa`, which is responsible for the same task as the LCB acquisition
    function. The higher the value of `xi` or `kappa` means that we are favoring exploration
    over exploitation, and vice versa. For more information about all of the parameters
    that are available in the `BayesSearchCV` class, you can refer to the official
    API reference of the `skopt` package ([https://scikit-optimize.github.io/stable/modules/classes.html](https://scikit-optimize.github.io/stable/modules/classes.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we can utilize `BayesSearchCV` to perform BOGP
    on the same example as the *Implementing Random Search* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.539` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced’, ‘model__criterion’:
    ‘entropy’, ‘model__min_samples_split’: 0.02363008892366518, ‘model__n_estimators’:
    94}` with an objective function score of `0.530`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement BOGP in `skopt` along with
    all of the important parameters available for the `BayesSearchCV` class. It is
    worth noting that `skopt` also has experiment tracking modules that include several
    native supports for plotting the result. We will learn more about those modules
    in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125), *Tracking Hyperparameter
    Tuning Experiments*. In the next section, we will learn how to perform another
    variant of Bayesian Optimization that utilizes RF as its surrogate model with
    `skopt`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayesian Optimization Random Forest (BORF)** is another variant of Bayesian
    Optimization hyperparameter tuning methods that utilize RF as the surrogate model.
    Note that this variant is different from **Sequential Model Algorithm Configuration**
    (**SMAC**) although both of them utilize RF as the surrogate model (see [*Chapter
    4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring Bayesian Optimization*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing BORF with `skopt` is actually very similar to implementing BOGP
    as discussed in the previous section. We just need to change the `base_estimator`
    parameter within `optimizer_kwargs` to *RF*. Let’s use the same example as in
    the *Implementing Bayesian Optimization Gaussian Process* section, but change
    the acquisition function from *EI* to *LCB*. Additionally, let’s change the `xi`
    parameter in the `acq_func_kwargs` to *kappa* since we are using *LCB* as our
    acquisition function. Note that we can also still use the same acquisition function.
    The changes made here just to show how you can interact with the interface of
    the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the final trained model on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.617` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.00043534042560206855,
    ‘model__n_estimators’: 85}` with an objective function score of `0.616`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement BORF in `skopt` through the
    `BayesSearchCV` class. In the next section, we will learn how to perform another
    variant of Bayesian Optimization, which utilizes Gradient Boosted Trees as its
    surrogate model with `skopt`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gradient Boosted Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`skopt` since we can just pass any other regressors from `sklearn` to be utilized
    as the `base_estimator` parameter. However, *GBRT* is part of the default surrogate
    model with predefined default hyperparameter values from the `skopt` package.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the *Implementing Bayesian Optimization Random Forest* section,
    we can just change the `base_estimator` parameter within `optimizer_kwargs` to
    *GBRT*. The following code shows you how to implement BOGBRT in `skopt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `BayesSearchCV` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the final trained model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.611` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.0005745541104096049,
    ‘model__n_estimators’: 143}` with an objective function score of `0.618`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement BOGBRT in `skopt` through
    the `BayesSearchCV` class by using the same example as in the *Implementing Bayesian
    Optimization Random Forest* section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned all the important things about the `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband` packages for hyperparameter tuning purposes.
    Additionally, we have learned how to implement various hyperparameter tuning methods
    using the help of those packages, along with understanding each of the important
    parameters of the classes and how are they related to the theory that we have
    learned in the previous chapters. From now on, you should be able to utilize these
    packages to implement your chosen hyperparameter tuning method and, ultimately,
    boost the performance of your ML model. Equipped with the knowledge from *Chapters
    3–6*, you will also be able to understand what’s happening if there are errors
    or unexpected results and how to set up the method configuration to match your
    specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the Hyperopt package and how to utilize
    it to perform various hyperparameter tuning methods. The goal of the next chapter
    is similar to this chapter, that is, to be able to utilize the package for hyperparameter
    tuning purposes and understand each of the parameters of the implemented classes.
  prefs: []
  type: TYPE_NORMAL
