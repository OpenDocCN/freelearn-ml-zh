- en: '*Chapter 7*: Hyperparameter Tuning via Scikit'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：通过Scikit进行超参数调整'
- en: '`scikit-learn` is one of the Python packages that is used the most by data
    scientists. This package provides a range of `scikit-learn`, there are also other
    packages for the hyperparameter tuning task that are built on top of `scikit-learn`
    or mimic the interface of `scikit-learn`, such as `scikit-optimize` and `scikit-hyperband`.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`是数据科学家使用最多的Python包之一。此包提供了一系列的`scikit-learn`功能，还有其他针对超参数调整任务的包，这些包建立在`scikit-learn`之上或模仿其接口，例如`scikit-optimize`和`scikit-hyperband`。'
- en: In this chapter, we’ll learn about all of the important things to do with `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband`, along with how to utilize them to implement
    the hyperparameter tuning methods that we learned about in the previous chapters.
    We’ll start by walking through how to install each of the packages. Then, we’ll
    learn not only how to utilize those packages with their default configurations
    but also discuss the available configurations along with their usage. Additionally,
    we’ll discuss how the implementation of the hyperparameter tuning methods is related
    to the theory that we learned in previous chapters, as there might be some minor
    differences or adjustments made in the implementation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习与`scikit-learn`、`scikit-optimize`和`scikit-hyperband`相关的所有重要事项，以及如何利用它们来实现我们在前几章中学到的超参数调整方法。我们将从介绍如何安装每个包开始。然后，我们将学习如何利用这些包的默认配置，并讨论可用的配置及其使用方法。此外，我们还将讨论超参数调整方法的实现与我们在前几章中学到的理论之间的关系，因为在实现过程中可能会有一些细微的差异或调整。
- en: Finally, equipped with the knowledge from previous chapters, you will also be
    able to understand what’s happening if there are errors or unexpected results
    and understand how to set up the method configuration to match your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，凭借前几章的知识，您还将能够理解如果出现错误或意外结果时会发生什么，并了解如何设置方法配置以匹配您特定的问题。
- en: 'In this chapter, we’ll be covering the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing scikit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍scikit
- en: Implementing Grid Search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Grid Search
- en: Implementing Random Search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Random Search
- en: Implementing Coarse-to-Fine Search
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Coarse-to-Fine Search
- en: Implementing Successive Halving
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Successive Halving
- en: Implementing Hyper Band
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Hyper Band
- en: Implementing Bayesian Optimization Gaussian Process
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Bayesian Optimization Gaussian Process
- en: Implementing Bayesian Optimization Random Forest
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Bayesian Optimization Random Forest
- en: Implementing Bayesian Optimization Gradient Boosted Trees
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Bayesian Optimization Gradient Boosted Trees
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will learn how to implement various hyperparameter tuning methods with `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband`. To ensure that you can reproduce the
    code examples in this chapter, you will require the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用`scikit-learn`、`scikit-optimize`和`scikit-hyperband`实现各种超参数调整方法。为了确保您能够复制本章中的代码示例，您需要以下要求：
- en: Python 3 (version 3.7 or above)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本3.7或更高）
- en: An installed `Pandas` package (version 1.3.4 or above)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`Pandas`包（版本1.3.4或更高）
- en: An installed `NumPy` package (version 1.21.2 or above)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`NumPy`包（版本1.21.2或更高）
- en: An installed `Scipy` package (version 1.7.3 or above)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`Scipy`包（版本1.7.3或更高）
- en: An installed `Matplotlib` package (version 3.5.0 or above)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`Matplotlib`包（版本3.5.0或更高）
- en: An installed `scikit-learn` package (version 1.0.1 or above)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`scikit-learn`包（版本1.0.1或更高）
- en: An installed `scikit-optimize` package (version 0.9.0 or above)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`scikit-optimize`包（版本0.9.0或更高）
- en: An installed `scikit-hyperband` package (directly cloned from the GitHub repository)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装的`scikit-hyperband`包（直接从GitHub仓库克隆）
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python)。
- en: Introducing Scikit
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Scikit
- en: '`scikit-learn`, which is commonly called `sklearn` package is the consistency
    of its interface across many implemented classes.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`，通常称为`sklearn`包，其接口在许多实现类中保持一致性。'
- en: For example, all of the implemented ML models, or `sklearn` have the same `fit()`
    and `predict()` methods for fitting the model on the training data and evaluating
    the fitted model on the test data, respectively. When working with data preprocessors,
    or `sklearn`, the typical method that every preprocessor has is the `fit()`, `transform()`,
    and `fit_transform()` methods for fitting the preprocessor, transforming new data
    with the fitted preprocessor, and fitting and directly transforming the data that
    is used to fit the preprocessor, respectively.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，所有实现的ML模型或`sklearn`都有相同的`fit()`和`predict()`方法，分别用于在训练数据上拟合模型和在测试数据上评估拟合的模型。当与数据预处理程序或`sklearn`一起工作时，每个预处理程序都有典型的`fit()`、`transform()`和`fit_transform()`方法，分别用于拟合预处理程序、使用拟合的预处理程序转换新数据以及直接拟合和转换用于拟合预处理程序的数据。
- en: In [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine
    Learning Models*, we learned how `sklearn` can be utilized to evaluate the performance
    of ML models through the concept of cross-validation, where the full data is split
    into several parts, such as train, validation, and test data. In *Chapters 3–6*,
    we always used the cross-validation score as our objective function. While we
    can manually perform hyperparameter tuning and calculate the cross-validation
    score based on the split data, `sklearn` provides dedicated classes for hyperparameter
    tuning that use the cross-validation score as the objective function during the
    tuning process. There are several hyperparameter tuning classes implemented in
    `sklearn`, such as `GridSearchCV`, `RandomizedSearchCV`, `HalvingGridSearchCV`,
    and `HalvingRandomSearchCV`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*中，我们学习了如何通过交叉验证的概念利用`sklearn`来评估ML模型的性能，其中完整的数据被分成几个部分，如训练数据、验证数据和测试数据。在*第3章至第6章*中，我们始终使用交叉验证分数作为我们的目标函数。虽然我们可以手动执行超参数调整并基于分割数据计算交叉验证分数，但`sklearn`提供了专门用于超参数调整的类，这些类在调整过程中使用交叉验证分数作为目标函数。`sklearn`中实现了几个超参数调整类，例如`GridSearchCV`、`RandomizedSearchCV`、`HalvingGridSearchCV`和`HalvingRandomSearchCV`。
- en: Also, all of the hyperparameter tuning classes implemented in `sklearn` have
    a consistent interface. We can use the `fit()` method to perform hyperparameter
    tuning on the given data where the cross-validation score is used as the objective
    function. Then, we can use the `best_params_` attribute to get the best set of
    hyperparameters, the `best_score_` attribute to get the average cross-validated
    score from the best set of hyperparameters, and the `cv_results_` attribute to
    get the details of the hyperparameter tuning process, including but not limited
    to the objective function score for each tested set of hyperparameters in each
    of the folds.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`sklearn`中实现的所有超参数调整类都有一个一致的接口。我们可以使用`fit()`方法在给定数据上执行超参数调整，其中交叉验证分数用作目标函数。然后，我们可以使用`best_params_`属性来获取最佳的超参数集，使用`best_score_`属性来获取从最佳超参数集中得到的平均交叉验证分数，以及使用`cv_results_`属性来获取超参数调整过程的详细信息，包括但不限于每个折中测试的超参数集的目标函数分数。
- en: To prevent data leakage when performing the data preprocessing steps (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*),
    `sklearn` also provides a `Pipeline` object that can be used along with the hyperparameter
    tuning classes. This `Pipeline` object will ensure that any data preprocessing
    steps are only fitted based on the train set during the cross-validation. Essentially,
    this object is just *a chain of several* `sklearn` *transformers and estimators*,
    which has the same `fit()` and `predict()` method, just like a usual `sklearn`
    estimator.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止在执行数据预处理步骤时（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*），数据泄露，`sklearn`也提供了一个`Pipeline`对象，可以与超参数调整类一起使用。这个`Pipeline`对象将确保任何数据预处理步骤仅在交叉验证期间基于训练集进行拟合。本质上，这个对象只是一个由几个`sklearn`
    *转换器和估计器*组成的*链*，它具有相同的`fit()`和`predict()`方法，就像一个普通的`sklearn`估计器一样。
- en: While `sklearn` can be utilized for many ML-related tasks, `scikit-optimize`,
    which is commonly called `sklearn` and can be utilized for implementing the `skopt`
    has a very similar interface to `sklearn`, so it will be very easy for you to
    get familiar with `skopt` once you are already familiar with `sklearn` itself.
    The main hyperparameter tuning class implemented in `skopt` is the `BayesSearchCV`
    class.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`sklearn`可以用于许多与机器学习相关的任务，但通常被称为`sklearn`的`scikit-optimize`，可以用于实现`skopt`，它与`sklearn`具有非常相似的接口，因此一旦你已经熟悉了`sklearn`本身，你将很容易熟悉`skopt`。在`skopt`中实现的主要超参数调整类是`BayesSearchCV`类。
- en: '`skopt` provides four implementations for the optimizer within the `BayesSearchCV`
    class, namely `sklearn` to be utilized as the optimizer. Note that, here, the
    optimizer refers to the `skopt` provides various implementations of the **acquisition
    function**, namely the **Expected Improvement** (**EI**), **Probability of Improvement**
    (**PI**), and **Lower Confidence Bound** (**LCB**) functions.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`skopt`在`BayesSearchCV`类中提供了四种优化器的实现，即用作优化器的`sklearn`。请注意，在这里，优化器指的是`skopt`提供了各种**获取函数**的实现，即**期望改进**（**EI**）、**改进概率**（**PI**）和**下置信界**（**LCB**）函数。'
- en: Last but not least, the `scikit-hyperband` package. Additionally, this package
    is built on top of `sklearn` and is specifically designed for the HB implementation.
    The hyperparameter tuning class implemented in this package is `HyperbandSearchCV`.
    It also has a very similar interface to `sklearn`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，`scikit-hyperband`包。此外，这个包建立在`sklearn`之上，专门为HB实现设计。在这个包中实现的超参数调整类是`HyperbandSearchCV`。它也具有与`sklearn`非常相似的接口。
- en: 'As for `sklearn` and `skopt`, you can easily install them via `pip install`,
    just like you usually install other packages. As for `scikit-hyperband`, the author
    of the package didn’t put this on `sklearn`. Luckily, there’s a forked version
    ([https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband))
    of the original repo that works nicely with the newer version of `sklearn` (`1.0.1`
    or above). To install `scikit-hyperband`, please follow the following steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`sklearn`和`skopt`，你可以像通常安装其他包一样通过`pip install`轻松安装它们。至于`scikit-hyperband`，该包的作者没有将其放在`sklearn`上。幸运的是，有一个分支版本（[https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband)）的原始仓库，它与较新版本的`sklearn`（`1.0.1`或更高版本）配合得很好。要安装`scikit-hyperband`，请按照以下步骤操作：
- en: 'Clone [https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband)
    to your loca machinel:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将[https://github.com/louisowen6/scikit-hyperband](https://github.com/louisowen6/scikit-hyperband)克隆到你的本地机器：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Open the cloned repository:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开克隆的仓库：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Move the `hyperband` folder to your working directory:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hyperband`文件夹移动到你的工作目录：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you are aware of the `scikit-learn`, `scikit-optimize`, and `scikit-hyperband`
    packages, in the following sections, we will learn how to utilize them to implement
    various hyperparameter tuning methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了`scikit-learn`、`scikit-optimize`和`scikit-hyperband`包，在接下来的章节中，我们将学习如何利用它们来实现各种超参数调整方法。
- en: Implementing Grid Search
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现网格搜索
- en: To implement `for loop` that tests all of the possible hyperparameter values
    in the search space. However, by using `sklearn`’s implementation of Grid Search,
    `GridSearchCV`, we can have a cleaner code since we just need to call a single
    line of code to instantiate the class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个`for loop`，测试搜索空间中所有可能的超参数值。然而，通过使用`sklearn`的网格搜索实现`GridSearchCV`，我们可以得到更简洁的代码，因为我们只需要调用一行代码来实例化类。
- en: Let’s walk through an example of how we can utilize `GridSearchCV` to perform
    Grid Search. Note that, in this example, we are performing hyperparameter tuning
    on an RF model. We will utilize `sklearn’s` implementation of RF, `RandomForestClassifier`.
    The dataset used in this example is the *Banking Dataset – Marketing Targets*
    provided on Kaggle ([https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看我们如何利用`GridSearchCV`来执行网格搜索。请注意，在这个例子中，我们正在对一个RF模型进行超参数调整。我们将使用`sklearn`对RF的实现，即`RandomForestClassifier`。在这个例子中使用的数据集是Kaggle上提供的*银行数据集
    – 营销目标*（[https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)）。
- en: Original Data Source
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据来源
- en: This data was first published in *A Data-Driven Approach to Predict the Success
    of Bank Telemarketing*, by Sérgio Moro, Paulo Cortez, and Paulo Rita, Decision
    Support Systems, Elsevier, 62:22–31, June 2014 ([https://doi.org/10.1016/j.dss.2014.03.001](https://doi.org/10.1016/j.dss.2014.03.001)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这份数据最初发表在Sérgio Moro、Paulo Cortez和Paulo Rita合著的《A Data-Driven Approach to Predict
    the Success of Bank Telemarketing》一文中，该文发表于《Decision Support Systems》，Elsevier出版社，第62卷第22-31页，2014年6月([https://doi.org/10.1016/j.dss.2014.03.001](https://doi.org/10.1016/j.dss.2014.03.001))。
- en: 'This is a binary classification dataset with 16 features related to the marketing
    campaigns conducted by a bank institution. The target variable consists of two
    classes, *yes* or *no*, indicating whether the client of the bank has subscribed
    to a term deposit or not. Hence, the goal of training an ML model on this dataset
    is to identify whether a customer is potentially wanting to subscribe to the term
    deposit or not. For more details, you can refer to the description on the Kaggle
    page:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二元分类数据集，包含与银行机构进行的营销活动相关的16个特征。目标变量由两个类别组成，*是*或*否*，表示银行客户的存款是否已订阅定期存款。因此，在这个数据集上训练机器学习模型的目的是确定客户是否可能想要订阅定期存款。更多详情，请参考Kaggle页面上的描述：
- en: 'There are two datasets provided, namely the `train.csv` dataset and the `test.csv`
    dataset. However, we will not use the provided `test.csv` dataset since it is
    sampled directly from the train data. We will manually split `train.csv` into
    two subsets, namely the train set and the test set, using the help of the `train_test_split`
    function from `sklearn` (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*). We will set the `test_size` parameter to
    `0.1`, meaning we will have `40,689` and `4,522` rows for the train set and the
    test set, respectively. The following code shows you how to load the data and
    perform the train set and the test set splitting:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供了两个数据集，即`train.csv`数据集和`test.csv`数据集。然而，我们不会使用提供的`test.csv`数据集，因为它直接从训练数据中采样。我们将使用`sklearn`中的`train_test_split`函数手动将`train.csv`分割成两个子集，即训练集和测试集（见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。我们将`test_size`参数设置为`0.1`，这意味着训练集将有`40,689`行，测试集将有`4,522`行。以下代码展示了如何加载数据并执行训练集和测试集的分割：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Out of the 16 features provided in the data, there are 7 numerical features
    and 9 categorical features. As for the target class distribution, 12% of them
    are *yes* and 88% of them are *no*, for both train and test datasets. This means
    that we can’t use accuracy as our metric since we have an imbalanced class problem—a
    situation where we have a very skewed distribution of the target classes. Instead,
    in this example, we will use the F1-score.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的数据中的16个特征中，有7个是数值特征，9个是分类特征。至于目标类别的分布，训练和测试数据集中都有12%是*是*，88%是*否*。这意味着我们不能使用准确率作为我们的指标，因为我们有一个不平衡的类别问题——目标类别分布非常倾斜的情况。因此，在这个例子中，我们将使用F1分数。
- en: 'Before performing Grid Search, let’s see how *RandomForestClassifier* with
    the default hyperparameter values work. Furthermore, let’s also try to train our
    model on only those seven numerical features for now. The following code shows
    you how to get only numerical features, train the model on those features in the
    train set, and finally, evaluate the model on the test set:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行网格搜索之前，让我们看看使用默认超参数值的*RandomForestClassifier*是如何工作的。此外，我们现在也尝试仅使用这七个数值特征来训练我们的模型。以下代码展示了如何获取仅数值特征，在训练集上使用这些特征训练模型，并最终在测试集上评估模型：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `X_train_numerical` variable only stores numerical features from the train
    data:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train_numerical`变量仅存储训练数据中的数值特征：'
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `X_test_numerical` variable only stores numerical features from the test
    data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_test_numerical`变量仅存储测试数据中的数值特征：'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Fit the model on train data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上拟合模型：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Evaluate the model on the test data:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估模型：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Based on the preceding code, we get around `0.436` for the F1-Score when testing
    our trained RF model on the test set. Remember that this is the result of only
    using numerical features and the default hyperparameters of the `RandomForestClassifier`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们在测试集上对训练好的RF模型进行测试时，F1-Score大约为`0.436`。请记住，这个结果仅使用了数值特征和`RandomForestClassifier`的默认超参数。
- en: 'Before performing Grid Search, we have to define the hyperparameter space in
    a dictionary of list format, where the keys refer to the name of the hyperparameters
    and the lists consist of all the values we want to test for each hyperparameter.
    Let’s say we define the hyperparameter space for `RandomForestClassifier` as follows:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行网格搜索之前，我们必须以列表字典格式定义超参数空间，其中键指的是超参数的名称，而列表包含我们想要为每个超参数测试的所有值。比如说，我们为`RandomForestClassifier`定义超参数空间如下：
- en: '[PRE9]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we have defined the hyperparameter space, we can apply the `GridSearchCV`
    class to the train data, use the best set of hyperparameters to train a new model
    on the full train data, and then evaluate that final trained model on the test
    data, just as we learned in *Chapters 3–6*. The following code shows you how to
    do that:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们定义了超参数空间，我们就可以将`GridSearchCV`类应用于训练数据，使用最佳的超参数集在全部训练数据上训练一个新的模型，然后在该最终训练模型上评估测试数据，正如我们在*第3-6章*中学到的那样。以下代码展示了如何进行：
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Initiate the model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化模型：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initiate the `GridSearchCV` class:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`GridSearchCV`类：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the `GridSearchCV` class:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`GridSearchCV`类：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳的超参数集：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练模型：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Look how clean our code is by utilizing `sklearn`’s implementation of Grid Search
    instead of writing our code from scratch! Notice that we just need to pass `sklearn`’s
    estimator and the hyperparameter space dictionary to the `GridSearchCV` class,
    and the rest will be handled by `sklearn`. In this example, we also pass several
    additional parameters to the class, such as `scoring=’f1’`, `cv=5`, `n_jobs=-1`,
    and `refit=True`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用`sklearn`对网格搜索的实现而不是从头编写代码，看看我们的代码是多么的整洁！注意，我们只需要将`sklearn`的估计器和超参数空间字典传递给`GridSearchCV`类，其余的将由`sklearn`处理。在这个例子中，我们还向类传递了几个额外的参数，例如`scoring='f1'`、`cv=5`、`n_jobs=-1`和`refit=True`。
- en: As its name suggests, the `scoring` parameter governs the scoring strategy that
    we want to use to evaluate our model during the cross-validation. While our objective
    function will always be the cross-validation score, this parameter controls what
    type of score we want to use as our metric. In this example, we are using the
    F1-score as our metric. However, you can also pass a custom callable function
    as the scoring strategy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`scoring`参数控制我们在交叉验证期间想要使用的评分策略。虽然我们的目标函数始终是交叉验证分数，但此参数控制我们想要用作度量标准的分数类型。在这个例子中，我们使用F1分数作为度量标准。然而，您也可以传递一个自定义的可调用函数作为评分策略。
- en: Available Scoring Strategies in Sklearn
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn中可用的评分策略
- en: You can refer to [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)
    for all of the implemented scoring strategies by `sklearn`, and refer to [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)
    if you want to implement your own custom scoring strategy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考[https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)了解`sklearn`实现的所有评分策略，如果您想实现自己的自定义评分策略，请参考[https://scikit-learn.org/stable/modules/model_evaluation.html#scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)。
- en: The `cv` parameter indicates how many folds of cross-validation you want to
    perform. The `n_jobs` parameter controls how many jobs you want to run in parallel.
    If you decide to use all of the processors, you can simply set `n_jobs=–1`, just
    as we did in the example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv`参数表示您想要执行多少个交叉验证折。`n_jobs`参数控制您想要并行运行多少个作业。如果您决定使用所有处理器，您只需将`n_jobs`设置为`-1`即可，就像我们在例子中所做的那样。'
- en: Last but not least, we have the `refit` parameter. This Boolean parameter is
    responsible for deciding whether at the end of the hyperparameter tuning process
    we want to refit our model on the full train set using the best set of hyperparameters
    or not. In this example, we set `refit=True`, meaning that `sklearn` will automatically
    refit our RF model on the full train set using the best set of hyperparameters.
    It is very important to retrain our model on the full train set after performing
    hyperparameter tuning since we only utilize subsets of the train set during the
    hyperparameter tuning process. There are several other parameters that you can
    control when initiating a `GridSearchCV` class. For more details, you can refer
    to the official page of `sklearn` ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们有`refit`参数。这个布尔参数负责决定在超参数调整过程结束后，我们是否想要使用最佳的超参数集在完整的训练集上重新拟合我们的模型。在这个例子中，我们设置`refit=True`，这意味着`sklearn`将自动使用最佳的超参数集在完整的训练集上重新拟合我们的随机森林模型。在执行超参数调整后重新在完整的训练集上重新训练我们的模型非常重要，因为我们只利用了训练集的子集。当初始化`GridSearchCV`类时，你可以控制的其他参数有几个。更多详情，你可以参考`sklearn`的官方页面（[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)）。
- en: 'Let’s go back to our example. By performing Grid Search in the predefined hyperparameter
    space, we are able to get an F1-score of `0.495` when evaluated on the test set.
    The best set of hyperparameters is `{‘class_weight’: ‘balanced’, ‘criterion’:
    ‘entropy’, ‘min_samples_split’: 0.01, ‘n_estimators’: 150}` with an objective
    function score of `0.493`. Note that we can get the best set of hyperparameters
    along with its objective function score via the `best_params_` and `best_score_`
    attributes, respectively. Not bad! We get around `0.06` of improvement in the
    F1-score. However, note that we are still only using numerical features.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们回到我们的例子。通过在预定义的超参数空间中执行网格搜索，我们在测试集上的评估中得到了`0.495`的F1分数。最佳的超参数集是`{''class_weight'':
    ''balanced'', ''criterion'': ''entropy'', ''min_samples_split'': 0.01, ''n_estimators'':
    150}`，其目标函数分数为`0.493`。请注意，我们可以通过`best_params_`和`best_score_`属性分别获取最佳的超参数集及其目标函数分数。还不错！我们在F1分数上获得了大约`0.06`的改进。然而，请注意，我们仍然只使用了数值特征。'
- en: Next, we will try to utilize not only numerical features but also categorical
    features from our data. To be able to utilize those categorical features, we need
    to perform the **categorical encoding** preprocessing step. Why? Because ML models
    are not able to understand non-numerical features. Therefore, we need to convert
    those non-numerical features into numerical ones so that the ML model is able
    to utilize those features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试利用不仅数值特征，还包括数据中的分类特征。为了能够利用这些分类特征，我们需要执行**分类编码**预处理步骤。为什么？因为机器学习模型无法理解非数值特征。因此，我们需要将这些非数值特征转换为数值，以便机器学习模型能够利用这些特征。
- en: 'Remember that when we want to perform any data preprocessing steps, we have
    to be very careful with it to prevent any data leakage problem where we might
    introduce part of our test data into the train data (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*). To prevent this problem, we can utilize
    the `Pipeline` object from `sklearn`. So, instead of passing an estimator to the
    `GridSearchCV` class, we can also pass a `Pipeline` object that consists of a
    chain of data preprocessors and an estimator:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当我们想要执行任何数据预处理步骤时，我们必须非常小心，以防止出现数据泄露问题，我们可能会将部分测试数据引入训练数据（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。为了防止这个问题，我们可以利用`sklearn`中的`Pipeline`对象。因此，我们不仅可以将估计器传递给`GridSearchCV`类，还可以传递一个由一系列数据预处理器和估计器组成的`Pipeline`对象：
- en: 'Since, in this example, not all of our features are categorical and we only
    want to perform categorical encoding on those non-numerical features, we can utilize
    the `ColumnTransformer` class to specify which features we want to apply the categorical
    encoding step. Let’s say we also want to perform a normalization preprocessing
    step on the numerical features. We can also pass those numerical features to the
    `ColumnTransformer` class along with the normalization transformer. Then, it will
    automatically apply the normalization step to only those numerical features. The
    following code shows you how to create such a `Pipeline` object with `ColumnTransformer`,
    where we use `StandardScaler` for the normalization step and `OneHotEncoder` for
    the categorical encoding step:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于在这个例子中，并非所有我们的特征都是分类的，我们只想对那些非数值特征执行分类编码，我们可以利用`ColumnTransformer`类来指定我们想要应用分类编码步骤的特征。假设我们还想对数值特征执行归一化预处理步骤。我们还可以将这些数值特征传递给`ColumnTransformer`类，以及归一化转换器。然后，它将自动仅对那些数值特征应用归一化步骤。以下代码展示了如何使用`ColumnTransformer`创建这样的`Pipeline`对象，其中我们使用`StandardScaler`进行归一化步骤，使用`OneHotEncoder`进行分类编码步骤：
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Get list of numerical features and categorical features:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 获取数值特征和分类特征的列表：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Initiate the preprocessor for numerical features and categorical features:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化数值特征和分类特征的预处理程序：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Delegate each preprocessor to the corresponding features:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个预处理程序委托给相应的特征：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a pipeline of preprocessors and models. In this example, we named our
    pr-processing steps as *“preprocessor”* and the modeling step as *“model”*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 创建预处理程序和模型的管道。在这个例子中，我们将预处理步骤命名为*“preprocessor”*，建模步骤命名为*“model”*：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see in the previous code blocks, the `ColumnTransformer` class is
    responsible for delegating each preprocessor to the corresponding features. Then,
    we can just reuse it for all of our preprocessing steps through a single preprocessor
    variable. Finally, we can create a pipeline consisting of the preprocessor variable
    and `RandomForestClassifier`. Note that within the `ColumnTransformer` class and
    the `Pipeline` class, we also have to provide the name of each preprocessor and
    step in the pipeline, respectively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码块所示，`ColumnTransformer`类负责将每个预处理程序委托给相应的特征。然后，我们只需通过单个预处理程序变量重复使用它，以执行所有预处理步骤。最后，我们可以创建一个由预处理程序变量和`RandomForestClassifier`组成的管道。请注意，在`ColumnTransformer`类和`Pipeline`类中，我们还需要分别提供每个预处理程序和管道步骤的名称。
- en: 'Now that we have defined the pipeline, we can see how our model performs on
    the test set (without hyperparameter tuning) by utilizing all of the features
    and preprocessors defined in the pipeline. The following code shows how we can
    directly use the pipeline to perform the same `fit()` and `predict()` methods
    as we did earlier:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了管道，我们可以通过利用管道中定义的所有特征和预处理程序来查看我们的模型在测试集上的表现（不进行超参数调整）。以下代码展示了我们如何直接使用管道执行与之前相同的`fit()`和`predict()`方法：
- en: '[PRE21]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Based on the preceding code, we get around `0.516` for the F1-score when testing
    our trained pipeline on the test set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们在测试训练好的管道时，F1分数大约为`0.516`。
- en: 'Next, we can start performing Grid Search over the pipeline, too. However,
    before we can do that, we need to redefine the hyperparameter space. We need to
    change the keys in the dictionary with the format of `<estimator_name_in_pipeline>__<hyperparameter_name>`.
    The following is the redefined version of our hyperparameter space:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们也可以开始对管道执行网格搜索。然而，在我们能够这样做之前，我们需要重新定义超参数空间。我们需要更改字典中的键，格式为`<estimator_name_in_pipeline>__<hyperparameter_name>`。以下是我们重新定义的超参数空间：
- en: '[PRE22]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The following code shows you how to perform Grid Search over the pipeline instead
    of the estimator itself. Essentially, the code is the same as the previous version.
    The only difference is that we are performing the Grid Search over the pipeline
    and on *all* of the features in the data, not just the numerical features.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码展示了如何对管道而不是估计器本身执行网格搜索。本质上，代码与上一个版本相同。唯一的区别是我们正在对管道和数据中的所有特征而不是仅数值特征执行网格搜索。
- en: 'Initiate the `GridSearchCV` class:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`GridSearchCV`类：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Run the `GridSearchCV` class:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`GridSearchCV`类：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Based on the preceding code, we get around `0.549` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.01, ‘model__n_estimators’:
    100}` with an objective function score of `0.549`.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试最终训练好的RF模型时，在测试集上使用最佳超参数集得到了大约`0.549`的F1分数。最佳超参数集是`{‘model__class_weight’:
    ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’:
    0.01, ‘model__n_estimators’: 100}`，其目标函数分数为`0.549`。'
- en: 'It is worth noting that we can also *create a pipeline within a pipeline*.
    For example, we can create a pipeline for `numeric_preprocessor` that consists
    of a chain of missing value imputation and normalization modules. The following
    code shows how we can create such a pipeline. The `SimpleImputer` class is the
    missing value imputation transformer from `sklearn` that can help us to perform
    mean, median, mode, or constant imputation strategies if there are any missing
    values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们还可以*在管道中创建管道*。例如，我们可以创建一个由缺失值插补和归一化模块组成的`numeric_preprocessor`管道。以下代码展示了我们如何创建这样的管道。`SimpleImputer`类是来自`sklearn`的缺失值插补转换器，它可以帮助我们在存在缺失值时执行均值、中位数、众数或常数插补策略：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this section, we have learned how to implement Grid Search in `sklearn` through
    the `GridSearchCV` class, starting from defining the hyperparameter space, setting
    each important parameter of the `GridSearchCV` class, learning how to utilize
    the `Pipeline` and `ColumnTransformer` classes to prevent data leakage issues,
    and learning how to create a pipeline within the pipeline.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在`sklearn`中通过`GridSearchCV`类实现网格搜索，从定义超参数空间开始，设置`GridSearchCV`类的每个重要参数，学习如何利用`Pipeline`和`ColumnTransformer`类来防止数据泄露问题，以及学习如何在管道中创建管道。
- en: In the next section, we will learn how to implement Random Search in `sklearn`
    via `RandomizedSearchCV`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何在`sklearn`中通过`RandomizedSearchCV`实现随机搜索。
- en: Implementing Random Search
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现`sklearn`的随机搜索
- en: Implementing `sklearn` is very similar to implementing Grid Search. The main
    difference is that we have to provide the number of trials or iterations since
    Random Search will not try all of the possible combinations in the hyperparameter
    space. Additionally, we have to provide the accompanying distribution for each
    of the hyperparameters when defining the search space. In `sklearn`, Random Search
    is implemented in the `RandomizedSearchCV` class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实现`sklearn`与实现网格搜索非常相似。主要区别是我们必须提供试验次数或迭代次数，因为随机搜索不会尝试超参数空间中的所有可能组合。此外，在定义搜索空间时，我们必须为每个超参数提供相应的分布。在`sklearn`中，随机搜索是通过`RandomizedSearchCV`类实现的。
- en: 'To understand how we can implement Random Search in `sklearn`, let’s use the
    same example from the *Implementing Grid Search* section. Let’s directly try using
    all of the features available in the dataset. All of the pipeline creation processes
    are exactly the same, so we will directly jump into the process of how to define
    the hyperparameter space and the `RandomizedSearchCV` class. The following code
    shows you how to define the accompanying distribution for each of the hyperparameters
    in the space:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们如何在`sklearn`中实现随机搜索，让我们使用*实现网格搜索*部分中的相同示例。让我们直接尝试使用数据集中可用的所有特征。所有管道创建过程都是完全相同的，所以我们将直接跳转到如何定义超参数空间和`RandomizedSearchCV`类的过程中。以下代码展示了如何为空间中的每个超参数定义相应的分布：
- en: '[PRE31]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see, the hyperparameter space is quite different from the one that
    we defined previously in the *Implementing Grid Search* section. Here, we are
    also specifying the distribution for each of the hyperparameters, where `randint`
    and `truncnorm` are utilized for the `n_estimators` and `min_samples_split` hyperparameters.
    As for `criterion` and `class_weight`, we are still using the same configuration
    as the previous search space. Note that *by not specifying any distribution* means
    we are *applying uniform distribution* to the hyperparameter, where all of the
    values will have the same probability to be tested.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，超参数空间与我们之前在*实现网格搜索*部分中定义的相当不同。在这里，我们还在每个超参数上指定了分布，其中`randint`和`truncnorm`用于`n_estimators`和`min_samples_split`超参数。至于`criterion`和`class_weight`，我们仍然使用与之前搜索空间相同的配置。请注意，*不指定任何分布*意味着我们对超参数应用了*均匀分布*，其中所有值都有相同的机会被测试。
- en: Essentially, the `randint` distribution is just a uniform distribution for discrete
    variables, while `truncnorm` stands for truncated normal distribution, which,
    as its name suggests, is a modified normal distribution bounded on a particular
    range. In this example, the range is bounded on a range from `a=0` and `b=0.5`,
    with a mean of `loc=0.005` and a standard deviation of `scale=0.01`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，`randint`分布是对离散变量的均匀分布，而`truncnorm`代表截断正态分布，正如其名称所暗示的，它是一个在特定范围内有界的修改后的正态分布。在这个例子中，范围被限制在`a=0`和`b=0.5`之间，均值为`loc=0.005`，标准差为`scale=0.01`。
- en: Distribution for Hyperparameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数分布：
- en: There are many other available distributions that you can utilize. `sklearn`
    accepts all distributions that have the `rvs` method, as in the distribution implementation
    from `Scipy`. Essentially, this method is just a method to sample a value from
    the specified distribution. For more details, please refer to the official documentation
    page of `Scipy` ([https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他可用的分布，您可以使用。`sklearn`接受所有具有`rvs`方法的分布，就像`Scipy`中的分布实现一样。本质上，这个方法只是一个从指定分布中采样值的方法。有关更多详细信息，请参阅`Scipy`的官方文档页面（[https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions)）。
- en: 'When initiating the `RandomizedSearchCV` class, we also have to define the
    `n_iter` and `random_state` parameters, which refer to the number of iterations
    and the random seed, respectively. The following code shows you how to perform
    Random Search over the same pipeline defined in the *Implementing Grid Search*
    section. In contrast with the example in the *Implementing Grid Search* section,
    which only performs `120` iterations of Grid Search, here, we perform `200` iterations
    of random search since we set `n_iter=200`. Additionally, we have a bigger hyperparameter
    space since we increase the granularity of the `n_estimators` and `min_samples_split`
    hyperparameter values:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当初始化`RandomizedSearchCV`类时，我们还需要定义`n_iter`和`random_state`参数，分别代表迭代次数和随机种子。以下代码展示了如何执行与*实现网格搜索*部分中定义的相同管道的随机搜索。与*实现网格搜索*部分中的示例相比，该示例仅执行`120`次网格搜索迭代，而在这里，我们执行`200`次随机搜索，因为我们设置了`n_iter=200`。此外，我们有一个更大的超参数空间，因为我们增加了`n_estimators`和`min_samples_split`超参数值的粒度：
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initiate the `RandomizedSearchCV` class:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`RandomizedSearchCV`类：
- en: '[PRE39]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Run the `RandomizedSearchCV` class:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`RandomizedSearchCV`类：
- en: '[PRE43]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE44]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练的模型：
- en: '[PRE45]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Based on the preceding code, we get around `0.563` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005155815445940717,
    ‘model__n_estimators’: 187}` with an objective function score of `0.562`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试集上使用最佳超参数集测试最终训练的RF模型时，F1分数大约为`0.563`。最佳超参数集是`{‘model__class_weight’:
    ‘balanced_subsample’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’:
    0.005155815445940717, ‘model__n_estimators’: 187}`，目标函数得分为`0.562`。'
- en: In this section, we have learned how to implement Random Search in `sklearn`
    through the `RandomizedSearchCV` class, starting from defining the hyperparameter
    space to setting each important parameter of the `RandomizedSearchCV` class. In
    the next section, we will learn how to perform CFS with `sklearn`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过`RandomizedSearchCV`类在`sklearn`中实现随机搜索，从定义超参数空间到设置`RandomizedSearchCV`类的每个重要参数。在下一节中，我们将学习如何使用`sklearn`执行CFS（粗到细搜索）。
- en: Implementing Coarse-to-Fine Search
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现粗到细搜索：
- en: '`sklearn` package, you can find the implemented custom class, `CoarseToFineSearchCV`,
    in the repo mentioned in the *Technical Requirements* section.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*技术要求*部分提到的仓库中，您可以在`sklearn`包中找到实现的自定义类`CoarseToFineSearchCV`。
- en: Let’s use the same example and hyperparameter space as in the *Implementing
    Random Search* section, to see how `CoarseToFineSearchCV` works in practice. Note
    that this implementation of CFS only utilizes Random Search and uses the top *N*
    percentiles scheme to define the promising subspace in each iteration, similar
    to the example shown in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054).
    However, *you can edit the code based on your own preference* since CFS is a very
    simple method with customizable modules.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与 *实现随机搜索* 部分相同的示例和超参数空间，以了解 `CoarseToFineSearchCV` 在实际中的应用。请注意，此实现中的CFS仅利用随机搜索，并使用前
    *N* 百分位方案来定义每一轮中的有希望子空间，类似于在 [*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054) 中显示的示例。然而，*你可以根据自己的偏好编辑代码*，因为CFS是一个非常简单的可自定义模块的方法。
- en: The following code shows you how to perform CFS with the `CoarseToFineSearchCV`
    class. It is worth noting that this class has very similar parameters to the `RandomizedSearchCV`
    class, with several additional parameters. The `random_iters` parameter controls
    the number of iterations for each random search trial, `top_n_percentile` controls
    the *N* value within the top N percentiles promising subspace definition (see
    [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)), `n_iter` defines the number
    of CFS iterations to be performed, and `continuous_hyperparams` stores the list
    of continuous hyperparameters in the predefined space.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用 `CoarseToFineSearchCV` 类执行CFS。值得注意的是，此类具有与 `RandomizedSearchCV`
    类非常相似的参数，但有几个额外的参数。`random_iters` 参数控制每个随机搜索试验的迭代次数，`top_n_percentile` 控制在定义有希望子空间的前
    N 百分位中的 *N* 值（参见 [*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)），`n_iter` 定义要执行的CFS迭代次数，而
    `continuous_hyperparams` 存储预定义空间中的连续超参数列表。
- en: 'Initiate the `CoarseToFineSearchCV` class:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `CoarseToFineSearchCV` 类：
- en: '[PRE46]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Run the `CoarseToFineSearchCV` class:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `CoarseToFineSearchCV` 类：
- en: '[PRE51]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE52]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型：
- en: '[PRE53]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Based on the preceding code, we get around `0.561` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005867409821769845,
    ‘model__n_estimators’: 106}` with an objective function score of `0.560`.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在使用最佳超参数集在测试集上测试最终训练好的RF模型时，F1分数大约为 `0.561`。最佳超参数集是 `{‘model__class_weight’:
    ‘balanced_subsample’，‘model__criterion’: ‘entropy’，‘model__min_samples_split’:
    0.005867409821769845，‘model__n_estimators’: 106}`，目标函数得分为 `0.560`。'
- en: In this section, we have learned how to implement CFS using a custom class on
    top of `sklearn` through the `CoarseToFineSearchCV` class. In the next section,
    we will learn how to perform SH with `sklearn`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过 `CoarseToFineSearchCV` 类在 `sklearn` 上实现CFS。在下一节中，我们将学习如何使用 `sklearn`
    执行SH。
- en: Implementing Successive Halving
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现连续减半法
- en: Similar to CFS, `sklearn`, namely `HalvingGridSearchCV` and `HalvingRandomSearchCV`.
    As their names suggest, the former class is an implementation of SH that utilizes
    Grid Search in each of the SH iterations, while the latter utilizes Random Search.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与CFS类似，`sklearn` 中有 `HalvingGridSearchCV` 和 `HalvingRandomSearchCV`。正如它们的名称所暗示的，前者是SH的实现，在每个SH迭代中使用网格搜索，而后者使用随机搜索。
- en: By default, SH implementations in `sklearn` use the number of samples, or *n_samples*,
    as the definition of the budget or resource in SH. However, it is also possible
    to define a budget with other definitions. For example, we can use `n_estimators`
    in RF as the budget, instead of using the number of samples. It is worth noting
    that we cannot use `n_estimators`, or any other hyperparameters, to define the
    budget if it is part of the hyperparameter space.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`sklearn` 中的SH实现使用样本数量，即 *n_samples*，作为SH中预算或资源的定义。然而，也可以使用其他定义来定义预算。例如，我们可以使用RF中的
    `n_estimators` 作为预算，而不是使用样本数量。值得注意的是，如果它是超参数空间的一部分，则不能使用 `n_estimators` 或任何其他超参数来定义预算。
- en: Both `HalvingGridSearchCV` and `HalvingRandomSearchCV` have similar standard
    SH parameters to control how the SH iterations will work, such as the `factor`
    parameter, which refers to the multiplier factor for SH, `resource`, which refers
    to what definition of budget we want to use, `max_resources` refers to the maximum
    budget or resource, and `min_resources`, which refers to the minimum number of
    resources to be used at the first iteration. By default, the `max_resources` parameter
    is set to *auto*, meaning it will use the total number of samples that we have
    when `resource=’n_samples’`. On the other hand, `sklearn` implemented a heuristic
    to define the default value for the `min_resources` parameter, referred to as
    *smallest*. This heuristic will ensure that we have a small value of `min_resources`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`HalvingGridSearchCV`和`HalvingRandomSearchCV`都有类似的标准SH参数来控制SH迭代的工作方式，例如`factor`参数，它指的是SH的乘数因子，`resource`指的是我们想要使用的预算定义，`max_resources`指的是最大预算或资源，而`min_resources`指的是第一次迭代中要使用的最小资源数量。默认情况下，`max_resources`参数设置为*auto*，这意味着当`resource=’n_samples’`时，它将使用我们拥有的总样本数。另一方面，`sklearn`实现了一个启发式方法来定义`min_resources`参数的默认值，称为*smallest*。这个启发式方法将确保我们有一个小的`min_resources`值。'
- en: Specific for `HalvingRandomSearchCV`, there is also the `n_candidates` parameter
    that refers to the initial number of candidates to be evaluated at the first iteration.
    Note that this parameter is not available in `HalvingGridSearchCV` since it will
    automatically evaluate all of the hyperparameter candidates in the predefined
    space. It is worth noting that `sklearn` implemented a strategy, called *exhaust*,
    to define the default value of the `n_candidates` parameter. This strategy ensures
    that we evaluate enough candidates at the first iteration so that we can utilize
    as many resources as possible at the last SH iteration.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`HalvingRandomSearchCV`来说，还有一个`n_candidates`参数，它指的是第一次迭代中要评估的初始候选者数量。请注意，这个参数在`HalvingGridSearchCV`中不可用，因为它将自动评估预定义空间中的所有超参数候选者。值得注意的是，`sklearn`实现了一个名为*exhaust*的策略来定义`n_candidates`参数的默认值。这个策略确保我们在第一次迭代中评估足够的候选者，以便在最后的SH迭代中尽可能多地利用资源。
- en: Besides those standard SH parameters, both of the classes also have the `aggressive_elimination`
    parameter, which can be utilized when we have a low number of resources. If this
    Boolean parameter is set to `True`, `sklearn` will automatically rerun the first
    SH iteration several times until the number of candidates is small enough. The
    goal of this parameter is to ensure that we only evaluate a maximum of `factor`
    candidates in the last SH iteration. Note that this parameter is only implemented
    in `sklearn`, the original SH doesn’t introduce this strategy as part of the tuning
    method (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那些标准SH参数之外，这两个类也都有`aggressive_elimination`参数，当资源较少时可以利用。如果这个布尔参数设置为`True`，`sklearn`将自动重新运行第一次SH迭代几次，直到候选者数量足够小。这个参数的目标是确保我们只在最后的SH迭代中评估最多`factor`个候选者。请注意，这个参数只在`sklearn`中实现，原始SH没有将这个策略作为调整方法的一部分（参见[*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)）。
- en: Similar to `GridSearchCV` and `RandomizedSearchCV`, `HalvingGridSearchCV` and
    `HalvingRandomSearchCV` also have the usual default `sklearn` parameters for hyperparameter
    tuning, such as `cv`, `scoring`, `refit`, `random_state`, and `n_jobs`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与`GridSearchCV`和`RandomizedSearchCV`类似，`HalvingGridSearchCV`和`HalvingRandomSearchCV`也有用于超参数调整的常用`sklearn`参数，例如`cv`、`scoring`、`refit`、`random_state`和`n_jobs`。
- en: Experimental Features of SH in sklearn
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn中SH的实验特性
- en: It is worth noting that as per `version 1.0.2` of `sklearn`, the SH implementations
    are still in the experimental phase. This means that there might be changes in
    the implementation or interface of the classes without any depreciation cycle.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，根据`sklearn`的`1.0.2`版本，SH的实现仍然处于实验阶段。这意味着类的实现或接口可能会有变化，而无需任何降级周期。
- en: 'The following code shows how `HalvingRandomSearchCV` works with its default
    SH parameters. Note that we still use the same example and hyperparameter space
    as in the *Implementing Random Search* section. It is also worth noting that we
    only use the `HalvingRandomSearchCV` class in this example since `HalvingGridSearchCV`
    has a very similar interface:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了`HalvingRandomSearchCV`如何使用其默认的SH参数进行工作。请注意，我们仍然使用与*实现随机搜索*部分相同的示例和超参数空间。也值得注意，在这个例子中我们只使用了`HalvingRandomSearchCV`类，因为`HalvingGridSearchCV`有一个非常相似的接口：
- en: '[PRE55]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Initiate the `HalvingRandomSearchCV` class:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`HalvingRandomSearchCV`类：
- en: '[PRE57]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Run the `HalvingRandomSearchCV` class:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`HalvingRandomSearchCV`类：
- en: '[PRE63]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE64]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练的模型：
- en: '[PRE65]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Based on the preceding code, we get around `0.556` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.007286406330027324,
    ‘model__n_estimators’: 42}` with an objective function score of `0.565`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '基于前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的RF模型时，F1分数大约为`0.556`。最佳超参数集为`{‘model__class_weight’:
    ‘balanced_subsample’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’:
    0.007286406330027324, ‘model__n_estimators’: 42}`，其目标函数得分为`0.565`。'
- en: 'The following code shows you how to generate a figure that shows the tuning
    process in each SH iteration:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码展示了如何生成一个显示每个SH迭代中调整过程的图：
- en: '[PRE66]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Get the fitting history of each trial:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 获取每个试验的拟合历史：
- en: '[PRE67]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Plot the fitting history for each trial:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制每个试验的拟合历史图：
- en: '[PRE68]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Based on the preceding code, we get the following figure:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们得到以下图：
- en: '![Figure 7.1 – The SH hyperparameter tuning process'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1 – SH超参数调整过程'
- en: '](img/B18753_07_001.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_07_001.jpg](img/B18753_07_001.jpg)'
- en: Figure 7.1 – The SH hyperparameter tuning process
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – SH超参数调整过程
- en: Based on *Figure 7.1*, we can see that we only utilized around 14,000 samples
    in the last iteration while we have around 40,000 samples in our training data.
    Indeed, this is not an ideal case since there are too many samples not being utilized
    in the last SH iteration. We can change the default value of the SH parameters
    set by `sklearn` to ensure that we utilize as many resources as possible at the
    last iteration, through the `min_resources` and `n_candidates` parameters.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*图7.1*，我们可以看到在最后一次迭代中，我们只使用了大约14,000个样本，而我们的训练数据中有大约40,000个样本。实际上，这并不是一个理想的情况，因为在最后一次SH迭代中，有太多的样本没有被利用。我们可以通过`min_resources`和`n_candidates`参数更改`sklearn`设置的SH参数的默认值，以确保在最后一次迭代中尽可能多地利用资源。
- en: In this section, we have learned how to implement SH in `sklearn` through the
    `HalvingRandomSearchCV` and `HalvingGridSearchCV` classes. We have also learned
    all of the important parameters available for both classes. In the next section,
    we will learn how to perform HB with `scikit-hyperband`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过`HalvingRandomSearchCV`和`HalvingGridSearchCV`类在`sklearn`中实现SH。我们还学习了这两个类中所有重要的参数。在下一节中，我们将学习如何使用`scikit-hyperband`执行HB。
- en: Implementing Hyper Band
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Hyper Band
- en: The extension of Successive Halving, the `scikit-hyperband` package. This package
    is built on top of `sklearn`, which means it also provides a very similar interface
    for `GridSearchCV`, `RandomizedSearchCV`, `HalvingGridSearchCV`, and `HalvingRandomSearchCV`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Successive Halving的扩展，`scikit-hyperband`包。这个包建立在`sklearn`之上，这意味着它也为`GridSearchCV`、`RandomizedSearchCV`、`HalvingGridSearchCV`和`HalvingRandomSearchCV`提供了非常相似的接口。
- en: In contrast with the default SH budget definition in the `sklearn` implementation,
    *Scikit-Hyperband defines the budget* as the number of estimators, *n_estimators*,
    in an ensemble of trees, or the number of iterations for estimators trained with
    stochastic gradient descent, such as the XGBoost algorithm. Additionally, we can
    use any other hyperparameters that exist in the estimator as the budget definition.
    However, `scikit-hyperband` *doesn’t allow us to use the number of samples as
    the budget definition*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与`sklearn`实现中默认的SH预算定义相比，*Scikit-Hyperband将预算定义为*树集成中的估计器数量，即*n_estimators*，或者使用随机梯度下降训练的估计器的迭代次数，例如XGBoost算法。此外，我们还可以使用估计器中存在的任何其他超参数作为预算定义。然而，`scikit-hyperband`*不允许我们将样本数量作为预算定义*。
- en: Let’s use the same example as in the *Implementing Successive Halving* section,
    but with a different hyperparameter space. Here, we use the number of estimators,
    *n_estimators*, as the resource, which means we have to take out this hyperparameter
    from our search space. Note that you also have to remove any other hyperparameters
    from the space when you use it as the resource definition, just like in the `sklearn`
    implementation of SH.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与*实现Successive Halving*部分相同的示例，但使用不同的超参数空间。在这里，我们使用估计器数量，即*n_estimators*，作为资源，这意味着我们必须从搜索空间中移除这个超参数。请注意，当您将其用作资源定义时，您还必须从空间中移除任何其他超参数，就像在`sklearn`的SH实现中一样。
- en: The following code shows you how `HyperbandSearchCV` works. The `resource_param`
    parameter refers to the hyperparameter that you want to use as the budget definition.
    The `eta` parameter is actually the same as the factor parameter in the `HalvingRandomSearchCV`
    or `HalvingGridSearchCV` classes, which refers to the multiplier factor for each
    SH run. The `min_iter` and `max_iter` parameters refer to the minimum and maximum
    resources for all brackets. Note that there’s no automatic strategy like in the
    `sklearn` implementation of SH for setting the value of the `min_iter` and `max_iter`
    parameters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `HyperbandSearchCV` 的工作原理。`resource_param` 参数指的是用作预算定义的超参数。`eta` 参数实际上与
    `HalvingRandomSearchCV` 或 `HalvingGridSearchCV` 类中的因子参数相同，它指的是每个 SH 运行的乘数因子。`min_iter`
    和 `max_iter` 参数指的是所有括号的最小和最大资源。请注意，与 `sklearn` 实现的 SH 相比，这里没有自动策略来设置 `min_iter`
    和 `max_iter` 参数的值。
- en: 'The remaining `HyperbandSearchCV` parameters are similar to any other `sklearn`
    implementation of the hyperparameter tuning methods. It is worth noting that the
    HB implementation used in this book is the modified version of the `scikit-hyperband`
    package. Please check the following folder in the book’s GitHub repo ([https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband)):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的 `HyperbandSearchCV` 参数与其他任何 `sklearn` 实现的超参数调整方法类似。值得注意的是，本书中使用的 HB 实现是
    `scikit-hyperband` 包的修改版。请检查本书 GitHub 仓库中的以下文件夹（[https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband)）：
- en: '[PRE69]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Initiate the `HyperbandSearchCV` class:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `HyperbandSearchCV` 类：
- en: '[PRE70]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Run the `HyperbandSearchCV` class:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `HyperbandSearchCV` 类：
- en: '[PRE76]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE77]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型：
- en: '[PRE78]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Based on the preceding code, we get around `0.569` in F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced’, ‘model__criterion’:
    ‘entropy’, ‘model__min_samples_split’: 0.0055643644642829684, ‘model__n_estimators’:
    33}` with an objective function score of `0.560`. Note that although we remove
    `model__n_estimators` from the search space, `HyperbandSearchCV` still outputs
    the best value for this hyperparameter by choosing from the best bracket.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的 RF 模型时，F1 分数大约为 `0.569`。最佳超参数集为 `{‘model__class_weight’:
    ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0055643644642829684,
    ‘model__n_estimators’: 33}`，目标函数得分为 `0.560`。请注意，尽管我们从搜索空间中移除了 `model__n_estimators`，但
    `HyperbandSearchCV` 仍然通过从最佳括号中选择来输出此超参数的最佳值。'
- en: In this section, we have learned how to implement HB using the help of the `scikit-hyperband`
    package along with all of the important parameters available for the `HyperbandSearchCV`
    class. In the next section, we will learn how to perform Bayesian Optimization
    with `scikit-optimize`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何借助 `scikit-hyperband` 包以及 `HyperbandSearchCV` 类的所有重要参数来实现 HB。在下一节中，我们将学习如何使用
    `scikit-optimize` 进行贝叶斯优化。
- en: Implementing Bayesian Optimization Gaussian Process
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化高斯过程
- en: '`skopt` package. Similar to `scikit-hyperband`, this package is also built
    on top of the `sklearn` package, which means the interface for the implemented
    Bayesian Optimization tuning class, `BayesSearchCV`, is very similar to `GridSearchCV`,
    `RandomizedSearchCV`, `HalvingGridSearchCV`, `HalvingRandomSearchCV`, and `HyperbandSearchCV`.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`skopt` 包。与 `scikit-hyperband` 类似，此包也是建立在 `sklearn` 包之上，这意味着实现的贝叶斯优化调整类的接口与
    `GridSearchCV`、`RandomizedSearchCV`、`HalvingGridSearchCV`、`HalvingRandomSearchCV`
    和 `HyperbandSearchCV` 非常相似。'
- en: However, unlike `sklearn` or `scikit-hyperband`, which works well directly with
    the distribution implemented in `scipy`, in `skopt`, we can only use the wrapper
    provided by the package when defining the hyperparameter space. The wrappers are
    defined within the `skopt.space.Dimension` instances and consist of three types
    of dimensions, such as `Real`, `Integer`, and `Categorical`. Within each of these
    dimension wrappers, `skopt` actually uses the same distribution from the `scipy`
    package.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 `sklearn` 或 `scikit-hyperband` 不同，后者可以直接与 `scipy` 中实现的分布一起工作，在 `skopt`
    中，我们只能在定义超参数空间时使用该包提供的包装器。包装器是在 `skopt.space.Dimension` 实例中定义的，包括三种类型的维度，如 `Real`、`Integer`
    和 `Categorical`。在每个维度包装器内部，`skopt` 实际上使用与 `scipy` 包中相同的分布。
- en: By default, the `Real` dimension only supports the `uniform` and `log-uniform`
    distributions and can take any real/numerical value as the input. As for the `Categorical`
    dimension, this wrapper can only take categorical values as the input, as implied
    by its name. It will automatically convert categorical values into integers or
    even real values, which means we can also utilize categorical hyperparameters
    for BOGP! Although we can do this, remember that BOGP only works best for the
    actual real variables (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*). Finally, we have the `Integer` dimension wrapper.
    By default, this wrapper only supports `uniform` and `log-uniform` distributions
    for integer formatting. The `uniform` distribution will utilize the `randint`
    distribution from `scipy`, while the `log-uniform` distribution is exactly the
    same as the one that is used in the Real wrapper.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Real` 维度仅支持 `uniform` 和 `log-uniform` 分布，并可以接受任何实数/数值作为输入。至于 `Categorical`
    维度，这个包装器只能接受分类值作为输入，正如其名称所暗示的。它将自动将分类值转换为整数甚至实数值，这意味着我们也可以利用分类超参数进行 BOGP！虽然我们可以这样做，但请记住，BOGP
    只在真实变量方面表现最佳（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）。最后，我们有
    `Integer` 维度包装器。默认情况下，此包装器仅支持用于整数格式的 `uniform` 和 `log-uniform` 分布。`uniform` 分布将使用
    `scipy` 中的 `randint` 分布，而 `log-uniform` 分布与 `Real` 包装器中使用的完全相同。
- en: 'It is worth noting that we can write our own wrapper for other distributions
    too; for example, the `truncnorm` distribution that we use in all of our earlier
    examples. In fact, you can find the custom `Real` wrapper that consists of the
    `truncnorm`, `uniform`, and `log-uniform` distributions in the repo mentioned
    in the *Technical Requirements* section. The following code shows you how we can
    define the hyperparameter space for `BayesSearchCV`. Note that we are still using
    the same example and hyperparameter space as the *Implementing Random Search*
    section. Here, `Integer` and `Categorical` are the original wrappers provided
    by `skopt`, while the `Real` wrapper is the custom wrapper that consists of the
    `truncnorm` distribution, too:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们也可以为其他分布编写自己的包装器；例如，我们在所有早期示例中都使用的 `truncnorm` 分布。实际上，你可以在*技术要求*部分提到的仓库中找到包含
    `truncnorm`、`uniform` 和 `log-uniform` 分布的自定义 `Real` 包装器。以下代码展示了我们如何为 `BayesSearchCV`
    定义超参数空间。请注意，我们仍在使用与*实现随机搜索*部分相同的示例和超参数空间。在这里，`Integer` 和 `Categorical` 是 `skopt`
    提供的原始包装器，而 `Real` 包装器是包含 `truncnorm` 分布的自定义包装器：
- en: '[PRE79]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: All of the parameters of the `BayesSearchCV` class are very similar to the `GridSearchCV`,
    `RandomizedSearchCV`, `HalvingGridSearchCV`, `HalvingRandomSearchCV`, or `HyperbandSearchCV`.
    The only specific parameters for `BayesSearchCV` are the `n_iter` and `optimizer_kwargs`
    which refer to the total number of trials to be performed and the parameter that
    consists of all related parameters for the `Optimizer`, respectively. Here, the
    `Optimizer` is a class that represents each of the Bayesian Optimization steps,
    starting from initializing the initial points, fitting the surrogate model, sampling
    the next set of hyperparameters using the help of the acquisition function, and
    optimizing the acquisition function (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`BayesSearchCV` 类的所有参数都与 `GridSearchCV`、`RandomizedSearchCV`、`HalvingGridSearchCV`、`HalvingRandomSearchCV`
    或 `HyperbandSearchCV` 非常相似。`BayesSearchCV` 的唯一特定参数是 `n_iter` 和 `optimizer_kwargs`，分别表示要执行的总试验次数和包含
    `Optimizer` 所有相关参数的参数。在这里，`Optimizer` 是一个表示每个贝叶斯优化步骤的类，从初始化初始点、拟合代理模型、使用获取函数的帮助来采样下一组超参数，以及优化获取函数（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）。'
- en: There are several parameters available that we can pass to the `optimizer_kwargs`
    dictionary. The `base_estimator` parameter refers to the type of surrogate model
    to be used. `skopt` has prepared several surrogate models with default setups,
    including the Gaussian Process or *GP*. The `n_initial_points` parameter refers
    to the number of random initial points before the actual Bayesian Optimization
    steps begin. The `initial_point_generator` parameter refers to the initialization
    method to be used. By default, `skopt` will initialize them randomly. However,
    you can also change the initialization method to *lhs*, *sobol*, *halton*, *hammersly*,
    or *grid*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个参数可以传递给`optimizer_kwargs`字典。`base_estimator`参数指的是要使用的代理模型类型。`skopt`已经准备了几个具有默认设置的代理模型，包括高斯过程或*GP*。`n_initial_points`参数指的是在开始实际贝叶斯优化步骤之前的随机初始点的数量。`initial_point_generator`参数指的是要使用的初始化方法。默认情况下，`skopt`将它们随机初始化。然而，您也可以将初始化方法更改为*lhs*、*sobol*、*halton*、*hammersly*或*grid*。
- en: As for the type of acquisition function to be used, by default, `skopt` will
    use *gp_hedge*, which is an acquisition function that will automatically choose
    either one of the `acq_func` parameter to *LCB*, *EI*, and *PI*, respectively.
    As explained in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036), besides
    choosing what acquisition function needs to be used, we also have to define what
    kind of optimizer to be utilized for the acquisition function itself. There are
    two options for the acquisition function’s optimizer provided by `skopt`, namely
    random sampling (*sampling*) and *lbfgs*, or the type of second-order optimization
    strategy mentioned in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036). By
    default, `skopt` sets the `acq_optimizer` parameter to *auto*, which will choose
    automatically when to use the *sampling* or *lbfgs* optimization methods.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 关于要使用的获取函数类型，默认情况下，`skopt`将使用`gp_hedge`，这是一个会自动选择`acq_func`参数中的`LCB`、`EI`和`PI`之一的获取函数。如[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)中所述，除了选择需要使用的获取函数外，我们还需要定义用于获取函数本身的优化器类型。`skopt`提供了两种获取函数优化器的选项，即随机采样（*sampling*）和*lbfgs*，或者[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)中提到的二阶优化策略类型。默认情况下，`skopt`将`acq_optimizer`参数设置为*auto*，这将自动选择何时使用*sampling*或*lbfgs*优化方法。
- en: Finally, we can also pass the `acq_func_kwargs` parameter within the `optimizer_kwargs`
    parameter. We can pass all parameters related to the acquisition function to this
    `acq_func_kwargs` parameter; for example, the `xi` parameter that controls the
    exploration and exploitation behavior of the BOGP, as explained in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036).
    While the `xi` parameter is responsible for controlling the exploration versus
    exploitation trade-off for EI and PI acquisition functions, there is also another
    parameter called `kappa`, which is responsible for the same task as the LCB acquisition
    function. The higher the value of `xi` or `kappa` means that we are favoring exploration
    over exploitation, and vice versa. For more information about all of the parameters
    that are available in the `BayesSearchCV` class, you can refer to the official
    API reference of the `skopt` package ([https://scikit-optimize.github.io/stable/modules/classes.html](https://scikit-optimize.github.io/stable/modules/classes.html)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以在`optimizer_kwargs`参数中传递`acq_func_kwargs`参数。我们可以将所有与获取函数相关的参数传递给这个`acq_func_kwargs`参数；例如，控制BOGP的探索和利用行为的`xi`参数，如[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)中所述。虽然`xi`参数负责控制EI和PI获取函数的探索与利用权衡，但还有一个名为`kappa`的参数，它负责与LCB获取函数相同的任务。`xi`或`kappa`的值越高，意味着我们更倾向于探索而非利用，反之亦然。有关`BayesSearchCV`类中所有可用参数的更多信息，您可以参考`skopt`包的官方API参考（[https://scikit-optimize.github.io/stable/modules/classes.html](https://scikit-optimize.github.io/stable/modules/classes.html))。
- en: 'The following code shows how we can utilize `BayesSearchCV` to perform BOGP
    on the same example as the *Implementing Random Search* section:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们如何利用`BayesSearchCV`在*实现随机搜索*部分相同的示例上执行BOGP：
- en: '[PRE87]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Initiate the `BayesSearchCV` class:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`BayesSearchCV`类：
- en: '[PRE88]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Run the `BayesSearchCV` class:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`BayesSearchCV`类：
- en: '[PRE101]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE102]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型：
- en: '[PRE103]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Based on the preceding code, we get around `0.539` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced’, ‘model__criterion’:
    ‘entropy’, ‘model__min_samples_split’: 0.02363008892366518, ‘model__n_estimators’:
    94}` with an objective function score of `0.530`.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的RF模型时，F1分数大约为`0.539`。最佳超参数集为`{‘model__class_weight’:
    ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.02363008892366518,
    ‘model__n_estimators’: 94}`，目标函数得分为`0.530`。'
- en: In this section, we have learned how to implement BOGP in `skopt` along with
    all of the important parameters available for the `BayesSearchCV` class. It is
    worth noting that `skopt` also has experiment tracking modules that include several
    native supports for plotting the result. We will learn more about those modules
    in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125), *Tracking Hyperparameter
    Tuning Experiments*. In the next section, we will learn how to perform another
    variant of Bayesian Optimization that utilizes RF as its surrogate model with
    `skopt`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在`skopt`中实现BOGP，以及`BayesSearchCV`类中所有重要的参数。值得注意的是，`skopt`还具有实验跟踪模块，包括几个用于绘制结果的本地支持。我们将在[*第13章*](B18753_13_ePub.xhtml#_idTextAnchor125)，*跟踪超参数调整实验*中了解更多关于这些模块的内容。在下一节中，我们将学习如何执行贝叶斯优化的另一种变体，该变体使用RF作为其代理模型，并通过`skopt`实现。
- en: Implementing Bayesian Optimization Random Forest
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化随机森林
- en: '**Bayesian Optimization Random Forest (BORF)** is another variant of Bayesian
    Optimization hyperparameter tuning methods that utilize RF as the surrogate model.
    Note that this variant is different from **Sequential Model Algorithm Configuration**
    (**SMAC**) although both of them utilize RF as the surrogate model (see [*Chapter
    4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring Bayesian Optimization*).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化随机森林（BORF）**是贝叶斯优化超参数调整方法的另一种变体，它使用RF作为代理模型。请注意，虽然这个变体与**顺序模型算法配置（SMAC）**不同，尽管两者都使用RF作为代理模型（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）。'
- en: 'Implementing BORF with `skopt` is actually very similar to implementing BOGP
    as discussed in the previous section. We just need to change the `base_estimator`
    parameter within `optimizer_kwargs` to *RF*. Let’s use the same example as in
    the *Implementing Bayesian Optimization Gaussian Process* section, but change
    the acquisition function from *EI* to *LCB*. Additionally, let’s change the `xi`
    parameter in the `acq_func_kwargs` to *kappa* since we are using *LCB* as our
    acquisition function. Note that we can also still use the same acquisition function.
    The changes made here just to show how you can interact with the interface of
    the `BayesSearchCV` class:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`skopt`实现BORF实际上与上一节中讨论的实现BOGP非常相似。我们只需要在`optimizer_kwargs`中将`base_estimator`参数更改为*RF*。让我们使用与*实现贝叶斯优化高斯过程*节中相同的示例，但将获取函数从*EI*更改为*LCB*。此外，我们将`acq_func_kwargs`中的`xi`参数更改为*kappa*，因为我们使用*LCB*作为获取函数。请注意，我们仍然可以使用相同的获取函数。这里所做的更改只是为了展示如何与`BayesSearchCV`类的接口交互：
- en: '[PRE104]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Initiate the `BayesSearchCV` class:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`BayesSearchCV`类：
- en: '[PRE105]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Run the `BayesSearchCV` class:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`BayesSearchCV`类：
- en: '[PRE118]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE119]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Evaluate the final trained model on the test data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型。
- en: '[PRE120]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Based on the preceding code, we get around `0.617` for the F1-score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.00043534042560206855,
    ‘model__n_estimators’: 85}` with an objective function score of `0.616`.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的RF模型时，F1分数大约为`0.617`。最佳超参数集为`{‘model__class_weight’:
    ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’:
    0.00043534042560206855, ‘model__n_estimators’: 85}`，目标函数得分为`0.616`。'
- en: In this section, we have learned how to implement BORF in `skopt` through the
    `BayesSearchCV` class. In the next section, we will learn how to perform another
    variant of Bayesian Optimization, which utilizes Gradient Boosted Trees as its
    surrogate model with `skopt`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在`skopt`中通过`BayesSearchCV`类实现BORF。在下一节中，我们将学习如何执行贝叶斯优化的另一种变体，该变体使用梯度提升树作为其代理模型，并通过`skopt`实现。
- en: Implementing Bayesian Optimization Gradient Boosted Trees
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化梯度提升树
- en: '`skopt` since we can just pass any other regressors from `sklearn` to be utilized
    as the `base_estimator` parameter. However, *GBRT* is part of the default surrogate
    model with predefined default hyperparameter values from the `skopt` package.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`skopt`允许我们传递任何其他来自`sklearn`的回归器作为`base_estimator`参数。然而，*GBRT*是`skopt`包中默认的代理模型的一部分，具有预定义的默认超参数值。'
- en: 'Similar to the *Implementing Bayesian Optimization Random Forest* section,
    we can just change the `base_estimator` parameter within `optimizer_kwargs` to
    *GBRT*. The following code shows you how to implement BOGBRT in `skopt`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 与*实现贝叶斯优化随机森林*部分类似，我们只需在`optimizer_kwargs`中将`base_estimator`参数更改为*GBRT*。以下代码展示了如何在`skopt`中实现BOGBRT：
- en: '[PRE121]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Initiate the `BayesSearchCV` class:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`BayesSearchCV`类：
- en: '[PRE122]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Run the `BayesSearchCV` class:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`BayesSearchCV`类：
- en: '[PRE135]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Print the best set of hyperparameters:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 打印最佳超参数集：
- en: '[PRE136]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Evaluate the final trained model on the test data:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估最终训练好的模型：
- en: '[PRE137]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Based on the preceding code, we get around `0.611` for the F1-Score when testing
    our final trained RF model with the best set of hyperparameters on the test set.
    The best set of hyperparameters is `{‘model__class_weight’: ‘balanced_subsample’,
    ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.0005745541104096049,
    ‘model__n_estimators’: 143}` with an objective function score of `0.618`.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '根据前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的RF模型时，F1-Score大约为`0.611`。最佳超参数集为`{‘model__class_weight’:
    ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’:
    0.0005745541104096049, ‘model__n_estimators’: 143}`，其目标函数评分为`0.618`。'
- en: In this section, we have learned how to implement BOGBRT in `skopt` through
    the `BayesSearchCV` class by using the same example as in the *Implementing Bayesian
    Optimization Random Forest* section.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过使用与*实现贝叶斯优化随机森林*部分相同的示例，学习了如何在`skopt`中通过`BayesSearchCV`类实现BOGBRT。
- en: Summary
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned all the important things about the `scikit-learn`,
    `scikit-optimize`, and `scikit-hyperband` packages for hyperparameter tuning purposes.
    Additionally, we have learned how to implement various hyperparameter tuning methods
    using the help of those packages, along with understanding each of the important
    parameters of the classes and how are they related to the theory that we have
    learned in the previous chapters. From now on, you should be able to utilize these
    packages to implement your chosen hyperparameter tuning method and, ultimately,
    boost the performance of your ML model. Equipped with the knowledge from *Chapters
    3–6*, you will also be able to understand what’s happening if there are errors
    or unexpected results and how to set up the method configuration to match your
    specific problem.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于`scikit-learn`、`scikit-optimize`和`scikit-hyperband`包在超参数调整方面的所有重要内容。此外，我们还学习了如何使用这些包实现各种超参数调整方法，并理解每个类的重要参数以及它们如何与我们在前几章中学到的理论相关。从现在开始，你应该能够利用这些包来实现你选择的超参数调整方法，并最终提高你的ML模型的性能。凭借第3至6章的知识，你还将能够理解如果出现错误或意外结果时会发生什么，以及如何设置方法配置以匹配你的特定问题。
- en: In the next chapter, we will learn about the Hyperopt package and how to utilize
    it to perform various hyperparameter tuning methods. The goal of the next chapter
    is similar to this chapter, that is, to be able to utilize the package for hyperparameter
    tuning purposes and understand each of the parameters of the implemented classes.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Hyperopt包及其如何用于执行各种超参数调整方法。下一章的目标与本章节类似，即能够利用该包进行超参数调整，并理解实现类中的每个参数。
