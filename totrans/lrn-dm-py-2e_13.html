<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Next Steps...</h1>
            </header>

            <article>
                
<p>During the course, there were lots of avenues not taken, options not presented, and subjects not fully explored. In this appendix, I've created a collection of next steps for those wishing to undertake extra learning and progress their data mining with Python.</p>
<p>This appendix is for learning more about data mining. Also included are some challenges to extend the work performed. Some of these will be small improvements; some will be quite a bit more work—I've made a note of those more tasks that are noticeably more difficult and involved than the others.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting Started with Data Mining</h1>
            </header>

            <article>
                
<p>In this chapter following are a few avenues that reader can explore:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Scikit-learn tutorials</h1>
            </header>

            <article>
                
<p>URL: <a href="http://scikit-learn.org/stable/tutorial/index.html">http://scikit-learn.org/stable/tutorial/index.html</a></p>
<p>Included in the scikit-learn documentation is a series of tutorials on data mining. The tutorials range from basic introductions to toy datasets, all the way through to comprehensive tutorials on techniques used in recent research. The tutorials here will take quite a while to get through—they are very comprehensive—but are well worth the effort to learn.</p>
<p>There are also a large number of algorithms that have been implemented for compatability with scikit-learn. These algorithms are not always included in scikit-learn itself for a number of reasons, but a list of many of these is maintained at <a href="https://github.com/scikit-learn/scikit-learn/wiki/Third-party-projects-and-code-snippets">https://github.com/scikit-learn/scikit-learn/wiki/Third-party-projects-and-code-snippets</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extending the Jupyter Notebook</h1>
            </header>

            <article>
                
<p>URL: <a href="http://ipython.org/ipython-doc/1/interactive/public_server.html">http://ipython.org/ipython-doc/1/interactive/public_server.html</a></p>
<p>The Jupyter Notebook is a powerful tool. It can be extended in many ways, and one of those is to create a server to run your Notebooks, separately from your main computer. This is very useful if you use a low-power main computer, such as a small laptop, but have more powerful computers at your disposal. In addition, you can set up nodes to perform parallelized computations.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More datasets</h1>
            </header>

            <article>
                
<p>URL: <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a></p>
<p>There are many datasets available on the Internet from a number of different sources. These include academic, commercial, and government datasets. A collection of well-labelled datasets is available at the UCI ML library, which is one of the best options to find datasets for testing your algorithms. Try out the OneR algorithm with some of these different datasets.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Other Evaluation Metrics</h1>
            </header>

            <article>
                
<p>There is a wide range of evaluation metrics for other takes. Some notable ones to investigate are:</p>
<ul>
<li>The Lift Metric: <a href="https://en.wikipedia.org/wiki/Lift_(data_mining)">https://en.wikipedia.org/wiki/Lift_(data_mining)</a></li>
<li>Segment evaluation metrics: <a href="http://segeval.readthedocs.io/en/latest/">http://segeval.readthedocs.io/en/latest/</a></li>
<li>Pearson's Correlation Coefficient: <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient</a></li>
<li>Area under the ROC Curve: <a href="http://gim.unmc.edu/dxtests/roc3.htm">http://gim.unmc.edu/dxtests/roc3.htm</a></li>
<li>Normalized Mutual Information: <a href="http://scikit-learn.org/stable/modules/clustering.html#mutual-info-score">http://scikit-learn.org/stable/modules/clustering.html#mutual-info-score</a></li>
</ul>
<p>Each of these metrics was <span>developed</span> with a particular application in mind. For example, the segment evaluation metrics evaluate how accurate breaking a document of text into chunks is, allowing for some variation between chunk boundaries. A good understanding of where evaluation metrics can be applied and where they can not is critical to ongoing success in data mining.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More application ideas</h1>
            </header>

            <article>
                
<p>URL: <a href="https://datapipeline.com.au/">https://datapipeline.com.au/</a></p>
<p>If you are looking for more ideas on data mining applications, specifically those for businesses, check out my company's blog. I post regularly about applications of data mining, focusing on practical outcomes for businesses.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classifying with scikit-learn Estimators</h1>
            </header>

            <article>
                
<p>A naïve implementation of the nearest neighbor algorithm is quite slow—it checks all pairs of points to find those that are close together. Better implementations exist, with some implemented in scikit-learn.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Scalability with the nearest neighbor</h1>
            </header>

            <article>
                
<p>URL: <a href="https://github.com/jnothman/scikit-learn/tree/pr2532">https://github.com/jnothman/scikit-learn/tree/pr2532</a></p>
<p> For instance, a kd-tree can be created that speeds up the algorithm (and this is already included in scikit-learn).</p>
<p>Another way to speed up this search is to use locality-sensitive hashing,  Locality-Sensitive Hashing (LSH). This is a proposed improvement for scikit-learn, and hasn't made it into the package at the time of writing. The preceding link gives a development branch of scikit-learn that will allow you to test out LSH on a dataset. Read through the documentation attached to this branch for details on doing this.</p>
<p>To install it, clone the repository and follow the instructions to install the Bleeding Edge code available at<a href="http://scikit-learn.org/stable/install.html"> http://scikit-learn.org/stable/install.html</a> on your computer. Remember to use the repository's code rather than the official source. I recommend that you use Anaconda for playing around with bleeding-edge packages so that they don't interfere with other libraries on your system.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More complex pipelines</h1>
            </header>

            <article>
                
<p>URL: <a href="http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces">http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces</a></p>
<p>The Pipelines we have used here follow a single stream—the output of one step is the input of another step.</p>
<p>Pipelines follow the transformer and estimator interfaces as well—this allows us to embed Pipelines within Pipelines. This is a useful construct for very complex models, but becomes very powerful when combined with Feature Unions, as shown in the preceding link.This allows us to extract multiple types of features at a time and then combine them to form a single dataset. For more details, see this example: <a href="http://scikit-learn.org/stable/auto_examples/feature_stacker.html">http://scikit-learn.org/stable/auto_examples/feature_stacker.html</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Comparing classifiers</h1>
            </header>

            <article>
                
<p>There are lots of classifiers in scikit-learn that are ready to use. The one you choose for a particular task is going to be based on a variety of factors. You can compare the f1-score to see which method is better, and you can investigate the deviation of those scores to see if that result is statistically significant.</p>
<p>An important factor is that they are trained and tested on the same data—that is, the test set for one classifier is the test set for all classifiers. Our use of random states allows us to ensure that this is the case—an important factor for replicating experiments.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Automated Learning</h1>
            </header>

            <article>
                
<p>URL: <a href="http://rhiever.github.io/tpot/">http://rhiever.github.io/tpot/</a></p>
<p>URL: <a href="https://github.com/automl/auto-sklearn">https://github.com/automl/auto-sklearn</a></p>
<p>It's almost cheating, but these packages will investigate a wide range of possible models for your data mining experiments for you. This removes the need to create a workflow testing a large number of parameters for a larger number of classifier types, and lets you focus on other things, such as feature extract--still critically important and not yet automated!</p>
<p>The general idea is that you extract your features and pass the resulting matrix onto one of these automated classification algorithms (or regression algorithms). It does the search for you and even exports the best model for you. In the case of TPOT, it even gives you Python code to create the model from scratch without having to install TPOT on your server.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Predicting Sports Winners with Decision Trees</h1>
            </header>

            <article>
                
<p>URL: <a href="http://pandas.pydata.org/pandas-docs/stable/tutorials.html">http://pandas.pydata.org/pandas-docs/stable/tutorials.html</a></p>
<p>The pandas library is a great package—anything you normally write to do data loading is probably already implemented in pandas. You can learn more about it from their tutorial.</p>
<p>There is also a great blog post written by Chris Moffitt that overviews common tasks people do in Excel and how to do them in pandas: <a href="http://pbpython.com/excel-pandas-comp.html">http://pbpython.com/excel-pandas-comp.html</a></p>
<p>You can also handle large datasets with pandas; see the answer, from user Jeff, to this StackOverflow question for an extensive overview of the process: <a href="http://stackoverflow.com/a/14268804/307363">http://stackoverflow.com/a/14268804/307363</a>.</p>
<p>Another great tutorial on pandas is written by Brian Connelly: <a href="http://bconnelly.net/2013/10/summarizing-data-in-python-with-pandas/">http://bconnelly.net/2013/10/summarizing-data-in-python-with-pandas/</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More complex features</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www.basketball-reference.com/teams/ORL/2014_roster_status.html">http://www.basketball-reference.com/teams/ORL/2014_roster_status.html</a></p>
<p>Larger exercise!</p>
<p>Sports teams change regularly from game to game. An easy win for a team can turn into a difficult game if a couple of the best players are suddenly injured. You can get the team rosters from basketball-reference as well. For example, the roster for the 2013-2014 season for the Orlando Magic is available at the preceding link. Similar data is available for all NBA teams.</p>
<p>Writing code to integrate how much a team changes and using that to add new features can improve the model significantly. This task will take quite a bit of work though!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dask</h1>
            </header>

            <article>
                
<p>URL: <a href="http://dask.pydata.org/en/latest/">http://dask.pydata.org/en/latest/</a></p>
<p>If you want to take the features of pandas and increase its scalability, then Dask is for you. Dask provides parallelized versions of NumPy arrays, Pandas DataFrames, and task scheduling. Often, the interface is <em>nearly </em>the same as the original NumPy or Pandas versions.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Research</h1>
            </header>

            <article>
                
<p>URL: <a href="https://scholar.google.com.au/">https://scholar.google.com.au/</a></p>
<p>Larger exercise!As you might imagine, there has been a lot of work performed on predicting NBA games, as well as for all sports. Search "&lt;SPORT&gt; prediction" in Google Scholar to find research on predicting your favorite &lt;SPORT&gt;.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recommending Movies Using Affinity Analysis</h1>
            </header>

            <article>
                
<p>There are many recommendation-based datasets that are worth investigating, each with its own issues.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">New datasets</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www2.informatik.uni-freiburg.de/~cziegler/BX/">http://www2.informatik.uni-freiburg.de/~cziegler/BX/</a></p>
<p>Larger exercise!</p>
<p>There are many recommendation-based datasets that are <span>worth</span> investigating, each with its own issues. For example, the Book-Crossing dataset contains more than 278,000 users and over a million ratings. Some of these ratings are explicit (the user did give a rating), while others are more implicit. The weighting to these implicit ratings probably shouldn't be as high as for explicit ratings. The music website www.last.fm has released a great dataset for music recommendation: <a href="http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/.">http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/.</a></p>
<p>There is also a joke recommendation dataset! See here: <a href="http://eigentaste.berkeley.edu/dataset/.">http://eigentaste.berkeley.edu/dataset/.</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Eclat algorithm</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www.borgelt.net/eclat.html">http://www.borgelt.net/eclat.html</a></p>
<p>The APriori algorithm implemented here is easily the most famous of the association rule mining graphs, but isn't necessarily the best. Eclat is a more modern algorithm that can be implemented relatively easily.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Collaborative Filtering</h1>
            </header>

            <article>
                
<p>URL: <a href="https://github.com/python-recsys">https://github.com/python-recsys</a></p>
<p>For those wanting to got much further with recommendation engines, it is necessary to investigate other formats for recommendations, such as collaborative filtering. This library provides some background into the algorithms and implementations, along with some tutorials. There is also a good overview at <a href="http://blogs.gartner.com/martin-kihn/how-to-build-a-recommender-system-in-python/">http://blogs.gartner.com/martin-kihn/how-to-build-a-recommender-system-in-python/</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting Features with Transformers</h1>
            </header>

            <article>
                
<p>Following topics, according to me, are also relevant when it comes to deeper understanding of Extracting Features with Transformers</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Adding noise</h1>
            </header>

            <article>
                
<p>We covered removing noise to improve features; however, improved performance can be obtained for some datasets by adding noise. The reason for this is simple—it helps stop overfitting by forcing the classifier to generalize its rules a little (although too much noise will make the model too general). Try implementing a Transformer that can add a given amount of noise to a dataset. Test that out on some of the datasets from UCI ML and see if it improves test-set performance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Vowpal Wabbit</h1>
            </header>

            <article>
                
<p>URL: <a href="http://hunch.net/~vw/">http://hunch.net/~vw/</a></p>
<p>Vowpal Wabbit is a great project, providing very fast feature extraction for text-based problems. It comes with a Python wrapper, allowing you to call it from with Python code. Test it out on large datasets.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">word2vec</h1>
            </header>

            <article>
                
<p>URL: <a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a></p>
<p>Word embeddings are receiving a lot of interest from research and industry, for a good reason: they perform very well on many text mining tasks. They are a big more complicated than the bag-of-words model and create larger models. Word embeddings are great features when you have lots of data and can even help in some cases with smaller amounts.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Social Media Insight Using Naive Bayes</h1>
            </header>

            <article>
                
<p>Do consider the following points after finishing with Social Media Insight Using Native Bayes.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Spam detection</h1>
            </header>

            <article>
                
<p>URL: <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a></p>
<p>Using the concepts here, you can create a spam detection method that is able to view a social media post and determine whether it is spam or not. Try this out by first creating a dataset of spam/not-spam posts, implementing the text mining algorithms, and then evaluating them.</p>
<p>One important consideration with spam detection is the false-positive/false-negative ratio. Many people would prefer to have a couple of spam messages slip through, rather than miss out on a legitimate message because the filter was too aggressive in stopping the spam. In order to turn your method for this, you can use a Grid Search with the f1-score as the evaluation criteria. See the preceding link for information on how to do this.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Natural language processing and part-of-speech tagging</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></p>
<p>The techniques we used here are quite lightweight compared to some of the linguistic models employed in other areas. For example, part-of-speech tagging can help disambiguate word forms, allowing for higher accuracy. It comes with NLTK.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Discovering Accounts to Follow Using Graph Mining</h1>
            </header>

            <article>
                
<p>Do give the following a read when done with the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More complex algorithms</h1>
            </header>

            <article>
                
<p>URL: <a href="https://www.cs.cornell.edu/home/kleinber/link-pred.pdf">https://www.cs.cornell.edu/home/kleinber/link-pred.pdf</a>Larger exercise!</p>
<p>There has been extensive research on predicting links in graphs, including for social networks. For instance, David Liben-Nowell and Jon Kleinberg published a paper on this topic that would serve as a great place for more complex algorithms, linked previously.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">NetworkX</h1>
            </header>

            <article>
                
<p>URL: <a href="https://networkx.github.io/">https://networkx.github.io/</a></p>
<p>If you are going to be using graphs and networks more, going in-depth into the NetworkX package is well worth your time—the visualization options are great and the algorithms are well implemented. Another library called SNAP is also available with Python bindings, at <a href="http://snap.stanford.edu/snappy/index.html">http://snap.stanford.edu/snappy/index.html</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Beating CAPTCHAs with Neural Networks</h1>
            </header>

            <article>
                
<p>You may find the following topics interesting as well:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Better (worse?) CAPTCHAs</h1>
            </header>

            <article>
                
<p>URL: <a href="http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html">http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html</a></p>
<p>Larger exercise!</p>
<p>The CAPTCHAs we beat in this example were not as complex as those normally used today. You can create more complex variants using a number of techniques as follows:</p>
<ul>
<li>Applying different transformations such as the ones in scikit-image (see the preceding link)</li>
<li>Using different colors and colors that don't translate well to grayscale</li>
<li>Adding lines or other shapes to the image: <a href="http://scikit-image.org/docs/dev/api/skimage.draw.html">http://scikit-image.org/docs/dev/api/skimage.draw.html</a></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deeper networks</h1>
            </header>

            <article>
                
<p>These techniques will probably fool our current implementation, so improvements will need to be made to make the method better. Try some of the deeper networks we used. Larger networks need more data, though, so you will probably need to generate more than the few thousand samples we did here in order to get good performance. Generating these datasets is a good candidate for parallelization—lots of small tasks that can be performed independently.</p>
<p>A good idea for increasing your dataset size, which applies to other datasets as well, is to create variants of existing images. Flip images upside down, crop them weirdly, add noise, blur the image, make some random pixels black and so on.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reinforcement learning</h1>
            </header>

            <article>
                
<p>URL: <a href="http://pybrain.org/docs/tutorial/reinforcement-learning.html">http://pybrain.org/docs/tutorial/reinforcement-learning.html</a></p>
<p>Reinforcement learning is gaining traction as the next big thing in data mining—although it has been around a long time! PyBrain has some reinforcement learning algorithms that are worth checking out with this dataset (and others!).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Authorship Attribution</h1>
            </header>

            <article>
                
<p>When it comes to Authorship Attribution do give the following topics a read.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Increasing the sample size</h1>
            </header>

            <article>
                
<p>The Enron application we used ended up using just a portion of the overall dataset. There is lots more data available in this dataset. Increasing the number of authors will likely lead to a drop in accuracy, but it is possible to boost the accuracy further than was achieved here, using similar methods. Using a Grid Search, try different values for n-grams and different parameters for support vector machines, in order to get better performance on a larger number of authors.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Blogs dataset</h1>
            </header>

            <article>
                
<p>The dataset used, provides authorship-based classes (each blogger ID is a separate author). This dataset can be tested using this kind of method as well. In addition, there are the other classes of gender, age, industry, and star sign that can be tested—are authorship-based methods good for these classification tasks?</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Local n-grams</h1>
            </header>

            <article>
                
<p>URL: <a href="https://github.com/robertlayton/authorship_tutorials/blob/master/LNGTutorial.ipynb">https://github.com/robertlayton/authorship_tutorials/blob/master/LNGTutorial.ipynb</a></p>
<p>Another form of classifier is local n-gram, which involves choosing the best features per-author, not globally for the entire dataset. I wrote a tutorial on using local n-grams for authorship attribution, available at the preceding link.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Clustering News Articles</h1>
            </header>

            <article>
                
<p>It won't hurt to read a little on the following topics</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Clustering Evaluation</h1>
            </header>

            <article>
                
<p>The evaluation of clustering algorithms is a difficult problem—on the one hand, we can sort of tell what good clusters look like; on the other hand, if we really know that, we should label some instances and use a supervised classifier! Much has been written on this topic. One slideshow on the topic that is a good introduction to the challenges follows: <a href="http://www.cs.kent.edu/~jin/DM08/ClusterValidation.pdf">http://www.cs.kent.edu/~jin/DM08/ClusterValidation.pdf</a>.</p>
<p>In addition, a very comprehensive (although now a little dated) paper on this topic is here: <a href="http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf.">http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf.</a></p>
<p>The scikit-learn package does implement a number of the metrics described in those links, with an overview here: <a href="http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation">http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation</a>.</p>
<p>Using some of these, you can start evaluating which parameters need to be used for better clusterings. Using a Grid Search, we can find parameters that maximize a metric—just like in classification.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Temporal analysis</h1>
            </header>

            <article>
                
<p>Larger exercise!</p>
<p>The code we developed here can be rerun over many months. By adding some tags to each cluster, you can track which topics stay active over time, getting a longitudinal viewpoint of what is being discussed in the world news. To compare the clusters, consider a metric such as the adjusted mutual information score, which was linked to the scikit-learn documentation earlier. See how the clusters change after one month, two months, six months, and a year.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Real-time clusterings</h1>
            </header>

            <article>
                
<p>The k-means algorithm can be iteratively trained and updated over time, rather than discrete analyses at given time frames. Cluster movement can be tracked in a number of ways—for instance, you can track which words are popular in each cluster and how much the centroids move per day. Keep the API limits in mind—you probably only need to do one check every few hours to keep your algorithm up-to-date.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classifying Objects in Images Using Deep Learning</h1>
            </header>

            <article>
                
<p>The following topics are also important when deeper study into Classifying objects is considered.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Mahotas</h1>
            </header>

            <article>
                
<p>URL: <a href="http://luispedro.org/software/mahotas/">http://luispedro.org/software/mahotas/</a></p>
<p>Another package for image processing is Mahotas, including better and more complex image processing techniques that can help achieve better accuracy, although they may come at a high computational cost. However, many image processing tasks are good candidates for parallelization. More techniques on image classification can be found in the research literature, with this survey paper as a good start: <a href="http://ijarcce.com/upload/january/22-A%20Survey%20on%20Image%20Classification.pdf">http://ijarcce.com/upload/january/22-A%20Survey%20on%20Image%20Classification.pdf</a>.</p>
<p>Other image datasets are available at <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html</a>.</p>
<p>There are many datasets of images available from a number of academic and industry-based sources. The linked website lists a bunch of datasets and some of the best algorithms to use on them. Implementing some of the better algorithms will require significant amounts of custom code, but the payoff can be well worth the pain.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Magenta</h1>
            </header>

            <article>
                
<p>URL: <a href="https://github.com/tensorflow/magenta/tree/master/magenta/reviews">https://github.com/tensorflow/magenta/tree/master/magenta/reviews</a></p>
<p>This repository contains a few high-quality deep learning papers that are worth reading, along with in-depth reviews of the paper and their techniques. If you want to go deep into deep learning, check out these papers first before expanding outwards.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Working with Big Data</h1>
            </header>

            <article>
                
<p>The following resources on Big Data would be helpful</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Courses on Hadoop</h1>
            </header>

            <article>
                
<p>Both Yahoo and Google have great tutorials on Hadoop, which go from beginner to quite advanced levels. They don't specifically address using Python, but learning the Hadoop concepts and then applying them in Pydoop or a similar library can yield great results.</p>
<p>Yahoo's tutorial: <a href="https://developer.yahoo.com/hadoop/tutorial/">https://developer.yahoo.com/hadoop/tutorial/</a></p>
<p>Google's tutorial: <a href="https://cloud.google.com/hadoop/what-is-hadoop">https://cloud.google.com/hadoop/what-is-hadoop</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Pydoop</h1>
            </header>

            <article>
                
<p>URL: <a href="http://crs4.github.io/pydoop/tutorial/index.html">http://crs4.github.io/pydoop/tutorial/index.html</a></p>
<p>Pydoop is a python library to run Hadoop jobs. Pydoop also works with HDFS, the Hadoop File System, although you can get that functionality in mrjob as well. Pydoop will give you a bit more control over running some jobs.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recommendation engine</h1>
            </header>

            <article>
                
<p>Building a large recommendation engine is a good test of your Big data skills. A great blog post by Mark Litwintschik covers an engine using Apache Spark, a big data technology: <a href="http://tech.marksblogg.com/recommendation-engine-spark-python.html">http://tech.marksblogg.com/recommendation-engine-spark-python.html</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">W.I.L.L</h1>
            </header>

            <article>
                
<p>URL: <a href="https://github.com/ironman5366/W.I.L.L">https://github.com/ironman5366/W.I.L.L<br/></a></p>
<p>Very large project!</p>
<p>This open source personal assistant can be your next JARVIS from Iron Man. You can add to this project using data mining techniques to allow it to learn to do some tasks that you need to do regularly. This is not easy, but the potential productivity gains are worth it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">More resources</h1>
            </header>

            <article>
                
<p>The following would serve as a really good resource for additional information:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Kaggle competitions</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www.kaggle.com/">www.kaggle.com/</a></p>
<p>Kaggle runs data mining competitions regularly, often with monetary prizes.<br/>
Testing your skills on Kaggle competitions is a fast and great way to learn to work with real-world data mining problems. The forums are nice and share environments—often, you will see code released for a top-10 entry during the competition!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Coursera</h1>
            </header>

            <article>
                
<p>URL: <a href="http://www.coursera.org">www.coursera.org</a></p>
<p>Coursera contains many <span>courses</span> on data <span>mining</span> and data science. Many of the courses are specialized, such as big data and image processing. A great general one to start with is Andrew Ng's famous course: <a href="https://www.coursera.org/learn/machine-learning/">https://www.coursera.org/learn/machine-learning/</a>.<br/>
It is a bit more advanced than this and would be a great next step for interested readers.<br/>
For neural networks, check out this course: <a href="https://www.coursera.org/course/neuralnets">https://www.coursera.org/course/neuralnets</a>.<br/>
If you complete all of these, try out the course on probabilistic graphical models at <a href="https://www.coursera.org/course/pgm">https://www.coursera.org/course/pgm</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>