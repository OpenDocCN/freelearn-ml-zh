- en: '*Chapter 10*: Advanced Hyperparameter Tuning with DEAP and Microsoft NNI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：使用DEAP和Microsoft NNI进行高级超参数调整'
- en: '**DEAP** and **Microsoft NNI** are Python packages that provide various hyperparameter
    tuning methods that are not implemented in other packages that we have discussed
    in *Chapters 7 – 9*. For example, Genetic Algorithm, Particle Swarm Optimization,
    Metis, Population-Based Training, and many more.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**DEAP**和**Microsoft NNI**是Python包，提供了其他包中未实现的多种超参数调整方法，这些包我们在第7-9章中讨论过。例如，遗传算法、粒子群优化、Metis、基于群体的训练以及更多。'
- en: In this chapter, we’ll learn how to perform hyperparameter tuning using both
    DEAP and Microsoft NNI packages, starting from getting ourselves familiar with
    the packages, along with the important modules and parameters we need to be aware
    of. We’ll learn not only how to utilize both DEAP and Microsoft NNI to perform
    hyperparameter tuning with their default configurations but also discuss other
    available configurations along with their usage. Moreover, we’ll also discuss
    how the implementation of the hyperparameter tuning methods is related to the
    theory that we have learned in previous chapters, since there may be some minor
    differences or adjustments made in the implementation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用DEAP和Microsoft NNI包进行超参数调整，从熟悉这些包以及我们需要注意的重要模块和参数开始。我们将学习如何利用DEAP和Microsoft
    NNI的默认配置进行超参数调整，并讨论其他可用的配置及其使用方法。此外，我们还将讨论超参数调整方法的实现如何与我们之前章节中学到的理论相关联，因为实现中可能会有一些细微的差异或调整。
- en: By the end of this chapter, you will be able to understand all of the important
    things you need to know about DEAP and Microsoft NNI and be able to implement
    various hyperparameter tuning methods available in these packages. You’ll also
    be able to understand each of the important parameters of the classes and how
    they are related to the theory that we have learned in the previous chapters.
    Finally, equipped with the knowledge from previous chapters, you will also be
    able to understand what’s happening if there are errors or unexpected results
    and understand how to set up the method configuration to match your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够理解关于DEAP和Microsoft NNI你需要知道的所有重要事项，并能够实现这些包中可用的各种超参数调整方法。你还将能够理解每个类的重要参数以及它们与我们之前章节中学到的理论之间的关系。最后，凭借前几章的知识，你还将能够理解如果出现错误或意外结果时会发生什么，并了解如何设置方法配置以匹配你的特定问题。
- en: 'The following are the main topics that will be discussed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主要主题：
- en: Introducing DEAP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍DEAP
- en: Implementing the Genetic Algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现遗传算法
- en: Implementing Particle Swarm Optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现粒子群优化
- en: Introducing Microsoft NNI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Microsoft NNI
- en: Implementing Grid Search
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现网格搜索
- en: Implementing Random Search
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现随机搜索
- en: Implementing Tree-structured Parzen Estimators
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现树结构Parzen估计器
- en: Implementing Sequential Model Algorithm Configuration
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现序列模型算法配置
- en: Implementing Bayesian Optimization Gaussian Process
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化高斯过程
- en: Implementing Metis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Metis
- en: Implementing Simulated Annealing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: Implementing Hyper Band
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Hyper Band
- en: Implementing Bayesian Optimization Hyper Band
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化Hyper Band
- en: Implementing Population-Based Training
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于群体的训练
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will learn how to implement various hyperparameter tuning methods with DEAP
    and Microsoft NNI. To ensure that you are able to reproduce the code examples
    in this chapter, you will require the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用DEAP和Microsoft NNI实现各种超参数调整方法。为了确保你能够复制本章中的代码示例，你需要以下条件：
- en: Python 3 (version 3.7 or above)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本3.7或以上）
- en: Installed `pandas` package (version 1.3.4 or above)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`pandas`包（版本1.3.4或以上）
- en: Installed `NumPy` package (version 1.21.2 or above)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`NumPy`包（版本1.21.2或以上）
- en: Installed `SciPy` package (version 1.7.3 or above)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`SciPy`包（版本1.7.3或以上）
- en: Installed `Matplotlib` package (version 3.5.0 or above)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`Matplotlib`包（版本3.5.0或以上）
- en: Installed `scikit-learn` package (version 1.0.1 or above)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`scikit-learn`包（版本1.0.1或以上）
- en: Installed `DEAP` package (version 1.3)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`DEAP`包（版本1.3）
- en: Installed `Hyperopt` package (version 0.1.2)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`Hyperopt`包（版本0.1.2）
- en: Installed `NNI` package (version 2.7)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`NNI`包（版本2.7）
- en: Installed `PyTorch` package (version 1.10.0)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装`PyTorch`包（版本1.10.0）
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb)。
- en: Introducing DEAP
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DEAP
- en: '`pip install deap` command.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`pip install deap`命令。
- en: 'DEAP allows you to craft your evolutionary algorithm optimization steps in
    a very flexible manner. The following steps show how to utilize DEAP to perform
    any hyperparameter tuning methods. More detailed steps, including the code implementation,
    will be given through various examples in the upcoming sections:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP允许你以非常灵活的方式构建进化算法的优化步骤。以下步骤展示了如何利用DEAP执行任何超参数调整方法。更详细的步骤，包括代码实现，将在接下来的章节中通过各种示例给出：
- en: Define the *type* classes through the `creator.create()` module. These classes
    are responsible for defining the type of objects that will be used in the optimization
    steps.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`creator.create()`模块定义*类型*类。这些类负责定义在优化步骤中将使用的对象类型。
- en: Define the *initializers* along with the hyperparameter space and register them
    in the `base.Toolbox()` container. The initializers are responsible for setting
    the initial value of the objects that will be used in the optimization steps.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义*初始化器*以及超参数空间，并在`base.Toolbox()`容器中注册它们。初始化器负责设置在优化步骤中将使用的对象的初始值。
- en: Define the *operators* and register them in the `base.Toolbox()` container.
    The operators refer to the evolutionary tools or **genetic operator** (see [*Chapter
    5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that need to be defined as part of
    the optimization algorithm. For example, the selection, crossover, and mutation
    operators in the Genetic Algorithm.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义*算子*并将它们注册在`base.Toolbox()`容器中。算子指的是作为优化算法一部分需要定义的进化工具或**遗传算子**（见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)）。例如，遗传算法中的选择、交叉和变异算子。
- en: Define the objective function and register it in the `base.Toolbox()` container.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数并将其注册在`base.Toolbox()`容器中。
- en: Define your own hyperparameter tuning algorithm function.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你自己的超参数调整算法函数。
- en: Perform hyperparameter tuning by calling the defined function in *step 5*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用定义在*步骤5*中的函数来执行超参数调整。
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: Test the final trained model on the test data.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。
- en: 'The type classes refer to the type of objects used in the optimization steps.
    These type classes are inherited from the base classes implemented in DEAP. For
    example, we can define the type of our fitness function as the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类型类指的是在优化步骤中使用的对象类型。这些类型类是从DEAP中实现的基础类继承而来的。例如，我们可以定义我们的适应度函数类型如下：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `base.Fitness` class is a base abstract class implemented in DEAP that can
    be utilized to define our own fitness function type. It expects a `weights` parameter
    to understand the type of optimization problem we are working on. If it’s a maximization
    problem, then we have to put a positive weight and the other way around for a
    minimization problem. Notice that it expects a tuple data structure instead of
    a float. This is because DEAP also allows us to work with a `(1.0, -1.0)` to the
    `weights` parameter, it means we have two objective functions where we want to
    maximize the first one and minimize the second one with equal weight.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`base.Fitness`类是DEAP中实现的一个基础抽象类，可以用来定义我们自己的适应度函数类型。它期望一个`weights`参数来理解我们正在处理的优化问题的类型。如果是最大化问题，那么我们必须放置一个正权重，反之亦然，对于最小化问题。请注意，它期望一个元组数据结构而不是浮点数。这是因为DEAP还允许我们将`(1.0,
    -1.0)`作为`weights`参数，这意味着我们有两个目标函数，我们希望第一个最大化，第二个最小化，权重相等。'
- en: 'The `creator.create()` function is responsible for creating a new class based
    on the base class. In the preceding code, we created the type class for our objective
    function with the name “`FitnessMax`”. This `creator.create()` function expects
    at least two parameters: specifically, the name of the newly created class and
    the base class itself. The rest of the parameters passed to this function will
    be treated as the attributes for this newly created class. Besides defining the
    type of the objective function, we can also define the type of individuals in
    the evolutionary algorithm that will be performed. The following code shows how
    to create the type of individuals inherited from the built-in `list` data structure
    in Python that has `fitness` as its attribute:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`creator.create()`函数负责基于基类创建一个新的类。在前面的代码中，我们使用名称“`FitnessMax`”创建了目标函数的类型类。此`creator.create()`函数至少需要两个参数：具体来说，是新创建的类的名称和基类本身。传递给此函数的其他参数将被视为新创建类的属性。除了定义目标函数的类型外，我们还可以定义将要执行的进化算法中个体的类型。以下代码展示了如何创建从Python内置的`list`数据结构继承的个体类型，该类型具有`fitness`属性：'
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the `fitness` attribute has a type of `creator.FitnessMax`, which
    is the type that we just created in the preceding code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`fitness`属性的类型为`creator.FitnessMax`，这是我们之前代码中刚刚创建的类型。
- en: Types Definition in DEAP
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP中的类型定义
- en: There are a lot of ways to define type classes in DEAP. While we have discussed
    the most straightforward and, arguably, most used type class, you may find other
    cases that need other definitions of type class. For more information on how to
    define other types in DEAP, please refer to the official documentation ([https://deap.readthedocs.io/en/master/tutorials/basic/part1.html](https://deap.readthedocs.io/en/master/tutorials/basic/part1.html)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在DEAP中有许多定义类型类的方法。虽然我们已经讨论了最直接且可以说是最常用的类型类，但你可能会遇到需要其他类型类定义的情况。有关如何在DEAP中定义其他类型的更多信息，请参阅官方文档（[https://deap.readthedocs.io/en/master/tutorials/basic/part1.html](https://deap.readthedocs.io/en/master/tutorials/basic/part1.html)）。
- en: 'Once we have finished defining the type of objects that will be used in the
    optimization steps, we now need to initiate the value of those objects using the
    initializers and register them in the `base.Toolbox()` container. You can think
    of this module as a box or container of initializers and other tools that will
    be utilized during the optimization steps. The following code shows how we can
    set the random initial values for individuals:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了将在优化步骤中使用的对象类型的定义，我们现在需要使用初始化器初始化这些对象的值，并在`base.Toolbox()`容器中注册它们。你可以将此模块视为一个盒子或容器，其中包含初始化器和将在优化步骤中使用的其他工具。以下代码展示了我们如何为个体设置随机的初始值：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code shows an example of how to register the `"individual"` object
    in the `base.Toolbox()` container, where each individual has a size of `10`. The
    individual is generated by repeatedly calling the `random.random` method 10 times.
    Note that, in the hyperparameter tuning setup, the size of `10` of each individual
    actually refers to the number of the hyperparameters we have in the space. The
    following shows the output of calling the registered individual via the `toolbox.individual()`
    method:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码展示了如何在`base.Toolbox()`容器中注册`"individual"`对象，其中每个个体的尺寸为`10`。该个体是通过重复调用`random.random`方法10次生成的。请注意，在超参数调整设置中，每个个体的`10`尺寸实际上指的是我们在空间中拥有的超参数数量。以下展示了通过`toolbox.individual()`方法调用已注册个体的输出：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the output of `toolbox.individual()` is just a list of 10 random
    values since we’ve defined `creator.Individual` to inherit from the built-in `list`
    data structure in Python. Furthermore, we also called `tools.initRepeat` when
    registering the individual with the `random.random` method by 10 times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`toolbox.individual()`的输出只是一个包含10个随机值的列表，因为我们已经定义`creator.Individual`从Python内置的`list`数据结构继承。此外，我们在注册个体时也调用了`tools.initRepeat`，通过`random.random`方法重复10次。
- en: You may now wonder, how do you define the actual hyperparameter space using
    this `toolbox.register()` method? Initiating a bunch of random values definitely
    doesn’t make any sense. We need to know the way to define the hyperparameter space
    that will be equipped for each individual. To do that, we can actually utilize
    another tool provided by DEAP, `tools.InitCycle`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能想知道，如何使用这个`toolbox.register()`方法定义实际的超参数空间？启动一串随机值显然没有意义。我们需要知道如何定义将为每个个体配备的超参数空间。为此，我们实际上可以利用DEAP提供的另一个工具，即`tools.InitCycle`。
- en: 'Where `tools.initRepeat` will just call the provided function `n` times, in
    our previous example, the provided function is `random.random`. Here, `tools.InitCycle`
    expects a list of functions and will call those functions for `n` cycles. The
    following code shows an example to define the hyperparameter space that will be
    equipped for each individual:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`tools.initRepeat`将只调用提供的函数`n`次，在我们之前的例子中，提供的函数是`random.random`。在这里，`tools.InitCycle`期望一个函数列表，并将这些函数调用`n`次。以下代码展示了如何定义将为每个个体配备的超参数空间的一个示例：
- en: 'We need to first register each of the hyperparameters that we have in the space
    along with their distribution. Note that we can pass all of the required parameters
    to the sampling distribution function to `toolbox.register()` as well. For example,
    here, we pass the `a=0,b=0.5,loc=0.005,scale=0.01` parameters of the `truncnorm.rvs()`
    method:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要首先注册空间中我们拥有的每个超参数及其分布。请注意，我们也可以将所有必需的参数传递给采样分布函数的`toolbox.register()`。例如，在这里，我们传递了`truncnorm.rvs()`方法的`a=0,b=0.5,loc=0.005,scale=0.01`参数：
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we have registered each hyperparameter we have, we can register the individual
    by utilizing `tools.initCycle` with only one cycle of repetition:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们注册了所有现有的超参数，我们可以通过使用`tools.initCycle`并只进行一次重复循环来注册个体：
- en: '[PRE10]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following shows the output of calling the registered individual via the
    `toolbox.individual()` method:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了通过`toolbox.individual()`方法调用已注册个体的输出：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have registered the individual in our toolbox, registering a population
    is very simple. We just need to utilize the `tools.initRepeat` module and pass
    the defined `toolbox.individual` as the argument. The following code shows how
    to register a population in general. Note that, here, the population is just a
    list of five individuals defined previously:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们在工具箱中注册了个体，注册一个种群就非常简单。我们只需要利用`tools.initRepeat`模块并将定义的`toolbox.individual`作为参数传递。以下代码展示了如何一般性地注册一个种群。请注意，在这里，种群只是之前定义的五个个体的列表：
- en: '[PRE12]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following shows the output when calling the `toolbox.population()` method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了调用`toolbox.population()`方法时的输出：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As mentioned previously, the `base.Toolbox()` container is responsible for
    storing not only initializers but also other tools that will be utilized during
    the optimization steps. Another important building block for an evolutionary algorithm,
    such as the GA, is the genetic operator. Fortunately, DEAP already implemented
    various genetic operators that we can utilize via the `tools` module. The following
    code shows an example of how to register the selection, crossover, and mutation
    operators for the GA (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`base.Toolbox()`容器不仅负责存储初始化器，还负责存储在优化步骤中将使用的其他工具。进化算法（如GA）的另一个重要构建块是遗传算子。幸运的是，DEAP已经实现了我们可以通过`tools`模块利用的各种遗传算子。以下代码展示了如何为GA注册选择、交叉和变异算子的示例（参见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047))：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `tools.selTournament` selection strategy works by selecting the best individuals
    among `tournsize` randomly chosen individuals, *NPOP* times, where `tournsize`
    is the number of individuals participating in the tournament and *NPOP* is the
    number of individuals in the population. The `tools.cxBlend` crossover strategy
    works by performing a linear combination between two continuous individual genes,
    where the weight for the linear combination is governed by the `alpha` hyperparameter.
    The `tools.mutPolynomialBounded` mutation strategy works by passing continuous
    individual genes to a pre-defined polynomial mapping.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`tools.selTournament`选择策略通过在随机选择的`tournsize`个个体中选出最佳个体，重复`NPOP`次来实现，其中`tournsize`是参加锦标赛的个体数量，而`NPOP`是种群中的个体数量。`tools.cxBlend`交叉策略通过执行两个连续个体基因的线性组合来实现，其中线性组合的权重由`alpha`超参数控制。`tools.mutPolynomialBounded`变异策略通过将连续个体基因传递给一个预定义的多项式映射来实现。'
- en: Evolutionary Tools in DEAP
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP中的进化工具
- en: There are various built-in evolutionary tools implemented in DEAP that we can
    utilize for our own needs, starting from initializers, crossover, mutation, selection,
    and migration tools. For more information regarding the implemented tools, please
    refer to the official documentation ([https://deap.readthedocs.io/en/master/api/tools.html](https://deap.readthedocs.io/en/master/api/tools.html)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP中实现了各种内置的进化工具，我们可以根据自己的需求使用，包括初始化器、交叉、变异、选择和迁移工具。有关实现工具的更多信息，请参阅官方文档([https://deap.readthedocs.io/en/master/api/tools.html](https://deap.readthedocs.io/en/master/api/tools.html))。
- en: 'To register the pre-defined objective function to the toolbox, we can just
    simply call the same `toolbox.register()` method and pass the objective function,
    as the following code shows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要将预定义的目标函数注册到工具箱中，我们只需调用相同的`toolbox.register()`方法并传递目标函数，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, `obj_func` is a Python function that expects to receive the `individual`
    object defined previously. We will see how to create such an objective function
    and how to define our own hyperparameter tuning algorithm function in the upcoming
    sections when we discuss how to implement the GA and PSO in DEAP.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`obj_func`是一个Python函数，它期望接收之前定义的`individual`对象。我们将在接下来的章节中看到如何创建这样的目标函数，以及如何定义我们自己的超参数调整算法函数，当我们讨论如何在DEAP中实现GA和PSO时。
- en: 'DEAP also allows us to utilize our parallel computing resources when calling
    the objective function. To do that, we can simply need to register the `multiprocessing`
    module to the toolbox as the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: DEAP还允许我们在调用目标函数时利用我们的并行计算资源。为此，我们只需将`multiprocessing`模块注册到工具箱中，如下所示：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have registered the `multiprocessing` module, we can simply apply this
    when calling the objective function, as shown in the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们注册了`multiprocessing`模块，我们就可以在调用目标函数时简单地应用它，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this section, we have discussed the DEAP package and its building blocks.
    You may wonder how to construct a real hyperparameter tuning method using all
    of the building blocks provided by DEAP. Worry no more; in the upcoming two sections,
    we will learn how to utilize all of the discussed building blocks to perform hyperparameter
    tuning with the GA and PSO methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了DEAP包及其构建块。你可能想知道如何使用DEAP提供的所有构建块构建一个真实的超参数调整方法。不用担心；在接下来的两个章节中，我们将学习如何利用所有讨论的构建块使用GA和PSO方法进行超参数调整。
- en: Implementing the Genetic Algorithm
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现遗传算法
- en: GA is one of the variants of the Heuristic Search hyperparameter tuning group
    (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that can be implemented
    by the DEAP package. To show you how we can implement GA with the DEAP package,
    let’s use the Random Forest classifier model and the same data as in the examples
    in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062). The dataset used in this
    example is the *Banking Dataset – Marketing Targets* dataset provided on Kaggle
    ([https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GA是启发式搜索超参数调整组（见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)）的变体之一，可以通过DEAP包实现。为了展示我们如何使用DEAP包实现GA，让我们使用随机森林分类器模型和与[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中示例相同的数据。本例中使用的数据库是Kaggle上提供的*Banking
    Dataset – Marketing Targets*数据库([https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets))。
- en: The target variable consists of two classes, `yes` or `no`, indicating whether
    the client of the bank has subscribed to a term deposit or not. Hence, the goal
    of training an ML model on this dataset is to identify whether a customer is potentially
    wanting to subscribe to the term deposit or not. Out of the 16 features provided
    in the data, there are seven numerical features and nine categorical features.
    As for the target class distribution, 12% of them are *yes* and 88% of them are
    *no*, for both train and test datasets. For more detailed information about the
    data, please refer to [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量由两个类别组成，`yes`或`no`，表示银行客户是否已订阅定期存款。因此，在这个数据集上训练机器学习模型的目的是确定客户是否可能想要订阅定期存款。在数据中提供的16个特征中，有7个数值特征和9个分类特征。至于目标类分布，训练和测试数据集中都有12%是`yes`，88%是`no`。有关数据的更详细信息，请参阅[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)。
- en: Before performing the GA, let’s see how the Random Forest classifier with default
    hyperparameters values works. As shown in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062),
    we get around `0.436` in the F1-score when evaluating the Random Forest classifier
    with default hyperparameter values on the test set. Note that we’re still using
    the same scikit-learn pipeline definition to train and evaluate the Random Forest
    classifier, as explained in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 GA 之前，让我们看看具有默认超参数值的随机森林分类器是如何工作的。如 [*第 7 章*](B18753_07_ePub.xhtml#_idTextAnchor062)
    所示，我们在测试集上评估具有默认超参数值的随机森林分类器时，F1 分数大约为 `0.436`。请注意，我们仍在使用如 [*第 7 章*](B18753_07_ePub.xhtml#_idTextAnchor062)
    中解释的相同的 scikit-learn 管道定义来训练和评估随机森林分类器。
- en: 'The following code shows how to implement the GA with the DEAP package. You
    can find the more detailed code in the GitHub repository mentioned in the *Technical
    requirements* section:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用 DEAP 包实现 GA。您可以在 *技术要求* 部分提到的 GitHub 仓库中找到更详细的代码：
- en: 'Define the GA parameters and type classes through the `creator.create()` module:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `creator.create()` 模块定义 GA 参数和类型类：
- en: '[PRE25]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Fix the seed for reproducibility:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 设置随机种子以实现可重复性：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the type of our fitness function. Here, we are working with a maximization
    problem and a single objective function, that’s why we set `weights=(1.0,)`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的适应度函数类型。在这里，我们正在处理一个最大化问题和一个单一目标函数，因此我们设置 `weights=(1.0,)`：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the type of individuals inherited from the built-in `list` data structure
    in Python that has `fitness` as its attribute:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 定义从 Python 内置 `list` 数据结构继承的个体类型，该类型具有 `fitness` 作为其属性：
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Define the initializers along with the hyperparameter space and register them
    in the `base.Toolbox()` container.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义初始化器以及超参数空间并将它们注册在 `base.Toolbox()` 容器中。
- en: 'Initialize the toolbox:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化工具箱：
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the naming of the hyperparameters:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义超参数的命名：
- en: '[PRE30]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Register each of the hyperparameters that we have in the space along with their
    distribution:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注册空间中的每个超参数及其分布：
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Register the individual by utilizing `tools.initCycle` with only one cycle
    of repetition:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `tools.initCycle` 仅进行一次循环重复来注册个体：
- en: '[PRE32]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Register the population:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注册种群：
- en: '[PRE33]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Define the operators and register them in the `base.Toolbox()` container.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义操作符并将它们注册在 `base.Toolbox()` 容器中。
- en: 'Register the selection strategy:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注册选择策略：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Register the cross-over strategy:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注册交叉策略：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define a custom mutation strategy. Note that all of the implemented mutation
    strategies in DEAP are not really suitable for hyperparameter tuning purposes
    since they can only be utilized for floating or binary values, while most of the
    time, our hyperparameter space will be a combination of real and discrete hyperparameters.
    The following function shows how to implement such a custom mutation strategy.
    You can follow the same structure to suit your own need:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个自定义变异策略。请注意，DEAP 中实现的全部变异策略实际上并不适合超参数调整目的，因为它们只能用于浮点或二进制值，而大多数情况下，我们的超参数空间将是一组真实和离散超参数的组合。以下函数展示了如何实现这样的自定义变异策略。您可以遵循相同的结构来满足您的需求：
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Register the custom mutation strategy:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注册自定义变异策略：
- en: '[PRE37]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define the objective function and register it in the `base.Toolbox()` container:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数并将其注册在 `base.Toolbox()` 容器中：
- en: '[PRE38]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Register the objective function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注册目标函数：
- en: '[PRE39]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the Genetic Algorithm with parallel processing:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义具有并行处理的遗传算法：
- en: '[PRE40]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Register the `multiprocessing` module:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注册 `multiprocessing` 模块：
- en: '[PRE41]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define empty arrays to store the best and average values of objective function
    scores in each trial:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 定义空数组以存储每个试验中目标函数得分的最佳值和平均值：
- en: '[PRE42]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define a `HallOfFame` class that is responsible for storing the latest best
    individual (set of hyperparameters) in the population:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个 `HallOfFame` 类，该类负责在种群中存储最新的最佳个体（超参数集）：
- en: '[PRE43]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the initial population:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 定义初始种群：
- en: '[PRE44]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Start the GA iterations:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 开始 GA 迭代：
- en: '[PRE45]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Select the next generation individuals/children/offspring.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 选择下一代个体/孩子/后代。
- en: '[PRE46]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Clone the selected individuals.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 复制选定的个体。
- en: '[PRE47]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Apply crossover on the offspring.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在后代上应用交叉：
- en: '[PRE48]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Apply mutation on the offspring.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在后代上应用变异。
- en: '[PRE49]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Evaluate the individuals with an invalid fitness.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 评估具有无效适应度的个体：
- en: '[PRE50]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The population is entirely replaced by the offspring.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 种群完全由后代取代。
- en: '[PRE51]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Perform hyperparameter tuning by running the defined algorithm in *step 5*.
    After running the GA, we can get the best set of hyperparameters based on the
    following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行定义的算法在*步骤5*中执行超参数调整。在运行GA之后，我们可以根据以下代码获取最佳超参数集：
- en: '[PRE52]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到以下结果：
- en: '[PRE53]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can also plot the trial history or the convergence plot based on the following
    code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以根据以下代码绘制试验历史或收敛图：
- en: '[PRE54]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Based on the preceding code, the following figure is generated. As you can
    see, the objective function score or the fitness score is increasing throughout
    the number of trials since the population is updated with the improved individuals:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，以下图生成。如图所示，目标函数得分或适应度得分在整个试验次数中都在增加，因为种群被更新为改进的个体：
- en: '![Figure 10.1 – Genetic Algorithm convergence plot'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – 遗传算法收敛图]'
- en: '](img/B18753_10_001.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_10_001.jpg]'
- en: Figure 10.1 – Genetic Algorithm convergence plot
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 遗传算法收敛图
- en: 'Train the model on full training data using the best set of hyperparameters
    found:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型：
- en: '[PRE55]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Test the final trained model on the test data:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练的模型：
- en: '[PRE56]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Based on the preceding code, we get around `0.608` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当使用最佳超参数集在测试集上测试我们最终的训练随机森林模型时，F1分数大约为`0.608`。
- en: In this section, we have learned how to implement the GA with the DEAP package,
    starting from defining the necessary objects and defining the GA procedures with
    parallel processing and custom mutation strategy, until plotting the history of
    the trials and testing the best set of hyperparameters in the test set. In the
    next section, we will learn how to implement the PSO hyperparameter tuning method
    with the DEAP package.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用DEAP包实现遗传算法（GA），从定义必要的对象开始，到使用并行处理和自定义变异策略定义GA过程，再到绘制试验历史和测试测试集中最佳超参数集。在下一节中，我们将学习如何使用DEAP包实现PSO超参数调整方法。
- en: Implementing Particle Swarm Optimization
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现粒子群优化
- en: PSO is also one of the variants of the Heuristic Search hyperparameter tuning
    group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that can be implemented
    by the DEAP package. We’ll still use the same example as in the previous section
    to see how we can implement PSO using the DEAP package.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: PSO也是启发式搜索超参数调整组（见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)）的一种变体，可以使用DEAP包实现。我们仍将使用上一节中的相同示例来查看我们如何使用DEAP包实现PSO。
- en: 'The following code shows how to implement PSO with the DEAP package. You can
    find the more detailed code in the GitHub repository mentioned in the *Technical
    requirements* section:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何使用DEAP包实现PSO。你可以在*技术要求*部分提到的GitHub仓库中找到更详细的代码：
- en: 'Define the PSO parameters and type classes through the `creator.create()` module:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`creator.create()`模块定义PSO参数和类型类：
- en: '[PRE57]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Fix the seed for reproducibility:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 设置随机种子以实现可重复性：
- en: '[PRE58]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define the type of our fitness function. Here, we are working with a maximization
    problem and a single objective function, which is why we set `weights=(1.0,)`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的适应度函数的类型。在这里，我们正在处理一个最大化问题和一个单一目标函数，这就是为什么我们设置`weights=(1.0,)`：
- en: '[PRE59]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define the type of particles inherited from the built-in `list` data structure
    in Python that has `fitness`, `speed`, `smin`, `smax`, and `best` as its attribute.
    These attributes will be utilized later on when updating each particle’s position
    (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 定义从Python内置的`list`数据结构继承的粒子类型，该结构具有`fitness`、`speed`、`smin`、`smax`和`best`作为其属性。这些属性将在稍后更新每个粒子的位置时被利用（见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)）：
- en: '[PRE60]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Define the initializers along with the hyperparameter space and register them
    in the `base.Toolbox()` container.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义初始化器以及超参数空间，并在`base.Toolbox()`容器中注册它们。
- en: 'Initialize the toolbox:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化工具箱：
- en: '[PRE61]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define the naming of the hyperparameters:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 定义超参数的命名：
- en: '[PRE62]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Register each of the hyperparameters that we have in the space along with their
    distribution. Remember that PSO only works with the numerical type hyperparameters.
    That’s why we encode the `"model__criterion"` and `"model__class_weight"` hyperparameters
    to integers:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在空间中注册我们拥有的每个超参数及其分布。记住，PSO只与数值类型超参数一起工作。这就是为什么我们将`"model__criterion"`和`"model__class_weight"`超参数编码为整数：
- en: '[PRE63]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Register the individual by utilizing `tools.initCycle` with only one cycle
    of repetition. Note that we need to also assign the `speed`, `smin`, and `smax`
    values to each individual. To do that, let’s just define a function called `generate`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`tools.initCycle`仅进行一次重复循环来注册个体。注意，我们还需要将`speed`、`smin`和`smax`值分配给每个个体。为此，让我们定义一个名为`generate`的函数：
- en: '[PRE64]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Register the individual:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`tools.initCycle`仅进行一次重复循环来注册个体：
- en: '[PRE65]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Register the population:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注册种群：
- en: '[PRE66]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Define the operators and register them in the `base.Toolbox()` container. The
    main operator in PSO is the particle’s position update operator, which is defined
    in the `updateParticle` function as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义操作符并将它们注册到`base.Toolbox()`容器中。PSO中的主要操作符是粒子的位置更新操作符，该操作符在`updateParticle`函数中定义如下：
- en: '[PRE67]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Register the operator. Note that the `is_int` attribute is responsible for
    marking which hyperparameter has an integer type of value:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注册操作符。注意，`is_int`属性负责标记哪个超参数具有整数值类型：
- en: '[PRE68]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Define the objective function and register it in the `base.Toolbox()` container.
    Note that we also decode the `"model__criterion"` and `"model__class_weight"`
    hyperparameters within the objective function:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数并将其注册到`base.Toolbox()`容器中。注意，我们还在目标函数中解码了`"model__criterion"`和`"model__class_weight"`超参数：
- en: '[PRE69]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Register the objective function:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注册目标函数：
- en: '[PRE70]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Define PSO with parallel processing:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义具有并行处理的PSO：
- en: '[PRE71]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Register the `multiprocessing` module:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注册`multiprocessing`模块：
- en: '[PRE72]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define empty arrays to store the best and average values of objective function
    scores in each trial:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 定义空数组以存储每个试验中目标函数分数的最佳和平均值：
- en: '[PRE73]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define a `HallOfFame` class that is responsible for storing the latest best
    individual (set of hyperparameters) in the population:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个`HallOfFame`类，该类负责存储种群中的最新最佳个体（超参数集）：
- en: '[PRE74]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define the initial population:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 定义初始种群：
- en: '[PRE75]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Start the PSO iterations:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 开始PSO迭代：
- en: '[PRE76]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Perform hyperparameter tuning by running the algorithm defined in *step 5*.
    After running PSO, we can get the best set of hyperparameters based on the following
    code. Note that we need to decode the `"model__criterion"` and `"model__class_weight"`
    hyperparameters before passing them to the final model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行第5步中定义的算法来执行超参数调整。在运行PSO之后，我们可以根据以下代码获取最佳超参数集。注意，在将它们传递给最终模型之前，我们需要解码`"model__criterion"`和`"model__class_weight"`超参数：
- en: '[PRE77]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到以下结果：
- en: '[PRE78]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Train the model on full training data using the best set of hyperparameters
    found:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型：
- en: '[PRE79]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Test the final trained model on the test data:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE80]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Based on the preceding code, we get around `0.569` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们在测试最终训练好的随机森林模型时，在测试集上获得了大约`0.569`的F1分数，该模型使用了最佳的超参数集。
- en: In this section, we have learned how to implement PSO with the DEAP package,
    starting from defining the necessary objects, encoding the categorical hyperparameter
    to integers, and defining the optimization procedures with parallel processing,
    until testing the best set of hyperparameters in the test set. In the next section,
    we will start learning about another hyperparameter tuning package called NNI,
    which is developed by Microsoft.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用DEAP包实现PSO，从定义必要的对象开始，将分类超参数编码为整数，并使用并行处理定义优化过程，直到在测试集上测试最佳超参数集。在下一节中，我们将开始学习另一个名为NNI的超参数调整包，该包由微软开发。
- en: Introducing Microsoft NNI
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍微软NNI
- en: '`pip install nni` command.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install nni`命令。'
- en: Although NNI refers to *Neural Network Intelligence*, it actually supports numerous
    ML frameworks including (but not limited to) scikit-learn, XGBoost, LightGBM,
    PyTorch, TensorFlow, Caffe2, and MXNet.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NNI指的是*神经网络智能*，但它实际上支持包括但不限于scikit-learn、XGBoost、LightGBM、PyTorch、TensorFlow、Caffe2和MXNet在内的多个机器学习框架。
- en: There are numerous hyperparameter tuning methods implemented in NNI; some of
    them are built-in and others are wrapped from other packages such as `Hyperopt`
    (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)) and `SMAC3`. Here,
    in NNI, the hyperparameter tuning methods are referred to as **tuners**. We will
    not discuss all of the tuners implemented in NNI since there are too many of them.
    We will only discuss the tuners that have been discussed in *Chapters 3 – 6*.
    Apart from tuners, some of the hyperparameter tuning methods, such as Hyper Band
    and BOHB, are treated as **advisors** in NNI.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: NNI实现了许多超参数调优方法；其中一些是内置的，而另一些则是从其他包如`Hyperopt`（见[*第8章*](B18753_08_ePub.xhtml#_idTextAnchor074)）和`SMAC3`中封装的。在这里，NNI中的超参数调优方法被称为**调优器**。由于调优器种类繁多，我们不会讨论NNI中实现的所有调优器。我们只会讨论在第3章至第6章中讨论过的调优器。除了调优器之外，一些超参数调优方法，如Hyper
    Band和BOHB，在NNI中被视为**顾问**。
- en: Available Tuners in NNI
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: NNI中的可用调优器
- en: To see all of the available tuners in NNI, please refer to the official documentation
    page ([https://nni.readthedocs.io/en/stable/hpo/tuners.html](https://nni.readthedocs.io/en/stable/hpo/tuners.html)).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看NNI中所有可用调优器的详细信息，请参阅官方文档页面([https://nni.readthedocs.io/en/stable/hpo/tuners.html](https://nni.readthedocs.io/en/stable/hpo/tuners.html))。
- en: Unlike other hyperparameter tuning packages that we have discussed so far, in
    NNI, we have to prepare a Python script containing the model definition before
    being able to run the hyperparameter tuning process from the notebook. Furthermore,
    NNI also allows us to run the hyperparameter tuning experiment from the command-line
    tool where we need to define several other additional files to store the hyperparameter
    space information and other configurations.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的其他超参数调优包不同，在NNI中，我们必须准备一个包含模型定义的Python脚本，然后才能从笔记本中运行超参数调优过程。此外，NNI还允许我们从命令行工具中运行超参数调优实验，在那里我们需要定义几个其他附加文件来存储超参数空间信息和其他配置。
- en: 'The following steps show how we can perform any hyperparameter tuning procedure
    with NNI with pure Python code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何使用纯Python代码通过NNI执行任何超参数调优过程：
- en: 'Prepare the model to be tuned in a script, for example, `model.py`. This script
    should include the model architecture definition, dataset loading function, training
    function, and testing function. It also has to include three NNI API calls, as
    follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型，例如，`model.py`。此脚本应包括模型架构定义、数据集加载函数、训练函数和测试函数。它还必须包括三个NNI API调用，如下所示：
- en: '`nni.get_next_parameter()` is responsible for collecting the hyperparameters
    to be evaluated in a particular trial.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nni.get_next_parameter()` 负责收集特定试验中要评估的超参数。'
- en: '`nni.report_intermediate_result()` is responsible for reporting the evaluation
    metric within each training iteration (epoch or steps). Note that this API call
    is not mandatory; if you can’t get the intermediate evaluation metric from your
    ML framework, then this API call is not required.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nni.report_intermediate_result()` 负责在每次训练迭代（epoch或步骤）中报告评估指标。请注意，此API调用不是强制的；如果您无法从您的机器学习框架中获取中间评估指标，则不需要此API调用。'
- en: '`nni.report_final_result()` is responsible for reporting the final evaluation
    metric score after the training process is finished.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nni.report_final_result()` 负责在训练过程完成后报告最终评估指标分数。'
- en: 'Define the hyperparameter space. NNI expects the hyperparameter space is in
    the form of a Python dictionary, where the first-level keys store the names of
    the hyperparameters. The second-level keys store the types of the sampling distribution
    and the hyperparameter values range. The following shows an example of how to
    define the hyperparameter space in the expected format:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间。NNI期望超参数空间以Python字典的形式存在，其中第一级键存储超参数的名称。第二级键存储采样分布的类型和超参数值范围。以下是如何以预期格式定义超参数空间的示例：
- en: '[PRE81]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: More Information on NNI
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 关于NNI的更多信息
- en: For more information regarding the supported sampling distributions in NNI,
    please refer to the official documentation ([https://nni.readthedocs.io/en/latest/hpo/search_space.html](https://nni.readthedocs.io/en/latest/hpo/search_space.html)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于NNI支持的采样分布的更多信息，请参阅官方文档([https://nni.readthedocs.io/en/latest/hpo/search_space.html](https://nni.readthedocs.io/en/latest/hpo/search_space.html))。
- en: Next, we need to set up the experiment configurations via the `Experiment` class.
    The following shows steps to set up several configurations before we can run the
    hyperparameter tuning process.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过`Experiment`类设置实验配置。以下展示了在我们可以运行超参数调整过程之前设置几个配置的步骤。
- en: 'Load the `Experiment` class. Here, we are using the `''local''` experiment
    mode, which means all the training and hyperparameter tuning processes will be
    done only on our local computer. NNI allows us to run the training procedures
    in various platforms, including (but not limited to) **Azure Machine Learning**
    (**AML**), Kubeflow, and OpenAPI. For more information, please refer to the official
    documentation ([https://nni.readthedocs.io/en/latest/reference/experiment_config.html](https://nni.readthedocs.io/en/latest/reference/experiment_config.html)):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`Experiment`类。在这里，我们使用的是`'local'`实验模式，这意味着所有训练和超参数调整过程都将在我们的本地计算机上完成。NNI允许我们在各种平台上运行训练过程，包括但不限于**Azure
    Machine Learning**（**AML**）、Kubeflow和OpenAPI。更多信息，请参阅官方文档([https://nni.readthedocs.io/en/latest/reference/experiment_config.html](https://nni.readthedocs.io/en/latest/reference/experiment_config.html))：
- en: '[PRE82]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Set up the trial code configuration. Here, we need to specify the command to
    run the defined script in *step 1* and the relative path to the script. The following
    shows an example of how to set up the trial code configuration:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 设置试验代码配置。在这里，我们需要指定运行在*步骤1*中定义的脚本的命令和脚本的相对路径。以下展示了如何设置试验代码配置的示例：
- en: '[PRE83]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Set up the hyperparameter space configuration. To set up the hyperparameter
    space configuration, we simply need to pass the defined hyperparameter space in
    *step 2*. The following code shows how to do that:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 设置超参数空间配置。要设置超参数空间配置，我们只需将定义的超参数空间传递到*步骤2*。以下代码展示了如何进行操作：
- en: '[PRE84]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Set up the hyperparameter tuning algorithm to be utilized. The following shows
    an example of how to use TPE as the hyperparameter tuning algorithm on a maximization
    problem:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 设置要使用的超参数调整算法。以下展示了如何将TPE作为超参数调整算法应用于最大化问题的示例：
- en: '[PRE85]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Set up the number of trials and concurrent processes. NNI allows us to set
    how many numbers of hyperparameter sets are to be evaluated concurrently at a
    single time. The following code shows an example of how to set the number of trials
    to 50, where five sets will be evaluated concurrently at a particular time:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 设置试验次数和并发进程数。NNI允许我们设置在单次运行中同时评估多少个超参数集。以下代码展示了如何将试验次数设置为50，这意味着在特定时间将同时评估五个超参数集：
- en: '[PRE86]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'It is worth noting that NNI also allows you to define the stopping criterion
    based on the time duration instead of the number of trials. The following code
    shows how you can set the limit of the experiment time to 1 hour:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，NNI还允许你根据时间长度而不是试验次数来定义停止标准。以下代码展示了你如何将实验时间限制为1小时：
- en: '[PRE87]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: If you don’t provide both `max_trial_number` and `max_experiment_duration`,
    then the experiment will run forever until you forcefully stop it via the *Ctrl
    + C* command.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有提供`max_trial_number`和`max_experiment_duration`两个参数，那么实验将永远运行，直到你通过*Ctrl
    + C*命令强制停止它。
- en: 'Run the hyperparameter tuning experiment. To run the experiment, we can simply
    call the `run` method on the `Experiment` class. Here, we have to also choose
    what port to be used. We can see the experiment status and various interesting
    stats via the launched web portal. The following code shows how to run the experiment
    on port `8080` in `local`, meaning you can open the web portal on `http://localhost:8080`:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调整实验。要运行实验，我们可以在`Experiment`类上简单地调用`run`方法。在这里，我们还需要选择要使用的端口。我们可以通过启动的Web门户查看实验状态和各种有趣的统计数据。以下代码展示了如何在`local`模式下在端口`8080`上运行实验，这意味着你可以在`http://localhost:8080`上打开Web门户：
- en: '[PRE88]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: There are two available Boolean parameters for the `run` method, namely `wait_completion`
    and `debug`. When we set `wait_completion=True`, we can’t run other cells in the
    notebook until the experiment is done or some errors are found. The `debug` parameter
    enables us to choose whether we want to start the experiment in debug mode or
    not.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`run`方法有两个可用的布尔参数，即`wait_completion`和`debug`。当我们设置`wait_completion=True`时，我们无法在实验完成或发现错误之前运行笔记本中的其他单元格。`debug`参数使我们能够选择是否以调试模式启动实验。'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: Test the final trained model on the test data.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。
- en: NNI Web Portal
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: NNI Web Portal
- en: For more information regarding features available in the web portal, please
    refer to the official documentation ([https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html](https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html)).
    Note that we will discuss the web portal more in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125),
    *Tracking Hyperparameter Tuning Experiments*.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Web门户中可用的更多功能，请参阅官方文档（[https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html](https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html)）。注意，我们将在[*第13章*](B18753_13_ePub.xhtml#_idTextAnchor125)中更详细地讨论Web门户，*跟踪超参数调整实验*。
- en: 'If you prefer to work with the command-line tool, the following steps show
    how to perform any hyperparameter tuning procedure with NNI with the command-line
    tool, JSON, and YAML config files:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢使用命令行工具，以下步骤展示了如何使用命令行工具、JSON和YAML配置文件执行任何超参数调整流程：
- en: Prepare the model to be tuned in a script. This step is exactly the same as
    the previous procedure to perform hyperparameter tuning with NNI with pure Python
    code.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调整的模型。这一步骤与使用纯Python代码进行NNI超参数调整的前一个流程完全相同。
- en: Define the hyperparameter space. The expected format of the hyperparameter space
    is exactly the same as in the procedure on how to perform any hyperparameter tuning
    procedure with NNI with pure Python code. However, here, we need to store the
    Python dictionary within a JSON file, for example, `hyperparameter_space.json`.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间。超参数空间的预期格式与使用纯Python代码进行任何超参数调整流程的流程完全相同。然而，在这里，我们需要将Python字典存储在一个JSON文件中，例如，`hyperparameter_space.json`。
- en: 'Set up the experiment configurations via the `config.yaml` file. The configurations
    that need to be set up are basically the same as in the procedure with NNI with
    pure Python code. However, instead of configuring the experiment via a Python
    class, here, we store all of the configuration details in a single YAML file.
    The following shows an example of what the YAML file will look like:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`config.yaml`文件设置实验配置。需要设置的配置基本上与使用纯Python代码的NNI流程相同。然而，这里不是通过Python类来配置实验，而是将所有配置细节存储在一个单独的YAML文件中。以下是一个YAML文件示例：
- en: '[PRE89]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Run the hyperparameter tuning experiment. To run the experiment, we can simply
    call the `nnictl create` command. The following code shows how to use the command
    to run the experiment on port `8080` in `local`:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调整实验。要运行实验，我们可以简单地调用`nnictl create`命令。以下代码展示了如何使用该命令在`local`的`8080`端口上运行实验：
- en: '[PRE90]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: When the experiment is done, you can easily stop the process via the `nnictl
    stop` command.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 实验完成后，你可以通过`nnictl stop`命令轻松停止进程。
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: Test the final trained model on the test data.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。
- en: Examples for Various ML Frameworks
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 各种机器学习框架的示例
- en: You can find all of the examples to perform hyperparameter tuning via NNI using
    your favorite ML frameworks in the official documentation ([https://github.com/microsoft/nni/tree/master/examples/trials](https://github.com/microsoft/nni/tree/master/examples/trials)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方文档中找到使用你喜欢的机器学习框架通过NNI执行超参数调整的所有示例（[https://github.com/microsoft/nni/tree/master/examples/trials](https://github.com/microsoft/nni/tree/master/examples/trials)）。
- en: scikit-nni
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-nni
- en: There is also a package called `scikit-nni`, which will automatically generate
    the required `config.yml` and `search-space.json` and build the `scikit-learn`
    pipelines based on your own custom needs. Please refer to the official repository
    for further information about this package ([https://github.com/ksachdeva/scikit-nni](https://github.com/ksachdeva/scikit-nni)).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个名为`scikit-nni`的包，它将自动生成所需的`config.yml`和`search-space.json`，并根据你的自定义需求构建`scikit-learn`管道。有关此包的更多信息，请参阅官方仓库（[https://github.com/ksachdeva/scikit-nni](https://github.com/ksachdeva/scikit-nni)）。
- en: 'Besides tuners or hyperparameter tuning algorithms, NNI also provides `nni.report_intermediate_result()`
    API call. There are only two built-in assessors in NNI: *median stop* and *curve
    fitting*. The first assessor will stop the experiment whenever a hyperparameter
    set performs worse than the median at any step. The latter assessor will stop
    the experiment if the learning curve is likely to converge to a suboptimal result.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调优器或超参数调优算法之外，NNI 还提供了 `nni.report_intermediate_result()` API 调用。NNI 中只有两个内置评估器：*中值停止*
    和 *曲线拟合*。第一个评估器将在任何步骤中，只要某个超参数集的表现不如中值，就会停止实验。后者评估器将在学习曲线可能收敛到次优结果时停止实验。
- en: 'Setting up an assessor in NNI is very straightforward. You can simply add the
    configuration on the `Experiment` class or within the `config.yaml` file. The
    following code shows how to configure the median stop assessor on the `Experiment`
    class:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NNI 中设置评估器非常简单。您只需在 `Experiment` 类或 `config.yaml` 文件中添加配置即可。以下代码展示了如何在 `Experiment`
    类上配置中值停止评估器：
- en: '[PRE91]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Custom Algorithms in NNI
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: NNI 中的自定义算法
- en: NNI also allows us to define our own custom tuners and assessors. To do that,
    you need to inherit the base `Tuner` or `Assessor` class, write several required
    functions, and add more details on the `Experiment` class or in the `config.yaml`
    file. For more information regarding how to define your own custom tuners and
    assessors, please refer to the official documentation ([https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html](https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html)).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: NNI 还允许我们定义自己的自定义调优器和评估器。为此，您需要继承基类 `Tuner` 或 `Assessor`，编写几个必需的函数，并在 `Experiment`
    类或 `config.yaml` 文件中添加更多详细信息。有关如何定义自己的自定义调优器和评估器的更多信息，请参阅官方文档（[https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html](https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html)）。
- en: In this section, we have discussed the NNI package and how to perform hyperparameter
    tuning experiments in general. In the upcoming sections, we will learn how to
    implement various hyperparameter tuning algorithms using NNI.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了 NNI 包及其如何进行一般性的超参数调优实验。在接下来的章节中，我们将学习如何使用 NNI 实现各种超参数调优算法。
- en: Implementing Grid Search
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现网格搜索
- en: Grid Search is one of the variants of the Exhaustive Search hyperparameter tuning
    group (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)) that the NNI
    package can implement. To show you how we can implement Grid Search with the NNI
    package, let’s use the same data and pipeline as in the examples in the previous
    section. However, here, we’ll define a new hyperparameter space since NNI supports
    only limited types of sampling distribution.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是 NNI 包可以实现的穷举搜索超参数调优组（参见 [*第 3 章*](B18753_03_ePub.xhtml#_idTextAnchor031)）的一种变体。为了向您展示我们如何使用
    NNI 包实现网格搜索，我们将使用与上一节示例中相同的数据和管道。然而，在这里，我们将定义一个新的超参数空间，因为 NNI 只支持有限类型的采样分布。
- en: 'The following code shows how to implement Grid Search with the NNI package.
    Here, we’ll use the NNI command-line tool (**nnictl**) instead of using pure Python
    code. You can find the more detailed code in the GitHub repository mentioned in
    the *Technical requirements* section:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用 NNI 包实现网格搜索。在这里，我们将使用 NNI 命令行工具（**nnictl**）而不是纯 Python 代码。更详细的代码可以在
    *技术要求* 部分提到的 GitHub 仓库中找到：
- en: Prepare the model to be tuned in a script. Here, we’ll name the script `model.py`.
    There are several functions defined within this script, including `load_data`,
    `get_default_parameters`, `get_model`, and `run`.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型。在这里，我们将脚本命名为 `model.py`。该脚本中定义了几个函数，包括 `load_data`、`get_default_parameters`、`get_model`
    和 `run`。
- en: 'The `load_data` function loads the original data and splits it into train and
    test data. Furthermore, it’s also responsible for returning the lists of numerical
    and categorical column names:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_data` 函数加载原始数据并将其分为训练数据和测试数据。此外，它还负责返回数值和分类列名的列表：'
- en: '[PRE92]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The `get_default_parameters` function returns the default hyperparameter values
    used in the experiment:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_default_parameters` 函数返回实验中使用的默认超参数值：'
- en: '[PRE93]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The `get_model` function defines the `sklearn` pipeline used in this example:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_model` 函数定义了在此示例中使用的 `sklearn` 管道：'
- en: '[PRE94]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Initiate the Normalization Pre-processing for Numerical Features.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为数值特征启动归一化预处理。
- en: '[PRE95]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Initiate the One-Hot-Encoding Pre-processing for Categorical Features.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为分类特征启动 One-Hot-Encoding 预处理。
- en: '[PRE96]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Create the ColumnTransformer Class to delegate each preprocessor to the corresponding
    features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 ColumnTransformer 类以将每个预处理程序委托给相应的特征。
- en: '[PRE97]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Create a Pipeline of preprocessor and model.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 创建预处理器和模型的 Pipeline。
- en: '[PRE98]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Set hyperparmeter values.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 设置超参数值。
- en: '[PRE99]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The `run` function is responsible for training the model and getting the cross-validation
    score:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`run` 函数负责训练模型并获取交叉验证分数：'
- en: '[PRE100]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Finally, we can call those functions in the same script:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在同一脚本中调用这些函数：
- en: '[PRE101]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Define the hyperparameter space in a JSON file called `hyperparameter_space.json`:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在名为 `hyperparameter_space.json` 的 JSON 文件中定义超参数空间：
- en: '[PRE102]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Set up the experiment configurations via the `config.yaml` file:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `config.yaml` 文件设置实验配置：
- en: '[PRE103]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Run the hyperparameter tuning experiment. We can see the experiment status
    and various interesting stats via the launched web portal. The following code
    shows how to run the experiment on port `8080` in `local`, meaning you can open
    the web portal on `http://localhost:8080`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优实验。我们可以通过启动的网络门户查看实验状态和各种有趣的统计数据。以下代码展示了如何在 `local` 模式下通过端口 `8080` 运行实验，这意味着您可以在
    `http://localhost:8080` 上打开网络门户：
- en: '[PRE104]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Train the model on full training data using the best set of hyperparameters
    found. To get the best set of hyperparameters, you can go to the web portal and
    see them from the **Overview** tab.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。要获取最佳超参数集，您可以访问网络门户并在 *概览* 选项卡中查看。
- en: 'Based on the experiment results shown in the web portal within the *Top trials*
    tab, the following are the best hyperparameter values found from the experiment.
    Note that we will discuss the web portal more in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125),
    *Tracking Hyperparameter Tuning Experiments*:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在 *Top trials* 选项卡中显示的实验结果，以下是从实验中找到的最佳超参数值。注意，我们将在 [*第 13 章*](B18753_13_ePub.xhtml#_idTextAnchor125)
    *跟踪超参数调优实验* 中更详细地讨论网络门户：
- en: '[PRE105]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We can now train the model on full training data:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE106]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Test the final trained model on the test data:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE107]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Based on the preceding code, we get around `0.517` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当我们在测试集上使用最佳超参数集测试我们最终的训练 Random Forest 模型时，F1 分数大约为 `0.517`。
- en: In this section, we have learned how to implement Grid Search with the NNI package
    via `nnictl`. In the next section, we will learn how to implement Random Search
    with NNI via pure Python code.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过 `nnictl` 使用 NNI 包实现网格搜索。在下一节中，我们将学习如何通过纯 Python 代码使用 NNI 实现 Random
    Search。
- en: Implementing Random Search
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Random Search
- en: Random Search is one of the variants of the Exhaustive Search hyperparameter
    tuning group (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to show you how to implement Random
    Search with NNI using pure Python code.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索是穷举搜索超参数调优组（见 [*第 3 章*](B18753_03_ePub.xhtml#_idTextAnchor031)）的一种变体，NNI
    包可以实施。让我们使用与上一节示例中相同的数据、管道和超参数空间，向您展示如何使用纯 Python 代码通过 NNI 实现 Random Search。
- en: 'The following code shows how to implement Random Search with the NNI package.
    Here, we’ll use pure Python code instead of using `nnictl` as in the previous
    section. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用 NNI 包实现随机搜索。在这里，我们将使用纯 Python 代码而不是像上一节那样使用 `nnictl`。您可以在 *技术要求*
    部分提到的 GitHub 仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型。我们将使用与上一节相同的 `model.py` 脚本。
- en: 'Define the hyperparameter space in the form of a Python dictionary:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以 Python 字典的形式定义超参数空间：
- en: '[PRE108]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there is only one parameter for the Random Search tuner, namely the random `seed`
    parameter:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `Experiment` 类设置实验配置。注意，对于随机搜索调优器，只有一个参数，即随机的 `seed` 参数：
- en: '[PRE109]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优实验：
- en: '[PRE110]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳超参数集：
- en: '[PRE111]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到了以下结果：
- en: '[PRE112]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'We can now train the model on full training data:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE113]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Test the final trained model on the test data:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE114]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Based on the preceding code, we get around `0.597` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当使用最佳超参数集在测试集上测试我们最终训练的随机森林模型时，F1分数大约为`0.597`。
- en: In this section, we have learned how to implement Random Search using NNI with
    pure Python code. In the next section, we will learn how to implement Tree-structured
    Parzen Estimators with NNI via pure Python code.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现随机搜索。在下一节中，我们将学习如何通过纯Python代码使用NNI实现树结构帕累托估计器。
- en: Implementing Tree-structured Parzen Estimators
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现树结构帕累托估计器
- en: '**Tree-structured Parzen Estimators** (**TPEs**) are one of the variants of
    the Bayesian Optimization hyperparameter tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036))
    that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement TPE with NNI using
    pure Python code.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**树结构帕累托估计器**（**TPEs**）是贝叶斯优化超参数调整组（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）中NNI包可以实现的变体之一。让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码实现TPE与NNI。'
- en: 'The following code shows how to implement TPE with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用纯Python代码通过NNI包实现TPE。你可以在*技术要求*节中提到的GitHub仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型。我们将使用与上一节相同的`model.py`脚本。
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与上一节相同的超参数空间。
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are three parameters for the TPE tuner: `optimize_mode`, `seed`, and `tpe_args`.
    Please refer to the official documentation page for more information regarding
    the TPE tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner)):'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Experiment`类设置实验配置。请注意，TPE调整器有三个参数：`optimize_mode`、`seed`和`tpe_args`。有关TPE调整器参数的更多信息，请参阅官方文档页面（[https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner)）：
- en: '[PRE115]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调整实验：
- en: '[PRE116]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳超参数集：
- en: '[PRE117]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到以下结果：
- en: '[PRE118]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We can now train the model on full training data:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE119]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Fit the pipeline on train data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上拟合管道。
- en: '[PRE120]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Test the final trained model on the test data:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练的模型：
- en: '[PRE121]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Based on the preceding code, we get around `0.618` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当使用最佳超参数集在测试集上测试我们最终训练的随机森林模型时，F1分数大约为`0.618`。
- en: In this section, we have learned how to implement TPE using NNI with pure Python
    code. In the next section, we will learn how to implement Sequential Model Algorithm
    Configuration with NNI via pure Python code.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现TPE。在下一节中，我们将学习如何通过纯Python代码使用NNI实现序列模型算法配置。
- en: Implementing Sequential Model Algorithm Configuration
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现序列模型算法配置
- en: '`pip install "nni[SMAC]"`. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement SMAC with NNI using
    pure Python code.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install "nni[SMAC]"`. 让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码实现SMAC与NNI。'
- en: 'The following code shows how to implement SMAC with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用纯Python代码通过NNI包实现SMAC。你可以在*技术要求*节中提到的GitHub仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型。我们将使用与上一节相同的`model.py`脚本。
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与上一节相同的超参数空间。
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are two parameters for the SMAC tuner: `optimize_mode`, and `config_dedup`.
    Please refer to the official documentation page for more information regarding
    the SMAC tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner)):'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Experiment`类设置实验配置。请注意，SMAC调优器有两个参数：`optimize_mode`和`config_dedup`。有关SMAC调优器参数的更多信息，请参阅官方文档页面([https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner))：
- en: '[PRE122]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优实验：
- en: '[PRE123]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳的超参数组合：
- en: '[PRE124]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到了以下结果：
- en: '[PRE125]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'We can now train the model on full training data:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE126]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Test the final trained model on the test data:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE127]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: Based on the preceding code, we get around `0.619` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们在测试集上使用最佳超参数组合测试最终训练好的随机森林模型时，F1分数大约为`0.619`。
- en: In this section, we have learned how to implement SMAC using NNI with pure Python
    code. In the next section, we will learn how to implement Bayesian Optimization
    Gaussian Process with NNI via pure Python code.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现SMAC。在下一节中，我们将学习如何通过纯Python代码使用NNI实现贝叶斯优化高斯过程。
- en: Implementing Bayesian Optimization Gaussian Process
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贝叶斯优化高斯过程
- en: '**Bayesian Optimization Gaussian Process** (**BOGP**) is one of the variants
    of the Bayesian Optimization hyperparameter tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036))
    that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement BOGP with NNI using
    pure Python code.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化高斯过程**（**BOGP**）是贝叶斯优化超参数调优组（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）的变体之一，NNI包可以实现。让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码通过NNI实现BOGP。'
- en: 'The following code shows how to implement BOGP with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用NNI包通过纯Python代码实现BOGP。更详细的代码可以在*技术要求*部分提到的GitHub仓库中找到：
- en: 'Prepare the model to be tuned in a script. Here, we’ll use a new script called
    `model_numeric.py`. In this script, we add a mapping for non-numeric hyperparameters
    since BOGP can only work with numerical hyperparameters:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调优的模型。在这里，我们将使用一个新的脚本，名为`model_numeric.py`。在这个脚本中，我们为非数值超参数添加了一个映射，因为BOGP只能处理数值超参数：
- en: '[PRE128]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Define the hyperparameter space in the form of a Python dictionary. We’ll use
    a similar hyperparameter space as in the previous section with the only difference
    on the non-numeric hyperparameters. Here, all of the non-numeric hyperparameters
    are encoded into integer types of values:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与上一节类似的超参数空间，唯一的区别在于非数值超参数。在这里，所有非数值超参数都被编码为整数值类型：
- en: '[PRE129]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are nine parameters for the BOGP tuner: `optimize_mode`, `utility`, `kappa`,
    `xi`, `nu`, `alpha`, `cold_start_num`, `selection_num_warm_up`, and `selection_num_starting_points`.
    Please refer to the official documentation page for more information regarding
    the BOGP tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner)):'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Experiment`类设置实验配置。请注意，BOGP调优器有九个参数：`optimize_mode`、`utility`、`kappa`、`xi`、`nu`、`alpha`、`cold_start_num`、`selection_num_warm_up`和`selection_num_starting_points`。有关BOGP调优器参数的更多信息，请参阅官方文档页面([https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner))：
- en: '[PRE130]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优实验：
- en: '[PRE131]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳超参数集：
- en: '[PRE132]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们得到了以下结果：
- en: '[PRE133]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'We can now train the model on full training data:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE134]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Fit the pipeline on train data.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上拟合管道。
- en: '[PRE135]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Test the final trained model on the test data:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE136]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Based on the preceding code, we get around `0.619` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的随机森林模型时，F1分数大约为`0.619`。
- en: In this section, we have learned how to implement BOGP using NNI with pure Python
    code. In the next section, we will learn how to implement Metis with NNI via pure
    Python code.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现BOGP。在下一节中，我们将学习如何通过纯Python代码使用NNI实现Metis。
- en: Implementing Metis
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Metis
- en: '**Metis** is one of the variants of the Bayesian Optimization hyperparameter
    tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement Metis with NNI using
    pure Python code.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**Metis**是贝叶斯优化超参数调整组（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）的一个变体，NNI包可以实现。让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码实现Metis。'
- en: 'The following code shows how to implement Metis with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用纯Python代码通过NNI包实现Metis。你可以在*技术要求*部分提到的GitHub仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. Here, we’ll use the same script as
    in the previous section, `model_numeric.py`, since Metis can only work with numerical
    hyperparameters.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调整的模型。这里，我们将使用与上一节相同的脚本`model_numeric.py`，因为Metis只能与数值超参数一起工作。
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与上一节相同的超参数空间。
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are six parameters for the Metis tuner: `optimize_mode`, `no_resampling`,
    `no_candidates`, `selection_num_starting_points`, `cold_start_num`, and `exploration_probability`.
    Please refer to the official documentation page for more information regarding
    the Metis tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner)):'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Experiment`类设置实验配置。请注意，Metis调整器有六个参数：`optimize_mode`、`no_resampling`、`no_candidates`、`selection_num_starting_points`、`cold_start_num`和`exploration_probability`。有关Metis调整器参数的更多信息，请参阅官方文档页面([https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner))：
- en: '[PRE137]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调整实验：
- en: '[PRE138]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳超参数集：
- en: '[PRE139]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们得到了以下结果：
- en: '[PRE140]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'We can now train the model on full training data:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在全部训练数据上训练模型：
- en: '[PRE141]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Test the final trained model on the test data:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE142]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Based on the preceding code, we get around `0.590` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的随机森林模型时，F1分数大约为`0.590`。
- en: In this section, we have learned how to implement Metis using NNI with pure
    Python code. In the next section, we will learn how to implement Simulated Annealing
    with NNI via pure Python code.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现Metis。在下一节中，我们将学习如何通过纯Python代码使用NNI实现模拟退火。
- en: Implementing Simulated Annealing
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: Simulated Annealing is one of the variants of the Heuristic Search hyperparameter
    tuning group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section, to implement Simulated Annealing
    with NNI using pure Python code.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟退火是启发式搜索超参数调整组（参见[*第五章*](B18753_05_ePub.xhtml#_idTextAnchor047)）的一种变体，NNI包可以实现。让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码实现模拟退火。
- en: 'The following code shows how to implement Simulated Annealing with the NNI
    package using pure Python code. You can find the more detailed code in the GitHub
    repository mentioned in the *Technical requirements* section:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用纯Python代码通过NNI包实现模拟退火。你可以在*技术要求*部分提到的GitHub仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the *Implementing Grid Search* section.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调整的模型。我们将使用与*实现网格搜索*部分相同的`model.py`脚本。
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the *Implementing Grid Search* section.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与*实现网格搜索*部分相同的超参数空间。
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there is one parameter for the Simulated Annealing tuner, namely `optimize_mode`:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Experiment`类设置实验配置。请注意，对于模拟退火调整器有一个参数，即`optimize_mode`：
- en: '[PRE143]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调整实验：
- en: '[PRE144]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: 'Get the best set of hyperparameters:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳的超参数集：
- en: '[PRE145]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到了以下结果：
- en: '[PRE146]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'We can now train the model on full training data:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用全部训练数据来训练模型：
- en: '[PRE147]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Test the final trained model on the test data:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE148]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Based on the preceding code, we get around `0.600` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当我们在测试集上使用最佳超参数集测试最终训练好的随机森林模型时，F1分数大约为`0.600`。
- en: In this section, we have learned how to implement Simulated Annealing using
    NNI with pure Python code. In the next section, we will learn how to implement
    Hyper Band with NNI via pure Python code.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯Python代码通过NNI实现模拟退火。在下一节中，我们将学习如何通过纯Python代码实现Hyper Band。
- en: Implementing Hyper Band
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Hyper Band
- en: Hyper Band is one of the variants of the Multi-Fidelity Optimization hyperparameter
    tuning group (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement Hyper Band with NNI
    using pure Python code.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: Hyper Band是多保真优化超参数调整组（参见[*第六章*](B18753_06_ePub.xhtml#_idTextAnchor054)）的一种变体，NNI包可以实现。让我们使用与上一节示例中相同的数据、管道和超参数空间，使用纯Python代码实现Hyper
    Band。
- en: 'The following code shows how to implement Hyper Band with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用纯Python代码通过NNI包实现Hyper Band。你可以在*技术要求*部分提到的GitHub仓库中找到更详细的代码：
- en: Prepare the model to be tuned in a script. Here, we’ll use a new script called
    `model_advisor.py`. In this script, we utilize the `TRIAL_BUDGET` value from the
    output of `nni.get_next_parameter()` to update the `'model__n_estimators'` hyperparameter.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调整的模型。在这里，我们将使用一个名为`model_advisor.py`的新脚本。在这个脚本中，我们利用`nni.get_next_parameter()`输出的`TRIAL_BUDGET`值来更新`'model__n_estimators'`超参数。
- en: 'Define the hyperparameter space in the form of a Python dictionary. We’ll use
    a similar hyperparameter space to the *Implementing Grid Search* section but we
    will remove the `''model__n_estimators''` hyperparameter since it will become
    the budget definition for Hyper Band:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以Python字典的形式定义超参数空间。我们将使用与*实现网格搜索*部分类似的超参数空间，但我们将移除`'model__n_estimators'`超参数，因为它将成为Hyper
    Band的预算定义：
- en: '[PRE149]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are four parameters for the Hyper Band advisor: `optimize_mode`, `R`, `eta`,
    and `exec_mode`. Please refer to the official documentation page for more information
    regarding the Hyper Band advisor parameters ([https://nni.readthedocs.io/en/latest/reference/hpo.html](https://nni.readthedocs.io/en/latest/reference/hpo.html#hyperband-tuner)#hyperband-tuner):'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'We can now train the model on full training data:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Fit the pipeline on train data.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'Test the final trained model on the test data:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: Based on the preceding code, we get around `0.593` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Hyper Band using NNI with
    pure Python code. In the next section, we will learn how to implement Bayesian
    Optimization Hyper Band with NNI via pure Python code.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Hyper Band
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayesian Optimization Hyper Band** (**BOHB**) is one of the variants of the
    Multi-Fidelity Optimization hyperparameter tuning group (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054))
    that the NNI package can implement. Note that to use BOHB in NNI, we need to install
    additional dependencies using the following command:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Let’s use the same data, pipeline, and hyperparameter space as in the example
    in the previous section to implement BOHB with NNI using pure Python code.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Hyper Band with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model_advisor.py`
    script as in the previous section.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are 11 parameters for the BOHB advisor: `optimize_mode`, `min_budget`, `max_budget`,
    `eta`, `min_points_in_model`, `top_n_percent`, `num_samples`, `random_fraction`,
    `bandwidth_factor`, `min_bandwidth`, and `config_space`. Please refer to the official
    documentation page for more information regarding the Hyper Band advisor parameters
    ([https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner](https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner)):'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Run the hyperparameter tuning experiment:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'Based on the preceding code, we get the following results:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'We can now train the model on full training data:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'Test the final trained model on the test data:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型：
- en: '[PRE163]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: Based on the preceding code, we get around `0.617` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们在测试集上使用最佳超参数集测试最终训练好的随机森林模型时，F1分数大约为`0.617`。
- en: In this section, we have learned how to implement Bayesian Optimization Hyper
    Band using NNI with pure Python code. In the next section, we will learn how to
    implement Population-Based Training with NNI via `nnictl`.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用纯 Python 代码实现 NNI 的贝叶斯优化超参数搜索。在下一节中，我们将学习如何通过 `nnictl` 使用 NNI
    实现 Population-Based Training。
- en: Implementing Population-Based Training
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基于群体的训练
- en: '**Population-Based Training** (**PBT**) is one of the variants of the Heuristic
    Search hyperparameter tuning group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047))
    that the NNI package can implement. To show you how to implement PBT with NNI
    using pure Python code, let’s use the same example provided by the NNI package.
    Here, the MNIST dataset and a convolutional neural network model are utilized.
    We’ll use PyTorch to implement the neural network model. For details of the code
    example provided by NNI, please refer to the NNI GitHub repository ([https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch](https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch)).'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于群体的训练**（**PBT**）是启发式搜索超参数调整组（参见 [*第 5 章*](B18753_05_ePub.xhtml#_idTextAnchor047)）的变体之一，NNI
    包可以实现。为了向您展示如何使用纯 Python 代码通过 NNI 实现 PBT，我们将使用 NNI 包提供的相同示例。在这里，我们使用了 MNIST 数据集和卷积神经网络模型。我们将使用
    PyTorch 来实现神经网络模型。有关 NNI 提供的代码示例的详细信息，请参阅 NNI GitHub 仓库（[https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch](https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch)）。'
- en: MNIST Dataset
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集
- en: MNIST is a dataset of handwritten digits that have been size-normalized and
    centered in a fixed-size image. Here, we’ll use the MNIST dataset provided directly
    by the PyTorch package ([https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)).
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 是一个手写数字数据集，这些数字已经被标准化并居中在一个固定大小的图像中。在这里，我们将使用 PyTorch 包直接提供的 MNIST 数据集（[https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)）。
- en: 'The following code shows how to implement PBT with the NNI package. Here, we’ll
    use `nnictl` instead of using pure Python code. You can find the more detailed
    code in the GitHub repository mentioned in the *Technical requirements* section:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用 NNI 包实现 PBT。在这里，我们将使用 `nnictl` 而不是使用纯 Python 代码。更详细的代码可以在 *技术要求*
    部分提到的 GitHub 仓库中找到：
- en: 'Prepare the model to be tuned in a script. Here, we’ll use the same `mnist.py`
    script from the NNI GitHub repository. Note that we save the script with a new
    name: `model_pbt.py`.'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本中准备要调整的模型。在这里，我们将使用来自 NNI GitHub 仓库的相同的 `mnist.py` 脚本。请注意，我们将脚本保存为新的名称：`model_pbt.py`。
- en: Define the hyperparameter space in a JSON file called `hyperparameter_space_pbt.json`.
    Here, we’ll use the same `search_space.json` file from the NNI GitHub repository.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在名为 `hyperparameter_space_pbt.json` 的 JSON 文件中定义超参数空间。在这里，我们将使用来自 NNI GitHub
    仓库的相同的 `search_space.json` 文件。
- en: 'Set up the experiment configurations via the `config_pbt.yaml` file. Note that
    there are six parameters for the PBT tuner: `optimize_mode`, `all_checkpoint_dir`,
    `population_size`, `factor`, `resample_probability`, and `fraction`. Please refer
    to the official documentation page for more information regarding the PBT tuner
    parameters ([https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner](https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner)):'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `config_pbt.yaml` 文件设置实验配置。请注意，PBT 调优器有六个参数：`optimize_mode`、`all_checkpoint_dir`、`population_size`、`factor`、`resample_probability`
    和 `fraction`。有关 PBT 调优器参数的更多信息，请参阅官方文档页面（[https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner](https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner)）：
- en: '[PRE164]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Run the hyperparameter tuning experiment. We can see the experiment status
    and various interesting stats via the launched web portal. The following code
    shows how to run the experiment on port `8080` in `local`, meaning you can open
    the web portal on `http://localhost:8080`:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优实验。我们可以通过启动的Web门户查看实验状态和各种有趣的统计数据。以下代码展示了如何在`local`模式下运行端口`8080`上的实验，这意味着你可以在`http://localhost:8080`上打开Web门户：
- en: '[PRE165]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: In this section, we have learned how to implement Population-Based Training
    with NNI via `nnictl` using the same example as provided in the official documentation
    of NNI.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过`nnictl`使用NNI官方文档中提供的相同示例来实现基于群体的训练。
- en: Summary
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned all the important things about the DEAP and
    Microsoft NNI packages. We also have learned how to implement various hyperparameter
    tuning methods with the help of these packages, along with understanding each
    of the important parameters of the classes and how are they related to the theory
    that we have learned in the previous chapters. From now on, you should be able
    to utilize these packages to implement your chosen hyperparameter tuning method,
    and ultimately, boost the performance of your ML model. Equipped with the knowledge
    from *Chapters 3 – 6*, you will also be able to debug your code if there are errors
    or unexpected results, and be able to craft your own experiment configuration
    to match your specific problem.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于DEAP和Microsoft NNI包的所有重要内容。我们还学习了如何借助这些包实现各种超参数调优方法，以及理解每个类的重要参数以及它们与我们之前章节中学到的理论之间的关系。从现在开始，你应该能够利用这些包来实现你选择的超参数调优方法，并最终提高你的机器学习模型性能。凭借第3章至第6章的知识，你还将能够调试代码，如果出现错误或意外结果，并且能够制定自己的实验配置以匹配你的特定问题。
- en: In the next chapter, we’ll learn about hyperparameters for several popular algorithms.
    There will be a wide explanation for each of the algorithms, including (but not
    limited to) the definition of each hyperparameter, what will be impacted when
    the value of each hyperparameter is changed, and the priority list of hyperparameters
    based on the impact.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习几种流行算法的超参数。每个算法都会有广泛的解释，包括但不限于每个超参数的定义、当每个超参数的值发生变化时将产生什么影响，以及基于影响的超参数优先级列表。
