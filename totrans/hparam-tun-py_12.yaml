- en: '*Chapter 10*: Advanced Hyperparameter Tuning with DEAP and Microsoft NNI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DEAP** and **Microsoft NNI** are Python packages that provide various hyperparameter
    tuning methods that are not implemented in other packages that we have discussed
    in *Chapters 7 – 9*. For example, Genetic Algorithm, Particle Swarm Optimization,
    Metis, Population-Based Training, and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn how to perform hyperparameter tuning using both
    DEAP and Microsoft NNI packages, starting from getting ourselves familiar with
    the packages, along with the important modules and parameters we need to be aware
    of. We’ll learn not only how to utilize both DEAP and Microsoft NNI to perform
    hyperparameter tuning with their default configurations but also discuss other
    available configurations along with their usage. Moreover, we’ll also discuss
    how the implementation of the hyperparameter tuning methods is related to the
    theory that we have learned in previous chapters, since there may be some minor
    differences or adjustments made in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand all of the important
    things you need to know about DEAP and Microsoft NNI and be able to implement
    various hyperparameter tuning methods available in these packages. You’ll also
    be able to understand each of the important parameters of the classes and how
    they are related to the theory that we have learned in the previous chapters.
    Finally, equipped with the knowledge from previous chapters, you will also be
    able to understand what’s happening if there are errors or unexpected results
    and understand how to set up the method configuration to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main topics that will be discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DEAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Genetic Algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Particle Swarm Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Microsoft NNI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Tree-structured Parzen Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Sequential Model Algorithm Configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gaussian Process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Metis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Simulated Annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Hyper Band
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Hyper Band
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Population-Based Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will learn how to implement various hyperparameter tuning methods with DEAP
    and Microsoft NNI. To ensure that you are able to reproduce the code examples
    in this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version 3.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `pandas` package (version 1.3.4 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `NumPy` package (version 1.21.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `SciPy` package (version 1.7.3 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `Matplotlib` package (version 3.5.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `scikit-learn` package (version 1.0.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `DEAP` package (version 1.3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `Hyperopt` package (version 0.1.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `NNI` package (version 2.7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `PyTorch` package (version 1.10.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/blob/main/10_Advanced_Hyperparameter-Tuning-via-DEAP-and-NNI.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DEAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip install deap` command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DEAP allows you to craft your evolutionary algorithm optimization steps in
    a very flexible manner. The following steps show how to utilize DEAP to perform
    any hyperparameter tuning methods. More detailed steps, including the code implementation,
    will be given through various examples in the upcoming sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the *type* classes through the `creator.create()` module. These classes
    are responsible for defining the type of objects that will be used in the optimization
    steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the *initializers* along with the hyperparameter space and register them
    in the `base.Toolbox()` container. The initializers are responsible for setting
    the initial value of the objects that will be used in the optimization steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the *operators* and register them in the `base.Toolbox()` container.
    The operators refer to the evolutionary tools or **genetic operator** (see [*Chapter
    5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that need to be defined as part of
    the optimization algorithm. For example, the selection, crossover, and mutation
    operators in the Genetic Algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the objective function and register it in the `base.Toolbox()` container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define your own hyperparameter tuning algorithm function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform hyperparameter tuning by calling the defined function in *step 5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the final trained model on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The type classes refer to the type of objects used in the optimization steps.
    These type classes are inherited from the base classes implemented in DEAP. For
    example, we can define the type of our fitness function as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `base.Fitness` class is a base abstract class implemented in DEAP that can
    be utilized to define our own fitness function type. It expects a `weights` parameter
    to understand the type of optimization problem we are working on. If it’s a maximization
    problem, then we have to put a positive weight and the other way around for a
    minimization problem. Notice that it expects a tuple data structure instead of
    a float. This is because DEAP also allows us to work with a `(1.0, -1.0)` to the
    `weights` parameter, it means we have two objective functions where we want to
    maximize the first one and minimize the second one with equal weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `creator.create()` function is responsible for creating a new class based
    on the base class. In the preceding code, we created the type class for our objective
    function with the name “`FitnessMax`”. This `creator.create()` function expects
    at least two parameters: specifically, the name of the newly created class and
    the base class itself. The rest of the parameters passed to this function will
    be treated as the attributes for this newly created class. Besides defining the
    type of the objective function, we can also define the type of individuals in
    the evolutionary algorithm that will be performed. The following code shows how
    to create the type of individuals inherited from the built-in `list` data structure
    in Python that has `fitness` as its attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `fitness` attribute has a type of `creator.FitnessMax`, which
    is the type that we just created in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: Types Definition in DEAP
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of ways to define type classes in DEAP. While we have discussed
    the most straightforward and, arguably, most used type class, you may find other
    cases that need other definitions of type class. For more information on how to
    define other types in DEAP, please refer to the official documentation ([https://deap.readthedocs.io/en/master/tutorials/basic/part1.html](https://deap.readthedocs.io/en/master/tutorials/basic/part1.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have finished defining the type of objects that will be used in the
    optimization steps, we now need to initiate the value of those objects using the
    initializers and register them in the `base.Toolbox()` container. You can think
    of this module as a box or container of initializers and other tools that will
    be utilized during the optimization steps. The following code shows how we can
    set the random initial values for individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows an example of how to register the `"individual"` object
    in the `base.Toolbox()` container, where each individual has a size of `10`. The
    individual is generated by repeatedly calling the `random.random` method 10 times.
    Note that, in the hyperparameter tuning setup, the size of `10` of each individual
    actually refers to the number of the hyperparameters we have in the space. The
    following shows the output of calling the registered individual via the `toolbox.individual()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output of `toolbox.individual()` is just a list of 10 random
    values since we’ve defined `creator.Individual` to inherit from the built-in `list`
    data structure in Python. Furthermore, we also called `tools.initRepeat` when
    registering the individual with the `random.random` method by 10 times.
  prefs: []
  type: TYPE_NORMAL
- en: You may now wonder, how do you define the actual hyperparameter space using
    this `toolbox.register()` method? Initiating a bunch of random values definitely
    doesn’t make any sense. We need to know the way to define the hyperparameter space
    that will be equipped for each individual. To do that, we can actually utilize
    another tool provided by DEAP, `tools.InitCycle`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where `tools.initRepeat` will just call the provided function `n` times, in
    our previous example, the provided function is `random.random`. Here, `tools.InitCycle`
    expects a list of functions and will call those functions for `n` cycles. The
    following code shows an example to define the hyperparameter space that will be
    equipped for each individual:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to first register each of the hyperparameters that we have in the space
    along with their distribution. Note that we can pass all of the required parameters
    to the sampling distribution function to `toolbox.register()` as well. For example,
    here, we pass the `a=0,b=0.5,loc=0.005,scale=0.01` parameters of the `truncnorm.rvs()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have registered each hyperparameter we have, we can register the individual
    by utilizing `tools.initCycle` with only one cycle of repetition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following shows the output of calling the registered individual via the
    `toolbox.individual()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have registered the individual in our toolbox, registering a population
    is very simple. We just need to utilize the `tools.initRepeat` module and pass
    the defined `toolbox.individual` as the argument. The following code shows how
    to register a population in general. Note that, here, the population is just a
    list of five individuals defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following shows the output when calling the `toolbox.population()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned previously, the `base.Toolbox()` container is responsible for
    storing not only initializers but also other tools that will be utilized during
    the optimization steps. Another important building block for an evolutionary algorithm,
    such as the GA, is the genetic operator. Fortunately, DEAP already implemented
    various genetic operators that we can utilize via the `tools` module. The following
    code shows an example of how to register the selection, crossover, and mutation
    operators for the GA (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `tools.selTournament` selection strategy works by selecting the best individuals
    among `tournsize` randomly chosen individuals, *NPOP* times, where `tournsize`
    is the number of individuals participating in the tournament and *NPOP* is the
    number of individuals in the population. The `tools.cxBlend` crossover strategy
    works by performing a linear combination between two continuous individual genes,
    where the weight for the linear combination is governed by the `alpha` hyperparameter.
    The `tools.mutPolynomialBounded` mutation strategy works by passing continuous
    individual genes to a pre-defined polynomial mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary Tools in DEAP
  prefs: []
  type: TYPE_NORMAL
- en: There are various built-in evolutionary tools implemented in DEAP that we can
    utilize for our own needs, starting from initializers, crossover, mutation, selection,
    and migration tools. For more information regarding the implemented tools, please
    refer to the official documentation ([https://deap.readthedocs.io/en/master/api/tools.html](https://deap.readthedocs.io/en/master/api/tools.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To register the pre-defined objective function to the toolbox, we can just
    simply call the same `toolbox.register()` method and pass the objective function,
    as the following code shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, `obj_func` is a Python function that expects to receive the `individual`
    object defined previously. We will see how to create such an objective function
    and how to define our own hyperparameter tuning algorithm function in the upcoming
    sections when we discuss how to implement the GA and PSO in DEAP.
  prefs: []
  type: TYPE_NORMAL
- en: 'DEAP also allows us to utilize our parallel computing resources when calling
    the objective function. To do that, we can simply need to register the `multiprocessing`
    module to the toolbox as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have registered the `multiprocessing` module, we can simply apply this
    when calling the objective function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have discussed the DEAP package and its building blocks.
    You may wonder how to construct a real hyperparameter tuning method using all
    of the building blocks provided by DEAP. Worry no more; in the upcoming two sections,
    we will learn how to utilize all of the discussed building blocks to perform hyperparameter
    tuning with the GA and PSO methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Genetic Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GA is one of the variants of the Heuristic Search hyperparameter tuning group
    (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that can be implemented
    by the DEAP package. To show you how we can implement GA with the DEAP package,
    let’s use the Random Forest classifier model and the same data as in the examples
    in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062). The dataset used in this
    example is the *Banking Dataset – Marketing Targets* dataset provided on Kaggle
    ([https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)).
  prefs: []
  type: TYPE_NORMAL
- en: The target variable consists of two classes, `yes` or `no`, indicating whether
    the client of the bank has subscribed to a term deposit or not. Hence, the goal
    of training an ML model on this dataset is to identify whether a customer is potentially
    wanting to subscribe to the term deposit or not. Out of the 16 features provided
    in the data, there are seven numerical features and nine categorical features.
    As for the target class distribution, 12% of them are *yes* and 88% of them are
    *no*, for both train and test datasets. For more detailed information about the
    data, please refer to [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
  prefs: []
  type: TYPE_NORMAL
- en: Before performing the GA, let’s see how the Random Forest classifier with default
    hyperparameters values works. As shown in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062),
    we get around `0.436` in the F1-score when evaluating the Random Forest classifier
    with default hyperparameter values on the test set. Note that we’re still using
    the same scikit-learn pipeline definition to train and evaluate the Random Forest
    classifier, as explained in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement the GA with the DEAP package. You
    can find the more detailed code in the GitHub repository mentioned in the *Technical
    requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the GA parameters and type classes through the `creator.create()` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fix the seed for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the type of our fitness function. Here, we are working with a maximization
    problem and a single objective function, that’s why we set `weights=(1.0,)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the type of individuals inherited from the built-in `list` data structure
    in Python that has `fitness` as its attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Define the initializers along with the hyperparameter space and register them
    in the `base.Toolbox()` container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the naming of the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Register each of the hyperparameters that we have in the space along with their
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the individual by utilizing `tools.initCycle` with only one cycle
    of repetition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Define the operators and register them in the `base.Toolbox()` container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Register the selection strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the cross-over strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a custom mutation strategy. Note that all of the implemented mutation
    strategies in DEAP are not really suitable for hyperparameter tuning purposes
    since they can only be utilized for floating or binary values, while most of the
    time, our hyperparameter space will be a combination of real and discrete hyperparameters.
    The following function shows how to implement such a custom mutation strategy.
    You can follow the same structure to suit your own need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the custom mutation strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the objective function and register it in the `base.Toolbox()` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the Genetic Algorithm with parallel processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the `multiprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define empty arrays to store the best and average values of objective function
    scores in each trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `HallOfFame` class that is responsible for storing the latest best
    individual (set of hyperparameters) in the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the initial population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the GA iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Select the next generation individuals/children/offspring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Clone the selected individuals.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Apply crossover on the offspring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Apply mutation on the offspring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the individuals with an invalid fitness.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The population is entirely replaced by the offspring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform hyperparameter tuning by running the defined algorithm in *step 5*.
    After running the GA, we can get the best set of hyperparameters based on the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the trial history or the convergence plot based on the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, the following figure is generated. As you can
    see, the objective function score or the fitness score is increasing throughout
    the number of trials since the population is updated with the improved individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Genetic Algorithm convergence plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_10_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Genetic Algorithm convergence plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model on full training data using the best set of hyperparameters
    found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.608` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement the GA with the DEAP package,
    starting from defining the necessary objects and defining the GA procedures with
    parallel processing and custom mutation strategy, until plotting the history of
    the trials and testing the best set of hyperparameters in the test set. In the
    next section, we will learn how to implement the PSO hyperparameter tuning method
    with the DEAP package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Particle Swarm Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PSO is also one of the variants of the Heuristic Search hyperparameter tuning
    group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that can be implemented
    by the DEAP package. We’ll still use the same example as in the previous section
    to see how we can implement PSO using the DEAP package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement PSO with the DEAP package. You can
    find the more detailed code in the GitHub repository mentioned in the *Technical
    requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the PSO parameters and type classes through the `creator.create()` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fix the seed for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the type of our fitness function. Here, we are working with a maximization
    problem and a single objective function, which is why we set `weights=(1.0,)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the type of particles inherited from the built-in `list` data structure
    in Python that has `fitness`, `speed`, `smin`, `smax`, and `best` as its attribute.
    These attributes will be utilized later on when updating each particle’s position
    (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Define the initializers along with the hyperparameter space and register them
    in the `base.Toolbox()` container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the naming of the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Register each of the hyperparameters that we have in the space along with their
    distribution. Remember that PSO only works with the numerical type hyperparameters.
    That’s why we encode the `"model__criterion"` and `"model__class_weight"` hyperparameters
    to integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the individual by utilizing `tools.initCycle` with only one cycle
    of repetition. Note that we need to also assign the `speed`, `smin`, and `smax`
    values to each individual. To do that, let’s just define a function called `generate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the individual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the operators and register them in the `base.Toolbox()` container. The
    main operator in PSO is the particle’s position update operator, which is defined
    in the `updateParticle` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the operator. Note that the `is_int` attribute is responsible for
    marking which hyperparameter has an integer type of value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the objective function and register it in the `base.Toolbox()` container.
    Note that we also decode the `"model__criterion"` and `"model__class_weight"`
    hyperparameters within the objective function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Define PSO with parallel processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register the `multiprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Define empty arrays to store the best and average values of objective function
    scores in each trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `HallOfFame` class that is responsible for storing the latest best
    individual (set of hyperparameters) in the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the initial population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the PSO iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform hyperparameter tuning by running the algorithm defined in *step 5*.
    After running PSO, we can get the best set of hyperparameters based on the following
    code. Note that we need to decode the `"model__criterion"` and `"model__class_weight"`
    hyperparameters before passing them to the final model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model on full training data using the best set of hyperparameters
    found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.569` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement PSO with the DEAP package,
    starting from defining the necessary objects, encoding the categorical hyperparameter
    to integers, and defining the optimization procedures with parallel processing,
    until testing the best set of hyperparameters in the test set. In the next section,
    we will start learning about another hyperparameter tuning package called NNI,
    which is developed by Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Microsoft NNI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip install nni` command.'
  prefs: []
  type: TYPE_NORMAL
- en: Although NNI refers to *Neural Network Intelligence*, it actually supports numerous
    ML frameworks including (but not limited to) scikit-learn, XGBoost, LightGBM,
    PyTorch, TensorFlow, Caffe2, and MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous hyperparameter tuning methods implemented in NNI; some of
    them are built-in and others are wrapped from other packages such as `Hyperopt`
    (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)) and `SMAC3`. Here,
    in NNI, the hyperparameter tuning methods are referred to as **tuners**. We will
    not discuss all of the tuners implemented in NNI since there are too many of them.
    We will only discuss the tuners that have been discussed in *Chapters 3 – 6*.
    Apart from tuners, some of the hyperparameter tuning methods, such as Hyper Band
    and BOHB, are treated as **advisors** in NNI.
  prefs: []
  type: TYPE_NORMAL
- en: Available Tuners in NNI
  prefs: []
  type: TYPE_NORMAL
- en: To see all of the available tuners in NNI, please refer to the official documentation
    page ([https://nni.readthedocs.io/en/stable/hpo/tuners.html](https://nni.readthedocs.io/en/stable/hpo/tuners.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other hyperparameter tuning packages that we have discussed so far, in
    NNI, we have to prepare a Python script containing the model definition before
    being able to run the hyperparameter tuning process from the notebook. Furthermore,
    NNI also allows us to run the hyperparameter tuning experiment from the command-line
    tool where we need to define several other additional files to store the hyperparameter
    space information and other configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps show how we can perform any hyperparameter tuning procedure
    with NNI with pure Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the model to be tuned in a script, for example, `model.py`. This script
    should include the model architecture definition, dataset loading function, training
    function, and testing function. It also has to include three NNI API calls, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nni.get_next_parameter()` is responsible for collecting the hyperparameters
    to be evaluated in a particular trial.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nni.report_intermediate_result()` is responsible for reporting the evaluation
    metric within each training iteration (epoch or steps). Note that this API call
    is not mandatory; if you can’t get the intermediate evaluation metric from your
    ML framework, then this API call is not required.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nni.report_final_result()` is responsible for reporting the final evaluation
    metric score after the training process is finished.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define the hyperparameter space. NNI expects the hyperparameter space is in
    the form of a Python dictionary, where the first-level keys store the names of
    the hyperparameters. The second-level keys store the types of the sampling distribution
    and the hyperparameter values range. The following shows an example of how to
    define the hyperparameter space in the expected format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: More Information on NNI
  prefs: []
  type: TYPE_NORMAL
- en: For more information regarding the supported sampling distributions in NNI,
    please refer to the official documentation ([https://nni.readthedocs.io/en/latest/hpo/search_space.html](https://nni.readthedocs.io/en/latest/hpo/search_space.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to set up the experiment configurations via the `Experiment` class.
    The following shows steps to set up several configurations before we can run the
    hyperparameter tuning process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the `Experiment` class. Here, we are using the `''local''` experiment
    mode, which means all the training and hyperparameter tuning processes will be
    done only on our local computer. NNI allows us to run the training procedures
    in various platforms, including (but not limited to) **Azure Machine Learning**
    (**AML**), Kubeflow, and OpenAPI. For more information, please refer to the official
    documentation ([https://nni.readthedocs.io/en/latest/reference/experiment_config.html](https://nni.readthedocs.io/en/latest/reference/experiment_config.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the trial code configuration. Here, we need to specify the command to
    run the defined script in *step 1* and the relative path to the script. The following
    shows an example of how to set up the trial code configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the hyperparameter space configuration. To set up the hyperparameter
    space configuration, we simply need to pass the defined hyperparameter space in
    *step 2*. The following code shows how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the hyperparameter tuning algorithm to be utilized. The following shows
    an example of how to use TPE as the hyperparameter tuning algorithm on a maximization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the number of trials and concurrent processes. NNI allows us to set
    how many numbers of hyperparameter sets are to be evaluated concurrently at a
    single time. The following code shows an example of how to set the number of trials
    to 50, where five sets will be evaluated concurrently at a particular time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'It is worth noting that NNI also allows you to define the stopping criterion
    based on the time duration instead of the number of trials. The following code
    shows how you can set the limit of the experiment time to 1 hour:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t provide both `max_trial_number` and `max_experiment_duration`,
    then the experiment will run forever until you forcefully stop it via the *Ctrl
    + C* command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the hyperparameter tuning experiment. To run the experiment, we can simply
    call the `run` method on the `Experiment` class. Here, we have to also choose
    what port to be used. We can see the experiment status and various interesting
    stats via the launched web portal. The following code shows how to run the experiment
    on port `8080` in `local`, meaning you can open the web portal on `http://localhost:8080`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are two available Boolean parameters for the `run` method, namely `wait_completion`
    and `debug`. When we set `wait_completion=True`, we can’t run other cells in the
    notebook until the experiment is done or some errors are found. The `debug` parameter
    enables us to choose whether we want to start the experiment in debug mode or
    not.
  prefs: []
  type: TYPE_NORMAL
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the final trained model on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NNI Web Portal
  prefs: []
  type: TYPE_NORMAL
- en: For more information regarding features available in the web portal, please
    refer to the official documentation ([https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html](https://nni.readthedocs.io/en/stable/experiment/web_portal/web_portal.html)).
    Note that we will discuss the web portal more in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125),
    *Tracking Hyperparameter Tuning Experiments*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you prefer to work with the command-line tool, the following steps show
    how to perform any hyperparameter tuning procedure with NNI with the command-line
    tool, JSON, and YAML config files:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. This step is exactly the same as
    the previous procedure to perform hyperparameter tuning with NNI with pure Python
    code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space. The expected format of the hyperparameter space
    is exactly the same as in the procedure on how to perform any hyperparameter tuning
    procedure with NNI with pure Python code. However, here, we need to store the
    Python dictionary within a JSON file, for example, `hyperparameter_space.json`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `config.yaml` file. The configurations
    that need to be set up are basically the same as in the procedure with NNI with
    pure Python code. However, instead of configuring the experiment via a Python
    class, here, we store all of the configuration details in a single YAML file.
    The following shows an example of what the YAML file will look like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment. To run the experiment, we can simply
    call the `nnictl create` command. The following code shows how to use the command
    to run the experiment on port `8080` in `local`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the experiment is done, you can easily stop the process via the `nnictl
    stop` command.
  prefs: []
  type: TYPE_NORMAL
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the final trained model on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples for Various ML Frameworks
  prefs: []
  type: TYPE_NORMAL
- en: You can find all of the examples to perform hyperparameter tuning via NNI using
    your favorite ML frameworks in the official documentation ([https://github.com/microsoft/nni/tree/master/examples/trials](https://github.com/microsoft/nni/tree/master/examples/trials)).
  prefs: []
  type: TYPE_NORMAL
- en: scikit-nni
  prefs: []
  type: TYPE_NORMAL
- en: There is also a package called `scikit-nni`, which will automatically generate
    the required `config.yml` and `search-space.json` and build the `scikit-learn`
    pipelines based on your own custom needs. Please refer to the official repository
    for further information about this package ([https://github.com/ksachdeva/scikit-nni](https://github.com/ksachdeva/scikit-nni)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides tuners or hyperparameter tuning algorithms, NNI also provides `nni.report_intermediate_result()`
    API call. There are only two built-in assessors in NNI: *median stop* and *curve
    fitting*. The first assessor will stop the experiment whenever a hyperparameter
    set performs worse than the median at any step. The latter assessor will stop
    the experiment if the learning curve is likely to converge to a suboptimal result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up an assessor in NNI is very straightforward. You can simply add the
    configuration on the `Experiment` class or within the `config.yaml` file. The
    following code shows how to configure the median stop assessor on the `Experiment`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Custom Algorithms in NNI
  prefs: []
  type: TYPE_NORMAL
- en: NNI also allows us to define our own custom tuners and assessors. To do that,
    you need to inherit the base `Tuner` or `Assessor` class, write several required
    functions, and add more details on the `Experiment` class or in the `config.yaml`
    file. For more information regarding how to define your own custom tuners and
    assessors, please refer to the official documentation ([https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html](https://nni.readthedocs.io/en/stable/hpo/custom_algorithm.html)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have discussed the NNI package and how to perform hyperparameter
    tuning experiments in general. In the upcoming sections, we will learn how to
    implement various hyperparameter tuning algorithms using NNI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grid Search is one of the variants of the Exhaustive Search hyperparameter tuning
    group (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)) that the NNI
    package can implement. To show you how we can implement Grid Search with the NNI
    package, let’s use the same data and pipeline as in the examples in the previous
    section. However, here, we’ll define a new hyperparameter space since NNI supports
    only limited types of sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Grid Search with the NNI package.
    Here, we’ll use the NNI command-line tool (**nnictl**) instead of using pure Python
    code. You can find the more detailed code in the GitHub repository mentioned in
    the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. Here, we’ll name the script `model.py`.
    There are several functions defined within this script, including `load_data`,
    `get_default_parameters`, `get_model`, and `run`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `load_data` function loads the original data and splits it into train and
    test data. Furthermore, it’s also responsible for returning the lists of numerical
    and categorical column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_default_parameters` function returns the default hyperparameter values
    used in the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_model` function defines the `sklearn` pipeline used in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Initiate the Normalization Pre-processing for Numerical Features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Initiate the One-Hot-Encoding Pre-processing for Categorical Features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Create the ColumnTransformer Class to delegate each preprocessor to the corresponding
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Create a Pipeline of preprocessor and model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Set hyperparmeter values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run` function is responsible for training the model and getting the cross-validation
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can call those functions in the same script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the hyperparameter space in a JSON file called `hyperparameter_space.json`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the experiment configurations via the `config.yaml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment. We can see the experiment status
    and various interesting stats via the launched web portal. The following code
    shows how to run the experiment on port `8080` in `local`, meaning you can open
    the web portal on `http://localhost:8080`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found. To get the best set of hyperparameters, you can go to the web portal and
    see them from the **Overview** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on the experiment results shown in the web portal within the *Top trials*
    tab, the following are the best hyperparameter values found from the experiment.
    Note that we will discuss the web portal more in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125),
    *Tracking Hyperparameter Tuning Experiments*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.517` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Grid Search with the NNI package
    via `nnictl`. In the next section, we will learn how to implement Random Search
    with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Search is one of the variants of the Exhaustive Search hyperparameter
    tuning group (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to show you how to implement Random
    Search with NNI using pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Random Search with the NNI package.
    Here, we’ll use pure Python code instead of using `nnictl` as in the previous
    section. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the hyperparameter space in the form of a Python dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there is only one parameter for the Random Search tuner, namely the random `seed`
    parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.597` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Random Search using NNI with
    pure Python code. In the next section, we will learn how to implement Tree-structured
    Parzen Estimators with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Tree-structured Parzen Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tree-structured Parzen Estimators** (**TPEs**) are one of the variants of
    the Bayesian Optimization hyperparameter tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036))
    that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement TPE with NNI using
    pure Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement TPE with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are three parameters for the TPE tuner: `optimize_mode`, `seed`, and `tpe_args`.
    Please refer to the official documentation page for more information regarding
    the TPE tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#tpe-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Fit the pipeline on train data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.618` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement TPE using NNI with pure Python
    code. In the next section, we will learn how to implement Sequential Model Algorithm
    Configuration with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Sequential Model Algorithm Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip install "nni[SMAC]"`. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement SMAC with NNI using
    pure Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement SMAC with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are two parameters for the SMAC tuner: `optimize_mode`, and `config_dedup`.
    Please refer to the official documentation page for more information regarding
    the SMAC tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#smac-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.619` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement SMAC using NNI with pure Python
    code. In the next section, we will learn how to implement Bayesian Optimization
    Gaussian Process with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Gaussian Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayesian Optimization Gaussian Process** (**BOGP**) is one of the variants
    of the Bayesian Optimization hyperparameter tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036))
    that the NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement BOGP with NNI using
    pure Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement BOGP with the NNI package using pure
    Python code. You can find the more detailed code in the GitHub repository mentioned
    in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the model to be tuned in a script. Here, we’ll use a new script called
    `model_numeric.py`. In this script, we add a mapping for non-numeric hyperparameters
    since BOGP can only work with numerical hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the hyperparameter space in the form of a Python dictionary. We’ll use
    a similar hyperparameter space as in the previous section with the only difference
    on the non-numeric hyperparameters. Here, all of the non-numeric hyperparameters
    are encoded into integer types of values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are nine parameters for the BOGP tuner: `optimize_mode`, `utility`, `kappa`,
    `xi`, `nu`, `alpha`, `cold_start_num`, `selection_num_warm_up`, and `selection_num_starting_points`.
    Please refer to the official documentation page for more information regarding
    the BOGP tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#gp-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Fit the pipeline on train data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.619` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement BOGP using NNI with pure Python
    code. In the next section, we will learn how to implement Metis with NNI via pure
    Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Metis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Metis** is one of the variants of the Bayesian Optimization hyperparameter
    tuning group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement Metis with NNI using
    pure Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Metis with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. Here, we’ll use the same script as
    in the previous section, `model_numeric.py`, since Metis can only work with numerical
    hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are six parameters for the Metis tuner: `optimize_mode`, `no_resampling`,
    `no_candidates`, `selection_num_starting_points`, `cold_start_num`, and `exploration_probability`.
    Please refer to the official documentation page for more information regarding
    the Metis tuner parameters ([https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner](https://nni.readthedocs.io/en/stable/reference/hpo.html#metis-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.590` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Metis using NNI with pure
    Python code. In the next section, we will learn how to implement Simulated Annealing
    with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Simulated Annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simulated Annealing is one of the variants of the Heuristic Search hyperparameter
    tuning group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section, to implement Simulated Annealing
    with NNI using pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Simulated Annealing with the NNI
    package using pure Python code. You can find the more detailed code in the GitHub
    repository mentioned in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model.py` script
    as in the *Implementing Grid Search* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the *Implementing Grid Search* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there is one parameter for the Simulated Annealing tuner, namely `optimize_mode`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.600` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Simulated Annealing using
    NNI with pure Python code. In the next section, we will learn how to implement
    Hyper Band with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Hyper Band
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyper Band is one of the variants of the Multi-Fidelity Optimization hyperparameter
    tuning group (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)) that the
    NNI package can implement. Let’s use the same data, pipeline, and hyperparameter
    space as in the example in the previous section to implement Hyper Band with NNI
    using pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Hyper Band with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. Here, we’ll use a new script called
    `model_advisor.py`. In this script, we utilize the `TRIAL_BUDGET` value from the
    output of `nni.get_next_parameter()` to update the `'model__n_estimators'` hyperparameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the hyperparameter space in the form of a Python dictionary. We’ll use
    a similar hyperparameter space to the *Implementing Grid Search* section but we
    will remove the `''model__n_estimators''` hyperparameter since it will become
    the budget definition for Hyper Band:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are four parameters for the Hyper Band advisor: `optimize_mode`, `R`, `eta`,
    and `exec_mode`. Please refer to the official documentation page for more information
    regarding the Hyper Band advisor parameters ([https://nni.readthedocs.io/en/latest/reference/hpo.html](https://nni.readthedocs.io/en/latest/reference/hpo.html#hyperband-tuner)#hyperband-tuner):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: Fit the pipeline on train data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.593` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Hyper Band using NNI with
    pure Python code. In the next section, we will learn how to implement Bayesian
    Optimization Hyper Band with NNI via pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bayesian Optimization Hyper Band
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayesian Optimization Hyper Band** (**BOHB**) is one of the variants of the
    Multi-Fidelity Optimization hyperparameter tuning group (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054))
    that the NNI package can implement. Note that to use BOHB in NNI, we need to install
    additional dependencies using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Let’s use the same data, pipeline, and hyperparameter space as in the example
    in the previous section to implement BOHB with NNI using pure Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement Hyper Band with the NNI package using
    pure Python code. You can find the more detailed code in the GitHub repository
    mentioned in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the model to be tuned in a script. We’ll use the same `model_advisor.py`
    script as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in the form of a Python dictionary. We’ll use
    the same hyperparameter space as in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `Experiment` class. Note that
    there are 11 parameters for the BOHB advisor: `optimize_mode`, `min_budget`, `max_budget`,
    `eta`, `min_points_in_model`, `top_n_percent`, `num_samples`, `random_fraction`,
    `bandwidth_factor`, `min_bandwidth`, and `config_space`. Please refer to the official
    documentation page for more information regarding the Hyper Band advisor parameters
    ([https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner](https://nni.readthedocs.io/en/latest/reference/hpo.html#bohb-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the best set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model on full training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.617` in the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to implement Bayesian Optimization Hyper
    Band using NNI with pure Python code. In the next section, we will learn how to
    implement Population-Based Training with NNI via `nnictl`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Population-Based Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Population-Based Training** (**PBT**) is one of the variants of the Heuristic
    Search hyperparameter tuning group (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047))
    that the NNI package can implement. To show you how to implement PBT with NNI
    using pure Python code, let’s use the same example provided by the NNI package.
    Here, the MNIST dataset and a convolutional neural network model are utilized.
    We’ll use PyTorch to implement the neural network model. For details of the code
    example provided by NNI, please refer to the NNI GitHub repository ([https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch](https://github.com/microsoft/nni/tree/1546962f83397710fe095538d052dc74bd981707/examples/trials/mnist-pbt-tuner-pytorch)).'
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Dataset
  prefs: []
  type: TYPE_NORMAL
- en: MNIST is a dataset of handwritten digits that have been size-normalized and
    centered in a fixed-size image. Here, we’ll use the MNIST dataset provided directly
    by the PyTorch package ([https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to implement PBT with the NNI package. Here, we’ll
    use `nnictl` instead of using pure Python code. You can find the more detailed
    code in the GitHub repository mentioned in the *Technical requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the model to be tuned in a script. Here, we’ll use the same `mnist.py`
    script from the NNI GitHub repository. Note that we save the script with a new
    name: `model_pbt.py`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space in a JSON file called `hyperparameter_space_pbt.json`.
    Here, we’ll use the same `search_space.json` file from the NNI GitHub repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the experiment configurations via the `config_pbt.yaml` file. Note that
    there are six parameters for the PBT tuner: `optimize_mode`, `all_checkpoint_dir`,
    `population_size`, `factor`, `resample_probability`, and `fraction`. Please refer
    to the official documentation page for more information regarding the PBT tuner
    parameters ([https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner](https://nni.readthedocs.io/en/latest/reference/hpo.html#pbt-tuner)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hyperparameter tuning experiment. We can see the experiment status
    and various interesting stats via the launched web portal. The following code
    shows how to run the experiment on port `8080` in `local`, meaning you can open
    the web portal on `http://localhost:8080`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we have learned how to implement Population-Based Training
    with NNI via `nnictl` using the same example as provided in the official documentation
    of NNI.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned all the important things about the DEAP and
    Microsoft NNI packages. We also have learned how to implement various hyperparameter
    tuning methods with the help of these packages, along with understanding each
    of the important parameters of the classes and how are they related to the theory
    that we have learned in the previous chapters. From now on, you should be able
    to utilize these packages to implement your chosen hyperparameter tuning method,
    and ultimately, boost the performance of your ML model. Equipped with the knowledge
    from *Chapters 3 – 6*, you will also be able to debug your code if there are errors
    or unexpected results, and be able to craft your own experiment configuration
    to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn about hyperparameters for several popular algorithms.
    There will be a wide explanation for each of the algorithms, including (but not
    limited to) the definition of each hyperparameter, what will be impacted when
    the value of each hyperparameter is changed, and the priority list of hyperparameters
    based on the impact.
  prefs: []
  type: TYPE_NORMAL
