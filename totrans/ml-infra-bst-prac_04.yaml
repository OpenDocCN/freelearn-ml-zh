- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data in Software Systems – Text, Images, Code, and Their Annotations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) systems are data-hungry applications, and they
    like their data well prepared for training and inference. Although it may sound
    obvious, it is more important to scrutinize the properties of data than to select
    an algorithm to process the data. The data, however, can come in many different
    formats and can be from different sources. We can consider data in its raw format
    – for example, a text document or an image file. We can also consider data in
    a format that is specific to a task at hand – for example, tokenized text (where
    words are divided into tokens) or an image with bounding boxes (where objects
    are identified and enclosed in rectangles).'
  prefs: []
  type: TYPE_NORMAL
- en: When considering the end user system, what we can do with the data and how we
    handle the data becomes crucial. However, identifying important elements in the
    data and transforming it into a format that is useful for ML algorithms depends
    on what our task is and which algorithm we use. Therefore, in this chapter, we
    will work both with data and with algorithms to process it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce three data types – images, text, and formatted
    text (program source code). We will explore how each of these types of data can
    be used in ML, how they should be annotated, and for what purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing these three types of data provides us with the possibility to explore
    different ways of annotating these sources of data. Therefore, in this chapter,
    we will focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw data and features – what are the differences?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every data has its purpose – annotations and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where different types of data can be used together – an outlook on multi-modal
    data models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw data and features – what are the differences?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML systems are data-hungry. They rely on the data to be trained and to make
    inferences. However, not all data is equally important. Before the era of **deep
    learning** (**DL**), the data was supposed to be processed in order to be used
    in ML. Before DL, the algorithms were limited in the amount of data that could
    be used for training. The storage and memory limitations were also limited, and
    therefore, ML engineers had to prepare the data much more than for DL. For example,
    ML engineers needed to spend more effort to find a small but still representative
    sample of data for training. After the introduction of DL, ML models can find
    complex patterns in much larger datasets. Therefore, the work of ML engineers
    is now focused on finding sufficiently large, and representative, datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Classical ML systems – that is, non-DL systems – require data in a tabular form
    in order to make inferences, and therefore it is important to design the right
    feature extraction mechanisms for this kind of system.
  prefs: []
  type: TYPE_NORMAL
- en: 'DL systems, on the other hand, require minimal data processing and can learn
    patterns from data in its (almost) raw format. Minimal processing of data is needed
    as DL systems need a bit of different information about the data for different
    tasks; they also extract information from the raw data by themselves. For example,
    they can capture the context of a text without the need to manually process it.
    *Figure 3**.1* illustrates these differences between different types of data based
    on the tasks that can be performed on them. In this case, the data is in the form
    of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Types of learning systems and the data that they require for
    images](img/B19548_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Types of learning systems and the data that they require for images
  prefs: []
  type: TYPE_NORMAL
- en: Raw images are often used for further processing, but they can be used in such
    tasks as image classification. The task of image classification relates to when
    the input to the algorithm is the raw image, and the output is the class of the
    image. We often see these kinds of tasks when we talk about images that contain
    “cats,” “dogs,” or “cars.”
  prefs: []
  type: TYPE_NORMAL
- en: There are considerable practical applications of this task. One application
    is in insurance. Several insurance companies have changed their business model
    and digitalized their businesses. Before the mid-2010s, insurance companies required
    an initial visit to a workshop to make a first assessment of damage to cars. Today,
    that first damage is assessed automatically by image classification algorithms.
    We take a picture of the damaged part with a smartphone and send it to the insurance
    company’s software, where trained ML algorithms are used to make an assessment.
    In rare, difficult cases, the image needs to be scrutinized by a human operator.
    This kind of workflow saves money and time and provides a better experience for
    the handling of damage claims.
  prefs: []
  type: TYPE_NORMAL
- en: Another application is medical image classification, where radiology images
    are classified automatically to provide an initial diagnosis, and therefore, reduce
    the burden on medical specialists (in this case, radiologists).
  prefs: []
  type: TYPE_NORMAL
- en: Masked images are processed using filters to emphasize aspects of interest.
    Most often, these filters are black-and-white filters or grayscale filters. They
    emphasize the differences between light and dark parts of the images to make it
    easier to recognize shapes and then to classify these shapes and trace them (in
    the case of video feeds). This kind of application is often used in perception
    systems – for example, in cars.
  prefs: []
  type: TYPE_NORMAL
- en: One practical application of a perception system that uses masked images is
    the recognition of horizontal road markings, such as lane markings. The vehicle’s
    camera takes a picture of the road in front of the car, then its software masks
    the image and sends it to an ML algorithm for detection and classification. **OpenCV**
    is one of the libraries used for this kind of task. Other practical applications
    include face recognition or **optical character** **recognition** (**OCR**).
  prefs: []
  type: TYPE_NORMAL
- en: Semantic map images include overlays that describe what can be seen in the image,
    covering a part of an image that contains specific information such as the sky,
    a car, a person, or a building. A semantic map can cover a part of the image that
    contains a car, the road that the car is on, the surroundings, and the sky. The
    semantic map provides rich information about the image that is used for advanced
    vision perception algorithms, which, in turn, provide information for decision
    algorithms. Vision perception is particularly needed in automotive systems for
    autonomous vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: One of the applications of semantic maps is in the active safety systems of
    vehicles. Images captured by the front camera are processed using **convolutional
    neural networks** (**CNNs**) to add a semantic map and then used in decision algorithms.
    These decision algorithms either provide feedback to the driver or take actions
    autonomously. We can see that often when a car reacts to driving too close to
    another car or when it detects an obstacle in its way.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of semantic maps include medical image analyses, whereby
    ML algorithms provide input to medical specialists as to what the image contains.
    An example can be brain tumor segmentation using **deep** **CNNs** (**DCNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, bounding-box images contain information about the boundaries of objects
    in images. For each shape of interest, such as a car, pedestrian, or tumor, there
    is a bounding box surrounding that part of the image, annotated with the class
    of that shape. These kinds of images are used for detecting objects and providing
    that information to other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: One of the applications where we use this kind of image is object recognition
    in robot coordination systems. A robot’s camera registers an image, the CNN identifies
    objects, and the robot’s decision software traces the object in order to avoid
    collisions. Tracing each object is used to change the behavior of the autonomous
    robot to reduce the risk of collisions and damage, as well as to optimize the
    operations of the robot and its environment.
  prefs: []
  type: TYPE_NORMAL
- en: Hence my first best practice in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #14'
  prefs: []
  type: TYPE_NORMAL
- en: Design the entire software system based on the task that you need to solve,
    not only the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Since every kind of algorithm that we use requires different processing of the
    image and provides different kinds of information, we need to understand how to
    create the entire system around it. In the previous chapter, we discussed pipelines,
    which include only the ML data pipeline, but a software system requires much more.
    For safety-critical functionality, we need to design safety cages and signaling
    to reduce the risk of wrong classifications/detections from ML models. Therefore,
    we need to understand what we want to do – whether the information is only for
    making simple decisions (for example, damaged bumper in a car versus not) or whether
    the classification is part of complex behavior decisions (for example, should
    the robot turn to the right to avoid the obstacle or should it slow down to let
    another robot move past?).
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are one type of data that we use in ML; another one is text. The use
    of text has been popularized in recent years with the introduction of **recurrent
    NNs** (**RNNs**) and transformers. These NN architectures are DL networks that
    are capable of capturing the context (and, by extension, basic semantics) of words.
    These models find statistical connections between tokens (and therefore, words)
    and so can identify similarities that classical ML models are not capable of.
    Machine translations were a popular application of these models in the beginning,
    but now, the applications are much wider than this – for example, in understanding
    programming languages. *Figure 3**.2* shows the type of text data that can be
    used with different types of models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Types of learning systems and the data they require for text](img/B19548_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Types of learning systems and the data they require for text
  prefs: []
  type: TYPE_NORMAL
- en: The raw text data is used today in training `word2vec` model, which translates
    text tokens into vectors of numbers – embeddings – which are distances from that
    token to other tokens in the vocabulary. We saw an example of this in [*Chapter
    2*](B19548_02.xhtml#_idTextAnchor023), where we counted the number of words in
    sentences. By using this technology, the `word2vec` model captures the context
    of tokens, also known as their similarity. This similarity can be extended to
    entire sentences or even paragraphs, depending on the size and depth of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Another application of raw text, although in a structured format, is in **sentiment
    analysis** (**SA**). We use a tabular format of the text data in order to analyze
    whether the text’s sentiment is positive, negative, or neutral. An extension of
    that task is understanding the intent of the text – whether it is an explanation,
    a query, or a description.
  prefs: []
  type: TYPE_NORMAL
- en: Masked text data refers to when we mask one or more tokens in a sequence of
    tokens, and we train the models to predict the token. This is an example of self-supervised
    training as the model is trained on data that is not annotated, but by masking
    tokens in different ways (for example, random, based on similarity, human annotations),
    the model can understand which tokens can be used in that specific context. The
    larger the model – the transformer – the more data is needed, and a more complex
    training process is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, annotated text refers to when we label pieces of text with a specific
    class, just as with images. An example of such annotation is a sentiment. Then,
    the model captures patterns in the data, and therefore, can repeat these patterns.
    An example of a task in this area is sentiment recognition, where the model is
    trained to recognize whether a piece of text is positive or negative in its tone.
  prefs: []
  type: TYPE_NORMAL
- en: 'A special case of textual data is programming language source code. The use
    of ML models for programming language tasks has become more popular in the last
    few years as it provides the possibility to increase the speed and quality of
    software development. *Figure 3**.3* shows types of programming language data
    and typical tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Types of programming language data and typical tasks](img/B19548_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Types of programming language data and typical tasks
  prefs: []
  type: TYPE_NORMAL
- en: Raw source code data is used for tasks that are related to programming language
    understanding – for example, translations between different programming languages,
    such as the one using the TransCoder model. This task is similar to translation
    between natural languages, although it adds additional steps to make the program
    compile and pass test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Masked programming language code is often used for training models with the
    purpose of repairing defects – the model is trained on a set of programs that
    correct defects and then applied on programs with defects. Masked programs are
    used to train models that can identify problems and provide fixes for them. These
    tasks are quite experimental at the moment of writing this book but with very
    promising results.
  prefs: []
  type: TYPE_NORMAL
- en: Annotated source code is used for a variety of tasks. These tasks include defect
    predictions, code reviews, and identification of design patterns or company-specific
    design rules. ML models provide much better results than any other techniques
    for these tasks – for example, compared to static code analysis tools.
  prefs: []
  type: TYPE_NORMAL
- en: Source code is used to train models for advanced software engineering tasks,
    such as creating programs. GitHub Copilot is one such tool that has been very
    successful, both in research and in commercial applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the aforementioned three types of data illustrate only a small number of
    applications of ML. The sky is the limit for those who want to utilize ML models
    for designing software systems. Before designing the systems, however, we need
    to understand how we work with data in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Raw image data is often stored in files with annotations in other files. Raw
    image data presents aspects that are relevant to the system in question. An example
    of data used for training active safety algorithms is presented in *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Front-camera image from a vehicle](img/B19548_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Front-camera image from a vehicle
  prefs: []
  type: TYPE_NORMAL
- en: The image with the car is used, in this example, to train a CNN to recognize
    whether it is safe to drive (for example, whether the road ahead is free from
    obstacles). With the data annotated on the image level – that is, without masks
    and bounding boxes – the ML models can either classify the entire image or identify
    objects. When identifying objects, the models add bounding-box information to
    the images.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train a CNN for images that contain many objects of significant
    size (such as high-definition resolution of 1920 x 1080 pixels), we need both
    large datasets and large computational resources. There are a few reasons for
    this.
  prefs: []
  type: TYPE_NORMAL
- en: First, colors require a lot of data to be recognized correctly. Although we
    humans see the color red as almost uniform, the actual pixel intensity of that
    color varies a lot, which means that we need to create a CNN so that it can understand
    that different shades of red are sometimes important to recognize braking vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the large size of images contains details that are not relevant. *Figure
    3**.5* presents how a CNN is designed. This is a CNN in a LeNet style:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Conceptual design of a CNN](img/B19548_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Conceptual design of a CNN
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.5* shows that the NN takes as input an image of size 192 x 108
    pixels (10 times smaller than an HD-quality image). It then uses `MaxPool` layers
    (for example) to reduce the number of elements, and then it uses convolutions
    to identify shapes. Finally, it uses two dense layers to classify the images into
    a vector of 64 different classes. The size of the images determines the complexity
    of the network. The larger the images, the more convolutions are required, and
    the larger the first layer. Larger networks take more time to train (the difference
    may be measured in days) and require more data (the difference may be measured
    in tens of thousands of images, depending on the number of classes and quality
    of images).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, for many applications, we use grayscale images and downsize them
    significantly. *Figure 3**.6* shows the same image as previously but in grayscale,
    downsized to 192 x 108 pixels. The size of the image has been significantly reduced
    and so have the requirements for the first convolutional layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Black-and-white transformed image (lower-quality lossy transformation
    illustrated on purpose)](img/B19548_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Black-and-white transformed image (lower-quality lossy transformation
    illustrated on purpose)
  prefs: []
  type: TYPE_NORMAL
- en: However, the object in the image is still perfectly visible and can be used
    for further analysis. Therefore, here is the next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #15'
  prefs: []
  type: TYPE_NORMAL
- en: Downsize the size of your images and use as few colors as possible to reduce
    the computational complexity of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Before designing the system, we need to understand what kinds of images we have
    and how we can use them. Then, we can perform these kinds of transformations so
    that the system that we design can handle the tasks that it is designed for. However,
    it’s important to note that downsizing images can also result in loss of information,
    which can affect the accuracy of the ML model. It’s important to carefully balance
    the trade-offs between computational complexity and information loss when deciding
    how to preprocess images for an ML task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downsizing and converting images to grayscale is a common practice in ML. In
    fact, there exist several well-known and widely used benchmark datasets that use
    this technique. One of them is the `MNIST` dataset of handwritten numbers. The
    dataset is available for download as part of the most popular ML libraries such
    as TensorFlow and Keras. Just use the following code to get hold of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code illustrates how to download the dataset, which is already split into
    `test` and `train` data, with annotations. It also shows how to visualize the
    dataset, which results in the images seen in *Figure 3**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Visualization of the first few images in the MNIST dataset;
    the images are rasterized on purpose to illustrate their real size](img/B19548_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Visualization of the first few images in the MNIST dataset; the
    images are rasterized on purpose to illustrate their real size
  prefs: []
  type: TYPE_NORMAL
- en: The size of the images in the MNIST dataset is 28 x 28 pixels, which is perfectly
    sufficient to train and test new ML models. Although the dataset is well known
    and used in ML, it is relatively small and uniform – only grayscale numbers. Therefore,
    for more advanced tasks, we should look for more diverse datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Images of handwritten numbers are naturally useful, but we often want to use
    more complex images, and therefore, the standard libraries contain images with
    more than just 10 classes (number of digits). One such dataset is the Fashion-MNIST
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download it by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The code, which we can use as part of the visualization shown in *Figure 3**.7*,
    produces the set of images seen in *Figure 3**.8*. The images are of the same
    size and the same number of classes, but with a larger complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Fashion-MNIST dataset; the images are rasterized on purpose
    to illustrate their real size](img/B19548_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Fashion-MNIST dataset; the images are rasterized on purpose to
    illustrate their real size
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can also use libraries that contain color images, such as the CIFAR-10
    dataset. The dataset can be accessed with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The dataset contains images of 10 different classes of similar size (32 x 32
    pixels), but with colors, as shown in *Figure 3**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – CIFAR-10 dataset; the images are rasterized on purpose to illustrate
    their real size](img/B19548_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – CIFAR-10 dataset; the images are rasterized on purpose to illustrate
    their real size
  prefs: []
  type: TYPE_NORMAL
- en: This is not the end of such benchmark datasets. Some datasets contain more classes,
    larger images, or both. Therefore, it is important to take a look at these datasets
    beforehand and in connection with the task that our system has to perform.
  prefs: []
  type: TYPE_NORMAL
- en: In the majority of cases, grayscale images are perfectly fine for classification
    tasks. They provide the ability to quickly get an orientation in the data, and
    they are small enough that the classification is of good quality.
  prefs: []
  type: TYPE_NORMAL
- en: The usual size of the benchmark datasets is about 50,000–100,000 images. This
    illustrates that even for such a small number of classes and for such small images,
    the number is significant. Just imagine annotating those 100,000 images. For more
    complex images, the size of the datasets can be significantly larger. For example,
    the BDD100K dataset used in automotive software contains over 100,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, here is my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #16'
  prefs: []
  type: TYPE_NORMAL
- en: Use a reference dataset (such as MNIST or STL) for benchmarking whether the
    system works or not.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand whether the entire system works or not, such benchmark
    datasets are very useful. They provide us with a preconfigured train/test split,
    and there are plenty of algorithms that can be used to understand the quality
    of our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We should also consider the next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #17'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever possible, use models that are already pre-trained for specific tasks
    (for example, neural models for image classification or semantic segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: Just as we should strive to reuse images for benchmarking, we should also strive
    to reuse models that are pre-trained. This saves previous design resources and
    reduces the risk of spending too much time to find optimal architectures of NN
    models or to find the optimal set of parameters (even if we use `GradientSearch`
    algorithms).
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the types of analysis done on text is SA – classification of whether
    a piece of text (a sentence, for example) is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.10* presents an example of data that can be used for SA. The data
    is publicly available and has been created from Amazon product reviews. Data for
    this kind of analysis is often structured in a table, where we have entities such
    as `ProductId` (I’ve truncated the `Id` columns for brevity) or `UserId`, as well
    as `Score` for reference and `Text` to classify.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This data structure provides us with the possibility to quickly summarize the
    text and visualize it. The visualization can be done in several ways – for example,
    by plotting a histogram of the scores. However, the most interesting visualizations
    are the ones that are provided by the statistics of words/tokens used in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Id | ProductId | UserId | Score | Summary | Text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | B001KFG0 | UHU8GW | 5 | Good Quality Dog Food | I have bought several
    of the Vitality canned dog food products and have found them all to be of good
    quality. The product looks more like a stew than a processed meat and it smells
    better. My Labrador is finicky and she appreciates this product better than most.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | B008GRG4 | ZCVE5NK | 1 | Not as Advertised | Product arrived labeled
    as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not
    sure if this was an error or if the vendor intended to represent the product as
    “Jumbo”. |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | B000OCH0 | WJIXXAIN | 4 | “Delight” says it all | This is a confection
    that has been around a few centuries. It is a light, pillowy citrus gelatin with
    nuts - in this case Filberts. And it is cut into tiny squares and then liberally
    coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy,
    and very flavorful. I highly recommend this yummy treat. If you are familiar with
    the story of C.S. Lewis’ “The Lion, The Witch, and The Wardrobe” - this is the
    treat that seduces Edmund into selling out his Brother and Sisters to the Witch.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | B000A0QIQ | C6FGVXV | 2 | Cough Medicine | If you are looking for the
    secret ingredient in Robitussin I believe I have found it. I got this in addition
    to the Root Beer Extract I ordered (which was good) and made some cherry soda.
    The flavor is very medicinal. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | B0062ZZ7K | LF8GW1T | 5 | Great taffy | Great taffy at a great price.
    There was a wide assortment of yummy taffy. Delivery was very quick. If your a
    taffy lover, this is a deal. |'
  prefs: []
  type: TYPE_TB
- en: Figure 3.10 – Example of data for product reviews, structured for SA; only the
    first five rows are shown
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to visualize the data is to use the word cloud visualization technique.
    A simple script for visualizing this kind of data is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running this script is shown in *Figure 3**.11*. A word cloud
    shows trends in terms of frequency of the use of words – words used more frequently
    are larger than words used less frequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Word cloud visualization of the Text column](img/B19548_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Word cloud visualization of the Text column
  prefs: []
  type: TYPE_NORMAL
- en: Hence, my next best practice is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #18'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize your raw data to get an understanding of patterns in your data.
  prefs: []
  type: TYPE_NORMAL
- en: Visual representation of data is important to understand the underlying patterns.
    I cannot stress that enough. I use both Python’s Matplotlib and Seaborn as well
    as visual analytics tools such as TIBCO Spotfire to plot charts and understand
    my data. Without such a visualization, and thus without such an understanding
    of the patterns, we are bound to make wrong conclusions and even design systems
    with flaws that are difficult to remove without a complete redesign.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of output from more advanced text processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization of text helps us to understand what the text contains, but it
    does not capture the meaning of it. In this book, we will work with advanced text
    processing algorithms – feature extractors. Therefore, we need to understand how
    to create visualizations of the output from these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of working with feature extraction is to use word embeddings – a method
    to convert words or sentences into vectors of numbers. `word2vec` is one model
    that can do that, but there are more powerful ones too. OpenAI’s GPT-3 model is
    one of the largest models that are openly available. Obtaining embeddings of paragraphs
    is quite straightforward. First, we connect to the OpenAI API and then query it
    for the embeddings. Here is the code that does the querying of the OpenAI API
    (in boldface):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'What we obtain by running this piece of code is `5` vectors (one for each row)
    of `2048` numbers, which we call embeddings. The entire vector is too large to
    be included in the page, but the first elements look something like this: `[-0.005302980076521635,
    0.018141526728868484, -0.018141526728868484,` `0.004692177753895521, …`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These numbers do not say much to us humans, but they have a meaning for the
    language model. The meaning is in the distance between them – words/tokens/sentences
    that are like one another are placed closer than words/tokens/sentences that are
    not similar. In order to understand these similarities, we use transformations
    that reduce the dimensions – one of them is **t-distribution Stochastic Neighbor
    Embedding** (**t-SNE**). *Figure 3**.12* presents this kind of visualization of
    the five embeddings that we obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – t-SNE visualization of the embedding vectors for the five reviews](img/B19548_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – t-SNE visualization of the embedding vectors for the five reviews
  prefs: []
  type: TYPE_NORMAL
- en: Each dot represents one review, and each cross represents the center of the
    cluster. The clusters are designated by the `Score` column from the original dataset.
    The screenshot shows that the text in each of the reviews is different (dots do
    not overlap) and that the clusters are separate – the crosses are positioned in
    a different part of the screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, my next best practice is about that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #19'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize your data when it has been turned into features to monitor whether
    the same patterns are still observable.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with the previous best practice, we need to visualize the data to check
    whether patterns that we observed in the raw data are still observable. This step
    is important as we need to know that the ML model can indeed learn these patterns.
    As these models are statistical in nature, they always capture patterns, but when
    a pattern is not there, they capture something that is not useful – even though
    it resembles a pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Structured text – source code of programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The source code of programs is a special case of text data. It has the same
    type of modality – text – but it contains additional information in the form of
    the grammatical/syntactical structure of the program. Since every programming
    language is based on grammar, there are specific rules for how a program should
    be structured. For instance, in C, there should be a specific function called
    `main`, which is the entry point for the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'These specific rules make the text structured in a specific way. They may make
    it more difficult to understand the text for a human being, but this structure
    can certainly be helpful. One of the models that uses this structure is `code2vec`
    ([https://code2vec.org/](https://code2vec.org/)). The `code2vec` model is similar
    to word2vec, but it takes as input the **Abstract Syntax Tree** (**AST**) of the
    program that it analyzes – for example, the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be represented by the AST in *Figure 3**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – AST for a simple “Hello World” program](img/B19548_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – AST for a simple “Hello World” program
  prefs: []
  type: TYPE_NORMAL
- en: The example program is visualized as a set of instructions with their context
    and the role that they play in the program. For example, the words `void` and
    `main` are two parts of the method declaration, together with the block statement
    (`BlockStmt`), which is the body of the method.
  prefs: []
  type: TYPE_NORMAL
- en: '`code2vec` is an example of a model that uses programming language information
    (in this case, grammar) as input to the model. Tasks that the model can do include
    finding similarities between words (such as word2vec models), finding combinations,
    and identifying analogies. For example, the model can identify all combinations
    of the words `int` and `main` and provide the following answers (with probabilities):
    `realMain` (71%), `isInt` (71%), and `setIntField` (69%). These tasks, by extension,
    can be used for program repair, where the model can identify mistakes and repair
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: However, using an AST or similar information has disadvantages. The main disadvantage
    is that the analyzed program must compile. This means that we cannot use these
    kinds of models in the context where we want to analyze programs that are incomplete
    – for example, in the context of **continuous integration** (**CI**) or modern
    code reviews. When we analyze only a small part of the code, the model cannot
    parse it, obtain its AST, and use it. Therefore, here’s my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #20'
  prefs: []
  type: TYPE_NORMAL
- en: Only use the necessary information as input to ML models. Too much information
    may require additional processing and make the training hard to converge (finish).
  prefs: []
  type: TYPE_NORMAL
- en: When designing the processing pipeline, make sure that the information provided
    to the model is necessary, as every piece of information poses new requirements
    for the entire software system. As in the example of an AST, when it is necessary,
    it is powerful information, but if not available, it can be a huge hindrance to
    getting the data analysis pipeline to work.
  prefs: []
  type: TYPE_NORMAL
- en: Every data has its purpose – annotations and tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data in raw format is important, but only the first step in the development
    and operations of ML software. The most important part, and the costliest one,
    is the annotation of the data. To train an ML model and then use it to make inferences,
    we need to define a task. Defining a task is both conceptual and operational.
    The conceptual definition is to define what we want the software to do, but the
    operational definition is how we want to achieve that goal. The operational definition
    boils down to a definition of what we see in the data and what we want the ML
    model to identify/replicate.
  prefs: []
  type: TYPE_NORMAL
- en: Annotations are the mechanisms by which we direct the ML algorithms. Every piece
    of data that we use requires some sort of label to denote what it is. In the raw
    format of the data, this annotation can be a label of what the data point contains.
    For example, such a label can be that the image contains the number 1 (from the
    MNIST dataset) or a car (from the CIFAR-10 dataset). However, these simple annotations
    are important only in dedicated tasks. For more advanced tasks, the annotations
    need to be richer.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the types of annotations relates to when we designate part of the data
    as interesting. In the case of images, this is done by drawing bounding boxes
    around objects of interest. *Figure 3**.14* presents such an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Image with bounding boxes](img/B19548_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Image with bounding boxes
  prefs: []
  type: TYPE_NORMAL
- en: The image contains boxes around elements that we want the model to recognize.
    In this case, we want to recognize objects that are vehicles (green boxes), other
    road users (orange boxes), and important background objects (gray boxes). These
    kinds of annotations are used for the ML model to learn shapes and to identify
    such shapes in new objects. In the case of this example, the bounding boxes identify
    elements that are important for active safety systems in cars, but this is not
    the only application.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of such bounding boxes include medical image analysis, where
    the task is to identify tissue that needs further analysis. These could also be
    systems for face recognition and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Although this task, and the bounding boxes, could be seen as a special case
    of annotating raw data, it is a bit different. Every box could be seen as a separate
    image that has a label, but the challenge is that every box is of a different
    size. Therefore, using such differently shaped images would require preprocessing
    (for example, rescaling). It would also only work for the training because, in
    the inference, we would need to identify objects before they are classified –
    and that’s exactly the task we need the NN to do for us.
  prefs: []
  type: TYPE_NORMAL
- en: My best practice for using this kind of data is set out next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #21'
  prefs: []
  type: TYPE_NORMAL
- en: Use bounding boxes in the data when the task requires the detection and tracking
    of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Since bounding boxes allow us to identify objects, the natural use of this data
    is in tracking systems. An example application is a system that monitors parking
    spots using a camera. It detects parking spots and tracks whether there is a vehicle
    parked in the spot or not.
  prefs: []
  type: TYPE_NORMAL
- en: An extension of the object detection task is a perception task, where our ML
    software needs to make decisions based on the context of the data – or a situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For image data, this context can be described by a semantic map. *Figure 3**.15*
    shows such a map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Image with a semantic map; building is one of the labels for
    the semantic map](img/B19548_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Image with a semantic map; building is one of the labels for the
    semantic map
  prefs: []
  type: TYPE_NORMAL
- en: The screenshot shows an overlay of different colors that cover objects of specific
    types. The orange overlay shows vehicles, while the purple one shows vulnerable
    road users, which is the pedestrian in this image. Finally, the pink color indicates
    buildings, and the red color covers the background/sky.
  prefs: []
  type: TYPE_NORMAL
- en: The semantic map provides more flexibility than bounding boxes (as some objects
    are more interesting than others) and allows the ML system to get the context
    of the image. By identifying what kinds of elements are present in the image,
    the ML model can provide information about where the image was taken to the decision
    algorithm of the software system that we design.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, here’s my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #22'
  prefs: []
  type: TYPE_NORMAL
- en: Use semantic maps when you need to get the context of the image or you need
    details of a specific area.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic maps require heavy computations to be used effectively; therefore,
    we should use them scarcely. We should use these maps when we have tasks related
    to the context, such as perception algorithms or image alteration – for example,
    changing the color of the sky in the image. Regarding the accuracy of the information,
    it is generally true that semantic maps require heavy computations and are therefore
    used selectively. An example of a tool that does such a semantic mapping is Segments.ai.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic maps are useful when the task requires understanding the context of
    an image or details of a specific area. For example, in autonomous driving, a
    semantic map can be used to identify objects on the road and their relationship
    to each other, allowing the vehicle to make informed decisions about its movement.
    However, specific use cases for semantic maps may vary depending on the application.
  prefs: []
  type: TYPE_NORMAL
- en: Annotating text for intent recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SA, which we mentioned before, is only one type of annotation of textual data.
    It is useful for assessing whether the text is positive or negative. However,
    instead of annotating text with a sentiment, we can annotate the text with – for
    example – the intent and train an ML model to recognize intent from other text
    passages. The table in *Figure 3**.16* provides such an annotation, based on the
    same review data as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Id** | **Score** | **Summary** | **Text** | **Intent** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | Good Quality Dog Food | I have bought several of the Vitality canned
    dog food products and have found them all to be of good quality. The product looks
    more like a stew than a processed meat and it smells better. My Labrador is finicky
    and she appreciates this product better than most. | Advertise |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | Not as Advertised | Product arrived labeled as Jumbo Salted Peanuts...the
    peanuts were actually small sized unsalted. Not sure if this was an error or if
    the vendor intended to represent the product as “Jumbo”. | Criticize |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4 | “Delight” says it all | This is a confection that has been around
    a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case
    Filberts. And it is cut into tiny squares and then liberally coated with powdered
    sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful.
    I highly recommend this yummy treat. If you are familiar with the story of C.S.
    Lewis’ “The Lion, The Witch, and The Wardrobe” - this is the treat that seduces
    Edmund into selling out his Brother and Sisters to the Witch. | Describe |'
  prefs: []
  type: TYPE_TB
- en: "| 4 | 2 | Cough\L Medicine | If you are looking for the secret ingredient in\
    \ Robitussin I believe I have found it. I got this in addition to the Root Beer\
    \ Extract I ordered (which was good) and made some cherry soda. The flavor is\
    \ very medicinal. | Criticize |"
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5 | Great taffy | Great taffy at a great price. There was a wide assortment
    of yummy taffy. Delivery was very quick. If your a taffy lover, this is a deal.
    | Advertise |'
  prefs: []
  type: TYPE_TB
- en: Figure 3.16 – Textual data annotation for intent recognition
  prefs: []
  type: TYPE_NORMAL
- en: 'The last column of the table shows the annotation of the text – the intent.
    Now, we can use the intent as a label to train an ML model to recognize intent
    in the new text. Usually, this task requires a two-step approach, as shown in
    *Figure 3**.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – A two-step approach for training models based on text](img/B19548_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – A two-step approach for training models based on text
  prefs: []
  type: TYPE_NORMAL
- en: The annotated text is organized into two parts. The first part is the text itself
    (for example, the `Text` column in our example table), and the second part is
    the annotation (e.g., the `Intent` column in our example). The text is processed
    using a model such as the word2vec model or a transformer, which encodes the text
    as a vector or a matrix. The annotations are encoded into a vector using techniques
    such as one-hot encoding so that they can be used as decision classes for the
    classification algorithm. The classification algorithm takes both the encoded
    annotations and the vectorized text. Then, it is trained to find the best fit
    of the vectorized text (*X*) to the annotation (*Y*).
  prefs: []
  type: TYPE_NORMAL
- en: Here is my best practice on how to perform that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #23'
  prefs: []
  type: TYPE_NORMAL
- en: Use a pre-trained embedding model such as GPT-3 or an existing BERT model to
    vectorize your text.
  prefs: []
  type: TYPE_NORMAL
- en: From my experience, working with text is often easier if we use a predefined
    language model to vectorize the text. The *Hugging Face* website ([www.huggingface.com](http://www.huggingface.com))
    is an excellent source of these models. Since LLMs require significant resources
    to train, the existing models are often good enough for most of the tasks. Since
    we develop the classifier model as the next step in the pipeline, we can focus
    our efforts on making that model better and align it with our task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of annotation of text data is the context in terms of the **part
    of speech** (**POS**). It can be seen as the semantic map used in the image data.
    Each word is annotated, whether it is a noun, verb, or adjective, regardless of
    which part of the sentence it belongs to. An example of such an annotation can
    be presented visually, using the Allen Institute’s AllenNLP **Semantic Role Labeling**
    (**SRL**) tool ([https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling)).
    *Figure 3**.18* presents a screenshot of such labeling for a simple sentence,
    while *Figure 3**.19* presents the labeling for a more complex one:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.18 – SRL using \uFEFFthe AllenNLP toolset](img/B19548_03_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – SRL using the AllenNLP toolset
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sentence, the role of each word is emphasized, and we see that there
    are three verbs with different associations – the last one is the main one as
    it links the other parts of the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – SRL for a more complex sentence](img/B19548_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – SRL for a more complex sentence
  prefs: []
  type: TYPE_NORMAL
- en: The complex sentence has a larger semantic role frame, as it contains two distinct
    parts of the sentence. We use this role labeling in order to extract the meaning
    of passages of text. It is particularly useful when designing software systems
    based on so-called *grounded models*, which are models that check the information
    toward a ground truth. Such models parse the text data, find the right anchor
    (for example, what the question is about), and find the relevant answer in their
    database. These are opposed to *ungrounded models*, which create answers based
    on which word fits best to finish the sentence – for example, ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, my best practice is shown next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #24'
  prefs: []
  type: TYPE_NORMAL
- en: Use role labels when designing software that needs to provide grounded decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Grounded decisions are often more difficult to provide, as the model needs to
    understand the context of the sentence, capture its meaning, and provide relevant
    answers. However, this is not always needed or even desired. Ungrounded models
    are often good enough for suggestions that can be later fixed by specialists.
    An example of such a software tool is ChatGPT, which provides answers that are
    sometimes incorrect and require manual intervention. However, they are a very
    good start.
  prefs: []
  type: TYPE_NORMAL
- en: Where different types of data can be used together – an outlook on multi-modal
    data models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced three types of data – images, text, and structured text.
    These three types of data are examples of data that is in a numerical form, such
    as matrices of numbers, or in forms of time series. Regardless of the form, however,
    working with data and ML systems is very similar. We need to extract the data
    from a source system, then transform it into a format that we can annotate, and
    then use this as input to an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: When we consider different types of data, we could start to think about whether
    we could use two types of data in the same system. There are a few ways of doing
    that. The first one is when we use different ML systems in different pipelines,
    but we connect the pipelines. GitHub Copilot is such a system. It uses a pipeline
    for processing a natural language to find similar programs and to transform them
    so that they fit the context of the program being developed now.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is a system that generates textual descriptions of images. It
    takes an image as an input, identifies objects in it, and then generates text
    based on these objects. The generation of text is done by a completely different
    ML model than the classification of images.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are new models that use two different modalities – images and
    texts – in the same NN – for example, the Gato model. By using the input from
    two sources and using a very narrow (in terms of the number of neurons) network
    in the middle, the model is trained to generalize concepts described by two different
    modalities. In this way, the model is trained to understand that an image of a
    cat and the word “cat” should be placed in the same embedding space very close
    to one another, if not exactly in the same places. Although still experimental,
    these kinds of networks are intended to mimic the human understanding of concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive a bit deeper into the understanding of data
    by making a deeper dive into the process of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Tao, J. et al., An object detection system based on YOLO in traffic scene.
    In 2017 6th International Conference on Computer Science and Network Technology
    (ICCSNT).* *2017\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Artan, C.T. and T. Kaya, Car Damage Analysis for Insurance Market Using Convolutional
    Neural Networks. In International Conference on Intelligent and Fuzzy Systems.*
    *2019\. Springer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nakaura, T. et al., A primer for understanding radiology articles about machine
    learning and deep learning. Diagnostic and Interventional Imaging, 2020\. 101(12):*
    *p. 765-770.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bradski, G., The OpenCV Library. Dr. Dobb’s Journal: Software Tools for the
    Professional Programmer, 2000\. 25(11):* *p. 120-123.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memon, J. et al., Handwritten optical character recognition (OCR): A comprehensive
    systematic literature review (SLR). IEEE Access, 2020\. 8:* *p. 142642-142668.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mosin, V. et al., Comparing autoencoder-based approaches for anomaly detection
    in highway driving scenario images. SN Applied Sciences, 2022\. 4(12):* *p. 1-25.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zeineldin, R.A. et al., DeepSeg: deep neural network framework for automatic
    brain tumor segmentation using magnetic resonance FLAIR images. International
    journal of computer assisted radiology and surgery, 2020\. 15(6):* *p. 909-920.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reid, R. et al., Cooperative multi-robot navigation, exploration, mapping
    and object detection with ROS. In 2013 IEEE Intelligent Vehicles Symposium (IV).*
    *2013\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mikolov, T. et al., Recurrent neural network based language model. In Interspeech.*
    *2010\. Makuhari.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vaswani, A. et al., Attention is all you need. Advances in neural information
    processing systems,* *2017\. 30.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ma, L. and Y. Zhang, Using Word2Vec to process big text data. In 2015 IEEE
    International Conference on Big Data (Big Data).* *2015\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ouyang, X. et al., Sentiment analysis using convolutional neural network.
    In 2015 IEEE International Conference on Computer and Information Technology;
    ubiquitous computing and communications; dependable, autonomic and secure computing;
    pervasive intelligence and computing.* *2015\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Roziere, B. et al., Unsupervised translation of programming languages. Advances
    in Neural Information Processing Systems, 2020\. 33:* *p. 20601-20611.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yasunaga, M. and P. Liang, Break-it-fix-it: Unsupervised learning for program
    repair. In International Conference on Machine Learning.* *2021\. PMLR.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Halali, S. et al., Improving defect localization by classifying the affected
    asset using machine learning. In International Conference on Software Quality.*
    *2019\. Springer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ochodek, M. et al., Recognizing lines of code violating company-specific coding
    guidelines using machine learning. In Accelerating Digital Transformation. 2019,
    Springer.* *p. 211-251.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nguyen, N. and S. Nadi, An empirical evaluation of GitHub copilot’s code suggestions.
    In Proceedings of the 19th International Conference on Mining Software* *Repositories.
    2022.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zhang, C.W. et al., Pedestrian detection based on improved LeNet-5 convolutional
    neural network. Journal of Algorithms & Computational Technology, 2019\. 13:*
    *p. 1748302619873601.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LeCun, Y. et al., Gradient-based learning applied to document recognition.
    Proceedings of the IEEE, 1998\. 86(11):* *p. 2278-2324.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xiao, H., K. Rasul, and R. Vollgraf, Fashion-MNIST: a novel image dataset
    for benchmarking machine learning algorithms. arXiv preprint* *arXiv:1708.07747,
    2017.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recht, B. et al., Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv preprint*
    *arXiv:1806.00451, 2018.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robert, T., N. Thome, and M. Cord, HybridNet: Classification and reconstruction
    cooperation for semi-supervised learning. In Proceedings of the European Conference
    on Computer Vision (**ECCV). 2018.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yu, F. et al., Bdd100k: A diverse driving video database with scalable annotation
    tooling. arXiv preprint arXiv:1805.04687, 2018\. 2(5):* *p. 6.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*McAuley, J.J. and J. Leskovec, From amateurs to connoisseurs: modeling the
    evolution of user expertise through online reviews. In Proceedings of the 22nd
    International Conference on World Wide* *Web. 2013.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Van der Maaten, L. and G. Hinton, Visualizing data using t-SNE. Journal of
    Machine Learning Research,* *2008\. 9(11).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sengupta, S. et al., Automatic dense visual semantic mapping from street-level
    imagery. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.*
    *2012\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Palmer, M., D. Gildea, and N. Xue, Semantic role labeling. Synthesis Lectures
    on Human Language Technologies, 2010\. 3(1):* *p. 1-103.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reed, S. et al., A generalist agent. arXiv preprint* *arXiv:2205.06175, 2022.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
