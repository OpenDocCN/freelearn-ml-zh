- en: Number Plate Recognition using SVM and Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces us to the steps needed to create an application for
    **Automatic Number Plate Recognition** (**ANPR**). There are different approaches
    and techniques based on different situations, for example, IR camera, fixed car
    position, light conditions, and so on. We can proceed to construct an ANPR application
    to detect automobile license plates in a photograph taken between 2 or 3 meters
    from a car, in ambiguous light condition and with non-parallel ground with minor
    perspective distortions in the automobile's plate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main purpose of this chapter is to introduce us to image segmentation and
    feature extraction, pattern recognition basics, and two important pattern recognition
    algorithms: that are **Support Vector Machine** (**SVM**) and **Artificial Neural
    Network** (**ANN**). In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ANPR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plate detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plate recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to ANPR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic Number Plate Recognition, or known by other terms such as **Automatic
    License-Plate Recognition** (**ALPR**), **Automatic Vehicle Identification** (**AVI**),
    or **Car Plate Recognition** (**CPR**), is a surveillance method that uses **Optical
    Character Recognition** (**OCR**) and other methods such as segmentations and
    detection to read vehicle registration plates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best results in an ANPR system can be obtained with an **Infrared** (**IR**)
    camera, because the segmentation steps for detection and OCR segmentation are
    easy, and clean, and they minimize errors. This is due to the laws of light, the
    basic one being that the angle of incidence equals the angle of reflection. We
    can see this basic reflection when we see a smooth surface such as a plane mirror.
    Reflection off of rough surfaces such as paper, leads to a type of reflection
    known as diffuse or scatter reflection. However, the majority of country plates
    have special characteristics named retro-reflection, that is, the surface of the
    plate is made with a material that is covered with thousands of tiny hemispheres
    that cause light to be reflected back to the source, as we can see in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we use a camera with filter-coupled, structured infrared light projector,
    we can retrieve just the Infrared light, and then, we have a very high quality
    image to segment, with which we can subsequently detect and recognize the plate
    number that is independent of any light environment, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: We will not use IR photographs in this chapter; we will use regular photographs
    so that we do not obtain the best results, and we get a higher level of detection
    errors and higher false recognition rate, as opposed to if we used an IR camera.
    However, the steps for both are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Each country has different license plate sizes and specifications. It is useful
    to know these specifications in order to get the best results and reduce errors.
    Algorithms used in every chapter are designed for explaining the basics of ANPR
    and concrete for license plates used in Spain, but we can extend it to any country
    or specification.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will work with license plates from Spain. In Spain, there
    are three different sizes and shapes of license plates, but we will only use the
    most common (large) license plate, which has a 520 mm width by a 110 mm height.
    Two groups of characters are separated by a 41 mm space, and a 14 mm width separates
    each individual character. The first group of characters have four numeric digits,
    and the second group has three letters without the vowels A, E, I, O, U, or the
    letters N or Q. All characters have dimensions of 45 mm by 77 mm.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data is important for character segmentation since we can check both the
    character and blank spaces to verify that we get a character and no other image
    segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_003.png)'
  prefs: []
  type: TYPE_IMG
- en: ANPR algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before explaining the ANPR code, we need to define the main steps and tasks
    in the ANPR algorithm. ANPR is divided in two main steps: plate detection and
    plate recognition. Plate detection has the purpose of detecting the location of
    the plate in the whole camera frame. When a plate is detected in an image, the
    plate segment is passed to the second step (plate recognition), which uses an
    OCR algorithm to determine the alphanumeric characters on the plate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the two main algorithm steps: plate detection
    and plate recognition. After these steps, the program draws over the camera frame
    the plate''s characters that have been detected. The algorithms can return bad
    results or may not return any result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In each step shown in the previous figure, we will define three additional
    steps that are commonly used in pattern recognition algorithms. These steps are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Segmentation**: This step detects and removes each patch/region of interest
    in the image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature extraction**: This step extracts from each patch a set of characteristics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Classification**: This step extracts each character from the plate recognition-step
    or classifies each image patch into *plate* or *no plate* in the plate-detection
    step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see these pattern recognition steps in the
    whole algorithm application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Aside from the main application, whose purpose is to detect and recognize a
    car plate number, we will briefly explain two more tasks that are usually not
    explained:'
  prefs: []
  type: TYPE_NORMAL
- en: How to train a pattern recognition system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tasks, however, can be more important than the main application, because
    if we do not train the pattern recognition system correctly, our system can fail
    and not work correctly; different patterns need different training's and evaluation.
    We need to evaluate our system in different environments, conditions, and features
    to get the best results. These two tasks are sometimes used together, since different
    features can produce different results that we can see in the evaluation section.
  prefs: []
  type: TYPE_NORMAL
- en: Plate detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we have to detect all the plates in a current camera frame. To
    do this task, we divide it in two main steps: segmentation and segment classification.
    The feature step is not explained because we use the image patch as a vector feature.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step (segmentation), we will apply different filters, morphological
    operations, contour algorithms, and validations to retrieve parts of the image
    that could have a plate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step (classification), we will apply a **Support Vector Machine**
    (**SVM**) classifier to each image patch, our feature. Before creating our main
    application, we will train with two different classes: *plate* and *non-plate*.
    We will work with parallel frontal view color images having 800 pixels of width
    and taken between 2 and 4 meters from a car. These requirements are important
    for correct segmentations. We can get perform detection if we create a multi-scale
    image algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next image, we will shown all process involved in plate detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Sobel filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threshold operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Close morphologic operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask of one of filled area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In red possible detected plates (features images)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detected plates after SVM classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_04_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Segmentation is the process of dividing an image into multiple segments. This
    process is to simplify the image for analysis and make feature extraction easier.
  prefs: []
  type: TYPE_NORMAL
- en: One important feature of plate segmentation is the high number of vertical edges
    in a license plate, assuming that the image was taken frontally and the plate
    is not rotated and without perspective distortion. This feature can be exploited
    during the first segmentation step to eliminate regions that don't have any vertical
    edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before finding vertical edges, we need to convert the color image to a grayscale
    image (because color can''t help us in this task) and remove possible noise generated
    from the camera or other ambient noise. We will apply a 5x5 Gaussian blur and
    remove noise. If we don''t apply a noise removal method, we can get a lot of vertical
    edges that produce fail detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To find the vertical edges, we will use a Sobel filter and find the first horizontal
    derivate. The derivate is a mathematic function that allows us to find vertical
    edges on an image. The definition of Sobel function in OpenCV is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `ddepth` is the destination image depth; `xorder` is the order of the
    derivate by x; `yorder` is the order of the derivate by y; `ksize` is the kernel
    size of 1, 3, 5, or 7; `scale` is an optional factor for computed derivative values;
    `delta` is an optional value added to the result; and `borderType` is the pixel
    interpolation method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, for our case, we can use `xorder=1`, `yorder=0,` and `ksize=3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying a Sobel filter, we will apply a threshold filter to obtain a
    binary image with a threshold value obtained through Otsu''s method. Otsu''s algorithm
    needs an 8-bit input image, and Otsu''s method automatically determines the optimal
    threshold value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To define Otsu's method in threshold function, we will combine the type parameter
    with the `CV_THRESH_OTSU` value and the threshold value parameter is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: When the `CV_THRESH_OTSU` value is defined, the threshold function returns the
    optimal threshold value obtained by Otsu's algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: By applying a close morphological operation, we can remove blank spaces between
    each vertical edge line and connect all regions that have a high number of edges.
    In this step, we have possible regions that can contain plates.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define our structural element to use in our morphological operation.
    We will use the `getStructuringElement` function to define a structural rectangular
    element with a 17x3 dimension size in our case; this may be different in other
    image sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use this structural element in a close morphological operation
    using the `morphologyEx` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying these functions, we have regions in the image that could contain
    a plate; however, most of the regions do not contain license plates. These regions
    can be split with a connected component analysis or using the `findContours` function.
    This last function retrieves the contours of a binary image with different methods
    and results. We only need to get the external contours with any hierarchical relationship
    and any polygonal approximation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For each contour detected, extract the bounding rectangle of minimal area.
    OpenCV brings up the `minAreaRect` function for this task. This function returns
    a rotated `RotatedRect` rectangle class. Then, using a vector iterator over each
    contour, we can get the rotated rectangle and make some preliminary validations
    before we classify each region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We make basic validations about the regions detected based on their area and
    aspect ratio. We will consider that a region can be a plate if the aspect ratio
    is approximately *520/110 = 4.727272* (plate width divided by plate height) with
    an error margin of 40 percent and an area based on a minimum of 15 pixels and
    maximum of 125 pixels for the height of plate. These values are calculated depending
    on the image size and camera position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can make even more improvements using the license plate's white background
    property. All plates have the same background color, and we can use a flood fill
    algorithm to retrieve the rotated rectangle for precise cropping.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to crop the license plate is to get several seeds near the last
    rotated rect center. Then, we will get the minimum size of plate between the width
    and height, and use it to generate random seeds near the patch center.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to select the white region, and we need several seeds to touch at least
    one white pixel. Then, for each seed, we use a `floodFill` function to draw a
    new mask image to store the new closest cropping region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `floodfill` function fills a connected component with a color into a mask
    image starting from a point seed, and sets maximal lower and upper brightness/color
    difference between the pixel to fill and the pixel neighbors or pixel seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `newval` parameter is the new color we want to put into the image when filling.
    Parameters `loDiff` and `upDiff` are the maximal lower and maximal upper brightness/color
    difference between the pixel to fill and the pixel neighbors or pixel seed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `flag` is a combination of the following bits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lower bits**: These bits contain connectivity value, 4 (by default) or 8,
    used within the function. Connectivity determines which neighbors of a pixel are
    considered'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upper bits**: These can be 0 or a combination of the following values-`CV_FLOODFILL_FIXED_RANGE`
    and `CV_FLOODFILL_MASK_ONLY`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_FLOODFILL_FIXED_RANGE` sets the difference between the current pixel and
    the seed pixel. `CV_FLOODFILL_MASK_ONLY` will only fill the image mask and not
    change the image itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a crop mask, we will get a minimal area rectangle from the image
    mask points and check the validity size again. For each mask, a white pixel gets
    the position and uses the `minAreaRect` function for retrieving the closest crop
    region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The segmentation process is finished, and we have valid regions. Now, we can
    crop each detected region, remove possible rotation, crop the image region, resize
    the image, and equalize the light of the cropped image regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to generate the transform matrix with `getRotationMatrix2D`
    to remove possible rotations in the detected region. We need to pay attention
    to height, because `RotatedRect` can be returned and rotated at 90 degrees. So,
    we have to check the rectangle aspect, and if it is less than `1`, we need to
    rotate it by 90 degrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the transform matrix, we now can rotate the input image by an affine transformation
    (affine transformation in geometry is a transformation that takes parallel lines
    to parallel lines) with the `warpAffine` function where we set the input and destination
    images, the transform matrix, the output size (same as input in our case), and
    the interpolation method to use. We can define the border method and border value
    if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After we rotate the image, we will crop the image with `getRectSubPix` which
    crops and copies an image portion of width and height centered in a point. If
    the image is rotated, we need to change the width and height sizes with the C++
    `swap` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Cropped images are not good for use in training and classification since they
    do not have the same size. Also, each image contains different light conditions,
    making them more different. To resolve this, we resize all the images to same
    width and height, and apply a light histogram equalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For each detected region, we store the cropped image and its position in a
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we preprocess and segment all possible parts of an image, we now need
    to decide whether each segment is (or is not) a license plate. To do this, we
    will use a **Support Vector Machine** (**SVM**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A Support Vector Machine is a pattern recognition algorithm included in a family
    of supervised learning algorithms that was originally created for binary classification.
    Supervised learning is the machine learning algorithm technique that is trained
    with labeled data. We need to train the algorithm with an amount of data that
    is labeled; each data set needs to have a class.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM creates one or more hyperplanes, which is used to discriminate each
    class of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classic example is a 2D point set that defines two classes; the SVM searches
    the optimal line that differentiates each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_007.png)'
  prefs: []
  type: TYPE_IMG
- en: The first task before any classification is to train our classifier; this is
    a job before the main application and it's named "offline training". This is not
    an easy job because it requires a sufficient amount of data to train the system,
    but a bigger dataset does not always imply the best results. In our case, we do
    not have enough data due to the fact that there are no public license plate databases.
    Because of this, we need to take hundreds of car photos, and then preprocess and
    segment all of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We trained our system with 75 license plate images and 35 images without license
    plates, containing a 144x33 pixel resolution. We can see a sample of this data
    in the following image. This is not a large dataset, but sufficient enough to
    get decent results for our chapter. In a real application, we would need to train
    with more data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_008.png)'
  prefs: []
  type: TYPE_IMG
- en: To easily understand how machine learning works, we will proceed to use image
    pixel features of the classifier algorithm (keep in mind that there are better
    methods and features to train an SVM, such as **Principal Components Analysis**
    (**PCA**), Fourier transform, texture analysis, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create the images to train our system using the `DetectRegions`
    class and set the `savingRegions` variable to "true" in order to save the images.
    We can use the `segmentAllFiles.sh` bash script to repeat the process on all image
    files under a folder. This can be taken from the source code of the book:'
  prefs: []
  type: TYPE_NORMAL
- en: To make this easier, we will store all image training data that is processed
    and prepared into an XML file for use directly with the SVM function. The `trainSVM.cpp`
    application creates this file using the folders and number of image files.
  prefs: []
  type: TYPE_NORMAL
- en: Training data for a machine learning OpenCV algorithm is stored in an *N*x*M*
    matrix, with *N* samples and *M* features. Each dataset is saved as a row in the
    training matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The classes are stored in another matrix with *n*x1 size, where each class is
    identified by a float number.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has an easy way to manage a data file in the XML or YAML format with
    the `FileStorage` class. This class lets us store and read OpenCV variables and
    structures or our custom variables. With this function, we can read the training
    data matrix and training classes and save it in `SVM_TrainingData` and `SVM_Classes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the training data in the `SVM_TrainingData` variable and labels
    in `SVM_Classes`. Then, we only have to create the training data object that connects
    data and labels to use in our machine learning algorithm. To do this, we will
    use the `TrainData` class as a OpenCV pointer `Ptr` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create the classifier object using the `SVM` class using the `Ptr`OpenCV
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to set the SVM parameters that define the basic parameters to
    use in an SVM algorithm. To do this, we only have to change some object variables.
    After different experiments, we will choose the next parameter''s setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We chose a 1000 iterations for training, a C param variable optimization of
    0.1, and finally, a kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need train our classifier with the`train` function and the train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Our classifier is ready to predict a possible cropped image using the `predict`
    function of our SVM class; this function returns the class identifier `i`. In
    our case, we will label a plate class with 1 and no plate class with 0\. Then,
    for each detected region that can be a plate, we will use SVM to classify it as
    plate or no plate, and save only the correct responses. The following code is
    a part of a main application called online processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Plate recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second step in License Plate Recognition aims to retrieve the characters
    of the license plate with Optical Character Recognition. For each detected plate,
    we proceed to segment the plate for each character and use an Artificial Neural
    Network machine learning algorithm to recognize the character. Also, in this section,
    you will learn how to evaluate a classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: OCR segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will obtain a plate image patch as an input to the OCR segmentation
    function with an equalized histogram. We then need to apply only a threshold filter
    and use this threshold image as the input of a Find Contours algorithm. We can
    observe this process in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This segmentation process is coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We used the `CV_THRESH_BINARY_INV` parameter to invert the threshold output
    by turning the white input values black and the black input values white. This
    is needed to get the contours of each character, because the contours algorithm
    looks for white pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each detected contour, we can make a size verification and remove all regions
    where the size is smaller or the aspect is not correct. In our case, the characters
    have a 45/77 aspect, and we can accept a 35 percent error of aspect for rotated
    or distorted characters. If an area is higher than 80 percent, we will consider
    that region to be a black block and not a character. For counting the area, we
    can use the `countNonZero` function that counts the number of pixels with a value
    higher than 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If a segmented character is verified, we have to preprocess it to set the same
    size and position for all characters, and save it in a vector with the auxiliary
    `CharSegment` class. This class saves the segmented character image and the position
    that we need to order the characters, because the Find Contour algorithm does
    not return the contours in the correct and needed order.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step for each segmented character is to extract the features for training
    and classify the Artificial Neural Network algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike plate detection, the feature extraction step used in SVM doesn''t use
    all of the image pixels. We will apply more common features used in OCR that contain
    horizontal and vertical accumulation histograms and low-resolution image samples.
    We can see this feature more graphically in the next image, as each image has
    a low resolution 5x5 image and the histogram accumulations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For each character, we will count the number of pixels in a row or column with
    a nonzero value using the `countNonZero` function and store it in a new data matrix
    called `mhist`. We will normalize it by looking for the maximum value in the data
    matrix using the `minMaxLoc` function and divide all elements of `mhist` by the
    maximum value with the `convertTo` function. We will create the `ProjectedHistogram`
    function to create the accumulation histograms that have a binary image and a
    type of histogram that we need, horizontal or vertical, as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Other features use a low-resolution sample image. Instead of using the whole
    character image, we will create a low-resolution character, for example, a character
    of 5x5\. We will train the system with 5x5, 10x10, 15x15, and 20x20 characters
    and then evaluate which one returns the best result to use it in our system. Once
    we have all features, we will create a matrix of *M* columns by one row where
    the columns are the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: OCR classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the classification step, we used an Artificial Neural Network machine learning
    algorithm, more specifically, a **Multi-Layer Perceptron** (**MLP**) which is
    the most commonly used ANN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: MLP consists of a network of neurons with an input layer, output layer, and
    one or more hidden layers. Each layer has one or more neurons connected with the
    previous and next layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example represents a three-layer perceptron (is a binary classifier
    that maps a real-valued vector input to a single binary value output) with three
    inputs, two outputs, and the hidden layer including five neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All neurons in an MLP are similar, and each one has several inputs (the previous
    linked neurons) and several output links with the same value (the next linked
    neurons). Each neuron calculates the output value as a sum of the weighted inputs
    plus a bias term and is transformed by a selected activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three widely used activation functions: Identity, Sigmoid, and Gaussian.
    The most common and default activation function is the Sigmoid function; it has
    an alpha and beta value set to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: An ANN-trained network has a vector of input with features; it passes the values
    to the hidden layer and computes the results with the weights and activation function.
    It passes outputs further downstream until it gets the output layer that has the
    number of neurons classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight of each layer, synapses, and neuron is computed and learned by training
    the ANN algorithm. To train our classifier, we will create two matrices of data,
    as we did in the SVM training, but the training labels are a bit different. Instead
    of an *N*x1 matrix, where *N* stands for training data rows and 1 is the column,
    we will use the label number identifier. We have to create an *N*x*M* matrix,
    where *N* is the training/samples data and *M* are the classes (10 digits + 20
    letters in our case), and set 1 in a position *i*, *j* if the data row *i* is
    classified with class *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_014.png)'
  prefs: []
  type: TYPE_IMG
- en: We will create an `OCR::train` function to create all needed matrix and train
    our system, with the training data matrix, classes matrix, and the number of hidden
    neurons in the hidden layers. The training data is loaded from an XML file, just
    as we did in SVM training.
  prefs: []
  type: TYPE_NORMAL
- en: We have to define the number of neurons in each layer to initialize the ANN
    class. For our sample, we will use only one hidden layer. Then, we will define
    a matrix of one row and three columns. The first column position is the number
    of features, the second column position is the number of hidden neurons on the
    hidden layer, and the third column position is the number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV defines an `ANN_MLP` class for ANN. With the `create` function, we can
    initiate the class pointer and later define the number of layers and neurons and
    the activation function. We can thencreate the training data like SVM, and `alpha`
    and `beta` parameters of training method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can classify any segmented plate features using the `OCR::classify`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `ANN_MLP` class uses the `predict` function for classifying a feature vector
    in a class. Unlike the SVM `classify` function, the ANN predict function returns
    a row with the size of equal to the number of classes, with the probability of
    belonging the input feature to each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the best result, we can use the `minMaxLoc` function to get the max
    and min response, and the position in the matrix. The class of our character is
    specified by the *x* position of higher value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To finish each plate detected, we order its characters and return a string
    with the `str()` function of the `Plate` class, and we can draw it on the original
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our project is finished. However, when we train a machine learning algorithm
    like OCR, for example, we need to know the best features and parameters to use
    and how to correct the classification, recognition, and detection errors in our
    system.
  prefs: []
  type: TYPE_NORMAL
- en: We need to evaluate our system with different situations and parameters and
    evaluate the errors produced in order to get the best parameters that minimize
    those errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we evaluated the OCR task with variables: size of low-level
    resolution image feature and the number of hidden neurons in the hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created the `evalOCR.cpp` application where we uses the XML training data
    file generated by the `trainOCR.cpp` application. The `OCR.xml` file contains
    the training data matrix for 5x5, 10x10, 15x15, and 20x20 downsampled image features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation application gets each downsampled matrix feature and gets 100
    random rows for traning, as well as other rows for testing the ANN algorithm and
    checking the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training the system, we will test each random sample and check whether
    the response is correct. If the response is not correct, we increment the error
    counter variable and then divide by the number of samples to evaluate. This indicates
    the error ratio between 0 and 1 for training with random data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The application returns output command-line error ratio for each sample size.
    For a good evaluation, we need to train the application with different random
    training rows. This produces different test error values. Then, we can add up
    all the errors and obtain an average. To do this task, we will create the bash
    UNIX script to automate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This script saves a `data.txt` file that contains all results for each size
    and neuron hidden layer number. This file can be used for plotting with *gnuplot*.
    We can see the result in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_016.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the lowest error is over 8 percent and is using 20 neurons in
    hidden layer and character's features extracted from a downscaled to 10x10 image
    patch.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how an Automatic Plate License Recognition program
    works and its two important steps: plate localization and plate recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, you learned how to segment an image looking for patches where
    we can have a plate, and use a simple heuristics and SVM algorithm to make a binary
    classification for patches with *plates* and *no plates*.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, you learned how to segment with the Find Contours algorithm,
    extract feature vector from each character, and use an ANN to classify each feature
    in a character class.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to evaluate a machine algorithm with training with random
    samples, and using different parameters and features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to create a face-recognition application
    using eigenfaces.
  prefs: []
  type: TYPE_NORMAL
