["```py\n%pip install openai-whisper\n```", "```py\n%pip install librosa\n```", "```py\n%pip install pytube\n```", "```py\n%pip install transformers\n```", "```py\n%pip install joblib\n```", "```py\nimport numpy as np\nimport sounddevice as sd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n```", "```py\n# Function to capture real-time audio\ndef capture_audio(duration=5, sampling_rate=44100):\n    print(\"Recording...\")\n    audio_data = sd.rec(int(sampling_rate * duration), \\\n        samplerate=sampling_rate, channels=1, dtype='int16')\n    sd.wait()\n    return audio_data.flatten()\n```", "```py\n# Function to collect training data\ndef collect_training_data(num_samples=10, label=0):\n    X = []\n    y = []\n    for _ in range(num_samples):\n        input(\"Press Enter and speak for a few seconds...\")\n    audio_sample = capture_audio()\n    X.append(audio_sample)\n    y.append(label)\n    return np.vstack(X), np.array(y)\n# Main program\nclass VoiceClassifier:\n    def __init__(self):\n        self.model = RandomForestClassifier()\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n# Collect positive samples (voice)\npositive_X, positive_y = collect_training_data(num_samples=10, label=1)\n# Collect negative samples (background noise or non-voice)\nnegative_X, negative_y = collect_training_data(num_samples=10, label=0)\n```", "```py\n# Combine and shuffle the data\nX = np.vstack([positive_X, negative_X])\ny = np.concatenate([positive_y, negative_y])\n```", "```py\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \\\n    test_size=0.2, random_state=42)\n```", "```py\n# Train the voice classifier model\nvoice_classifier = VoiceClassifier()\nvoice_classifier.train(X_train, y_train)\n```", "```py\n# Make predictions on the test set\npredictions = voice_classifier.predict(X_test)\n```", "```py\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n```", "```py\nimport joblib\n# Save the trained model during training\njoblib.dump(voice_classifier, \"voice_classifier_model.pkl\")\nimport numpy as np\nimport sounddevice as sd\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.externals import joblib # For model persistence\n# Load the pre-trained model\nvoice_classifier = joblib.load(\"voice_classifier_model.pkl\")\n# Function to capture real-time audio\ndef capture_audio(duration=5, sampling_rate=44100):\n    print(\"Recording...\")\n    audio_data = sd.rec(int(sampling_rate * duration), \\\n        samplerate=sampling_rate, channels=1, dtype='int16')\n    sd.wait()\n    return audio_data.flatten()\n# Function to predict voice using the trained model\ndef predict_voice(audio_sample):\n    prediction = voice_classifier.predict([audio_sample])\n    return prediction[0]\n# Main program for real-time voice classification\ndef real_time_voice_classification():\n    while True:\n        input(\"Press Enter and speak for a few seconds...\")\n    # Capture new audio\n        new_audio_sample = capture_audio()\n    # Predict if it's voice or non-voice\n        result = predict_voice(new_audio_sample)\n    if result == 1:\n        print(\"Voice detected!\")\n    else:\n        print(\"Non-voice detected.\")\nif __name__ == \"__main__\":\n    real_time_voice_classification()\n```", "```py\nimport whisper\nImport pytube\n```", "```py\nmodel = whisper.load_model(\"base\")\n```", "```py\n#we are importing Pytube library\nimport pytube\n#we are downloading YouTube video from YouTube link\nvideo = \"https://youtu.be/g8Q452PEXwY\"\ndata = pytube.YouTube(video)\n```", "```py\n# Converting and downloading as 'MP4' file\naudio = data.streams.get_audio_only()\naudio.download()\n```", "```py\n# Set the FFMPEG environment variable to the path of your ffmpeg executable\nos.environ['PATH'] = '/<your_path>/audio-orchestrator-ffmpeg/bin:' + os.environ['PATH']\n```", "```py\nmodel = whisper.load_model('base')\ntext = model.transcribe('Mel Spectrograms with Python and Librosa Audio Feature Extraction.mp4')\n#printing the transcribe\ntext['text']\n```", "```py\nmodel = whisper.load_model('base')\ntext = model.transcribe('/Users/<username>/PacktPublishing/DataLabeling/Ch11/customer_call_audio.m4a')\n#printing the transcribe\ntext['text']\n```", "```py\n' Hello, I have not received the product yet. I am very disappointed. Are you going to replace if my product is damaged or missed? I will be happy if you replace with your product in case I miss the product due to incorrect shipping address.'\n```", "```py\nfrom transformers import pipeline\n# Load the sentiment analysis pipeline\nsentiment_classifier = pipeline('sentiment-analysis')\n# text for sentiment analysis\ntext=\"Hello, I have not received the product yet. I am very disappointed.are you going to replace if my product is damaged or missed.I will be happy if you replace with new product incase i missed the product die to incorrect shipping address\"\n# Perform sentiment analysis\nresult = sentiment_classifier(text)\n# Display the result\nprint(result)\n```", "```py\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). Using a pipeline without specifying a model name and revision in production is not recommended.\n[{'label': 'NEGATIVE', 'score': 0.9992625117301941}]\n```", "```py\nimport os\nimport librosa\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.image import resize\nfrom tensorflow.keras.models import load_model\n```", "```py\n# Define your folder structure\ndata_dir = '../cats_dogs/data/'\nclasses = ['cat', 'dog']\n```", "```py\n# define the function for Load and preprocess audio data\ndef load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):\ndata = []\nlabels = []\nfor i, class_name in enumerate(classes):\n    class_dir = os.path.join(data_dir, class_name)\n    for filename in os.listdir(class_dir):\n        if filename.endswith('.wav'):\n            file_path = os.path.join(class_dir, filename)\n            audio_data, sample_rate = librosa.load(file_path, sr=None)\n            # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n            mel_spectrogram = \\\n                librosa.feature.melspectrogram( \\\n                    y=audio_data, sr=sample_rate)\n            mel_spectrogram = resize( \\\n                np.expand_dims(mel_spectrogram, axis=-1), \\\n                target_shape)\n            print(mel_spectrogram)\n            data.append(mel_spectrogram)\n        labels.append(i)\nreturn np.array(data), np.array(labels)\n```", "```py\n# Split data into training and testing sets\ndata, labels = load_and_preprocess_data(data_dir, classes)\nlabels = to_categorical(labels, num_classes=len(classes)) # Convert labels to one-hot encoding\nX_train, X_test, y_train, y_test = train_test_split(data, \\\n    labels, test_size=0.2, random_state=42)\n```", "```py\n# Create a neural network model\ninput_shape = X_train[0].shape\ninput_layer = Input(shape=input_shape)\nx = Conv2D(32, (3, 3), activation='relu')(input_layer)\nx = MaxPooling2D((2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D((2, 2))(x)\nx = Flatten()(x)\nx = Dense(64, activation='relu')(x)\noutput_layer = Dense(len(classes), activation='softmax')(x)\nmodel = Model(input_layer, output_layer)\n```", "```py\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), \\\n    loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n```", "```py\n# Train the model\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, \\\n    validation_data=(X_test, y_test))\n```", "```py\n#Test  the accuracy of model\ntest_accuracy=model.evaluate(X_test,y_test,verbose=0)\nprint(test_accuracy[1])\n```", "```py\n# Save the model\nmodel.save('audio_classification_model.h5')\n```", "```py\n# Load the saved model\nmodel = load_model('audio_classification_model.h5')\n# Define the target shape for input spectrograms\ntarget_shape = (128, 128)\n# Define your class labels\nclasses = ['cat', 'dog']\n# Function to preprocess and classify an audio file\ndef test_audio(file_path, model):\n# Load and preprocess the audio file\naudio_data, sample_rate = librosa.load(file_path, sr=None)\nmel_spectrogram = librosa.feature.melspectrogram( \\\n    y=audio_data, sr=sample_rate)\nmel_spectrogram = resize(np.expand_dims(mel_spectrogram, \\\n    axis=-1), target_shape)\nmel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))\n```", "```py\n# Make predictions\npredictions = model.predict(mel_spectrogram)\n# Get the class probabilities\nclass_probabilities = predictions[0]\n# Get the predicted class index\npredicted_class_index = np.argmax(class_probabilities)\nreturn class_probabilities, predicted_class_index\n# Test an audio filetest_audio_file = '../Ch10/cat-meow-14536.mp3'\nclass_probabilities, predicted_class_index = test_audio( \\\n    test_audio_file, model)\n```", "```py\n# Display results for all classes\nfor i, class_label in enumerate(classes):\n    probability = class_probabilities[i]\n    print(f'Class: {class_label}, Probability: {probability:.4f}')\n```", "```py\npredicted_class = classes[predicted_class_index]\naccuracy = class_probabilities[predicted_class_index]\nprint(f'The audio is labeled Spectrogram Visualization\nas: {predicted_class}')\nprint(f'Accuracy: {accuracy:.4f}')\n```", "```py\nimport numpy as np\ndef add_noise(data, noise_factor):\n    noise = np.random.randn(len(data))\n    augmented_data = data + noise_factor * noise\n    # Cast back to same data type\n    augmented_data = augmented_data.astype(type(data[0]))\n    return augmented_data\n```", "```py\n# Sample audio data\nsample_data = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n# Sample noise factor\nsample_noise_factor = 0.05\n# Apply augmentation\naugmented_data = add_noise(sample_data, sample_noise_factor)\n# Print the original and augmented data\nprint(\"Original Data:\", sample_data)\nprint(\"Augmented Data:\", augmented_data)\n```", "```py\n# Load and preprocess audio data\ndef load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):\n    data = []\n    labels = []\n    noise_factor = 0.05\n    for i, class_name in enumerate(classes):\n    class_dir = os.path.join(data_dir, class_name)\n    for filename in os.listdir(class_dir):\n    if filename.endswith('.wav'):\n    file_path = os.path.join(class_dir, filename)\n    audio_data, sample_rate = librosa.load(file_path, sr=None)\n    # Apply noise manipulation\n    noise = np.random.randn(len(audio_data))\n    augmented_data = audio_data + noise_factor * noise\n    augmented_data = augmented_data.astype(type(audio_data[0]))\n    # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n    mel_spectrogram = librosa.feature.melspectrogram( \\\n        y=augmented_data, sr=sample_rate)\n    mel_spectrogram = resize( \\\n        np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n    print(mel_spectrogram)\n    data.append(mel_spectrogram)\n    labels.append(i)\n    return np.array(data), np.array(labels)\n```", "```py\ntest_accuracy=model.evaluate(X_test,y_test,verbose=0)\nprint(test_accuracy[1])\n```", "```py\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io.wavfile import write\n# Load the audio file\naudio_file_path = \"../ch10/cats_dogs/cat_1.wav\"\ny, sr = librosa.load(audio_file_path)\n# Function for time stretching augmentation\ndef time_stretching(y, rate):\n    return librosa.effects.time_stretch(y, rate=rate)\n# Function for pitch shifting augmentation\ndef pitch_shifting(y, sr, pitch_factor):\n    return librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_factor)\n# Function for dynamic range compression augmentation\ndef dynamic_range_compression(y, compression_factor):\n    return y * compression_factor\n# Apply dynamic range compression augmentation\ncompression_factor = 0.5 # Adjust as needed\ny_compressed = dynamic_range_compression(y, compression_factor)\n# Apply time stretching augmentation\ny_stretched = time_stretching(y, rate=1.5)\n# Apply pitch shifting augmentation\ny_pitch_shifted = pitch_shifting(y, sr=sr, pitch_factor=3)\n# Display the original and augmented waveforms\nplt.figure(figsize=(12, 8))\nplt.subplot(4, 1, 1)\nlibrosa.display.waveshow(y, sr=sr)\nplt.title('Original Waveform')\nplt.subplot(4, 1, 2)\nlibrosa.display.waveshow(y_stretched, sr=sr)\nplt.title('Time Stretched Waveform')\nplt.subplot(4, 1, 3)\nlibrosa.display.waveshow(y_pitch_shifted, sr=sr)\nplt.title('Pitch Shifted Waveform')\nplt.subplot(4, 1, 4)\nlibrosa.display.waveshow(y_compressed, sr=sr)\nplt.title('Dynamic Range Compressed Waveform')\nplt.tight_layout()\nplt.show()\n```"]