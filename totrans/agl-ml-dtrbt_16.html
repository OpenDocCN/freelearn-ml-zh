<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer256">
			<h1 id="_idParaDest-164"><a id="_idTextAnchor186"/><a id="_idTextAnchor187"/><em class="italic">Chapter 13</em>: Model Governance and MLOps</h1>
			<p>In the previous chapters, we learned how to build, understand, and deploy models. We will now learn how to govern these models and how to responsibly use these models in operations. In earlier chapters, we discussed the methods for understanding the business problem, the system in which the model will operate, and the potential consequences of using a model's predictions. <strong class="bold">MLOps</strong> is a word made up of <strong class="bold">machine learning and DevOps</strong>. It is made of processes and practices to efficiently, reliably, and effectively operationalize the production of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models within an enterprise. MLOps aims to ensure commercial value and regulatory requirements are met continuously by ensuring production models' outcomes are of good quality and automation is in place. It provides a centralized system to manage the entire life cycle of all ML models in production.</p>
			<p>Activities within MLOps cover all aspects of model deployment, provide real-time tracking accuracy of models in production, offer a champion challenger process that continuously learns and evaluates models using real-time data, track model bias and fairness, and provide a <strong class="bold">model governance</strong> framework to ensure that models continue to deliver business impact while meeting the regulatory requirements. In <a href="B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Scoring and Deployment</em>, we covered model deployment on the DataRobot platform. </p>
			<p>Furthermore, in <a href="B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Scoring and Deployment</em>, we extensively covered aspects of monitoring models in production. Given the crucial role model governance plays within the MLOps process, in this chapter, we will introduce the model governance framework. One key aspect of model monitoring is to ensure that models are not biased and are fair towards all people impacted by the model, which we will explore in this chapter. After that, we will take a deeper look at how to enable other aspects of MLOps, including how to maintain and monitor models. Thus, we're going to cover the following main topics:</p>
			<ul>
				<li><a id="_idTextAnchor188"/>Governing models</li>
				<li>Addressing model bias and fairness</li>
				<li>Implementing MLOps</li>
				<li><a id="_idTextAnchor189"/>Notifications and changing models in production</li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor190"/>Technical requirements</h1>
			<p>Most parts of this chapter require access to the DataRobot software. The example utilizes a relatively small dataset, <strong class="bold">Book-Crossing</strong>, consisting of three tables, whose manipulation was described earlier in <a href="B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139"><em class="italic">Chapter 10</em></a>, <em class="italic">Recommender Systems</em>. As will be covered in the data description, we will create new fields in addition to those used in <a href="B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139"><em class="italic">Chapter 10</em></a>, <em class="italic">Recommender Systems</em>.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor191"/>Book-Crossing dataset</h2>
			<p>The example used to illustrate the aspects of model governance is the same as the one used for <a id="_idIndexMarker663"/>building recommendation systems in <a href="B17159_10_Final_NM_ePub.xhtml#_idTextAnchor139"><em class="italic">Chapter 10</em></a>, <em class="italic">Recommender Systems</em>. The dataset is based on the Book-Crossing dataset by Cai-Nicolas Ziegler and colleagues (<a href="http://www2.informatik.uni-freiburg.de/~cziegler/BX/">http://www2.informatik.uni-freiburg.de/~cziegler/BX/</a>). The data was collected during a 4-week crawl from the Book-Crossing community between August and September 2004. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Before using this dataset, the authors of this book have informed the owner of the dataset about its usage in this book: </p>
			<p class="callout"><em class="italic">Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen (2005). Improving Recommendation Lists Through Topic Diversification. Proceedings of the 14th International World Wide Web Conference (WWW 2005). May 2010–2014, 2005, Chiba, Japan</em></p>
			<p>The subsequent three tables, provided in <strong class="source-inline">.csv</strong> format, make up this dataset. <a id="_idTextAnchor192"/></p>
			<ul>
				<li>Users: This table presents the profile of the users, with anonymized <strong class="source-inline">User-ID</strong> and presented as integers. Also provided are the user <strong class="source-inline">Location</strong> and <strong class="source-inline">Age</strong>.</li>
				<li>Books: This table contains the characteristics of the books. Its features include <strong class="source-inline">ISBM</strong>, <strong class="source-inline">Book-Title</strong>, <strong class="source-inline">Book-Author</strong>, <strong class="source-inline">Year-Of-Publication</strong>, and <strong class="source-inline">Publisher</strong>.</li>
				<li>Ratings: This table shows the ratings. Each row provides a user's rating for a book. The <strong class="source-inline">Book-Rating</strong> is either implicit as <strong class="source-inline">0</strong> or explicit between <strong class="source-inline">1</strong> and <strong class="source-inline">10</strong> (the higher, the more appreciated). However, within the context of this project, we will focus solely on ratings that are explicit for the model development. The table also includes the <strong class="source-inline">User-ID</strong> and <strong class="source-inline">ISBN</strong> fields.</li>
			</ul>
			<p>In addition, using Excel, we <a id="_idIndexMarker664"/>created two extra fields using age and a rating column. We created the <strong class="source-inline">RatingClass</strong> field, which considers a rating over <strong class="source-inline">7</strong> as a <strong class="source-inline">High</strong> rating or else it is <strong class="source-inline">Low</strong>. Similarly, we created the <strong class="source-inline">AgeGroup</strong> field; this classes ages over 40 as <strong class="source-inline">Over Forty</strong> and those under 25 as <strong class="source-inline">Under 25</strong>, or else they are considered simply <strong class="source-inline">Between 25 and 40</strong>. Finally, we dropped out data rows with a missing age column. </p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor193"/>Governing models</h1>
			<p>Organizations using ML governance define a framework of rules and controls for managing the ML workflows pertaining to model development, production, and post-production monitoring. The commercial importance of ML is well established. Still, only a fraction of <a id="_idIndexMarker665"/>companies investing in ML are realizing the benefits. Some establishments have struggled to ensure that the outcomes of ML projects are well aligned with their strategic direction. Importantly, many organizations are subject to regulations, such as the recently implemented General Data Protection Regulation within the European Union and European Economic Area, which affect the use of these models and their outputs. Businesses, in general, need to steer their ML use to ensure regulatory requirements are satisfied and strategic goals and values are continually realized.</p>
			<p>Having an established governance framework in place ensures that data scientists can focus on the innovative part of their role, which is solving new problems. With governance, data scientists spend less time assessing the commercial value their models are delivering to the business, evaluating models' performance of production, and examining whether there has been data drift. Model governance simplifies the model versioning and change tracking process for all production models. This is always a key aspect of ML audit trailing. In addition, notifications could be set up to alert stakeholders when a model in production encounters anomalies and changes in performance. When there is a significant decline in performance, models in production could be swapped with better-performing challenger models in a seamless fashion. Although this process might require reviews and authorization from other stakeholders, it is much more simple and straightforward than a typical data science workflow. </p>
			<p>It is clear that governing models throughout the entire process is a complex and time-consuming undertaking. Without tool support, it is easy for the data science teams to miss key steps. Tools such as DataRobot make this task easier, ensuring that many required tasks are performed automatically. This ease of use can also sometimes make the teams use these tools without thinking. This can be dangerous too. Thus, a judicious mix of using the tools such as DataRobot and setting up process controls and reviews is needed to ensure proper governance.</p>
			<p>DataRobot's MLOps provides organizations with an ML model governance framework that helps in the management of risks. Using the model governance tool, a business executive can track important business metrics and ensure that <a id="_idIndexMarker666"/>regulatory requirements are met on a continuous basis. They can easily assess the model performance in production to ensure that models are fit for purpose. Furthermore, with governance in place, the commercial criticality of models is defined before deployment. This ensures that when models are critical to the business, certain changes to the model need to be reviewed and authorized by stakeholders before such changes are fully implemented. In line with ethics, the use of ML models is expected to enable a fair process. So, models' outputs should be purged of any form of biases. In subsequent sections in this chapter, in addition to other aspects of MLOps, we will examine how bias could be mitigated in the ML model in development as well as in production.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor194"/>Addressing model bias and fairness</h1>
			<p>A key characteristic of ML lies in its learning from the past to predict the future. This implies that future predictions would be influenced by the past. Some training datasets are structured <a id="_idIndexMarker667"/>in ways that could introduce bias into ML models. These biases are based on unspoken unfairness evident in human systems. Bias is known to maintain prejudice and unfairness that preexisted the models and could lead to unintended consequences. An AI system that is unable to understand human bias mirrors, if not exacerbates, the bias present in the training dataset. It is easy to see why women are more likely to receive lower salary predictions by ML models than their male counterparts. In a similar example, credit card companies using historic data-driven ML models could be steered into offering higher rates to individuals from minority backgrounds. Such <strong class="bold">unwarranted associated</strong> are caused by human bias that is inherent in the training dataset. It is unfair to include bias-laden features with unbiased ones in model development. A fair process considers an individual's payment history in making predictions about their credit, but unfair outcomes are possible when predictions are made based on the payment history of their family.</p>
			<p>Supervised learning models can be particularly unfair, as certain data has circular dependency. For instance, to obtain a credit card, people need a credit history, and to have credit histories, credit cards are required. Since models are critical to credit assessment, it becomes nearly impossible for some people to get a credit card. Also, limited data about certain subgroups makes them more vulnerable to bias. This is because a minimal outcome distribution change in training data for such groups could skew the prediction outcomes for members of the group. These all point toward the extent to which ML models should manage bias and support a fair process.</p>
			<p>Many industries – for instance, health care, insurance, and banking – take specific measures to <a id="_idIndexMarker668"/>guard against any form of bias and unfairness as a regulatory expectation. While it is inherently challenging to address bias in humans, it is somewhat easier to address ML bias. So, as part of ML governance, addressing ML bias could be pivotal in ensuring that their products don't amplify the skepticism about the ethical aspects of ML systems.</p>
			<p>In addressing potential unwarranted outcomes, DataRobot has introduced a bias and fairness monitoring and control capability. This capability is selected and configured during model development. Let's step back and demonstrate how bias could be addressed in DataRobot. As with the typical platform, we upload the data as described in the preceding chapters. In the project configuration window (as within <em class="italic">Figure 13.1</em>), we open <strong class="bold">Advanced Options</strong> and the <strong class="bold">Bias and Fairness</strong> tab: </p>
			<ol>
				<li>It is within this tab that we define protected features, how fairness is established and measured, as well as the target variable. We specify the fields in the prediction dataset that need to be protected. These are entered in the <strong class="bold">Protected Features</strong> input field. In this case, the <strong class="source-inline">AgeGroup</strong> field is selected as to be protected (see <em class="italic">Figure 13.1</em>). In some industry datasets, attributes such as sex, ethnicity, age, and religion must be selected. In this way, DataRobot manages and presents metrics to measure any potential model bias within each of the protected fields:<div id="_idContainer241" class="IMG---Figure"><img src="Images/B17159_13_01.jpg" alt="Figure 13.1 – Configuring bias and fairness during the model development&#13;&#10;" width="1076" height="936"/></div><p class="figure-caption">Figure 13.1 – Configuring bias and fairness during the model development</p></li>
				<li>Next, the <strong class="bold">Favorable Target Outcome</strong> field is selected. This is the level of the target variable <a id="_idIndexMarker669"/>that is desirable. In this case, the target variable is the <strong class="source-inline">RatingClass</strong> level of <strong class="source-inline">High</strong>. This enables the measurement of bias on this level of the target variable. </li>
				<li>The <strong class="bold">Primary Fairness Metric</strong> field outlines the metric against which fairness is measured. It is important to highlight that fairness differs considerably across use cases. The fairness for an insurance risk modeling tool would strive to ensure that the risk all potential clients carry is representative, whereas fairness within that employment tool would aim for members of a protected group to have similar chances of being employed when compared to those from other groups. The choice of <strong class="bold">Primary Fairness Metric</strong> helps DataRobot understand how to measure bias. A few metrics are presented to be selected. These include <strong class="source-inline">Proportional Parity</strong>, <strong class="source-inline">Equal Parity</strong>, <strong class="source-inline">Prediction Balance</strong>, <strong class="source-inline">True Favorable Rate &amp; True Unfavorable Rate Parity</strong>, and <strong class="source-inline">Favorable Predictive &amp; Unfavorable Predictive Value Parity</strong>.  </li>
				<li>If a user is unsure of the metric to choose, they can click on <strong class="bold">Help Me Choose</strong>, which presents <a id="_idIndexMarker670"/>a further set of questions. Answering these questions presents a recommendation of a <strong class="bold">Fairness Metric</strong> value, as shown in <em class="italic">Figure 13.2</em>: <div id="_idContainer242" class="IMG---Figure"><img src="Images/B17159_13_02.jpg" alt="Figure 13.2 – Fairness Metric recommendation&#13;&#10;" width="1313" height="1313"/></div><p class="figure-caption">Figure 13.2 – Fairness Metric recommendation</p><p>In choosing our metric, because we are keenly interested in our model having similar prediction accuracy across age group membership, the <strong class="bold">Equal Error</strong> option is selected in response to how we want to measure model fairness. Since our outcome distribution is somewhat balanced between high and low, we choose <strong class="bold">No</strong> to the <strong class="bold">Does the favorable target outcome occur for a very small percentage of the population?</strong> question. Following this, DataRobot suggests <strong class="bold">True Favorable Rate &amp; True Unfavorable Rate Parity</strong>. All throughout the process, the platform offers a description of the options and presents an explanation of the recommended metric, as well as those for other metrics.</p></li>
				<li>A click on <strong class="bold">Select</strong> finalizes <a id="_idIndexMarker671"/>the process and the modeling process proceeds, as suggested in earlier chapters.<p>Model bias could be further examined after models have been developed. Since model bias and fairness were configured during model development, the <strong class="bold">Bias and Fairness</strong> tab is presented as part of the model's details (see <em class="italic">Figure 13.3</em>). When this tab is selected for any of the created models, the <strong class="bold">Per-Class Bias</strong> window is presented. Within this window, the relative extent to which the model is biased given the <strong class="bold">Fairness Score</strong> value is displayed. The <strong class="source-inline">AgeGroup</strong> <strong class="bold">Per-Class Bias</strong> value for the <strong class="source-inline">Light Gradient Boosting on Elastic Predictions</strong> model presented in <em class="italic">Figure 13.3</em> is below the default threshold:</p></li>
			</ol>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="Images/B17159_13_03.jpg" alt="Figure 13.3 – Per-Class Bias exploration&#13;&#10;" width="1650" height="823"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – Per-Class Bias exploration</p>
			<p>According to this <a id="_idIndexMarker672"/>outcome, the accuracy of the model in predicting the true unfavorable outcome (of a <strong class="source-inline">Low</strong> rating) for individuals within the <strong class="source-inline">Between 25 and 40</strong> class is lower than the other two classes. The score for this class falls below the default 80% threshold. The default threshold of 80% was applied for <strong class="bold">Primary Fairness Metric</strong>, as we didn't set a value for it during the model development, as seen in <em class="italic">Figure 13.3</em>. By clicking the <strong class="bold">Show absolute values</strong> tab, the absolute measures are presented instead. While the other chart (not visible in <em class="italic">Figure 13.3</em>) suggests that the accuracy for the favorable outcome was consistent across classes, this model could still be unfair, as it will most likely falsely predict unfavorable outcomes for individuals in the <strong class="source-inline">Between 25 and 40</strong> class. <em class="italic">Figure 13.4</em> demonstrates how <strong class="bold">Cross-Class Accuracy</strong>, a set of more holistic accuracy metrics, could be used to assess accuracy across the protected classes:</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="Images/B17159_13_04.jpg" alt="Figure 13.4 – Cross-Class Accuracy examination&#13;&#10;" width="1650" height="693"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.4 – Cross-Class Accuracy examination</p>
			<p><strong class="bold">Cross-Class Accuracy</strong> presents a set of accuracy metrics, assessing the model across differing levels of the <strong class="source-inline">AgeGroup</strong> class. As the outcome in <em class="italic">Figure 13.4</em> suggests, the accuracy of <a id="_idIndexMarker673"/>the model seems to be lower for the <strong class="source-inline">Between 25 and 40</strong> class across all accuracy measures. Because, as earlier alluded to, the performance of the model is similar across classes when it is the favorable class, only the lower true rate for the unfavorable outcome for the <strong class="source-inline">Between 25 and 40</strong> class seems to affect the fairness of the model. Because models learn from past data, exploring the features that might be responsible for this bias might be crucial in taking further actions. <em class="italic">Figure 13.5</em> shows the <strong class="bold">Cross-Class Data Disparity</strong> capability, which presents deep dives into why bias exists in ML models:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="Images/B17159_13_05.jpg" alt="Figure 13.5 – A Cross-Class Data Disparity comparison between two age classes&#13;&#10;" width="1650" height="573"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – A Cross-Class Data Disparity comparison between two age classes</p>
			<p>To explore the rationale behind the model bias, the <strong class="bold">Cross-Class Data Disparity</strong> comparison compares the data distribution across two groups of a protected feature. In doing so, it presents the importance of the features against their distributional disparity. Of lowest importance, yet for obvious reasons with the largest disparity, the <strong class="source-inline">Age-Group</strong> feature seems to affect the model's accuracy. This is because <strong class="source-inline">Age-Group</strong>, being the predicted variable, will have the largest disparity in comparison to other variables, as it is identical to the predicted variable. The <strong class="source-inline">year</strong> book had a lower data disparity but had greater importance than the <strong class="source-inline">Age-Group</strong> feature. Further examination of the distribution of the <a id="_idIndexMarker674"/>year in the right-hand chart (<em class="italic">Figure 13.6</em>) shows that older books and books with a missing year seem to have been rated more by the <strong class="source-inline">Over Forty</strong> group in comparison with the <strong class="source-inline">Between 25 and 40</strong> group. On the contrary, the <strong class="source-inline">Between 25 and 40</strong> cohorts seem to be rated more of the newer books than their older counterparts. </p>
			<p>When model bias exceeds an enterprise-established threshold, steps need to be taken to manage this unfairness. Options to address this unfairness include dropping features that might be responsible for the bias and retraining the model, or changing the model for a more ethical model. Most of the time, these changes ultimately affect the overall accuracy of the model. However, in our example case, <strong class="source-inline">Light Gradient Boosting on Elastic Predictions</strong> wasn't our best-performing model. DataRobot has within its bias and fairness toolk<a id="_idTextAnchor195"/>it the <strong class="bold">Bias vs Accuracy</strong> leaderboard comparison capability (see <em class="italic">Figure 13.6</em>):</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="Images/B17159_13_06.jpg" alt="Figure 13.6 – Bias vs Accuracy leaderboard&#13;&#10;" width="1650" height="957"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.6 – Bias vs Accuracy leaderboard</p>
			<p>The <strong class="bold">Bias vs Accuracy</strong> chart assesses multiple models on their bias and accuracy. Here, we see that <strong class="source-inline">Keras Residual AutoInt Classifier using Training Schedule</strong> was the most accurate model and met the ethic threshold. In this case, this model could be deployed into <a id="_idIndexMarker675"/>production. It is important to note that neural network-based models are generally not accepted by many regulators today, but this could change in the future.</p>
			<p>Processes concerning the assessment of ML model bias and fairness are expected to be integrated into the data science workflow to ensure model outcomes support a fair process. This becomes more important as conversations pertaining to ethical AI are becoming more ubiquitous across industries. Having looked at ways to ensure models are fair, we now progress into deploying the fair model, monitoring model performance in production, and other aspects of implementing MLOps in the next section.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor196"/>Implementing MLOps</h1>
			<p>DataRobot, through its MLOps suite, provides capabilities to enable users to not only deploy models in production, but govern, monitor, and manage the models in production. In previous chapters, we have looked at how models are deployed on the platform and <a id="_idIndexMarker676"/>using the Python API client. MLOps provides an automated model monitoring capability, which tracks the service health, accuracy, and data drift of models in production. The automated real-time monitoring of production models ensures that models have high-quality outputs. Also, when there is a performance degeneration, stakeholders are notified, so action can be taken.</p>
			<p>In this section, we will focus on aspects of model monitoring that we didn't cover in <a href="B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Scoring and Deployment</em>, of this book. We looked at how to examine the quality of deployment services, as well as changes in the underline feature distribution between the training and prediction data across time through the service health and data drift capabilities. As time passes, more recent data with target variables is introduced to the deployment. DataRobot can then examine models' initial predictions and establish models' actual accuracy in production. DataRobot also provides the capability to switch between alternative models in production. This section focuses on the evaluation of production model accuracy, setting up notifications, as well as switching models in production.</p>
			<p>As you can guess by now, the job of the data science team does not end once a model is deployed. We now must monitor our models in production. After models have been deployed, before engaging in conversations pertaining to the monitoring of models, we need to control what individuals can do with those deployments. Stakeholder roles and responsibilities are important aspects of MLOps governance. Successful implementation of ML solutions depends on a clear definition of roles and what the actual duties of stakeholders are throughout ML models' production life cycles. As <em class="italic">Figure 13.7</em> highlights, when deployments are shared with other stakeholders, each stakeholder is given a role that defines their access level to that deployment:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="Images/B17159_13_07.jpg" alt="Figure 13.7 – Sharing deployments&#13;&#10;" width="1309" height="963"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.7 – Sharing deployments</p>
			<p>To open the deployment sharing window (as shown in <em class="italic">Figure 13.7</em>), after the model was deployed, the deployment action button (the triple dash icon) on the top right-hand side was selected. Then, <strong class="bold">Share</strong> was chosen. Here, this <strong class="bold">RatingClass Predictions</strong> deployment <a id="_idIndexMarker677"/>was shared with a stakeholder, <a href="mailto:ben@aaaa.com">ben@aaaa.com</a>. Importantly, this individual was given the role of <strong class="bold">User</strong>. With the <strong class="bold">User</strong> role, this stakeholder can write and read. In an actual sense, they can view the deployment, consume predictions, view deployment inventory, use the API to get data, and add other users to the deployment. The <strong class="bold">Owner</strong> level has additional administration rights and can perform business-critical operations, such as deleting the deployment, replacing the model, and editing the deployment metadata. The lowest user role is <strong class="bold">Consumer</strong>, which only allows stakeholders the right to consume predictions via the API route.</p>
			<p>Production model monitoring ensures that models continue to deliver high-quality business impact as expected during development. A decline in this quality is a result of an alteration in the production data distribution or changes in the extent to which features affect the endogenous variable. For instance, changes in usage affect customer attrition, a variable of importance to a business. During a holiday period, the predictions for attrition would be higher. Such fluctuations in attrition prediction cause worry to the business if they are not expecting this change in distribution or data drift. In the same way, the extent to which predictor variables could influence a business outcome could also change. A point in case could be the effect of price on the propensity to buy. During the peak <a id="_idIndexMarker678"/>of a pandemic, individuals are far more conservative in their purchase of non-essentials. Now, imagine the chances of the accuracy of an in-production buying propensity model built for a non-essential product built before the pandemic. It is easy to see that the accuracy of the model will decay in production quite rapidly, thus having a significant impact on the business performance. Such situations raise the need to monitor the performance of models post-deployment.</p>
			<p>In <a href="B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Scoring and Deployment</em>, we covered data drift, which examines changes in distribution between the training and production datasets, while accounting for their feature importance. Here, our focus will shift to monitoring the effect of variables on outcomes while in production. Changes in this effect could be established through the monitoring of production models' accuracy, a capability DataRobot offers. As part of the <strong class="bold">Deployments Settings</strong> window, as shown in <em class="italic">Figure 13.8</em>, there is an <strong class="bold">Accuracy</strong> tab:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="Images/B17159_13_08.jpg" alt="Figure 13.8 – Deployment window for accuracy setup&#13;&#10;" width="1573" height="1183"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.8 – Deployment window for accuracy setup</p>
			<p>The <strong class="bold">Accuracy</strong> tab offers insight into the accuracy of production models. This capability allows users <a id="_idIndexMarker679"/>the ability to examine the performance of their production models over time. To compute the accuracy of a production model, actual outcomes need to be provided. After actuals have been uploaded, to generate accuracies, a set of fields needs to be completed. These include the <strong class="bold">Actual Response</strong> and <strong class="bold">Association ID</strong> fields, as well as those that are optional, <strong class="bold">Was acted on</strong> and <strong class="bold">Timestamp</strong> (see <em class="italic">Figure 13.9</em>):</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="Images/B17159_13_09.jpg" alt="Figure 13.9 – Accuracy setup features&#13;&#10;" width="1234" height="708"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.9 – Accuracy setup features</p>
			<p>The <strong class="bold">Actual Response</strong> field specifies where the true outcome is in the data. In this case, the <a id="_idIndexMarker680"/>field is <strong class="source-inline">RatingClass</strong>. To link this to the earlier prediction dataset, <strong class="bold">Association ID</strong>, presented as <strong class="source-inline">rowid</strong> in this example, is requested to enable this connection. It is important to note that sometimes as a result of the models' predictions, action is taken by the business that could ultimately influence the outcome. To account for this possibility in calculating accuracy, the <strong class="bold">Was acted on</strong> and <strong class="bold">Timestamp</strong> variables are optionally requested (see <em class="italic">Figure 13.10</em> for the selection of these features): </p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="Images/B17159_13_10.jpg" alt="Figure 13.10 – Production accuracy identification feature selection&#13;&#10;" width="1650" height="1092"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.10 – Production accuracy identification feature selection</p>
			<p>After the <a id="_idIndexMarker681"/>mandatory variables are selected, the <strong class="bold">Save</strong> button is clicked. This sets the computation off, thereafter opening the <strong class="bold">Accuracy</strong> window, displaying the production accuracy of the model. The performance of the production model is presented as tiles and as a graphical time series. <em class="italic">Figure 13.11</em> presents the <strong class="bold">Deployment Accuracy</strong> window. The <strong class="bold">LogLoss</strong>, <strong class="bold">AUC</strong>, <strong class="bold">Accuracy</strong>, <strong class="bold">Kolmogorov-Smirmov</strong>, and <strong class="bold">Gini Norm</strong> metrics tiles are selected. <strong class="bold">Start</strong> shows the model's performance against the holdout dataset during the development process. It appears that this model is better in production than during the training. Through the customize tiles, other metrics and their order could be chosen. The <strong class="bold">Accuracy over Time</strong> graph shows how the accuracy of the model has changed over time. The leftmost green spot on the graph indicates the model accuracy against the holdout dataset during development:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="Images/B17159_13_11.jpg" alt="Figure 13.11 – production model performance assessment over time&#13;&#10;" width="1650" height="716"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.11 – production model performance assessment over time</p>
			<p>The <strong class="bold">Predicted &amp; Actual</strong> chart tells a similar story. Here, the selected class is <strong class="source-inline">Low</strong>. There is an <a id="_idIndexMarker682"/>option to change the class being explored. It is important to note that with these, the accuracy of the model on the differing levels of the <strong class="source-inline">AgeClass</strong> protected variable could be monitored. This could be done by selecting <strong class="source-inline">AgeClass</strong> in the <strong class="bold">Segment Attribute</strong> option and then choosing either of the levels in the <strong class="bold">Segment Value</strong> field. While in the present scenario production accuracy mirrors those of data drift, it is possible to configure notifications so that stakeholders are notified when metrics depart in a manner that adversely affects the business. In the next section, we will cover these notifications, as well as how to change models in deployment. </p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor197"/>Notifications and changing models in production</h1>
			<p>In this chapter, we have established why the commercial impact of models can decay and ways <a id="_idIndexMarker683"/>to track this impact <a id="_idIndexMarker684"/>in the DataRobot platform. In cases where the end-to-end prediction process is fully automated and human intervention is limited, it becomes crucial that systems that notify stakeholders of any significant changes in the performance of production models are available. DataRobot can send notifications for significant changes in service health, data drift, and accuracy. These notifications can be set up and configured within the <strong class="bold">Deployment</strong> window:</p>
			<ol>
				<li value="1">From the <strong class="bold">Settings</strong> tab, select <strong class="bold">Notifications</strong>. As shown in <em class="italic">Figure 13.12</em>, three options are presented: notifications being sent for all events, notifications for critical <a id="_idIndexMarker685"/>events, and no notifications being sent. Notifications for all events <a id="_idIndexMarker686"/>are sent by email; all changes to the deployments are emailed to the owner:<div id="_idContainer252" class="IMG---Figure"><img src="Images/B17159_13_12.jpg" alt="Figure 13.12 – Deployment Notifications setup&#13;&#10;" width="892" height="858"/></div><p class="figure-caption">Figure 13.12 – Deployment Notifications setup</p><p>In <em class="italic">Figure 13.12</em>, notifications are set to notify me about only critical deployment activities. This setting implies that the stakeholders are notified when there are critical activities occurring on the deployment.</p></li>
				<li>The <strong class="bold">Monitoring</strong> tab (in <em class="italic">Figure 13.13</em>) presents options for defining the notifications that are to be sent. Here, <strong class="bold">Service Health</strong> notification is set to be sent daily at <strong class="source-inline">1:00</strong>. There are options to set notifications to occur anywhere between an hourly and quarterly monitoring cadence. When the box is unchecked, the notification is disabled: <div id="_idContainer253" class="IMG---Figure"><img src="Images/B17159_13_13.jpg" alt="Figure 13.13 – Monitoring notification setup&#13;&#10;" width="1449" height="1035"/></div><p class="figure-caption">Figure 13.13 – Monitoring notification setup</p></li>
				<li>Notification for <strong class="bold">Data Drift</strong> has a few thresholds and configurations to be completed. As discussed in <a href="B17159_08_Final_NM_ePub.xhtml#_idTextAnchor116"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Scoring and Deployment</em>, data drift compares <a id="_idIndexMarker687"/>the distribution of incoming data <a id="_idIndexMarker688"/>to that used for the model development. Essentially, it looks at how recent production data differs from the training data across all features. Setting up a <strong class="bold">Data Drift</strong> notification involves the following considerations:<ul><li><strong class="bold">Range</strong> defines the period from which data is drawn to be compared with the development data. For the example in <em class="italic">Figure 13.13</em>, the range is set to <strong class="source-inline">Last 7 days</strong>, meaning that the data distribution for the preceding seven days is compared with that of the training data.</li><li>Being a feature drift metric, the <strong class="bold">Population Stability Index (PSI)</strong> threshold defines the extent to which feature drift needs to occur for a notification to be triggered. Here, the threshold is set to <strong class="source-inline">0.2</strong>. Some features can be excluded from drift tracking using the <strong class="bold">excluded features</strong> options.</li><li>The <strong class="bold">Feature Importance</strong> threshold allows users to define the threshold that differentiates the most important features from others is. In<em class="italic"> Figure 13.13</em>, <strong class="source-inline">0.45</strong> is entered as the <strong class="bold">Permutation Importance</strong> metric threshold to achieve this goal. By so doing, features with permutation importance over <strong class="source-inline">0.45</strong> are deemed as <strong class="bold">Failing</strong>, while those with lower importance are considered <strong class="bold">At Risk</strong>. Here, some <a id="_idIndexMarker689"/>features are seen as important irrespective of the actual feature importance, and these can be selected using the <strong class="bold">starred features</strong> options.</li><li>The <strong class="bold">At Risk</strong> and <strong class="bold">Failing</strong> thresholds alternatively enable the configuration of the minimum <a id="_idIndexMarker690"/>number of low- and high-importance features that are necessary for sending <strong class="bold">At Risk</strong> or <strong class="bold">Failing</strong> notifications. The rule present in <em class="italic">Figure 13.13</em> allows the following:<p>a. At-risk notifications to be sent when two or more low-importance features (with <strong class="bold">Permutation Importance</strong> of less than <strong class="source-inline">0.45</strong>) drifts beyond a <strong class="bold">PSI</strong> of <strong class="source-inline">0.2</strong> </p><p>b. Failing notifications to be sent when five or more low-importance features have significant drift</p><p>c. Failing notifications to be sent when one or more high-importance features have a drift whose <strong class="bold">PSI</strong> exceeds <strong class="source-inline">0.45</strong></p></li></ul></li>
				<li>Notifications pertaining to the <strong class="bold">Accuracy</strong> production model need to be set up within the monitoring window (as shown in <em class="italic">Figure 13.13</em>). Here, the <strong class="bold">Metric</strong> accuracy, <strong class="bold">Measurement</strong> threshold, and rules for <strong class="bold">At Risk</strong> and <strong class="bold">Failing</strong> notifications are defined:<p>a. Because the deployment is based on a classification problem, its <strong class="bold">Metric</strong> accuracy is selected from classification options. These include <strong class="source-inline">AUC</strong>, <strong class="source-inline">Accuracy</strong>, <strong class="source-inline">Balance Accuracy</strong>, <strong class="source-inline">LogLoss</strong>, and <strong class="source-inline">FVE Binomial</strong>, among others. In this case, <strong class="source-inline">Logloss</strong> is chosen.</p><p>b. The <strong class="bold">Measurement</strong> option defines how changes in the accuracy metric between production prediction and training data are compared. Here, the <strong class="source-inline">percent</strong> change is selected.</p><p>c. Rules are then set for <strong class="bold">At Risk</strong> and <strong class="bold">Failing</strong> notifications to be sent. In this case, <strong class="bold">At Risk</strong> notifications are sent when the accuracy of the model for prediction data is 5% that of the training. Similarly, 10% is the threshold that triggers a <strong class="bold">Failing</strong> notification.</p><p>d. As with <strong class="bold">Data Drift</strong>, the <strong class="bold">Accuracy</strong> notifications are set to be sent <strong class="source-inline">Every day</strong> at <strong class="source-inline">1:00</strong>. These could be configured to any cadence between daily and quarterly.</p></li>
				<li>After this setup, clicking the <strong class="bold">Save new setting</strong> button activates the notification routine. However, it is noteworthy that any stakeholder who has access to <a id="_idIndexMarker691"/>the deployment can configure the notifications they want to receive. When models' changes become significant, it might become <a id="_idIndexMarker692"/>necessary to replace the model in the deployment.<p>The performance of production models tends to decay with time. This raises the need for models to replace those in a deployment. Within the MLOps offerings, DataRobot offers the model replacement functionality. To change the model in a deployment, you navigate to the <strong class="bold">Deployment Overview</strong> window. The <strong class="bold">Replace model</strong> option is selected from the <strong class="bold">Action</strong> button on the right-hand side of the <strong class="bold">Deployment Overview </strong>window (see <em class="italic">Figure 13.14</em>):</p></li>
				<li>Clicking the <strong class="bold">Replace model</strong> option presents a <strong class="bold">Paste DataRobot model URL</strong> request. This URL is for the location at which the new model can be found when opened from the leaderboard:<div id="_idContainer254" class="IMG---Figure"><img src="Images/B17159_13_14.jpg" alt="Figure 13.14 – Production model replacement&#13;&#10;" width="882" height="575"/></div><p class="figure-caption">Figure 13.14 – Production model replacement</p></li>
				<li>When <strong class="bold">Select</strong> is clicked, there is a prompt for the rationale for the model replacement. For <a id="_idIndexMarker693"/>this, options <a id="_idIndexMarker694"/>for <strong class="bold">Replacement Reason</strong> include <strong class="source-inline">Accuracy</strong>, <strong class="source-inline">Data Drift</strong>, <strong class="source-inline">Errors</strong>, <strong class="source-inline">Scheduled Refresh</strong>, and <strong class="source-inline">Scoring Speed</strong>. As shown in <em class="italic">Figure 13.15</em>, <strong class="source-inline">Data Drift</strong> is selected in this case.</li>
				<li>Finally, <strong class="bold">Accept and replace</strong> is clicked:</li>
			</ol>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="Images/B17159_13_15.jpg" alt="Figure 13.15 – Selecting the rationale for model replacement&#13;&#10;" width="1096" height="646"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.15 – Selecting the rationale for model replacement</p>
			<p>Having replaced the model in the deployment, future predictions from this deployment will use the updated model. It is important to highlight that model replacement can only be carried out <a id="_idIndexMarker695"/>by the deployment <a id="_idIndexMarker696"/>owner. There are situations when the commercial impact of models is significant. In such situations, it is advisable to test the new or challenger model in a synthetic or simulated environment before switching models. In the typical data science workflow, the champion/challenger model scenario is well established. Here, challenger models compute predictions and their performance is compared with the one in production, the champion. With the testing and impact analyses complete, we are now ready to deploy our model. DataRobot provides data scientists the ability to test multiple challenger models while the champion is in production. This simplifies the model selection process when a model is to be replaced.</p>
			<p>MLOps also offers the capability for changes in the model to be reviewed by different stakeholders. For this to happen, models are assigned importance levels as part of their deployments. These importance levels depend on the strategic commercial impact the model outcomes have on the business, the volume of predictions, and regulatory expectations. The importance levels thereafter drive who needs to review changes of the deployments before they are implemented.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor198"/>Summary</h1>
			<p>In this chapter, we highlighted the value of establishing a framework guiding the use of ML models in businesses. ML governance capability supports users in ensuring that ML models continue to deliver commercial value while meeting regulatory expectations. Also, we set controls for what different levels of stakeholders can do with ML deployments. In some industries, there is a need to seriously consider the impact of bias in any decision process. Because ML models are based on data that might have been affected by human bias, it is possible that these models will compound such bias. As such, we explored ways to mitigate ML bias during and after model development.</p>
			<p>We also examined the effects features have on the outcome variable. Such changes could have a critical bearing on business outcomes, hence the need to monitor the performance of model outcomes in production. During this chapter, we explored ways the performance of models could be assessed over time. Importantly, we learned how to configure notifications when there are significant changes in data drift or/and model accuracy. Additionally, we examined how a model in production could be switched to a challenger as needed. </p>
			<p>We also highlighted some other MLOps features that were not covered in depth as part of this chapter. In the next chapter, we are going to look at what we think the future holds for DataRobot and automated ML in general. Also, given that this book is not all-encompassing with regards to DataRobot and the platform keeps expanding its capabilities, in the next chapter, we will point out some places where additional information for further development could be accessed.</p>
		</div>
	</div></body></html>