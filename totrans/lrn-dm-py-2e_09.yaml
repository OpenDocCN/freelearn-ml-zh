- en: Authorship Attribution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者身份归属
- en: '**Authorship analysis** is a text mining task that aims to identify certain
    aspects about an author, based only on the content of their writings. This could
    include characteristics such as age, gender, or background. In the specific **authorship
    attribution** task, we aim to identify which of a set of authors wrote a particular
    document. This is a classic classification task. In many ways, authorship analysis
    tasks are performed using standard data mining methodologies, such as cross-fold
    validation, feature extraction, and classification algorithms.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者身份分析**是一种文本挖掘任务，旨在仅根据作者的写作内容来识别关于作者的一些方面。这可能包括诸如年龄、性别或背景等特征。在具体的**作者身份归属**任务中，我们的目标是确定一组作者中哪位作者撰写了特定的文档。这是一个经典的分类任务。在许多方面，作者身份分析任务都是通过标准的数据挖掘方法来执行的，例如交叉验证、特征提取和分类算法。'
- en: 'In this chapter, we will use the problem of authorship attribution to piece
    together the parts of the data mining methodology we developed in the previous
    chapters. We identify the problem and discuss the background and knowledge of
    the problem. This lets us choose features to extract, which we will build a pipeline
    for achieving. We will test two different types of features: function words and
    character n -grams. Finally, we will perform an in-depth analysis of the results.
    We will work first with a dataset of books, and then a messy, real-world corpus
    of e-mails.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用作者身份归属问题来整合我们在前几章中开发的数据挖掘方法的部分。我们确定问题并讨论问题的背景和知识。这使得我们可以选择要提取的特征，我们将为此构建一个管道。我们将测试两种不同类型的特征：功能词和字符n-gram。最后，我们将对结果进行深入分析。我们首先将使用一组书籍数据集，然后是一个混乱的、现实世界的电子邮件语料库。
- en: 'The topics we will cover in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Feature engineering and how feature choice differs based on application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程以及特征选择如何根据应用而不同
- en: Revisiting the bag-of-words model with a specific goal in mind
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带着特定目标重新审视词袋模型
- en: Feature types and the character n-grams model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征类型和字符n-gram模型
- en: Support Vector Machines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Cleaning up a messy dataset for data mining
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理用于数据挖掘的混乱数据集
- en: Attributing documents to authors
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文档归因于作者
- en: Authorship analysis has a background in **stylometry**, which is the study of
    an author's style of writing. The concept is based on the idea that everyone learns
    language slightly differently, and that measuring these nuances in people's writing
    will enable us to tell them apart using only the content of their writing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作者身份分析有**风格学**的背景，这是研究作者写作风格的研究。这个概念基于这样一个想法，即每个人学习语言的方式略有不同，通过测量人们写作中的这些细微差别，我们可以仅使用他们写作的内容来区分他们。
- en: Authorship analysis has historically (pre-1990) been performed using repeatable
    manual analysis and statistics, which is a good indication that it could be automated
    with data mining. Modern authorship analysis studies are almost entirely data
    mining-based, although quite a significant amount of work is still done with more
    manually driven analysis using linguistic styles and stylometrics. Many of the
    advances in feature engineering today are driven by advances in stylometrics.
    In other words, manual analysis discovers new features, which are then codified
    and used as part of the data mining process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作者身份分析在历史上（1990年之前）一直是通过可重复的手动分析和统计来执行的，这是一个很好的迹象，表明它可以利用数据挖掘进行自动化。现代作者身份分析研究几乎完全是基于数据挖掘的，尽管相当大一部分工作仍然是使用更多手动驱动的分析，如语言风格和风格学。今天在特征工程方面的许多进步都是由风格学的进步驱动的。换句话说，手动分析发现了新的特征，然后这些特征被编码并作为数据挖掘过程的一部分使用。
- en: A key underlying feature of stylometry is that of **writer invariants**, which
    are features that a particular author has in all of their documents, but are not
    shared with other authors. In practice these writer invariants do not seem to
    exist, as authorship styles change over time, but the use of data mining can get
    us close to classifiers working off this principle.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 风格学的一个关键基础特征是**作者不变性**，即特定作者在其所有文档中都有的特征，但与其他作者不共享。在实践中，这些作者不变性似乎并不存在，因为作者的风格会随时间而变化，但数据挖掘的使用可以使我们接近基于这一原则工作的分类器。
- en: 'As a field, authorship analysis has many sub-problems, and the main ones are
    as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一门学科，作者身份分析有许多子问题，其中主要问题如下：
- en: '**Authorship profiling:** This determines the age, gender, or other traits
    of the author based on the writing. For example, we can detect the first language
    of a person speaking English by looking for specific ways in which they speak
    the language.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作者身份分析**：这是根据写作确定作者的年龄、性别或其他特征。例如，我们可以通过观察他们使用英语的具体方式来检测说英语的人的第一语言。'
- en: '**Authorship verification:** This checks whether the author of this document
    also wrote the other document. This problem is what you would normally think about
    in a legal court setting. For instance, the suspect''s writing style (content-wise)
    would be analyzed to see if it matched the ransom note.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作者身份验证**：这是检查这份文件的作者是否也撰写了另一份文件。这个问题是你通常在法律法庭环境中会考虑的问题。例如，嫌疑人的写作风格（从内容上分析）将被分析，以查看它是否与勒索信相符。'
- en: '**Authorship clustering:** This is an extension of authorship verification,
    where we use cluster analysis to group documents from a big set into clusters,
    and each cluster is written by the same author.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作者身份聚类**：这是作者身份验证的扩展，我们使用聚类分析将大量文档分组到簇中，每个簇由同一作者撰写。'
- en: However, the most common form of authorship analysis study is that of **authorship
    attribution**, a classification task where we attempt to predict which of a set
    of authors wrote a given document.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最常见的作者身份分析研究形式是**作者身份归因**，这是一个分类任务，我们试图预测一组作者中哪位撰写了给定的文件。
- en: Applications and use cases
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用案例
- en: Authorship analysis has a number of **use cases**. Many use cases are concerned
    with problems such as verifying authorship, proving shared authorship/provenance,
    or linking social media profiles with real-world users.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者身份分析有许多**应用案例**。许多案例都与验证作者身份、证明共同作者身份/来源或关联社交媒体资料与真实用户等问题相关。
- en: In a historical sense, we can use authorship analysis to verify whether certain
    documents were indeed written by their supposed authors. Controversial authorship
    claims include some of Shakespeare's plays, the Federalist papers from the USA's
    foundation period, and other historical texts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史的角度来看，我们可以使用作者身份分析来验证某些文件是否确实是由其声称的作者撰写的。有争议的作者身份索赔包括一些莎士比亚的戏剧、美国建国时期的《联邦党人文集》以及其他历史文献。
- en: Authorship studies alone cannot prove authorship but can provide evidence for
    or against a given theory, such as whether a particular person wrote a given document.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的作者身份研究不能证明作者身份，但可以为或反对某个理论提供证据，例如是否有人撰写了给定的文件。
- en: For example, we can analyze Shakespeare's plays to determine his writing style,
    before testing whether a given sonnet actually does originate from him (some recent
    research indicates multiple authorship of some of his work).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以分析莎士比亚的戏剧来确定他的写作风格，然后再测试给定的十四行诗是否确实出自他之手（一些最近的研究表明他的某些作品有多位作者）。
- en: A more modern use case is that of linking social network accounts. For example,
    a malicious online user could set up accounts on multiple online social networks.
    Being able to link them allows authorities to track down the user of a given account—for
    example if a person is harassing other online users.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更现代的一个用例是链接社交网络账户。例如，一个恶意在线用户可能在多个在线社交网络上建立账户。能够将它们联系起来，使当局能够追踪特定账户的用户——例如，如果有人在网上骚扰其他用户。
- en: Another example used in the past is to be a backbone to provide expert testimony
    in court to determine whether a given person wrote a document. For instance, the
    suspect could be accused of writing an e-mail harassing another person. The use
    of authorship analysis could determine whether it is likely that person did, in
    fact, write the document. Another court-based use is to settle claims of stolen
    authorship. For example, two authors may claim to have written a book, and authorship
    analysis could provide evidence on which is the more likely author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 过去使用的一个例子是作为骨干在法庭上提供专家证词，以确定某个人是否撰写了某份文件。例如，嫌疑人可能被指控撰写了一封骚扰他人的电子邮件。使用作者身份分析可以确定那个人实际上是否真的撰写了该文件。另一种基于法庭的使用是解决被盗作者身份的索赔。例如，两位作者可能声称撰写了一本书，作者身份分析可以提供证据，证明哪位作者更有可能是真正的作者。
- en: Authorship analysis is not foolproof, though. A recent study found that attributing
    documents to authors can be made considerably harder by simply asking people,
    who are otherwise untrained, to hide their writing style. This study also looked
    at a framing exercise where people were asked to write in the style of another
    person. This framing of another person proved quite reliable, with the faked document
    commonly attributed to the person being framed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管作者归属分析并非万无一失，但最近的一项研究发现，仅仅要求那些未经训练的人隐藏他们的写作风格，就可以使将文档归因于作者变得更加困难。这项研究还考察了一个框架练习，其中人们被要求以另一个人的风格写作。这种对另一个人风格的模仿证明相当可靠，伪造的文档通常被归因于被模仿的人。
- en: Despite these issues, authorship analysis is proving useful in a growing number
    of areas and is an interesting data mining problem to investigate.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些问题，作者归属分析在越来越多的领域中被证明是有用的，并且是一个有趣的数据挖掘问题进行研究。
- en: Authorship attribution can be used in expert testimony, but by itself is hard
    to classify as hard evidence. Always check with a lawyer before using it for formal
    matters, such as authorship disputes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者归属可以用于专家证词，但仅凭它本身很难被归类为硬证据。在使用它来解决正式事项，如作者归属争议之前，请务必咨询律师。
- en: Authorship attribution
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者归属
- en: '**Authorship attribution** (as distinct from authorship *analysis*) is a classification
    task by which we have a set of candidate authors, a set of documents from each
    of those authors namely the **training set**, and a set of documents of unknown
    authorship otherwise known as the test set. If the documents of unknown authorship
    definitely belong to one of the candidates, we call this a closed problem, as
    per the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者归属**（与作者**分析**不同）是一个分类任务，其中我们有一组候选作者，每个作者有一组文档，即所谓的**训练集**，以及一组未知作者身份的文档，通常称为测试集。如果未知作者身份的文档肯定属于候选人之一，我们称之为封闭问题，如下面的图所示：'
- en: '![](img/B06162_09_01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_09_01.png)'
- en: 'If we cannot be sure of that the actual author is part of the training set,
    we call this an open problem. This distinction isn''t just specific to authorship
    attribution - any data mining application where the actual class may not be in
    the training set is considered an open problem, with the task being to find the
    candidate author or to select none of them. This is shown in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不能确定实际作者是否是训练集的一部分，我们称之为开放问题。这种区别不仅限于作者归属——任何实际类别可能不在训练集中的数据挖掘应用都被认为是开放问题，任务就是找到候选作者或选择他们中的任何一个。这如下面的图所示：
- en: '![](img/B06162_09_02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_09_02.png)'
- en: 'In authorship attribution, we typically have two restrictions on the tasks.
    They have been listed as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在作者归属中，我们通常有两个任务限制。它们如下列出：
- en: First, we only use content information from the documents - not metadata regarding
    the time of writing, delivery, handwriting style, and so on. There are ways to
    combine models from these different types of information, but that isn't generally
    considered authorship attribution and is more a **data fusion** application.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们只使用文档的内容信息——不是关于写作时间、交付、手写风格等方面的元数据。有方法可以结合来自这些不同类型信息的数据模型，但这通常不被认为是作者归属，而更多的是一个**数据融合**应用。
- en: The second restriction is that we don't look at the topic of the documents;
    instead, we look for more salient features such as word usage, punctuation, and
    other text-based features. The reasoning here is that a person can write on many
    different topics, so worrying about the topic of their writing isn't going to
    model their actual authorship style. Looking at topic words can also lead to **overfitting**
    on the training data—our model may train on documents from the same author and
    also on the same topic. For instance, if you were to model my authorship style
    by looking at this book, you might conclude the words *data mining* are indicative
    of *my* writing style when, in fact, I write on other topics as well.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个限制是我们不关注文档的主题；相反，我们寻找更显著的特征，如词汇使用、标点符号和其他基于文本的特征。这里的推理是，一个人可以就许多不同的主题进行写作，因此担心他们写作的主题并不能模拟他们的实际写作风格。查看主题词也可能导致训练数据上的**过拟合**——我们的模型可能在同一作者的同一主题的文档上进行训练。例如，如果你通过查看这本书来模拟我的写作风格，你可能会得出结论，单词“数据挖掘”是**我的**写作风格的指示性特征，而实际上我也在其他主题上写作。
- en: From here, the pipeline for performing authorship attribution looks a lot like
    the one we developed in [Chapter 6](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)*,
    Social Media Insight Using Naive Bayes*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，执行作者归属分析的流程与我们在第 6 章[ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml]*，使用朴素贝叶斯进行社交媒体洞察*中开发的流程非常相似。
- en: First, we extract features from our text.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从我们的文本中提取特征。
- en: Then, we perform some feature selection on those features.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对这些特征进行一些特征选择。
- en: Finally, we train a classification algorithm to fit a model, which we can then
    use to predict the class (in this case, the author) of a document.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们训练一个分类算法来拟合模型，然后我们可以使用它来预测文档的类别（在这种情况下，作者）。
- en: There are some differences between classifying content and classifying authorship,
    mostly having to do with which features are used, that we will cover in this chapter.
    It is critical to choose features based on the application.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在内容分类和作者分类之间有一些区别，主要与使用哪些特征有关，我们将在本章中介绍。根据应用选择特征至关重要。
- en: Before we delve into these issue, we will define the scope of the problem and
    collect some data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究这些问题之前，我们将定义问题的范围并收集一些数据。
- en: Getting the data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'The data we will use for the first part of this chapter is a set of books from
    **Project Gutenberg** at [www.gutenberg.org](http://www.gutenberg.org), which
    is a repository of public domain literature works. The books I used for these
    experiments come from a variety of authors:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于本章第一部分的数据是一套来自 [www.gutenberg.org](http://www.gutenberg.org) 的 **Project
    Gutenberg** 的书籍，这是一个公共领域文学作品库。我用于这些实验的书籍来自各种作者：
- en: Booth Tarkington (22 titles)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布思·塔金顿（22部作品）
- en: Charles Dickens (44 titles)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查尔斯·狄更斯（44部作品）
- en: Edith Nesbit (10 titles)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊迪丝·内斯比特（10部作品）
- en: Arthur Conan Doyle (51 titles)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚瑟·柯南·道尔（51部作品）
- en: Mark Twain (29 titles)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马克·吐温（29部作品）
- en: Sir Richard Francis Burton (11 titles)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理查德·弗朗西斯·伯顿爵士（11部作品）
- en: Emile Gaboriau (10 titles)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 埃米尔·加博里奥（10部作品）
- en: Overall, there are 177 documents from 7 authors, giving a significant amount
    of text to work with. A full list of the titles, along with download links and
    a script to automatically fetch them, is given in the code bundle called getdata.py.
    If running the code results in significantly fewer books than above, the mirror
    may be down. See this website for more mirror URLs to try in the script: [https://www.gutenberg.org/MIRRORS.ALL](https://www.gutenberg.org/MIRRORS.ALL)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有 177 篇来自 7 位作者的文章，提供了大量的文本供我们使用。文章的完整列表，包括下载链接和自动获取它们的脚本，可以在名为 getdata.py
    的代码包中找到。如果运行代码的结果比上面少得多，镜像可能已关闭。请参阅此网站以获取更多镜像 URL 以在脚本中尝试：[https://www.gutenberg.org/MIRRORS.ALL](https://www.gutenberg.org/MIRRORS.ALL)
- en: To download these books, we use the requests library to download the files into
    our data directory.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载这些书籍，我们使用 requests 库将文件下载到我们的数据目录中。
- en: 'First, in a new Jupyter Notebook, set up the data directory and ensure the
    following code links to it:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在一个新的 Jupyter Notebook 中，设置数据目录并确保以下代码链接到它：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, download the data bundle from the code bundle supplied by Packt. Decompress
    the file into this directory. The books folder should then directly contain one
    folder for each author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从 Packt 提供的代码包中下载数据包。将文件解压缩到该目录中。书籍文件夹应直接包含每个作者的文件夹。
- en: After taking a look at these files, you will see that many of them are quite
    messy—at least from a data analysis point of view. There is a large project Gutenberg
    disclaimer at the start of the files. This needs to be removed before we do our
    analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看这些文件后，你会发现其中许多文件相当杂乱——至少从数据分析的角度来看。文件开头有一个很大的 Project Gutenberg 声明。在我们进行分析之前，需要将其删除。
- en: 'For example, most books begin with information such as the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，大多数书籍以以下信息开始：
- en: '*The* *Project Gutenberg eBook of Mugby Junction, by Charles Dickens, et al, **Illustrated
    by Jules A.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*《Project Gutenberg 电子书：Mugby Junction》，作者：查尔斯·狄更斯等人，由 Jules A. **插画**'
- en: Goodman This eBook is for the use of anyone anywhere at no cost and with*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 古德曼 这本电子书对任何地方的任何人都是免费的，并且可以*
- en: '*almost no restrictions whatsoever. You may copy it, give it away or*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*几乎没有任何限制。您可以复制它，赠送它或* '
- en: '*re-use it under the terms of the Project Gutenberg License included*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*根据 Project Gutenberg 许可证重新使用*'
- en: '*with this eBook or online at www.gutenberg.org*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*本电子书或在线于 www.gutenberg.org*'
- en: '*Title: Mugby Junction*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*标题：Mugby Junction*'
- en: '*Author: Charles Dickens*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：查尔斯·狄更斯*'
- en: '*Release Date: January 28, 2009 [eBook #27924]Language: English*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*发布日期：2009年1月28日 [电子书 #27924]语言：英语*'
- en: '*Character set encoding: UTF-8'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*字符集编码：UTF-8'
- en: '***START OF THE PROJECT GUTENBERG EBOOK MUGBY JUNCTION****'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '***《古腾堡项目》MUGBY JUNCTION 电子书开始***'
- en: After this point, the actual text of the book starts. The use of a line starting
    ***START OF THE PROJECT GUTENBERG is fairly consistent, and we will use that as
    a cue on when the text starts - anything before this line will be ignored.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点之后，书籍的实际文本开始。使用以***《古腾堡项目》开始***开头的行相当一致，我们将使用这个作为文本开始的线索——任何在此行之前的都将被忽略。
- en: 'We could alter the individual files on disk to remove this stuff. However,
    what happens if we were to lose our data? We would lose our changes and potentially
    be unable to replicate the study. For that reason, we will perform the preprocessing
    as we load the files—this allows us to be sure our results will be replicable
    (as long as the data source stays the same). The following code removes the main
    source of noise from the books, which is the prelude that Project Gutenberg adds
    to the files:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更改磁盘上的单个文件来移除这些内容。然而，如果我们丢失了数据会怎样？我们会丢失我们的更改，并且可能无法复制这项研究。因此，我们将预处理作为加载文件时执行——这使我们能够确保我们的结果将是可复制的（只要数据源保持不变）。以下代码移除了书籍中的主要噪声源，即古腾堡项目添加到文件中的序言：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may want to add to this function to remove other sources of noise, such
    as inconsistent formatting, footer information, and so on. Investigate the files
    to examine what issues they have.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想添加到这个函数中，以移除其他噪声源，例如不一致的格式、页脚信息等。调查文件以检查它们有什么问题。
- en: We can now get our documents and classes using the following function, which
    loops through these folders, loads the text documents and records a number assigned
    to the author as the target class.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下函数获取我们的文档和类别，该函数会遍历这些文件夹，加载文本文档，并将分配给作者的编号作为目标类别。
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then call this function to actually load the books:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来调用这个函数来实际加载书籍：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This dataset fits into memory quite easily, so we can load all of the text at
    once. In cases where the whole dataset doesn't fit, a better solution is to extract
    the features from each document one-at-a-time (or in batches) and save the resulting
    values to a file or in-memory matrix
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集很容易放入内存，因此我们可以一次性加载所有文本。在整个数据集不适合的情况下，更好的解决方案是逐个（或批量）提取每个文档的特征，并将结果值保存到文件或内存矩阵中。
- en: 'To get a gauge on the properties of the data, one of the first things I usually
    do is create a simple histogram of the document lengths. If the lengths are relatively
    consistent, this is often easier to learn from than wildly different document
    lengths. In this case, there is quite a large variance in document lengths. To
    view this, first we extract the lengths into a list:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解数据的属性，我通常首先做的事情之一是创建一个简单的文档长度直方图。如果长度相对一致，这通常比文档长度差异很大更容易学习。在这种情况下，文档长度有很大的差异。要查看这一点，首先我们将长度提取到一个列表中：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we plot those. Matplotlib has a `hist` function that will do this, as
    does Seaborn, which produces nicer looking graphs by default.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制这些数据。Matplotlib有一个`hist`函数可以做到这一点，Seaborn也可以，它默认产生更美观的图表。
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting graph shows the variation in document lengths:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示了文档长度的变化：
- en: '![](img/B06162_09_06.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_09_06.png)'
- en: Using function words
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用功能词
- en: One of the earliest types of features, and one that still works quite well for
    authorship analysis, is to use function words in a bag-of-words model. Function
    words are words that have little meaning on their own, but are required for creating
    (English!) sentences. For example, the words *this* and *which* are words that
    are really only defined by what they do within a sentence, rather than their meaning
    in themselves. Contrast this with a content word such as *tiger*, which has an
    explicit meaning and invokes imagery of a large cat when used in a sentence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 早期类型的一种特征，并且对于作者身份分析仍然相当有效，是使用词袋模型中的功能词。功能词是本身意义不大的词，但却是创建（英语！）句子所必需的。例如，*这个*和*那个*这样的词，它们实际上只由它们在句子中的功能定义，而不是它们本身的意义。这与像*老虎*这样的内容词形成对比，内容词有明确的意义，并在句子中使用时唤起大型猫的形象。
- en: The set of words that are considered function words is not always obvious. A
    good rule of thumb is to choose the most frequent words in usage (over all possible
    documents, not just ones from the same author).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为是功能词的词集并不总是显而易见的。一个好的经验法则是选择使用频率最高的词（在所有可能的文档中，而不仅仅是同一作者的文档）。
- en: Typically, the more frequently a word is used, the better it is for authorship
    analysis. In contrast, the less frequently a word is used, the better it is for
    content-based text mining, such as in the next chapter, where we look at the topic
    of different documents.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个词使用得越频繁，对作者身份分析就越好。相反，一个词使用得越少，对基于内容的文本挖掘就越好，例如在下一章中，我们将探讨不同文档的主题。
- en: 'The graph here gives a better idea between word and frequency relationship:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此图展示了单词与频率之间的关系：
- en: '![](img/B06162_09_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_09_03.png)'
- en: The use of function words is less defined by the content of the document and
    more by the decisions made by the author. This makes them good candidates for
    separating the authorship traits between different users. For instance, while
    many Americans are particular about the different in usage between *that* and
    *which* in a sentence, people from other countries, such as Australia, are less
    concerned with the distinction. This means that some Australians will lean towards
    almost exclusively using one word or the other,  while others may use *which*
    much more.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 功能词的使用更多地由作者的决策决定，而不是文档的内容。这使得它们成为区分不同用户作者特征的理想候选词。例如，虽然许多美国人特别注意句子中*that*和*which*的用法差异，但来自其他国家的人，如澳大利亚人，对此区别不太关心。这意味着一些澳大利亚人可能会几乎只使用一个词或另一个词，而其他人可能会更多地使用*which*。
- en: This difference, combined with thousands of other nuanced differences, makes
    a model of authorship.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异，加上成千上万的细微差异，构成了作者身份模型。
- en: Counting function words
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数功能词
- en: We can count function words using the CountVectorizer class we used in [Chapter
    6](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)*, Social Media Insight Using Naive
    Bayes*. This class can be passed a vocabulary, which is the set of words it will
    look for. If a vocabulary is not passed (we didn't pass one in the code of [Chapter
    6](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)*, Social Media Insight Using Naive
    Bayes*), then it will learn this vocabulary from the training dataset. All the
    words are in the training set of documents (depending on the other parameters
    of course).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在[第6章](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)*，使用朴素贝叶斯进行社交媒体洞察*中使用的CountVectorizer类来计数功能词。这个类可以传递一个词汇表，即它将查找的单词集合。如果没有传递词汇表（我们在[第6章](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)*，使用朴素贝叶斯进行社交媒体洞察*的代码中没有传递），那么它将从训练数据集中学习这个词汇表。所有单词都在文档的训练集中（当然，取决于其他参数）。
- en: 'First, we set up our vocabulary of function words, which is just a list containing
    each of them. Exactly which words are function words and which are not is up for
    debate. I''ve found the following list, from published research, to be quite good,
    obtained from my own research combining word lists from other researchers. Remember
    that the code bundle is available from Packt publishing (or the official github
    channel), and therefore you don''t need to type this out:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置我们的功能词词汇表，它只是一个包含每个单词的列表。哪些是功能词，哪些不是，这是一个有争议的问题。我发现以下列表，来自发表的研究，相当不错，是从我自己的研究中获得的，结合了其他研究者的单词列表。请记住，代码包可以从Packt出版社（或官方GitHub频道）获得，因此您不需要亲自输入：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we can set up an extractor to get the counts of these function words. Note
    the passing of the function words list as the `vocabulary` into the `CountVectorizer`
    initialiser.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以设置一个提取器来获取这些功能词的计数。注意将功能词列表作为`vocabulary`传递给`CountVectorizer`初始化器。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For this set of function words, the frequency within these documents is very
    high - as you would expect. We can use the extractor instance to obtain these
    counts, by fitting it on the data, and then calling `transform` (or, the shortcut
    using `fit_transform`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这组功能词，这些文档中的频率非常高——正如预期的那样。我们可以使用提取器实例通过在数据上拟合并调用`transform`（或使用`fit_transform`的快捷方式）来获取这些计数。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before plotting, we normalized these counts by dividing by the relevant document
    lengths. The following code does this, resulting in the percentage of words accounted
    for by each function word:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘图之前，我们通过除以相关文档长度对这些计数进行了归一化。以下代码执行此操作，得到每个功能词所占的百分比：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then average these percentages across all documents:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们平均所有文档的这些百分比：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally we plot them using Matplotlib (Seaborn lacks easy interfaces to basic
    plots like this).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Matplotlib（Seaborn缺乏此类基本图表的简单接口）来绘制它们。
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/B06162_09_07.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_09_07.png)'
- en: Classifying with function words
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用功能词进行分类
- en: The only new thing here is the use of **Support Vector Machines** (**SVM**),
    which we will cover in the next section (for now, just consider it a standard
    classification algorithm).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的新颖之处在于使用了**支持向量机**（**SVM**），我们将在下一节中介绍（现在，只需将其视为一个标准的分类算法）。
- en: 'Next, we import our classes. We import the SVC class, an SVM for classification,
    as well as the other standard workflow tools we have seen before:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入我们的类。我们导入SVC类，这是一个用于分类的SVM，以及我们之前见过的其他标准工作流程工具：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'SVMs take a number of parameters. As I said, we will use one blindly here,
    before going into detail in the next section. We then use a dictionary to set
    which parameters we are going to search. For the `kernel` parameter, we will try
    `linear` and `rbf`. For C, we will try values of 1 and 10 (descriptions of these
    parameters are covered in the next section). We then create a grid search to search
    these parameters for the best choices:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs有多个参数。正如我所说的，在下一节详细说明之前，我们将在这里盲目地使用一个参数。然后我们使用一个字典来设置我们要搜索的参数。对于`kernel`参数，我们将尝试`linear`和`rbf`。对于C，我们将尝试1和10的值（这些参数的描述将在下一节中介绍）。然后我们创建一个网格搜索来搜索这些参数的最佳选择：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Gaussian kernels (such as RBF) only work for reasonably sized data sets, such
    as when the number of features is fewer than about 10,000.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核（如RBF）仅适用于合理大小的数据集，例如当特征数量少于约10,000时。
- en: 'Next, we set up a pipeline that takes the feature extraction step using the
    `CountVectorizer` (only using function words), along with our grid search using
    SVM. The code is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个管道，使用`CountVectorizer`（仅使用功能词）进行特征提取步骤，以及使用SVM的网格搜索。代码如下：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, apply `cross_val_score` to get our cross-validated score for this pipeline.
    The result is 0.811, which means we approximately get 80 percent of the predictions
    correct.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，应用`cross_val_score`来获取这个管道的交叉验证分数。结果是0.811，这意味着我们大约有80%的预测是正确的。
- en: Support Vector Machines
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: SVMs are classification algorithms based on a simple and intuitive idea, backed
    by some complex and innovative mathematics. SVMs perform classification between
    two classes (although we can extend it to more classes using various meta-algorithms),
    by simply drawing a separating line between the two (or a hyperplane in higher-dimensions).
    The intuitive idea is to choose the best line of separation, rather than just
    any specific line.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs是基于简单直观的想法的分类算法，背后有一些复杂和创新数学。SVMs通过在两个类别之间简单地画一条分隔线（或在更高维度中的超平面）来进行分类（尽管我们可以使用各种元算法将其扩展到更多类别）。直观的想法是选择最佳的分隔线，而不仅仅是任何特定的线。
- en: 'Suppose that our two classes can be separated by a line such that any points
    above the line belong to one class and any below the line belong to the other
    class. SVMs find this line and use it for prediction, much the same way as linear
    regression works. SVMs, however, find the best line for separating the dataset.
    In the following figure, we have three lines that separate the dataset: blue,
    black, and green. Which would you say is the best option?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的两个类别可以通过一条线分开，使得线上的任何点属于一个类别，而线下的任何点属于另一个类别。SVMs找到这条线并使用它进行预测，这与线性回归的工作方式非常相似。然而，SVMs会找到最佳的分隔线来分隔数据集。在下面的图中，我们有三条分隔数据集的线：蓝色、黑色和绿色。你会说哪个是最好的选择？
- en: '![](img/B06162_09_04.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_09_04.png)'
- en: Intuitively, a person would normally choose the blue line as the best option,
    as this separates the data in the cleanest way. More formally, it has the maximum
    distance from the line to any point in each class. Finding this line of maximum
    separation is an optimization problem, based on finding the lines of margin with
    the maximum distance between them. Solving this optimisation problem is the main
    task of the training phase of an SVM.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，人们通常会选择蓝色线作为最佳选择，因为它以最干净的方式分隔数据。更正式地说，它具有从线到每个类别中任何点的最大距离。找到这条最大分隔线是一个优化问题，基于找到具有最大距离的边缘线。解决这个优化问题是SVM训练阶段的主要任务。
- en: 'The equations to solve SVMs is outside the scope of this book, but I recommend
    interested readers to go through the derivations at:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 解决SVMs的方程式超出了本书的范围，但我建议感兴趣的读者去查阅以下推导：
- en: '[http://en.wikibooks.org/wiki/Support_Vector_Machines](http://en.wikibooks.org/wiki/Support_Vector_Machines)
    for the details.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://en.wikibooks.org/wiki/Support_Vector_Machines](http://en.wikibooks.org/wiki/Support_Vector_Machines)
    了解详情。'
- en: 'Alternatively, you can visit:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以访问：
- en: '[http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html](http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html](http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)'
- en: Classifying with SVMs
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机（SVMs）进行分类
- en: 'After training the model, we have a line of maximum margin. The classification
    of new samples is then simply asking the question: does it fall above the line,
    or below it? If it falls above the line, it is predicted as one class. If it is
    below the line, it is predicted as the other class.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，我们得到一条最大边界的线。对新样本的分类简单来说就是询问：它是否位于线上方，还是下方？如果它位于线上方，它被预测为某一类。如果它位于线下方，它被预测为另一类。
- en: For multiple classes, we create multiple SVMs—each a binary classifier. We then
    connect them using any one of a variety of strategies. A basic strategy is to
    create a one-versus-all classifier for each class, where we train using two classes—the
    given class and all other samples. We do this for each class and run each classifier
    on a new sample, choosing the best match from each of these. This process is performed
    automatically in most SVM implementations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类分类，我们创建多个SVMs——每个都是一个二元分类器。然后我们使用任何一种策略将它们连接起来。一种基本策略是为每个类别创建一个一对一分类器，我们使用两个类别——给定的类别和所有其他样本进行训练。我们对每个类别都这样做，并在新样本上运行每个分类器，从这些分类器中选择最佳匹配。这个过程在大多数SVM实现中是自动完成的。
- en: 'We saw two parameters in our previous code: **C** and kernel. We will cover
    the kernel parameter in the next section, but the **C** parameter is an important
    parameter for fitting SVMs. The **C** parameter relates to how much the classifier
    should aim to predict all training samples correctly, at the risk of overfitting.
    Selecting a higher **C** value will find a line of separation with a smaller margin,
    aiming to classify all training samples correctly. Choosing a lower **C** value
    will result in a line of separation with a larger margin—even if that means that
    some training samples are incorrectly classified. In this case, a lower **C**
    value presents a lower chance of overfitting, at the risk of choosing a generally
    poorer line of separation'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的代码中，我们看到了两个参数：**C** 和核。我们将在下一节中介绍核参数，但**C**参数是拟合SVMs的一个重要参数。**C**参数与分类器应该努力预测所有训练样本正确性的程度有关，这可能会带来过拟合的风险。选择更高的**C**值将找到具有较小边界的分离线，旨在正确分类所有训练样本。选择较低的**C**值将导致具有较大边界的分离线——即使这意味着一些训练样本被错误分类。在这种情况下，较低的**C**值提供了较低的过拟合风险，但可能会选择一个通常较差的分离线。
- en: One limitation with SVMs (in their basic form) is that they only separate data
    that is linearly separable. What happens if the data isn't? For that problem,
    we use kernels.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs（在其基本形式）的一个局限性是，它们只能分离线性可分的数据。如果数据不是线性可分的会怎样？针对这个问题，我们使用核。
- en: Kernels
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核
- en: When the data cannot be separated linearly, the trick is to embed it on to a
    higher dimensional space. What this means, with a lot of hand-waving about the
    details, is to add new features to the dataset until the data is linearly separable.
    If you add the right kinds of features, this linear separation will always, eventually,
    happen.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据不能线性分离时，诀窍是将它嵌入到更高维的空间中。这意味着，在许多关于细节的手势中，就是向数据集中添加新的特征，直到数据变得线性可分。如果你添加了正确的特征类型，这种线性分离最终总是会发生的。
- en: The trick is that we often compute the inner-produce of the samples when finding
    the best line to separate the dataset. Given a function that uses the dot product,
    we effectively manufacture new features without having to actually define those
    new features. This is known as the kernel trick and is handy because we don't
    know what those features were going to be anyway. We now define a kernel as a
    function that itself is the dot product of the function of two samples from the
    dataset, rather than based on the samples (and the made-up features) themselves.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍在于，我们在寻找最佳分离数据集的直线时，通常会计算样本的内积。给定一个使用点积的函数，我们实际上制造了新的特征，而无需真正定义这些新特征。这被称为核技巧，它很方便，因为我们无论如何也不知道这些特征会是什么。我们现在定义核为一个函数，它本身是数据集中两个样本的函数的点积，而不是基于样本（和虚构的特征）本身。
- en: We can now compute what that dot product is (or approximate it) and then just
    use that.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算这个点积（或者近似它），然后直接使用这个结果。
- en: There are a number of kernels in common use. The **linear kernel** is the most
    straightforward and is simply the dot product of the two sample feature vectors,
    the weight feature, and a bias value. There is also a **polynomial kernel**, which
    raises the dot product to a given degree (for instance, 2). Others include the
    **Gaussian** (**rbf**) and **Sigmoidal** functions. In our previous code sample,
    we tested between the **linear** kernel and the **rbf** kernel options.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的核函数有很多。**线性核**是最直接的，它是两个样本特征向量、权重特征和偏置值的点积。还有一个**多项式核**，它将点积提升到给定的度（例如，2）。其他包括**高斯**（**rbf**）和**Sigmoidal**函数。在我们的前一个代码示例中，我们在**线性**核和**rbf**核选项之间进行了测试。
- en: The end result from all this derivation is that these kernels effectively define
    a distance between two samples that is used in the classification of new samples
    in SVMs. In theory, any distance could be used, although it may not share the
    same characteristics that enable easy optimization of the SVM training.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些推导的最终结果是，这些核函数有效地定义了两个样本之间的距离，这个距离用于SVMs中新的样本的分类。理论上，可以使用任何距离，尽管它可能不具有使SVM训练易于优化的相同特性。
- en: In scikit-learn's implementation of SVMs, we can define the kernel parameter
    to change which kernel function is used in computations, as we saw in the previous
    code sample.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn对SVMs的实现中，我们可以定义核参数来改变计算中使用的核函数，正如我们在之前的代码示例中看到的。
- en: Character n-grams
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符n-gram
- en: We saw how function words can be used as features to predict the author of a
    document. Another feature type is character n-grams. An n-gram is a sequence of
    *n* tokens, where *n* is a value (for text, generally between 2 and 6). Word n-grams
    have been used in many studies, usually relating to the topic of the documents
    - as per the previous chapter. However, character n-grams have proven to be of
    high quality for authorship attribution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用功能词作为特征来预测文档的作者。另一种特征类型是字符n-gram。n-gram是一系列*n*个标记的序列，其中*n*是一个值（对于文本，通常在2到6之间）。词n-gram已在许多研究中使用，通常与文档的主题相关——如前一章所述。然而，字符n-gram已被证明在作者归属方面具有高质量。
- en: Character n-grams are found in text documents by representing the document as
    a sequence of characters. These n-grams are then extracted from this sequence
    and a model is trained. There are a number of different models for this, but a
    standard one is very similar to the bag-of-words model we have used earlier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将文档表示为字符序列，可以在文本文档中找到字符n-gram。然后从这个序列中提取这些n-gram，并训练一个模型。为此有几种不同的模型，但一个标准的模型与我们之前使用的词袋模型非常相似。
- en: For each distinct n-gram in the training corpus, we create a feature for it.
    An example of an n-gram is `<e t>`, which is the letter e, space, and then the
    letter t (the angle brackets are used to denote the start and end of the n-gram
    and are not part of the n-gram itself). We then train our model using the frequency
    of each n-gram in the training documents and train the classifier using the created
    feature matrix.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练语料库中的每个不同的n-gram，我们为其创建一个特征。一个n-gram的例子是`<e t>`，它代表字母e，空格，然后是字母t（尖括号用来表示n-gram的开始和结束，但它们本身不是n-gram的一部分）。然后我们使用训练文档中每个n-gram的频率来训练我们的模型，并使用创建的特征矩阵来训练分类器。
- en: Character n-grams are defined in many ways. For instance, some applications
    only choose within-word characters, ignoring whitespace and punctuation. Some
    use this information (like our implementation in this chapter) for classification.
    Ultimately, this is the purpose of the model, chosen by the data miner (you!).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 字符n-gram的定义方式有很多种。例如，有些应用只选择单词内的字符，忽略空格和标点符号。有些使用这些信息（如本章中的我们的实现）进行分类。最终，这是模型的目的，由数据挖掘者（也就是你！）选择。
- en: A common theory for why character n-grams work is that people more typically
    write words they can easily say and character n-grams (at least when n is between
    2 and 6) are a good approximation for **phonemes**—the sounds we make when saying
    words. In this sense, using character n-grams approximates the sounds of words,
    which approximates your writing style. This is a common pattern when creating
    new features. First, we have a theory on what concepts will impact the end result
    (authorship style) and then create features to approximate or measure those concepts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 字符n-gram之所以有效的一个常见理论是，人们更倾向于写他们容易说出的单词，而字符n-gram（至少当n在2到6之间时）是对**音素**——我们说单词时发出的声音——的良好近似。从这个意义上说，使用字符n-gram近似单词的声音，这近似了你的写作风格。这是创建新特征时的一个常见模式。首先，我们对哪些概念会影响最终结果（作者风格）有一个理论，然后创建特征来近似或衡量这些概念。
- en: One key feature of a character n-gram matrix is that it is sparse and increases
    in sparsity with higher n-values quite quickly. For an n-value of 2, approximately
    75 percent of our feature matrix is zeros. For an n-value of 5, over 93 percent
    is zeros. This is typically less sparse than a word n-gram matrix of the same
    type though and shouldn't cause many issues using a classifier that is used for
    word-based classifications.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 字符n-gram矩阵的一个关键特征是它是稀疏的，并且随着n值的增加，稀疏性会迅速增加。对于n值为2时，大约75%的特征矩阵是零。对于n值为5时，超过93%是零。尽管如此，这通常比相同类型的单词n-gram矩阵稀疏，使用用于基于单词的分类器的分类器时不应引起许多问题。
- en: Extracting character n-grams
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取字符n-gram
- en: We are going to use our `CountVectorizer` class to extract character n-grams.
    To do that, we set the analyzer parameter and specify a value for n to extract
    n-grams with.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的`CountVectorizer`类来提取字符n-gram。为此，我们需要设置分析器参数并指定一个n值来提取n-gram。
- en: The implementation in scikit-learn uses an n-gram range, allowing you to extract
    n-grams of multiple sizes at the same time. We won't delve into different n-values
    in this experiment, so we just set the values the same. To extract n-grams of
    size 3, you need to specify (3, 3) as the value for the n-gram range.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的实现使用n-gram范围，允许你同时提取多个大小的n-gram。在这个实验中，我们不会深入研究不同的n值，所以我们只设置相同的值。要提取大小为3的n-gram，你需要将(3,
    3)指定为n-gram范围的值。
- en: 'We can reuse the grid search from our previous code. All we need to do is specify
    the new feature extractor in a new pipeline and run it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重用之前代码中的网格搜索。我们只需要在新的管道中指定新的特征提取器并运行它：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There is a lot of implicit overlap between function words and character n-grams,
    as character sequences in function words are more likely to appear. However, the
    actual features are very different and character n-grams capture punctuation,
    a characteristic that function words do not capture. For example, a character
    n-gram includes the full stop at the end of a sentence, while a function word-based
    method would only use the preceding word itself.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 函数词和字符n-gram之间存在大量的隐含重叠，因为函数词中的字符序列更有可能出现。然而，实际特征非常不同，字符n-gram可以捕捉到标点符号，这是函数词所不能捕捉的。例如，一个字符n-gram包括句子末尾的句号，而基于函数词的方法只会使用前面的单词本身。
- en: The Enron dataset
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Enron数据集
- en: Enron was one of the largest energy companies in the world in the late 1990s,
    reporting revenue over $100 billion. It had over 20,000 staff and—as of the year
    2000—there seemed to be no indications that something was very wrong.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 安然是20世纪90年代末世界上最大的能源公司之一，报告的年收入超过1000亿美元。它拥有超过20,000名员工，截至2000年，似乎没有任何迹象表明出了大问题。
- en: In 2001, the *Enron Scandal* occurred, where it was discovered that Enron was
    undertaking systematic, fraudulent accounting practices. This fraud was deliberate,
    wide-ranging across the company, and for significant amounts of money. After this
    was publicly discovered, its share price dropped from more than $90 in 2000 to
    less than $1 in 2001\. Enron shortly filed for bankruptcy in a mess that would
    take more than 5 years to finally be resolved.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，发生了*安然丑闻*，当时发现安然正在进行系统性的欺诈性会计实践。这种欺诈是故意的，涉及公司广泛，涉及大量资金。在公开发现之后，其股价从2000年的90多美元降至2001年的不到1美元。安然随后在一片混乱中申请破产，最终需要超过5年才能最终解决。
- en: As part of the investigation into Enron, the Federal Energy Regulatory Commission
    in the United States made more than 600,000 e-mails publicly available. Since
    then, this dataset has been used for research into everything from social network
    analysis to fraud analysis. It is also a great dataset for authorship analysis,
    as we are able to extract e-mails from the sent folder of individual users. This
    allows us to create a dataset much larger than many previous datasets.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对安然调查的一部分，美国联邦能源监管委员会公开了超过60万封电子邮件。从那时起，这个数据集被用于从社交网络分析到欺诈分析的各种研究。它也是一个很好的作者分析数据集，因为我们能够从单个用户的发送文件夹中提取电子邮件。这使得我们能够创建一个比许多先前数据集都要大的数据集。
- en: Accessing the Enron dataset
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问安然数据集
- en: The full set of Enron emails is available at [https://www.cs.cmu.edu/~./enron/](https://www.cs.cmu.edu/~./enron/)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 全套的安然电子邮件可在[https://www.cs.cmu.edu/~./enron/](https://www.cs.cmu.edu/~./enron/)找到
- en: The full dataset is quite large, and provided in a compression format called
    gzip. If you don't have a Linux-based machine to decompress (unzip) this file,
    get an alternative program, such as 7-zip ([http://www.7-zip.org/](http://www.7-zip.org/))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集相当大，以gzip压缩格式提供。如果您没有基于Linux的机器来解压缩（解压）此文件，请获取替代程序，例如7-zip ([http://www.7-zip.org/](http://www.7-zip.org/))
- en: 'Download the full corpus and decompress it into your data folder. By default,
    this will decompress into a folder called `enron_mail_20110402` which then contains
    a folder called `maildir`. In the Notebook, setup the data folder for the Enron
    dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完整语料库并将其解压缩到您的数据文件夹中。默认情况下，这将解压缩到名为`enron_mail_20110402`的文件夹中，然后包含一个名为`maildir`的文件夹。在笔记本中，为安然数据集设置数据文件夹：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Creating a dataset loader
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据集加载器
- en: 'As we are looking for authorship information, we only want the e-mails we can
    attribute to a specific author. For that reason, we will look in each user''s
    sent folder—that is, emails they have sent. We can now create a function that
    will choose a couple of authors at random and return each of the emails in their
    sent folder. Specifically, we are looking for the payloads—that is, the content
    rather than the e-mails themselves. For that, we will need an e-mail parser. The
    code is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找作者信息时，我们只希望获取可以归因于特定作者的电子邮件。因此，我们将查看每个用户的已发送文件夹——即他们发送的电子邮件。现在我们可以创建一个函数，该函数将随机选择几位作者，并返回他们发送文件夹中的每封电子邮件。具体来说，我们寻找的是有效载荷——即内容而不是电子邮件本身。为此，我们需要一个电子邮件解析器。代码如下：
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We will be using this later to extract the payloads from the e-mail files that
    are in the data folder.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将稍后使用它从数据文件夹中的电子邮件文件中提取有效载荷。
- en: With our data loading function, we are going to have a lot of options. Most
    of these ensure that our dataset is relatively balanced. Some authors will have
    thousands of e-mails in their sent mail, while others will have only a few dozen.
    We limit our search to only authors with at least 10 e-mails using `min_docs_author`
    and take a maximum of 100 e-mails from each author using the `max_docs_author`
    parameter. We also specify how many authors we want to get—10 by default using
    the `num_authors` parameter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的数据加载函数，我们将有很多选项。其中大部分确保我们的数据集相对平衡。一些作者在他们的已发送邮件中可能有数千封电子邮件，而其他人可能只有几十封。我们通过`min_docs_author`参数限制搜索范围，只包括至少有10封电子邮件的作者，并使用`max_docs_author`参数从每位作者那里获取最多100封电子邮件。我们还指定了我们想要获取的作者数量——默认为10位，使用`num_authors`参数。
- en: The function is below. Its main purpose is to loop through the authors, retrieve
    a number of emails for that author, and store the **document** and **class** information
    in some lists. We also store the mapping between an author's name and their numerical
    class value, which lets us retrieve that information later.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 函数如下。其主要目的是遍历作者，为该作者检索一定数量的电子邮件，并将**文档**和**类别**信息存储在列表中。我们还存储了作者姓名与其数值类别值之间的映射，这让我们可以在以后检索该信息。
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It may seem odd that we sort the e-mail addresses, only to shuffle them around.
    The `os.listdir` function doesn't always return the same results, so we sort it
    first to get some stability. We then shuffle using a random state, which means
    our shuffling can reproduce a past result if needed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能觉得我们排序电子邮件地址，然后又打乱它们，这似乎有些奇怪。`os.listdir`函数并不总是返回相同的结果，所以我们首先对其进行排序以获得一些稳定性。然后我们使用随机状态进行洗牌，这意味着如果需要，我们的洗牌可以重现过去的结果。
- en: 'Outside of this function, we can now get a dataset by making the following
    function call. We are going to use a random state of 14 here (as always in this
    book), but you can try other values or set it to none to get a random set each
    time the function is called:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数外部，我们现在可以通过以下函数调用获取数据集。我们将在这里使用一个随机状态14（就像在这本书中一样），但你也可以尝试其他值或将它设置为none，以便每次调用函数时都得到一个随机集：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you have a look at the dataset, there is still a further preprocessing set
    we need to undertake. Our e-mails are quite messy, but one of the worst bits (from
    an authorship analysis perspective) is that these e-mails contain writings from
    other authors, in the form of attached replies. Take the following email, which
    is `documents[100]`, for instance:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看数据集，我们还需要进行进一步的前处理。我们的电子邮件相当混乱，但最糟糕的部分（从作者分析的角度来看）是这些电子邮件包含其他作者的文字，以附件回复的形式存在。以下电子邮件，即`documents[100]`为例：
- en: '*I would like to be on the panel but I have on a conflict on the conference*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*我想加入这个小组，但我有会议冲突*'
- en: '*dates. Please keep me in mind for next year.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*日期。请记住我明年。*'
- en: '*Mark Haedicke*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*马克·海迪克*'
- en: Email is a notoriously messy format. Reply quoting, for instance, is sometimes
    (but not always) prepended with a > character. Other times, the reply is embedded
    in the original message. If you are doing larger scale data mining with email,
    be sure to spend more time cleaning the data to get better results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件是一个臭名昭著的混乱格式。例如，回复引用有时（但不总是）以一个>字符开头。其他时候，回复被嵌入到原始消息中。如果你在进行更大规模的数据挖掘，请确保花更多时间清理数据以获得更好的结果。
- en: 'As with the books dataset, we can plot the histogram of document lengths to
    get a sense of the document length distributions:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与书籍数据集一样，我们可以绘制文档长度的直方图，以了解文档长度分布：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The result appears to show a strong grouping around shorter documents. While
    this is true, it also shows that some documents are very, very long. This may
    skew the results, particularly if some authors are prone to writing long documents.
    To compensate for this, one extension to this work may be to normalise document
    lengths to the first 500 characters before doing the training.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 结果似乎显示出围绕较短文档的强烈分组。虽然这是真的，但它也显示出一些文档非常非常长。这可能会扭曲结果，尤其是如果一些作者倾向于撰写长文档的话。为了补偿这一点，这项工作的一个扩展可能是在训练之前将文档长度标准化到前500个字符。
- en: '![](img/B06162_09_08.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_09_08.png)'
- en: Putting it all together
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 把所有这些都放在一起
- en: We can use the existing parameter space and the existing classifier from our
    previous experiments—all we need to do is refit it on our new data. By default,
    training in scikit-learn is done from scratch—subsequent calls to `fit()` will
    discard any previous information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用现有的参数空间和之前实验中现有的分类器——我们只需要在新数据上重新拟合它。默认情况下，scikit-learn中的训练是从头开始的——随后的`fit()`调用将丢弃任何先前信息。
- en: There is a class of algorithms called *online learning* that update the training
    with new samples and don't restart their training each time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种称为*在线学习*的算法类别，它会用新样本更新训练，而不是每次都重新启动训练。
- en: 'As before, we can compute our scores by using `cross_val_score` and print the
    results. The code is as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以通过使用`cross_val_score`来计算我们的分数，并打印结果。代码如下：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The result is 0.683, which is a reasonable result for such a messy dataset.
    Adding more data (such as increasing `max_docs_author` in the dataset loading)
    can improve these results, as will improving the quality of the data with extra
    cleaning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是0.683，对于这样一个混乱的数据集来说，这是一个合理的结果。增加更多数据（例如在数据集加载中增加`max_docs_author`）可以改善这些结果，同样，通过额外的清理提高数据质量也会有所帮助。
- en: Evaluation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: It is generally never a good idea to base an assessment on a single number.
    In the case of the f-score, it is usually more robust to *tricks* that give good
    scores despite not being useful. An example of this is accuracy. As we said in
    our previous chapter, a spam classifier could predict everything as being spam
    and get over 80 percent accuracy, although that solution is not useful at all.
    For that reason, it is usually worth going more in-depth on the results.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，基于单一数字进行评估从来不是一个好主意。在f-score的情况下，通常更稳健的是那些尽管不实用但能给出好分数的技巧。一个例子是准确性。正如我们在上一章所说，一个垃圾邮件分类器可以预测所有内容都是垃圾邮件，并得到超过80%的准确性，尽管这种解决方案完全无用。因此，通常值得更深入地研究结果。
- en: 'To start with, we will look at the confusion matrix, as we did in [Chapter
    8](ddd1527c-d895-4519-b709-8fe9680518c1.xhtml)*, Beating CAPTCHAs with Neural
    Networks*. Before we can do that, we need to predict a testing set. The previous
    code uses `cross_val_score`, which doesn''t actually give us a trained model we
    can use. So, we will need to refit one. To do that, we need training and testing
    subsets:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看混淆矩阵，就像我们在[第8章](ddd1527c-d895-4519-b709-8fe9680518c1.xhtml)*，使用神经网络战胜CAPTCHAs*中所做的那样。在我们能够这样做之前，我们需要预测一个测试集。之前的代码使用了`cross_val_score`，它实际上并没有给我们一个可以使用的训练模型。因此，我们需要重新拟合一个。为此，我们需要训练和测试子集：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we fit the pipeline to our training documents and create our predictions
    for the testing set:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将管道拟合到我们的训练文档上，并为测试集创建预测：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'At this point, you might be wondering what the best combination of parameters
    actually was. We can extract this quite easily from our grid search object (which
    is the classifier step of our pipeline):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你可能想知道最佳参数组合实际上是什么。我们可以很容易地从我们的网格搜索对象（这是我们管道中的分类步骤）中提取出来：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The results give you all of the parameters for the classifier. However, most
    of the parameters are the defaults that we didn't touch. The ones we did search
    for were C and kernel, which were set to 1 and linear, respectively.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 结果给出了分类器的所有参数。然而，大多数参数都是我们未更改的默认值。我们确实搜索过的参数是C和内核，分别设置为1和线性。
- en: 'Now we can create a confusion matrix:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个混淆矩阵：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we get our author''s names, allowing us to that we can label the axis
    correctly. For this purpose, we use the authors dictionary that our Enron dataset
    loaded. The code is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取作者的名字，这样我们就可以正确地标记轴。为此，我们使用我们的Enron数据集加载的作者字典。代码如下：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we show the confusion matrix using matplotlib. The only changes from
    the last chapter are highlighted below; just replace the letter labels with the
    authors from this chapter''s experiments:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用matplotlib展示混淆矩阵。与上一章相比，以下仅突出显示的变化；只需将字母标签替换为本章实验的作者即可：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The results are shown in the following figure:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下图所示：
- en: '![](img/B06162_09_05.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_09_05.png)'
- en: 'We can see that authors are predicted correctly in most cases—there is a clear
    diagonal line with high values. There are some large sources of error though (darker
    values are larger): emails from user rapp-b are typically predicted as being from
    reitmeyer-j for instance.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在大多数情况下，作者被正确预测——存在一条清晰的带有高值的对角线。尽管如此，还有一些大的错误来源（较暗的值较大）：例如，用户rapp-b的电子邮件通常被预测为来自reitmeyer-j。
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at the text mining-based problem of authorship attribution.
    To perform this, we analyzed two types of features: function words and character
    n-grams. For function words, we were able to use the bag-of-words model—simply
    restricted to a set of words we chose beforehand. This gave us the frequencies
    of only those words. For character n-grams, we used a very similar workflow using
    the same class. However, we changed the analyzer to look at characters and not
    words. In addition, we used n-grams that are sequences of n tokens in a row—in
    our case characters. Word n-grams are also worth testing in some applications,
    as they can provide a cheap way to get the context of how a word is used.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了基于文本挖掘的作者归属问题。为了执行此操作，我们分析了两种类型的特征：功能词和字符n-gram。对于功能词，我们能够使用词袋模型——仅限于我们事先选择的一组词。这给了我们仅那些词的频率。对于字符n-gram，我们使用了一个非常相似的流程，使用相同的类。然而，我们将分析器更改为查看字符而不是单词。此外，我们使用了n-gram，它是n个连续标记的序列——在我们的情况下是字符。在某些应用中，词n-gram也值得测试，因为它们可以提供一种廉价的方式来获取单词使用上下文的信息。
- en: For classification, we used SVMs that optimize a line of separation between
    the classes based on the idea of finding the maximum margin. Anything above the
    line is one class and anything below the line is another class. As with the other
    classification tasks we have considered, we have a set of samples (in this case,
    our documents).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，我们使用了基于最大边缘优化类之间分离线的SVMs。线上方的属于一个类别，线下方的属于另一个类别。与其他我们考虑过的分类任务一样，我们有一组样本（在这种情况下，我们的文档）。
- en: We then used a very messy dataset, the Enron e-mails. This dataset contains
    lots of artifacts and other issues. This resulted in a lower accuracy than the
    books dataset, which was much cleaner. However, we were able to choose the correct
    author more than half the time, out of 10 possible authors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用了一个非常杂乱的数据库，即安然电子邮件。这个数据库包含了许多人工制品和其他问题。这导致其准确率低于书籍数据库，后者要干净得多。然而，在10个可能的作者中，我们能够超过一半的时间选择正确的作者。
- en: To take the concepts in this chapter further, look for new datasets containing
    authorship information. For instance, can you predict the author of a blog post?
    What about the author of a tweet (you may be able to reuse your data from [Chapter
    6](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml), *Social Media Insight Using Naive
    Bayes*)?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨本章的概念，寻找包含作者信息的新数据库。例如，你能预测博客文章的作者吗？关于推文的作者（你可能能够重复使用[第6章](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)，*使用朴素贝叶斯进行社交媒体洞察*)的数据呢？
- en: In the next chapter, we consider what we can do if we don't have target classes.
    This is called unsupervised learning, an exploratory problem rather than a prediction
    problem. We also continue to deal with messy text-based datasets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们考虑如果我们没有目标类别时我们能做什么。这被称为无监督学习，这是一个探索性问题而不是预测性问题。我们还将继续处理基于文本的杂乱数据集。
