["```py\nThe tree has entered my hands,\nThe sap has ascended my arms,\nThe tree has grown in my breast-Downward,\nThe branches grow out of me, like arms.\nTree you are,\nMoss you are,\nYou are violets with wind above them.\nA child - so high - you are,\nAnd all this is folly to the world.\n```", "```py\n*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n\nTHE END\n```", "```py\n# including the languageR library\nlibrary(\"languageR\")\n# loading the \"Aliceâ€™s Adventures in Wonderland\" to memory\ndata(alice)\n# printing the loaded text\nprint(alice)\n```", "```py\n[1] \"ALICE\"           \"S\"                \"ADVENTURES\"       \"IN\"               \"WONDERLAND\"      \n[6] \"Lewis\"            \"Carroll\"          \"THE\"              \"MILLENNIUM\"       \"FULCRUM\"        \n  [11] \"EDITION\"          \"3\"                \"0\"                \"CHAPTER\"          \"I\"              \n  [16] \"Down\"             \"the\"              \"Rabbit-Hole\"      \"Alice\"            \"was\"            \n  [21] \"beginning\"        \"to\"               \"get\"              \"very\"             \"tired\"          \n  [26] \"of\"               \"sitting\"          \"by\"               \"her\"              \"sister\"         \n  [31] \"on\"               \"the\"              \"bank\"             \"and\"              \"of\"             \n  [36] \"having\"           \"nothing\"          \"to\"               \"do\"               \"once\"           \n  [41] \"or\"               \"twice\"            \"she\"              \"had\"              \"peeped\"         \n  [46] \"into\"             \"the\"              \"book\"             \"her\"              \"sister\"         \n  [51] \"was\"              \"reading\"          \"but\"              \"it\"               \"had\"            \n  [56] \"no\"         \"pictures\"         \"or\"               \"conversations\"    \"in\"              \n```", "```py\nalice_in_wonderland<-paste(alice,collapse=\" \")\nprint(alice_in_wonderland)\n```", "```py\n[1] \"ALICE S ADVENTURES IN WONDERLAND Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3 0 CHAPTER I Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought Alice without pictures or conversation So she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies when suddenly a White Rabbit with pink eyes ran close by her There was nothing so VERY remarkable in that nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself Oh dear Oh dear I shall be late when she thought it over afterwards it occurred to her that she ought to have wondered at this but at the time it all seemed quite natural but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT- POCKET and looked at it and then hurried on Alice started to her feet for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket or a watch to take out of it and burning with curiosity she ran across the field after it and fortunately was just in time to see it pop down a large rabbit-hole under the hedge In another moment down went Alice after it never once considering how in the world she was to get out again The rabbit-hole we .......\n```", "```py\n# including the required libraries\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"stringi\")\nlibrary(\"mxnet\")\nlibrary(\"languageR\")\n```", "```py\ndata(alice)\n```", "```py\nmake_data <- function(txt, seq.len = 32, dic=NULL) {\n  text_vec <- as.character(txt)\n  text_vec <- stri_enc_toascii(str = text_vec)\n  text_vec <- str_replace_all(string = text_vec, pattern = \"[^[:print:]]\", replacement = \"\")\n  text_vec <- strsplit(text_vec, '') %>% unlist\n  if (is.null(dic)) {\n    char_keep <- sort(unique(text_vec))\n  } else char_keep <- names(dic)[!dic == 0]\n```", "```py\ntext_vec <- text_vec[text_vec %in% char_keep]\n```", "```py\ndic <- 1:length(char_keep)\n names(dic) <- char_keep\n # reversing the dictionary\n rev_dic <- names(dic)\n names(rev_dic) <- dic\n # Adjust by -1 to have a 1-lag for labels\n num.seq <- (length(text_vec) - 1) %/% seq.len\n features <- dic[text_vec[1:(seq.len * num.seq)]]\n labels <- dic[text_vec[1:(seq.len*num.seq) + 1]]\n features_array <- array(features, dim = c(seq.len, num.seq))\n labels_array <- array(labels, dim = c(seq.len, num.seq))\n return (list(features_array = features_array, labels_array = labels_array, dic = dic, rev_dic\n = rev_dic))\n }\n```", "```py\nseq.len <- 100\n alice_in_wonderland<-paste(alice,collapse=\" \")\n data_prep <- make_data(alice_in_wonderland, seq.len = seq.len, dic=NULL)\n```", "```py\nprint(str(data_prep))\n```", "```py\n> print(str(data_prep))\nList of 4\n $ features_array: int [1:100, 1:1351] 9 31 25 13 17 1 45 1 9 15 ...\n $ labels_array  : int [1:100, 1:1351] 31 25 13 17 1 45 1 9 15 51 ...\n $ dic           : Named int [1:59] 1 2 3 4 5 6 7 8 9 10 ...\n  ..- attr(*, \"names\")= chr [1:59] \" \" \"-\" \"[\" \"]\" ...\n $ rev_dic       : Named chr [1:59] \" \" \"-\" \"[\" \"]\" ...\n  ..- attr(*, \"names\")= chr [1:59] \"1\" \"2\" \"3\" \"4\" ...\n```", "```py\n# Viewing the feature array\nView(data_prep$features_array)\n```", "```py\n# Viewing the labels array\nView(data_prep$labels_array)\n```", "```py\n# printing the dictionary - the unique characters\nprint(data_prep$dic)\n```", "```py\n> print(data_prep$dic)\n    -  [  ]  *  0  3  a  A  b  B  c  C  d  D  e  E  f  F  g  G  h  H  i  I  j  J  k  K  l  L  m  M  n  N  o  O  p\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38\n P  q  Q  r  R  s  S  t  T  u  U  v  V  w  W  x  X  y  Y  z  Z\n39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n```", "```py\n# printing the indexes of the characters\nprint(data_prep$rev_dic)\n```", "```py\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28\n\" \" \"-\" \"[\" \"]\" \"*\" \"0\" \"3\" \"a\" \"A\" \"b\" \"B\" \"c\" \"C\" \"d\" \"D\" \"e\" \"E\" \"f\" \"F\" \"g\" \"G\" \"h\" \"H\" \"i\" \"I\" \"j\" \"J\" \"k\"\n 29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n\"K\" \"l\" \"L\" \"m\" \"M\" \"n\" \"N\" \"o\" \"O\" \"p\" \"P\" \"q\" \"Q\" \"r\" \"R\" \"s\" \"S\" \"t\" \"T\" \"u\" \"U\" \"v\" \"V\" \"w\" \"W\" \"x\" \"X\" \"y\"\n 57  58  59\n\"Y\" \"z\" \"Z\"\n```", "```py\nX <- data_prep$features_array\nY <- data_prep$labels_array\ndic <- data_prep$dic\nrev_dic <- data_prep$rev_dic\nvocab <- length(dic)\nsamples <- tail(dim(X), 1)\ntrain.val.fraction <- 0.9\nX.train.data <- X[, 1:as.integer(samples * train.val.fraction)]\nX.val.data <- X[, -(1:as.integer(samples * train.val.fraction))]\nX.train.label <- Y[, 1:as.integer(samples * train.val.fraction)]\nX.val.label <- Y[, -(1:as.integer(samples * train.val.fraction))]\ntrain_buckets <- list(\"100\" = list(data = X.train.data, label = X.train.label))\neval_buckets <- list(\"100\" = list(data = X.val.data, label = X.val.label))\ntrain_buckets <- list(buckets = train_buckets, dic = dic, rev_dic = rev_dic)\neval_buckets <- list(buckets = eval_buckets, dic = dic, rev_dic = rev_dic)\n```", "```py\nvocab <- length(eval_buckets$dic)\nbatch.size <- 32\ntrain.data <- mx.io.bucket.iter(buckets = train_buckets$buckets, batch.size = batch.size, data.mask.element = 0, shuffle = TRUE)\neval.data <- mx.io.bucket.iter(buckets = eval_buckets$buckets, batch.size = batch.size,data.mask.element = 0, shuffle = FALSE)\n```", "```py\nrnn_graph_one_one <- rnn.graph(num_rnn_layer = 3,\n                               num_hidden = 96,\n                               input_size = vocab,\n                               num_embed = 64,\n                               num_decode = vocab,\n                               dropout = 0.2,\n                               ignore_label = 0,\n                               cell_type = \"lstm\",\n                               masking = F,\n                               output_last_state = T,\n                               loss_output = \"softmax\",\n                               config = \"one-to-one\")\n```", "```py\ngraph.viz(rnn_graph_one_one, type = \"graph\",\n          graph.height.px = 650, shape=c(500, 500))\n```", "```py\ndevices <- mx.cpu()\n```", "```py\ninitializer <- mx.init.Xavier(rnd_type = \"gaussian\", factor_type = \"avg\", magnitude = 3)\n```", "```py\noptimizer <- mx.opt.create(\"adadelta\", rho = 0.9, eps = 1e-5, wd = 1e-8,\n                           clip_gradient = 5, rescale.grad = 1/batch.size)\n```", "```py\nlogger <- mx.metric.logger()\nepoch.end.callback <- mx.callback.log.train.metric(period = 1, logger = logger)\nbatch.end.callback <- mx.callback.log.train.metric(period = 50)\nmx.metric.custom_nd <- function(name, feval) {\n  init <- function() {\n    c(0, 0)\n  }\n  update <- function(label, pred, state) {\n    m <- feval(label, pred)\n    state <- c(state[[1]] + 1, state[[2]] + m)\n    return(state)\n  }\n  get <- function(state) {\n    list(name=name, value = (state[[2]] / state[[1]]))\n  }\n  ret <- (list(init = init, update = update, get = get))\n  class(ret) <- \"mx.metric\"\n  return(ret)\n}\n```", "```py\nmx.metric.Perplexity <- mx.metric.custom_nd(\"Perplexity\", function(label, pred) {\n  label <- mx.nd.reshape(label, shape = -1)\n  label_probs <- as.array(mx.nd.choose.element.0index(pred, label))\n  batch <- length(label_probs)\n  NLL <- -sum(log(pmax(1e-15, as.array(label_probs)))) / batch\n  Perplexity <- exp(NLL)\n  return(Perplexity)\n}\n```", "```py\nmodel <- mx.model.buckets(symbol = rnn_graph_one_one,\n                          train.data = train.data, eval.data = eval.data,\n                          num.round = 20, ctx = devices, verbose = TRUE,\n                          metric = mx.metric.Perplexity,\n                          initializer = initializer,\n   optimizer = optimizer,\n                          batch.end.callback = NULL,\n                          epoch.end.callback = epoch.end.callback)\n```", "```py\nStart training with 1 devices\n[1] Train-Perplexity=23.490355102639\n[1] Validation-Perplexity=17.6250266989171\n[2] Train-Perplexity=14.4508382001841\n[2] Validation-Perplexity=12.8179427398927\n[3] Train-Perplexity=10.8156810097278\n[3] Validation-Perplexity=9.95208184606089\n[4] Train-Perplexity=8.6432934902383\n[4] Validation-Perplexity=8.21806492033906\n[5] Train-Perplexity=7.33073759154393\n[5] Validation-Perplexity=7.03574648385079\n[6] Train-Perplexity=6.32024660528852\n[6] Validation-Perplexity=6.1394327776089\n[7] Train-Perplexity=5.61888374338248\n[7] Validation-Perplexity=5.59925324885983\n[8] Train-Perplexity=5.14009899947491]\n[8] Validation-Perplexity=5.29671693342219\n[9] Train-Perplexity=4.77963053659987\n[9] Validation-Perplexity=4.98471501141549\n[10] Train-Perplexity=4.5523402301526\n[10] Validation-Perplexity=4.84636357676712\n[11] Train-Perplexity=4.36693337145912\n[11] Validation-Perplexity=4.68806078057635\n[12] Train-Perplexity=4.21294955131918\n[12] Validation-Perplexity=4.53026345109037\n[13] Train-Perplexity=4.08935886339982\n[13] Validation-Perplexity=4.50495393289961\n[14] Train-Perplexity=3.99260373800419\n[14] Validation-Perplexity=4.42576079641165\n[15] Train-Perplexity=3.91330125104996\n[15] Validation-Perplexity=4.3941619024578\n[16] Train-Perplexity=3.84730588206837\n[16] Validation-Perplexity=4.33288830915229\n[17] Train-Perplexity=3.78711049085869\n[17] Validation-Perplexity=4.28723362252784\n[18] Train-Perplexity=3.73198720637659\n[18] Validation-Perplexity=4.22839393379393\n[19] Train-Perplexity=3.68292148768833\n[19] Validation-Perplexity=4.22187018296206\n[20] Train-Perplexity=3.63728269095417\n[20] Validation-Perplexity=4.17983276293299\n```", "```py\nmx.model.save(model, prefix = \"one_to_one_seq_model\", iteration = 20)\n# the generated text is expected to be similar to the training data\nset.seed(0)\nmodel <- mx.model.load(prefix = \"one_to_one_seq_model\", iteration = 20)\ninternals <- model$symbol$get.internals()\nsym_state <- internals$get.output(which(internals$outputs %in% \"RNN_state\"))\nsym_state_cell <- internals$get.output(which(internals$outputs %in% \"RNN_state_cell\"))\nsym_output <- internals$get.output(which(internals$outputs %in% \"loss_output\"))\nsymbol <- mx.symbol.Group(sym_output, sym_state, sym_state_cell)\n```", "```py\ninfer_raw <- c(\"e\")\ninfer_split <- dic[strsplit(infer_raw, '') %>% unlist]\ninfer_length <- length(infer_split)\ninfer.data <- mx.io.arrayiter(data = matrix(infer_split), label = matrix(infer_split), batch.size = 1, shuffle = FALSE)\ninfer <- mx.infer.rnn.one(infer.data = infer.data,\n                          symbol = symbol,\n                          arg.params = model$arg.params,\n                          aux.params = model$aux.params,\n                          input.params = NULL,\n                          ctx = devices)\npred_prob <- as.numeric(as.array(mx.nd.slice.axis(infer$loss_output, axis = 0, begin = infer_length-1, end = infer_length)))\npred <- sample(length(pred_prob), prob = pred_prob, size = 1) - 1\npredict <- c(predict, pred)\nfor (i in 1:200) {\n  infer.data <- mx.io.arrayiter(data = as.matrix(pred), label = as.matrix(pred), batch.size = 1,\nshuffle = FALSE) \n  infer <- mx.infer.rnn.one(infer.data = infer.data,\n                            symbol = symbol,\n                            arg.params = model$arg.params,\n                            aux.params = model$aux.params,\n                            input.params = list(rnn.state = infer[[2]],\n                            rnn.state.cell = infer[[3]]),\n                            ctx = devices)\n  pred_prob <- as.numeric(as.array(infer$loss_output))\n  pred <- sample(length(pred_prob), prob = pred_prob, size = 1, replace = T) - 1\n  predict <- c(predict, pred)\n}\n```", "```py\npredict_txt <- paste0(rev_dic[as.character(predict)], collapse = \"\")\npredict_txt_tot <- paste0(infer_raw, predict_txt, collapse = \"\")\n# printing the predicted text\nprint(predict_txt_tot)\n```", "```py\n[1] \"eNAHare I eat and in Heather where and fingo I ve next feeling or fancy to livery dust a large pived as a pockethion What isual child for of cigstening to get in a strutching voice into saying she got reaAlice glared in a Grottle got to sea-paticular and when she heard it would heard of having they began whrink bark of Hearnd again said feeting and there was going to herself up it Then does so small be THESE said Alice going my dear her before she walked at all can t make with the players and said the Dormouse sir your mak if she said to guesss I hadn t some of the crowd and one arches how come one mer really of a gomoice and the loots at encand something of one eyes purried asked to leave at she had Turtle might I d interesting tone hurry of the game the Mouse of puppled it They much put eagerly\"\n```"]