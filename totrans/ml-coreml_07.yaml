- en: Assisted Drawing with CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络（CNN）辅助绘图
- en: So far, we have seen how we can leverage Core ML and, in general, **machine
    learning** (**ML**) to better understand the physical world we live in (perceptual
    tasks). From the perspective of designing user interfaces, this allows us to reduce
    the friction between the user and the system. For example, if you are able to
    identify the user from a picture of their face, you can remove the steps required
    for authentication, as demonstrated with Apple's Face ID feature which is available
    on iPhone X. With Core ML, we have the potential to have devices better serve
    us rather than us serving them. This adheres to a rule stated by developer Eric
    Raymond that *a* *computer should never ask the user for any information that
    it can auto detect, copy, or deduce*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何利用Core ML以及一般性的**机器学习**（**ML**）来更好地理解我们所生活的物理世界（感知任务）。从设计用户界面的角度来看，这使我们能够减少用户和系统之间的摩擦。例如，如果你能够从用户的面部照片中识别出用户，你可以省去认证所需的步骤，就像iPhone
    X上的Apple Face ID功能所展示的那样。有了Core ML，我们有可能让设备更好地为我们服务，而不是我们为它们服务。这符合开发者Eric Raymond提出的一条规则：*一台*
    *计算机永远不应该要求用户提供任何它可以自动检测、复制或推断的信息*。
- en: We can take this idea even further; given sufficient amounts of data, we can
    anticipate what the user is trying to do and assist them in achieving their tasks.
    This is the premise of this chapter. Largely inspired and influenced by Google's
    AutoDraw AI experiment, we will implement an application that will attempt to
    guess what the user is trying to draw and provide pre-drawn drawings that the
    user can subtitute with (image search).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法进一步深化；给定足够的数据量，我们可以预测用户试图做什么，并帮助他们完成任务。这是本章的基点。在很大程度上受到谷歌的AutoDraw
    AI实验的启发和影响，我们将实现一个应用程序，试图猜测用户试图画什么，并提供预先绘制的图案供用户替换（图像搜索）。
- en: In this chapter, we'll explore this idea by looking at how we can try to predict
    what the user is trying to draw, and find suggestions for them to substitute it
    with. We will be exploring two techniques. The first is using a **convolutional
    neural network** (**CNN**), which we are becoming familiar with, to make the prediction,
    and then look at how we can apply a context-based similarity sorting strategy
    to better align the suggestions with what the user is trying to sketch. In the
    next chapter, we will continue our exploration by looking at how we can use a
    **recurrent neural network** (**RNN**) for the same task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过探讨如何尝试预测用户试图画什么，并为他们找到替换建议来探索这个想法。我们将探讨两种技术。第一种是使用我们越来越熟悉的**卷积神经网络**（**CNN**）来进行预测，然后看看我们如何应用基于上下文的相似度排序策略，以更好地将建议与用户试图绘制的图像对齐。在下一章中，我们将继续探索，看看我们如何使用**循环神经网络**（**RNN**）来完成同样的任务。
- en: 'By the end of this chapter, you will have:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将：
- en: Applied CNNs to the task of sketch recognition
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将卷积神经网络（CNN）应用于草图识别任务
- en: Gained further experience preparing input for a model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得为模型准备输入的更多经验
- en: Learned how feature maps can be extracted from CNNs and used to measure how
    similar two images are
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何从卷积神经网络（CNN）中提取特征图，并用于测量两张图像的相似度
- en: There is a lot to cover, so let's get started by building a simple drawing application.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多内容要介绍，所以让我们从构建一个简单的绘图应用程序开始。
- en: Towards intelligent interfaces
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向智能界面迈进
- en: Before jumping into how, let's quickly discuss the why in order to motivate
    us as well as encourage creative exploration of this concept. As alluded to in
    the introduction, the first motivator is to reduce friction. Consider the soft
    keyboard (keyboard with no physical buttons) on your phone; due to the constraints
    of the medium, such as lack of space and feedback, inputting text without predictive
    text would be cumbersome to the point of rendering it unusable. Similarly, despite the
    convenience of drawing with our fingers, our fingers are not that accurate, which
    makes things difficult to draw.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨如何之前，让我们先快速讨论一下为什么，以便激励我们并鼓励对这一概念进行创造性探索。正如引言中提到的，第一个动机是减少摩擦。考虑一下你手机上的软键盘（没有物理按钮的键盘）；由于媒介的限制，如空间和反馈的缺乏，没有预测文本的输入将会变得繁琐到无法使用的地步。同样，尽管用手指画画很方便，但我们的手指并不那么精确，这使得绘画变得困难。
- en: The other reason why this concept (augmentation) is advantageous is its ability
    to democratize the technical skill of drawing. It's common for people to not even
    attempt to draw because they have convinced themselves that it is beyond their
    abilities, or possibly that we can enhance one's ability to draw. This was the
    motivation behind the research project *ShadowDraw* presented at SIGGRAPH in 2011
    by Yong Jae Lee, Larry Zitnick, and Michael Cohen. Their project had shown that
    guiding the user with a shadow image underlying the user's stroke significantly
    improved the quality of the output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念（增强）的另一个优点是其民主化绘图技术技能的能力。人们通常甚至不尝试绘图，因为他们已经说服自己这超出了他们的能力范围，或者可能我们认为我们可以提高一个人的绘图能力。这是2011年在SIGGRAPH上由Yong
    Jae Lee、Larry Zitnick和Michael Cohen提出的名为*ShadowDraw*的研究项目的动机。他们的项目表明，通过在用户的笔触下方引导阴影图像，可以显著提高输出质量。
- en: Finally, the last reason I want to highlight as to why this concept is interesting
    is providing a way for users to work at a higher level of abstraction. For example,
    imagine you were tasked with sketching out the storyboard for a new animation.
    As you sketch out your scene, the system would substitute your sketches with their
    associated characters and props as they were being worked on, allowing you to
    design at a higher level of fidelity without sacrificing speed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想强调的最后一个原因是，这个概念为用户提供了一种在更高层次抽象上工作的方法。例如，想象一下，如果你被要求绘制一个新动画的故事板草图。当你绘制场景时，系统会根据正在工作的内容替换你的草图及其相关的角色和道具，让你能够在更高保真度下设计，而不牺牲速度。
- en: Hopefully, by now, I have convinced you of the potential opportunity of integrating
    artificial intelligence into the user interface. Let's shift our focus to the how,
    which we will begin to do in the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在为止，我已经说服你将人工智能集成到用户界面中的潜在机会。让我们将我们的重点转向“如何”，我们将在下一节开始讨论。
- en: Drawing
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘画
- en: In this section, we will start off by inspecting an existing starter application
    and implement the drawing functionality. Then, in the next section, we will look
    at how we can augment the user by predicting what they are trying to draw and
    providing substitutes they can swap with.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先检查现有的入门应用程序并实现绘图功能。然后，在下一节中，我们将探讨如何通过预测用户试图绘制的图像并提供他们可以替换的替代图像来增强用户。
- en: 'If you haven''t done so, pull down the latest code from the accompanying repository
    at [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the `Chapter7/Start/QuickDraw/` directory and open
    the project `QuickDraw.xcodeproj`. Once loaded, you will see the starter project
    for this chapter, as shown in the following screenshot:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从随附的存储库[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)中拉取最新代码。下载后，导航到`Chapter7/Start/QuickDraw/`目录并打开项目`QuickDraw.xcodeproj`。加载后，你将看到本章的入门项目，如下面的屏幕截图所示：
- en: '![](img/ee8848b5-3cae-4787-ad88-f0bd5555414d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee8848b5-3cae-4787-ad88-f0bd5555414d.png)'
- en: In the previous screenshot, you can see the application in its entirety; the
    interface consists of a single view, which contains a simple toolbar down on the
    left, allowing the user to toggle between sketch and move. There is a button for
    clearing everything. The area to the right of the toolbar is the canvas, which
    will be responsible for rendering what the user has drawn and any substituted
    images. Finally, the top area consists of a label and collection view. The collection
    view will make the suggested images that the user can substitute with available,
    while the label is simply something that is made visible to make the user aware
    of the purpose of the images presented to them via the collection view.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的屏幕截图中，你可以看到应用程序的整体应用；界面由一个视图组成，其中在左侧下方有一个简单的工具栏，允许用户在草图和移动之间切换。有一个清除所有内容的按钮。工具栏右侧的区域是画布，它将负责渲染用户所绘制的任何图像和替代图像。最后，顶部区域由一个标签和集合视图组成。集合视图将使用户可以替换的建议图像可用，而标签只是为了让用户意识到通过集合视图呈现给他们的图像的目的。
- en: 'Our first task, as previously mentioned, will be to implement the functionality
    of drawing. Some of the plumbing has already been done but a majority is left
    out, giving us the opportunity to walk through the code to better understand the
    architecture of the application and how ML has been integrated. Before jumping
    into the code, let''s briefly discuss the purpose of each relevant source file
    within the project. Like a contents page of a book, this will give you a better
    sense of how things are stitched together and help you become more familiar with
    the project:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的第一个任务将是实现绘图功能。一些管道工作已经完成，但大部分工作尚未完成，这给了我们一个机会来遍历代码，更好地理解应用程序的架构以及机器学习是如何被集成的。在深入代码之前，让我们简要讨论项目内每个相关源文件的目的。就像一本书的目录一样，这将让你更好地了解事物是如何拼接在一起的，并帮助你更熟悉项目：
- en: '`SketchView`: This is a custom `UIControl` that will be responsible for capturing
    the user''s touches and converting them to drawings. It will also be responsible
    for rendering these drawings and substituted drawings, that is, sketches that
    have been replaced. As seen earlier, this control has already been added to the
    view. In this section, we will be implementing the functionality of touch events.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SketchView`：这是一个自定义的`UIControl`，负责捕捉用户的触摸并将其转换为绘图。它还将负责渲染这些绘图和替代绘图，即已被替换的草图。如前所述，此控件已添加到视图中。在本节中，我们将实现触摸事件的功能。'
- en: '`SketchViewController`: The controller behind the main view, and it is responsible
    for listening for when the user finishes editing (lifts their finder) and passing
    the current sketch to the `QueryFacade` for processing. This controller is also
    responsible for handling mode switches (sketching, moving, or clearing everything)
    and dragging the sketches around the screen when in the move mode.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SketchViewController`：这是主视图背后的控制器，负责监听用户完成编辑（抬起他们的指针）并将当前草图传递给`QueryFacade`进行处理。此控制器还负责处理模式切换（草图、移动或清除所有内容）以及在移动模式下在屏幕上拖动草图。'
- en: '`BingService`: We will be using Microsoft''s Bing Image Search API to find
    our suggested images. Bing provides a simple RESTful service to allow for image
    searches, along with relevant parameters for fine-tuning of your search. Note:
    we won''t be editing this.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BingService`：我们将使用微软的Bing图像搜索API来查找我们的建议图像。Bing提供了一个简单的RESTful服务，允许进行图像搜索，以及一些相关参数来微调你的搜索。注意：我们不会编辑这个。'
- en: '`SketchPreviewCell`: A simple extension of the `UICollectionViewCell` class
    that makes the `UIImageView` nested within the cell available. Note: we won''t
    be editing this.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SketchPreviewCell`：这是`UICollectionViewCell`类的一个简单扩展，使得嵌套在单元格内的`UIImageView`可用。注意：我们不会编辑这个。'
- en: '`CIImage`: This should look familiar to you—something we implemented back in
    [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognizing Objects
    in the World*. We''ll use it extensively in this chapter for resizing and getting
    access to raw data of the images (including the sketch).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CIImage`：你应该很熟悉——这是我们之前在[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)中实现的，*识别世界中的物体*。我们将在此章中广泛使用它来进行缩放和获取图像的原始数据（包括草图）。'
- en: '`Sketch`: This is our model of a sketch; we will implement two versions of
    this. One is for rendering sketches from the user, created by strokes, and the
    other is for encapsulating a `UIImage`, which has substituted a sketch (of strokes).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sketch`：这是我们草图的模式；我们将实现两个版本。一个是用于渲染用户创建的草图，由笔触构成，另一个是用于封装一个`UIImage`，它替代了草图（笔触）。'
- en: '`Stroke`: A data object that describes part of a sketch, essentially encoding
    the path the user draws so that we can render it.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stroke`：一个描述草图一部分的数据对象，本质上编码了用户绘制的路径，以便我们可以渲染它。'
- en: '`QueryFacade`: This is the class that will do all the heavy lifting. Once the
    user has finished editing, the view controller will export the sketch and pass
    it to `QueryFacade`, which will be responsible for three things: guessing what
    the user is trying to draw, fetching and downloading relevant suggestions, and
    sorting them before passing back to the view controller to present to the user
    via the collection view. An illustration of this process can be seen here:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QueryFacade`：这是一个将执行所有繁重工作的类。一旦用户完成编辑，视图控制器将导出草图并将其传递给`QueryFacade`，该类将负责三件事：猜测用户试图绘制的内容、获取和下载相关建议，并在将它们排序后传递回视图控制器，通过集合视图向用户展示。此过程的示意图如下：'
- en: '![](img/9185a2cc-6f90-4294-8ae9-771fc9f21e0f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9185a2cc-6f90-4294-8ae9-771fc9f21e0f.png)'
- en: 'Hopefully, you now have a better sense of how everything fits together; let''s
    bring it to life, starting at the bottom and working our way up. Click on the
    `Stroke.swift` file to focus the file in the main area; once open, you will be
    greeted by an unassuming amount of code, as shown in this snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对如何将所有事物串联起来有了更好的理解；让我们从底部开始，逐步构建。点击`Stroke.swift`文件以将文件聚焦在主区域；一旦打开，你会看到一个不太显眼的代码量，正如这个片段所示：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Just to recap, the purpose of a `Stroke` is to encapsulate a single path the
    user has drawn such as to be able to recreate it when rendering it back onto the
    screen. A path is nothing more than a list of points that are captured as the
    user moves their finger along the screen. Along with the path, we will also store
    a color and width of the stroke; these determine the visual aesthetics of the
    stroke. Add the following properties to the `Stroke` class along with the class''s
    constructor:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 只为了回顾一下，`Stroke`的目的在于封装用户绘制的单个路径，以便在将其重新渲染到屏幕上时能够重新创建它。路径不过是一系列点，这些点在用户沿着屏幕移动手指时被捕获。除了路径之外，我们还将存储笔迹的颜色和宽度；这些决定了笔迹的视觉美学。将以下属性添加到`Stroke`类中，以及类的构造函数：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will add some computed properties to our `Stroke` class that we will
    make use of when rendering and exporting the sketch. Starting with the property
    to assist with rendering, we''ll be using the Core Graphics framework to render
    the path of each stroke associated with a sketch. Rendering is done using a Core
    Graphics context (`CGContext`), which conveniently exposes methods for rendering
    a path using the methods `addPath` and `drawPath`, as we''ll see soon. The `addPath`
    method expects a type of `CGPath`, which is nothing more than a series of drawing
    instructions describing how to draw the path, something we can easily derive from
    our stroke''s points. Let''s do that now; add the `path` property to the `Stroke`
    class:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向我们的`Stroke`类添加一些计算属性，这些属性将在渲染和导出草图时使用。从辅助渲染的属性开始，我们将使用Core Graphics框架来渲染与草图关联的每条笔迹的路径。渲染是通过Core
    Graphics上下文（`CGContext`）完成的，它方便地公开了使用`addPath`和`drawPath`方法渲染路径的方法，我们很快就会看到。`addPath`方法期望一个`CGPath`类型，这不过是一系列绘图指令，描述了如何绘制路径，我们可以很容易地从笔迹的点中推导出来。现在让我们这样做；将`path`属性添加到`Stroke`类中：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As mentioned previously, a `CGPath` is made up of a series of drawing instructions.
    In the preceding snippet, we are creating a path using the points associated with
    `Stroke`. All except the first connects each point by a line while the first simply
    moves it into position.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`CGPath`由一系列绘图指令组成。在前面的片段中，我们正在使用与`Stroke`关联的点创建路径。除了第一个之外，所有其他点都通过线条连接，而第一个只是将其移动到正确的位置。
- en: The Core Graphics framework is a lightweight and low-level 2D drawing engine.
    It includes drawing functionality, such as path-based drawing, transformations,
    color management, off-screen rendering, patterns and shadings, image creation,
    and image masking.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Core Graphics框架是一个轻量级且底层的2D绘图引擎。它包括基于路径的绘图、变换、颜色管理、离屏渲染、图案和阴影、图像创建和图像蒙版等功能。
- en: 'Our next two properties are used to obtain the bounding box of the sketch,
    that is, the bounds that would encompass the minimum and maximum `x` and `y` positions
    of all strokes. Implementing these within the stroke itself will make our task
    easier later on. Add the properties `minPoint` and `maxPoint` to your `Stroke`
    class, as shown in the following code block:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的两个属性用于获取草图的边界框，即包含所有笔迹的最小和最大`x`和`y`位置的边界。在笔迹本身中实现这些将使我们的任务更容易。将`minPoint`和`maxPoint`属性添加到你的`Stroke`类中，如下面的代码块所示：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For each property, we simply map each axis (*x* and *y*) into its own array
    and then find either the minimum or maximum with respect to their method. This
    now completes our `Stroke` class. Let''s move our way up the layers and implement
    the functionality of the `Sketch` class. Select `Sketch.swift` from the left-hand-side
    panel to open in the editing window. Before making amendments, let''s inspect
    what is already there and what''s left to do:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个属性，我们只是将每个轴（*x*和*y*）映射到它自己的数组中，然后根据它们的方法找到最小值或最大值。现在我们已经完成了`Stroke`类的构建。让我们向上移动到层，并实现`Sketch`类的功能。从左侧面板中选择`Sketch.swift`以在编辑窗口中打开。在做出修改之前，让我们检查一下已经存在的内容以及还需要完成的工作：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Currently, no concrete class exists and this will be our task for this part
    of this section. Before we start coding, let's review the responsibility of the
    `Sketch`. As implied earlier, our `Sketch` will be responsible for rendering either
    the collection of strokes associated with the user's drawing or an image that
    the user has selected to substitute their own drawing with. For this reason, we
    will be using two implementations of the `Stroke` class, one specifically for
    dealing with strokes and the other for images; we'll start with the one responsible
    for managing and rendering strokes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，尚无具体的类存在，这将是本节这一部分的我们的任务。在我们开始编码之前，让我们回顾一下 `Sketch` 的职责。如前所述，我们的 `Sketch`
    将负责渲染与用户绘图相关的笔划集合或用户选择的用于替代自己绘图的图像。因此，我们将使用 `Stroke` 类的两个实现，一个专门用于处理笔划，另一个用于图像；我们将从负责管理和渲染笔划的那个开始。
- en: Each implementation is expected to expose a `draw` and `exportSketch` method
    and the properties `boundingBox` and `center`. Let's now briefly describe each
    of these methods, starting with the most obvious: `draw`. We are expecting `Sketch`
    to be responsible for rendering itself, either drawing each stroke or rendering
    the assigned image depending on the type of sketch. The `exportSketch` method
    will be used to obtain a rasterized version of the sketch and is dependent on
    the `boundingBox` property, using it to determine what area of the canvas contains
    information (that is, drawings). Then, it proceeds to rasterize the sketch to
    a `CIImage`, which can then be used to feed the model. The last property, `center`,
    returns and sets the center and is used when the user drags it around the screen
    while in move mode, as described earlier.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实现都应公开一个 `draw` 和 `exportSketch` 方法以及 `boundingBox` 和 `center` 属性。现在让我们简要地描述一下这些方法，从最明显的
    `draw` 方法开始。我们期望 `Sketch` 负责渲染自身，无论是绘制每一笔还是根据草图类型渲染指定的图像。`exportSketch` 方法将用于获取草图的矢量版本，并依赖于
    `boundingBox` 属性，使用它来确定画布上包含信息（即绘图）的区域。然后，它将草图矢量化为 `CIImage`，这可以随后用于喂养模型。最后一个属性
    `center` 返回并设置中心点，在用户在移动模式下拖动它时使用，如前所述。
- en: 'Let''s now proceed to implement a concrete version of a `Sketch` for dealing
    with strokes. Add the following code in the `Sketch` class, still within the `Sketch.swift`
    file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续实现一个处理笔划的 `Sketch` 的具体版本。在 `Sketch` 类中添加以下代码，仍然在 `Sketch.swift` 文件中：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we have defined a new class, `StrokeSketch`, adhering to the `Sketch`
    protocol. We have defined two properties: a list for holding all the strokes and
    a string we can use to annotate the sketch. We have also exposed two helper methods.
    One is for returning the current stroke, which will be used while the user is
    drawing, and another is a convenient method for adding a new stroke.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个新的类 `StrokeSketch`，遵循 `Sketch` 协议。我们定义了两个属性：一个用于存储所有笔划的列表，一个字符串，我们可以用它来注释草图。我们还公开了两个辅助方法。一个是返回当前笔划，在用户绘图时使用，另一个是方便地添加新笔划的方法。
- en: 'Let''s now implement the functionality that will be responsible for rendering
    the sketch; add the following code to the `StrokeSketch` class:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现负责渲染草图的函数；将以下代码添加到 `StrokeSketch` 类中：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will implement the protocol''s `draw` method but delegate the task of drawing
    to the methods `drawStrokes` and `drawStroke`. The `drawStrokes` method simply
    iterates over all strokes currently held by our sketch class and passes them to
    the `drawStoke` method, passing a reference of the Core Graphics context and current
    `Stroke`. Within the `drawStroke` method, we first update the context''s stroke
    color and line width, and then we proceed to add and draw the associated path.
    With this now implemented, we have enough functionality for the user to draw.
    But for completeness, let''s implement the functionality for obtaining the bounding
    box, obtaining and updating the sketches, center, and rasterizing the sketch to
    a `CIImage`. We start with the `boundingBox` property and associated methods.
    Add the following code to the `StrokeSketch` class:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现协议的 `draw` 方法，但将绘制任务委托给 `drawStrokes` 和 `drawStroke` 方法。`drawStrokes` 方法简单地遍历我们草图类当前持有的所有笔划，并将它们传递给
    `drawStroke` 方法，传递 Core Graphics 上下文和当前的 `Stroke` 引用。在 `drawStroke` 方法中，我们首先更新上下文的笔划颜色和线宽，然后继续添加并绘制相关的路径。现在我们已经实现了这一点，用户可以绘制了。但为了完整性，让我们实现获取边界框、获取和更新草图、中心和将草图矢量化为
    `CIImage` 的功能。我们从 `boundingBox` 属性及其相关方法开始。将以下代码添加到 `StrokeSketch` 类中：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We first implement the properties `minPoint` and `maxPoint`; they resemble our
    `minPoint` and `maxPoint` in our `Stroke` class. But instead of operating on a
    collection of points, they operate on a collection of strokes and utilize their
    counterparts (the `minPoint` and `maxPoint` properties of the `Stroke` class).
    Next, we implement the `boundingBox` property, which creates a `CGRect` that encapsulates
    these minimum and maximum points with the addition of some padding to avoid cropping
    the stroke itself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现了`minPoint`和`maxPoint`属性；它们类似于我们`Stroke`类中的`minPoint`和`maxPoint`。但它们不是在点集合上操作，而是在笔划集合上操作，并利用它们的对应物（`Stroke`类的`minPoint`和`maxPoint`属性）。接下来，我们实现了`boundingBox`属性，它创建一个`CGRect`，包含这些最小和最大点，并添加一些填充以避免裁剪笔划本身。
- en: 'Now, we will implement the `center` property declared within the `Stroke` protocol.
    The protocol of this is expecting both `get` and `set` blocks to be implemented.
    The getter will simply return the center of the bounding box, which we have just
    implemented, while the setter will iterate over all strokes and translate each
    point using the difference of the previous center and new center value. Let''s
    implement this now. Add the following code to your `StrokeSketch` class; here,
    the `boundingBox` property is a good place:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现`Stroke`协议中声明的`center`属性。这个协议期望实现`get`和`set`块。获取器将简单地返回我们刚刚实现的边界框的中心，而设置器将遍历所有笔划并使用前一个中心和新的中心值之间的差异来平移每个点。让我们现在实现它。将以下代码添加到您的`StrokeSketch`类中；在这里，`boundingBox`属性是一个很好的位置：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we first obtain the current center and then calculate the difference between
    this and the new center assigned to the property. After that, we iterate over
    all strokes and their corresponding points, adding this offset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先获取当前中心，然后计算这个中心与分配给属性的新的中心之间的差异。之后，我们遍历所有笔划及其相应的点，添加这个偏移量。
- en: The final method we need to implement to adhere to the Sketch protocol is `exportSketch`.
    The purpose of this method is to rasterize the sketch into an image (`CIImage`)
    along with scaling it in accordance to the `size` argument, if available; otherwise
    it defaults to the actual size of the sketch itself. The method itself is fairly
    long but does nothing overly complicated. We have already implemented the functionality
    to render the sketch (via the `draw` method). But rather than rendering to a Core
    Graphic context that has been passed in by the view, we want to create a new context,
    adjust the scale with respect to the size argument and the actual sketch size,
    and finally create a `CIImage` instance from it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现的最后一个方法以符合Sketch协议是`exportSketch`。这个方法的目的是将草图光栅化成图像（`CIImage`），并根据`size`参数进行缩放（如果有的话）；否则，它默认为草图本身的实际大小。这个方法本身相当长，但并没有做过于复杂的事情。我们已经在`draw`方法中实现了渲染草图的功能。但不是将渲染到由视图传入的Core
    Graphics上下文中，而是想要创建一个新的上下文，根据大小参数和实际的草图尺寸调整比例，并最终从它创建一个`CIImage`实例。
- en: 'To make it more readable, let''s break the method down into these parts, starting
    with calculating the scale. Then we''ll look at creating and rendering to a `context`,
    and finally wrap it in a `CIImage`; add the following code to your `StrokeSketch`
    class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更易于阅读，让我们将方法分解为这些部分，从计算比例开始。然后我们将查看创建和渲染到`context`的过程，最后将其包裹在`CIImage`中；将以下代码添加到您的`StrokeSketch`类中：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this code block, we declare our method and implement the functionality that
    determines the export size and scale. If no size is passed in, we simply fall
    back to the size of the sketch's bounding box property. Finally, we ensure that
    we have something to export.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块中，我们声明了我们的方法并实现了确定导出大小和比例的功能。如果没有传递大小，我们则简单地回退到草图边界框属性的尺寸。最后，我们确保我们有一些可以导出的内容。
- en: 'Now our task is to create the `context` and render out our sketch with respect
    to the derived scale; append the following code to the `exportSketch` method:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们任务是创建`context`并根据导出的比例渲染草图；将以下代码添加到`exportSketch`方法中：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use `UIGraphicsBeginImageContextWithOptions` from Core Graphics to create
    a new `context` and obtain reference to this `context` using the `UIGraphicsGetCurrentContext`
    method. `UIGraphicsBeginImageContextWithOptions` creates a temporary rendering
    context, where the first argument is the target size for this context, the second
    determines whether we are using an opaque or transparent background, and the final
    argument determines the display scale factor. We then fill the `context` with
    white and update the context''s `CGAffineTransform` property using the `scaleBy`
    method. Subsequent draw methods, such as moving and drawing, will be transformed
    by this, which nicely takes care of scaling for us. We then pass in this `context`
    to our sketch''s `draw` method, which takes care of rendering the sketch to the
    context. Our final task is obtaining the image from the `context` and wrapping
    it in an instance of `CIImage`. Let''s do that now; append the following code
    to your `exportSketch` method:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Core Graphics 中的 `UIGraphicsBeginImageContextWithOptions` 方法来创建一个新的 `context`，并通过
    `UIGraphicsGetCurrentContext` 方法获取对这个 `context` 的引用。`UIGraphicsBeginImageContextWithOptions`
    创建一个临时的渲染上下文，其中第一个参数是此上下文的目标大小，第二个参数确定我们是否使用不透明或透明背景，最后一个参数确定显示缩放因子。然后我们用白色填充
    `context`，并使用 `scaleBy` 方法更新上下文的 `CGAffineTransform` 属性。随后的绘制方法，如移动和绘制，都将通过这种方式进行变换，这为我们很好地处理了缩放。然后我们将这个
    `context` 传递给我们的 sketch 的 `draw` 方法，该方法负责将 sketch 渲染到上下文中。我们的最终任务是获取 `context`
    中的图像并将其包装在一个 `CIImage` 实例中。现在让我们来做这件事；将以下代码添加到您的 `exportSketch` 方法中：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Thanks to the Core Graphics method `UIGraphicsGetImageFromCurrentImageContext`,
    the task is painless. `UIGraphicsGetImageFromCurrentImageContext` returns an instance
    of `CGImage` with a rasterized version of the context. To create an instance of
    `CIImage`, we simply pass in our image to the constructor and return it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了 Core Graphics 方法 `UIGraphicsGetImageFromCurrentImageContext`，这项任务变得非常简单。`UIGraphicsGetImageFromCurrentImageContext`
    返回一个包含上下文光栅化版本的 `CGImage` 实例。要创建一个 `CIImage` 实例，我们只需将我们的图像传递给构造函数并返回它。
- en: 'We have finished the `Sketch` class—for now—and slowly we''re making our way
    up the layers. Next, we will flesh our the `SketchView` class, which will be responsible for
    facilitating the creation and drawing of the sketches. Select the `SketchView.swift`
    file from the left-hand panel to bring it up in the editing window, and let''s
    quickly review the existing code. `SketchView` has been broken down into chunks
    using extensions; to make the code more legible, we will present each of the chunks
    along with its core functionality:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了 `Sketch` 类——至少目前是这样——并且我们正缓慢地向上层移动。接下来，我们将完善 `SketchView` 类，它将负责促进 sketch
    的创建和绘制。从左侧面板选择 `SketchView.swift` 文件，将其在编辑窗口中打开，让我们快速回顾现有的代码。`SketchView` 已经使用扩展被分解成块；为了使代码更易读，我们将展示每个块及其核心功能：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The majority of the previous code should be self-explanatory, but I do want
    to quickly draw your attention to the `currentSketch` property; we will use this
    getter to provide a convenient way for us to get access to the last sketch, which
    we will consider the currently active sketch. The setter is a little more ambiguous;
    it provides us with an easy way of replacing the currently active (last) sketch,
    which we will use when we come to handling the replacement of a user''s sketch
    with an image suggested to them. The next chunk implements the drawing functionality,
    which should look familiar to you; here, we simply clear the `context` and iterate
    over all sketches, delegating the drawing to them:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码的大部分应该是自解释的，但我确实想快速将您的注意力引到 `currentSketch` 属性上；我们将使用这个获取器为我们提供一个方便的方式来获取最后一个
    sketch，我们将认为它是当前活动的 sketch。设置器稍微有些模糊；它为我们提供了一个方便的方式来替换当前活动（最后一个）的 sketch，当我们处理用用户建议的图像替换用户的
    sketch 时，我们将使用它。下一个块实现了绘制功能，这应该对您来说很熟悉；在这里，我们只是清除 `context` 并遍历所有 sketch，将绘制委托给它们：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our final chunk will be responsible for implementing the drawing functionality;
    currently, we have just stubbed out the methods to intercept the touch events.
    Fleshing these methods out will be our next task:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的块将负责实现绘制功能；目前，我们只是创建了拦截触摸事件的占位方法。完善这些方法将是我们的下一个任务：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Before we proceed with writing the code, let's briefly review what we are trying
    to achieve here. As mentioned previously, `SketchView` will be responsible for
    the functionality, allowing the user to sketch using their finger. We have spent
    the past few pages building the data objects (`Stroke` and `Sketch`) to support
    this functionality and it is here that we will make use of them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们编写代码之前，让我们简要回顾一下我们在这里试图实现的目标。如前所述，`SketchView`将负责功能，允许用户用手指绘图。我们过去几页构建了支持此功能的数据对象（`Stroke`和`Sketch`），我们将在这里使用它们。
- en: 'A touch begins when the user first touches the view (`beginTracking`). When
    we detect this, we want to first check whether we have a currently active and
    appropriate sketch; if not, then we will create one and set it as the current
    sketch. Next, we will create a stroke that will be used to track the user''s finger
    as they drag it around the screen. It is considered complete once the user has
    either lifted their finger or their finger is dragged outside the bounds of the
    view. We will then request the view to redraw itself and finally notify any listening
    parties by broadcasting the event `UIControlEvents.editingDidBegin` action. Let''s
    put this into code; append the following code to `beginTracking` within the `SketchView`
    class:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户第一次触摸视图时（`beginTracking`），触摸开始。当我们检测到这一点时，我们首先检查是否有一个当前活动且合适的草图；如果没有，我们将创建一个并将其设置为当前草图。接下来，我们将创建一个笔划，用于跟踪用户在屏幕上拖动手指。一旦用户抬起手指或手指被拖出视图边界，它就被认为是完成的。然后我们将请求视图重新绘制自己，并通过广播事件`UIControlEvents.editingDidBegin`动作通知任何监听方。让我们将这些放入代码中；将以下代码添加到`SketchView`类中的`beginTracking`方法内：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As described in the iOS documentation, here, we are adhering to the target-action
    mechanism, common in controls, by which we broadcast interesting events to simplify
    how other classes can integrate with this control.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如iOS文档所述，在此，我们遵循常见的控件中的目标-动作机制，通过广播有趣的事件来简化其他类如何与该控件集成。
- en: 'Next, we will implement the body of the `continueTracking` method; here, we
    simply append a new point to the current sketches current stroke. As we did before,
    we request that the view to redraw itself and broadcast the `UIControlEvents.editingChanged`
    action. Append the following code to the body of the `continueTracking` method:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`continueTracking`方法的主体；在这里，我们只是将一个新的点添加到当前草图当前笔划中。正如我们之前所做的那样，我们请求视图重新绘制自己并广播`UIControlEvents.editingChanged`动作。将以下代码添加到`continueTracking`方法的主体中：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The previous code resembles much of what we need when the user lifts their
    finger, with the exception of returning true (which tells the platform that this
    view wishes to continue consuming events) and replacing the `UIControlEvents.editingChanged`
    event with `UIControlEvents.editingDidEnd`. Add the following code to the body
    of your `endTracking` method:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码与用户抬起手指时我们需要的大部分代码相似，除了返回true（这告诉平台该视图希望继续消耗事件）以及将`UIControlEvents.editingChanged`事件替换为`UIControlEvents.editingDidEnd`。将以下代码添加到你的`endTracking`方法主体中：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The final piece of code we need to add to the `SketchView` class is for dealing
    with when the current finger tracking is canceled (triggered when the finger moves
    off the current view or out of the device''s tracking range, that is, off the
    screen). Here, we are simply treating it as if the tracking has finished, with
    the exception of not adding the last point. Append the following code to the body
    of your `cancelTracking` method:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加到`SketchView`类的最后一部分代码是处理当前手指跟踪被取消的情况（当手指从当前视图移出或超出设备的跟踪范围，即移出屏幕时触发）。在这里，我们只是将其视为跟踪已完成，除了不添加最后一个点。将以下代码添加到你的`cancelTracking`方法主体中：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With our `SketchView` finished, our application now supports the functionality
    of sketching. Now would be a good time to build and run the application on either
    the simulator or device and check that everything is working correctly. If it
    is, then you should be able to draw onto the screen, as shown in the following
    image:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`SketchView`完成之后，我们的应用程序现在支持绘图功能。现在是时候在模拟器或设备上构建并运行应用程序，以检查一切是否正常工作。如果是这样，那么你应该能够在屏幕上绘制，如下面的图像所示：
- en: '![](img/51a522b4-2f37-42e4-9721-a5c2528ab43c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/51a522b4-2f37-42e4-9721-a5c2528ab43c.png)'
- en: The functionality of moving and clearing the canvas has already been implemented;
    tap on the Move button to drag your sketch around, and tap on the Trash button
    to clear the canvas. Our next task will be to import a trained Core ML model and
    implement the functionality of classifying and suggesting images to the user.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 移动和清除画布的功能已经实现；点击移动按钮可以拖动你的草图，点击垃圾桶按钮可以清除画布。我们的下一个任务将是导入一个训练好的Core ML模型，并实现为用户分类和推荐图像的功能。
- en: Recognizing the user's sketch
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别用户的草图
- en: In this section, we will first review the dataset and model we will use to guess
    what the user is drawing. We will then proceed to integrate it into the workflow
    of the user who is sketching, and implement the functionality to support swapping
    out the user's sketch with a selected image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先回顾我们将使用的数据集和模型，以猜测用户正在绘制的内容。然后，我们将将其集成到正在绘制用户的流程中，并实现替换用户草图与所选图像的功能。
- en: Reviewing the training data and model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查训练数据和模型
- en: 'For this chapter, a CNN was trained on the dataset that was used and made available
    from the research paper *How Do Humans Sketch Objects?* by Mathias Eitz, James
    Hays, and Marc Alexa. The paper, presented at SIGGRAPH in 2012, compares the performance
    of humans classifying sketches to that of a machine. The dataset consists of 20,000
    sketches evenly distributed across 250 object categories, ranging from airplanes
    to zebras; a few examples are shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们使用Mathias Eitz、James Hays和Marc Alexa在2012年SIGGRAPH上发表的研究论文《人类如何绘制物体？》中使用并公开的数据集训练了一个CNN。该论文比较了人类对草图进行分类的性能与机器的性能。数据集包含20,000个草图，均匀分布在250个对象类别中，从飞机到斑马；这里展示了几个示例：
- en: '![](img/4070f2c9-8baa-4b34-8861-9dd706d5bdf7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4070f2c9-8baa-4b34-8861-9dd706d5bdf7.png)'
- en: From a perceptual study, they found that humans correctly identified the object
    category (such as snowman, grapes, and many more) of a sketch 73% of the time.
    The competitor, their ML model, got it right 56% of the time. Not bad! You can
    find out more about the research and download the accompanying dataset here at
    the official web page: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从感知研究中，他们发现人类正确识别草图的对象类别（如雪人、葡萄等）的比例为73%。他们的竞争对手，他们的机器学习模型，正确识别的比例为56%。还不错！你可以在官方网站上了解更多关于这项研究并下载相关数据集：[http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/).
- en: In this project, we will be using a slightly smaller set, with 205 out of the
    250 categories; the exact categories can be found in the CSV file `/Chapter7/Training/sketch_classes.csv`,
    along with the Jupyter Notebooks used to prepare the data and train the model.
    The original sketches are available in SVG and PNG formats. Because we're using
    a CNN, rasterized images (PNG) were used but rescaled from 1111 x 1111 to 256
    x 256; this is the expected input of our model. The data was then split into a
    training and a validation set, using 80% (64 samples from each category) for training
    and 20% (17 samples from each category) for validation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用一个稍微小一些的集合，其中250个类别中有205个；具体的类别可以在CSV文件 `/Chapter7/Training/sketch_classes.csv`
    中找到，以及用于准备数据和训练模型的Jupyter Notebooks。原始草图以SVG和PNG格式提供。由于我们使用的是CNN，因此使用了光栅化图像（PNG），但已从1111
    x 1111重新缩放为256 x 256；这是我们模型的预期输入。然后，数据被分为训练集和验证集，其中80%（每个类别的64个样本）用于训练，20%（每个类别的17个样本）用于验证。
- en: 'The architecture of the network was not too dissimilar to what has been used
    in previous chapters, with the exception of a larger kernel window used in the
    first layer to extract the spare features of the sketch, as presented here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构与之前章节中使用的大致相似，只是第一层使用了更大的核窗口来提取草图的辅助特征，如下所示：
- en: '![](img/51fd8cbe-382a-45a9-8556-fc90861c62c2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/51fd8cbe-382a-45a9-8556-fc90861c62c2.png)'
- en: 'Recall that stacking convolution layers on top of each other allows the model
    to build up a shared set of high-level patterns that can then be used to perform
    classification, as opposed to using the raw pixels. The last convolution layer
    is flattened and then fed into a fully connected layer, where the prediction is
    finally made. You can think of these fully connected nodes as switches that turn
    on when certain (high-level) patterns are present in the input, as illustrated
    in the following diagram. We will return to this concept later on in this chapter
    when we implement sorting:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，将卷积层堆叠在一起允许模型构建一组共享的高级模式，然后可以使用这些模式进行分类，而不是使用原始像素。最后一个卷积层被展平，然后输入到全连接层，在那里最终做出预测。你可以将这些全连接节点视为当输入中存在某些（高级）模式时开启的开关，如下面的图表所示。我们将在本章后面实现排序时回到这个概念。
- en: '![](img/0dfa0a19-ff7b-422b-b116-4a951b3fde8b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0dfa0a19-ff7b-422b-b116-4a951b3fde8b.png)'
- en: 'After 68 iterations (epochs), the model was able to achieve an accuracy of
    approximately 65% on the validation data. Not exceptional, but if we consider
    the top two or three predictions, then this accuracy increases to nearly 90%.
    The following diagram shows the plots comparing training and validation accuracy,
    and loss during training:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 经过68次迭代（周期）后，模型在验证数据上达到了大约65%的准确率。并不算出色，但如果考虑前两个或三个预测，那么这个准确率会增加到近90%。以下图表显示了训练和验证准确率以及训练过程中的损失：
- en: '![](img/79e0e42a-6233-4050-b0c7-48826ad7c30d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79e0e42a-6233-4050-b0c7-48826ad7c30d.png)'
- en: With our model trained, our next step is to export it using the Core ML Tools
    made available by Apple (as discussed in previous chapters) and imported into
    our project.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型训练完成后，下一步是使用苹果提供的Core ML工具（如前几章所述）将其导出，并将其导入到我们的项目中。
- en: Classifying sketches
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘图分类
- en: In this section, we will walk though importing the Core ML model into our project
    and hooking it up, including using the model to perform inference on the user's
    sketch and also searching and suggesting substitute images for the user to swap
    their sketch with. Let's get started with importing the Core ML model into our
    project.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何将Core ML模型导入到我们的项目中，并将其连接起来，包括使用模型对用户的草图进行推理，以及搜索和推荐替代图像供用户替换草图。让我们开始将Core
    ML模型导入到我们的项目中。
- en: 'Locate the model in the project repositories folder `/CoreMLModels/Chapter7/cnnsketchclassifier.mlmodel`;
    with the model selected, drag it into your Xcode project, leaving the defaults
    for the Import options. Once imported, select the model to inspect the details,
    which should look similar to the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目仓库文件夹`/CoreMLModels/Chapter7/cnnsketchclassifier.mlmodel`中定位模型；选择模型后，将其拖入你的Xcode项目，保留导入选项的默认设置。一旦导入，选择模型以检查详细信息，应该类似于以下截图：
- en: '![](img/9be8dc44-63da-4ee5-9107-3beb970920b6.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9be8dc44-63da-4ee5-9107-3beb970920b6.png)'
- en: As with all our models, we verify that the model is included in the target by
    verifying that the appropriate Target Membership is checked, and then we turn
    our attention to the inputs and outputs, which should be familiar by now. We can
    see that our model is expecting a single-channel (grayscale) 256 x 256 image and
    it returns the dominate class via the classLabel property of the output, along
    with a dictionary of probabilities of all classes via the classLabelProbs property.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所有的模型一样，我们验证模型是否包含在目标中，通过检查适当的Target Membership是否被选中，然后我们将注意力转向输入和输出，这些现在应该已经很熟悉了。我们可以看到，我们的模型期望一个单通道（灰度）256
    x 256的图像，并通过输出对象的classLabel属性返回主导类别，同时通过classLabelProbs属性返回所有类别的概率字典。
- en: 'With our model now imported, let''s discuss the details of how we will be integrating
    it into our project. Recall that our `SketchView` emits the events `UIControlEvents.editingDidStart`,
    `UIControlEvents.editingChanged`, and `UIControlEvents.editingDidEnd` as the user
    draws. If you inspect the `SketchViewController`, you will see that we have already
    registered to listen for the `UIControlEvents.editingDidEnd` event, as shown in
    the following code snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了模型，让我们讨论如何将其集成到我们的项目中的细节。回想一下，我们的`SketchView`在用户绘制时发出`UIControlEvents.editingDidStart`、`UIControlEvents.editingChanged`和`UIControlEvents.editingDidEnd`事件。如果你检查`SketchViewController`，你会看到我们已经注册了监听`UIControlEvents.editingDidEnd`事件，如下面的代码片段所示：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Each time the user ends a stroke, we will start the process of trying to guess
    what the user is sketching and search for suitable substitutes. This functionality
    is triggered via the `.editingDidEnd` action method `onSketchViewEditingDidEnd`,
    but will be delegated to the class `QueryFacade`, which will be responsible for
    implementing this functionality. This is where we will spend the majority of our
    time in this section and the next section. It's also probably worth highlighting
    the statement `queryFacade.delegate = self` in the previous code snippet. `QueryFacade`
    will be performing most of its work off the main thread and will notify this delegate
    of the status and results once finished, which we will get to in a short while.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每当用户结束一笔画时，我们将开始尝试猜测用户正在绘制的草图并搜索合适的替代品。此功能通过`.editingDidEnd`动作方法`onSketchViewEditingDidEnd`触发，但将被委托给`QueryFacade`类，该类将负责实现此功能。这就是我们将在这个部分和下一个部分花费大部分时间的地方。同时，也值得在之前的代码片段中突出显示`queryFacade.delegate
    = self`这一声明。`QueryFacade`将在主线程之外执行大部分工作，并在完成后通知此代理状态和结果，我们将在稍后讨论。
- en: 'Let''s start by implementing the functionality of the `onSketchViewEditingDidEnd`
    method, before turning our attention to the `QueryFacade` class. Within the `SketchViewController`
    class, navigate to the `onSketchViewEditingDidEnd` method and append the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先实现`onSketchViewEditingDidEnd`方法的功能，然后再关注`QueryFacade`类。在`SketchViewController`类中，导航到`onSketchViewEditingDidEnd`方法，并添加以下代码：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we are getting the current sketch, and returning it if no sketch is available
    or if it''s not a `StrokeSketch`; we hand it over to our `queryFacade` (an instance
    of the `QueryFacade` class). Let''s now turn our attention to the `QueryFacade`
    class; select the `QueryFacade.swift` file from the left-hand panel within Xcode
    to bring it up in the editor area. A lot of plumbing has already been implemented
    to allow us to focus our attention on the core functionality of predicting, searching,
    and sorting. Let''s quickly discuss some of the details, starting with the properties:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在获取当前草图，如果没有草图可用或它不是一个`StrokeSketch`，则返回它；我们将其交给我们的`queryFacade`（`QueryFacade`类的一个实例）。现在让我们将注意力转向`QueryFacade`类；在Xcode的左侧面板中选择`QueryFacade.swift`文件，将其在编辑区域中打开。已经实现了很多底层代码，以便我们能够专注于预测、搜索和排序的核心功能。让我们快速讨论一些细节，从属性开始：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`QueryFacade` is only concerned with the most current sketch. Therefore, each
    time a new sketch is assigned using the `currentSketch` property, `queryCanceled`
    is set to `true`. During each task (such as performing prediction, search, and
    downloading), we check the `isInterrupted` property, and if `true`, we will exit
    early and proceed to process the latest sketch.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`QueryFacade`只关注最新的草图。因此，每次使用`currentSketch`属性分配新的草图时，`queryCanceled`被设置为`true`。在每次任务（如执行预测、搜索和下载）期间，我们检查`isInterrupted`属性，如果为`true`，则提前退出并继续处理最新的草图。'
- en: 'When you pass the sketch to the `asyncQuery` method, the sketch is assigned
    to the `currentSketch` property and then proceeds to call `queryCurrentSketch` to
    do the bulk of the work, unless there is one currently being processed:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将草图传递给`asyncQuery`方法时，草图被分配给`currentSketch`属性，然后继续调用`queryCurrentSketch`方法来完成大部分工作，除非当前有一个正在处理中：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Eventually, we end up in the `queryCurrentSketch` method, where we will now
    turn our attention and implement the required functionality. But before doing
    so, let's quickly discuss what we'll be doing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们到达`queryCurrentSketch`方法，我们将在这里转向并实现所需的功能。但在这样做之前，让我们快速讨论我们将要做什么。
- en: Recall that our goal is to assist the user in quickly sketching out a scene;
    we plan on achieving this by anticipating what the user is trying to draw and
    suggesting images, which the user can swap with their sketch. Prediction is a
    major component of this system and is made using the trained model we have just
    imported, but recall that we achieved approximately 65% accuracy on the validation
    dataset. This leaves a lot of room for errors, potentially inhibiting the user
    rather than augmenting them. To mitigate this and provide more utility, we will
    take the top 3-4 predictions and pull down the relevant images rather than relying
    on a single classification.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的目标是帮助用户快速绘制场景；我们计划通过预测用户试图绘制的内容并建议图像来实现这一点，用户可以用这些图像替换他们的草图。预测是这个系统的主要组成部分，使用我们刚刚导入的训练模型进行，但请记住，我们在验证数据集上达到了大约65%的准确率。这留下了很多错误的空间，可能会抑制用户而不是增强他们。为了减轻这一点并提供更多功能，我们将选择前3-4个预测并拉下相关图像，而不是依赖于单一分类。
- en: We pass these predicted classes to Microsoft's Bing Image Search API to find
    relevant images and then proceed to download each of them (admittedly not the
    most optimized approach, but sufficient for realizing this prototype). Once we
    have downloaded the images, we will perform some further processing by sorting
    the images based on how similar each image is to what the user has sketched; we
    will return to this in the next section, but for now we will concentrate on the
    steps preceding this. Let's move on to guessing what the user is trying to do.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些预测类别传递给微软的Bing图像搜索API以查找相关图像，然后继续下载每个图像（虽然这并不是最优化方法，但对于实现这个原型来说是足够的）。一旦我们下载了图像，我们将根据每个图像与用户所绘制的图像的相似度对图像进行排序，我们将在下一节中回到这一点，但现在我们将专注于这一步骤之前的过程。让我们继续猜测用户试图做什么。
- en: 'As we have done previously, let''s work bottom-up by implementing all the supporting
    methods before we tie everything together within the `queryCurrentSketch` method.
    Let''s start by declaring an instance of our model; add the following variable
    within the `QueryFacade` class near the top:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，让我们自下而上地工作，在将所有支持方法在 `queryCurrentSketch` 方法中整合在一起之前先实现它们。让我们首先声明我们模型的实例；在
    `QueryFacade` 类的顶部附近添加以下变量：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, with our model instantiated and ready, we will navigate to the `classifySketch`
    method of the `QueryFacade` class; it is here that we will make use of our imported
    model to perform inference, but let''s first review what already exists:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型已经实例化并准备就绪，我们将导航到 `QueryFacade` 类的 `classifySketch` 方法；在这里，我们将使用我们导入的模型进行推理，但让我们首先回顾一下已经存在的内容：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we see that the `classifySketch` is overloaded, with one method accepting
    a `Sketch` and the other a `CIImage`. The former, when called, will obtain the
    rasterize version of the sketch using the `exportSketch` method. If successful,
    it will resize the rasterized image using the `targetSize` property. Then, it
    will rescale the pixels before passing the prepared `CIImage` along to the alternative `classifySketch`
    method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 `classifySketch` 是重载的，一个方法接受一个 `Sketch`，另一个接受一个 `CIImage`。当调用前者时，将使用
    `exportSketch` 方法获取草图的光栅化版本。如果成功，它将使用 `targetSize` 属性调整光栅化图像的大小。然后，它将调整像素值，然后将准备好的
    `CIImage` 传递给备选的 `classifySketch` 方法。
- en: Pixel values are in the range of 0-255 (per channel; in this case, it's just
    a single channel). Typically, you try to avoid having large numbers in your network.
    The reason is that they make it more difficult for your model to learn (converge)—somewhat
    analogous to trying to drive a car whose steering wheel can only be turned hard
    left or hard right. These extremes would cause a lot of over-steering and make
    navigating anywhere extremely difficult.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 像素值在0-255（每个通道；在这种情况下，只有一个通道）的范围内。通常，你试图避免在网络中拥有大数字。原因是它们会使你的模型学习（收敛）变得更加困难——某种程度上类似于试图驾驶一个方向盘只能向左或向右硬转的车。这些极端会导致大量过度转向，使导航变得极其困难。
- en: 'The second `classifySketch` method will be responsible for performing the actual
    inference; we have already seen how we can do this in [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognizing
    Objects in the world*. Add the following code within the `classifySketch(image:CIImage)`
    method:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 `classifySketch` 方法将负责执行实际推理；我们已经在[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)，“世界中的物体识别”中看到如何做到这一点。在
    `classifySketch(image:CIImage)` 方法内添加以下代码：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we use the images, `toPixelBuffer` method, an extension we added to the
    `CIImage` class back in [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml),
    *Recognizing Objects in the World*, to obtain a grayscale `CVPixelBuffer` representation
    of itself. Now, with reference to its buffer, we pass it onto the `prediction`
    method of our model instance, `sketchClassifier`, to obtain the probabilities
    for each label. We finally sort these probabilities from the most likely to the
    least likely before returning the sorted results to the caller.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用图像、`toPixelBuffer`方法，这是我们之前在[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)中添加到`CIImage`类的一个扩展，以获得其自身的灰度`CVPixelBuffer`表示。现在，根据其缓冲区，我们将其传递给我们的模型实例`sketchClassifier`的`prediction`方法，以获得每个标签的概率。我们最终将这些概率从最有可能到最不可能进行排序，然后将排序后的结果返回给调用者。
- en: 'Now, with some inkling as to what the user is trying to sketch, we will proceed
    to search and download the ones we are most confident about. The task of searching
    and downloading will be the responsibility of the `downloadImages` method within
    the `QueryFacade` class. This method will make use of an existing `BingService`
    that exposes methods for searching and downloading images. Let''s hook this up
    now; jump into the `downloadImages` method and append the following highlighted
    code to its body:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据我们对用户试图绘制的对象的了解，我们将继续搜索和下载我们最有信心的一些图像。搜索和下载的任务将由`QueryFacade`类中的`downloadImages`方法负责。此方法将使用现有的`BingService`，该服务公开了搜索和下载图像的方法。现在让我们连接这些功能；跳转到`downloadImages`方法并将其体中的以下突出显示代码添加进去：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `downloadImages` method takes the arguments `searchTerms`, `searchTermsCount`,
    and `searchResultsCount`. The `searchTerms` is a sorted list of labels returned
    by our `classifySketch` method, from which the `searchTermsCount` determines how
    many of these search terms we use (defaulting to 4). Finally, `searchResultsCount`
    limits the results returned for each search term.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`downloadImages`方法接受`searchTerms`、`searchTermsCount`和`searchResultsCount`参数。`searchTerms`是我们`classifySketch`方法返回的标签的排序列表，`searchTermsCount`确定我们使用这些搜索术语的数量（默认为4）。最后，`searchResultsCount`限制了每个搜索术语返回的结果数量。'
- en: The preceding code performs a sequential search using the search terms passed
    into the method. And as mentioned previously, here we are using Microsoft's Bing
    Image Search API, which requires registration, something we will return to shortly.
    After each search, we check the property `isInterrupted` to see whether we need
    to exit early; otherwise, we continue on to the next search.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用传递给方法中的搜索术语进行顺序搜索。如前所述，这里我们使用微软的Bing图像搜索API，这需要注册，我们将在稍后回到这个问题。在每次搜索后，我们检查`isInterrupted`属性以确定是否需要提前退出；否则，我们继续进行下一个搜索。
- en: 'The result returned by the search includes a URL referencing an image; we will
    use this next to download the image with each of the results, before returning
    an array of `CIImage` to the caller. Let''s add this now. Append the following
    code to the `downloadImages` method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索返回的结果包括一个引用图像的URL；我们将使用这个URL下载每个结果中的图像，在返回`CIImage`数组给调用者之前。现在让我们添加这个功能。将以下代码添加到`downloadImages`方法中：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As before, the process is synchronous and after each download, we check the
    `isInterrupted` property to see if we need to exit early, otherwise returning
    the list of downloaded images to the caller.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个过程是同步的，在每次下载后，我们检查`isInterrupted`属性以确定是否需要提前退出，否则将下载的图像列表返回给调用者。
- en: 'So far, we have implemented the functionality to support prediction, searching,
    and downloading; our next task is to hook all of this up. Head back to the `queryCurrentSketch`
    method and add the following code within the `queryQueue.async` block. Ensure
    that you replace the `DispatchQueue.main.async` block:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了支持预测、搜索和下载的功能；我们的下一个任务是连接所有这些功能。回到`queryCurrentSketch`方法，并在`queryQueue.async`块内添加以下代码。确保您替换掉`DispatchQueue.main.async`块：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It's a large block of code but nothing complicated; let's quickly walk our way
    through it. We start by calling the `classifySketch` method we just implemented.
    As you may recall, this method returns a sorted list of label and probability
    peers unless interrupted, in which case `nil` will be returned. We should handle
    this by notifying the delegate before exiting the method early (a check we apply
    to all of our tasks).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段大量的代码，但并不复杂；让我们快速浏览一下。我们首先调用我们刚刚实现的`classifySketch`方法。如您所忆，除非被中断，否则此方法将返回标签和概率的排序列表，否则将返回`nil`。我们应该通过在方法早期退出之前通知委托来处理这种情况（这是我们应用于所有任务的一个检查）。
- en: Once we've obtained the list of sorted labels, we pass them to the `downloadImages`
    method to receive the associated images, which we then pass to the `sortByVisualSimilarity`
    method. This method currently returns just the list of images, but it's something
    we will get back to in the next section. Finally, the method passes the status
    and sorted images wrapped in a `QueryResult` instance to the delegate via the
    main thread, before checking whether it needs to process a new sketch (by calling
    the `processNextQuery` method).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了排序标签的列表，我们就将它们传递给`downloadImages`方法以接收相关的图像，然后我们将这些图像传递给`sortByVisualSimilarity`方法。这个方法目前只返回图像列表，但这是我们将在下一节中返回的内容。最后，该方法通过主线程将状态和排序后的图像包装在`QueryResult`实例中传递给委托，然后在调用`processNextQuery`方法之前检查是否需要处理新的草图。
- en: At this stage, we have implemented all the functionality required to download
    our substitute images based on our guess as to what the user is currently sketching.
    Now, we just need to jump into the `SketchViewController` class to hook this up,
    but before doing so, we need to obtain a subscription key to use Bing's Image
    Search.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经实现了根据我们对用户当前绘制的猜测下载替代图像所需的所有功能。现在，我们只需跳转到`SketchViewController`类来设置这个功能，但在这样做之前，我们需要获取一个订阅密钥来使用Bing的图像搜索。
- en: 'Within your browser, head to [https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/](https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/)
    and click on the Try Bing Image Search API, as shown in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的浏览器中，访问[https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/](https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/)并点击“尝试Bing图像搜索API”，如图所示：
- en: '![](img/a7e42186-fa61-40fd-a2c0-8054baed9ee7.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7e42186-fa61-40fd-a2c0-8054baed9ee7.png)'
- en: 'After clicking on Try Bing Image Search API, you will be presented with a series
    of dialogs; read, and once (if) agreed, sign in or register. Continue following
    the screens until you reach a page informing you that the Bing Search API has
    been successfully added to your subscription, as shown in the following screenshot:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“尝试Bing图像搜索API”后，您将看到一系列对话框；阅读并（如果）同意后，登录或注册。继续按照屏幕提示操作，直到你到达一个页面，告知Bing搜索API已成功添加到您的订阅中，如图所示：
- en: '![](img/ef546e09-8bd5-4d38-b920-8c5387185db8.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef546e09-8bd5-4d38-b920-8c5387185db8.png)'
- en: 'On this page, scroll down until you come across the entry Bing Search APIs
    v7\. If you inspect this block, you should see a list of Endpoints and Keys. Copy
    and paste one of these keys within the `BingService.swift` file, replacing the
    value of the constant `subscriptionKey`; the following screenshot shows the web
    page containing the service key:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上，向下滚动直到你找到条目“Bing搜索API v7”。如果你检查这个块，你应该会看到一个端点和密钥的列表。将其中一个密钥复制并粘贴到`BingService.swift`文件中，替换常量`subscriptionKey`的值；以下截图显示了包含服务密钥的网页：
- en: '![](img/13940879-843b-4e07-b6fa-e1c5a76483be.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13940879-843b-4e07-b6fa-e1c5a76483be.png)'
- en: 'Return to the `SketchViewController` by selecting the `SketchViewController.swift`
    file from the left-hand panel, and locate the method `onQueryCompleted`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从左侧面板中选择`SketchViewController.swift`文件返回到`SketchViewController`，并定位到方法`onQueryCompleted`：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Recall that this is a method signature defined in the `QueryDelegate` protocol,
    which the `QueryFacade` uses to notify the delegate if the query fails or completes.
    It is here that we will present the matching images we have found through the
    process we just implemented. We do this by first checking the status. If deemed
    successful (greater than zero), we remove every item that is referenced in the `queryImages`
    array, which is the data source for our `UICollectionView` used to present the
    suggested images to the user. Once emptied, we iterate through all the images
    referenced within the `QueryResult` instance, adding them to the `queryImages`
    array before requesting the `UICollectionView` to reload the data. Add the following
    code to the body of the `onQueryCompleted` method:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这是一个在`QueryDelegate`协议中定义的方法签名，`QueryFacade`使用它来通知代理查询是否失败或完成。正是在这里，我们将展示通过我们刚刚实现的过程找到的匹配图像。我们首先检查状态。如果被认为成功（大于零），我们将从`queryImages`数组中移除所有引用的项，这是我们的`UICollectionView`数据源，用于向用户展示建议的图像。一旦清空，我们遍历`QueryResult`实例中引用的所有图像，在请求`UICollectionView`重新加载数据之前，将它们添加到`queryImages`数组中。将以下代码添加到`onQueryCompleted`方法的主体中：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'There we have it; everything is in place to handle guessing of what the user
    draws and present possible suggestions. Now is a good time to build and run the
    application on either the simulator or the device to check whether everything
    is working correctly. If so, then you should see something similar to the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经准备好处理猜测用户绘制的内容并展示可能的建议。现在是时候在模拟器或设备上构建和运行应用程序，以检查一切是否正常工作。如果是这样，你应该会看到以下类似的内容：
- en: '![](img/dc92b5f4-4c43-459a-9714-ad6669acfa16.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc92b5f4-4c43-459a-9714-ad6669acfa16.png)'
- en: There is one more thing left to do before finishing off this section. Remembering
    that our goal is to assist the user to quickly sketch out a scene or something
    similar, our hypothesis is that guessing what the user is drawing and suggesting
    ready-drawn images will help them achieve their task. So far, we have performed
    prediction and provided suggestions to the user, but currently the user is unable
    to replace their sketch with any of the presented suggestions. Let's address this
    now.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本节之前，还有一件事要做。记住我们的目标是帮助用户快速绘制场景或类似的东西，我们的假设是猜测用户正在绘制的内容并建议预先绘制的图像将帮助他们完成任务。到目前为止，我们已经进行了预测并向用户提供了建议，但当前用户无法用任何建议替换他们的草图。现在让我们解决这个问题。
- en: 'Our `SketchView` currently only renders `StrokeSketch` (which encapsulates
    the metadata of the user''s drawing). Because our suggestions are rasterized images,
    our choice is to either extend this class (to render strokes and rasterized images)
    or create a new concrete implementation of the `Sketch` protocol. In this example,
    we will opt for the latter and implement a new type of `Sketch` capable of rendering
    a rasterized image. Select the `Sketch.swift` file to bring it to focus in the
    editor area of Xcode, scroll to the bottom, and add the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`SketchView`目前只渲染`StrokeSketch`（它封装了用户绘制的元数据）。因为我们的建议是光栅化图像，我们的选择是扩展这个类（以渲染笔触和光栅化图像）或创建一个新的`Sketch`协议的具体实现。在这个例子中，我们将选择后者，并实现一个能够渲染光栅化图像的新类型的`Sketch`。选择`Sketch.swift`文件，将其带到Xcode的编辑区域焦点，滚动到最底部，并添加以下代码：
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have defined a simple class that is referencing an image, origin, size, and
    label. The origin determines the top-left position where the image should be rendered,
    while the size determines its, well, size! To satisfy the `Sketch` protocol, we
    must implement the properties `center` and `boundingBox` along with the methods
    `draw` and `exportSketch`. Let's implement each of these in turn, starting with
    `boundingBox`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个简单的类，它引用了一个图像、原点、大小和标签。原点决定了图像应该渲染的左上角位置，而大小决定了其大小！为了满足`Sketch`协议，我们必须实现`center`和`boundingBox`属性以及`draw`和`exportSketch`方法。让我们依次实现这些，首先是`boundingBox`。
- en: 'The `boundingBox` property is a computed property derived from the properties
    `origin` and `size`. Add the following code to your `ImageSketch` class:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`boundingBox`属性是从`origin`和`size`属性派生出的计算属性。将以下代码添加到您的`ImageSketch`类中：'
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Similarly, `center` will be another computed property derived from the origin
    and size properties, simply translating the `origin` with respect to the `size`.
    Add the following code to your `ImageSketch` class:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`center`将是另一个从`origin`和`size`属性派生出的计算属性，简单地将`origin`相对于`size`进行转换。将以下代码添加到您的`ImageSketch`类中：
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `draw` method will simply use the passed-in `context` to render the assigned
    `image` within the `boundingBox`; append the following code to your `ImageSketch` class:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`draw`方法将简单地使用传入的`context`在`boundingBox`内渲染分配的`image`；将以下代码添加到您的`ImageSketch`类中：'
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Our last method, `exportSketch`, is also fairly straightforward. Here, we create
    an instance of `CIImage`, passing in the `image` (of type `UIImage`). Then, we
    resize it using the extension method we implemented back in [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml),
    *Recognizing Objects in the World*. Add the following code to finish off the `ImageSketch`
    class:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后一个方法`exportSketch`也非常直接。在这里，我们创建一个`CIImage`实例，传入`image`（类型为`UIImage`）。然后，我们使用我们在[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)中实现的扩展方法对其进行调整，*识别世界中的物体*。将以下代码添加到完成`ImageSketch`类的代码中：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We now have an implementation of `Sketch` that can handle rendering of rasterized
    images (like those returned from our search). Our final task is to swap the user's
    sketch with an item the user selects from the `UICollectionView`. Return to `SketchViewController`
    class by selecting the `SketchViewController.swift` from the left-hand-side panel
    in Xcode to bring it up in the editor area. Once loaded, navigate to the method
    `collectionView(_ collectionView:, didSelectItemAt:)`; this should look familiar
    to most of you. It is the delegate method for handling cells selected from a `UICollectionView`
    and it's where we will handle swapping of the user's current sketch with the selected
    item.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个可以处理渲染光栅化图像（如我们搜索返回的图像）的`Sketch`实现。我们的最终任务是替换用户绘制的草图与用户从`UICollectionView`中选择的项。通过在Xcode左侧面板中选择`SketchViewController.swift`来返回`SketchViewController`类，将其在编辑区域中打开。一旦加载，导航到方法`collectionView(_
    collectionView:, didSelectItemAt:)`；这对大多数人来说应该很熟悉。这是处理从`UICollectionView`中选择的单元格的代理方法，我们将在这里处理替换用户当前草图与所选项。
- en: 'Let''s start by obtaining the current sketch and associated image that was
    selected. Add the following code to the body of the `collectionView(_collectionView:,didSelectItemAt:)`
    method:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从获取当前草图和所选的关联图像开始。将以下代码添加到`collectionView(_collectionView:,didSelectItemAt:)`方法体中：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, with reference to the current sketch and image, we want to try and keep
    the size relatively the same as the user''s sketch. We will do this by simply
    obtaining the sketch''s bounding box and scaling the dimensions to respect the
    aspect ratio of the selected image. Add the following code, which handles this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据当前的草图和图像，我们想要尝试保持大小与用户的草图相对一致。我们将通过简单地获取草图的边界框并按比例缩放尺寸以尊重所选图像的宽高比来实现这一点。添加以下代码以处理此操作：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we obtain the origin (top left of the image) by obtaining the center
    of the sketch and offsetting it relative to its width and height. Do this by appending
    the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过获取草图的中心并相对于其宽度和高度进行偏移来获取原点（图像的左上角）。通过添加以下代码来完成此操作：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can now use the image, size, and origin to create an `ImageSketch`, and
    replace it with the current sketch simply by assigning it to the `currentSketch`
    property of the `SketchView` instance. Add the following code to do just that:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用图像、大小和原点来创建一个`ImageSketch`，并通过将其分配给`SketchView`实例的`currentSketch`属性来简单地替换当前的草图。将以下代码添加以执行此操作：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, some housekeeping; we''ll clear the `UICollectionView` by removing
    all images from the `queryImages` array (its data source) and request it to reload
    itself. Add the following block to complete the `collectionView(_ collectionView:,didSelectItemAt:)` method:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，进行一些清理工作；我们将通过从`queryImages`数组（其数据源）中移除所有图像并请求它重新加载自身来清除`UICollectionView`。将以下代码块添加到完成`collectionView(_
    collectionView:,didSelectItemAt:)`方法的代码中：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Everything is now hooked up; we have implemented all of the functionality that
    guesses what the user is drawing, presents suggestions, and allows the user to
    swap their rough sketch with an alternative. Now is a good time to build and run
    to ensure that everything is working as planned. If so then, you should be able
    to swap out your sketch with one of the suggestions presented at the top, as shown
    in the following screenshot:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有连接都已就绪；我们已经实现了所有猜测用户绘制的内容、提供建议以及允许用户将他们的草图与替代方案进行交换的功能。现在是构建和运行的好时机，以确保一切按计划进行。如果是这样，那么您应该能够用顶部显示的其中一个建议替换您的草图，如图所示：
- en: '![](img/46e73980-ff72-45ff-a499-9b7c0a43bc23.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46e73980-ff72-45ff-a499-9b7c0a43bc23.png)'
- en: One last section before wrapping this chapter up. In this section, we will look
    at a technique to fine-tune our search results to better match what the user has
    drawn.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，还有一个最后的章节。在本节中，我们将探讨一种技术，以微调我们的搜索结果，使其更好地匹配用户所绘的内容。
- en: Sorting by visual similarity
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按视觉相似度排序
- en: So far, we have achieved what we set out to do, that is, inferring what the
    user is trying to draw and providing them with suggestions that they can swap
    their sketch with. But our solution currently falls short of understanding the
    user. Sure, it may predict correctly and provide the correct category of what
    the user is drawing, but it dismisses any style or details of the user's drawing.
    For example, if the user is drawing, and only wanting, a cats head, our model
    may predict correctly that the user is drawing a cat but ignore the fact that
    their drawing lacks a body. It is likely to suggest images of full-bodied cats.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了我们设定的目标，即推断用户试图绘制的内容，并向他们提供可以交换草图的建议。但我们的解决方案目前还不足以理解用户。当然，它可能预测正确并提供用户所绘内容的正确类别，但它忽略了用户绘图中任何风格或细节。例如，如果用户在画，并且只想画一个猫头，我们的模型可能会正确预测用户在画猫，但忽略了他们的画缺少身体的事实。它可能会建议全身猫的图片。
- en: 'In this section, we will look at a technique to be more sensitive with respect
    to the user''s input, and provide a very rudimentary solution but one that can
    be built upon. This approach will attempt to sort images by how similar they are
    with the user''s sketch. Before jumping into the code, let''s take a quick detour
    to discuss similarity metrics, by looking at how we can measure the similarity
    between something in a different domain, such as sentences. The following are
    three sentences we will base our discussion on:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一种更敏感地对待用户输入的技术，并提供一个非常基础的解决方案，但可以在此基础上构建。这种方法将尝试根据与用户草图相似的程度对图像进行排序。在深入代码之前，让我们先简要讨论一下相似度度量，通过查看我们如何测量不同领域中的相似度，例如句子。以下是我们将基于讨论的三个句子：
- en: '**"the quick brown fox jumped over the lazy dog"**'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"the quick brown fox jumped over the lazy dog"**'
- en: '**"the quick brown fox runs around the lazy farm dog" **'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"the quick brown fox runs around the lazy farm dog"**'
- en: '**"machine learning creates new opportunities for interacting with computers" **'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"machine learning creates new opportunities for interacting with computers"**'
- en: 'This exercise will be familiar to those withttps://packt-type-cloud.s3.amazonaws.com/uploads/sites/1956/2018/06/B09544_08_14.pngal
    representation. Here, we will create a vocabulary with all words that exist in
    our corpus (the three sentences, in this instance) and then create vectors for
    each sentence by incrementing the sentences words with their corresponding index
    in the vocabulary, as shown in the following screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习对于那些熟悉表示方法的人来说很熟悉。在这里，我们将创建一个包含我们语料库（在这个例子中是三个句子）中所有单词的词汇表，然后通过以下截图所示，为每个句子创建向量，即通过将句子中的单词与词汇表中的相应索引进行增量。
- en: '![](img/67898693-ff13-4279-996c-b20e8f0c25fa.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/67898693-ff13-4279-996c-b20e8f0c25fa.png)'
- en: 'With our sentences now encoded as vectors, we can measure the similarity between
    each sentence by performing distance operations such as **Euclidean Distance**
    and **Cosine Distance**. The equations for each of these are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的句子现在已被编码为向量，我们可以通过执行距离操作（如**欧几里得距离**和**余弦距离**）来衡量每句话之间的相似度。以下为每个这些距离的方程式：
- en: '![](img/ddb57d2f-f085-4302-aa51-7df5ee99d794.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ddb57d2f-f085-4302-aa51-7df5ee99d794.png)'
- en: 'Let''s now calculate the distances between each of the sentences and compare
    the results. See the following screenshot for the results:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来计算每句话之间的距离，并比较结果。以下截图显示了结果：
- en: '![](img/1430e9fd-3ceb-460b-9e98-1765d1101a2d.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1430e9fd-3ceb-460b-9e98-1765d1101a2d.png)'
- en: As you would expect, the sentences **"the quick brown fox jumped over the lazy
    dog"** and **"the quick brown fox ran around the lazy farm dog"** have a smaller
    distance between them compared to that for the sentence **"machine learning creates
    new opportunities for interacting with computers"**. If you were to build a recommendation
    engine, albeit a naive one, you would likely rank the sentences with more words
    in common higher than the ones with less words in common. The same is true for
    images, but unlike sentences, where we are using words as features, we use the
    features derived from layers of the network.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所期望的，句子“the quick brown fox jumped over the lazy dog”和“the quick brown fox
    ran around the lazy farm dog”之间的距离比句子“machine learning creates new opportunities
    for interacting with computers”之间的距离要小。如果你要构建一个推荐引擎，尽管是一个简单的推荐引擎，你可能会将具有更多共同词汇的句子排名高于具有较少共同词汇的句子。对于图像来说，情况也是如此，但与句子不同，我们在这里使用的是来自网络层的特征。
- en: 'Recall that our network for classifying sketches consists of a stack of convolution
    layers, with each layer building higher level patterns based on the patterns from
    the layers below it. Intuitively, we can think of these higher level patterns
    as our words (features) and the fully connected network as the sentences representing
    what words are present for a given image. To make this clearer, a simple illustration
    is shown here:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们用于分类草图的网络由一系列卷积层组成，每一层都是基于其下层的模式构建更高层次的模式。直观上，我们可以将这些更高层次的模式视为我们的词汇（特征），而全连接网络则代表给定图像中存在的词汇。为了使这一点更清晰，这里展示了一个简单的插图：
- en: '![](img/9e694871-cf9c-422e-8aa3-5bb4c45f2186.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](img/9e694871-cf9c-422e-8aa3-5bb4c45f2186.png)'
- en: Examining the figure, we can see the set of feature maps on the left, which
    can be thought of as convolutional kernels used to extract horizontal, vertical,
    left, and right diagonal edges from the images.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这张图，我们可以看到左侧的特征图集合，这些可以被视为用于从图像中提取水平、垂直、左对角和右对角边缘的卷积核。
- en: In the middle are the samples from which we will be extracting these features.
    Finally, on the far right, we have the extracted features (histogram) of each
    of the samples. We use these extracted features as our feature vectors and can
    use them to calculate the distance between them, as we saw in the previous figure.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 中间是我们将从中提取这些特征的样本。最后，在最右边，我们有每个样本提取的特征（直方图）。我们使用这些提取的特征作为我们的特征向量，并可以使用它们来计算它们之间的距离，就像我们在之前的图中看到的那样。
- en: 'So, if we are able to extract this type of feature vector from an image, then
    we will also be able to sort them relative to the user''s sketch (using its extracted
    feature vectors). But how do we get this feature vector? Recall that we already
    have a network that has learned high-level feature maps. If we are able to obtain
    a vector indicating which of these features are most active for a given image,
    then we can use this vector as our feature vector and use it to calculate the
    distance between other images, such as the user''s sketch and downloaded images.
    This is exactly what we will do; instead of feeding the network through a softmax
    activation layer (to perform prediction on the classes), we will remove this layer
    from our network, leaving the last fully connected layer as the new output layer.
    This essentially provides us with a feature vector that we can then use to compare
    with other images. The following figure shows how the updated network looks diagrammatically:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够从一个图像中提取这种类型的特征向量，那么我们也将能够根据用户的草图（使用其提取的特征向量）对它们进行排序。但我们是怎样得到这个特征向量的呢？回想一下，我们已经有了一个学习高级特征图的网络。如果我们能够获得一个向量，指示给定图像中哪些特征最为活跃，那么我们可以使用这个向量作为我们的特征向量，并使用它来计算其他图像之间的距离，例如用户的草图和下载的图像。这正是我们将要做的；我们不会通过一个softmax激活层（用于对类别进行预测）来喂给网络，而是从我们的网络中移除这个层，留下最后的全连接层作为新的输出层。这实际上为我们提供了一个特征向量，我们可以用它来与其他图像进行比较。以下图显示了更新后的网络的结构：
- en: '![](img/834497fa-4925-47df-bb6c-78269ebd85cd.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](img/834497fa-4925-47df-bb6c-78269ebd85cd.png)'
- en: If you compare this with the network presented in the previous section, you
    will notice that the only change is the absence of the fully connected layer.
    The output of this network is now a feature vector of size 512\. Let's make this
    concept more concrete by playing with it.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个网络与上一节中展示的网络进行比较，你会注意到唯一的变化是缺少了全连接层。现在这个网络的输出是一个大小为512的特征向量。让我们通过实际操作来使这个概念更加具体。
- en: 'I assume you have already pulled down the accompanying code from the repository [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Navigate to the `Chapter7/Start/QuickDraw/` directory and open the playground
    `FeatureExtraction.playground`. This playground includes the generated code and
    compiled model described earlier, along with some views and helper methods that
    we will make use of; all should be fairly self-explanatory. Let''s begin by importing
    some dependencies and declaring some variables by adding the following code to
    the top of the playground:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经从仓库[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)中拉取了相应的代码。导航到`Chapter7/Start/QuickDraw/`目录并打开游乐场`FeatureExtraction.playground`。这个游乐场包括之前描述的生成代码和编译模型，以及一些我们将使用的视图和辅助方法；所有这些都应该相当直观。让我们首先通过添加以下代码到游乐场的顶部来导入一些依赖项并声明一些变量：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here, we declare two rectangles; they will determine the frame of the views
    we will create later and, most importantly, instantiate our model, which we will
    use to extract features from each image. Talking about this, if you expand the
    `Resources` folder on the left-hand panel, then again in the `Images` folder,
    you''ll see the images we will be using, as shown here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们声明了两个矩形；它们将确定我们稍后创建的视图的框架，最重要的是，实例化我们的模型，我们将使用它从每个图像中提取特征。关于这一点，如果你在左侧面板上展开`Resources`文件夹，然后在`Images`文件夹中再次展开，你会看到我们将使用的图片，如图所示：
- en: '![](img/dd2707ae-53ab-4857-8a6f-16e9ce4bf209.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd2707ae-53ab-4857-8a6f-16e9ce4bf209.png)'
- en: 'As we discussed, we want to be able to sort the images so that the suggested
    images closely match what the user is drawing. Continuing on from our example
    from the user drawing just a cat''s head, we want a way to sort out the images
    so that those with just a cat''s head show up before those with a cat and its
    body. Let''s continue on with our experiment; add the following methods, which
    we will use to extract the features from a given image:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们希望能够对图像进行排序，以便建议的图像与用户绘制的图像尽可能接近。继续我们的用户只画猫头的例子，我们希望有一种方法可以排序图像，使得只有猫头的图像出现在有猫和身体的图像之前。让我们继续我们的实验；添加以下方法，我们将使用这些方法从给定的图像中提取特征：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Most of the code should look familiar to you; we have an overloaded method for
    handling `UIImage`, which simply creates a `CIImage` instance of it before passing
    it to the other method. This will handle preparing the image and finally feed
    it into the model. Once inference has been performed, we return the model's property
    `classActiviations` as discussed previously. This is the output from the last
    fully connected layer, which we'll use as our feature vector for comparison.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代码应该对你来说都很熟悉；我们有一个用于处理`UIImage`的重载方法，该方法在将其传递给其他方法之前，简单地创建一个`CIImage`实例。这将处理图像的准备工作，并将其最终输入到模型中。一旦完成推理，我们返回模型属性`classActiviations`，正如之前所讨论的。这是来自最后一个全连接层的输出，我们将将其用作我们的特征向量进行比较。
- en: 'Next, we will load all of our images and extract the features from each of
    them. Add the following code to your playground:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载所有图像并从每个图像中提取特征。将以下代码添加到您的游乐场中：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With our images and features now available, let''s inspect a few of the images
    and their feature maps. We can do this by creating an instance of `HistogramView`
    and passing in the features. Here is the code to do just that:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了图像和特征，让我们检查一些图像及其特征图。我们可以通过创建一个`HistogramView`实例并将特征传递给它来完成此操作。以下是完成此操作的代码：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can manually inspect each of them by clicking on the eye icon within the
    preview view associated with the state, as shown in the following screenshot:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在预览视图中的眼睛图标上单击来手动检查每个图像，如图中所示：
- en: '![](img/454353fc-845b-47b1-a7e4-d2d52c49cb66.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/454353fc-845b-47b1-a7e4-d2d52c49cb66.png)'
- en: 'Inspecting each of them individually doesn''t provide much insight. So in this
    figure, I have presented three images that we can inspect:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 单独检查它们并没有提供太多见解。因此，在这个图中，我展示了我们可以检查的三张图片：
- en: '![](img/22bb497d-0132-458b-bee8-8c3d0c2cefc2.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/22bb497d-0132-458b-bee8-8c3d0c2cefc2.png)'
- en: Without too much focus, you get a sense that the cat heads' feature vectors
    are more closely aligned than the feature vector of the side on view of the cat,
    especially on the right-hand of the plot.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 没有太多关注，你可以感觉到猫头的特征向量比猫的侧面视图的特征向量更接近，尤其是在图表的右侧。
- en: 'Let''s further explore this by calculating the cosine distance between each
    of the images and plotting them on a heat map. Start by adding the following code;
    it will be used to calculate the cosine distance:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步通过计算每张图像之间的余弦距离并在热图上绘制它们来探索这一点。首先添加以下代码；它将用于计算余弦距离：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The details of the equation were presented before and this is just a translation
    of these into Swift; what is important is the use of the **vector Digital Signal
    Processing** (**vDSP **) functions available within iOS's Accelerate framework.
    As described in the documentation, the vDSP API provides mathematical functions
    for applications such as speech, sound, audio, video processing, diagnostic medical
    imaging, radar signal processing, seismic analysis, and scientific data processing.
    Because it's built on top of Accelerate, it inherits the performance gains achieved
    through **single instruction, multiple data** (**SIMD**) running the same instruction
    concurrently across a vector of data—something very important when dealing with
    large vectors such as those from neural networks. Admittedly, at first it seems
    unintuitive, but the documentation provides most of what you'll need to make good
    use of it; let's inspect the `magnitude` method to get a feel for it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的细节之前已经介绍过，这只是在Swift中将其翻译过来；重要的是使用iOS Accelerate框架中可用的**向量数字信号处理**（**vDSP**）函数。如文档所述，vDSP
    API为语音、声音、音频、视频处理、诊断医学成像、雷达信号处理、地震分析和科学数据处理等应用提供数学函数。因为它建立在Accelerate之上，所以继承了通过**单指令多数据**（**SIMD**）实现的性能提升——在数据向量上同时运行相同的指令——这在处理来自神经网络等大型向量时非常重要。诚然，一开始这似乎不太直观，但文档提供了你使用它所需的大部分信息；让我们检查`magnitude`方法来感受一下。
- en: 'We use the `vDSP_svsD` function to calculate the magnitude of our feature vectors;
    the function is expecting these arguments (in order): a pointer to the data (`UnsafePointer<Double>`),
    strides (`vDSP_Stride`), a pointer to the output variable (`UnsafeMutablePointer<Double>`),
    and finally the length (`vDSP_Length`). Most of the work is in preparing these
    arguments, as shown in this code snippet:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`vDSP_svsD`函数来计算特征向量的幅度；该函数期望以下参数（按顺序）：数据指针（`UnsafePointer<Double>`），步长（`vDSP_Stride`），输出变量指针（`UnsafeMutablePointer<Double>`），以及最后的长度（`vDSP_Length`）。大部分工作在于准备这些参数，如下代码片段所示：
- en: '[PRE46]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After this function returns, we will have the calculated the magnitude of a
    given vector stored in the `output` variable. Let''s now make use of this and
    calculate the distance between each of the images. Add the following code to your
    playground:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数返回后，我们将计算出存储在`output`变量中的给定向量的幅度。现在让我们利用这个结果来计算每张图像之间的距离。将以下代码添加到你的playground中：
- en: '[PRE47]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here, we are iterating through each of the images twice to create a matrix
    (multi-dimensional array, in this case) to store the distances (similarities)
    between each of the images. We will now feed this, along with the associated images,
    to an instance of `HeatmapView`, which will visualize the distances between each
    of the images. Add the following code and then expand the view by clicking on
    the eye icon within the results panel to see the result:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在对每张图像进行两次迭代，以创建一个矩阵（多维数组，在这种情况下）来存储每张图像之间的距离（相似性）。我们现在将这个矩阵以及相关的图像输入到`HeatmapView`实例中，它将可视化每张图像之间的距离。添加以下代码，然后通过在结果面板中点击眼睛图标来扩展视图以查看结果：
- en: '[PRE48]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As mentioned previously, by previewing the view, you should see something similar
    to the following figure:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，通过预览视图，你应该看到以下类似的图：
- en: '![](img/14695986-814c-4877-bbda-7c1ef8f298e0.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14695986-814c-4877-bbda-7c1ef8f298e0.png)'
- en: 'This visualization shows the distance between each of the images; the darker
    the cell, the closer they are. For example, if you look at cell 1 x 1, cell 2
    x 2, and so on, you will see that each of these cells are darker (a distance of
    0 because they are the same image). You''ll also notice another pattern form:
    clusters of four cells diagonally down the plot. This, consequently, was our goal—to
    see whether we could sort sketches by their similarities, such as cats drawn front
    on, cat heads, and cats drawn side on.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可视化显示了每张图像之间的距离；单元格越暗，它们越接近。例如，如果你查看1x1单元格、2x2单元格等，你会看到这些单元格都较暗（距离为0，因为它们是同一张图像）。你还会注意到另一个模式：沿着图表对角线排列的四个单元格的集群。这，因此，是我们的目标——看看我们是否能够通过它们的相似性对草图进行排序，例如正面绘制的猫、猫头和侧面绘制的猫。
- en: Armed with our new knowledge, let's return to the iPhone project `QuickDraw.xcodeproj`,
    where we will copy this code across and implement sorting.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 带着我们的新知识，让我们回到 iPhone 项目 `QuickDraw.xcodeproj`，我们将复制此代码并实现排序。
- en: With the `QuickDraw` project now open, locate the feature extractor model from the
    project repositories folder `/CoreMLModels/Chapter7/cnnsketchfeatureextractor.mlmodel`.
    With the model selected, drag it onto your Xcode project, leaving the defaults
    for the import options.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `QuickDraw` 项目现在已打开，从项目仓库文件夹 `/CoreMLModels/Chapter7/cnnsketchfeatureextractor.mlmodel`
    定位到特征提取模型。选择模型后，将其拖放到你的 Xcode 项目中，保留导入选项的默认设置。
- en: 'With the model now imported, select the file `QueryFacade.swift` from the left-hand
    panel (within Xcode) to bring it up in the editor area. With the class open, add
    an instance variable to the top of the `QueryFacade` class, as shown here:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已导入，从左侧面板（在 Xcode 中）选择文件 `QueryFacade.swift` 以将其在编辑区域中打开。在打开类后，在 `QueryFacade`
    类的顶部添加一个实例变量，如下所示：
- en: '[PRE49]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, copy across the methods `extractFeaturesFromImage`, `cosineSimilarity`,
    `dot`, and `magnitude` from your playground to the `QueryFacade` class, as shown
    here:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将 `extractFeaturesFromImage`、`cosineSimilarity`、`dot` 和 `magnitude` 方法从你的游乐场复制到
    `QueryFacade` 类中，如下所示：
- en: '[PRE50]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'With our methods now it place, it''s time to make use of them. Locate the method `sortByVisualSimilarity(images:[CIImage],
    sketchImage:CIImage)`; this method is already called within the `queryCurrentSketch`
    method, but currently it just returns the list that was passed in. It''s within
    this method that we want to add some order by sorting the list so that the images
    most similar to the user''s sketch are first. Let''s build this up in chunks,
    starting with extracting the image features of the user''s sketch. Add the following
    code to the body of the `sortByVisualSimilarity` method, replacing its current
    contents:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些方法，是时候利用它们了。定位到方法 `sortByVisualSimilarity(images:[CIImage], sketchImage:CIImage)`；此方法已在
    `queryCurrentSketch` 方法中调用，但目前它只是返回传入的列表。我们想要在这个方法中添加一些排序，以便将最类似于用户草图的照片放在前面。让我们分块构建，从提取用户草图的照片特征开始。将以下代码添加到
    `sortByVisualSimilarity` 方法的主体中，替换其当前内容：
- en: '[PRE51]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we want the features of all the other images, which we do simply by iterating
    over the list and storing them in an array. Add the following code to do just
    that:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要所有其他图像的特征，这可以通过遍历列表并将它们存储在数组中简单地完成。添加以下代码以实现这一点：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'As we did previously, after each image, we check whether the process has been
    interrupted by checking the property `isInterrupted`, before moving on to the
    next image. Our final task is to sort and return this images; add the following
    code to the body of the method `sortByVisualSimilarity`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，在处理每张图像后，我们检查属性 `isInterrupted` 以确定进程是否被中断，然后再继续处理下一张图像。我们的最终任务是排序并返回这些图像；将以下代码添加到
    `sortByVisualSimilarity` 方法的主体中：
- en: '[PRE53]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: With that implemented, now is a good time to build and run your project to see
    that is everything is working, and compare the results with the previous build.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施之后，现在是构建和运行你的项目的好时机，以查看一切是否正常工作，并将结果与之前的构建进行比较。
- en: And this concludes the chapter; we will briefly wrap up in the summary before
    moving on to the next chapter.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本章；在进入下一章之前，我们将简要总结。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You are still here. I'm impressed, and congratulations! It was a long but fruitful
    chapter. We saw another example of how we can apply CNNs, and in doing so, we
    further developed our understanding of how they work, how to tune them, and ways
    in which we can modify them. We saw how we could use the learned features not
    just for classification but ranking, a technique used in many domains such as
    fashion discovery and recommendation engines. We also spent a significant amount
    of time building a drawing application, which we will continue to use in the next
    chapter. There, we will again explore how to perform sketch classification using
    a RNN trained on Google's `QuickDraw` dataset. Lots of fun ahead, so let's get
    started.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你还在这里。我很惊讶，恭喜你！这是一个漫长但富有成效的章节。我们看到了另一个例子，说明了我们可以如何应用卷积神经网络（CNNs），在这个过程中，我们进一步了解了它们的工作原理、如何调整它们以及我们可以如何修改它们。我们看到了如何使用学习到的特征不仅用于分类，还用于排名，这是一种在许多领域（如时尚发现和推荐引擎）中使用的技巧。我们还花了很多时间构建一个绘图应用程序，我们将在下一章继续使用它。在那里，我们将再次探索如何使用在
    Google 的 `QuickDraw` 数据集上训练的循环神经网络（RNN）执行草图分类。前方有很多乐趣，让我们开始吧。
