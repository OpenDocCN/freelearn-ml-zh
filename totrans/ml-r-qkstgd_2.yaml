- en: Predicting Failures of Banks - Data Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In each model development, we will need to obtain enough data to build the model.
    It is very common to read the expression g*arbage in, garbage out*, which relates
    to the fact that if you develop a model with bad data, the resulting model will
    be also bad.
  prefs: []
  type: TYPE_NORMAL
- en: Especially in machine learning applications, what we expect is to have a huge
    amount of data, although in many cases that's not the case. Regardless of the
    amount of information available, the quality of this data is the most important
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as a developer, it is important to have structured data, because it
    can be immediately manipulated. However, data is commonly found in an unstructured
    form, meaning that it takes a lot of time to process and prepare for development.
    Many people consider machine learning applications to only be based on the use
    of new algorithms or techniques using lines of code. The process is more complex
    than this, and it requires more time spent understanding the data you have and
    to obtain the maximum of all your observations. Through the real-world cases that
    we will discuss in this book, we will observe that data collection, cleansing,
    and preparation are some of the most important and time-consuming steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore how to collect data for our problem statements:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting financial data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting the target variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structuring data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting financial data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will obtain our data from the **Federal Deposit Insurance Corporation** (**FDIC**)
    website ([https://www.fdic.gov/](https://www.fdic.gov/)). The FDIC is an independent
    agency led by the US Congress with an aim to maintain the confidence of the people
    and the stability of the financial system.
  prefs: []
  type: TYPE_NORMAL
- en: Why FDIC?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FDIC provides deposit insurance to depositors in US commercial banks and savings
    institutions. Thus, if a US bank fails and closes, the FDIC guarantees that the
    depositors do not lose their savings. Up to US$250,000 is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The FDIC also examines and supervises certain financial institutions. These
    institutions are obliged to periodically report detailed information about their
    financial statements related to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Capital level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solvency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality, type, liquidity, and diversification of assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loan and investment concentrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earnings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liquidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information on banks is publicly available on the FDIC website, and we can download
    it for our purpose. We will find that information is already structured in the
    so-called **Uniform Bank Performance Reports** (**UBPR**), which includes several
    ratios combining different accounts from the financial statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you want a UBPR for a particular bank, or if you just wish
    to view any other UBPR report, you can select Uniform Bank Performance Report
    (UBPR) at [https://cdr.ffiec.gov/public/ManageFacsimiles.aspx](https://cdr.ffiec.gov/public/ManageFacsimiles.aspx):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f168b9e0-cffe-46bc-aa23-9b5483195789.png)'
  prefs: []
  type: TYPE_IMG
- en: The Report drop-down menu allows the selection of the UBPR. We can search for
    an individual bank using the name or the FDIC certificate number among other alternatives.
    Additionally, information can be downloaded for all the available banks at the
    same time by visiting this link at [https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx](https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the following screenshot shows how it is possible to download
    the bulk data of financial ratios for 2016 in text format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4bcf192-e456-41a4-8c76-d5e182620f67.png)'
  prefs: []
  type: TYPE_IMG
- en: You should only select the UBPR Ratio -- Single Period option, then select the
    desired date (12/31/2016), and finally set the output format, for example, Tab
    Delimited.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we will need to download many files, one for each year from
    2002 to 2016\. It is not necessary to download the data if you don't want to.
    After the application of a relevant step in the code, the R workspace is saved
    and this backup is available for readers without spending time running the code
    or downloading information.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience of learning any programming language, spending time finding
    errors in code while the rest of your co-learners advance is very frustrating.
    For that reason, these workspaces allow the reader to never be frustrated by a
    problem with a specific line of code or even a concrete package that does not
    work properly on our computer.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, information is downloaded in text delimited files, making it easier
    to upload on to R later. For each year, one ZIP file is downloaded containing
    several text files. Each of these text files contains quarterly-relevant information
    on specific areas of a bank. The size of all the ZIP files for the year 2002 to
    2016 reaches 800MB.
  prefs: []
  type: TYPE_NORMAL
- en: Listing files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should create a folder for each year in our computer, where each ZIP file
    is decompressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once folders are created, we can write the following code in R to list all
    the folders that we have created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `pattern` option allows us to search for all the folders that contain `20`
    in their names, going through all the folders that we have created before.
  prefs: []
  type: TYPE_NORMAL
- en: Finding files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s read all the `.txt` files included in each folder in the `myfiles` list.
    Once `.txt` files are read for each year, they are merged together in only one
    table. This process takes several minutes to finish (in my case, almost 30 minutes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hence, it first lists all the folders we have created. Then, it lists all the
    `.txt` files in each folder and reads them into R. Individual `.txt` files provide
    different data frames, which are then merged into a single table. As a result
    of the code, 16 different tables are created, each of them with information for
    one specific year.
  prefs: []
  type: TYPE_NORMAL
- en: Combining results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now merge the yearly tables using the `rbind` function. This is possible
    because all the tables contain exactly the same number of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Removing tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the `rm()` command, we can remove all the tables in the workspace apart
    from `database`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Knowing your observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The database contains a total of `420404` observations and `1571` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what the dataset now looks like, or at least, the first few observations
    and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the first column is the identifier of each bank. In the second
    column, the reference date of the financial information is provided. The rest
    of the columns are codified with the `UBPR` prefix and a number. This situation
    is very common in real situations where a huge number of variables are available
    and their meanings are unknown. This situation can be very problematic because
    we do not exactly know if some variables are calculated considering the target
    variable, or if the variable will be available at the moment when the model will
    be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, this issue is not really a problem because you can find a dictionary
    with the meaning of the variables at [https://cdr.ffiec.gov/CDRDownload/CDR/UserGuide/v96/FFIEC%20UBPR%20Complete%20User%20Guide_2019-01-11.Zip](https://cdr.ffiec.gov/CDRDownload/CDR/UserGuide/v96/FFIEC%20UBPR%20Complete%20User%20Guide_2019-01-11.Zip).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the meaning of the first variable, `UBPR1795`, is net credit losses,
    measuring the total amount of loans that generated losses to a bank because they
    were not paid.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we joined the different text files into only one table per year, some columns
    were duplicated because they were included in several text files at the same time.
    For example, all the ratios included in text files named `Summary ratios` will
    be duplicated in the other text files. In those cases, R assigns a `.x` or `.y`
    suffix to the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we remove the variables with the suffix `.x` because
    they are duplicated in the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `grep` function searches for the `.x` pattern among the columns names.
    If this pattern is detected in a column, this column will be removed. Additionally,
    the `.y` suffix is removed from the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the import process also creates some erroneous and uninformed variables.
    The name of these columns starts with `X`. These variables are also removed, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s save the workspace until this following step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Operating our problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The database contains a column indicating the date of the financial statements
    for each bank (called the `Reporting Period` field). Each bank can appear several
    times in the dataset, once a quarter from December 2002 to December 2016.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this field is not recognized as a date format in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s transform this field into a date format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, extract the left part of the `Reporting Period` column. The first 10
    characters are extracted in a new variable called `Date`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s convert this new column into a date using the `as.Date` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, remove the `Reporting Period` field as it is no longer relevant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have information about all the quarters from 2002 to 2016, but we are only
    interested in the financial information provided on year-end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s filter the dataset to consider information from December of each year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After the preceding line of code, our database contains `110239` observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, it contains `1494` variables, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, let''s save a backup of the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now take a look at all the variables in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As the number of variables is substantially high, it is recommended to save
    the name of variables in an Excel file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are variables in the dataset whose names are a kind of
    code. And we also know that it is possible to obtain the meaning of each variable
    in the FDIC website. This situation is really normal, especially in credit risk
    applications, where information gives details about account movements or transactions.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand in some way the meaning of variables, or at least
    how they are generated. If not, we can include some variables very close to the
    target variable as predictors, or even include variables that will not be available
    at the moment of model implementation. However, we know that there is no apparent
    target in the dataset. So let's collect the target variable for our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the target variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to determine whether or not a bank has failed in the past – this will
    be our target. This information is also available on the FDIC website at [https://www.fdic.gov/bank/individual/failed/banklist.html](https://www.fdic.gov/bank/individual/failed/banklist.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The website includes banks that have failed since October 2000, which covers
    all our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e7f89fb-1b52-410f-a94a-279333b63e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see the steps to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download this information into a `.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Even this list is updated periodically, as historical information does not change,
    but the results are still replicable. Anyway, the file used in the development
    is also available in the data repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, upload the downloaded file into R, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to see all the variables and some details about the
    data contained in the list of failed banks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the first ten rows, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The file contains these relevant pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of failed banks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The states where these banks were located
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The date when they failed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The acquiring institution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It will be quite interesting to plot the evolution of failures over time. For
    that purpose, let''s check whether the `Closing.Date` column is recognized as
    a date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This column is not a date. Let''s convert it to a date by using another command
    similar to `as.Date` using the `lubridate` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Structuring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having acquired our target variable and knowing our dataset, we can now
    move on to the actual data collection based on our target. Here, we will try acquiring
    the data of the bank according to different years as described in the *Collecting
    the target variable* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we create a new variable extracting only the year when a bank went
    bankrupt, and then we count the number of banks by year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s view our data graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef3821cc-1c58-42ce-8217-4edcd35e9ae2.png)'
  prefs: []
  type: TYPE_IMG
- en: As the preceding graph shows, the number of failed banks increased during the
    dot-com crisis in 2001 and 2002, and then during the financial crisis that started
    in 2008.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to merge the list of failed banks with our database. In the failed
    banks dataset, there is a column that contains the ID of each bank, specifically
    the `Certificate number` column. This is a number assigned by the FDIC that uniquely
    identifies institutions and insurance certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in the other database, which contains financial information, the
    ID number is called RSSD ID, which is different. This number is a unique identifier
    assigned to institutions by the Federal Reserve System.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we join both datasets? We need a mapping between both identifiers.
    This mapping can also be found on the FDIC website, again in the same part where
    we previously downloaded the bulk data of all financial statements. Remember,
    the website can be accessed at [https://cdr.ffiec.gov/public/pws/downloadbulkdata.aspx](https://cdr.ffiec.gov/public/pws/downloadbulkdata.aspx).
  prefs: []
  type: TYPE_NORMAL
- en: 'On this website, we''ll need to download the Call Reports -- Single Period
    files during the relevant period (2002-2016):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56f8df48-eb9b-4635-bd71-a7dadadde700.png)'
  prefs: []
  type: TYPE_IMG
- en: In each of the recently downloaded files, we can find a file with the name `FFIEC
    CDR Call Bulk POR mmddyyyy.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: This file contains all the information about each bank. First, we use them to
    assign an `ID RSSD` number for each bank in the failed bank list. Then, we can
    join the financial ratios with the list of failed banks using the `ID RSSD` field.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have downloaded the files, list all the available files in your system
    using the `list.files` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to find all the files that contain `FFIEC CDR Call Bulk POR` in their
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will read all the files into R and merge them into one data frame called
    `IDs`.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a new column is created called `year`. This column reflects the
    year of the corresponding information. We need to store the `IDs` and the date
    because identifiers can change over the time. For example, changes could occur
    when two banks merge; one of the bank's will disappear in the dataset and the
    other can maintain the same number or get a new number.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a new empty frame called `IDs`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iteratively read all the text files and merge them together in this
    `IDs` data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the resulting table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, a master table containing the `ID RSSD` frame and the `Certificate number`
    column of each bank over the time is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can remove the irrelevant information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll merge the `failed banks` list and the `IDs` dataset using the
    certificate date, but first, we need to convert the certificate numbers into a
    numeric format in both datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If we try to merge the list of failed banks with the `IDs` dataset, we will
    find a problem. In the `failed banks` list we have a column indicating the year
    when a bank went bankrupt, which will not be found in the table `IDs` if we join
    both tables using the column year.
  prefs: []
  type: TYPE_NORMAL
- en: As the `IDs` snapshot corresponds to December of each year, a failed bank cannot
    already be found at the end of this specific year.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to merge both datasets correctly, create a new variable (`id_year`)
    in the `failed banks` dataset, subtracting one year from the `year` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The failed banks are now joined with the `IDs` information using the `merge`
    function. Using this function is easy; you only need to specify both tables and
    the name of the columns used to make the join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A new backup of the workspace is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is possible to merge the database containing financial statements with
    the list of failed banks and to then create the target variable. We will join
    both tables using the `ID RSSD` identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Two new columns are included in the database: `CERT` and `Closing.Date`. The
    preceding code alerted us to a previously non-detected duplicated column. So,
    we should remove one of the duplicated columns. Using the `grep` function, we
    will obtain the number of columns where the `UBPR4340` variable is present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the second column where the repeated variable appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'When a missing value is found in any of these two new variables (`CERT` and
    `Closing.Date`) it implies that this bank continues operating in the US financial
    system. On the other hand, if a bank contains information in these variables it
    implies that this bank failed. We can see how many failed observations are in
    our database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: There are `3.705` observations corresponding to failed banks in the dataset.
    As you can see, the number of failed observations makes up a small part of total
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Failed observations do not represent unique failed banks. It means that a failed
    bank has different financial statements some time before it finally goes to bankrupt.
    For example, for the bank mentioned in the following code block, different years
    of financial information are available. According to our database, this bank went
    bankrupt in 2010:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We should assess the time horizon of our predictive model. The higher the difference
    between the information date and the closing date, the lower the expected predictive
    power of our model. The explanation is quite simple; it is more difficult to forecast
    the failure of a bank from the current information in, for example, five years'
    time than in only one or two years' time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the difference between the closing and the financial statements
    dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: What will our target variable be? What do we want to predict? Well, we could
    develop a model to predict bankruptcies in the next six months, one year, or even
    five years after the current financial information.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the target variable should be done according to the purpose
    of the model, but also taking into account the number of failed banks or *bad*
    banks in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: The standard period is different depending on the portfolio, the model's purpose,
    and the sample of *bad* banks or the minority class, which should be large enough
    to develop a robust model.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the time horizon is very important, determining the objective
    of our model and its future use.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can classify as *bad* those banks that failed less than a year
    after the financial statements in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'According to this definition, the number of *bad* banks will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Only `476` banks in the dataset failed less than one year after the observed
    financial information.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following bank failed only half of a year after the observed
    financial information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, a new backup of the workspace is made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In this problem, we have seen that most of the banks are **solvent** and these
    banks are repeated in the sample several times, although with different financial
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is not relevant to keep all the good banks in the sample and increase
    the importance of *bad* banks. There are some techniques to deal with this problem.
  prefs: []
  type: TYPE_NORMAL
- en: One of them could be to assign different weights to each good and bad observation
    in such a way that two classes can be more balanced. This approach, although useful,
    makes the execution of the machine learning algorithms much more time consuming
    because we will be using the entire dataset, which in our case, is more than 100,00
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Very unbalanced classes, as we find in this problem, can impact model fitting
    in a negative way. In order to keep all the observations, it is very common to
    subsample the data. Three main techniques are usually carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Undersampling**: This is perhaps the simplest strategy. It consists of randomly
    reducing the majority class to the same size of the minority class. By undersampling,
    the imbalance issue is solved, but in general, we reduce the dataset, especially
    in cases where the minority class is very scarce. If this is the case, it is very
    likely that model results will be poor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oversampling**: The minority class is randomly selected many times to reach
    the same size of the majority class. The most common way to do this is to replicate
    the minority observations several times. Oversampling could be problematic at
    this point of the problem solution, where we have not already selected what data
    will be used for training or testing our future algorithms. We will be duplicating
    examples of the minority that could be found in the future in both training and
    validation sets, resulting in both overfitting and misleading results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other techniques**: Techniques such as **Synthetic Minority Over-sampling
    Technique** (**SMOTE**) and **Random Over-Sampling Examples** (**ROSE**) reduce
    the majority class and create artificially new observations in the minority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we'll follow a hybrid approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make some of the following steps easier, we''ll rename the first column,
    which contains the identifier of each bank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will treat failed and non-failed banks in a different way. Let''s start
    with the part of the database that contains only the failed banks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'There are `3705` observations containing information of failed banks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what this sample looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As it is displayed, in the list of failed banks we have financial information
    for several years for the same banks. The closer financial information to the
    bankruptcy date for each bank will be finally selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that, we create an auxiliary table. This table will contain the minimum
    distance of a bank observation to the failure date. For that purpose, we''ll now
    use a useful package, `sqldf`. This package allows us to write queries as if we
    are using the SQL language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our sample with failed banks and this auxiliary table are merged together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will only select observations where the differences between the financial
    statement date and the closing date are the same as the `min_diff` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The recently created columns are removed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now we want to reduce the number of non-failed banks. For that purpose, we select
    one year of financial statements for each bank at random.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total observations of non-failed banks is extracted with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To randomly select financial statements, we should follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, establish a `seed`. A seed is needed to obtain reproducible results
    when random numbers are generated. Using the same `seed` will allow you to obtain
    the same results as those described in this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate random numbers; we generate as many random numbers as the number of
    rows existing in the non-failed banks dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the random numbers to the database as a new column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum random number of each bank is calculated and a new data frame called
    `max` is created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Join the data frame with the non-failed banks to the `max` data frame. Then,
    only the observations where the random number is the same as the maximum value
    for each bank is selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the irrelevant columns as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `dim` function, we can obtain the number of observations of non-failed
    banks. You can see the number of *good* banks has been significantly reduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: There are only `9654` observations and `1496` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can finally build our dataset to develop our model by combining
    both previous data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The target variable can now be defined as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the objects loaded in the workspace can be removed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: It is usually possible to use current variables to define new features and then
    include them in the development. These new variables are usually named **derived
    variables**. Thus, we can define derived variables as new variables calculated
    from one or more base variables.
  prefs: []
  type: TYPE_NORMAL
- en: A very intuitive example is the calculation of a variable named `age` from a
    dataset with information about different customers. This variable could be calculated
    as the difference between the date when this customer is stored in the system
    and their birth date.
  prefs: []
  type: TYPE_NORMAL
- en: New variables should add useful and non-redundant information that will facilitate
    the subsequent learning and will help in generalizing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Feature generation should not be confused with feature extraction. Feature extraction
    is related to dimensionality reduction as it transforms original features and
    selects a subset from the pool of potential original and derived features that
    can be used in our model.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the problem we are dealing with, it is not very relevant to build
    additional variables. We have a very large dataset measuring all relevant aspects
    in the analysis of a financial institution.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in this part of the development, variables that have been included
    at the data extraction or manipulation phase, but which serve no purpose for the
    model development, must be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the following variables will be removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'All these steps were needed to build our database. You can see how much time
    we have spent collecting the data, the target variable, and trying to organize
    all the data in this chapter. In the next chapter, we will start with the analysis
    of our dataset. You can do one final backup before continuing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have started collecting the data needed to develop our model
    to predict bank failures. In this case, we have downloaded a large amount of data
    and we have structured it. Moreover, we have created our target variable. At the
    end of this chapter, you should have learned that data collection is the first
    and one of the most important steps in the model development. When you deal with
    your own problems, take time to understand the problem and then think about what
    kind of data you need and how to obtain it. In the next chapter, we will do a
    descriptive analysis of the data that we have acquired.
  prefs: []
  type: TYPE_NORMAL
