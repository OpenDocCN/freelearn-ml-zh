<html><head></head><body>
		<div id="_idContainer103" class="Content">
			<h1 id="_idParaDest-166"><em class="italics"><a id="_idTextAnchor167"/>Chapter 4:</em></h1>
		</div>
		<div id="_idContainer104" class="Content">
			<h1 id="_idParaDest-167"><a id="_idTextAnchor168"/>Regression</h1>
		</div>
		<div id="_idContainer105" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Formulate regression problems</li>
				<li class="bullets">Implement various types of regression approaches and its use cases</li>
				<li class="bullets">Analyze and choose the right regression approach</li>
				<li class="bullets">Connect statistics and machine learning through the lens of regression</li>
				<li class="bullets">Deep dive into model diagnostics</li>
			</ul>
			<p>In this chapter, we will focus on various type of regression and when to use which one along with demonstrations in R.</p>
		</div>
		<div id="_idContainer177" class="Content">
			<h2 id="_idParaDest-168"><a id="_idTextAnchor169"/>Introduction</h2>
			<p>In the previous chapter, we understood linear regression models and the linear relationship between an input variable (independent variable) and a target variable (dependent variable or explanatory variable). If one variable is used as an independent variable, it is defined as <strong class="bold">simple linear regression</strong>. If more than one explanatory (independent) variable is used, it's called <strong class="bold">multiple linear regression</strong>.</p>
			<p>Regression algorithms and problems are based on predicting a numeric target variable (often called <strong class="keyword">dependent</strong>), given all the input variables (often called <strong class="keyword">independent</strong> variables), for example, predicting a house price based on location, area, proximity to a shopping mall, and many other factors. Many of the concepts of regression are derived from statistics.</p>
			<p>The entire field of machine learning is now a right balance of mathematics, statistics, and computer science. In this chapter, we will use regression techniques to understand how to establish a relationship between input(s) and the target variable. We will also emphasize on model diagnostics as regression is full of assumptions, which needs to be checked before a model can be used in the real world.</p>
			<p><em class="italics">Essentially, all models are wrong, but some are useful. – George Box</em></p>
			<p>We have briefly touched upon simple and multiple linear regression in <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>. In this chapter, we will focus more on <strong class="bold">model diagnostics</strong> and other types of <strong class="bold">regression algorithm</strong>, and how it is different from linear regression.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/>Linear Regression</h2>
			<p>Let's revisit the multiple linear regression from <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>. The following equation is the mathematical representation of a linear equation, or linear predictor function, with <strong class="inline">p</strong> explanatory variables and <strong class="inline">n</strong> observations:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/C12624_04_22.jpg" alt=""/>
				</div>
			</div>
			<p>Where each <img src="image/C12624_04_23.png" alt="A picture containing furniture, table&#10;&#10;Description automatically generated"/> is a vector of column values (<strong class="bold">explanatory variable</strong>) and <img src="image/C12624_04_24.png" alt="A picture containing furniture&#10;&#10;Description automatically generated"/> is the <strong class="bold">unknown parameters</strong> or <strong class="bold">coefficients</strong>. <img src="image/C12624_04_25.png" alt="A picture containing furniture, seat&#10;&#10;Description automatically generated"/>, makes this equation suitable for simple linear regression. There are many algorithms to fit this function onto the data. The most popular one is <strong class="keyword">Ordinary Least Square</strong> (<strong class="keyword">OLS</strong>).</p>
			<p>Before understanding the details of OLS, first let's interpret the equation we got while trying to fit the Beijing PM2.5 data from the model building section of simple and multiple linear regression from <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>.</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/C12624_04_26.jpg" alt=""/>
				</div>
			</div>
			<p>If we substitute the values of regression coefficients, <img src="image/C12624_04_27.png" alt="A drawing of a face&#10;&#10;Description automatically generated"/> and <img src="image/C12624_04_28.png" alt=""/> from the output of the <strong class="inline">lm()</strong> function, we get:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/C12624_04_29.jpg" alt=""/>
				</div>
			</div>
			<p>The preceding equation attempts to answer the question "Are the factors <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong> important for predicting the <strong class="inline">pm2.5</strong> level?"</p>
			<p>The model estimates how, on average, the <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong> values affect the <strong class="inline">pm2.5</strong> level. For example, a unit increase in <strong class="inline">DEWP</strong> will increase the <strong class="inline">pm2.5</strong> value by <strong class="inline">4.384196</strong>. That is why we often call these coefficients <strong class="bold">weights</strong>. It is important to note that if the <strong class="bold">R-squared value</strong> is low, these estimated coefficients are not reliable.</p>
			<h3 id="_idParaDest-170"><a id="_idTextAnchor171"/>Exercise 50: Print the Coefficient and Residual Values Using the multiple_PM_25_linear_model Object</h3>
			<p>In this exercise, we will print the coefficient and residual values using the <strong class="inline">multiple_PM25_linear_model</strong> object.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Extract the attribute coefficients using the <strong class="inline">$</strong> operator on the <strong class="inline">multiple_PM25_linear_model</strong> object:<p class="snippet">multiple_PM25_linear_model$coefficients</p><p>The output is as follows:</p><p class="snippet">(Intercept)        DEWP        TEMP         Iws </p><p class="snippet">161.1512066   4.3841960  -5.1335111  -0.2743375</p></li>
				<li>Extract the attribute residuals using the <strong class="inline">$</strong> operator on the <strong class="inline">multiple_PM25_linear_model</strong> object:<p class="snippet">multiple_PM25_linear_model$residuals</p></li>
			</ol>
			<p>The output is as follows:</p>
			<p class="snippet">25            26            27            28 </p>
			<p class="snippet">  17.95294914   32.81291348   21.38677872   26.34105878 </p>
			<p class="snippet">           29            30            31            32</p>
			<h3 id="_idParaDest-171"><a id="_idTextAnchor172"/>Activity 7: Printing Various Attributes Using Model Object Without Using the Summary Function</h3>
			<p>In the <em class="italics">Multiple Linear Regression Model</em> section of <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, we created a multiple linear regression model and stored it in the model object <strong class="inline">multiple_PM25_linear_model</strong> using the model object.</p>
			<p>This activity will help in understanding how to extract some important model attributes once the model is built. In few cases, we will use the <strong class="inline">$</strong> operator, and in other cases, we will perform some simple calculation. Print the following model attributes using the <strong class="inline">multiple_PM25_linear_model</strong> object:</p>
			<ul>
				<li>Residuals</li>
				<li>Fitted values</li>
				<li>R-Ssquared value</li>
				<li>F-statistic</li>
				<li>Coefficient p-value</li>
			</ul>
			<p>Let's print these values using the model object:</p>
			<ol>
				<li value="1">First, print the coefficient values. Make sure the output is like the output of the <strong class="inline">summary</strong> function using the <strong class="inline">coefficients</strong> option. The coefficients are fitted values from the model that uses the OLS algorithm:<p class="snippet">(Intercept)        DEWP        TEMP         Iws </p><p class="snippet">161.1512066   4.3841960  -5.1335111  -0.2743375</p></li>
				<li>Find the residual value (difference) of the predicted and actual values of PM2.5, which should be as small as possible. Residual reflects how far the fitted values using the coefficients are from the actual value:<p class="snippet">25            26            27            28 </p><p class="snippet">  17.95294914   32.81291348   21.38677872   26.34105878 </p><p class="snippet">           29            30            31            32 </p></li>
				<li>Next, find the fitted values, which should be closer to the actual PM2.5 values for best model. Using the coefficients, we can compute the fitted values:<p class="snippet">25         26         27         28         29 </p><p class="snippet">111.047051 115.187087 137.613221 154.658941 154.414781 </p><p class="snippet">        30         31         32         33         34 </p></li>
				<li>Find the R-Squared values. They should look the same as the ones you obtained in the output of the <strong class="inline">summary</strong> function next to the multiple R-squared text. R-square helps in evaluating the model performance. If the value is closer to 1, the better the model is:<p class="snippet">summary(multiple_PM25_linear_model)$r.squared</p><p>The output is as follows:</p><p class="snippet">[1] 0.2159579</p></li>
				<li>Find the F statistic values. Make sure the output looks the same as the one you obtained in the output of the <strong class="inline">summary</strong> function next to the text F statistics. This will tell you if your model fits better than just using the mean of the target variables. In many practical applications, F-Statistic is used along with p-values:<p class="snippet">   value     numdf     dendf </p><p class="snippet">3833.506     3.000 41753.000 </p></li>
				<li>Finally, find the coefficient p-values and make sure the values look the same as the one you obtained in the output of the <strong class="inline">summary</strong> function under <em class="italics">coefficients</em> for each variable. It will be present under the column titled <strong class="inline">Pr(&gt;|t|):</strong>. If the value is less than 0.05, the variable is statistically significant in predicting the target variable:<p class="snippet">  (Intercept)          DEWP          TEMP           Iws </p><p class="snippet"> 0.000000e+00  0.000000e+00  0.000000e+00 4.279601e-224</p><h4>Note</h4><p class="callout">The solution for this activity can be found on page 449.</p></li>
			</ol>
			<p><strong class="bold">Ordinary Least Square</strong> (<strong class="bold">OLS</strong>)</p>
			<p>In <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, we saw sum of squared residuals <img src="image/C12624_04_30.png" alt=""/> (also called the <strong class="bold">error sum of square</strong> or <strong class="bold">residual sum of squares</strong>), which is a measure of the overall model fit, is given by the following equation:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/C12624_04_31.jpg" alt=""/>
				</div>
			</div>
			<p>Where <em class="italics">T</em> represents the matrix transpose, and the rows of <em class="italics">X</em> represent the values of all the input variables related to a specific value of the target variable are <img src="image/C12624_04_32.png" alt=""/>. The value of <img src="image/C12624_04_33.png" alt="A picture containing furniture&#10;&#10;Description automatically generated"/> that minimizes <img src="image/C12624_04_34.png" alt=""/>is called the <strong class="bold">OLS estimator</strong> for β. The OLS algorithm is designed to find the global minimum of <img src="image/C12624_04_35.png" alt="A picture containing furniture&#10;&#10;Description automatically generated"/> that will minimize <img src="image/C12624_04_36.png" alt=""/>.</p>
			<p>From the previous chapter, you also learned that the R-squared value for <strong class="inline">multiple_PM25_linear_model</strong> on the Beijing PM2.5 dataset is quite low for this model to be useful in practical applications. One way of interpreting the poor results is to say the predictor variables <strong class="inline">DEWP</strong> and <strong class="inline">TEMP</strong> do not fully explain the variance in PM2.5, so they fall short of producing good results.</p>
			<p>Before we could jump into the diagnostics of this model, let's see if we could explain some of the variances in PM2.5 using the variable <strong class="inline">month</strong> (of the readings). We will also use an interaction variable (more on this in the <em class="italics">Improving the Model</em> section) <em class="italics">DEWP*TEMP*month</em> in the <strong class="inline">lm()</strong> function that generates all possible combination of <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">month</strong>.</p>
			<p>The reason for using <strong class="inline">month</strong> is justified by <em class="italics">Figure 3.3</em> in <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, where we saw the seasonal effect in the values of <strong class="inline">TEMP</strong>, <strong class="inline">DEWP</strong>, and <strong class="inline">PRES</strong> (showing a nice sinusoidal pattern). The output of the following exercise shows all the interaction terms that got created to explain the PM2.5 dataset.</p>
			<h4>Note</h4>
			<p class="callout">The expression like <strong class="inline">DEWP:TEMP</strong> means multiplication and each value of <strong class="inline">month</strong> is a separate variable in <strong class="inline">multiple_PM25_linear_model,</strong> because we converted <strong class="inline">month</strong> into <strong class="inline">factor</strong> before running the model.</p>
			<h3 id="_idParaDest-172"><a id="_idTextAnchor173"/>Exercise 51: Add the Interaction Term DEWP:TEMP:month in the lm() Function</h3>
			<p>In this exercise, we will add the interaction term to improve the model performance.</p>
			<p>We will see how adding an additional interaction term helps in improving the model performance in terms of R-squared values. Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Read the Beijing PM2.5 dataset using the following command:<p class="snippet">PM25 &lt;- read.csv("PRSA_data_2010.1.1-2014.12.31.csv")</p></li>
				<li>Now, convert the <strong class="inline">month</strong> object into the <strong class="inline">factor</strong> variable as shown here:<p class="snippet">PM25$month &lt;- as.factor(PM25$month)</p></li>
				<li>Use the linear model with interaction terms of <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">month</strong>. Observe the term <strong class="inline">DEWP*TEMP*month</strong>, which will generate all the combinations of the variable <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">month</strong>:<p class="snippet">multiple_PM25_linear_model &lt;- lm(pm2.5 ~ Iws + DEWP*TEMP*month, data = PM25)</p></li>
				<li>Print the summary of the model to see the changes in coefficients and r-squared values because of the interaction term:<p class="snippet">summary(multiple_PM25_linear_model)</p><p>The output is as follows:</p><p class="snippet">## Call:</p><p class="snippet">## lm(formula = pm2.5 ~ Iws + DEWP * TEMP * month, data = PM25)</p><p class="snippet">## </p><p class="snippet">## Residuals:</p><p class="snippet">##     Min      1Q  Median      3Q     Max </p><p class="snippet">## -298.41  -42.77   -9.35   30.91  967.39 </p><p class="snippet">## </p><p class="snippet">## Coefficients:</p><p class="snippet">##                     Estimate Std. Error t value Pr(&gt;|t|)</p><p class="snippet">...</p><p class="snippet">## (Intercept)        2.917e+02  4.338e+00  67.257  &lt; 2e-16 ***</p><p class="snippet">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</p><p class="snippet">## </p><p class="snippet">## Residual standard error: 70.04 on 41708 degrees of freedom</p><p class="snippet">##   (2067 observations deleted due to missingness)</p><p class="snippet">## Multiple R-squared:  0.4217, Adjusted R-squared:  0.4211 </p><p class="snippet">## F-statistic: 633.7 on 48 and 41708 DF,  p-value: &lt; 2.2e-16</p></li>
			</ol>
			<p>Notice the two-fold jump in the R-squared value from 0.216 to 0.4217. However, such a jump is at the cost of <strong class="bold">model interpretability</strong>. Though it is simple to explain the explanatory power of the model using individual variables, their multiplication creates an effect that is difficult to articulate.</p>
			<p>In our example of Beijing PM2.5, it is more logical to think of the interaction <strong class="inline">DEWP</strong> and <strong class="inline">TEMP</strong> have with the <strong class="inline">month</strong> object of the <strong class="inline">year</strong> object, since both of these are environmental factors which vary with season.</p>
			<p>However, we would also like to perform some diagnostics to fully understand how a linear regression model is studied end to end and not just look at the R-squared value.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor174"/>Model Diagnostics</h2>
			<p>Often statistical models such as linear regression and logistic regressions come with many assumptions that need to be validated before accepting the final solution. A model violating the assumptions will result in erroneous prediction and results will be prone to misinterpretation.</p>
			<p>The following code shows a method for obtaining the diagnostic plots from the output of the <strong class="inline">lm()</strong> method. The plot has four different plots looking at the residuals. Let's understand how to interpret each plot. All these plots are about how well the fit matches the regression assumptions. If there is a violation, it will be clearly shown in the plots of the following code:</p>
			<p class="snippet">par(mfrow = c(2,2))</p>
			<p class="snippet">plot(multiple_PM25_linear_model)</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/C12624_04_01.jpg" alt="Figure 4.1: Diagnostics plot for the linear model fit on the Beijing PM2.5 dataset&#13; &#10;"/>
				</div>
			</div>
			<h6>Figure 4.1: Diagnostics plot for the linear model fit on the Beijing PM2.5 dataset</h6>
			<p>In the next four sections, we will explore each of the plots with randomly generated data from a linear equation <img src="image/C12624_04_37.png" alt=""/>and a quadratic equation <img src="image/C12624_04_38.png" alt=""/>, and later come back to explain how the four plots in <em class="italics">Figure 4.1</em> fare in comparison with the ideal scenarios.</p>
			<h4>Note</h4>
			<p class="callout">In the quadratic equation, <img src="image/C12624_04_39.png" alt=""/> <img src="image/C12624_04_40.png" alt=""/> is assumed to be normally distributed with mean 0 and variance 2.</p>
			<p>In the following exercise, we will generate the plots using the linear and quadratic equations. Later, we will deep dive into understanding the various assumptions a linear model should follow using the model fit on the random data generated through the two equations.</p>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor175"/>Exercise 52: Generating and Fitting Models Using the Linear and Quadratic Equations</h3>
			<p>In this exercise, we will understand the linear and polynomial function and what happens when we fit a linear model on both.</p>
			<p>Generate random numbers using a linear and a polynomial equation, and fit a linear model on both. Observe the difference between the two plots.</p>
			<p>Perform the following steps to generate the required plots:</p>
			<ol>
				<li value="1">First, define the linear function using the following code:<p class="snippet">linear_function &lt;- function(x){return (5+(12*x)-(3*x))}</p></li>
				<li>Define the quadratic function as shown in the following command:<p class="snippet">quadratic_function &lt;- function(x){return (5+(12*x)-(3*(x^2)))}</p></li>
				<li>Now, generate the uniform random numbers (<strong class="inline">x</strong>), as shown here:<p class="snippet">uniform_random_x &lt;- runif(50, min=0, max=15)</p></li>
				<li>Generate the linear values (<strong class="inline">y</strong>) using (<strong class="inline">x</strong>), as shown here:<p class="snippet">linear_values_y &lt;- linear_function(uniform_random_x) + rnorm(50,mean = 0, sd =sqrt(2))</p></li>
				<li>Generate the quadratic values (<strong class="inline">y</strong>) using (<strong class="inline">x</strong>):<p class="snippet">quadratic_values_y &lt;- quadractic_function(uniform_random_x) + rnorm(50,mean = 0, sd =sqrt(2))</p><p class="snippet">df &lt;- data.frame(linear_values_y, quadratic_values_y, uniform_random_x)</p></li>
				<li>Fit a linear model for <strong class="inline">linear_values_y</strong> using <strong class="inline">uniform_random_x</strong>:<p class="snippet">model_df_linear &lt;- lm(linear_values_y ~ uniform_random_x, data = df)</p></li>
				<li>Plot the diagnostic plot fora linear relationship:<p class="snippet">par(mfrow = c(2,2))</p><p class="snippet">plot(model_df_linear)</p><p>The plot is as follows:</p><div id="_idContainer126" class="IMG---Figure"><img src="image/C12624_04_02.jpg" alt="Figure 4.2: Plots using the linear regression&#13;&#10;"/></div><h6>Figure 4.2: Plots using the linear regression</h6></li>
				<li>Fit a linear model for <strong class="inline">quadratic_values_y</strong> using <strong class="inline">uniform_random_x</strong>:<p class="snippet">model_df_quad &lt;- lm(quadratic_values_y ~ uniform_random_x, data = df)</p></li>
				<li>Generate a diagnostic for non-linear relationships:<p class="snippet">par(mfrow = c(2,2))</p><p class="snippet">plot(model_df_quad)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/C12624_04_03.jpg" alt="Figure 4.3: Plots using the quadratic regression&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.3: Plots using the quadratic regression</h6>
			<p>The difference between the plot in step 7 and 9 show the good and a poor fit of a linear relationship. A linear model can't fit a non-linear relationship between <em class="italics">y</em> and <em class="italics">x</em>. In the next sections, we will deep dive into understanding the four parts of the plots generated in step 7 and 9.</p>
			<p>In <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, <em class="italics">Figure 3.5</em> we discussed the various assumptions to consider while building a linear regression model. Through the four plots mentioned earlier in the chapter, we will examine if any of the assumptions are violated or not.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor176"/>Residual versus Fitted Plot</h2>
			<p>This type of plot is between the fitted values and the residual (difference between <strong class="bold">actual</strong> and <strong class="bold">fitted</strong> values) from the <strong class="inline">lm()</strong> method. If the predictor and target variables have a non-linear relationship, the plot will help us identify.</p>
			<p>In the following figure, the top plot shows the point scattered all around and the linear relationship between the predictor and target variable is clearly captured. In the bottom plot, the unexplained non-linear relationship is left out in the residuals, and hence the curve. The bottom plot clearly shows it is not the right fit for a linear regression model, a violation of the linear relationship between the predictor and target variable:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/C12624_04_04.jpg" alt="Figure 4.4: [Top] Residual versus fitted plot of the linear function. [Bottom] Residual versus fitted plot of the quadratic function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.4: [Top] Residual versus fitted plot of the linear function. [Bottom] Residual versus fitted plot of the quadratic function</h6>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor177"/>Normal Q-Q Plot</h2>
			<p><strong class="bold">Q-Q plot</strong>, also called <strong class="bold">Quantile-Quantile plot</strong>, supports to check if the data plausibly comes from approximately theoretical distribution; in this instance, <strong class="keyword">Normal Distribution</strong>. A Q-Q plot is a scatterplot shaped by plotting two sets of quantiles (points below which a certain proportion of the data falls) in contrast to one another. If both groups of quantiles came from a similar distribution, we must see the points creating a coarsely straight line. Provided a vector of data, the normal Q-Q plot plots the data in sorted order versus quantiles from a standard normal distribution.</p>
			<p>The second assumption in linear regression was that all the predictor variables are normally distributed. If it is true, the residuals will also be normally distributed. Normal Q-Q is a plot between standardized residuals and theoretical quantiles. Visually, we can inspect whether the residuals follow the straight line, if it is normally distributed, or if there is any deviation that indicates violation.</p>
			<p>In the following figure, the top part demonstrates the linear function, which shows an alignment with the straight diagonal line, with a few exceptions like observation number 39, 30, and 50. On the other hand, the bottom part of the figure shows the quadratic function, which surprisingly shows a fair alignment with the straight line, not exactly like the linear, as some divergence is seen in the top-right side of the plot:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/C12624_04_05.jpg" alt="Figure 4.5: [Top] Normal Q-Q plot of the linear function. [Bottom] Normal Q-Q plot of the quadratic function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.5: [Top] Normal Q-Q plot of the linear function. [Bottom] Normal Q-Q plot of the quadratic function</h6>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>Scale-Location Plot</h2>
			<p><strong class="bold">Scale-Location plot</strong> shows whether residuals are spread equally along the ranges of input variables (predictor). The assumption of equal variance (<strong class="keyword">homoscedasticity</strong>) could also be checked with this plot. If we see a horizontal line with randomly spread points, it means that the model is good.</p>
			<p>The plot is between fitted values and the square root of standardized residuals. In the following figure, the top plot shows the linear function, and the residuals are spread randomly along the horizontal lines, whereas in the bottom plot, there seems to be a pattern that is not random. Hence, the variance is not equal:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/C12624_04_06.jpg" alt="Figure 4.6: [Top] Scale-Location plot of the linear function. [Bottom] Scale-Location plot of the quadratic function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.6: [Top] Scale-Location plot of the linear function. [Bottom] Scale-Location plot of the quadratic function</h6>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/>Residual versus Leverage</h2>
			<p>If there are any influential points in the data, the <strong class="bold">Residual versus Leverage</strong> plot helps in identifying it. It's common to think that all outlier points are influential, that is, it decides how the regression line comes out. However, not all outliers are influential points. Even if a point is within a reasonable range of values (not an outlier), it could still be an influential point.</p>
			<p>In the next plot, we will look out for far off values at the top-right corner or at the bottom-right corner. Those regions are the spaces where observation can be <em class="italics">influential</em> in contrast to a regression line. In <em class="italics">Figure 4.7</em>, the observations of the red dashed line with high <strong class="bold">Cook's distance</strong> are influential for the regression results. The regression results will be changed if we remove those observations. In the following figure, the bottom plot shows that observation <strong class="inline">40</strong> and <strong class="inline">39</strong> outside of the dashed line (high Cook's distance). Note that these observations are consistently appearing in the other three plots as well, giving us a strong reason to eliminate these points, if we would like to see the linear relationship in the data. The plot on the top seems to have no red dashed line, ascertaining a good fit:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/C12624_04_07.jpg" alt="Figure 4.7: [Top] Residual versus Leverage plot of the linear function. [Bottom] Residual versus Leverage plot of the quadratic function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.7: [Top] Residual versus Leverage plot of the linear function. [Bottom] Residual versus Leverage plot of the quadratic function</h6>
			<p>Now, if we revisit <em class="italics">Figure 4.1</em>, the diagnostics plot we obtained from the Beijing PM2.5 dataset; it seems like the model fit is <em class="italics">not the best</em> to be used for practical purposes. All the four plots show slight violation of linearity, normality and homoscedasticity assumptions.</p>
			<p>In the next section, we have listed a few ways to improve the model, which may incrementally help increase the R-squared value and better fit the data. Also, similar to the visual inspection methods we just discussed, many statistical methods such as <strong class="bold">Kolmogorov-Smirnov test</strong> for testing normality, <strong class="bold">Correlation</strong> for testing multicollinearity, <strong class="bold">Goldfeld–Quandt test</strong> for testing homoscedasticity could be used.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor180"/>Improving the Model</h2>
			<p>So far, we have seen the problems in the data, but you may ask whether you can fix or improve it. Let's discuss some ways to do that. In this section, you will learn some of the ways, such as variable transformation, dealing with outlier points, adding interaction effect and deciding to go with a non-linear model.</p>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor181"/>Transform the Predictor or Target Variable</h3>
			<p>The most common way to improve the model is to transform one or more variables (could also be the target variable) using a <strong class="inline">log</strong> function.</p>
			<p>Log transformation corrects the skewed distribution. It gives the ability to handle the skewness in the data and at the same time the original value could be easily computed once the model is built. The most popular log transformation is natural <em class="italics">log</em>. A more detailed explanation for log transformation could be found in the section <em class="italics">Log Transformation</em> of <em class="italics">Chapter 6</em>, <em class="italics">Feature Selection and Dimensionality Reduction</em>.</p>
			<p>The objective is to bring the normal distribution in the data by transforming. So, whichever function helps in attaining that is a good transformation. After log, square root is also widely used. Look at the distribution of the transformed variable to see if a symmetrical distribution (bell shaped) is obtained; if yes, then the transformation is going to be useful.</p>
			<h3 id="_idParaDest-181"><a id="_idTextAnchor182"/>Choose a Non-Linear Model</h3>
			<p>It might be possible to get struck in a scenario where a linear model is not a right fit because there is a non-linear relationship between the predictor and the target variable and only a non-linear function could fit such data. See the section <em class="italics">Polynomial Regression</em> later in this chapter for more details on such a model.</p>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor183"/>Remove an Outlier or Influential Point</h3>
			<p>As we discussed in the <em class="italics">Residual versus Leverage</em> section's diagnostic plot, we may find an outlier or influential point playing a spoil spot in getting us the best model. If you have identified it properly, try seeing by removing the observation and see if things improve.</p>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor184"/>Adding the Interaction Effect</h3>
			<p>We might at times see the value(s) of two or more predictor (independent) variables in the dataset influencing the dependent variable in a multiplicative way. A linear regression equation with an interaction term might look like this:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/C12624_04_41.jpg" alt=""/>
				</div>
			</div>
			<p>One can go for the higher order of such an interaction (for example, using three variables); however, those are difficult to interpret and usually avoided.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor185"/>Quantile Regression</h2>
			<p>When the data presents outliers, high skewness, and conditions leading to heteroscedasticity, we employ quantile regression for modelling. Also, one key question quantile regression answers, which linear regression cannot, is "Does <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong> influence PM2.5 levels differently for high PM2.5 than for average PM2.5?"</p>
			<p>Quantile regression is quite similar to linear regression; however, the quantile regression parameter estimates the change in a certain quantile of the <strong class="inline">response</strong> variable produced by a unit change in the input <strong class="inline">predictor</strong> variable. In order to fully understand this statement, let's fit our Beijing data using quantile regression (without using the interaction terms).</p>
			<p>We need to install the <strong class="inline">quantreg</strong> package to fit the quantile regression into the data. The package offers the method, <strong class="inline">rq()</strong> to fit the data using the argument <strong class="inline">tau</strong>, which is the model parameter specifying the value of quantile to be used for fitting the model into the data. Observe that the other parts of the arguments to the <strong class="inline">rq()</strong> method looks similar to <strong class="inline">lm()</strong>.</p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor186"/>Exercise 53: Fit a Quantile Regression on the Beijing PM2.5 Dataset</h3>
			<p>In this exercise, we will observe the difference in the quantile regression fit at various quantiles, particularly 25th, 50th, and 75th. The <strong class="inline">rq()</strong> function from <strong class="inline">quantreg</strong> will be used for building the model. In <em class="italics">Figure 4.8</em>, we will compare the coefficient values obtained through the <strong class="inline">lm()</strong> function versus the <strong class="inline">rq()</strong> function to compare the two types of regression.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Read the Beijing PM2.5 dataset using the following command:<p class="snippet">PM25 &lt;- read.csv("PRSA_data_2010.1.1-2014.12.31.csv")</p></li>
				<li>Now, the next step is to install the required package. Use the following command to load the <strong class="inline">quantreg</strong> package:<p class="snippet">library(quantreg)</p></li>
				<li>Run the quantile regression tau values as 0.25, 0.5, and 0.75, which corresponds to the 25th, 50th, and 75th quantiles, respectively:<p class="snippet">quantile_regression_PM25_all &lt;- rq(pm2.5 ~ DEWP+TEMP+Iws, data = PM25, tau = seq(0.25,0.99,by = 0.25))</p></li>
				<li>Print the summary of the quantile regression model:<p class="snippet">summary(quantile_regression_PM25_all)</p><p>The output is as follows:</p><p class="snippet">## tau: [1] 0.25</p><p class="snippet">## </p><p class="snippet">## Coefficients:</p><p class="snippet">##             Value     Std. Error t value   Pr(&gt;|t|) </p><p class="snippet">## (Intercept)  63.62367   0.52894  120.28453   0.00000</p><p class="snippet">## DEWP          2.08932   0.01859  112.39914   0.00000</p><p class="snippet">## TEMP         -1.89485   0.02196  -86.27611   0.00000</p><p class="snippet">## Iws          -0.09590   0.00179  -53.59211   0.00000</p><p class="snippet">## </p><p class="snippet">## tau: [1] 0.5</p><p class="snippet">## </p><p class="snippet">## Coefficients:</p><p class="snippet">##             Value      Std. Error t value    Pr(&gt;|t|)  </p><p class="snippet">## (Intercept)  117.37344    0.73885  158.85921    0.00000</p><p class="snippet">## DEWP           3.43276    0.02835  121.07849    0.00000</p><p class="snippet">## TEMP          -3.37448    0.03225 -104.65011    0.00000</p><p class="snippet">## Iws           -0.16659    0.00202  -82.56604    0.00000</p><p class="snippet">## </p><p class="snippet">## tau: [1] 0.75</p><p class="snippet">## </p><p class="snippet">## Coefficients:</p><p class="snippet">##             Value      Std. Error t value    Pr(&gt;|t|)  </p><p class="snippet">## (Intercept)  201.16377    1.31859  152.55927    0.00000</p><p class="snippet">## DEWP           5.12661    0.04901  104.59430    0.00000</p><p class="snippet">## TEMP          -5.62333    0.05567 -101.01841    0.00000</p><p class="snippet">## Iws           -0.25807    0.00510  -50.55327    0.00000</p></li>
			</ol>
			<p>The following table summarizes the coefficient values of the linear regression that we obtained using <strong class="inline">lm()</strong> in the <em class="italics">Regression</em> section of <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em> and the values we obtained using <strong class="inline">rq()</strong> in three quantiles.</p>
			<p>According to the linear regression model, the mean PM2.5 level in the atmosphere increases by <strong class="inline">4.384196</strong> with one unit increase in <strong class="inline">DEWP</strong>. The quantile regression results in the following table, and it indicates that <strong class="inline">DEWP</strong> has a larger negative impact on the higher quantiles (observe the 75th quantile) of PM2.5:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/C12624_04_08.jpg" alt="Figure 4.8: Coefficient estimates for the 25th, 50th, 75th quantile regression and the linear regression coefficient estimates for the Beijing's PM2.5 estimation model&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.8: Coefficient estimates for the 25th, 50th, 75th quantile regression and the linear regression coefficient estimates for the Beijing's PM2.5 estimation model</h6>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor187"/>Exercise 54: Plotting Various Quantiles with More Granularity</h3>
			<p>In this exercise, instead of using the 25th, 50th and 75th quantiles, we will use the more granular values for tau in the <strong class="inline">rq</strong> function. The plot will help visualize the change in the coefficient values based on the quantile value. Use the <strong class="inline">seq()</strong> function from R that sets the quantile values starting from 0.05 to 0.95 with an increment of 0.05.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Create a <strong class="inline">quantile_regression_PM25_granular</strong> variable:<p class="snippet">quantile_regression_PM25_granular &lt;- rq(pm2.5 ~ DEWP + TEMP + Iws, data = PM25, tau = seq(0.05,0.95,by = 0.05))</p></li>
				<li>Now, store the value from the previously created variable using the <strong class="inline">summary</strong> function:<p class="snippet">plot_granular &lt;- summary(quantile_regression_PM25_granular)</p></li>
				<li>Let's use the following command to plot the graph. Observe for the different values of tau, how the values of <strong class="inline">Intercept</strong>, <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong> change:<p class="snippet">plot(plot_granular)</p><p>The output plot is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/C12624_04_09.jpg" alt="Figure 4.9: Shows the various values of coefficients of DEWP, TEMP, and Iws for various values of quantiles&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.9: Shows the various values of coefficients of DEWP, TEMP, and Iws for various values of quantiles</h6>
			<p>In this exercise, we explored the granularity of the variable using more granular values for tau in the <strong class="inline">rq</strong> function. The previous figure shows the various values of the coefficients of <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong>. The X-axis in the plot shows the quantile. The single dotted line shows the estimation of the quantile regression, and the gray area is the confidence interval. The middle gray line is a representation of the OLS coefficient estimates, and the double dotted lines display the confidence intervals around the OLS coefficients. Observe that the red and the gray areas do not overlap, which justifies our use of quantile regression. If the two lines overlap, then there is no difference in the estimates using OLS and quantile regression.</p>
			<h4>Note</h4>
			<p class="callout">We are not claiming that quantile regression is giving better results than linear regression. The adjusted R-squared value is still low for this model and it works well in the real world. However, we claim that quantile regression can help in estimating the PM2.5 at different levels than just the average, which provides a robust interpretation for data with outliers, high skewness, and heteroscedasticity.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/>Polynomial Regression</h2>
			<p>Often in real-world data, the response variable and the predictor variable don't have a linear relationship, and we may need a <strong class="bold">nonlinear polynomial function</strong> to fit the data. Various scatterplot-like residual versus each predictor and residual versus fitted values reveal the violation of linearity if any, which could potentially help in identifying the need for introducing the quadratic or cubic term in the equation. The following function is a generic polynomial equation:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/C12624_04_42.jpg" alt=""/>
				</div>
			</div>
			<p>Where <em class="italics">k</em> is the degree of the polynomial. For <em class="italics">k=2</em>, <em class="italics">f(X)</em> is called <strong class="bold">quadratic</strong> and <em class="italics">h=4</em> is called <strong class="bold">cubic</strong>. Note that polynomial regression is still considered linear regression since it is still linear in coefficient <img src="image/C12624_04_43.png" alt="A drawing of a face&#10;&#10;Description automatically generated"/>.</p>
			<p>Before revisiting the Beijing PM2.5 example, let's understand how polynomial regression works using simulated data from the quadratic equation we introduced in the <em class="italics">Linear Regression</em> section.</p>
			<h3 id="_idParaDest-188"><a id="_idTextAnchor189"/>Exercise 55: Performing Uniform Distribution Using the runif() Function</h3>
			<p>In this exercise, we will generate 50 random numbers from a uniform distribution using the function <strong class="inline">runif()</strong> in R and store the results in <strong class="inline">uniform_random_x</strong>. We have defined a function to generate values using the previous quadratic equation. Note that we will separately add <img src="image/C12624_04_40b.png" alt="A close up of a logo&#10;&#10;Description automatically generated"/> to the values returned by the function; <img src="image/C12624_04_40a.png" alt="A close up of a logo&#10;&#10;Description automatically generated"/> is generated from the normal distribution using the <strong class="inline">rnorm()</strong> function in R. The final value will then be stored in <strong class="inline">quadratic_values_y</strong>:</p>
			<p>Perform these steps to perform t<a id="_idTextAnchor190"/>he uniform distribution using the <strong class="inline">runif()</strong> function:</p>
			<ol>
				<li value="1">First, define the quadratic equation as illustrated in the following command:<p class="snippet">quadratic_function &lt;- function(x){return (5+(12*x)-(3*(x^2)))}</p></li>
				<li>Now, generate the uniform random number for <strong class="inline">x</strong>:<p class="snippet">uniform_random_x &lt;- runif(50, min=0, max=15)</p></li>
				<li>Add the error term to the quadratic equation, which is normally distributed with mean <strong class="inline">0</strong> and variance <strong class="inline">2</strong> (<em class="italics">standard deviation(sd) = square root of variance</em>):<p class="snippet">quadratic_values_y &lt;- quadratic_function(uniform_random_x) + rnorm(50,mean = 0, sd =sqrt(2))</p></li>
				<li>To store the data in data frame, let's use the following command:<p class="snippet">df &lt;- data.frame(quadratic_values_y,uniform_random_x)</p></li>
				<li>Now, plot the relationship between <strong class="inline">x</strong> and <strong class="inline">y</strong> based on the quadratic equation:<p class="snippet">library(ggplot2)</p><p class="snippet">ggplot(df, aes(x=uniform_random_x,y=quadratic_values_y))+</p><p class="snippet">  geom_point()</p><p>The output is as follows:</p><div id="_idContainer139" class="IMG---Figure"><img src="image/C12624_04_10.jpg" alt="Figure 4.10: Performing uniform distribution using the function runif()&#13;&#10;"/></div><h6>Figure 4.10: Performing uniform distribution using the function runif()</h6><p>The following figure clearly shows the relationship between <strong class="inline">uniform_random_x</strong> and <strong class="inline">quadratic_values_y</strong> is not linear as expected. Now, if we try to fit a linear model, we expect to see some trouble in the diagnostics plot.</p><p>Residuals versus fitted value plots in <em class="italics">Figure 4.12</em> display a curvature and they do not demonstrate uniform randomness as we have seen before. Also, <strong class="keyword">Normal Probability Plot</strong> (<strong class="keyword">NPP</strong>) seems to diverge from a straight line and curves down at the far away percentiles. These plots suggest that there is something incorrect with the model being used and indicate that a higher-order model may be needed.</p><div id="_idContainer140" class="IMG---Figure"><img src="image/C12624_04_11.jpg" alt="Figure 4.11: The plot shows the non-linear relationship between uniformly generated random number(x) and the value of x in the quadratic equation&#13;&#10;"/></div><h6>Figure 4.11: The plot shows the non-linear relationship between uniformly generated random number(x) and the value of x in the quadratic equation</h6></li>
				<li>Now, fit a linear regression model to the polynomial (quadratic) equation and display the diagnostic plot:<p class="snippet">par(mfrow = c(2,2))</p><p class="snippet">plot(lm(quadratic_values_y~uniform_random_x,data=df))</p><p>The output is as follows: </p><div id="_idContainer141" class="IMG---Figure"><img src="image/C12624_04_12.jpg" alt="Figure 4.12: Diagnostic plot for the quadratic equation data fit using the lm() method&#13; &#10;"/></div><h6>Figure 4.12: Diagnostic plot for the quadratic equation data fit using the lm() method</h6><p>Now, let's see how polynomial regression fares on the Beijing PM2.5 dataset. We have introduced an additional quadratic term <strong class="inline">DEWP^2</strong>, which is simply <strong class="inline">DEWP</strong> raised to the power <strong class="inline">2</strong>. Refer to the scatterplot illustrated in <em class="italics">Figure 3.5</em> of <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em> to justify the addition of such a higher order term.</p></li>
				<li>Use polynomial regression on the Beijing PM2.5 dataset with the quadratic and cubic terms:<p class="snippet">multiple_PM25_poly_model &lt;- lm(pm2.5 ~ DEWP^2 + TEMP + Iws + DEWP*TEMP*month, data = PM25)</p></li>
				<li>To print the model summary, use the following command:<p class="snippet">summary(multiple_PM25_poly_model)</p><p>The output is as follows:</p><p class="snippet">## Residuals:</p><p class="snippet">##     Min      1Q  Median      3Q     Max </p><p class="snippet">## -298.41  -42.77   -9.35   30.91  967.39 </p><p class="snippet">## </p><p class="snippet">## Coefficients:</p><p class="snippet">##                     Estimate Std. Error t value Pr(&gt;|t|)    </p><p class="snippet">## (Intercept)        2.917e+02  4.338e+00  67.257  &lt; 2e-16 ***</p><p class="snippet">## DEWP               1.190e+01  2.539e-01  46.879  &lt; 2e-16 ***</p><p class="snippet">## TEMP              -9.830e+00  8.806e-01 -11.164  &lt; 2e-16 ***</p><p class="snippet">## Iws               -1.388e-01  7.707e-03 -18.009  &lt; 2e-16 ***</p><p class="snippet">## month2            -2.388e+01  5.011e+00  -4.766 1.89e-06 ***</p><p class="snippet">## month3            -1.228e+02  5.165e+00 -23.780  &lt; 2e-16 ***</p><p class="snippet">## DEWP:TEMP:month9   4.455e-01  6.552e-02   6.800 1.06e-11 ***</p><p class="snippet">## DEWP:TEMP:month10  5.066e-01  5.862e-02   8.642  &lt; 2e-16 ***</p><p class="snippet">## DEWP:TEMP:month11  5.111e-02  5.526e-02   0.925  0.35500    </p><p class="snippet">## DEWP:TEMP:month12  1.492e-01  6.599e-02   2.261  0.02375 *  </p><p class="snippet">## ---</p><p class="snippet">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</p><p class="snippet">## </p><p class="snippet">## Residual standard error: 70.04 on 41708 degrees of freedom</p><p class="snippet">##   (2067 observations deleted due to missingness)</p><p class="snippet">## Multiple R-squared:  0.4217, Adjusted R-squared:  0.4211 </p><p class="snippet">## F-statistic: 633.7 on 48 and 41708 DF,  p-value: &lt; 2.2e-16</p><p>Observe that in spite of an additional quadratic term, we are not attaining any better R-squared value than the linear model. At this juncture, we may conclude that the PM2.5 prediction needs a better independent variable, which could explain the variance in it to get the R-squared value to any higher level. The diagnostics plot seems to show similar interpretation.</p></li>
				<li>Plot the diagnostics plot using the following command:<p class="snippet">par(mfrow = c(2,2))</p><p class="snippet">plot(multiple_PM25_poly_model)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/C12624_04_13.jpg" alt="Figure 4.13: The diagnostics plot for polynomial regression model fit on the Beijing PM2.5 dataset&#13; &#10;"/>
				</div>
			</div>
			<h6>Figure 4.13: The diagnostics plot for polynomial regression model fit on the Beijing PM2.5 dataset</h6>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor191"/>Ridge Regression</h2>
			<p>As we saw in linear regression, <strong class="bold">Ordinary Least Square</strong> (<strong class="bold">OLS</strong>) estimates the value of <img src="image/C12624_04_44.png" alt="A picture containing furniture&#10;&#10;Description automatically generated"/> in such a way that the sum of squares of residual <img src="image/C12624_04_45.png" alt=""/> is minimized.</p>
			<p>Since <img src="image/C12624_04_46.png" alt="A picture containing furniture&#10;&#10;Description automatically generated"/> is an estimate we compute from a given sample and it's not a <em class="italics">true population parameter</em>, we need to be careful of certain characteristics of an estimate. The two such primary characteristics are the bias and the variance.</p>
			<p>If <img src="image/C12624_04_48.png" alt="A close up of a logo&#10;&#10;Description automatically generated"/> is the fit at the <img src="image/C12624_04_49.png" alt=""/> value of <img src="image/C12624_04_50.png" alt="A close up of a logo&#10;&#10;Description automatically generated"/>, then the average (or expected) <img src="image/C12624_04_47.png" alt=""/> on the test dataset could be decomposed into three quantities, the variance, the squared bias, and the variance of error terms as represented by the following equation:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/C12624_04_51.jpg" alt=""/>
				</div>
			</div>
			<p>For the best estimate, a suitable algorithm such as OLS should simultaneously achieve low bias and low variance. We commonly call this the <strong class="bold">Bias-Variance</strong> trade off. The popular bull's eye picture shown in the following figure helps understand the various scenarios of the tradeoff:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/C12624_04_14.jpg" alt="Figure 4.14: The popular bull's eye picture for explaining Bias and Variance scenarios&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.14: The popular bull's eye picture for explaining Bias and Variance scenarios</h6>
			<p>The bull's eye represents the true population value that OLS is trying to estimate, and the shots at it are the values of our estimates resulting from four different estimators. These are broadly classified into the following:</p>
			<ul>
				<li>Low Bias and Low Variance (most favorable)</li>
				<li>Low Bias and High Variance</li>
				<li>High Bias and Low Variance</li>
				<li>High Bias and High Variance (least favorable)</li>
			</ul>
			<p>OLS method treats all variables as equally likely, thus having a low bias (results in <strong class="bold">under-fitting</strong> during training) and high variance (results in <strong class="bold">prediction error</strong> in testing data) as shown in the <em class="italics">Figure 4.11</em>. Such behavior is not ideal for obtaining the optimal model complexity. The general solution to this issue is to reduce variance at the cost of introducing some bias. This approach is called regularization. So, ridge regression could be thought of as an extension of linear regression with an additional regularization term.</p>
			<p>The general form of multiple linear regression could be expressed as follows:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/C12624_04_52.jpg" alt=""/>
				</div>
			</div>
			<p>Where, <strong class="bold">argmin</strong> means the minimum value of βs that make the function attain the minimum. In the context, it finds the βs that minimize the RSS. The βs are subject to the following constraints:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/C12624_04_53.jpg" alt=""/>
				</div>
			</div>
			<h3 id="_idParaDest-190"><a id="_idTextAnchor192"/>Regularization Term – L2 Norm</h3>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/C12624_04_54.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/C12624_04_55.jpg" alt=""/>
				</div>
			</div>
			<p>The penalty term in the ridge beta increases if RSS increases. The following figure shows a plot between <strong class="bold">Model Complexity</strong> (number of predictors) and <strong class="bold">Error</strong>. It shows that when the number of predictors increase (model complexity increases), the <strong class="bold">Variance</strong> goes up and the <strong class="bold">Bias</strong> goes down. </p>
			<p>The OLS estimate finds a place in the right side, away from the optimal trade-off point. This scenario necessitates the introduction of the regularization term and hence ridge regression becomes suitable choice of model:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/C12624_04_15.jpg" alt="Figure 4.15: Bias versus variance&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.15: Bias versus variance</h6>
			<p>The OLS loss function for ridge regression could be represented by the following equation:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/C12624_04_56.jpg" alt=""/>
				</div>
			</div>
			<p>Minimizing the <img src="image/C12624_04_57.png" alt="A drawing of a face&#10;&#10;Description automatically generated"/> function with the regularization term that gives the ridge regression estimates. The interesting property of this loss function is that as <img src="image/C12624_04_58.png" alt=""/> becomes larger, the variance decreases and the bias increases.</p>
			<h3 id="_idParaDest-191"><a id="_idTextAnchor193"/>Exercise 56: Ridge Regression on the Beijing PM2.5 dataset</h3>
			<p>This exercise fits the ridge regression on the Beijing PM2.5 dataset. We will use <strong class="inline">glmnet</strong> library's cross-validation function <strong class="inline">cv.glmnet()</strong> with the parameter <strong class="inline">alpha = 0</strong> and varying lambda values. The aim is to obtain an optimal value for lambda that will be returned in the <strong class="inline">lambda.min</strong> attribute of the function output.</p>
			<p>Let's perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Load the <strong class="inline">glmnet</strong> library and preprocess the PM25 DataFrame:<p class="snippet">library(glmnet)</p><p class="snippet">PM25 &lt;- na.omit(PM25)</p><p class="snippet">X &lt;- as.matrix(PM25[,c("DEWP","TEMP","Iws")])</p><p class="snippet">Y &lt;- PM25$pm2.5</p></li>
				<li>Now, let's use the following code to set up the <strong class="inline">seed</strong> to get similar results:<p class="snippet">set.seed(100)</p><p class="snippet">model_ridge = cv.glmnet(X,Y,alpha = 0,lambda = 10^seq(4,-1,-0.1))</p></li>
				<li>To find the optimal value of lambda after cross validation, execute the following command:<p class="snippet">optimal_lambda &lt;- model_ridge$lambda.min</p></li>
				<li>Coefficient values from the model fit:<p class="snippet">ridge_coefficients &lt;- predict(model_ridge, s = optimal_lambda, type = "coefficients")</p><p class="snippet">ridge_coefficients</p><p>The output is as follows:</p><p class="snippet">## 4 x 1 sparse Matrix of class "dgCMatrix"</p><p class="snippet">##                       1</p><p class="snippet">## (Intercept) 160.7120263</p><p class="snippet">## DEWP          4.3462480</p><p class="snippet">## TEMP         -5.0902943</p><p class="snippet">## Iws          -0.2756095</p></li>
				<li>Use the <strong class="inline">predict</strong> function again and pass the matrix X to the <strong class="inline">newx</strong> parameter:<p class="snippet">ridge_prediction &lt;- predict(model_ridge, s = optimal_lambda, newx = X)</p><p class="snippet">head(ridge_prediction)</p><p>The output is as follows:</p><p class="snippet">         1</p><p class="snippet">25 111.0399</p><p class="snippet">26 115.1408</p><p class="snippet">27 137.3708</p><p class="snippet">28 154.2625</p><p class="snippet">29 154.0172</p><p class="snippet">30 158.8622</p></li>
			</ol>
			<p>We see how ridge regression could be used to fit the Beijing PM2.5 dataset using the <strong class="inline">glmnet</strong> library.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor194"/>LASSO Regression</h2>
			<p><strong class="keyword">Least Absolute Shrinkage and Selection Operator</strong> (<strong class="keyword">LASSO</strong>) follows a similar structure to that of ridge regression, except for the penalty term, which in LASSO regression is L1 (sum of absolute values of the coefficient estimates) in contrast to ridge regression where it's L2 (sum of squared coefficients):</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/C12624_04_59.jpg" alt=""/>
				</div>
			</div>
			<p>LASSO regression turns some coefficients to zero, thus the effect of a particular variable is nullified. This makes it efficient in feature selection while fitting data.</p>
			<h3 id="_idParaDest-193"><a id="_idTextAnchor195"/>Exercise 57: LASSO Regression</h3>
			<p>In this exercise, we will apply LASSO regression on the Beijing PM2.5 dataset. We will use the same <strong class="inline">cv.glmnet()</strong> function to find the optimal lambda value.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">First, let's set up <strong class="inline">seed</strong> to get similar results using the following command:<p class="snippet">set.seed(100) #Setting the seed to get similar results.</p><p class="snippet">model_LASSO = cv.glmnet(X,Y,alpha = 1,lambda = 10^seq(4,-1,-0.1))</p></li>
				<li>Now, use the following command to find the optimal value of lambda after cross validation:<p class="snippet">optimal_lambda_LASSO &lt;- model_LASSO$lambda.min</p></li>
				<li>Execute the following command to find the coefficient values from the model fit:<p class="snippet">LASSO_coefficients &lt;- predict(model_LASSO, s = optimal_lambda_LASSO, type = "coefficients")</p><p class="snippet">LASSO_coefficients</p><p>The output is as follows:</p><p class="snippet">## 4 x 1 sparse Matrix of class "dgCMatrix"</p><p class="snippet">##                       1</p><p class="snippet">## (Intercept) 160.4765008</p><p class="snippet">## DEWP          4.3324461</p><p class="snippet">## TEMP         -5.0725046</p><p class="snippet">## Iws          -0.2739729</p></li>
				<li>Use the following command to find the prediction from the model:<p class="snippet">LASSO_prediction &lt;- predict(model_LASSO, s = optimal_lambda_LASSO, newx = X)</p><p class="snippet">head(LASSO_prediction)</p><p>The output is as follows:</p><p class="snippet">          1</p><p class="snippet">25 110.9570</p><p class="snippet">26 115.0456</p><p class="snippet">27 137.2040</p><p class="snippet">28 154.0434</p><p class="snippet">29 153.7996</p><p class="snippet">30 158.6282</p></li>
			</ol>
			<p>Observe the similarity in the predictions of ridge and LASSO regression. The Beijing PM2.5 dataset doesn't show any difference in these two approaches.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor196"/>Elastic Net Regression</h2>
			<p><strong class="bold">Elastic Net</strong> combines the penalty terms of ridge and LASSO regression to avoid the overdependence on data for variable selection (coefficient values tending to zero by which highly correlated variables are kept in check). Elastic Net minimizes the following loss function:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/C12624_04_60.jpg" alt=""/>
				</div>
			</div>
			<p>Where the parameter <strong class="inline">α</strong> controls the right mix between ridge and LASSO.</p>
			<p>In summary, if a model has many predictor variables or correlated variables, introducing the regularization term helps in reducing the variance and increase bias optimally, thus bringing the right balance of model complexity and error. <em class="italics">Figure 4.16</em> provides a flow diagram to help one choose between multiple, ridge, LASSO, and elastic net regression:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/C12624_04_16.jpg" alt="Figure 4.16: Selection criteria to choose between multiple, eidge, LASSO, and elastic net regression&#13; &#10;"/>
				</div>
			</div>
			<h6>Figure 4.16: Selection criteria to choose between multiple, ridge, LASSO, and elastic net regression</h6>
			<h3 id="_idParaDest-195"><a id="_idTextAnchor197"/>Exercise 58: Elastic Net Regression</h3>
			<p>In this exercise, we will perform elastic net regression on the Beijing PM2.5 dataset.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Let's first set up <strong class="inline">seed</strong> to get similar results using the following command:<p class="snippet">set.seed(100)</p><p class="snippet">model_elanet = cv.glmnet(X,Y,alpha = 0.5,lambda = 10^seq(4,-1,-0.1))</p></li>
				<li>Now, use the following command to find the optimal value of lambda after cross validation:<p class="snippet">optimal_lambda_elanet &lt;- model_LASSO$lambda.min</p></li>
				<li>Next, execute the following command to find the coefficient values from the model fit:<p class="snippet">elanet_coefficients &lt;- predict(model_elanet, s = optimal_lambda_elanet, type = "coefficients")</p><p class="snippet">elanet_coefficients</p><p>The output is as follows:</p><p class="snippet">## 4 x 1 sparse Matrix of class "dgCMatrix"</p><p class="snippet">##                       1</p><p class="snippet">## (Intercept) 160.5950551</p><p class="snippet">## DEWP          4.3393969</p><p class="snippet">## TEMP         -5.0814722</p><p class="snippet">## Iws          -0.2747902</p></li>
				<li>Use the following command to find the prediction from the model:<p class="snippet">elanet_prediction &lt;- predict(model_elanet, s = optimal_lambda_elanet, newx = X)</p><p>The output is as follows:</p><p class="snippet">25 110.9987</p><p class="snippet">26 115.0936</p><p class="snippet">27 137.2880</p><p class="snippet">28 154.1538</p><p class="snippet">29 153.9092</p><p class="snippet">30 158.7461</p></li>
			</ol>
			<p>Elastic Net Regression gives more or less the same predictions as of ridge and LASSO regression. In the next section, we compare all three together.</p>
			<h3 id="_idParaDest-196"><a id="_idTextAnchor198"/>Comparison between Coefficients and Residual Standard Error</h3>
			<p>The following tables show the comparison of <strong class="bold">Residual Standard Error</strong> (<strong class="bold">RSE</strong>) and Coefficient values between linear, ridge, LASSO, and elastic net regression. With our Beijing PM2.5 dataset and three predictor variables (<strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong>), there isn't much difference in the values, which suggests that ridge, LASSO, and elastic net regression with regularization terms are not any better than multiple linear regression approach. This also suggests that <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, and <strong class="inline">Iws</strong> are independent variables with low or no correlations:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/C12624_04_17.jpg" alt="Figure 4.17: Comparison of residual standard error between linear, ridge, LASSO, and elastic net regression&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.17: Comparison of residual standard error between linear, ridge, LASSO, and elastic net regression</h6>
			<p>The following figure shows a comparison of the coefficient values of intercept and DEWP, TEMP and Iws variables using Linear, Ridge, LASSO and Elastic net regression:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/C12624_04_18.jpg" alt="Figure 4.18: Comparison of coefficient values between Linear, Ridge, LASSO, and Elastic Net regression&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.18: Comparison of coefficient values between Linear, Ridge, LASSO, and Elastic Net regression</h6>
			<h3 id="_idParaDest-197"><a id="_idTextAnchor199"/>Exercise 59: Computing the RSE of Linear, Ridge, LASSO, and Elastic Net Regressions</h3>
			<p>In this exercise, we will compute the RSE of Linear, Ridge, LASSO, and Elastic Net regressions.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Use the following code to fit a linear model using the <strong class="inline">Iws</strong>, <strong class="inline">DEWP</strong>, and <strong class="inline">TEMP</strong> variables:<p class="snippet">multiple_PM25_linear_model &lt;- lm(pm2.5 ~ Iws + DEWP + TEMP, data = PM25)</p></li>
				<li>Now, use the following command for finding the <strong class="bold">Residual Standard Error</strong> (<strong class="bold">RSE</strong>) of linear regression:<p class="snippet">sqrt(sum(multiple_PM25_linear_model$residuals^2)/41753)</p><p>The output is as follows:</p><p class="snippet">## [1] 81.51</p><p>Similarly, we will find the RSE of the remaining regression.</p></li>
				<li>RSE of ridge regression:<p class="snippet">sqrt(sum((Y-ridge_prediction)^2)/41753)</p><p>The output is as follows:</p><p class="snippet">## [1] 81.51059</p></li>
				<li>RSE of LASSO regression:<p class="snippet">sqrt(sum((Y-LASSO_prediction)^2)/41753)</p><p>The output is as follows:</p><p class="snippet">## [1] 81.51123</p></li>
				<li>RSE of Elastic Net regression:<p class="snippet">sqrt(sum((Y-elanet_prediction)^2)/41753)</p><p>The output is as follows:</p><p class="snippet">## [1] 81.51087</p></li>
			</ol>
			<p>This shows that the RSE for all three isn't significantly different.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor200"/>Poisson Regression</h2>
			<p>In linear regression, we saw an equation of the form:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/C12624_04_61.jpg" alt=""/>
				</div>
			</div>
			<p>In <strong class="bold">Poisson Regression</strong>, the response variable <strong class="inline">Y</strong> is a count or rate (<strong class="inline">Y/t</strong>) that has a <strong class="bold">Poisson distribution</strong> with expected (mean) count of <img src="image/C12624_04_62.png" alt=""/> as <img src="image/C12624_04_63.png" alt=""/>, which is equal to variance.</p>
			<p>In case of logistic regression, we would probe for values that can maximize log-likelihood to get the <strong class="keyword">maximum likelihood estimators</strong> (<strong class="keyword">MLEs</strong>) for coefficients.</p>
			<p>There are no closed-form solutions, hence the estimations of maximum likelihood would be obtained using iterative algorithms such as <strong class="bold">Newton-Raphson</strong> and <strong class="keyword">Iteratively re-weighted least squares</strong> (<strong class="keyword">IRWLS</strong>).</p>
			<p>Poisson regression is suitable for the count-dependent variable, which must meet the following guidelines:</p>
			<ul>
				<li>It follows a Poisson distribution</li>
				<li>Counts are not negative</li>
				<li>Values are whole numbers (no fractions)<h4>Note</h4><p class="callout">The dataset used here to demonstrate Poisson regression comes from A. Colin Cameron and Per Johansson, "<em class="italics">Count Data Regression Using Series Expansion: With Applications</em>", Journal of Applied Econometrics, Vol. 12, No. 3, 1997, pp. 203-224.</p></li>
			</ul>
			<p>The following table succinctly describes the variables:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/C12624_04_19.jpg" alt="Figure 4.19: Variables and its description from an Australian Health Survey dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.19: Variables and its description from an Australian Health Survey dataset</h6>
			<h4>Note</h4>
			<p class="callout">The blog <a href="http://www.econ.uiuc.edu/~econ508/Stata/e-ta16_Stata.html">http://www.econ.uiuc.edu/~econ508/Stata/e-ta16_Stata.html</a> demonstrates the usage of the dataset.</p>
			<h3 id="_idParaDest-199"><a id="_idTextAnchor201"/>Exercise 60: Performing Poisson Regression</h3>
			<p>In this exercise, we will perform Poisson regression on the dataset.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Carry out Poisson regression load the library <strong class="inline">foreign</strong> to read <strong class="inline">dta</strong> data:<p class="snippet">library(foreign)</p></li>
				<li>Using the <strong class="inline">read.data</strong> function from the <strong class="inline">foreign</strong> library to read the Australian health survey dataset:<p class="snippet">df_health &lt;- read.dta("health.dta")</p></li>
				<li>Fit a generalized linear model using the <strong class="inline">glm()</strong> function with Poisson regression as the value in the family parameter:<p class="snippet">poisson_regression_health &lt;- glm(NONDOCCO ~ ., data = df_health, family=poisson(link=log))</p></li>
				<li>Print the summary of the model:<p class="snippet">summary(poisson_regression_health)</p><p>The output is as follows:</p><p class="snippet">## Coefficients:##              Estimate Std. Error z value Pr(&gt;|z|)    </p><p class="snippet">## (Intercept) -3.116128   0.137763 -22.620  &lt; 2e-16 ***</p><p class="snippet">## SEX          0.336123   0.069605   4.829 1.37e-06 ***</p><p class="snippet">## AGE          0.782335   0.200369   3.904 9.44e-05 ***</p><p class="snippet">## INCOME      -0.123275   0.107720  -1.144 0.252459    </p><p class="snippet">## LEVYPLUS     0.302185   0.097209   3.109 0.001880 ** </p><p class="snippet">## FREEPOOR     0.009547   0.210991   0.045 0.963910    </p><p class="snippet">## FREEREPA     0.446621   0.114681   3.894 9.84e-05 ***</p><p class="snippet">## ILLNESS      0.058322   0.021474   2.716 0.006610 ** </p><p class="snippet">## ACTDAYS      0.098894   0.006095  16.226  &lt; 2e-16 ***</p><p class="snippet">## HSCORE       0.041925   0.011613   3.610 0.000306 ***</p><p class="snippet">## CHCOND1      0.496751   0.086645   5.733 9.86e-09 ***</p><p class="snippet">## CHCOND2      1.029310   0.097262  10.583  &lt; 2e-16 ***</p><p class="snippet">## ---</p><p class="snippet">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</p><p class="snippet">## </p><p class="snippet">## (Dispersion parameter for poisson family taken to be 1)</p><p class="snippet">## </p><p class="snippet">##     Null deviance: 6127.9  on 5189  degrees of freedom</p><p class="snippet">## Residual deviance: 5052.5  on 5178  degrees of freedom</p><p class="snippet">## AIC: 6254.3</p><p class="snippet">## </p><p class="snippet">## Number of Fisher Scoring iterations: 7</p></li>
				<li>Load the <strong class="inline">ggplot2</strong> library:<p class="snippet">library(ggplot2)</p></li>
				<li>Combine the actual values of <strong class="inline">NONDOCCO</strong> and Poisson regression-fitted values of <strong class="inline">NONDOCCO</strong>:<p class="snippet">df_pred_actual &lt;- data.frame(cbind(df_health$NONDOCCO,poisson_regression_health$fitted.values))</p></li>
				<li>Name the columns:<p class="snippet">colnames(df_pred_actual) &lt;- c("actual_NONDOCCO","predicted_NONDOCCO")</p></li>
				<li>Plot the actual versus predicted values of the <strong class="inline">NONDOCCO</strong> target variable:<p class="snippet">ggplot(df_pred_actual, aes(x=actual_NONDOCCO, y =predicted_NONDOCCO))+</p><p class="snippet">   geom_point()</p><p>The output plot is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/C12624_04_20.jpg" alt="Figure 4.20: Comparing actual and predicted values of NONDOCCO&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.20: Comparing actual and predicted values of NONDOCCO</h6>
			<p>Given the value of the residual deviance statistic of 5052.5 with 5178 degree of freedom, the p-value is zero and the <em class="italics">5052.5/5178 = 0.975</em> is less than 1, so the model does to a certain level. We can also check overdispersion (presence of greater variability in a dataset than would be expected based on a given statistical model). Overdispersion is computed by dividing <strong class="inline">sample_variance</strong> with <strong class="inline">sample_mean</strong>. Let's examine the following exercise.</p>
			<h3 id="_idParaDest-200"><a id="_idTextAnchor202"/>Exercise 61: Computing Overdispersion</h3>
			<p>In this exercise, we will perform computing overdispersion on the dataset.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">First, let's find the sample mean using the following command:<p class="snippet">s_mean &lt;- mean(df_health$NONDOCCO)</p><p class="snippet">s_mean</p><p>The output is as follows:</p><p class="snippet">## [1] 0.2146435</p></li>
				<li>Now, use the following command for finding the sample variance:<p class="snippet">s_variance &lt;- var(df_health$NONDOCCO)</p><p class="snippet">s_variance</p><p>The output is as follows:</p><p class="snippet">## [1] 0.931757</p></li>
				<li>Similarly, overdispersion can be computed using the following command:<p class="snippet">s_variance/s_mean</p><p>The output is as follows:</p><p class="snippet">## [1] 4.34095</p><p>So, even if we try adding predictor variables to model fit, overdispersion starts to go down. In our example, the dispersion is well within limits.</p></li>
				<li>Now, let's calculate the dispersion using the following command:<p class="snippet">summary.glm(poisson_regression_health)$dispersion</p><p>The output is as follows:</p><p class="snippet">## [1] 1</p></li>
			</ol>
			<p>However, in cases where dispersion is over the limit, a higher order Poisson regression is a suitable solution. Keeping the scope of this book in mind, we will not delve into such model in detail here. Interested readers could read more on Baseline Density Poisson (<strong class="keyword">Poisson Polynomial of order p</strong> (<strong class="keyword">PPp</strong>) models).</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor203"/>Cox Proportional-Hazards Regression Model</h2>
			<p>The basis for the Cox regression models comes from the survival analysis, a set of statistical methods helpful in investigating the time it takes for an event to occur. Some examples are as follows:</p>
			<ul>
				<li>	Time until a lead is converted to sales</li>
				<li>	Time until a product failure from the start of usage</li>
				<li>	Time after the start of the insurance policy until death</li>
				<li>	Time after diagnosing until death</li>
				<li>	Time until a warranty is claimed for a product</li>
				<li>	Time from customer registration</li>
			</ul>
			<p>All these examples are some of the use cases of survival analysis. In most of the survival analysis, there are three wide-spread methods used for carrying out such time-to-event analysis:</p>
			<ul>
				<li>Kaplan-Meier survival curves for analysis of different groups</li>
				<li>The logrank test for comparing two or more survival curves</li>
				<li>Cox proportional hazards regression to describe the effect of variables on survival</li>
			</ul>
			<p>Keeping in mind the scope of this chapter and book, we will focus only on the Cox proportional hazards regression. The fundamental idea is that the first two methods only help in performing univariate analysis, in other words, you can understand the effect of only one factor on the time-to-event, whereas Cox regression helps in assessing the effect of multiple factors on the survival time. Also, Cox regression works equally good with both categorical and numeric factors, while the first two methods only work with categorical factors.</p>
			<p>The Cox model is expressed by the hazard function denoted by <strong class="inline">h(t)</strong>, which represents in the medical research, where its predominately used, the risk of dying at time <strong class="inline">t</strong>:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/C12624_04_64.jpg" alt=""/>
				</div>
			</div>
			<p>Some observations from this equation are as follows:</p>
			<ul>
				<li><strong class="inline">t</strong> denotes the survival time.</li>
				<li><strong class="inline">h(t)</strong> represents the hazard function determined by the <strong class="inline">p</strong> covariates <img src="image/C12624_04_65.png" alt=""/>. Covariates is the term used for describing predictor variables in survival analysis.</li>
				<li>The coefficients <img src="image/C12624_04_66.png" alt=""/> suggest the impact of covariates.</li>
				<li>The term <img src="image/C12624_04_67.png" alt=""/> is called the baseline hazard at time t. If all the coefficients are zero <img src="image/C12624_04_68.png" alt=""/>, <img src="image/C12624_04_69.png" alt=""/> becomes the value of the hazard function.</li>
			</ul>
			<p>This function looks somewhat relatable to logistic regression (uses an exponential term), which will be discussed in detail in <em class="italics">Chapter 5</em>, <em class="italics">Classification</em>. We have logically split all the supervised learning algorithms discussed in this book into <strong class="bold">Regression</strong> and <strong class="bold">Classification</strong>. Though logistic regression predicts numeric values in the form of probabilities, it is used widely as a <strong class="bold">Classification algorithm</strong>. However, in the context of this section, it suffices to note that logistic regression uses a binary dependent variable (<strong class="inline">yes/no</strong>, <strong class="inline">1/0</strong>) but disregards the timing of events.</p>
			<p>As you may have observed from the hazard function, survival models comprise of:</p>
			<ul>
				<li>A continuous variable demonstrative of the time to event</li>
				<li>A binary variable illustrative of the status whether event happened or not</li>
			</ul>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor204"/>NCCTG Lung Cancer Data</h2>
			<p><strong class="bold">NCCTG Lung Cancer Data</strong> from survival in patients with advanced lung cancer is from the <em class="italics">North Central Cancer Treatment Group</em>. The data is a collection of few metadata, such as which institution collected it, age of the patient, sex, and so on. The performance scores in this dataset rates how well the patient can perform the daily activities. The most important variable in any survival analysis dataset is the <em class="italics">knowledge</em> about the <strong class="bold">time-to-event</strong>, for example, time until death.</p>
			<p>Survival analysis is usually defined as a set of methods for examining data where the outcome variable is the time till the incidence of an event of interest.</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/C12624_04_21.jpg" alt="Figure 4.21: Variables and its descriptions of North Central Cancer Treatment Group&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 4.21: Variables and its descriptions of North Central Cancer Treatment Group</h6>
			<p>In the next exercise, we will learn how to create the survival object using the method <strong class="inline">Surv</strong> from the <strong class="inline">survival</strong> package. Note that in the summary of the dataset after adding the survival object, two additional variables <strong class="inline">SurvObject.time</strong> and <strong class="inline">SurvObject.status</strong> are created, which stores the information about time-to-event (time until death), which then becomes the dependent variable for the <strong class="bold">Cox Proportional-Hazards Regression Model</strong>.</p>
			<p>Observations are <strong class="bold">censored</strong> when there is a scarce number of indications around a patient's survival time. Popularly, the most prevalent form is right censoring. Let's assume that we are following a study for 20 weeks. A patient not going through the event of interest during the study can be called as right censored. The person's survival time is at least the duration of the study; in this case, 20 weeks.</p>
			<h3 id="_idParaDest-203"><a id="_idTextAnchor205"/>Exercise 62: Exploring the NCCTG Lung Cancer Data Using Cox-Regression</h3>
			<p>In this exercise, we will explore the NCCTG Lung Cancer Data using Cox-Regression.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the <strong class="inline">survival</strong> library:<p class="snippet">library(survival)</p></li>
				<li>Import the Lung Cancer Data:<p class="snippet">df_lung_cancer &lt;- lung</p></li>
				<li>Print the dataset using the <strong class="inline">head</strong> function:<p class="snippet">head(df_lung_cancer)</p><p>The output is as follows:</p><p class="snippet">##   inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss</p><p class="snippet">## 1    3  306      2  74   1       1       90       100     1175      NA</p><p class="snippet">## 2    3  455      2  68   1       0       90        90     1225      15</p><p class="snippet">## 3    3 1010      1  56   1       0       90        90       NA      15</p><p class="snippet">## 4    5  210      2  57   1       1       90        60     1150      11</p><p class="snippet">## 5    1  883      2  60   1       0      100        90       NA       0</p><p class="snippet">## 6   12 1022      1  74   1       1       50        80      513       0</p></li>
				<li>Lung Cancer Data where <strong class="inline">status == 2</strong> represents death:<p class="snippet">df_lung_cancer$SurvObject &lt;- with(df_lung_cancer, Surv(time, status == 2))</p></li>
				<li>Find the Cox Proportional Hazards Regression model:<p class="snippet">cox_regression &lt;- coxph(SurvObject ~ age + sex + ph.karno + wt.loss, data =  df_lung_cancer)</p><p class="snippet">cox_regression</p><p>The output is as follows:</p><p class="snippet">## Call:</p><p class="snippet">## coxph(formula = SurvObject ~ age + sex + ph.karno + wt.loss, </p><p class="snippet">##     data = df_lung_cancer)</p><p class="snippet">## </p><p class="snippet">## </p><p class="snippet">##              coef exp(coef) se(coef)     z      p</p><p class="snippet">## age       0.01514   1.01525  0.00984  1.54 0.1238</p><p class="snippet">## sex      -0.51396   0.59813  0.17441 -2.95 0.0032</p><p class="snippet">## ph.karno -0.01287   0.98721  0.00618 -2.08 0.0374</p><p class="snippet">## wt.loss  -0.00225   0.99776  0.00636 -0.35 0.7239</p><p class="snippet">## </p><p class="snippet">## Likelihood ratio test=18.8  on 4 df, p=0.000844</p><p class="snippet">## n= 214, number of events= 152 </p><p class="snippet">##    (14 observations deleted due to missingness)</p></li>
			</ol>
			<p>This exercise demonstrates the Cox proportional hazards regression model using the survival library.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor206"/>Summary</h2>
			<p>In this chapter, we discussed linear regression in more detail after a brief introduction in the previous chapter. Certainly, the discussion on linear regression led to a series of diagnostics that gave directions to discussing other type of regression algorithms. Quantile, polynomial, ridge, LASSO, and elastic net, all of these are derived from linear regression, with the differences coming from the fact that there are some limitations in linear regression that each of these algorithms helped overcome. Poisson and Cox proportional hazards regression model came out as a special case of regression algorithms that work with count and time-to-event dependent variables, respectively, unlike the others that work with any quantitative dependent variable.</p>
			<p>In the next chapter, we will explore the second most commonly applied machine learning algorithm and solve problems associated with it. You will also learn more about classification in detail. <em class="italics">Chapter 5</em>, <em class="italics">Classification</em>, similar to this chapter, is designed to cover classification algorithms ranging from <strong class="bold">Decision Trees</strong> to <strong class="bold">Deep Neural Network</strong> in detail.</p>
		</div>
	</body></html>