- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Hyperparameter Optimization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: How a Kaggle solution performs is not simply determined by the type of learning
    algorithm you choose. Aside from the data and the features that you use, it is
    also strongly determined by the algorithm’s **hyperparameters**, the parameters
    of the algorithm that have to be fixed *prior to* training, and cannot be learned
    during the training process. Choosing the right variables/data/features is most
    effective in tabular data competitions; however, hyperparameter optimization is
    effective in *all* competitions, of any kind. In fact, given fixed data and an
    algorithm, hyperparameter optimization is the only sure way to enhance the predictive
    performance of the algorithm and climb the leaderboard. It also helps in ensembling,
    because an ensemble of tuned models always performs better than an ensemble of
    untuned ones.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle解决方案的表现并不仅仅取决于你选择的机器学习算法类型。除了数据和使用的特征之外，它还强烈取决于算法的**超参数**，这些参数必须在训练之前固定，并且在训练过程中无法学习。在表格数据竞赛中选择正确的变量/数据/特征是最有效的；然而，超参数优化在**所有**类型的竞赛中都是有效的。实际上，给定固定的数据和算法，超参数优化是唯一确保提高算法预测性能并攀登排行榜的方法。它还有助于集成，因为经过调优的模型集成总是比未经调优的模型集成表现更好。
- en: You may hear that tuning hyperparameters manually is possible if you know and
    understand the effects of your choices on the algorithm. Many Kaggle Grandmasters
    and Masters have declared that they often rely on directly tuning their models
    in competitions. They operate selectively on the most important hyperparameters
    in a bisection operation style, exploring smaller and smaller intervals of a parameter’s
    values until they find the value that produces the best result. Then, they move
    on to another parameter. This works perfectly well if there is a single minimum
    for each parameter and if the parameters are independent from each other. In this
    case, the search is mostly driven by experience and knowledge of learning algorithms.
    In our experience, however, that is not the case with most tasks you will encounter
    on Kaggle. The sophistication of the problems and the algorithms used requires
    a systematic approach that only a search algorithm can provide. Hence, we decided
    to write this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会听说，如果你了解并理解你的选择对算法的影响，手动调整超参数是可能的。许多Kaggle大师和专家都宣称，他们在比赛中经常直接调整他们的模型。他们以二分法操作风格有选择性地操作最重要的超参数，探索参数值的越来越小的区间，直到他们找到产生最佳结果的价值。然后，他们转向另一个参数。如果每个参数只有一个最小值，并且参数之间相互独立，这种方法将完美无缺。在这种情况下，搜索主要是由经验和学习算法的知识驱动的。然而，根据我们的经验，在Kaggle上遇到的大多数任务并非如此。问题的复杂性和使用的算法需要一种只有搜索算法才能提供的系统方法。因此，我们决定编写这一章节。
- en: In this chapter, we will explore how to extend your cross-validation approach
    to find the best hyperparameters that can generalize to your test set. The idea
    is to deal with the pressure and scarcity of time and resources that you experience
    in competitions. For this reason, we will concentrate on **Bayesian optimization
    methods**, which are a proven way to optimize for complex models and data problems
    based on the resources you have available. We won’t limit ourselves to searching
    for the best values for pre-defined hyperparameters; we will also delve into the
    problem of neural network architecture.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何扩展你的交叉验证方法来找到最佳的超参数，这些参数可以推广到你的测试集。这个想法是处理你在比赛中经历的压力和资源稀缺。因此，我们将专注于**贝叶斯优化方法**，这是一种基于你可用资源的复杂模型和数据问题优化的有效方法。我们不会限制自己只搜索预定义超参数的最佳值；我们还将深入研究神经网络架构的问题。
- en: 'We will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Basic optimization techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本优化技术
- en: Key parameters and how to use them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键参数及其使用方法
- en: Bayesian optimization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Let’s start!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Basic optimization techniques
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本优化技术
- en: The core algorithms for hyperparameter optimization, found in the Scikit-learn
    package, are **grid search** and **random search**. Recently, the Scikit-learn
    contributors have also added the **halving algorithm** to improve the performances
    of both grid search and random search strategies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包中用于超参数优化的核心算法是**网格搜索**和**随机搜索**。最近，Scikit-learn的贡献者还添加了**减半算法**来提高网格搜索和随机搜索策略的性能。
- en: In this section, we will discuss all these basic techniques. By mastering them,
    not only will you have effective optimization tools for some specific problems
    (for instance, SVMs are usually optimized by grid search) but you will also be
    familiar with the basics of how hyperparameter optimization works.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论所有这些基本技术。通过掌握它们，你不仅将拥有针对某些特定问题的有效优化工具（例如，SVMs通常通过网格搜索进行优化），而且你还将熟悉超参数优化的工作原理的基础。
- en: 'To start with, it is crucial to figure out what the necessary ingredients are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，弄清楚必要的成分至关重要：
- en: A model whose hyperparameters have to be optimized
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要优化超参数的模型
- en: A search space containing the boundaries of the values to search between for
    each hyperparameter
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含每个超参数搜索之间值边界的搜索空间
- en: A cross-validation scheme
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证方案
- en: An evaluation metric and its score function
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个评估指标及其评分函数
- en: All these elements come together in the search method to determine the solution
    you are looking for. Let’s see how it works.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些元素都在搜索方法中汇集起来，以确定你正在寻找的解决方案。让我们看看它是如何工作的。
- en: Grid search
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: '**Grid search** is a method that searches through the hyperparameters exhaustively,
    and is not feasible in high-dimensional space. For every parameter, you pick a
    set of values you want to test. You then test all the possible combinations in
    this set. That is why it is exhaustive: you try everything. It is a very simple
    algorithm and it suffers from the curse of dimensionality, but, on the positive
    side, it’s *embarrassingly parallel* (see [https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html](https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html)
    for a definition of this computer science term). This means you can obtain an
    optimal tuning very quickly, if you have enough processors to run the search on.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**是一种遍历超参数的方法，在高维空间中不可行。对于每个参数，你选择一组你想要测试的值。然后，你测试这个集合中所有可能的组合。这就是为什么它是穷举的：你尝试了所有可能的情况。这是一个非常简单的算法，它受到维度灾难的影响，但，从积极的一面来看，它是**令人尴尬地并行**的（参见[https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html](https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html)以了解这个计算机科学术语的定义）。这意味着如果你有足够的处理器来运行搜索，你可以非常快速地获得最优调整。'
- en: 'As an example, let’s take a classification problem and **support-vector machine
    classification** (**SVC**). **Support-vector machines** (**SVMs**) for both classification
    and regression problems are probably the machine learning algorithm that you will
    use grid search for the most. Using the `make_classification` function from Scikit-learn,
    we can generate a classification dataset quickly:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个分类问题为例，让我们看看**支持向量机分类**（**SVC**）。对于分类和回归问题，支持向量机（**SVMs**）可能是你将最频繁使用网格搜索的机器学习算法。使用Scikit-learn的`make_classification`函数，我们可以快速生成一个分类数据集：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For our next step, we define a basic SVC algorithm and set the search space.
    Since the **kernel** **function** of the SVC (the internal function that transforms
    the input data in an SVM) determines the different hyperparameters to set, we
    provide a list containing two dictionaries of distinct search spaces for parameters
    to be used depending on the type of kernel chosen. We also set the evaluation
    metric (we use accuracy in this case, since the target is perfectly balanced):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '对于我们的下一步，我们定义了一个基本的SVC算法并设置了搜索空间。由于SVC的**核函数**（SVM中转换输入数据的内部函数）决定了要设置的不同的超参数，我们提供了一个包含两个不同搜索空间字典的列表，用于根据选择的核类型使用参数。我们还设置了评估指标（在这种情况下，我们使用准确率，因为目标是完美平衡的）： '
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In our example, a linear kernel doesn’t require the tuning of the `gamma` parameter,
    though it is very important for a radial basis function kernel. Therefore, we
    provide two dictionaries: the first containing the parameters for the linear kernel,
    the second containing parameters for a radial basis function kernel. Each dictionary
    only contains a reference to the kernel it is relevant to and only the range of
    parameters that are relevant for that kernel.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，线性核不需要调整`gamma`参数，尽管它对于径向基函数核非常重要。因此，我们提供了两个字典：第一个包含线性核的参数，第二个包含径向基函数核的参数。每个字典只包含与该核相关的引用以及对该核相关的参数范围。
- en: It is important to note that the evaluation metric can be different from the
    cost function optimized by the algorithm. In fact, as discussed in *Chapter 5*,
    *Competition Tasks and Metrics*, you may encounter scenarios in which the evaluation
    metric for the competition is different, but you cannot modify the cost function
    of your algorithm. Under these circumstances, tuning the hyperparameters according
    to your evaluation metric can still help in obtaining a well-performing model.
    Though built around the algorithm’s cost function, the optimal set of hyperparameters
    found will be the one returning the best evaluation metric under such constraints.
    It probably won’t be the theoretically best result that you could obtain for the
    problem, but it may often not be far from it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，评估指标可能与算法优化的成本函数不同。事实上，如第5章“竞赛任务和指标”中所述，你可能会遇到竞赛的评估指标不同，但你无法修改算法的成本函数。在这种情况下，根据你的评估指标调整超参数仍然有助于获得性能良好的模型。虽然这个最优的超参数集是基于算法的成本函数构建的，但找到的将是在这种约束下返回最佳评估指标的参数集。这或许不是理论上你能为该问题获得的最佳结果，但它可能通常不会离最佳结果太远。
- en: 'All the ingredients (model, search space, evaluation metric, cross-validation
    scheme) are combined into the `GridSearchCV` instance, and then the model is fit
    to the data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有成分（模型、搜索空间、评估指标、交叉验证方案）都组合到`GridSearchCV`实例中，然后模型被拟合到数据上：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After a while, depending on the machine you are running the optimization on,
    you will obtain the best combination based on cross-validated results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一段时间，根据你在其上运行优化的机器，你将根据交叉验证的结果获得最佳组合。
- en: In conclusion, grid search is a very simple optimization algorithm that can
    leverage the availability of multi-core computers. It can work fine with machine
    learning algorithms that do not require many tunings (such as SVM and the ridge
    and lasso regressions) but, in all other cases, its applicability is quite narrow.
    First, it is limited to optimizing hyperparameters by discrete choice (you need
    a limited set of values to cycle through). In addition, you cannot expect it to
    work effectively on algorithms requiring *multiple* hyperparameters to be tuned.
    This is because of the exploding complexity of the search space, and because most
    of the computational inefficiency is due to the fact that the search is trying
    parameter values blindly, most of which do not work for the problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，网格搜索是一种非常简单的优化算法，可以利用多核计算机的可用性。它可以很好地与不需要很多调整的机器学习算法（如SVM和岭回归和Lasso回归）一起工作，但在所有其他情况下，其适用性相当有限。首先，它限于通过离散选择来优化超参数（你需要一个有限的值集来循环）。此外，你不能期望它在需要调整*多个*超参数的算法上有效工作。这是由于搜索空间的爆炸性复杂性，以及由于大多数计算效率低下是由于搜索在盲目地尝试参数值，其中大多数对问题不起作用。
- en: Random search
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索
- en: '**Random search**, which simply samples the search space randomly, is feasible
    in high-dimensional spaces and is widely used in practice. The downside of random
    search, however, is that it doesn’t use information from prior experiments to
    select the next setting (a problem shared by grid search, we should note). In
    addition, to find the best solution as fast as possible, you cannot do anything
    except hope to be lucky you catch the right hyperparameters.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机搜索**，简单地随机采样搜索空间，在高维空间中是可行的，并且在实践中被广泛使用。然而，随机搜索的缺点是它没有使用先前实验的信息来选择下一个设置（我们应注意的是，这是与网格搜索共享的问题）。此外，为了尽可能快地找到最佳解决方案，你除了希望幸运地找到正确的超参数外，别无他法。'
- en: 'Random search works incredibly well and it is simple to understand. Despite
    the fact it relies on randomness, it isn’t just based on blind luck, though it
    may initially appear to be. In fact, it works like random sampling in statistics:
    the main point of the technique is that if you do enough random tests, you have
    a good possibility of finding the right parameters without wasting energy on testing
    slightly different combinations of similarly performing combinations.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索工作得非常好，而且很容易理解。尽管它依赖于随机性，但它并不仅仅基于盲目的运气，尽管一开始看起来可能如此。实际上，它就像统计学中的随机抽样：这种技术的主要观点是，如果你进行足够的随机测试，你就有很好的机会找到正确的参数，而无需在测试稍微不同的相似性能组合上浪费能量。
- en: 'Many AutoML systems rely on random search when there are too many parameters
    to set (see Golovin, D. et al. *Google Vizier: A Service for Black-Box Optimization*,
    2017). As a rule of thumb, consider looking at random search when the dimensionality
    of your hyperparameter optimization problem is sufficiently high (for example,
    over 16).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '当参数设置过多时，许多AutoML系统依赖于随机搜索（参见Golovin, D. 等人 *Google Vizier: A Service for Black-Box
    Optimization*，2017）。作为一个经验法则，当你的超参数优化问题的维度足够高时（例如，超过16），可以考虑使用随机搜索。'
- en: 'Below, we run the previous example using random search:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们使用随机搜索来运行之前的示例：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that, now, we don’t care about running the search on separate spaces
    for the different kernels. Contrary to grid search, where each parameter, even
    the ineffective ones, is systematically tested, which requires computational time,
    here the efficiency of the search is not affected by the set of hyperparameters
    tested. The search doesn’t depend on irrelevant parameters, but is guided by chance;
    any trial is useful, even if you are testing only one valid parameter among many
    for the chosen kernel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在我们不再关心在单独的空间中运行搜索以针对不同的核。与网格搜索不同，在网格搜索中，每个参数（即使是无效的参数）都会系统地测试，这需要计算时间，而在这里，搜索的效率不受测试的超参数集的影响。搜索不依赖于无关参数，而是由机会引导；任何试验都是有用的，即使你只测试了所选核中许多有效参数中的一个。
- en: Halving search
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减半搜索
- en: 'As we mentioned, both grid search and random search work in an uninformed way:
    if some tests find out that certain hyperparameters do not impact the result or
    that certain value intervals are ineffective, the information is not propagated
    to the following searches.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，网格搜索和随机搜索都以无信息的方式工作：如果某些测试发现某些超参数不会影响结果或某些值区间无效，则该信息不会传播到后续的搜索中。
- en: For this reason, Scikit-learn has recently introduced the `HalvingGridSearchCV`
    and `HalvingRandomSearchCV` estimators, which can be used to search a parameter
    space using **successive halving** applied to the grid search and random search
    tuning strategies.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Scikit-learn最近引入了`HalvingGridSearchCV`和`HalvingRandomSearchCV`估计器，可以使用这些估计器通过应用于网格搜索和随机搜索调整策略的**连续减半**来搜索参数空间。
- en: In halving, a large number of hyperparameter combinations are evaluated in an
    initial round of tests but using a small amount of computational resources. This
    is achieved by running the tests on a subsample of a few cases from your training
    data. A smaller training set needs fewer computations to be tested, so fewer resources
    (namely time) are used at the cost of more imprecise performance estimations.
    This initial round allows the selection of a subset of candidate hyperparameter
    values, which have performed better on the problem, to be used for the second
    round, when the training set size is increased.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在减半法中，在初始测试轮次中，会评估大量超参数组合，但使用的计算资源却很少。这是通过在训练数据的一小部分案例上运行测试来实现的。较小的训练集需要更少的计算来测试，因此，在牺牲更精确的性能估计的代价下，使用的资源（即时间）更少。这个初始轮次允许选择一组候选超参数值，这些值在问题上的表现更好，用于第二轮，当训练集大小增加时。
- en: The following rounds proceed in a similar way, allocating larger and larger
    subsets of the training set to be searched as the range of tested values is restricted
    (testing now requires more time to execute, but returns a more precise performance
    estimation), while the number of candidates continues to be halved.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的轮次以类似的方式进行，随着测试值的范围受到限制（现在测试需要更多的时间来执行，但返回更精确的性能估计），将更大的训练集子集分配给搜索，而候选者的数量继续减半。
- en: 'Here is an example applied to the previous problem:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个应用于之前问题的示例：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this way, halving provides information to the successive optimization steps
    via the selection of the candidates. In the next sections, we will discuss even
    smarter ways to achieve a more precise and efficient search through the space
    of hyperparameters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，减半通过选择候选者向后续优化步骤提供信息。在下一节中，我们将讨论通过超参数空间实现更精确和高效搜索的更智能的方法。
- en: '![](img/Kazuki_Onodera.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Kazuki_Onodera](img/Kazuki_Onodera.png)'
- en: Kazuki Onodera
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Kazuki Onodera
- en: '[https://www.kaggle.com/onodera](https://www.kaggle.com/onodera)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/onodera](https://www.kaggle.com/onodera)'
- en: Let’s pause for an interview with another Kaggler. Kazuki Onodera is a Competitions
    Grandmaster and Discussions Master who has around 7 years of competition experience.
    He’s also a Senior Deep Learning Data Scientist at NVIDIA and a member of the
    NVIDIA KGMON (Kaggle Grandmasters of NVIDIA) team.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，进行另一位Kaggler的采访。小野田和树是一位拥有约7年比赛经验的竞赛大师和讨论大师。他也是NVIDIA的高级深度学习数据科学家，并是NVIDIA
    KGMON（Kaggle NVIDIA大师）团队的一员。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: Instacart Market Basket Analysis. *This competition proved quite challenging
    for the Kaggle community because of its use of anonymized data related to customer
    orders over time in order to predict which previously purchased products will
    be in a user’s next order. The reason why I like it is that I love feature engineering
    and I could come up with a bunch of good and interesting features everyone else
    couldn’t, which allowed me to get second place in the competition.*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Instacart购物篮分析。*这项比赛对Kaggle社区来说相当具有挑战性，因为它使用了与客户订单相关的匿名数据，以预测用户下一次订单中将会购买哪些之前购买的产品。我喜欢它的原因是我热爱特征工程，我能想出一堆其他人无法想到的好奇特征，这让我在比赛中获得了第二名。*
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何应对Kaggle比赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*I try to imagine how a model works, and delve into false negatives and false
    positives. Same as in my daily work.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*我试图想象一个模型是如何工作的，并深入研究假阴性和假阳性。这和我的日常工作一样。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请告诉我们您参加过的特别具有挑战性的比赛，以及您使用了哪些见解来应对这项任务。
- en: Human Protein Atlas - Single Cell Classification. *This competition was a kind
    of instance segmentation competition, but no masks were provided. So, it turned
    into being a weakly supervised multi-label classification problem. I created a
    two-stage pipeline for removing label noise.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 人类蛋白质图谱 - 单细胞分类。*这项比赛是一种实例分割比赛，但没有提供掩码。因此，它变成了一个弱监督的多标签分类问题。我创建了一个两阶段管道来去除标签噪声。*
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你的职业生涯？如果是的话，是如何帮助的？
- en: '*Yes. I’m now working in the NVIDIA KGMON (Kaggle Grandmasters of NVIDIA) team.
    Kaggle launches many different machine learning competitions, different with regards
    to data type, tabular, image, natural language, and signal, as well as with regards
    to sector and domain: industry, finance, astronomy, pathology, sports, retail,
    and so on. I bet nobody can access and have experience with all these kinds of
    data except Kagglers.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的。我现在在NVIDIA KGMON（Kaggle NVIDIA大师）团队工作。Kaggle推出了许多不同的机器学习比赛，这些比赛在数据类型、表格、图像、自然语言和信号等方面各不相同，以及在与行业和领域相关方面：工业、金融、天文学、病理学、体育、零售等等。我相信除了Kagglers之外，没有人能够访问并拥有所有这些类型的数据经验。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kaggler通常忽略了什么？你现在知道的事情，你希望在你最初开始时就知道？
- en: '*Target analysis. Also, seed averaging is quite overlooked: always simple but
    powerful.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标分析。此外，种子平均法常常被忽视：总是简单但强大。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的比赛中，你犯过哪些错误？
- en: '*Target analysis. Top teams always analyze the target better than others, so
    if I couldn’t get a better place in a competition, I go and read about the top
    solutions, because they always describe to me the knowledge about the data that
    I missed during the competition.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标分析。顶尖团队总是比其他人更好地分析目标，所以如果我在比赛中没有获得更好的名次，我会去阅读关于顶尖解决方案的内容，因为他们总是向我描述我在比赛中遗漏的数据知识。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否推荐使用特定的工具或库来进行数据分析或机器学习？
- en: '*Just Python and Jupyter* *Notebooks.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*仅Python和Jupyter笔记本。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做些什么最重要？
- en: '*If you can learn from a defeat, you haven’t really lost.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你能从失败中学习，那么你实际上并没有真正失败。*'
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否使用其他竞赛平台？它们与Kaggle相比如何？
- en: '*KDD Cup and RecSys. Both meet the minimum requirements for being interesting
    and challenging.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*KDD Cup和RecSys。两者都满足有趣和具有挑战性的最低要求*。'
- en: Key parameters and how to use them
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键参数及其使用方法
- en: The next problem is using the right set of hyperparameters for each kind of
    model you use. In particular, in order to be efficient in your optimization, you
    need to know the values of each hyperparameter that it actually makes sense to
    test for each distinct algorithm.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是为你使用的每种模型选择正确的超参数集。特别是，为了在优化中提高效率，你需要知道每个算法中实际有意义的每个超参数的值。
- en: In this section, we will examine the most common models used in Kaggle competitions,
    especially the tabular ones, and discuss the hyperparameters you need to tune
    in order to obtain the best results. We will distinguish between classical machine
    learning models and gradient boosting models (which are much more demanding in
    terms of their space of parameters) for generic tabular data problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将检查Kaggle竞赛中最常用的模型，特别是表格模型，并讨论你需要调整的超参数以获得最佳结果。我们将区分用于通用表格数据问题的经典机器学习模型和梯度提升模型（在参数空间方面要求更高）。
- en: As for neural networks, we can give you an idea about specific parameters to
    tune when we present the standard models (for instance, the TabNet neural model
    has some specific parameters to set so that it works properly). However, most
    of the optimization on deep neural networks in Kaggle competitions is not performed
    on standard models, but on *custom* ones. Consequently, apart from basic learning
    parameters such as the learning rate and the batch size, optimization in neural
    networks is based on the specific characteristics of the neural architecture of
    your model. You have to deal with the problem in an ad hoc way. Near the end of
    the chapter, we will discuss an example of **neural architecture search** (**NAS**)
    using KerasTuner ([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 至于神经网络，当我们介绍标准模型时（例如，TabNet神经网络模型有一些特定的参数需要设置，以便它能够正常工作），我们可以给你一些关于调整特定参数的想法。然而，Kaggle竞赛中大多数深度神经网络的优化并不是在标准模型上进行的，而是在*自定义*模型上进行的。因此，除了基本的学习参数（如学习率和批量大小）之外，神经网络的优化基于你模型神经架构的特定特征。你必须以专门的方式处理这个问题。在章节的末尾，我们将讨论使用KerasTuner（[https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))进行**神经架构搜索**（**NAS**）的示例。
- en: Linear models
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'The linear models that need to be tuned are usually linear regressions or logistic
    regressions with regularization:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 需要调整的线性模型通常是带有正则化的线性回归或逻辑回归：
- en: '`C`: The range you should search is `np.logspace(-4, 4, 10)`; smaller values
    specify stronger regularization.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：你应该搜索的范围是`np.logspace(-4, 4, 10)`；较小的值指定更强的正则化。'
- en: '`alpha`: You should search the range `np.logspace(-2, 2, 10)`; smaller values
    specify stronger regularization, larger values specify stronger regularization.
    Also take note that higher values take more time to process when using lasso.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：你应该在范围`np.logspace(-2, 2, 10)`中搜索；较小的值指定更强的正则化，较大的值指定更强的正则化。此外，请注意，当使用lasso时，较高的值需要更多的时间来处理。'
- en: '`l1_ratio`: You should pick from the list `[.1, .5, .7, .9, .95, .99, 1]`;
    it applies only to elastic net.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_ratio`：你应该从列表`[.1, .5, .7, .9, .95, .99, 1]`中选择；它仅适用于弹性网络。'
- en: In Scikit-learn, depending on the algorithm, you find either the hyperparameter
    `C` (logistic regression) or `alpha` (lasso, ridge, elastic net).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中，根据算法，你可以找到超参数`C`（逻辑回归）或`alpha`（lasso、ridge、弹性网络）。
- en: Support-vector machines
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**SVMs** are a family of powerful and advanced supervised learning techniques
    for classification and regression that can automatically fit linear and non-linear
    models. Scikit-learn offers an implementation based on `LIBSVM`, a complete library
    of SVM classification and regression implementations, and `LIBLINEAR`, a scalable
    library for linear classification ideal for large datasets, especially sparse
    text-based ones. In their optimization, SVMs strive to separate target classes
    in classification problems using a decision boundary characterized by the largest
    possible margin between classes.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVMs**是一系列用于分类和回归的强大且先进的监督学习技术，可以自动拟合线性和非线性模型。Scikit-learn提供了一个基于`LIBSVM`的实现，这是一个完整的SVM分类和回归实现库，以及`LIBLINEAR`，一个适用于大型数据集（尤其是稀疏文本数据集）的线性分类可扩展库。在它们的优化中，SVMs通过使用具有最大可能类间边界的决策边界来尝试在分类问题中分离目标类别。'
- en: 'Though SVMs work fine with default parameters, they are often not optimal,
    and you need to test various value combinations using cross-validation to find
    the best ones. Listed according to their importance, you have to set the following
    parameters:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SVM使用默认参数可以正常工作，但它们通常不是最优的，你需要通过交叉验证测试各种值组合来找到最佳组合。按照其重要性列出，你必须设置以下参数：
- en: '`C`: The penalty value. Decreasing it makes the margin between classes larger,
    thus ignoring more noise but also making the model more generalizable. A best
    value can normally be found in the range `np.logspace(-3, 3, 7)`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：惩罚值。减小它会使类之间的间隔更大，从而忽略更多的噪声，但也使模型更具泛化能力。最佳值通常可以在范围 `np.logspace(-3, 3,
    7)` 中找到。'
- en: '`kernel`: This parameter will determine how non-linearity will be implemented
    in an SVM and it can be set to `''linear''`, `''poly''`, `''``rbf''`, `''sigmoid''`,
    or a custom kernel. The most commonly used value is certainly `rbf`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel`：此参数将确定在SVM中如何实现非线性。它可以设置为 `''linear''`、`''poly''`、`''rbf''`、`''sigmoid''`
    或自定义核。最常用的值无疑是 `rbf`。'
- en: '`degree`: Works with `kernel=''poly''`, signaling the dimensionality of the
    polynomial expansion. It is ignored by other kernels. Usually, setting its values
    to between `2` and `5` works the best.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`degree`：与 `kernel=''poly''` 一起使用，表示多项式展开的维度。其他核函数会忽略它。通常，将其值设置为 `2` 到 `5`
    之间效果最佳。'
- en: '`gamma`: A coefficient for `''rbf''`, `''``poly''`, and `''sigmoid''`. High
    values tend to fit data in a better way, but can lead to some overfitting. Intuitively,
    we can imagine `gamma` as the influence that a single example exercises over the
    model. Low values make the influence of each example reach further. Since many
    points have to be considered, the SVM curve will tend to take a shape less influenced
    by local points and the result will be a smoother decision contour curve. High
    values of `gamma`, instead, mean the curve takes into account how points are arranged
    locally more and, as a result, you get a more irregular and wiggly decision curve.
    The suggested grid search range for this hyperparameter is `np.logspace(-3, 3,
    7)`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：是 `''rbf''`、`''poly''` 和 `''sigmoid''` 的系数。高值往往能更好地拟合数据，但可能导致一些过拟合。直观上，我们可以将
    `gamma` 视为单个示例对模型的影响。低值使每个示例的影响范围更广。由于必须考虑许多点，SVM曲线将倾向于形成一个受局部点影响较小的形状，结果将得到一个更平滑的决定边界曲线。相反，高值的
    `gamma` 意味着曲线更多地考虑局部点的排列方式，因此你得到一个更不规则和波动的决定曲线。此超参数的建议网格搜索范围是 `np.logspace(-3,
    3, 7)`。'
- en: '`nu`: For regression and classification with nuSVR and nuSVC, this parameter
    sets a tolerance for the training points that are near to the margin and are not
    classified correctly. It helps in ignoring misclassified points just near or on
    the margin, hence it can render the classification decision curve smoother. It
    should be in the range `[0,1]` since it is a proportion relative to your training
    set. Ultimately, it acts like `C`, with high proportions enlarging the margin.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nu`：对于使用 nuSVR 和 nuSVC 的回归和分类，此参数设置接近边界的训练点的容差，这些点没有被正确分类。它有助于忽略刚好在边界附近或被错误分类的点，因此可以使分类决策曲线更平滑。它应该在
    `[0,1]` 范围内，因为它与你的训练集的比例相关。最终，它类似于 `C`，高比例会扩大间隔。'
- en: '`epsilon`: This parameter specifies how much error SVR will accept, by defining
    an `epsilon` large range where no penalty is associated with an incorrect prediction
    of the example during the training of the algorithm. The suggested search range
    is `np.logspace(-4, 2, 7)`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`：此参数指定SVR将接受多少误差，通过定义一个大的 `epsilon` 范围，在该范围内，算法训练期间对示例的错误预测不会关联任何惩罚。建议的搜索范围是
    `np.logspace(-4, 2, 7)`。'
- en: '`penalty`, `loss`, and `dual`: For LinearSVC, these parameters accept the `(''l1'',
    ''squared_hinge'', False)`, `(''l2'', ''hinge'', True)`, `(''l2'', ''squared_hinge'',
    True)`, and `(''l2'', ''squared_hinge'', False)` combinations. The `(''l2'', ''hinge'',
    True)` combination is analogous to the `SVC(kernel=''linear'')` learner.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty`、`loss` 和 `dual`：对于 LinearSVC，这些参数接受 `(''l1'', ''squared_hinge'',
    False)`、`(''l2'', ''hinge'', True)`、`(''l2'', ''squared_hinge'', True)` 和 `(''l2'',
    ''squared_hinge'', False)` 组合。`(''l2'', ''hinge'', True)` 组合类似于 `SVC(kernel=''linear'')`
    学习器。'
- en: It may appear that an SVM has many hyperparameters to set, but many settings
    are specific only to implementations or to kernels, so you only have to select
    the relevant parameters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来支持向量机（SVM）有很多超参数需要设置，但实际上许多设置仅针对特定实现或核函数，因此你只需要选择相关的参数。
- en: Random forests and extremely randomized trees
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林和极端随机树
- en: '*Leo Breiman* and *Adele Cutler* originally devised the idea at the core of
    the random forest algorithm, and the name of the algorithm remains a trademark
    of theirs today (though the algorithm is open source). Random forests are implemented
    in Scikit-learn as `RandomForestClassifier` or `RandomForestRegressor`.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*Leo Breiman* 和 *Adele Cutler*最初设计了随机森林算法核心的想法，并且算法的名字至今仍然是他们的商标（尽管算法是开源的）。随机森林在Scikit-learn中实现为`RandomForestClassifier`或`RandomForestRegressor`。'
- en: A random forest works in a similar way to bagging, also devised by Leo Breiman,
    but operates only using binary split decision trees, which are left to grow to
    their extremes. Moreover, it samples the cases to be used in each of its models
    using **bootstrapping**. As the tree is grown, at each split of a branch, the
    set of variables considered for the split is drawn randomly, too.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的工作方式与袋装法类似，也是由Leo Breiman提出的，但它仅使用二分分割决策树，这些树被允许生长到极端。此外，它使用**重抽样**来为每个模型中的案例进行采样。当树生长时，在每次分支的分割中，考虑用于分割的变量集合也是随机抽取的。
- en: 'This is the secret at the heart of the algorithm: it ensembles trees that,
    due to different samples and variables considered at the splits, are very different
    from each other. As they are different, they are also uncorrelated. This is beneficial
    because when the results are ensembled, much variance is ruled out, as the extreme
    values on both sides of a distribution tend to balance out. In other words, bagging
    algorithms guarantee a certain level of diversity in the predictions, allowing
    them to develop rules that a single learner (such as a decision tree) might not
    come across. All this diversity is useful because it helps in building a distribution
    whose average is a better predictor than any of the individual trees in the ensemble.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是算法核心的秘密：它组合了由于在不同分割点考虑了不同的样本和变量，彼此之间非常不同的树。由于它们不同，它们也是不相关的。这很有益处，因为当结果被组合时，可以排除很多方差，因为分布两端的极端值往往相互平衡。换句话说，袋装算法保证了预测的一定多样性，使得它们可以发展出单个学习器（如决策树）可能不会遇到的规则。所有这种多样性都是有用的，因为它有助于构建一个平均值为更好的预测器，比集成中的任何单个树都要好。
- en: '**Extra Trees** (also known as **extremely randomized trees**), represented
    in Scikit-learn by the `ExtraTreesClassifier`/`ExtraTreesRegressor` classes, are
    a more randomized kind of random forest that produces a lower variance in the
    estimates at the cost of greater bias of the estimators. However, when it comes
    to CPU efficiency, Extra Trees can deliver a considerable speed-up compared to
    random forests, so they can be ideal when you are working with large datasets
    in terms of both examples and features. The reason for the resulting higher bias
    but better speed is the way splits are built in an Extra Tree. Random forests,
    after drawing a random set of features to be considered for splitting a branch
    of a tree, carefully search among them for the best values to assign to each branch.
    By contrast, in Extra Trees, both the set of candidate features for the split
    and the actual split value are decided completely randomly. So, there’s no need
    for much computation, though the randomly chosen split may not be the most effective
    one (hence the bias).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**Extra Trees**（也称为**极端随机树**），在Scikit-learn中由`ExtraTreesClassifier`/`ExtraTreesRegressor`类表示，是一种更随机的随机森林，它以更大的偏差为代价，在估计中产生更低的方差。然而，当涉及到CPU效率时，Extra
    Trees与随机森林相比可以提供相当的速度提升，因此在处理大型数据集（包括示例和特征）时，它们可能是理想的。导致更高偏差但速度更好的原因是Extra Tree中构建分割的方式。随机森林在为树的分支分割抽取一个随机特征集后，会仔细搜索这些特征以找到分配给每个分支的最佳值。相比之下，在Extra
    Trees中，分割的候选特征集和实际的分割值都是完全随机决定的。因此，不需要太多的计算，尽管随机选择的分割可能不是最有效的（因此有偏差）。'
- en: 'For both algorithms, the key hyperparameters that should be set are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种算法，应该设置的关键超参数如下：
- en: '`max_features`: This is the number of sampled features that are present at
    every split, which can determine the performance of the algorithm. The lower the
    number, the speedier, but with higher bias.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`: 这是每个分割中存在的采样特征的数量，这可以确定算法的性能。数字越低，速度越快，但偏差越高。'
- en: '`min_samples_leaf`: This allows you to determine the depth of the trees. Large
    numbers diminish the variance and increase the bias.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`: 这允许你确定树的深度。大数值会减少方差并增加偏差。'
- en: '`bootstrap`: This is a Boolean that allows bootstrapping.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`: 这是一个布尔值，允许进行重抽样。'
- en: '`n_estimators`: This is the number of trees. Remember that the more trees the
    better, though there is a threshold beyond which we get diminishing returns depending
    on the data problem. Also, this comes at a computational cost that you have to
    take into account based on the resources you have available.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这是树的数量。记住，树越多越好，尽管存在一个阈值，超过这个阈值，根据数据问题，我们会得到递减的回报。此外，这会带来计算成本，你必须根据你拥有的资源来考虑。'
- en: Extra Trees are a good alternative to random forests, especially when the data
    you have is particularly noisy. Since they trade some variance reduction for more
    bias given their random choice of splits, they tend to overfit less on important
    yet noisy features that would otherwise dominate the splits in a random forest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 额外树是随机森林的良好替代品，尤其是在你拥有的数据特别嘈杂的情况下。由于它们在随机选择分割时牺牲了一些方差减少以换取更多的偏差，它们在重要但嘈杂的特征上不太容易过拟合，这些特征在其他情况下会主导随机森林的分割。
- en: Gradient tree boosting
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Gradient tree boosting or **gradient boosting decision trees** (**GBDT**) is
    an improved version of boosting (boosting works by fitting a sequence of weak
    learners on reweighted versions of the data). Like AdaBoost, GBDT is based on
    a gradient descent function. The algorithm has proven to be one of the most proficient
    ones from the family of models that are based on ensembles, though it is characterized
    by an increased variance of estimates, more sensitivity to noise in data (both
    problems can be mitigated by using subsampling), and significant computational
    costs due to non-parallel operations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树或**梯度提升决策树**（**GBDT**）是提升算法的改进版本（提升算法通过在数据的重新加权版本上拟合一系列弱学习器）。像AdaBoost一样，GBDT基于梯度下降函数。该算法已被证明是基于集成模型家族中最有效的一种，尽管它以估计的方差增加、对数据噪声的敏感性增加（这两个问题可以通过使用子采样来缓解）以及由于非并行操作而产生的显著计算成本为特征。
- en: Apart from deep learning, gradient boosting is the most developed machine learning
    algorithm. Since AdaBoost and the initial gradient boosting implementation, as
    developed by *Jerome Friedman*, various other implementations of the algorithms
    appeared, the most recent ones being XGBoost, LightGBM, and CatBoost.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了深度学习之外，梯度提升是最发达的机器学习算法。自从AdaBoost和由*杰罗姆·弗里德曼*开发的初始梯度提升实现以来，出现了各种算法的其他实现，最新的包括XGBoost、LightGBM和CatBoost。
- en: LightGBM
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM
- en: 'The high-performance LightGBM algorithm ([https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM))
    is capable of being distributed on multiple computers and handling large amounts
    of data quickly. It was developed by a team at Microsoft as an open-source project
    on GitHub (there is also an academic paper: [https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能的LightGBM算法([https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM))能够分布到多台计算机上，并快速处理大量数据。它是由微软的一个团队作为GitHub上的开源项目开发的（还有一个学术论文：[https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html))。
- en: LightGBM is based on decision trees, like XGBoost, but it follows a different
    strategy. While XGBoost uses decision trees to split on a variable and explore
    different tree splits at that variable (the **level-wise** tree growth strategy),
    LightGBM concentrates on one split and goes on splitting from there in order to
    achieve a better fit (the **leaf-wise** tree growth strategy). This allows LightGBM
    to quickly reach a good fit of the data, and to generate alternative solutions
    compared to XGBoost (which is good, if you expect to blend the two solutions together
    in order to reduce the variance of the estimates). Algorithmically speaking, if
    we think of the structure of splits operated by a decision tree as a graph, XGBoost
    pursues a *breadth-first* search (BFS) and LightGBM a *depth-first* search (DFS).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM基于决策树，就像XGBoost一样，但它遵循不同的策略。虽然XGBoost使用决策树在变量上进行分割并探索该变量的不同树分割（**按层**树增长策略），LightGBM专注于一个分割，并从这里继续分割以实现更好的拟合（**按叶**树增长策略）。这使得LightGBM能够快速达到数据的良好拟合，并生成与XGBoost相比的替代解决方案（如果你预计将两种解决方案结合起来以减少估计的方差，这是好的）。从算法的角度来看，如果我们把决策树操作的分割结构看作一个图，XGBoost追求的是*广度优先搜索*（BFS），而LightGBM追求的是*深度优先搜索*（DFS）。
- en: 'Tuning LightGBM may appear daunting; it has more than a hundred parameters
    to tune that you can explore at this page: [https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)
    (also here: [https://lightgbm.readthedocs.io/en/latest/Parameters.html](https://lightgbm.readthedocs.io/en/latest/Parameters.html)).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 调整LightGBM可能看起来令人畏惧；它有超过一百个可调整的参数，您可以在本页面上探索这些参数：[https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)（也在这里：[https://lightgbm.readthedocs.io/en/latest/Parameters.html](https://lightgbm.readthedocs.io/en/latest/Parameters.html)）。
- en: 'As a rule of thumb, you should focus on the following hyperparameters, which
    usually have the most impact on the results:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项经验法则，你应该关注以下超参数，它们通常对结果影响最大：
- en: '`n_estimators`: An integer between 10 and 10,000 that sets the number of iterations.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：一个介于10和10,000之间的整数，用于设置迭代次数。'
- en: '`learning_rate`: A real number between 0.01 and 1.0, usually sampled from a
    log-uniform distribution. It represents the step size of the gradient descent
    procedure that computes the weights for the summed ensemble of all the iterations
    of the algorithm up to this point.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：一个介于0.01和1.0之间的实数，通常从对数均匀分布中采样。它表示梯度下降过程计算权重时的步长，该过程计算算法到这一点为止所有迭代的加权和。'
- en: '`max_depth`: An integer between 1 and 16, representing the maximum number of
    splits on features. Setting it to a number below 0 allows the maximum possible
    number of splits, usually risking overfitting to data.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：一个介于1和16之间的整数，表示特征上的最大分割数。将其设置为小于0的数字允许最大可能的分割数，通常冒着对数据进行过拟合的风险。'
- en: '`num_leaves`: An integer between 2 and 2^`max_depth`, representing the number
    of final leaves each tree will have at most.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`：一个介于2和`max_depth`的2次幂之间的整数，表示每棵树最多拥有的最终叶子数。'
- en: '`min_data_in_leaf`: An integer between 0 and 300 that determines the minimum
    number of data points in one leaf.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_data_in_leaf`：一个介于0和300之间的整数，用于确定一个叶子中数据点的最小数量。'
- en: '`min_gain_to_split`: A float between 0 and 15; it sets the minimum gain of
    the algorithm for tree partitioning. By setting this parameter, you can avoid
    unnecessary tree splits and thus reduce overfitting (it corresponds to the `gamma`
    parameter in XGBoost).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_gain_to_split`：一个介于0和15之间的浮点数；它设置了算法进行树分割的最小增益。通过设置此参数，你可以避免不必要的树分割，从而减少过拟合（它对应于XGBoost中的`gamma`参数）。'
- en: '`max_bin`: An integer between 32 and 512 that sets the maximum number of bins
    that feature values will be bucketed into. Having this parameter larger than the
    default value of 255 implies more risk of producing overfitting results.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_bin`：一个介于32和512之间的整数，用于设置特征值将被分桶的最大数量。如果此参数大于默认值255，则意味着产生过拟合结果的风险更高。'
- en: '`subsample`: A real number between 0.01, and 1.0, representing the portion
    of the sample to be used in training.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：一个介于0.01和1.0之间的实数，表示用于训练的样本部分。'
- en: '`subsample_freq`: An integer between 0 and 10 specifying the frequency, in
    terms of iterations, at which the algorithm will subsample the examples.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample_freq`：一个介于0和10之间的整数，指定算法在迭代过程中进行子样本采样的频率。'
- en: Note that, if set to zero, the algorithm will ignore any value given to the
    `subsample` parameter. In addition, it is set to zero by default, therefore just
    setting the `subsample` parameter won’t work.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果设置为0，算法将忽略对`subsample`参数给出的任何值。此外，它默认设置为0，因此仅设置`subsample`参数将不起作用。
- en: '`feature_fraction`: A real number between 0.1 and 1.0 allowing you to specify
    the portion of features to be subsampled. Subsampling the features is another
    way to allow more randomization to play a role in the training, fighting noise
    and multicollinearity present in the features.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_fraction`：一个介于0.1和1.0之间的实数，允许您指定要子样本的特征部分。对特征进行子样本是允许更多随机化在训练中发挥作用，以对抗特征中存在的噪声和多重共线性。'
- en: '`subsample_for_bin`: An integer between 30 and the number of examples. This
    sets the number of examples that are sampled for the construction of histogram
    bins.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample_for_bin`：一个介于30和示例数量之间的整数。这设置了用于构建直方图桶的示例数量。'
- en: '`reg_lambda`: A real number between 0 and 100.0 that sets the L2 regularization.
    Since it is more sensitive to the scale than to the exact number of the parameter,
    it is usually sampled from a log-uniform distribution.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_lambda`：一个介于0和100.0之间的实数，用于设置L2正则化。由于它对尺度的敏感性大于对参数确切数量的敏感性，它通常从对数均匀分布中采样。'
- en: '`reg_alpha`: A real number between 0 and 100.0, usually sampled from a log-uniform
    distribution, which sets the L1 regularization.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_alpha`：一个介于 0 和 100.0 之间的实数，通常从对数均匀分布中采样，用于设置 L1 正则化。'
- en: '`scale_pos_weight`: A real number between 1e-6 and 500, better sampled from
    the log-uniform distribution. The parameter weights the positive cases (thus effectively
    upsampling or downsampling) against the negative cases, which are kept to the
    value of 1.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`：一个介于 1e-6 和 500 之间的实数，最好从对数均匀分布中采样。该参数对正例进行加权（从而有效地上采样或下采样）以对抗负例，负例的值保持为
    1。'
- en: 'Although the number of hyperparameters to tune when using LightGBM may appear
    daunting, in reality only a few of them matter a lot. Given a fixed number of
    iterations and learning rate, just a few are the most impactful (`feature_fraction`,
    `num_leaves`, `subsample`, `reg_lambda`, `reg_alpha`, `min_data_in_leaf`), as
    explained in this blog article by *Kohei Ozaki*, a Kaggle Grandmaster: [https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258).
    Kohei Ozaki leverages this fact in order to create a fast-tuning procedure for
    Optuna (you’ll find more on the Optuna optimizer at the end of this chapter).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在使用 LightGBM 时需要调整的超参数数量可能看起来令人畏惧，但事实上只有少数几个非常重要。给定固定的迭代次数和学习率，只有少数几个是最有影响力的（`feature_fraction`、`num_leaves`、`subsample`、`reg_lambda`、`reg_alpha`、`min_data_in_leaf`），正如
    Kaggle 大师 Kohei Ozaki 在其博客文章中解释的那样：[https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)。Kohei
    Ozaki 利用这一事实来为 Optuna 创建快速调整程序（你将在本章末尾找到更多关于 Optuna 优化器的信息）。
- en: XGBoost
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost ([https://github.com/dmlc/XGBoost](https://github.com/dmlc/XGBoost))
    stands for **eXtreme Gradient Boosting**. It is an open-source project that is
    not part of Scikit-learn, though it has recently been expanded by a Scikit-learn
    wrapper interface that makes it easier to incorporate XGBoost into a Scikit-learn-style
    data pipeline.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost ([https://github.com/dmlc/XGBoost](https://github.com/dmlc/XGBoost))
    代表着**极梯度提升**。这是一个开源项目，虽然它不是 Scikit-learn 的一部分，但最近通过 Scikit-learn 包装接口进行了扩展，这使得将其集成到
    Scikit-learn 风格的数据管道中变得更加容易。
- en: The XGBoost algorithm gained momentum and popularity in 2015 data science competitions,
    such as those on Kaggle and the KDD Cup 2015\. As the creators (*Tianqui Chen*,
    *Tong He*, and *Carlos Guestrin*) report in papers they wrote on the algorithm,
    out of 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost
    as a standalone solution or as part of an ensemble of multiple different models.
    Since then, the algorithm has always retained a strong appeal among the community
    of data scientists, though it struggled to keep pace with the innovation brought
    about by other GBM implementations such as LightGBM and CatBoost.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 算法在 2015 年的数据科学竞赛中获得了动力和人气，例如 Kaggle 和 KDD Cup 2015 的竞赛。正如算法的创造者（陈天奇、何通和卡洛斯·古埃斯特林）在关于该算法的论文中所报告的那样，在
    2015 年 Kaggle 举办的 29 个挑战中，有 17 个获胜方案使用了 XGBoost 作为独立解决方案或作为多个不同模型的集成的一部分。从那时起，尽管它在与
    LightGBM 和 CatBoost 等其他 GBM 实现的创新竞争中有些吃力，但该算法在数据科学家社区中始终保持着强大的吸引力。
- en: Aside from good performance both in terms of accuracy and computational efficiency,
    XGBoost is also a *scalable* solution, using at best multi-core processors as
    well as distributed machines.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在准确性和计算效率方面都表现出良好的性能外，XGBoost 还是一个**可扩展**的解决方案，它最好地利用了多核处理器以及分布式机器。
- en: 'XGBoost represents a new generation of GBM algorithms thanks to important tweaks
    to the initial tree boost GBM algorithm:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 通过对初始树提升 GBM 算法的重要调整，代表着新一代的 GBM 算法：
- en: Sparsity-awareness; it can leverage sparse matrices, saving both memory (no
    need for dense matrices) and computation time (zero values are handled in a special
    way).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏感知；它可以利用稀疏矩阵，节省内存（不需要密集矩阵）和计算时间（零值以特殊方式处理）。
- en: Approximate tree learning (weighted quantile sketch), which produces similar
    results but in much less time compared to the classical complete explorations
    of possible branch cuts.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似树学习（加权分位数草图），与经典的可能分支切割的完整探索相比，在更短的时间内产生类似的结果。
- en: Parallel computing on a single machine (using multi-threading during the search
    for the best split) and, similarly, distributed computations on multiple machines.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单台机器上进行并行计算（在搜索最佳分割时使用多线程）以及类似地，在多台机器上进行分布式计算。
- en: Out-of-core computations on a single machine, leveraging a data storage solution
    called **column block**. This arranges data on a disk by columns, thus saving
    time by pulling data from the disk in the way the optimization algorithm (which
    works on column vectors) expects it.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单机上执行离核计算，利用一种称为**列块**的数据存储解决方案。这种方式通过列在磁盘上排列数据，从而通过以优化算法（该算法在列向量上工作）期望的方式从磁盘中拉取数据来节省时间。
- en: XGBoost can also deal with missing data in an effective way. Other tree ensembles
    based on standard decision trees require missing data first to be imputed using
    an off-scale value, such as a negative number, in order to develop an appropriate
    branching of the tree to deal with missing values.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost也可以有效地处理缺失数据。其他基于标准决策树的树集成方法需要首先使用一个离群值（如负数）来填充缺失数据，以便开发出适当的树分支来处理缺失值。
- en: 'As for XGBoost’s parameters ([https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)),
    we have decided to highlight a few key ones you will find across competitions
    and projects:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于XGBoost的参数 ([https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html))，我们决定突出一些你在竞赛和项目中都会遇到的关键参数：
- en: '`n_estimators`: Usually an integer ranging from 10 to 5,000.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：通常是一个介于10到5,000之间的整数。'
- en: '`learning_rate`: A real number ranging from 0.01 to 1.0, better sampled from
    the log-uniform distribution.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：一个介于0.01到1.0之间的实数，最好从对数均匀分布中采样。'
- en: '`min_child_weight`: Usually an integer between 1 and 10.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_child_weight`：通常是一个介于1到10之间的整数。'
- en: '`max_depth`: Usually an integer between 1 and 50.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：通常是一个介于1到50之间的整数。'
- en: '`max_delta_step`: Usually an integer sampled between 0 and 20, representing
    the maximum delta step we allow for each leaf output.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_delta_step`：通常是一个介于0到20之间的整数，表示我们允许每个叶输出允许的最大delta步长。'
- en: '`subsample`: A real number from 0.1 to 1.0 indicating the proportion of examples
    to be subsampled.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：一个介于0.1到1.0之间的实数，表示要子采样的样本比例。'
- en: '`colsample_bytree`: A real number from 0.1 to 1.0 indicating the subsample
    ratio of columns by tree.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：一个介于0.1到1.0之间的实数，表示按树对列的子采样比例。'
- en: '`colsample_bylevel`: A real number from 0.1 to 1.0 indicating the subsample
    ratio by level in trees.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bylevel`：一个介于0.1到1.0之间的实数，表示树中按层级的子采样比例。'
- en: '`reg_lambda`: A real number between 1e-9 and 100.0, preferably sampled from
    the log-uniform distribution. This parameter controls the L2 regularization.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_lambda`：一个介于1e-9和100.0之间的实数，最好从对数均匀分布中采样。此参数控制L2正则化。'
- en: '`reg_alpha`: A real number between 1e-9 and 100.0, preferably sampled from
    the log-uniform distribution. This parameter controls the L1 regularization.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_alpha`：一个介于1e-9和100.0之间的实数，最好从对数均匀分布中采样。此参数控制L1正则化。'
- en: '`gamma`: Specifying the minimum loss reduction for tree partitioning, this
    parameter requires a real number between 1e-9 and 0.5, preferably sampled from
    the log-uniform distribution.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：指定树分区所需的最小损失减少量，此参数需要一个介于1e-9和0.5之间的实数，最好从对数均匀分布中采样。'
- en: '`scale_pos_weight`: A real number between 1e-6 and 500.0, preferably sampled
    from the log-uniform distribution, which represents a weight for the positive
    class.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`：一个介于1e-6和500.0之间的实数，最好从对数均匀分布中采样，它代表正类的一个权重。'
- en: Like LightGBM, XGBoost also has many similar hyperparameters to tune, hence
    all of the considerations previously made for LightGBM are also valid for XGBoost.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与LightGBM类似，XGBoost也有许多类似的超参数需要调整，因此之前为LightGBM做出的所有考虑也适用于XGBoost。
- en: CatBoost
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CatBoost
- en: In July 2017, Yandex, the Russian search engine, made another interesting GBM
    algorithm public, CatBoost ([https://catboost.ai/](https://catboost.ai/)), whose
    name comes from putting together the two words “Category” and “Boosting.” In fact,
    its strong point is its ability to handle categorical variables, which make up
    most of the information in most relational databases, by adopting a mixed strategy
    of one-hot encoding and target encoding. Target encoding is a way to express categorical
    levels by assigning them an appropriate numeric value for the problem at hand;
    more on this can be found in *Chapter 7*, *Modeling for Tabular Competitions*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年7月，俄罗斯搜索引擎Yandex公开了另一个有趣的GBM算法CatBoost ([https://catboost.ai/](https://catboost.ai/))，其名称来源于将“Category”和“Boosting”两个词组合在一起。实际上，它的优势在于其处理分类变量的能力，这些变量构成了大多数关系数据库中的大部分信息，它通过采用一热编码和目标编码的混合策略来实现。目标编码是一种通过为特定问题分配适当的数值来表示分类级别的方法；更多关于这一点可以在*第7章*，*表格竞赛建模*中找到。
- en: The idea used by CatBoost to encode categorical variables is not new, but it
    is a kind of feature engineering that has been used before, mostly in data science
    competitions. Target encoding, also known as likelihood encoding, impact coding,
    or mean encoding, is simply a way to transform your labels into a number based
    on their association with the target variable. If you have a regression, you could
    transform labels based on the mean target value typical of that level; if it is
    a classification, it is simply the probability of classification of your target
    given that label (the probability of your target conditional on each category
    value). It may appear a simple and smart feature engineering trick but it has
    side effects, mostly in terms of overfitting, because you are taking information
    from the target into your predictors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 用于编码分类变量的想法并不新颖，但它是一种之前已经使用过的特征工程方法，主要在数据科学竞赛中使用。目标编码，也称为似然编码、影响编码或均值编码，简单来说，是一种根据与目标变量的关联将标签转换为数字的方法。如果你有一个回归，你可以根据该级别的典型目标均值来转换标签；如果是分类，那么就是给定该标签的目标分类概率（每个类别值的条件目标概率）。这可能看起来是一个简单而聪明的特征工程技巧，但它有副作用，主要是过拟合，因为你在预测器中获取了来自目标的信息。
- en: 'CatBoost has quite a few parameters (see [https://catboost.ai/en/docs/references/training-parameters/](https://catboost.ai/en/docs/references/training-parameters/)).
    We have limited our discussion to the eight most important ones:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 有相当多的参数（见[https://catboost.ai/en/docs/references/training-parameters/](https://catboost.ai/en/docs/references/training-parameters/)）。我们只讨论了其中最重要的八个：
- en: '`iterations`: Usually an integer between 10 and 1,000, but it can increase
    based on the problem.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterations`: 通常是一个介于 10 和 1,000 之间的整数，但根据问题可以增加。'
- en: '`depth`: An integer between 1 and 8; usually higher values require longer fitting
    times and do not produce better results.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth`: 一个介于 1 和 8 之间的整数；通常较高的值需要更长的拟合时间，并且不会产生更好的结果。'
- en: '`learning_rate`: A real value between 0.01 and 1.0, better sampled from the
    log-uniform distribution.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`: 一个介于 0.01 和 1.0 之间的实数值，最好从对数均匀分布中采样。'
- en: '`random_strength`: A real number log-linearly sampled from the range 1e-9 to
    10.0, which specifies the randomness level for scoring splits.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_strength`: 从 1e-9 到 10.0 的范围内以对数线性方式采样的实数，它指定了评分分割的随机水平。'
- en: '`bagging_temperature`: A real value between 0.0 and 1.0 that sets the Bayesian
    bootstrap.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bagging_temperature`: 一个介于 0.0 和 1.0 之间的实数值，用于设置贝叶斯自助抽样。'
- en: '`border_count`: An integer between 1 and 255 indicating the splits for numerical
    features.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`border_count`: 一个介于 1 和 255 之间的整数，表示数值特征的分割。'
- en: '`l2_leaf_reg`: An integer between 2 and 30; the value for L2 regularization.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2_leaf_reg`: 一个介于 2 和 30 之间的整数；L2 正则化的值。'
- en: '`scale_pos_weight`: A real number between 0.01 and 10.0 representing the weight
    for the positive class.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`: 一个介于 0.01 和 10.0 之间的实数，表示正类权重。'
- en: Even if CatBoost may appear to be just another GBM implementation, it has quite
    a few differences (highlighted also by the different parameters being used) that
    may provide great help in a competition, both as a single-model solution and as
    a model integrated into a larger ensemble.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 CatBoost 可能看起来只是另一种 GBM 实现，但它有一些差异（也通过不同的参数使用突出显示），这些差异在比赛中可能提供极大的帮助，无论是作为单一模型解决方案还是作为集成模型的一部分。
- en: HistGradientBoosting
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HistGradientBoosting
- en: 'Recently, Scikit-learn has introduced a new version of gradient boosting inspired
    by LightGBM’s binned data and histograms (see this presentation at EuroPython
    by *Olivier Grisel*: [https://www.youtube.com/watch?v=urVUlKbQfQ4](https://www.youtube.com/watch?v=urVUlKbQfQ4)).
    Either as a classifier (`HistGradientBoostingClassifier`) or a regressor (`HistGradientBoostingRegressor`),
    it can be used for enriching ensembles with different models and it presents a
    much shorter and essential range of hyperparameters to be tuned:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Scikit-learn 引入了一个新的梯度提升版本，灵感来自 LightGBM 的分箱数据和直方图（见 EuroPython 上的这个演示：[https://www.youtube.com/watch?v=urVUlKbQfQ4](https://www.youtube.com/watch?v=urVUlKbQfQ4)）。无论是作为分类器（`HistGradientBoostingClassifier`）还是回归器（`HistGradientBoostingRegressor`），它都可以用于用不同模型丰富集成，并且它提供了一个更短、更关键的超参数范围需要调整：
- en: '`learning_rate`: A real number between 0.01 and 1.0, usually sampled from a
    log-uniform distribution.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`: 一个介于 0.01 和 1.0 之间的实数，通常从对数均匀分布中采样。'
- en: '`max_iter`: An integer that can range from 10 to 10,000.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_iter`: 一个介于 10 到 10,000 之间的整数。'
- en: '`max_leaf_nodes`: An integer from 2 to 500\. It interacts with `max_depth`;
    it is advisable to set only one of the two and leave the other set to `None`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`：一个介于2到500之间的整数。它与`max_depth`相互作用；建议只设置这两个参数中的一个，并将另一个设置为`None`。'
- en: '`max_depth`: An integer between 2 and 12.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：一个介于2到12之间的整数。'
- en: '`min_samples_leaf`: An integer between 2 and 300.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`：一个介于2到300之间的整数。'
- en: '`l2_regularization`: A float between 0.0 and 100.0.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2_regularization`：一个介于0.0到100.0之间的浮点数。'
- en: '`max_bins`: An integer between 32 and 512.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_bins`：一个介于32到512之间的整数。'
- en: Even if Scikit-learn’s `HistGradientBoosting` is nothing too different from
    LightGBM or XGBoost, it does provide a different way to implement GBMs in a competition,
    and models built by `HistGradientBoosting` may provide a contribution when ensembling
    multiple predictions, such as in blending and stacking.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 即使Scikit-learn的`HistGradientBoosting`与LightGBM或XGBoost没有太大区别，但它确实提供了一种在比赛中实现GBM的不同方法，由`HistGradientBoosting`构建的模型在集成多个预测时（如混合和堆叠）可能会提供一些贡献。
- en: Having reached the end of this section, you should be more familiar with the
    most common machine learning algorithms (only deep learning solutions have not
    been discussed) and their most important hyperparameters to tune, which will help
    you in building an outstanding solution in a Kaggle competition. Knowing the basic
    optimization strategies, usable algorithms, and their key hyperparameters is just
    a starting point. In the next section, we will begin an in-depth discussion about
    how to tune them more optimally using Bayesian optimization.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到达本节末尾，你应该对最常见的机器学习算法（尚未讨论深度学习解决方案）及其最重要的超参数有了更熟悉的了解，这将有助于你在Kaggle竞赛中构建出色的解决方案。了解基本的优化策略、可用的算法及其关键超参数只是一个起点。在下一节中，我们将开始深入讨论如何使用贝叶斯优化更优地调整它们。
- en: '![](img/Alberto_Danese.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Alberto_Danese.png)'
- en: Alberto Danese
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Alberto Danese
- en: '[https://www.kaggle.com/albedan](https://www.kaggle.com/albedan)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/albedan](https://www.kaggle.com/albedan)'
- en: Our second interview of the chapter is with Alberto Danese, Head of Data Science
    at Nexi, an Italian credit card and digital payments company. A Competitions Grandmaster
    who joined the platform in 2015, he obtained most of his gold medals as a solo
    competitor.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章的第二位访谈对象是Alberto Danese，他是意大利信用卡和数字支付公司Nexi的数据科学负责人。这位在2015年加入平台的竞赛大师，作为单独的竞争者获得了大部分金牌。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在Kaggle上，你在技术和解决方法方面有什么专长？
- en: '*I’ve always worked in the Financial Services industry, dealing mostly with
    structured data, and I do prefer competitions that belong to this category. I
    enjoy being able to have a practical grasp of what the data is all about and doing
    some smart feature engineering in order to squeeze every bit of information out
    of the data.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*我一直从事金融服务行业，主要处理结构化数据，我更倾向于这类比赛。我喜欢能够实际掌握数据的本质，并做一些智能的特征工程，以从数据中提取每一丝信息。*'
- en: '*Technically speaking, I’ve got good experience with classical ML libraries
    and especially with Gradient Boosting Decision Trees: the most common libraries
    (XGBoost, LightGBM, CatBoost) are always my first choice.*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*从技术角度讲，我在经典机器学习库方面有丰富的经验，尤其是梯度提升决策树：最常用的库（XGBoost、LightGBM、CatBoost）总是我的首选。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*I always spend a lot of time just exploring the data and trying to figure
    out what the problem that the sponsor is actually trying to solve with machine
    learning is. Different from what newbies usually think about Kaggle, I don’t spend
    so much time on all the “tweaking” of the specific ML algorithm – and apparently
    this approach has paid off!*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*我总是花很多时间探索数据，试图弄清楚赞助商实际上想用机器学习解决什么问题。与新手通常对Kaggle的看法不同，我不会花太多时间在特定ML算法的所有“调整”上——显然这种方法是有效的！*'
- en: '*In my daily job, understanding the data is also extremely important, but there
    are some additional phases that are completely missing in a Kaggle competition.
    I’ve got to:*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的日常工作中，理解数据也非常重要，但在Kaggle竞赛中却完全缺失了一些额外的阶段。我必须做到：*'
- en: '*Define a business problem to be solved with ML (together with colleagues in
    the business departments)*'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定义一个用ML解决的问题（与业务部门的同事一起）*'
- en: '*Find the data, sometimes also from external data providers*'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**找到数据，有时也来自外部数据提供商**'
- en: '*And when the ML part is done, understand how to put it in production and manage
    the evolutions*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当机器学习部分完成时，理解如何将其投入生产并管理其演变**'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你参加的一个特别具有挑战性的比赛，以及你使用了哪些见解来应对任务。
- en: '*I enjoyed the*TalkingData AdTracking Fraud Detection Challenge*, with which
    I became a Grandmaster. Besides being on an extremely interesting topic (fighting
    fraud from click-farms), it really pushed me to do efficient feature engineering,
    as the volumes were huge (more than 100M labeled rows) and cutting on computation
    times was key in order to test different approaches. It also forced me to understand
    how to exploit lag/lead features (and other window functions) in the best way,
    in order to create a sort of time series in an otherwise classical ML problem.*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我很喜欢参加*TalkingData AdTracking Fraud Detection Challenge*，通过这个挑战我成为了大师。除了它是一个非常有趣的话题（打击点击农场中的欺诈）之外，它还真正推动了我进行高效的特征工程，因为数据量巨大（超过1亿个标记行），减少计算时间对于测试不同的方法至关重要。它还迫使我以最佳方式理解如何利用滞后/领先特征（以及其他窗口函数），以便在本质上是一个经典机器学习问题中创建一种时间序列。
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进步？如果是的话，是如何帮助的？
- en: '*Definitely! Being able to achieve great objective and verifiable results is
    no doubt something that makes a resume stand out. When I was hired by Cerved (a
    marketing intelligence service company) in 2016, the hiring manager was perfectly
    aware of what Kaggle was – and having some real-world projects to talk about during
    an interview is something extremely valuable. For sure Kaggle had an important
    role in the evolution of my career.*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**当然！能够实现伟大的目标和可验证的结果无疑是使简历脱颖而出的因素之一。当我2016年被Cerved（一家市场情报服务公司）雇佣时，招聘经理完全清楚Kaggle是什么——在面试中谈论一些真实世界的项目是非常有价值的。当然，Kaggle在我的职业生涯发展中扮演了重要的角色。**'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kagglers通常忽略了什么？你现在知道的事情，你希望在你最初开始时就知道？
- en: '*I think that everyone just starts coding, maybe forking a public kernel and
    just changing a few lines or parameters. This is perfectly fine at the beginning!
    But you do have to spend a decent amount of time not coding, but studying the
    data and understanding the problem.*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为每个人都是从编码开始的，也许是从一个公共内核开始，只是更改几行或参数。这在开始时是完全可以接受的！但你确实需要花相当多的时间不编码，而是研究数据和理解问题。
- en: What mistakes have you made in competitions in the past?
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*Not sure if it counts as a mistake, but I have often preferred to compete
    solo: on one hand it’s great as it forces you to handle every single aspect of
    a competition, and you’re able to manage your time as you wish. But I’ve really
    enjoyed collaborating with teammates on a couple of competitions as well: I probably
    should consider teaming up more often, as you can learn a lot from collaborating.*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定这算不算一个错误，但我经常更喜欢单独竞争：一方面，这很好，因为它迫使你处理比赛的每一个方面，你可以按照自己的意愿管理时间。但我也非常喜欢与队友在几个比赛中合作：我可能应该更经常地考虑团队合作，因为你可以从合作中学到很多东西。**'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析或机器学习？
- en: '*Besides the* *usual ones, I’ve always been a great fan of* `data.table` *(starting
    from the R version): I think it’s not getting the credit it deserves! It’s really
    a great package when you want to deal with huge data on a local machine.*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那些**通常的**，我一直非常喜欢`data.table`（从R版本开始）：我认为它没有得到应有的认可！当你想在本地机器上处理大量数据时，它真的是一个非常棒的包。
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做最重要的事情是什么？
- en: '*Understand the problem and the data first: don’t start coding right away!*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**首先理解问题和数据：不要立即开始编码！**'
- en: Bayesian optimization
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Leaving behind grid search (feasible only when the space of experiments is limited),
    the usual choice for the practitioner is to apply random search optimization or
    try a **Bayesian optimization** (**BO**) technique, which requires a more complex
    setup.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃网格搜索（仅在实验空间有限时可行），实践者通常会选择应用随机搜索优化或尝试**贝叶斯优化**（**BO**）技术，这需要更复杂的设置。
- en: Originally introduced in the paper *Practical Bayesian optimization of machine
    learning algorithms* by Snoek, J., Larochelle, H., and Adams, R. P. ([http://export.arxiv.org/pdf/1206.2944](http://export.arxiv.org/pdf/1206.2944)),
    the key idea behind Bayesian optimization is that we optimize a **proxy function**
    (also called a **surrogate function**) rather than the true objective function
    (which grid search and random search both do). We do this if there are no gradients,
    if testing the true objective function is costly (if it is not, then we simply
    go for random search), and if the search space is noisy and complex enough.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: TPEs最初在Snoek, J.，Larochelle, H.和Adams, R. P.的论文《Practical Bayesian optimization
    of machine learning algorithms》中提出，该论文的网址为[http://export.arxiv.org/pdf/1206.2944](http://export.arxiv.org/pdf/1206.2944)。贝叶斯优化的关键思想是，我们优化一个**代理函数**（也称为**替代函数**），而不是真正的目标函数（网格搜索和随机搜索都这样做）。我们这样做是因为没有梯度，如果测试真正的目标函数成本高昂（如果不是，那么我们简单地进行随机搜索），以及如果搜索空间是嘈杂且足够复杂的情况下。
- en: Bayesian search balances *exploration* with *exploitation*. At the start, it
    explores randomly, thus training the surrogate function as it goes. Based on that
    surrogate function, the search exploits its initial approximate knowledge of how
    the predictor works in order to sample more useful examples and minimize the cost
    function. As the *Bayesian* part of the name suggests, we are using priors in
    order to make smarter decisions about sampling during optimization. This way,
    we reach a minimization more quickly by limiting the number of evaluations we
    need to make.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯搜索在**探索**和**利用**之间取得平衡。一开始，它随机探索，从而在过程中训练替代函数。基于这个替代函数，搜索利用其对预测器如何工作的初始近似知识，以便采样更有用的示例并最小化成本函数。正如名称中的**贝叶斯**部分所暗示的，我们在优化过程中使用先验来做出更明智的采样决策。这样，我们通过限制需要进行的评估次数来更快地达到最小化。
- en: Bayesian optimization uses an **acquisition function** to tell us how promising
    an observation will be. In fact, to manage the tradeoff between exploration and
    exploitation, the algorithm defines an acquisition function that provides a single
    measure of how useful it would be to try any given point.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化使用一个**获取函数**来告诉我们一个观察结果的前景如何。实际上，为了管理探索和利用之间的权衡，算法定义了一个获取函数，它提供了一个单一指标，说明尝试任何给定点将有多有用。
- en: Usually, Bayesian optimization is powered by Gaussian processes. Gaussian processes
    perform better when the search space has a smooth and predictable response. An
    alternative when the search space is more complex is using tree algorithms (for
    instance, random forests), or a completely different approach called **Tree Parzen
    Estimators** or **Tree-structured Parzen Estimators** (**TPEs**).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，贝叶斯优化由高斯过程提供动力。当搜索空间具有平滑且可预测的响应时，高斯过程表现更好。当搜索空间更复杂时，一个替代方案是使用树算法（例如，随机森林），或者一个完全不同的方法，称为**树帕尔森估计器**或**树结构帕尔森估计器**（**TPEs**）。
- en: Instead of directly building a model that estimates the success of a set of
    parameters, thus acting like an oracle, TPEs estimate the parameters of a multivariate
    distribution that define the best-performing values of the parameters, based on
    successive approximations provided by the experimentations. In this way, TPEs
    derive the best set of parameters by sampling them from a probabilistic distribution,
    and not directly from a machine learning model like Gaussian processes does.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接构建一个估计参数集成功度的模型不同，从而像先知一样行动，TPEs（Tree Parzen Estimators）根据实验提供的连续近似，估计一个多变量分布的参数，这些参数定义了参数的最佳性能值。通过这种方式，TPEs通过从概率分布中采样来推导出最佳参数集，而不是直接从机器学习模型（如高斯过程）中直接推导。
- en: 'We will discuss each of these approaches, first by examining Scikit-optimize
    and KerasTuner, both based on Gaussian processes (Scikit-optimize can also use
    random forests and KerasTuner can use multi-armed bandits), and then Optuna, which
    is principally based on TPE (though it also offers different strategies: [https://optuna.readthedocs.io/en/stable/reference/samplers.html](https://optuna.readthedocs.io/en/stable/reference/samplers.html)).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论这些方法中的每一个，首先通过检查基于高斯过程的 Scikit-optimize 和 KerasTuner，Scikit-optimize 还可以使用随机森林，而
    KerasTuner 可以使用多臂老虎机，然后是主要基于 TPE 的 Optuna（尽管它也提供不同的策略：[https://optuna.readthedocs.io/en/stable/reference/samplers.html](https://optuna.readthedocs.io/en/stable/reference/samplers.html))。
- en: Though Bayesian optimization is considered the state of the art for hyperparameter
    tuning, always keep in mind that for more complex parameter spaces, using Bayesian
    optimization provides no advantage in terms of time and computation spent over
    a solution simply found by random search. For instance, in Google Cloud Machine
    Learning Engine services, the usage of Bayesian optimization is limited to problems
    involving at most sixteen parameters. For larger numbers of parameters, it resorts
    to random sampling.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管贝叶斯优化被认为是超参数调整的当前最佳实践，但始终记住，对于更复杂的参数空间，使用贝叶斯优化在时间和计算上并不比随机搜索简单找到的解决方案有优势。例如，在
    Google Cloud Machine Learning Engine 服务中，贝叶斯优化的使用仅限于涉及最多十六个参数的问题。对于更多参数的情况，它将回退到随机抽样。
- en: Using Scikit-optimize
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Scikit-optimize
- en: Scikit-optimize (`skopt`) has been developed using the same API as Scikit-learn,
    as well as making extensive use of NumPy and SciPy functions. In addition, it
    was created by some of the contributors to the Scikit-learn project, such as *Gilles
    Louppe*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-optimize (`skopt`) 是使用与 Scikit-learn 相同的 API 开发的，同时广泛使用了 NumPy 和 SciPy
    函数。此外，它是由 Scikit-learn 项目的一些贡献者创建的，例如 *Gilles Louppe*。
- en: Based on Gaussian process algorithms, the package is well maintained, though
    sometimes it has to catch up because of improvements on the Scikit-learn, NumPy,
    or SciPy sides. For instance, at the time of writing, in order to run it properly
    on Kaggle Notebooks you have to roll back to older versions of these packages,
    as explained in a GitHub issue ([https://github.com/scikit-optimize/scikit-optimize/issues/981](https://github.com/scikit-optimize/scikit-optimize/issues/981)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该包基于高斯过程算法，维护得很好，尽管有时它必须因为 Scikit-learn、NumPy 或 SciPy 方面的改进而赶上来。例如，在撰写本文时，为了在
    Kaggle 笔记本上正确运行，你必须回滚到这些包的旧版本，正如 GitHub 上的一个问题所解释的 ([https://github.com/scikit-optimize/scikit-optimize/issues/981](https://github.com/scikit-optimize/scikit-optimize/issues/981))。
- en: The package has an intuitive API and it is quite easy to hack it and use its
    functions in custom optimization strategies. Scikit-optimize is also renowned
    for its useful graphical representations. In fact, by visualizing the results
    of an optimization process (using Scikit-optimize’s `plot_objective` function),
    you can figure out whether you can re-define the search space for the problem
    and formulate an explanation of how optimization works for a problem.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 该包具有直观的 API，相当容易对其进行修改并使用其函数在自定义优化策略中。Scikit-optimize 还以其有用的图形表示而闻名。实际上，通过可视化优化过程的结果（使用
    Scikit-optimize 的 `plot_objective` 函数），你可以判断是否可以重新定义问题的搜索空间，并制定一个关于问题优化工作原理的解释。
- en: 'In our worked example, we will refer to the work that can be found in the following
    Kaggle Notebooks:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将参考以下 Kaggle 笔记本中的工作：
- en: '[https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm](https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm](https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm)'
- en: '[https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm](https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm](https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm)'
- en: Our purpose here is to show you how to quickly handle an optimization problem
    for a competition such as *30 Days of ML*, a recent competition that involved
    many Kagglers in learning new skills and applying them in a competition lasting
    30 days. The goal of this competition is to predict the value of an insurance
    claim, so it is a regression problem. You can find out more about this initiative
    and download the data necessary for the example we are going to present (materials
    are always available to the public), by visiting [https://www.kaggle.com/thirty-days-of-ml](https://www.kaggle.com/thirty-days-of-ml).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目的就是向您展示如何快速处理一场比赛，例如 *30 Days of ML* 的优化问题，这是一场最近举办的比赛，许多 Kagglers 参与其中，学习新技能并在为期
    30 天的比赛中将它们应用。这场比赛的目标是预测保险索赔的价值，因此它是一个回归问题。您可以通过访问 [https://www.kaggle.com/thirty-days-of-ml](https://www.kaggle.com/thirty-days-of-ml)
    了解更多关于这个项目的信息并下载我们将要展示的示例所需的数据（材料总是对公众开放）。
- en: 'If you cannot access the data because you have not taken part in the competition
    previously, you can use this Kaggle Dataset: [https://www.kaggle.com/lucamassaron/30-days-of-ml](https://www.kaggle.com/lucamassaron/30-days-of-ml).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法访问数据，因为您之前没有参加过比赛，您可以使用这个 Kaggle 数据集：[https://www.kaggle.com/lucamassaron/30-days-of-ml](https://www.kaggle.com/lucamassaron/30-days-of-ml)。
- en: The following code will present how to load the data for this problem and then
    set up a Bayesian optimization process that will improve the performance of a
    LightGBM model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将展示如何加载此问题的数据，然后设置一个贝叶斯优化过程，该过程将提高 LightGBM 模型的性能。
- en: 'We start by loading the packages:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载所需的包：
- en: '[PRE5]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a next step, we load the data. The data doesn’t need much processing, aside
    from turning some categorical features with alphabetical letters as levels into
    ordered numeric ones:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们加载数据。除了将一些具有字母作为级别的分类特征转换为有序数字之外，数据不需要太多处理：
- en: '[PRE6]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After making the data available, we define a reporting function that can be
    used by Scikit-optimize for various optimization tasks. The function takes the
    data and the optimizer as inputs. It can also handle **callback functions**, which
    are functions that perform actions such as reporting, early stopping based on
    having reached a certain threshold of time spent searching or performance not
    improving (for instance, not seeing improvements for a certain number of iterations),
    or saving the state of the processing after each optimization iteration:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据可用后，我们定义了一个报告函数，该函数可以被 Scikit-optimize 用于各种优化任务。该函数接受数据和优化器作为输入。它还可以处理 **回调函数**，这些函数执行诸如报告、基于达到一定搜索时间阈值或性能没有提高（例如，在特定次数的迭代中看不到改进）的早期停止，或者在每个优化迭代后保存处理状态等操作：
- en: '[PRE7]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We now have to prepare the scoring function (upon which the evaluation is based),
    the validation strategy (based on cross-validation), the model, and the search
    space. For the scoring function, which should be a root mean squared error metric,
    we refer to the practices in Scikit-learn where you always minimize a function
    (if you have to maximize, you minimize its negative).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须准备评分函数（评估基于此），验证策略（基于交叉验证），模型和搜索空间。对于评分函数，它应该是一个均方根误差指标，我们参考了 Scikit-learn
    中的实践，在那里你总是最小化一个函数（如果你必须最大化，你最小化其负值）。
- en: 'The `make_scorer` wrapper can easily replicate such practices:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_scorer` 包装器可以轻松地复制这样的实践：'
- en: '[PRE8]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Setting the search space requires the use of different functions from Scikit-optimize,
    such as `Real`, `Integer`, or `Choice`, each one sampling from a different kind
    of distribution that you define as a parameter (usually the uniform distribution,
    but the log-uniform is also used when you are more interested in the scale effect
    of a parameter than its exact value):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 设置搜索空间需要使用 Scikit-optimize 中的不同函数，例如 `Real`、`Integer` 或 `Choice`，每个函数从您定义的不同类型的分布中进行采样，作为参数（通常是均匀分布，但在您对参数的规模效应比其确切值更感兴趣时，也使用对数均匀分布）：
- en: '[PRE9]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once you have defined:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您已经定义：
- en: Your cross-validation strategy
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的交叉验证策略
- en: Your evaluation metric
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的评估指标
- en: Your base model
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的基础模型
- en: Your hyperparameter search space
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的超参数搜索空间
- en: 'All that is left is just to feed them into your optimization function, `BayesSearchCV`.
    Based on the CV scheme provided, this function will look for the minimum of your
    scoring function based on values within the search space. You can set a maximum
    number of iterations performed, the kind of surrogate function (Gaussian processes
    (`GP`) works on most occasions), and the random seed for reproducibility:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的只是将它们输入到您的优化函数`BayesSearchCV`中。根据提供的CV方案，此函数将根据搜索空间内的值寻找评分函数的最小值。您可以设置最大迭代次数、代理函数的类型（高斯过程`GP`在大多数情况下都适用），以及随机种子以实现可重复性：
- en: '[PRE10]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: At this point, you can start the search using the reporting function we defined
    previously. After a while, the function will return the best parameters for the
    problem.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可以使用我们之前定义的报告函数开始搜索。一段时间后，该函数将返回该问题的最佳参数。
- en: '[PRE11]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the example, we set a limit on operations by specifying a maximum time allowed
    (6 hours) before stopping and reporting the best results. Since the Bayesian optimization
    approach blends together exploration and exploitation of different combinations
    of hyperparameters, stopping at any time will always return the best solution
    found so far (but not necessarily the best one possible). This is because the
    acquisition function will always give priority of exploration to the most promising
    parts of the search space, based on the estimated performances returned by the
    surrogate function and their uncertainty intervals.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，我们通过指定一个最大允许时间（6小时）来限制操作，并在停止和报告最佳结果之前。由于贝叶斯优化方法结合了不同超参数组合的探索和利用，因此任何时间停止都将始终返回迄今为止找到的最佳解决方案（但不一定是可能的最佳解决方案）。这是因为获取函数将始终根据代理函数返回的估计性能及其不确定性区间，优先探索搜索空间中最有希望的部分。
- en: Customizing a Bayesian optimization search
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定制贝叶斯优化搜索
- en: 'The `BayesSearchCV` function offered by Scikit-optimize is certainly convenient,
    because it wraps and arranges all the elements of a hyperparameter search by itself,
    but it also has limitations. For instance, you may find it useful in a competition
    to:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-optimize提供的`BayesSearchCV`函数确实很方便，因为它自己封装并安排了超参数搜索的所有元素，但它也有局限性。例如，您可能发现在比赛中这样做很有用：
- en: Have more control over each search iteration, for instance mixing random search
    and Bayesian search
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每次搜索迭代有更多控制，例如混合随机搜索和贝叶斯搜索
- en: Be able to apply early stopping on algorithms
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在算法上应用早期停止
- en: Customize your validation strategy more
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多地定制您的验证策略
- en: Stop experiments that do not work early (for instance, immediately evaluating
    the performance of the single cross-validation folds when it is available, instead
    of waiting to have all folds averaged at the end)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早期停止无效的实验（例如，当可用时立即评估单个交叉验证折的性能，而不是等待所有折的平均值）
- en: Create clusters of hyperparameter sets that perform in a similar way (for instance,
    in order to create multiple models differing only in the hyperparameters used,
    to be used for a blending ensemble)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建表现相似的超参数集簇（例如，为了创建多个模型，这些模型仅在使用的超参数上有所不同，用于混合集成）
- en: 'Each of these tasks would not be too complex if you could modify the `BayesSearchCV`
    internal procedure. Luckily, Scikit-optimize lets you do just this. In fact, behind
    `BayesSearchCV`, as well as behind other wrappers from the package, there are
    specific minimizing functions that you can use as standalone parts of your own
    search function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能够修改`BayesSearchCV`内部过程，那么这些任务中的每一个都不会太复杂。幸运的是，Scikit-optimize允许您做到这一点。实际上，在`BayesSearchCV`以及该包的其他封装器后面，都有特定的最小化函数，您可以将它们用作您自己的搜索函数的独立部分：
- en: '`gp_minimize`: Bayesian optimization using Gaussian processes'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gp_minimize`：使用高斯过程进行贝叶斯优化'
- en: '`forest_minimize`: Bayesian optimization using random forests or extremely
    randomized trees'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forest_minimize`：使用随机森林或极端随机树进行贝叶斯优化'
- en: '`gbrt_minimize`: Bayesian optimization using gradient boosting'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbrt_minimize`：使用梯度提升的贝叶斯优化'
- en: '`dummy_minimize`: Just random search'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dummy_minimize`：仅仅是随机搜索'
- en: In the following example, we are going to modify the previous search using our
    own custom search function. The new custom function will accept early stopping
    during training and it will prune experiments if one of the fold validation results
    is not a top-performing one.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将修改之前的搜索，使用我们自己的自定义搜索函数。新的自定义函数将在训练期间接受早期停止，并在其中一个折的验证结果不是最佳表现时剪枝实验。
- en: You can find the next example working in a Kaggle Notebook at [https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kaggle笔记本中找到下一个示例：[https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization)。
- en: As in the previous example, we start by importing the necessary packages.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们首先导入必要的包。
- en: '[PRE12]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the same way as before, we upload the data from the *30 Days of ML* competition:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们从*30 Days of ML*竞赛上传数据：
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we set all the necessary elements for a hyperparameter search, that is,
    the scoring function, the validation strategy, the search space, and the machine
    learning model to be optimized. The scoring function and the validation strategy
    will later become the core elements constituting the objective function, the function
    the Bayesian optimization will strive to minimize.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置了进行超参数搜索所需的所有必要元素，即评分函数、验证策略、搜索空间以及要优化的机器学习模型。评分函数和验证策略将后来成为构成目标函数的核心元素，目标函数是贝叶斯优化努力最小化的函数。
- en: '[PRE14]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice this time that we have not included the number of estimators (the `n_estimators`
    parameter) in the search space. Instead, we set it when instantiating the model
    and we enter a high value, since we expect to stop the model early based on a
    validation set.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这次我们没有在搜索空间中包含估计器的数量（即`n_estimators`参数）。相反，我们在实例化模型时设置它，并输入一个高值，因为我们期望根据验证集提前停止模型。
- en: As a next step, you now need to create the objective function. The objective
    function should just accept as input the parameters to be optimized and return
    the resulting score. However, the objective function also needs to accept the
    elements necessary for the search you have just prepared. Naturally, you could
    refer to them from inside the function. However, it is a good practice to take
    them into the function itself, in its internal memory space. This has its advantages;
    for instance, you will make the elements immutable and they will be carried along
    with the objective function (by pickling or if you distribute the search task
    on a multi-processor level). You can obtain this second result by creating a `make`
    function that takes in the elements, with the modified objective function being
    returned by the `make` function. With this simple structure, your objective function
    will incorporate all the elements such as the data and the model, and you will
    only need to pass in the parameters to be tested.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，你现在需要创建目标函数。目标函数应该只接受要优化的参数作为输入，并返回相应的分数。然而，目标函数还需要接受你刚刚准备好的搜索所需的元素。自然地，你可以从函数内部引用它们。然而，将它们带入函数本身，在其内部内存空间中，是一种良好的实践。这有其优点；例如，你会使元素不可变，并且它们将与目标函数（通过序列化或如果你在多处理器级别上分配搜索任务）一起携带。你可以通过创建一个`make`函数来实现第二个结果，该函数接受元素，并通过`make`函数返回修改后的目标函数。通过这种简单的结构，你的目标函数将包含所有元素，如数据和模型，而你只需要传递要测试的参数。
- en: 'Let’s start coding the function. We will stop along the way to discuss some
    relevant aspects:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编写函数。我们将沿途讨论一些相关方面：
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this first part of the function, you simply create an objective function,
    doing cross-validation and fitting the data using early stopping. We have used
    an aggressive early stopping strategy to save time, but you could raise the number
    of patient rounds if you believe that it might work better for your problem. Notice
    that the validation examples are sequentially taken out from the examples in the
    training folds (see how `train_index` and `val_index` are defined in the code),
    leaving the out-of-fold examples (`test_index` derived from the `kf` cross-validation
    splitting) untouched for the final validation. This is important if you do not
    want to incur adaptive overfitting on the data you use for early stopping.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的第一个部分，你只需创建一个目标函数，进行交叉验证并使用提前停止来拟合数据。我们使用了一种激进的提前停止策略以节省时间，但如果你认为它可能对你的问题更有效，你可以增加耐心轮数。请注意，验证示例是按顺序从训练折叠中的示例中取出的（参见代码中`train_index`和`val_index`的定义），而将折叠外的示例（由`kf`交叉验证分割得到的`test_index`）保留用于最终验证。如果你不希望对用于提前停止的数据产生自适应过拟合，这一点很重要。
- en: 'In the next part, before moving on to the cross-validation loop and proceeding
    to the remaining cross-validation folds to be trained and tested, you analyze
    the result obtained by the fold on the out-of-fold set:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，在进入交叉验证循环并继续训练和测试剩余的交叉验证折之前，你分析出在出折集上获得的折的结果：
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that we are keeping a global dictionary, `history`, containing the results
    obtained from each fold up to now. We can compare the results across multiple
    experiments and cross-validations; the cross-validation is reproducible due to
    the random seed, so the results of the same fold are perfectly comparable. If
    the result of the present fold is sub-par compared to the previously obtained
    folds in other iterations (using the bottom quartile as a reference), the idea
    is to stop and return the average of the folds tested so far. The rationale for
    this is that if one fold doesn’t present acceptable results, then the whole cross-validation
    probably won’t either. You can therefore just quit and move on to another set
    of more promising parameters. It is a kind of early stopping on cross-validation
    that should speed up your search and allow you to cover more experiments in less
    time.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在维护一个全局字典`history`，其中包含到目前为止从每个折中获得的成果。我们可以比较多个实验和交叉验证的结果；由于随机种子，交叉验证是可重复的，因此相同折的成果可以完美比较。如果当前折的结果与其他迭代中获得的先前折的结果（以底部四分位数作为参考）相比较差，那么想法是停止并返回迄今为止测试的折的平均值。这样做的原因是，如果一个折没有呈现可接受的结果，那么整个交叉验证可能也不会。因此，你可以直接退出并转向另一组更有希望的参数。这是一种交叉验证的早期停止，应该会加快你的搜索速度，并允许你在更短的时间内覆盖更多实验。
- en: 'Next, using our `make_objective` function, we put together all the elements
    (model, data, search space, validation strategy, and scoring function) into a
    single function, the objective function. As a result, we now have a function that
    only takes in the parameters to be optimized and returns a score, based on which
    the minimization engine of the optimization will decide the next experiments:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用我们的`make_objective`函数，我们将所有元素（模型、数据、搜索空间、验证策略和评分函数）组合成一个单一函数，即目标函数。结果，我们现在有一个只接受要优化的参数并返回得分的函数，基于这个得分，优化的最小化引擎将决定下一个实验：
- en: '[PRE17]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since we want to control each step of the optimization and save it for later
    use, we also prepare a callback function that will save a list of the experiments
    executed and their results, at every iteration of the minimization process. Simply
    by using these two pieces of information, the minimization engine can be halted
    at any time, and it can thereafter resume the optimization from the checkpoint:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要控制优化的每一步并保存以供以后使用，我们还准备了一个回调函数，该函数将在最小化过程的每个迭代中保存执行实验及其结果列表。只需使用这两条信息，最小化引擎就可以在任何时候停止，然后可以从检查点恢复优化：
- en: '[PRE18]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At this point, we are ready to start. Bayesian optimization needs some starting
    points to work properly. We create a number of experiments with random search
    (using the `dummy_minimize` function) and save their results:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好开始。贝叶斯优化需要一些起始点才能正常工作。我们通过随机搜索（使用`dummy_minimize`函数）创建了一系列实验，并保存了它们的结果：
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can then retrieve the saved experiments and print the sequence of sets of
    hyperparameters that the Bayesian optimization has tested, along with their results.
    In fact, we can find the set of parameters and their results contained in the
    `x0` and `y0` lists:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以检索保存的实验并打印出贝叶斯优化测试的超参数集序列及其结果。实际上，我们可以在`x0`和`y0`列表中找到参数及其结果：
- en: '[PRE20]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At this point, we can even resume the Bayesian optimization with some changes
    in the search space, the acquisition function, the number of calls, or the callbacks:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们甚至可以带一些对搜索空间、获取函数、调用次数或回调的更改来恢复贝叶斯优化：
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once we are satisfied that we don’t need to continue calling the optimization
    function, we can print both the best score obtained (based on our inputs and validation
    scheme) and the set of best hyperparameters:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们满意地认为不需要继续调用优化函数，我们可以打印出最佳得分（基于我们的输入和验证方案）以及最佳超参数集：
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Based on the best result, we can re-train our model for use in the competition.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最佳结果，我们可以重新训练我们的模型，以便在比赛中使用。
- en: Now we have the set of parameters and their results (the `x0` and `y0` lists),
    we could also explore the different results and cluster together the ones that
    are similar in output but different in the set of parameters used. This will help
    us to train a more diverse set of models with similar performances but different
    optimization strategies. This is the ideal situation for **blending**, which is
    the averaging of multiple models in order to lower the variance of the estimates
    and obtain a better public and private leaderboard score.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了参数集及其结果（`x0`和`y0`列表），我们还可以探索不同的结果，并将那些输出相似但使用的参数集不同的结果聚类在一起。这将帮助我们训练一个具有相似性能但不同优化策略的更多样化的模型集。这是**混合**的理想情况，即通过平均多个模型来降低估计的方差，并获得更好的公共和私人排行榜分数。
- en: Refer to *Chapter 9*, *Ensembling with Blending and Stacking Solutions*, for
    a discussion on blending.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 参考第9章，关于**混合和堆叠解决方案的集成**的讨论。
- en: Extending Bayesian optimization to neural architecture search
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将贝叶斯优化扩展到神经网络架构搜索
- en: 'Moving on to deep learning, neural networks also seem to have quite a few hyperparameters
    to fix:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 进入深度学习领域，神经网络似乎也有许多超参数需要调整：
- en: Batch size
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小
- en: Learning rate
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The kind of optimizer and its internal parameters
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器的类型及其内部参数
- en: All these parameters influence how the network learns and they can make a big
    impact; just a slight difference in batch size or learning rate can determine
    whether a network can reduce its error beyond a certain threshold or not.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些参数都会影响网络的学习方式，它们可以产生重大影响；仅仅批处理大小或学习率的一点点差异就可能导致网络能否将其错误降低到某个阈值以下。
- en: That being said, these learning parameters are not the only ones that you can
    optimize when working with **deep neural networks** (**DNNs**). How the network
    is organized in layers and the details of its architecture can make even more
    of a difference.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，这些学习参数并不是你在与**深度神经网络**（DNNs）一起工作时唯一可以优化的参数。网络在层中的组织方式和其架构的细节可以产生更大的影响。
- en: In fact, technically speaking, an **architecture** implies the representational
    capacity of the deep neural network, which means that, depending on the layers
    you use, the network will either be able to read and process all the information
    available in the data, or it will not. While you had a large but limited set of
    choices with other machine learning algorithms, with DNNs your choices seem unlimited,
    because the only apparent limit is your knowledge and experience in handling parts
    of neural networks and putting them together.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，从技术上来说，**架构**意味着深度神经网络的表示能力，这意味着，根据你使用的层，网络要么能够读取和处理数据中所有可用的信息，要么不能。与其他机器学习算法相比，虽然你的选择似乎无限，但唯一明显的限制是你处理神经网络部分以及将它们组合在一起的知识和经验。
- en: 'Common best practices for great deep learning practitioners when assembling
    well-performing DNNs depend mainly on:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的深度学习实践者在组装高性能深度神经网络（DNNs）时常用的常见最佳实践主要依赖于：
- en: Relying on pre-trained models (so you have to be very knowledgeable about the
    solutions available, such as those found on Hugging Face ([https://huggingface.co/models](https://huggingface.co/models))
    or on GitHub)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于预训练模型（因此你必须非常了解可用的解决方案，例如在Hugging Face([https://huggingface.co/models](https://huggingface.co/models))或GitHub上找到的）
- en: Reading cutting-edge papers
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读前沿论文
- en: Copying top Kaggle Notebooks from the same competition or previous ones
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制同一比赛或之前的Kaggle笔记本
- en: Trial and error
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试和错误
- en: Ingenuity and luck
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵感和运气
- en: In a famous lesson given by *Professor Geoffrey Hinton*, he states that you
    can achieve similar and often better results using automated methods such as Bayesian
    optimization. Bayesian optimization will also avoid you getting stuck because
    you cannot figure out the best combinations of hyperparameters among the many
    possible ones.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在Geoffrey Hinton教授的一次著名课程中，他提到，你可以使用自动化方法，如贝叶斯优化，来实现相似甚至更好的结果。贝叶斯优化还可以避免你陷入困境，因为你无法在众多可能的超参数组合中找到最佳组合。
- en: For a recording of Prof. Geoffrey Hinton’s lesson, see [https://www.youtube.com/watch?v=i0cKa0di_lo](https://www.youtube.com/watch?v=i0cKa0di_lo).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Geoffrey Hinton教授课程的录音，请参阅[https://www.youtube.com/watch?v=i0cKa0di_lo](https://www.youtube.com/watch?v=i0cKa0di_lo)。
- en: For the slides, see [https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf](https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 关于幻灯片，请参阅[https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf](https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf)。
- en: As we mentioned before, even in most sophisticated AutoML systems, when you
    have too many hyperparameters, relying on random optimization may produce better
    results or the same results in the same amount of time as Bayesian optimization.
    In addition, in this case, you also have to fight against an optimization landscape
    with sharp turns and surfaces; in DNN optimization, many of your parameters won’t
    be continuous but Boolean instead, and just one change could unexpectedly transform
    the performance of your network for the better or for the worse.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，即使在大多数复杂的AutoML系统中，当你有太多的超参数时，依赖于随机优化可能会产生更好的结果，或者与贝叶斯优化在相同的时间内产生相同的结果。此外，在这种情况下，你还得对抗一个具有尖锐转弯和表面的优化景观；在深度神经网络优化中，许多参数不会是连续的，而是布尔值，仅仅一个变化可能会意外地改善或恶化你网络的性能。
- en: 'Our experience tells us that random optimization may not be suitable for a
    Kaggle competition because:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的经验告诉我们，随机优化可能不适合Kaggle比赛，因为：
- en: You have limited time and resources
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有限的时间和资源
- en: You can leverage your previous optimization results in order to find better
    solutions
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以利用你之前的优化结果来找到更好的解决方案
- en: 'Bayesian optimization in this scenario is ideal: you can set it to work based
    on the time and computational resources that you have and do it by stages, refining
    your settings through multiple sessions. Moreover, it is unlikely that you will
    easily be able to leverage parallelism for tuning DNNs, since they use GPUs, unless
    you have multiple very powerful machines at hand. By working sequentially, Bayesian
    optimization just needs one good machine to perform the task. Finally, even if
    it is hard to find optimal architectures by a search, due to the optimization
    landscape you leverage information from previous experiments, especially at the
    beginning, totally avoiding combinations of parameters that won’t work. With random
    optimization, unless you change the search space along the way, all combinations
    are always liable to be tested.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，贝叶斯优化是理想的：你可以根据你拥有的时间和计算资源来设置它，分阶段进行，通过多次会话来细化你的设置。此外，你不太可能轻松地利用并行性来调整深度神经网络，因为它们使用GPU，除非你手头上有多台非常强大的机器。通过顺序工作，贝叶斯优化只需要一台性能良好的机器来完成这项任务。最后，即使通过搜索难以找到最优架构，由于你利用了先前实验的信息，尤其是在开始时，可以完全避免那些不会工作的参数组合。而在随机优化中，除非你在过程中改变搜索空间，否则所有组合都始终有可能被测试。
- en: There are also drawbacks, however. Bayesian optimization models the hyperparameter
    space using a surrogate function built from previous trials, which is not an error-free
    process. It is not a remote possibility that the process ends up concentrating
    only on a part of the search space while ignoring other parts (which may instead
    contain the minimum you are looking for). The solution to this is to run a large
    number of experiments to be safe, or to alternate between random search and Bayesian
    optimization, challenging the Bayesian model with random trials that can force
    it to reshape its search model in a more optimal way.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也存在一些缺点。贝叶斯优化使用从先前试验构建的代理函数来模拟超参数空间，这并不是一个没有错误的过程。过程最终只关注搜索空间的一部分而忽略其他部分（这些部分可能包含你正在寻找的最小值）的可能性并不是微乎其微的。解决这个问题的方法是运行大量实验以确保安全，或者交替进行随机搜索和贝叶斯优化，用随机试验挑战贝叶斯模型，迫使其以更优的方式重塑搜索模型。
- en: For our example, we use again the data from the *30 Days of ML* initiative by
    Kaggle, a regression task. Our example is based on TensorFlow, but with small
    modifications it can run on other deep learning frameworks such as PyTorch or
    MXNet.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们再次使用Kaggle的“30天机器学习”活动中的数据，这是一个回归任务。我们的例子基于TensorFlow，但经过一些小的修改，它可以在其他深度学习框架上运行，例如PyTorch或MXNet。
- en: 'As before, you can find the example on Kaggle here: [https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以在Kaggle上找到这个例子：[https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns)。
- en: 'Let’s begin:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: '[PRE23]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After importing the TensorFlow package, we leverage its `Dataset` function
    to create an iterable capable of feeding our neural network with batches of data:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入TensorFlow包后，我们利用其`Dataset`函数创建一个可迭代的对象，能够为我们的神经网络提供数据批次：
- en: '[PRE24]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We have also made leaky ReLU activation a custom object for our model; it can
    be called by a string, and there is no need to directly use the function.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将漏激活ReLU作为我们模型的自定义对象；可以通过字符串调用它，无需直接使用该函数。
- en: 'We proceed to code a function that creates our deep neural network model based
    on a set of hyperparameters:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续编写一个函数，该函数根据一组超参数创建我们的深度神经网络模型：
- en: '[PRE25]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Internally, the code in the `create_model` function customizes the neural network
    architecture based on the inputs provided. For instance, as parameters for the
    function you can provide the dimensions of the embeddings for each categorical
    variable, or define the structure and number of dense layers present in the network.
    All these parameters are related to the parameter space you want to be explored
    by Bayesian optimization, hence every input parameter of the function creating
    the model should be related to a **sampling function** defined in the search space.
    All you have to do is to place the sampling functions in a list, in the same order
    as expected by the `create_model` function:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`create_model`函数中的代码根据提供的输入自定义神经网络架构。例如，作为函数的参数，您可以提供每个分类变量的嵌入维度，或者定义网络中存在的密集层的结构和数量。所有这些参数都与贝叶斯优化要探索的参数空间相关，因此创建模型的函数的每个输入参数都应该与搜索空间中定义的**采样函数**相关。您需要做的只是将采样函数放在一个列表中，按照`create_model`函数期望的顺序：
- en: '[PRE26]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As previously illustrated, you now combine all the elements related to the
    search into an objective function to be created by a function incorporating your
    basic search elements, such as the data and the cross-validation strategy:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您现在将所有与搜索相关的元素组合成一个目标函数，由一个函数创建，该函数结合了您的基本搜索元素，如数据和交叉验证策略：
- en: '[PRE27]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next step is to provide a sequence of random search runs (as a way to start
    building some feedback from the search space) and gather the results as a starting
    point. Then, we can feed them into a Bayesian optimization and proceed by using
    `forest_minimize` as a surrogate function:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是提供一个随机搜索的序列（作为从搜索空间中开始构建一些反馈的一种方式），并将结果作为起点收集。然后，我们可以将它们输入到贝叶斯优化中，并通过使用`forest_minimize`作为代理函数来继续：
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice that after the first ten rounds of random search, we proceed with our
    search using a random forest algorithm as a surrogate function. That will ensure
    better and faster results than using a Gaussian process.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在随机搜索的前十轮之后，我们使用随机森林算法作为代理函数继续我们的搜索。这将确保比使用高斯过程获得更好的和更快的成果。
- en: As before, in this process we have to strive to make the optimization feasible
    within the time and resources we have (for instance, by setting a low number of
    `n_calls`). Hence, we can proceed with batches of search iterations by saving
    the state of the optimization, checking the results obtained, and deciding thereafter
    to proceed or conclude the optimization process and not invest more time and energy
    into looking for a better solution.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在这个过程中，我们必须努力在现有时间和资源范围内使优化变得可行（例如，通过设置一个较低的`n_calls`数量）。因此，我们可以通过保存优化状态、检查获得的结果，并在之后决定继续或结束优化过程，不再投入更多时间和精力去寻找更好的解决方案，来分批进行搜索迭代。
- en: Creating lighter and faster models with KerasTuner
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用KerasTuner创建更轻量化和更快的模型
- en: If the previous section has puzzled you because of its complexity, KerasTuner
    can offer you a fast solution for setting up an optimization without much hassle.
    Though it uses Bayesian optimization and Gaussian processes by default, the new
    idea behind KerasTuner is **hyperband optimization**. Hyperband optimization uses
    the bandit approach to figure out the best parameters (see [http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf](http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf)).
    This works quite well with neural networks, whose optimization landscape is quite
    irregular and discontinuous, and thus not always suitable for Gaussian processes.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前一节因为其复杂性而让您感到困惑，KerasTuner可以为您提供一种快速设置优化的解决方案，无需太多麻烦。尽管它默认使用贝叶斯优化和高斯过程，但KerasTuner背后的新想法是**超带优化**。超带优化使用bandit方法来确定最佳参数（参见[http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf](http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf)）。这种方法与神经网络配合得相当好，因为神经网络的优化景观非常不规则和不连续，因此并不总是适合高斯过程。
- en: Keep in mind that you cannot avoid building the function that builds a custom
    network using input hyperparameters; KerasTuner just makes it much easier to handle.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，你无法避免构建一个使用输入超参数构建自定义网络的函数；KerasTuner只是使这个过程变得更加容易。
- en: Let’s start from the beginning. KerasTuner ([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))
    was announced as a “flexible and efficient hyperparameter tuning for Keras models”
    by *François Chollet*, the creator of Keras.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从开始讲起。KerasTuner([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))被其创造者、Keras的创始人*弗朗索瓦·肖莱特*宣布为“为Keras模型提供灵活且高效的超参数调整”。
- en: 'The recipe proposed by Chollet for running KerasTuner is made up of simple
    steps, starting from your existing Keras model:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 肖莱特提出的运行KerasTuner的配方由简单的步骤组成，从你现有的Keras模型开始：
- en: Wrap your model in a function with `hp` as the first parameter.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的模型封装在一个函数中，其中`hp`作为第一个参数。
- en: Define hyperparameters at the beginning of the function.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数的开始处定义超参数。
- en: Replace DNN static values with hyperparameters.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将DNN的静态值替换为超参数。
- en: Write the code that models a complex neural network from the given hyperparameters.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，从给定的超参数构建一个复杂的神经网络模型。
- en: If necessary, dynamically define hyperparameters as you build the network.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，在构建网络时动态定义超参数。
- en: We’ll now explore how all these steps can work for you in a Kaggle competition
    by using an example. At the moment, KerasTuner is part of the stack offered by
    any Kaggle Notebook, hence you don’t need to install it. In addition, the TensorFlow
    add-ons are part of the Notebook’s pre-installed packages.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过使用一个示例来探索所有这些步骤如何在Kaggle比赛中为你工作。目前，KerasTuner是任何Kaggle笔记本提供的堆栈的一部分，因此你不需要安装它。此外，TensorFlow附加组件是笔记本预安装的包的一部分。
- en: 'If you are not using a Kaggle Notebook and you need to try KerasTuner, you
    can easily install both using the following commands:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用Kaggle笔记本并且需要尝试KerasTuner，你可以使用以下命令轻松安装：
- en: '[PRE29]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can find this example already set up on a Kaggle Notebook here: [https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/](https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kaggle笔记本上找到这个示例已经设置好的链接：[https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/](https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/)。
- en: 'Our first step is to import the necessary packages (creating shortcuts for
    some commands, such as for `pad_sequences`) and to upload the data we will be
    using directly from Keras:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是导入必要的包（为一些命令创建快捷方式，例如`pad_sequences`），并直接从Keras上传我们将使用的数据：
- en: '[PRE30]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This time, we are using the IMDb dataset, which is available in the Keras package
    ([https://keras.io/api/datasets/imdb/](https://keras.io/api/datasets/imdb/)).
    The dataset has some interesting characteristics:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们使用的是IMDb数据集，该数据集包含在Keras包中([https://keras.io/api/datasets/imdb/](https://keras.io/api/datasets/imdb/))。该数据集具有一些有趣的特性：
- en: It is a dataset of 25,000 movie reviews from IMDb
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个包含25,000条IMDb电影评论的数据集。
- en: The reviews are labeled by sentiment (positive/negative)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论通过情感（正面/负面）进行标记。
- en: The target classes are balanced (hence accuracy works as a scoring measure)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标类别是平衡的（因此准确率作为评分指标）。
- en: Each review is encoded as a list of word indexes (integers)
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条评论都被编码为一个单词索引列表（整数）。
- en: For convenience, words are indexed by overall frequency
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了方便，单词通过整体频率进行索引。
- en: In addition, it has been successfully used in a popular Kaggle competition on
    word embeddings ([https://www.kaggle.com/c/word2vec-nlp-tutorial/overview](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview)).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它已经在Kaggle上的一项关于词嵌入的流行比赛中成功应用([https://www.kaggle.com/c/word2vec-nlp-tutorial/overview](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview))。
- en: 'This example involves natural language processing. This type of problem is
    often solved by using **recurrent neural networks** (**RNNs**) based on LSTM or
    GRU layers. BERT, RoBERTa, and the other transformer-based models often achieve
    better results – being pre-trained models relying on large language corpora –
    but this is not necessarily true in all problems, and RNNs can prove a strong
    baseline to beat or a good addition to an ensemble of neural models. In our example,
    all words are already numerically indexed. We just add to the existing indices
    the numeric codes that denote padding (so we can easily normalize all the text
    to the phrase length), the start of the sentence, an unknown word, and an unused
    word:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子涉及自然语言处理。这类问题通常是通过使用基于LSTM或GRU层的**循环神经网络**（**RNNs**）来解决的。BERT、RoBERTa和其他基于转换器的模型通常能取得更好的结果——作为依赖大型语言语料库的预训练模型——但这并不一定在所有问题中都成立，RNNs可以证明是一个强大的基线，可以击败或是一个神经模型集成的良好补充。在我们的例子中，所有单词都已经进行了数字索引。我们只是将表示填充的数字代码添加到现有的索引中（这样我们就可以轻松地将所有文本归一化到短语长度），句子的开始，一个未知单词和一个未使用的单词：
- en: '[PRE31]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The next step involves creating a custom layer for **attention**. Attention
    is the foundation of transformer models and it is one of the most innovative ideas
    in neural NLP of recent times.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及创建一个用于**注意力**的自定义层。注意力是转换器模型的基础，也是近年来神经自然语言处理中最具创新性的想法之一。
- en: 'For all the details of how these kinds of layers work, see the seminal paper
    on attention: Vaswani, A. et al. *Attention is all you need.* Advances in neural
    information processing systems. 2017 ([https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些类型层如何工作的所有细节，请参阅关于注意力的开创性论文：Vaswani, A. 等人。*注意力即一切*。神经信息处理系统进展。2017 ([https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))。
- en: 'The idea of attention can be easily conveyed. LSTM and GRU layers output processed
    sequences, but not all the elements in these output sequences are necessarily
    important for your predictions. Instead of averaging all the output sequences
    using a pool layer across the stratified sequences, you can actually take a *weighted
    average* of them (and during the training phase learn the correct weights to be
    used). This weighting process (**attention**) definitely improves the results
    you are going to pass on further. Of course, you can make this approach even more
    sophisticated using multiple attention layers (we call this **multi-head attention**),
    but in our example a single layer will suffice because we want to demonstrate
    that using attention is more effective in this problem than simply averaging or
    just concatenating all the results together:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的概念可以很容易地传达。LSTM和GRU层输出处理过的序列，但并非这些输出序列中的所有元素对于你的预测都一定是重要的。你不必使用池化层在分层序列上平均所有输出序列，实际上你可以对它们进行**加权平均**（并且在训练阶段学习使用正确的权重）。这种加权过程（**注意力**）无疑会提高你将要传递的结果。当然，你可以通过使用多个注意力层（我们称之为**多头注意力**）使这种方法更加复杂，但在我们的例子中，一个单独的层就足够了，因为我们想证明在这个问题上使用注意力比简单地平均或简单地连接所有结果更有效：
- en: '[PRE32]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As a further variation in our experiments on the architecture of the DNNs for
    this problem, we also want to test the effectiveness of using different kinds
    of optimizers such as **Rectified Adam** (an adaptive learning Adam optimizer;
    read this post to learn more: [https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b](https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b))
    or **Stochastic Weighted Averaging** (**SWA**). SWA is a way to average the weights
    traversed during the optimization based on a modified learning rate schedule:
    if your model tends to overfit or overshoot, SWA helps in getting near to an optimal
    solution and it is proven to work especially in NLP problems.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们对这个问题的DNN架构进行实验的进一步变化，我们还想测试使用不同类型的优化器（如**修正的Adam**（自适应学习率的Adam优化器；阅读这篇帖子了解更多信息：[https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b](https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b)）或**随机加权平均**（**SWA**）的有效性。SWA是一种基于修改后的学习率计划来平均优化过程中遍历的权重的方法：如果你的模型倾向于过拟合或过度估计，SWA有助于接近最优解，并且在NLP问题中已被证明是有效的。
- en: '[PRE33]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Having defined two key functions, we now face the most important function to
    code: the one that will provide different neural architectures given the parameters.
    We don’t encode all the various parameters we want to connect to the different
    architectural choices; we only provide the `hp` parameter, which should contain
    all the possible parameters we want to use, and that will be run by KerasTuner.
    Aside from `hp` in the function input, we fix the size of the vocabulary and the
    length to be padded (adding dummy values if the effective length is shorter or
    cutting the phrase if the length is longer):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了两个关键函数后，我们现在面临最重要的编码函数：该函数将根据参数提供不同的神经网络架构。我们不会对所有我们想要连接到不同架构选择的参数进行编码；我们只提供
    `hp` 参数，它应该包含我们想要使用的所有可能的参数，并且将由 KerasTuner 运行。除了函数输入中的 `hp` 之外，我们还固定了词汇表的大小和要填充的长度（如果实际长度较短，则添加虚拟值；如果长度较长，则截断短语）：
- en: '[PRE34]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the first part of the function, we simply recover all the settings from the
    `hp` parameter. We also make explicit the range of the search space for each of
    them. Contrary to the solutions we’ve seen so far, this part of the work is done
    *inside* the model function, not outside.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的第一部分，我们简单地从 `hp` 参数中恢复所有设置。我们还明确指出了每个参数的搜索范围。与迄今为止我们看到的所有解决方案相反，这部分工作是在模型函数内部完成的，而不是外部。
- en: The function continues by defining the different layers using the parameters
    extracted from `hp`. In some cases, a parameter will switch on or off a part of
    the network performing certain data processing. For instance, in the code we inserted
    a branch of the graph (`conv_filters` and `conv_kernel`) that processes the sequence
    of words using convolutional layers, which, in their 1D form, can also prove useful
    for NLP problems, since they can catch local sequences of words and meanings that
    LSTMs may find harder to grasp.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 函数继续通过使用从 `hp` 中提取的参数定义不同的层。在某些情况下，一个参数会开启或关闭网络的一部分，执行特定的数据处理。例如，在代码中我们插入了一个图的分支（`conv_filters`
    和 `conv_kernel`），它使用卷积层处理单词序列，这些卷积层在它们的1D形式中，也可以对NLP问题很有用，因为它们可以捕捉到LSTMs可能更难把握的局部单词序列和意义。
- en: 'Now we can define the actual model:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义实际的模型：
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We start by defining the input layer and transform it with a subsequent embedding
    layer that will encode the sequence values into dense layers. Some dropout regularization
    is applied to the process using `SpatialDropout1D`, a function that will randomly
    drop entire columns of the output matrix (standard dropout drops random single
    elements in the matrix instead). After these initial phases, we split the network
    into one pipeline based on convolutions (`Conv1D`) and another based on recurrent
    layers (GRU or LSTM). It is after the recurrent layers that we apply the attention
    layer. Finally, the outputs of these two pipelines are concatenated and, after
    a few more dense layers, they arrive at the final output node, a sigmoid since
    we have to represent a probability bounded in the range 0 to 1.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义输入层，并使用随后的嵌入层对其进行转换，该嵌入层将序列值编码到密集层中。在处理过程中应用了一些 `SpatialDropout1D` 正则化，这是一个会随机丢弃输出矩阵的整个列的功能（标准dropout会在矩阵中随机丢弃单个元素）。在这些初始阶段之后，我们将网络分为基于卷积（`Conv1D`）的一个管道和基于循环层（GRU或LSTM）的另一个管道。在循环层之后，我们应用了注意力层。最后，这两个管道的输出被连接起来，经过几个更多的密集层后，到达最终的输出节点，一个sigmoid，因为我们必须表示一个范围在0到1之间的概率。
- en: 'After the model definition, we set the learning parameters and compile the
    model before returning it:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型定义之后，我们设置学习参数并编译模型，然后返回它：
- en: '[PRE36]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that we have built the model using the functional API of Keras, not the
    sequential one. We would advise you to avoid the sequential one, in fact; it is
    easier to set up, but severely restricts your potential architectures.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用 Keras 的功能API构建了模型，而不是顺序API。实际上，我们建议您避免使用顺序API；它更容易设置，但严重限制了您的潜在架构。
- en: At this point, most of the work is already done. As a suggestion, having worked
    out many optimizations using KerasTuner ourselves, we prefer to first build a
    *non-parametric* model, using all the possible architectural features that we
    want to test, with the mutually exclusive parts of the network set to the most
    complex solutions. After we have set up the generative function and our model
    seems to be working properly, we can, for instance, represent its graph and have
    it successfully fit some examples as a test. After that, we start inserting the
    parametric variables into the architecture and setting up the `hp` parameter definitions.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大部分工作已经完成。作为一个建议，我们自己使用KerasTuner进行了许多优化后，我们更喜欢首先构建一个*非参数化*模型，使用我们想要测试的所有可能的架构特性，将网络中相互排斥的部分设置为最复杂的解决方案。在我们设置了生成函数并且我们的模型看起来运行正常之后，例如，我们可以表示其图，并成功地将一些示例作为测试拟合。之后，我们开始将参数化变量插入架构，并设置`hp`参数定义。
- en: In our experience, starting with a parametric function immediately will take
    more time and debugging effort. The idea behind KerasTuner is to let you think
    of your DNNs as a set of modular circuits and to help you optimize how the data
    flows inside them.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，立即从参数化函数开始将需要更多的时间和调试努力。KerasTuner背后的想法是让您将DNN视为一组模块化电路，并帮助您优化数据在其中的流动方式。
- en: 'Now, we import KerasTuner. First, we set the tuner itself, and then we start
    the search:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们导入KerasTuner。首先，我们设置调优器本身，然后开始搜索：
- en: '[PRE37]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As a tuner, we opt for the Bayesian optimization one, but you can also try the
    Hyperband tuner ([https://keras.io/api/keras_tuner/tuners/hyperband/](https://keras.io/api/keras_tuner/tuners/hyperband/))
    and check if it works better for your problem. We provide our model function to
    the `hypermodel` parameter. Then, we set the objective using a string or a function,
    the maximum number of trials (KerasTuner will stop earlier if there is nothing
    more to be done), and the initial number of random trials – the more the better
    – in order to inform the Bayesian process. Early stopping is a standard and well-performing
    practice in modeling DNNs that you absolutely cannot ignore. Finally, but importantly,
    we set the directory where we want to save our search and a seed number for reproducibility
    of the optimization steps.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 作为调优器，我们选择了贝叶斯优化，但您也可以尝试Hyperband调优器（[https://keras.io/api/keras_tuner/tuners/hyperband/](https://keras.io/api/keras_tuner/tuners/hyperband/)）并检查它是否更适合您的问题。我们将我们的模型函数提供给`hypermodel`参数。然后，我们使用字符串或函数设置目标，最大尝试次数（如果没有什么更多的事情要做，KerasTuner将提前停止），以及初始随机尝试次数——越多越好——以告知贝叶斯过程。早期停止是建模DNN的标准且表现良好的实践，您绝对不能忽视。最后，但同样重要的是，我们设置了保存搜索结果的目录以及用于优化步骤可重复性的种子数字。
- en: 'The search phase is run like a standard fit of a Keras model and – this is
    quite important – it accepts callbacks. Therefore, you can easily add early stopping
    to your model. In this case, the given epoch number should therefore be considered
    the maximum number of epochs. You may also want to optimize the batch size, which
    we haven’t done in our example. This still requires some extra work, but you can
    get an idea of how to achieve it by reading this GitHub closed issue: [https://github.com/keras-team/keras-tuner/issues/122](https://github.com/keras-team/keras-tuner/issues/122).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索阶段就像运行一个标准的Keras模型拟合一样，而且这一点非常重要——它接受回调。因此，您可以轻松地将早期停止添加到您的模型中。在这种情况下，给定的epoch数应被视为最大epoch数。您可能还希望优化批大小，我们在这个例子中没有这样做。这仍然需要一些额外的工作，但您可以通过阅读这个GitHub已关闭的问题来了解如何实现它：[https://github.com/keras-team/keras-tuner/issues/122](https://github.com/keras-team/keras-tuner/issues/122)。
- en: 'After the optimization is complete, you can extract the best parameters and
    save the best model without any need to retrain it:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 优化完成后，您可以提取最佳参数并保存最佳模型，而无需重新训练：
- en: '[PRE38]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In this example, KerasTuner finds a solution that uses:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，KerasTuner找到了一个解决方案，它使用了：
- en: A larger embedding layer
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个更大的嵌入层
- en: Just plain GRU and LSTM layers (no bi-directional layers)
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只简单的GRU和LSTM层（没有双向层）
- en: Stacking of multiple one-dimensional convolution layers (Conv1D)
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个一维卷积层（Conv1D）的堆叠
- en: More and larger dense layers
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多和更大的密集层
- en: Interestingly, the solution is not only more effective, but also lighter and
    faster than our previous attempts based on intuition and experience with the problem.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这个解决方案不仅更有效，而且比我们基于直觉和问题经验的前期尝试更轻、更快。
- en: Chollet himself suggests using KerasTuner not just to make your DNNs perform
    better but also to shrink them to a more manageable size, something that may make
    the difference in Code competitions. This allows you to put together more models
    that work together within the limited inference time provided by the sponsors
    of the competition.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: Chollet本人建议使用KerasTuner不仅是为了让你的深度神经网络（DNNs）表现更好，而且是为了将它们缩小到更易于管理的尺寸，这在代码竞赛中可能起到决定性作用。这允许你在有限的推理时间内，结合更多协同工作的模型。
- en: 'If you would like to examine more examples of using KerasTuner, François Chollet
    also created a series of Notebooks for Kaggle competitions in order to showcase
    the workings and functionalities of his optimizer:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要查看更多使用KerasTuner的示例，François Chollet还创建了一系列用于Kaggle竞赛的Notebooks，以展示他的优化器的运作和功能：
- en: '[https://www.kaggle.com/fchollet/keras-kerastuner-best-practices](https://www.kaggle.com/fchollet/keras-kerastuner-best-practices)
    for the *Digit Recognizer* datasets'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras-kerastuner最佳实践](https://www.kaggle.com/fchollet/keras-kerastuner-best-practices)
    用于 *数字识别* 数据集'
- en: '[https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices](https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices)
    for the *Titanic* dataset'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras-kerastuner最佳实践](https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices)
    用于 *泰坦尼克号* 数据集'
- en: '[https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices](https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices)
    for the *Mechanisms of Action (MoA) Prediction* competition'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras-kerastuner最佳实践](https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices)
    用于 *作用机制（MoA）预测* 竞赛'
- en: The TPE approach in Optuna
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optuna中的TPE方法
- en: We complete our overview of Bayesian optimization with another interesting tool
    and approach to it. As we have discussed, Scikit-optimize uses Gaussian processes
    (as well as tree algorithms) and it directly models the surrogate function and
    the acquisition function.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过另一个有趣的工具和贝叶斯优化的方法来完成我们对贝叶斯优化的概述。正如我们讨论的，Scikit-optimize使用高斯过程（以及树算法），并直接模拟代理函数和获取函数。
- en: As a reminder of these topics, the **surrogate function** helps the optimization
    process to model the potential performance result when you try a set of hyperparameters.
    The surrogate function is built using the previous experiments and their results;
    it is just a predictive model applied in order to forecast the behavior of a specific
    machine learning algorithm on a specific problem. For each parameter input provided
    to the surrogate function, you get an expected performance output. That’s intuitive
    and also quite hackable, as we have seen.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对这些主题的提醒，**代理函数**帮助优化过程模拟尝试一组超参数时的潜在性能结果。代理函数是使用之前的实验及其结果构建的；它只是一个应用于预测特定机器学习算法在特定问题上的行为的预测模型。对于提供给代理函数的每个参数输入，你都会得到一个预期的性能输出。这既直观又相当易于操作，正如我们所看到的。
- en: The **acquisition function** instead points out what set of hyperparameters
    could be tested in order to improve the ability of the surrogate function to predict
    the performances of the machine learning algorithm. It is also useful for really
    testing if we can reach a top-performing result based on the surrogate function’s
    forecasts. These two objectives represent the *explore* part (where you run experiments)
    and the *exploit* part (where you test the performance) of a Bayesian optimization
    process.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取函数**则指出哪些超参数组合可以被测试，以改善代理函数预测机器学习算法性能的能力。它也有助于真正测试我们是否可以根据代理函数的预测达到顶级性能。这两个目标代表了贝叶斯优化过程中的*探索*部分（进行实验）和*利用*部分（测试性能）。'
- en: Instead, optimizers based on **TPE** tackle the problem by estimating the likelihood
    of success of the values of parameters. In other words, they model the success
    distribution of the parameters themselves using successive refinements, assigning
    a higher probability to more successful value combinations.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，基于**TPE**的优化器通过估计参数值的成功可能性来解决问题。换句话说，它们通过连续的细化来模拟参数自身的成功分布，为更成功的值组合分配更高的概率。
- en: In this approach, the set of hyperparameters is divided into good and bad ones
    by these distributions, which take the role of the surrogate and acquisition functions
    in Bayesian optimization, since the distributions tell you where to sample to
    get better performances or explore where there is uncertainty.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，通过这些分布将超参数集分为好和坏，这些分布充当贝叶斯优化中的代理和获取函数，因为分布告诉你在哪里采样以获得更好的性能或探索存在不确定性的地方。
- en: To explore the technical details of TPE, we suggest reading Bergstra, J. et
    al. *Algorithms for hyper-parameter optimization*. Advances in neural information
    processing systems 24, 2011 ([https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索TPE的技术细节，我们建议阅读Bergstra，J.等人撰写的*超参数优化算法*。神经网络信息处理系统24卷，2011年（[https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)）。
- en: Therefore, TPE can model the search space and simultaneously suggest what the
    algorithm can try next, by sampling from the adjusted probability distribution
    of parameters.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TPE可以通过从调整后的参数概率分布中进行采样，来模拟搜索空间并同时建议算法下一步可以尝试的内容。
- en: For a long time, **Hyperopt** was the option for those preferring to use TPE
    instead of Bayesian optimization based on Gaussian processes. In October 2018,
    however, Optuna appeared in the open source and it has become the preferred choice
    for Kagglers due to its versatility (it also works out of the box for neural networks
    and even for ensembling), speed, and efficiency in finding better solutions compared
    to previous optimizers.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 很长一段时间里，**Hyperopt**是那些喜欢使用TPE而不是基于高斯过程的贝叶斯优化的用户的选项。然而，2018年10月，Optuna出现在开源领域，由于其多功能性（它也适用于神经网络甚至集成），速度和效率，以及与先前优化器相比找到更好解决方案的能力，它已成为Kagglers的首选选择。
- en: In this section, we will demonstrate just how easy is to set up a search, which
    is called a *study* under Optuna terminology. All you need to do is to write an
    objective function that takes as input the parameters to be tested by Optuna and
    then returns an evaluation. Validation and other algorithmic aspects can be handled
    in a straightforward manner inside the objective function, also using references
    to variables external to the function itself (both global variables or local ones).
    Optuna also allows **pruning**, that is, signaling that a particular experiment
    is not going well and that Optuna can stop and forget about it. Optuna provides
    a list of functions that activate this callback (see [https://optuna.readthedocs.io/en/stable/reference/integration.html](https://optuna.readthedocs.io/en/stable/reference/integration.html));
    the algorithm will run everything efficiently for you after that, which will significantly
    reduce the time needed for optimization.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示设置搜索有多容易，在Optuna术语中，这被称为*研究*。你所需要做的就是编写一个目标函数，该函数接受Optuna要测试的参数作为输入，然后返回一个评估结果。验证和其他算法方面可以在目标函数内以直接的方式处理，也可以使用对函数本身外部的变量的引用（既可以是全局变量也可以是局部变量）。Optuna还允许**剪枝**，即表示某个特定实验进展不佳，Optuna可以停止并忘记它。Optuna提供了一系列激活此回调的函数（见[https://optuna.readthedocs.io/en/stable/reference/integration.html](https://optuna.readthedocs.io/en/stable/reference/integration.html)）；在此之后，算法将为你高效地运行一切，这将显著减少优化所需的时间。
- en: All of this is in our next example. We return to optimizing for the *30 Days
    of ML* competition. This time, we are trying to figure out what parameters make
    XGBoost work for this competition.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都在我们的下一个示例中。我们回到优化*30 Days of ML*竞赛。这次，我们试图找出哪些参数使XGBoost适用于这个竞赛。
- en: You can find the Notebook for this example at [https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization](https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization](https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization)找到这个示例的Notebook。
- en: 'As a first step, we upload the libraries and the data, as before:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们上传库和数据，就像之前一样：
- en: '[PRE39]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When using Optuna, you just have to define an objective function containing
    the model, the cross-validation logic, the evaluation measure, and the search
    space.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Optuna时，你只需定义一个包含模型、交叉验证逻辑、评估指标和搜索空间的目標函数。
- en: 'Naturally, for data you can refer to objects outside the function itself, rendering
    the construction of the function much easier. As in KerasTuner, here you need
    a special input parameter based on a class from Optuna:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，对于数据，你可以参考函数本身之外的对象，这使得函数的构建变得容易得多。例如，在KerasTuner中，你需要一个基于Optuna类的特殊输入参数：
- en: '[PRE40]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In this example, for performance reasons, we won’t cross-validate but use one
    fixed dataset for training, one for validation (early stopping), and one for testing
    purposes. We are using GPU in this example, and we are also subsetting the available
    data in order to fit the execution of 60 trials into a reasonable length of time.
    If you don’t want to use GPU, just remove the `tree_method` and `predictor` parameters
    from the `XGBRegressor` instantiation. Also notice how we set a callback in the
    `fit` method in order to provide Optuna feedback on how the model is performing,
    so the optimizer can stop an underperforming experiment early to give space to
    other attempts.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，出于性能考虑，我们不会进行交叉验证，而是使用一个固定的数据集进行训练，一个用于验证（早期停止），一个用于测试目的。在这个例子中，我们使用GPU，并且我们还对可用的数据进行子集化，以便将60次试验的执行时间缩短到合理的长度。如果你不想使用GPU，只需从`XGBRegressor`实例化中移除`tree_method`和`predictor`参数。同时请注意，我们如何在`fit`方法中设置回调，以便提供关于模型性能的Optuna反馈，这样优化器就可以在实验表现不佳时提前停止，为其他尝试腾出空间。
- en: '[PRE41]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Another notable aspect is that you can decide to optimize either for minimization
    or maximization, depending on your problem (Scikit-optimize works only on minimization
    problems).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的方面是，你可以根据你的问题选择优化最小化或最大化，因为Scikit-optimize只适用于最小化问题。
- en: '[PRE42]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To complete the run, you just have to print or export the best test performance
    and the best parameters found by the optimization.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成运行，你只需打印或导出最佳测试性能和优化找到的最佳参数即可。
- en: '![](img/Ruchi_Bhatia.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![Ruchi Bhatia](img/Ruchi_Bhatia.png)'
- en: Ruchi Bhatia
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Ruchi Bhatia
- en: '[https://www.kaggle.com/ruchi798](https://www.kaggle.com/ruchi798)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/ruchi798](https://www.kaggle.com/ruchi798)'
- en: As a conclusion to this dense chapter, let’s look at one last interview. This
    time, we’re speaking to Ruchi Bhatia, a Grandmaster in Datasets and Notebooks.
    Ruchi is currently a graduate student at Carnegie Mellon University, a Data Scientist
    at OpenMined, and a Data Science Global Ambassador at Z by HP.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的总结，让我们来看最后一个访谈。这次，我们将与Ruchi Bhatia进行对话，她是数据集和笔记的大师级人物。Ruchi目前是卡内基梅隆大学的硕士研究生，OpenMined的数据科学家，以及Z
    by HP的数据科学全球大使。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*My favorite kinds of competitions are NLP and Analytics competitions. Being
    multilingual has played a significant role in my main focus and interest: Natural
    Language Processing.*'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '*我最喜欢的竞赛类型是自然语言处理和数据分析竞赛。多语言能力在我的主要关注点和兴趣——自然语言处理中发挥了重要作用。*'
- en: '*As for Analytics competitions, I enjoy making sense out of complex data and
    backing my answers to questions with the support of data! Every competition on
    Kaggle is novel and requires different techniques. I mainly follow a data-driven
    approach to algorithm selection and have no set favorites.*'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '*至于数据分析竞赛，我喜欢从复杂的数据中找出意义，并用数据支持我的答案！Kaggle上的每一场竞赛都是新颖的，需要不同的技术。我主要遵循数据驱动的算法选择方法，没有固定的偏好。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何应对Kaggle竞赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*When a new competition is announced, my priority is to understand the problem
    statement in depth. Sometimes problem statements can be out of our comfort zone
    or domain, so it’s* *crucial to ensure we grasp them well before moving on to
    exploratory data analysis. While performing EDA, my goal is to understand data
    distribution and focus on getting to know the data at hand. During this, we are
    likely to come across patterns, and we should make an effort to understand those
    and form a hypothesis for outliers and exceptional cases.*'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*当一项新的竞赛被宣布时，我的首要任务是深入理解问题陈述。有时问题陈述可能超出了我们的舒适区或领域，因此确保我们在进行探索性数据分析之前充分理解它们是至关重要的。在进行EDA的过程中，我的目标是理解数据分布，并专注于了解手头的数据。在这个过程中，我们可能会遇到模式，我们应该努力理解这些模式，并为异常值和特殊情况形成假设。*'
- en: '*After this, I spend time understanding the competition metrics. The creation
    of a leak-free cross-validation strategy is my next step. After this, I choose
    a baseline model and make my first submission. If the correlation between the
    local validation and the competition leaderboard is not satisfying, I iterate
    for as long as needed to understand possible discrepancies and account for them.*'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '*在此之后，我花时间理解竞争指标。创建一个无泄漏的交叉验证策略是我的下一步。之后，我选择一个基线模型并提交我的第一个版本。如果本地验证和竞赛排行榜之间的相关性不令人满意，我会根据需要迭代，以理解可能的差异并加以考虑。*'
- en: '*Then I move on to improve my modeling approach with time. Apart from this,
    tweaking parameters and trying new experiments help to gain an understanding of
    what works best with the data at hand (ensuring that I’m preventing overfitting
    during the whole process). Finally, in the last few weeks of the competition,
    I perform model ensembling and check the robustness of my solution.*'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '*然后我继续随着时间的推移改进我的建模方法。除此之外，调整参数和尝试新的实验有助于了解什么最适合手头的数据（确保在整个过程中防止过拟合）。最后，在竞赛的最后几周，我执行模型集成并检查我解决方案的鲁棒性。*'
- en: '*As for my projects outside of Kaggle, most of my time is spent in data gathering,
    cleaning, and getting relevant value out of the data.*'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*至于我在Kaggle之外的项目，我大部分时间都花在数据收集、清理和从数据中获得相关价值上。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你的职业生涯？如果是，是如何帮助的？
- en: '*Kaggle has tremendously helped me accelerate my career. Not only did it help
    me find my passion for data science, but it also motivated me to contribute effectively
    and stay consistent. It’s the perfect place to try hands-on experiments with an
    enormous amount of data at our fingertips and showcase our work on a global scale.
    In addition, our work is easily accessible, so we can reach a broader audience
    as well.*'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle极大地帮助我加速了我的职业生涯。它不仅帮助我发现我对数据科学的热情，还激励我有效地贡献并保持一致性。这是一个在指尖上有大量数据可以尝试动手实验并展示我们工作的全球平台。此外，我们的工作易于访问，因此我们可以触及更广泛的受众。*'
- en: '*I have used a majority of my Kaggle work on my portfolio to indicate the diversity
    of work I have done in my journey thus far. Kaggle competitions aim to solve novel
    and real-world problems, and I feel employers look for our ability and aptitude
    to solve such problems. I’ve also curated a broad range of datasets that helped
    me highlight my acumen in working with raw data. These projects helped me secure
    multiple job opportunities.*'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '*我已经将大部分Kaggle工作用于我的作品集，以表明我在迄今为止的旅程中完成的工作的多样性。Kaggle竞赛旨在解决新颖和现实世界的问题，我认为雇主寻找我们解决这类问题的能力和天赋。我还整理了广泛的数据集，这有助于突出我在处理原始数据方面的敏锐度。这些项目帮助我获得了多个工作机会。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验丰富的Kagglers通常忽略了什么？你现在知道什么，而当你刚开始时希望知道的呢？
- en: '*In my* *experience, I’ve noticed that many Kagglers get disheartened when
    their ranking in competitions isn’t what they expected it to be. After weeks and
    months of hard work, I can see why they might give up early, but winning Kaggle
    competitions is no easy feat. There are several people of different educational
    backgrounds and work experience competing, and having the courage to try is all
    that’s important. We should focus on our individualistic growth and see how far
    we’ve come in our journey.*'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的经验中，我注意到许多Kagglers在竞赛中的排名不符合他们的预期时，会感到沮丧。经过几周甚至几个月的辛勤工作，我明白他们为什么可能会早早放弃，但赢得Kaggle竞赛并非易事。有来自不同教育背景和工作经验的人参与竞争，有勇气尝试就足够了。我们应该专注于个人的成长，看看我们在旅程中走了多远。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据分析或机器学习，有没有任何特定的工具或库你推荐使用？
- en: '*Comprehensive exploratory data analysis combined with relevant visualizations
    help us spot data trends and context that can improve our methodology. Since I
    believe in the power of visualizations, my favorite data science libraries would
    be Seaborn and TensorBoard. Seaborn for EDA and TensorBoard for visualizations
    needed during the machine learning workflow. I occasionally use Tableau too.*'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '*综合性的探索性数据分析结合相关的可视化有助于我们发现数据趋势和背景，从而改进我们的方法。由于我相信可视化的重要性，我最喜欢的数据科学库将是Seaborn和TensorBoard。Seaborn用于EDA，TensorBoard用于机器学习工作流程中的可视化。我也偶尔使用Tableau。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做最重要的事情是什么？
- en: '*When people enter a competition, I believe they should prepare themselves
    for deep diving into the problem statement and researching. Competitions on Kaggle
    are particularly challenging and help solve real-life problems in many cases.
    People should have a positive mindset and not get disheartened. Kaggle competitions
    provide the perfect opportunity to learn and grow!*'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '*当人们进入比赛时，我相信他们应该为深入分析问题陈述和研究做好准备。Kaggle的比赛尤其具有挑战性，并且在许多情况下有助于解决现实生活中的问题。人们应该保持积极的心态，不要灰心丧气。Kaggle的比赛提供了学习和成长的最佳机会！*'
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed hyperparameter optimization at length as a way
    to increase your model’s performance and score higher on the leaderboard. We started
    by explaining the code functionalities of Scikit-learn, such as grid search and
    random search, as well as the newer halving algorithms.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细讨论了超参数优化作为提高模型性能和在排行榜上获得更高分数的方法。我们首先解释了Scikit-learn的代码功能，例如网格搜索和随机搜索，以及较新的折半算法。
- en: Then, we progressed to Bayesian optimization and explored Scikit-optimize, KerasTuner,
    and finally Optuna. We spent more time discussing the direct modeling of the surrogate
    function by Gaussian processes and how to hack it, because it can allow you greater
    intuition and a more ad hoc solution. We recognize that, at the moment, Optuna
    has become a gold standard among Kagglers, for tabular competitions as well as
    for deep neural network ones, because of its speedier convergence to optimal parameters
    in the time allowed in a Kaggle Notebook.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进一步讨论了贝叶斯优化，并探讨了Scikit-optimize、KerasTuner，最后是Optuna。我们花了更多的时间讨论通过高斯过程直接建模代理函数以及如何对其进行黑客攻击，因为它可以让你有更强的直觉和更灵活的解决方案。我们认识到，目前Optuna已经成为Kagglers中的黄金标准，无论是表格竞赛还是深度神经网络竞赛，因为它在Kaggle笔记本允许的时间内更快地收敛到最优参数。
- en: However, if you want to stand out among the competition, you should strive to
    test solutions from other optimizers as well.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想从竞争中脱颖而出，你应该努力测试其他优化器的解决方案。
- en: 'In the next chapter, we will move on to discuss another way to improve your
    performance in Kaggle competitions: ensembling models. By discovering the workings
    of averaging, blending, and stacking, we will illustrate how you can boost your
    results beyond what you can obtain by tuning hyperparameters alone.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论另一种提高你在Kaggle比赛中表现的方法：集成模型。通过了解平均、混合和堆叠的工作原理，我们将展示如何通过仅调整超参数之外的方式提升你的结果。
- en: Join our book’s Discord space
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作空间，参加每月的“问我任何问题”活动，与作者交流：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code40480600921811704671.png)'
