- en: Neural Networks - MNIST Handwriting Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 - MNIST 手写识别
- en: Imagine, you were a postal worker. Your would be job to deliver letters. Most
    of the time, the recipient's name and address would be printed and quite legible,
    and your job becomes quite easy. But come Thanksgiving and Christmas, the number
    of envelopes with handwritten addresses increases as people give their personal
    touches and flourishes. And, to be frank, some people (me included) just have
    terrible handwriting.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一名邮递员。你的工作就是递送信件。大多数时候，收件人的姓名和地址都会打印出来，并且非常清晰，你的工作就变得相当简单。但是到了感恩节和圣诞节，带有手写地址的信封数量会增加，因为人们会添加个人风格和装饰。坦白说，有些人（包括我）的书写实在糟糕。
- en: 'Blame it on schools for no longer emphasizing cursive handwriting if you must,
    but the problem remains: handwriting is hard to read and interpret. God forbid
    you have to deliver a letter penned by a doctor (good luck doing that!).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须责怪学校不再强调书写，那么问题仍然存在：书写难以阅读和理解。上帝保佑，如果你不得不递送一封由医生手写的信（祝你好运！）。
- en: Imagine, instead, if you had built a machine learning system that allows you
    to read handwriting. That's what we will be doing this chapter and the next; we
    will be building a type of machine-learning algorithm known as an artificial neural
    network, and in the next chapter, we will be expanding on the concept with deep
    learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你构建了一个机器学习系统，可以让你阅读手写文字。这正是我们将在本章和下一章中要做的事情；我们将构建一种称为人工神经网络的机器学习算法，在下一章中，我们将通过深度学习来扩展这一概念。
- en: In this chapter, we will learn the basics of neural networks, see how it's inspired
    by biological neurons, find a better way of representing them, and finally apply
    neural networks on handwriting to recognize digits.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习神经网络的基础知识，了解它是如何受到生物神经元启发的，找到更好的表示方法，并最终将神经网络应用于手写识别数字。
- en: A neural network
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个神经网络
- en: The term **neural network** can mean one of two things in modern parlance. The
    first refers to a network of neurons found in your brain. These neurons form specific
    networks and pathways and are vital to you understanding this very sentence. The
    second meaning of the term refers to an artificial neural network; that is, things
    we build in software to emulate a neural network in the brain.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代用语中，**神经网络**这个术语可以指两种不同的事物。第一种指的是你大脑中发现的神经元网络。这些神经元形成特定的网络和路径，对你理解这个句子至关重要。该术语的第二种含义指的是人工神经网络；即我们在软件中构建来模拟大脑中神经网络的实体。
- en: This, of course, has led to very many unfortunate comparisons between a biological
    neural network and an artificial neural network. To understand why, we must start
    at the beginning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这导致了生物神经网络和人工神经网络之间非常多的不幸比较。为了理解原因，我们必须从开始讲起。
- en: From here on, I shall spell **neuron** with a British spelling denoting a real
    neuron cell, while the American spelling, **neuron,** will be reserved for the
    artificial variant.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我将使用英国式的拼写来表示**神经元**，以表示真实的神经元细胞，而美国式的拼写，**neuron**，将保留用于人工变体。
- en: 'This following diagram is of a neuron:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示是一个神经元：
- en: '![](img/6ccfd1f8-ed10-48a7-beb1-2df575822988.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6ccfd1f8-ed10-48a7-beb1-2df575822988.png)'
- en: In general, a neuron typically consists of the soma (the general body of the
    cell that contains its nucleus), an optional axon covered in a kind of fatty tissue
    known as **myelin**, and dendrites. The latter two components ( the axon and dendrites)
    are particularly interesting because together they form a structure known as a
    synapse. Specifically, it's the end of an axon the terminal) that forms such synapses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个神经元由细胞体（包含其核的细胞的一般部分）、一个可选的、被一种称为**髓鞘**的脂肪组织覆盖的轴突和树突组成。后两个组成部分（轴突和树突）特别有趣，因为它们共同形成了一个称为突触的结构。具体来说，是轴突的末端（即突触）形成了这样的突触。
- en: The vast majority of synapses in mammalian brains are between axon terminals
    and dendrites. The typical flow of signals (chemical or electrical impulses) goes
    from one neuron, travels along the axon, and deposits its signal onto the next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 哺乳动物大脑中的绝大多数突触位于轴突末端和树突之间。信号的典型流动（化学或电脉冲）从一个神经元开始，沿着轴突传播，并将其信号沉积到下一个神经元上。
- en: '![](img/fee48211-f139-4b97-9a5b-3ef35750c1fe.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fee48211-f139-4b97-9a5b-3ef35750c1fe.png)'
- en: In the image above, we have three neurons, labelled A, B, and C. Imagine A receives
    a signal from an external source (like your eyes). It receives a signal that is
    strong enough that it passes the signal down the axon, which touches the dendrites
    of B via a synapse. B receives the signal and decides it doesn't warrant passing
    along the signal to C, so nothing goes down the axon of B.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图像中，我们有三个神经元，标记为A、B和C。想象一下A从外部来源（比如你的眼睛）接收一个信号。它接收到的信号足够强，以至于它可以通过轴突传递，通过突触接触到B的树突。B接收信号并决定它不值得将信号传递给C，所以B的轴突没有信号传递。
- en: And so we will now explore how you might emulate this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在将探讨如何模拟这一点。
- en: Emulating a neural network
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟神经网络
- en: 'Let''s simplify the preceding diagram of the neural network:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简化一下前面的神经网络图：
- en: '![](img/affc082b-26f6-48c2-b27c-7a5950eab893.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/affc082b-26f6-48c2-b27c-7a5950eab893.png)'
- en: We'll have a circle represent the body of the neuron, and we'll call it the
    **neuron**. The "dendrites" of the neuron receive inputs from other neurons (unshown)
    and add up all the inputs. Each input represents an input from another neuron;
    so, if you see three inputs, it means that this neuron is connected to three other
    neurons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会用一个圆圈来代表神经元的主体，我们称之为**神经元**。神经元的“树突”接收来自其他神经元（未显示）的输入并将所有输入加起来。每个输入代表来自另一个神经元的输入；所以，如果你看到三个输入，这意味着这个神经元连接了三个其他神经元。
- en: 'If the sum of the inputs exceeds a threshold value, then we can say the neuron
    "fires" or is activated. This simulates the activation potential of an actual
    neuron. For simplicity, let''s say if it fires, then the output will be 1; otherwise,
    it will be 0\. Here is a good emulation of it in Go code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入的总和超过一个阈值值，那么我们可以说神经元“放电”或被激活。这模拟了实际神经元的激活潜力。为了简单起见，让我们假设如果它放电，则输出将是1；否则，将是0。以下是它在Go代码中的良好模拟：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is generally known as a **perceptron**, and it's a faithful emulation of
    how neurons work, if your knowledge of how neurons work is stuck in the 1940s
    and 1950s.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为**感知器**，如果你对神经元工作原理的了解还停留在20世纪40年代和50年代，那么它就是对神经元工作原理的忠实模拟。
- en: 'Here is a rather interesting anecdote: As I was writing this section, King
    Princess'' 1950 started playing in the background and I thought it would be rather
    apt to imagine ourselves in the 1950s, developing the perceptron. There remains
    a problem: the artificial network we emulated so far cannot learn! It is programmed
    to do whatever the inputs tell it to do.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个相当有趣的故事：当我写这个部分的时候，King Princess的1950开始在背景中播放，我觉得想象自己在20世纪50年代开发感知器是非常合适的。仍然存在一个问题：我们迄今为止模拟的人工网络无法学习！它是被编程去做输入告诉它做的事情。
- en: 'What does it mean for an artificial neural network "to learn" exactly? There''s
    an idea that arose in neuroscience in the 1950s, called the **Hebbian Rule**,
    which can be briefly summed up as: *Neurons that fire together grow together*.
    This gives rise to an idea that some synapses are thicker; hence,they have stronger
    connections, and other synapses are thinner; hence, they  have weaker connections.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “人工神经网络学习”究竟意味着什么？在20世纪50年代的神经科学中出现了一个想法，称为**赫布规则**，可以简要概括为：“一起放电的神经元会一起生长”。这引发了一个想法，即某些突触更厚；因此，它们有更强的连接，而其他突触更薄；因此，它们有较弱的连接。
- en: 'To emulate this, we would need to introduce the concept of a weighted value,
    the weight of which corresponds to the strength of the input from another neuron.
    Here''s a good approximation of this idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这一点，我们需要引入加权值的概念，其权重对应于来自另一个神经元的输入强度。以下是这个想法的一个很好的近似：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'At this point, if you are familiar with linear algebra, you might think to
    yourself that `total` is essentially a vector product. You would be absolutely
    correct. Additionally, if the threshold is 0, then you have simply applied a `heaviside`
    step function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，如果你熟悉线性代数，你可能会想到“total”本质上是一个向量积。你会完全正确。此外，如果阈值是0，那么你只是应用了一个`heaviside`阶跃函数：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In other words, we can summarize a single neuron in the following way:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以用以下方式总结一个单个神经元：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note in the last two examples, I switched over from `int` to a more canonical `float64`.
    The point remains the same: a single neuron is simply a function applied to a
    vector product.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在最后两个例子中，我从`int`切换到了更标准的`float64`。要点仍然是：单个神经元只是一个应用于向量积的函数。
- en: 'A single neuron does not do much. But stack a bunch of them together and arrange
    them by layers like so, and then suddenly they start to do more:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元并没有做什么。但是将它们堆叠起来，并按层排列，然后突然它们开始做更多的事情：
- en: '![](img/357ea737-22c2-41a6-93c6-7bf87ee3c5c5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/357ea737-22c2-41a6-93c6-7bf87ee3c5c5.png)'
- en: 'Now we come to the part that requires a conceptual leap: if a neuron is essentially
    just a vector product, *stacking* the neurons simply makes it a matrix!'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到需要概念飞跃的部分：如果一个神经元本质上只是一个向量积，*堆叠*神经元就简单地使其成为一个矩阵！
- en: 'Given an image can be represented as a flat slice of `float64` , the `vectorDot` function
    is replaced with `matVecMul`, which is a function that multiplies a matrix and
    vector to return a vector. We can write a function representing the neural layer
    like so:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图像可以表示为一个 `float64` 的平面切片，`vectorDot` 函数被 `matVecMul` 函数替换，这是一个将矩阵和向量相乘以返回向量的函数。我们可以这样写一个表示神经层的函数：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Linear algebra 101
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数 101
- en: I want to take a detour to talk about linear algebra. It's featured quite a
    bit so far in this book, although it was scarcely mentioned by name. In fact linear
    algebra underlies every chapter we've done so far.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我想绕道谈谈线性代数。到目前为止，这本书中已经提到了很多，尽管它并没有被明确提及。事实上，线性代数是我们迄今为止所做每一章的基础。
- en: 'Imagine you have two equations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有两个方程：
- en: '![](img/7132731e-26cc-4ef8-bb5e-5042f0bfa77b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7132731e-26cc-4ef8-bb5e-5042f0bfa77b.png)'
- en: 'Let''s say ![](img/08f454a0-6b2a-4c0b-88b5-cd421a39f6bb.png) and ![](img/8f907ffd-9c6a-4dc8-9f15-16fd50bfe601.png) is ![](img/2795f1bc-6f85-449c-a35c-4eeb72ace9c3.png) and ![](img/92bc7148-9b76-4912-b340-d2b1bb340fd8.png), respectively.
    We can now write the following equations as such:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/08f454a0-6b2a-4c0b-88b5-cd421a39f6bb.png) 和 ![](img/8f907ffd-9c6a-4dc8-9f15-16fd50bfe601.png)
    分别是 ![](img/2795f1bc-6f85-449c-a35c-4eeb72ace9c3.png) 和 ![](img/92bc7148-9b76-4912-b340-d2b1bb340fd8.png)，我们可以现在这样写出以下方程：
- en: '![](img/7e57f3c1-9194-4c7e-99fd-bf1aea04ff75.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e57f3c1-9194-4c7e-99fd-bf1aea04ff75.png)'
- en: And we can solve it using basic algebra (please do work it out on your own): ![](img/6a2280ac-9c8d-464b-8d66-b85679e23414.png) and ![](img/bd4f7b98-14cc-46c9-a203-c9a3433c88c8.png).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用基本的代数来解它（请自己动手计算）：![](img/6a2280ac-9c8d-464b-8d66-b85679e23414.png) 和
    ![](img/bd4f7b98-14cc-46c9-a203-c9a3433c88c8.png)。
- en: 'What if you have three, four, or five simultaneous equations? It starts to
    get cumbersome to calculate these values. Instead, we invented a new notation:
    the matrix notation, which will allow us to solve simultaneous equations faster.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有三个、四个或五个联立方程呢？计算这些值开始变得繁琐。相反，我们发明了一种新的符号：矩阵符号，这将使我们能够更快地解联立方程。
- en: It had been used for about 100 years without a name (it was first termed "matrix"
    by James Sylvester) and formal rules were being used until Arthur Cayley formalized
    the rules in 1858\. Nonetheless, the idea of grouping together parts of an equation
    into a bunch had been long used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它在没有名字的情况下使用了大约100年（它最初被詹姆斯·西尔维斯特称为“矩阵”），并且直到1858年亚瑟·凯莱将这些规则形式化之前，一直在使用正式的规则。尽管如此，将方程的一部分组合在一起作为一个整体的想法已经被长期使用。
- en: 'We start by "factoring" out the equations into their parts:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将方程“分解”成它们的各个部分：
- en: '![](img/5a2fcb74-6071-44a3-92d4-ead6ddc3d1b6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a2fcb74-6071-44a3-92d4-ead6ddc3d1b6.png)'
- en: The horizontal line indicates that it's two different equations, not that they
    are ratios. Of course, we realize that we've been making too many repetitions
    so we simplify the matrix of ![](img/42c27d9e-dee6-4c3f-88cb-2c7b7a5d15c7.png) and ![](img/9900269b-ee42-4ed5-b00f-83d1626aa677.png)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 水平线表示这是两个不同的方程，而不是它们是比例关系。当然，我们意识到我们已经重复得太多了，所以我们简化了 ![](img/42c27d9e-dee6-4c3f-88cb-2c7b7a5d15c7.png)
    和 ![](img/9900269b-ee42-4ed5-b00f-83d1626aa677.png) 的矩阵。
- en: '![](img/fc172162-8e7b-4190-878a-134847ec9ca1.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc172162-8e7b-4190-878a-134847ec9ca1.png)'
- en: 'Here, you can see that ![](img/fd1741f7-280b-4b26-b879-2f85d5566a96.png) and ![](img/39e7e985-0ee3-413e-9821-7a30732beda3.png) is
    only ever written once. It''s rather unneat to write it the way we just wrote
    it, so instead we write it like so to be neater:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到 ![](img/fd1741f7-280b-4b26-b879-2f85d5566a96.png) 和 ![](img/39e7e985-0ee3-413e-9821-7a30732beda3.png)
    只被写了一次。我们刚才写的方式相当不整洁，所以我们用这种方式来写得更整洁：
- en: '![](img/aa7b5549-dad6-4e96-bbf6-f70d9b3aa10c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa7b5549-dad6-4e96-bbf6-f70d9b3aa10c.png)'
- en: 'Not only do we write it like so, we give specific rule on how to read this
    notation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅我们这样写，我们还给出了如何阅读这种符号的具体规则：
- en: '![](img/43c4d6a6-d9c2-4fa6-accd-860c1c436f07.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43c4d6a6-d9c2-4fa6-accd-860c1c436f07.png)'
- en: 'We should give the matrices names so we can refer to them later on:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该给矩阵命名，这样我们以后可以引用它们：
- en: '![](img/62d17701-c859-467c-ad8c-2596e593f698.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62d17701-c859-467c-ad8c-2596e593f698.png)'
- en: The bold indicates that the variable holds multiple values. An uppercase indicates
    a matrix (![](img/a768d36c-8050-4467-bb39-65ba0d19dd87.png)), and lowercase indicates
    a vector ( ![](img/babf308e-bdf6-464d-9744-e4c275f220ff.png) and ![](img/42393974-379f-47dc-9f4c-b991cd3bcaff.png).
    This is to distinguish it from scalar variables (variables that only hold one
    value), which are typically written without boldface (for example, ![](img/c248220b-8bfa-49d8-bb7f-bf56e777ef95.png) and ![](img/a5e79da2-e584-41e7-a42c-fddcd77de875.png)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 粗体表示变量持有多个值。大写表示矩阵 (![](img/a768d36c-8050-4467-bb39-65ba0d19dd87.png))，小写表示向量
    ( ![](img/babf308e-bdf6-464d-9744-e4c275f220ff.png) 和 ![](img/42393974-379f-47dc-9f4c-b991cd3bcaff.png))。这是为了与只持有单个值的标量变量（通常不使用粗体）区分开来（例如， ![](img/c248220b-8bfa-49d8-bb7f-bf56e777ef95.png) 和 ![](img/a5e79da2-e584-41e7-a42c-fddcd77de875.png))。
- en: 'To solve the equations, the solution is simply this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解这些方程，解法很简单：
- en: '![](img/2223ba92-4d76-4dea-a40c-60ce62525d48.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2223ba92-4d76-4dea-a40c-60ce62525d48.png)'
- en: The ![](img/bb1d7cdb-9e10-4977-9680-9a41e8341f59.png) superscript indicates
    an inverse is to be taken. This is rather consistent with normal algebra.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh:  ![](img/bb1d7cdb-9e10-4977-9680-9a41e8341f59.png) 的上标表示需要取逆。这与正常的代数相当一致。
- en: Consider a problem ![](img/d594d7b5-c706-4002-8aea-bc9f6c475e27.png) where you
    are asked to solve for ![](img/4606db1b-66e9-4465-a4a3-7ea25d8a7d28.png). The
    solution is simply ![](img/84bc32bb-efd6-4cc3-bd10-034f7a25b4c4.png). Or we can
    rewrite it as a series of multiplications as![](img/2877c3bd-3326-4c07-8d7e-f1ebddc3d4da.png).
    And what do we know about fractions where one is the numerator? They can simply
    be written as a power to the -1\. Hence, we arrive at this solution equation: ![](img/3287306b-cdd3-47c6-979e-a3a489b222bd.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个问题 ![](img/d594d7b5-c706-4002-8aea-bc9f6c475e27.png)，其中要求你解出 ![](img/4606db1b-66e9-4465-a4a3-7ea25d8a7d28.png)。解法很简单 ![](img/84bc32bb-efd6-4cc3-bd10-034f7a25b4c4.png)。或者我们可以将其重写为一系列乘法，如![](img/2877c3bd-3326-4c07-8d7e-f1ebddc3d4da.png)。关于分数，我们知道什么？如果一个分数是分子，它可以简单地写成-1的幂。因此，我们得到了这个解方程： ![](img/3287306b-cdd3-47c6-979e-a3a489b222bd.png)
- en: Now if you squint very carefully, the scalar version of the equation looks very
    much like the matrix notation version of the equation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果你非常仔细地眯着眼睛看，方程的标量版本看起来非常像方程的矩阵表示版本。
- en: How to calculate the inverse of a matrix is not what this book aims to do. Instead,
    I encourage you to pick up a linear algebra text book. I highly recommend Sheldon
    Axler's *Linear Algebra Done Right* (Springer Books).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算矩阵的逆不是这本书的目标。相反，我鼓励你找一本线性代数教科书。我强烈推荐Sheldon Axler的《线性代数这样做是正确的》（Springer
    Books）。
- en: 'To recap, here are the main points:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下是主要观点：
- en: Matrix multiplication and notation were invented to solve simultaneous equations.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法和符号是为了解决联立方程而发明的。
- en: To solve the simultaneous equation, we treat the equation as though the variables
    were scalar variables and use inverses.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了解这个联立方程，我们将方程视为变量是标量变量，并使用逆来处理。
- en: 'Now comes the interesting part. Using the same two equations, we will turn
    the question around. What if we knew what ![](img/711bc8fc-a9f0-415b-8a8f-b44628ae2ae0.png) and ![](img/a93465f3-1ea7-4725-bd05-ab0aa6dabfbd.png) is
    instead? The equations would now look something like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分。使用相同的两个方程，我们将问题反过来。如果我们知道 ![](img/711bc8fc-a9f0-415b-8a8f-b44628ae2ae0.png) 和 ![](img/a93465f3-1ea7-4725-bd05-ab0aa6dabfbd.png) 是什么，方程现在看起来会像这样：
- en: '![](img/64ca7973-411f-4c88-af64-a1df61a8a353.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64ca7973-411f-4c88-af64-a1df61a8a353.png)'
- en: 'Writing it in matrix form, we get the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将其写成矩阵形式，我们得到以下：
- en: '![](img/993b3a8b-ad23-404d-a5d5-171116a8911a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/993b3a8b-ad23-404d-a5d5-171116a8911a.png)'
- en: 'Careful readers would have caught an error by now: there are *four* variables
    (![](img/9eca9944-2c2a-4642-a5c0-a79f3c6aa4c2.png), ![](img/8e698660-7b72-497f-a335-8e797a420e28.png), ![](img/9a666a8f-89b7-4038-b0e9-37dcf01f05ed.png),
    and ![](img/ddf46523-0dcc-437d-89d2-0d66c71ed1e0.png)), but only *two* equations.
    From high-school math, we learn that you can''t solve a system of equations where
    there are fewer equations than there are variables!'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者现在应该已经发现了错误：这里有*四个*变量 (![](img/9eca9944-2c2a-4642-a5c0-a79f3c6aa4c2.png),
    ![](img/8e698660-7b72-497f-a335-8e797a420e28.png), ![](img/9a666a8f-89b7-4038-b0e9-37dcf01f05ed.png),
    和 ![](img/ddf46523-0dcc-437d-89d2-0d66c71ed1e0.png))，但只有*两个*方程。从高中数学中，我们知道你不能解一个方程组，其中方程的数量少于变量的数量！
- en: The thing is, your high school math teacher kind of lied to you. It is sort
    of possible to solve this, and you've already done so yourself in [Chapter 2](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml),
    *Linear Regression - House Price Prediction*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，你的高中数学老师有点骗了你。某种程度上是可以解决的，你已经在[第二章](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml)，*线性回归
    - 房价预测*中自己做到了。
- en: 'In fact, most machine learning problems can be re-expressed in linear algebra,
    specifically of this form:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，大多数机器学习问题都可以用线性代数重新表达，具体形式如下：
- en: '![](img/0736a77e-0079-4872-8275-c5b4c2de61b1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0736a77e-0079-4872-8275-c5b4c2de61b1.png)'
- en: 'And this in my opinion, is the right way to think about artificial neural networks:
    a series of mathematical functions, not an analogue of biological neurons. We
    will explore this a bit more in the next chapter. In fact, this understanding
    is vital to the understanding of deep learning and why it works.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这是正确思考人工神经网络的方式：一系列数学函数，而不是生物神经元的模拟。我们将在下一章中进一步探讨这一点。实际上，这种理解对于理解深度学习和它为什么有效至关重要。
- en: For now, it suffices to follow on with the more common notion that an artificial
    neural network is similar in actions to a biologically inspired neural network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，继续跟随更常见的观点，即人工神经网络在行为上类似于受生物启发的神经网络。
- en: Exploring activation functions
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索激活函数
- en: The thing about linear algebra is, it's linear. It is useful when the change
    of the output is proportional to the change in input. The real world is full of
    non-linear functions and equations. Solving non-linear equation is hard with a
    capital H. But we've got a trick. We can take a linear equation, and then add
    a non-linearity to it. This way, the function becomes non-linear!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的特性是它是线性的。当输出变化与输入变化成比例时，它是有用的。现实世界充满了非线性函数和方程。用大写的H解决非线性方程是困难的。但我们有一个技巧。我们可以取一个线性方程，然后向其中添加一个非线性。这样，函数就变得非线性了！
- en: Following from this view, you can view an artificial neural network as a generic
    version of all the previous chapters we've gone through so far.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个观点出发，你可以将人工神经网络视为我们迄今为止所经历的所有章节的通用版本。
- en: Throughout the history of artificial neural networks, the community has favored
    particular activation functions in a fashionable way. In the early days, the Heaviside
    function was favored. Gradually, the community moved toward favoring differentiable,
    continuous functions, such as sigmoid and tanh. But lately, the pendulum of fashion
    has swung back toward the harder, seemingly discontinuous functions. The key is
    that we've learned new tricks on how to differentiate functions, such as the **rectified
    linear unit** (**ReLu**).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络的历史上，社区以一种时尚的方式青睐特定的激活函数。在早期，Heaviside函数受到青睐。逐渐地，社区转向青睐可微分的连续函数，如sigmoid和tanh。但最近，时尚的钟摆又回到了更硬、看似不连续的函数上。关键是，我们学会了如何对函数进行微分的新技巧，例如**修正线性单元**（**ReLu**）。
- en: 'Here are some of the more popular activation functions over time:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，一些激活函数因其流行而受到青睐：
- en: '![](img/bacc949b-f005-4866-b8f2-c595cafa4c97.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bacc949b-f005-4866-b8f2-c595cafa4c97.png)'
- en: '![](img/de354d5c-72cd-4045-8481-84ba7681540d.png)'''
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/de354d5c-72cd-4045-8481-84ba7681540d.png)'
- en: '![](img/ce2830b2-db5e-4508-aa69-2f478de83a22.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce2830b2-db5e-4508-aa69-2f478de83a22.png)'
- en: One thing to note about these is that these functions are all nonlinear and
    they all have a hard limit on the y axis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些函数的一点需要注意的是，这些函数都是非线性的，它们在y轴上都有一个硬限制。
- en: The vertical ranges of the activation functions are limited, but the horizontal
    ranges are not. We can use biases to adjust how our activation functions look.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的垂直范围是有限的，但水平范围不是。我们可以使用偏置来调整我们的激活函数看起来如何。
- en: It should be noted that biases can be zero. It also means that we can omit biases.
    Most of the time, for more complex projects, this is fine, though adding biases
    will add to the accuracy of the neural network.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，偏置可以是零。这也意味着我们可以省略偏置。大多数时候，对于更复杂的项目来说，这是可以的，尽管添加偏置会增加神经网络的准确性。
- en: Learning
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习
- en: I want you to think about how you learn. Not the learning styles, mind; no,
    I want you to give your learning process a long hard deep thought. Think of the
    various ways you learn. Maybe you've touched a stove while it's hot once. Or if
    you ever learned a new language, maybe you started out by memorizing phrases before
    becoming fluent. Think about all the chapters that had preceded this. What do
    they have in common?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我想让你思考你是如何学习的。不是学习风格，不，我想让你对你的学习过程进行深入的思考。想想你学习的各种方式。也许你曾经热的时候碰过炉子。或者如果你曾经学习过一门新语言，也许你开始时是通过记忆短语来成为流利的。想想所有先于这一章的章节。它们有什么共同点？
- en: In broad strokes, learning is done by means of corrections. If you touched a
    stove while it's hot, you made a mistake. The correction is to never touch a stove
    when it's hot ever again. You've learned how not to touch the stove while it's
    hot.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，学习是通过纠正来实现的。如果你在热的时候碰到了炉子，你犯了错误。纠正的方法是永远不要再在热的时候碰炉子。你已经学会了如何在热的时候不碰炉子。
- en: Similarly, the way a neural network learns is by means of correction. If we
    want to train a machine to learn to classify handwriting, we would need to provide
    some sample images, and tell the machine which are the correct labels. If the
    machine predicted the labels wrongly, we need to tell it to change something in
    the neural network and try again.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，神经网络的学习方式是通过纠正来实现的。如果我们想要训练一台机器学习识别手写文字，我们需要提供一些样本图像，并告诉机器哪些是正确的标签。如果机器预测标签错误，我们需要告诉它改变神经网络中的某些东西并再次尝试。
- en: 'What can be changed? The weights of course. The inputs can''t be changed; they''re
    inputs. But we can always try different weights. Hence, the process of learning
    can be broken down into two steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可以改变什么？当然是权重。输入不能改变；它们是输入。但我们可以尝试不同的权重。因此，学习过程可以分为两个步骤：
- en: Telling the neural network that it is wrong when it made a mistake.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当神经网络犯错时，告诉它它是错的。
- en: Updating the weights so that the next try will yield a better result.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重，以便下一次尝试能得到更好的结果。
- en: 'When broken down like this, we have a good idea of how to proceed next. One
    way would be a binary determination mechanism: if the neural network predicted
    the correct answer, don''t update the weights. If it''s wrong, update the weights.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这样分解后，我们就有了一个很好的下一步进行的方法。一种方法是通过二元决定机制：如果神经网络预测了正确答案，则不更新权重。如果它错了，则更新权重。
- en: How to update the weights, then? Well, one way would be to completely replace
    the weight matrix with new values and try again. Since the weight matrix is filled
    from values pulled from a random distribution, the new weight matrix would be
    a new random matrix.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何更新权重呢？好吧，一种方法是用新的值完全替换权重矩阵并再次尝试。由于权重矩阵是由从随机分布中抽取的值填充的，新的权重矩阵将是一个新的随机矩阵。
- en: It should be quite obvious that these two methods, when combined, would take
    a very very long time before the neural network learns anything; it's as if we're
    simply guessing our way into the correct weight matrices.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这两种方法结合在一起，神经网络学会任何东西都需要非常非常长的时间；这就像我们只是在猜测正确的权重矩阵一样。
- en: Instead, modern neural networks use the concept of **backpropagation** to tell
    the neural network that it's made a mistake, and some form of **gradient descent** to
    update the weights.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，现代神经网络使用**反向传播**的概念来告诉神经网络它犯了错误，并使用某种形式的**梯度下降**来更新权重。
- en: 'The specifics of backpropagation and gradient descent are outside the scope
    of this chapter (and book). I''ll, however, briefly run through the big ideas
    by sharing a story. I was having lunch with a couple of friends who also work
    in machine learning and that lunch ended with us arguing. This was because I had
    casually mentioned that backpropagation was "discovered", as opposed to "invented".
    My friends were adamant that backpropagation was invented, not discovered. My
    reasoning was simple: Mathematics is "discovered" if multiple people stumble upon
    it with the same formulation. Mathematics is "invented" if there were no parallel
    discovery of it.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播和梯度下降的具体内容超出了本章（和本书）的范围。然而，我会通过分享一个故事来简要地介绍这些基本概念。我和几位也在机器学习领域工作的朋友一起吃午饭，那次午餐以我们争论告终。这是因为我随意提到反向传播是“发现”的，而不是“发明”的。我的朋友们坚决认为反向传播是发明的，而不是发现的。我的推理很简单：如果多个人以相同的公式偶然发现数学，那么数学是“发现”的。如果没有人平行地发现它，那么数学是“发明”的。
- en: 'Backpropagation, in various forms, has been constantly rediscovered over time.
    The first time backpropagation was discovered was in the invention of linear regression.
    I should note that it was a very specific form of backpropagation specific to
    linear regression: the sum of squared errors can be propagated back to its inputs
    by differentiating the result of the sum of squared errors with regard to the
    inputs.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播，以各种形式，在时间中被不断重新发现。反向传播第一次被发现是在线性回归的发明中。我应该指出，它是一种非常具体的反向传播形式，专门针对线性回归：平方误差之和可以通过对平方误差之和关于输入的导数来反向传播到其输入。
- en: We start with a cost. Remember how we have to tell the neural network that it's
    made a mistake. We do so by telling the neural network the cost of making a prediction.
    This is called a cost function. We can define a cost so that when the neural network
    makes a correct prediction, the cost is low, and when the neural network makes
    a wrong prediction, the cost is high.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个成本开始。记住我们不得不告诉神经网络它犯了一个错误。我们通过告诉神经网络预测的成本来做这件事。这被称为成本函数。我们可以定义一个成本，使得当神经网络做出正确预测时，成本较低，而当神经网络做出错误预测时，成本较高。
- en: 'Imagine for now, that the cost function is ![](img/29e4aff9-eb8a-4967-8815-49f749fe2f4f.png).
    How do you know at what values of ![](img/68ac22cd-9ee1-4dbc-ad85-7c9461a01602.png) the
    cost will be lowest? From high- school math, we know that the solution is to differentiate ![](img/3ac8cdda-cb4c-49f4-9fca-cb77ddb71c50.png) with
    regard to ![](img/0676e8ac-1a8f-4534-b806-dcfb07833e16.png) and solve for the
    solution when it''s 0:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，成本函数是 ![](img/29e4aff9-eb8a-4967-8815-49f749fe2f4f.png)。你如何知道在 ![](img/68ac22cd-9ee1-4dbc-ad85-7c9461a01602.png) 的哪些值下成本最低？从高中数学我们知道，解决方案是对 ![](img/3ac8cdda-cb4c-49f4-9fca-cb77ddb71c50.png) 关于 ![](img/0676e8ac-1a8f-4534-b806-dcfb07833e16.png) 求导，并求解当它为0时的解：
- en: '![](img/b2bc33e9-f947-41d0-9f7b-681e8fc3ef30.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2bc33e9-f947-41d0-9f7b-681e8fc3ef30.png)'
- en: Backpropagation takes the same cue. In short, backpropagtion is just a bunch
    of partial differentiations with regard to the weights. The main difference between
    our toy example and real backpropagation is that the derivation of our expression
    is easy to solve. For more complex mathematical expressions, it can be computationally
    too expensive to compute the solution. Instead, we rely on gradient descent to
    find the answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播采取了同样的线索。简而言之，反向传播只是一系列关于权重的偏导数。我们的玩具示例和真实反向传播之间的主要区别在于，我们表达式的推导很容易解决。对于更复杂的数学表达式，计算解决方案可能过于昂贵。相反，我们依赖于梯度下降来找到答案。
- en: Gradient descent assumes we start our x somewhere and we update the x iteratively
    toward the lowest cost. In each iteration, we update the weights. The simplest
    form of gradient descent is to add the gradient of the weights to the weights
    themselves.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降假设我们从某个地方开始我们的x，并通过迭代更新x以趋向最低成本。在每次迭代中，我们更新权重。梯度下降的最简单形式是将权重的梯度加到权重本身上。
- en: The key takeaway is the powerful notion that you can tell the inputs that an
    error has occurred by performing differentiation of the function and finding a
    point at which the derivatives are at its minimum.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的启示是这样一个强大的概念：通过执行函数的微分并找到一个导数最小的点，你可以告诉输入已经发生了一个错误。
- en: The project
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: The project we're embarking on is the one as mentioned in the opening paragraphs.
    The dataset which we are going to classify is a collection of handwritten numbers
    originally collected by the National Institute of Standards and Technology and
    later modified by Yann LeCun's team. Our goal is to classify the handwritten numbers
    as either one of 0, 1, 2... 9.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将开始的项目就是开头段落中提到的那个。我们将要分类的数据集是由国家标准与技术研究院最初收集的一组手写数字，后来由Yann LeCun的团队修改。我们的目标是将这些手写数字分类为0、1、2...
    9中的一个。
- en: We're going to build a basic neural network with the understanding that neural
    networks are applied linear algebra, and we'll be using Gorgonia for this and
    the next chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于神经网络是应用线性代数的理解来构建一个基本的神经网络，并且我们将使用Gorgonia来完成这一章和下一章的内容。
- en: To install Gorgonia, simply run `go get -u gorgonia.org/gorgonia` and `go get
    -u gorgonia.org/tensor`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Gorgonia，只需运行 `go get -u gorgonia.org/gorgonia` 和 `go get -u gorgonia.org/tensor`。
- en: Gorgonia
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gorgonia
- en: Gorgonia is a library that facilitates efficient mathematical operations for
    the purposes of building deep neural networks. It operates on the fundamental
    understanding that neural networks are mathematical expressions. As such it is
    quite easy to build neural networks using Gorgonia.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 是一个库，它通过在构建深度神经网络时进行高效数学运算来简化操作。它基于这样的基本理解：神经网络是数学表达式。因此，使用 Gorgonia
    构建神经网络相当容易。
- en: 'A note on the chapters: Because Gorgonia is a relatively huge library, parts
    of this chapter will elide over some things about Gorgonia but will be expanded
    upon in the next chapter, as well as another Packt book,  *Hands On Deep Learning
    in Go*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关于章节的说明：因为 Gorgonia 是一个相对庞大的库，所以本章将省略一些关于 Gorgonia 的内容，但将在下一章以及另一本 Packt 书籍《Hands
    On Deep Learning in Go》中进一步展开。
- en: Getting the data
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The data for the MNIST data can be found in the repository for this chapter.
    In its original form, it's not in a standard image format. So, we will need to
    parse the data into an acceptable format.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据的数据可以在本章的存储库中找到。在其原始形式中，它不是标准的图像格式。因此，我们需要将数据解析为可接受格式。
- en: 'The dataset comes in two parts: labels and images. So here are a couple of
    functions, designed to read and parse the MNIST file:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集分为两部分：标签和图像。因此，这里有一些函数，用于读取和解析 MNIST 文件：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: First, the functions read the file from a `io.Reader` and reading a set of `int32`s.
    These are the metadata of the file. The first `int32` is a magic number that is
    used to indicate if a file is a labels file or a file of images. `n` indicates
    the number of images or labels the file contains. `nrow` and `ncol` are metadata
    that exists in the file, and indicates how many rows/columns there are in each
    image.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，函数从 `io.Reader` 读取文件并读取一组 `int32`。这些是文件的元数据。第一个 `int32` 是一个魔术数字，用于指示文件是标签文件还是图像文件。`n`
    表示文件包含的图像或标签数量。`nrow` 和 `ncol` 是文件中存在的元数据，表示每个图像中的行数/列数。
- en: 'Zooming into the `readImageFile` function, we can see that after all the metadata
    has been read, we know to create a `[]RawImage` of size `n`. The image format
    used in the MNIST dataset is essentially a slice of 784 bytes (28 columns and
    28 rows). Each byte therefore represents a pixel in the image. The value of each
    byte represents how bright the pixel is, ranging from 0 to 255:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深入到 `readImageFile` 函数中，我们可以看到在读取所有元数据之后，我们知道要创建一个大小为 `n` 的 `[]RawImage`。MNIST
    数据集中使用的图像格式基本上是一个 784 字节的切片（28 列和 28 行）。因此，每个字节代表图像中的一个像素。每个字节的值表示像素的亮度，范围从 0
    到 255：
- en: '![](img/6c023a57-bc75-44bd-9a8b-8783b8f4e46e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![示例图像](img/6c023a57-bc75-44bd-9a8b-8783b8f4e46e.png)'
- en: 'The preceding image is an example of an MNIST image blown up. At the top-left
    corner, the index of the pixel in a flat slice is 0\. At the top right corner,
    the index of the pixel in a flat slice is 27\. At the bottom-left corner, the
    index of the pixel in a flat slice is 755\. And, finally, at the bottom-right
    corner, the index is 727.  This is an important concept to keep in mind: A 2D
    image can be represented as a 1D slice.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是 MNIST 图像放大的示例。在左上角，平面切片中像素的索引为 0。在右上角，平面切片中像素的索引为 27。在左下角，平面切片中像素的索引为
    755。最后，在右下角，索引为 727。这是一个需要记住的重要概念：一个二维图像可以表示为一个一维切片。
- en: Acceptable format
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可接受格式
- en: 'What is an acceptable format to represent the image? A slice of bytes is useful
    for reading and displaying the image, but it''s not particularly useful for doing
    any machine learning. Rather, we should want to represent the image as a slice
    of floating points. So, here''s a function to convert a byte into a `float64`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 什么格式是表示图像的可接受格式？字节切片对于读取和显示图像很有用，但并不特别适用于进行任何机器学习。相反，我们希望将图像表示为浮点数的切片。所以，这里有一个将字节转换为
    `float64` 的函数：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is essentially a scaling function that scales from 0-255 to between 0.0
    and 1.0\. There is an additional check; if the value is 1.0, we return 0.999 instead
    of 1\. This is mainly due to the fact that when values are 1.0, numerical instability
    tends to happen, as mathematical functions tend to act weirdly. So instead, replace
    1.0 with values that are very close to 1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的缩放函数，将 0-255 的范围缩放到 0.0 和 1.0 之间。还有一个额外的检查；如果值是 1.0，我们返回 0.999 而不是 1。这主要是因为当值为
    1.0 时，数值不稳定性往往会发生，因为数学函数往往会表现得古怪。所以，用非常接近 1 的值来替换 1.0。
- en: So now, we can make a `RawImage` into a `[]float64`. And because we have `N` images
    in the form of `[]RawImage`, we can make it into a `[][]float64`, or a matrix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: From images to a matrix
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we've established that we can convert a list of images in a special format
    in to a slice of slices of `float64`. Recall from earlier, that when you stack
    neurons together they form a matrix, and the activation of a neural layer is simply
    a matrix-vector multiplication. And when the inputs are stacked together, it's
    simply matrix-matrix multiplication.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: We technically can build a neural network with just `[][]float64`. But the end
    result will be quite slow. Collectively as a species, we have had approximately
    40 years of developing algorithms for efficient linear algebra operations, such
    as matrix multiplication and matrix-vector multiplication. This collection of
    algorithms are generally known as BLAS (Basic Linear Algebra Subprograms).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: We have been, up to this point in the book, using libraries built on top of
    a library that provide BLAS functions, namely  Gonum's BLAS library. If you had
    been following the book up to this point, you would have it installed already.
    Otherwise, run `go get -u gonum.org/v1/gonum/...`, which would install the entire
    suite of Gonum libraries.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the way BLAS works in general, we need a better way of representing
    matrices than `[][]float64`. Here we have two options:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Gonum's `mat` library
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gorgonia's `tensor` library
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Gorgonia's `tensor`? The reason for `tensor` is quite simple. It plays well
    with Gorgonia itself, which requires multidimensional arrays. Gonum's `mat` only
    takes up to two dimensions, while in the next chapter we'll see a use of four-dimensional
    arrays.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor?
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fundamentally tensors are very much like vectors. The idea is stolen from physics.
    Imagine pushing a box on a two-dimensional plane. If you push the box with a force
    of 1 Newton along the *x* axis, there is no force applied to the *y* axis. You
    would write the vector as such: `[1, 0]`. If the box were moving along the *x* axis
    with at a speed of 10 km/h and along the *y* axis with a speed of 2 km/h, you
    would write the vector as such: `[10, 2]`. Note that they are unitless: the first
    example was a vector of Newtons, the second example was a vector with km/h as
    its units.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In short, it is a representation of something (a force, a speed, or anything
    with magnitude and direction) applied to a direction. From this idea, computer
    science co-opted the name vector. But in Go, they're called a **slice**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'So what is a tensor? Eliding a lot of the details but without a loss of generality,
    a tensor is like a vector. Except multidimensional. Imagine if you were to describe
    two speeds along the plane (imagine a silly putty being stretched in two directions
    at different speeds): `[1, 0]` and `[10, 2]`. You would write it as such:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '`⎡ 1 0⎤`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`⎣10 2⎦`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This is also called a matrix (when it's two-dimensional). It's called a 3-Tensor
    when it's three-dimensional, 4-Tensor when its four-dimensional, and so on and
    so forth. Note that if you have a third speed (that is, the silly putty being
    stretched in a third direction), you wouldn't have a 3-Tensor. Instead you'd still
    have  a matrix, with three rows.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: To visualize a 3-Tensor while building on the previous example, imagine if you
    will, that the two directions that the silly putty was being pulled at was a slice
    in time. Then imagine another slice in time where the same silly putty is pulled
    in two directions again. So now you'd have two matrices. A 3-Tensor is what happens
    when you imagine stacking these matrices together.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert a `[]RawImage` to a `tensor.Tensor`, the code is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Gorgonia may be a bit confusing to beginners. So let me explain the code line
    by line. But first, you must be aware that like Gonum matrices, Gorgonia tensors,
    no matter how many dimensions, are also internally represented as a flat slice.
    Gorgonia tensors are a little more flexible in the sense that they can take more
    than a flat slice of `float64` ( it takes slices of other types too). This is
    called the backing slice or array. This is one of the fundamental reasons why
    performing linear algebra operations is more efficient in Gonum and Gorgonia than
    using plain `[][]float64`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '`rows := len(M)` and `cols := len(M[0])` are pretty self explanatory. We want
    to know the rows (that is, number of images) and columns (the number of pixels
    in the image).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '`b := make([]float64, 0, rows*cols)` creates the backing array with a capacity
    of `rows * cols`. This backing array is called a backing *array* because throughout
    the lifetime of `b`, the size will not change. Here we start with a length of
    `0` because we want to use the `append` function later on.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '`a := make([]T, 0, capacity)` is a good pattern to use to pre-allocate a slice.
    Consider a snippet that looks like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '`a := make([]int, 0)`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '`    for i := 0; i < 10; i++ {`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '`        a = append(a, i)`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: During the first call to append, the Go runtime will look at the capacity of `a`,
    and find it's `0`. So it will allocate some memory to create a slice of size 1\.
    Then on the second call to append, the Go runtime will look at the capacity of `a` and
    find that it's `1`, which is insufficient. So it will allocate twice the current
    capacity of the slice. On the fourth iteration, it will find the capacity of `a` is insufficient
    for appending and once again allocates twice the current capacity of the slice.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The thing about allocation is that it is an expensive operation. Occasionally
    the Go runtime may not only have to allocate memory, but copy the memory to a
    new location. This adds to the cost of appending to a slice.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: So instead, if we know the capacity of the slice upfront, it's best to allocate
    all of it in one shot. We can specify the length, but it's often a cause of indexing
    errors. So my recommendation is to allocate with the capacity and a length of
    `0`. That way, you can safely use append without having to worry about indexing
    errors.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: After creating a backing slice, we simply populate the backing slice with the
    values of the pixel, converted to a `float64` using the `pixelWeight` function
    that we described earlier.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call `tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))`, which
    returns a `*tensor.Dense`. The `tensor.WithShape(rows, cols)` construction option
    creates a `*tensor.Dense` with the specified shape while `tensor.WithBacking(b)` simply
    uses the already pre-allocated and pre-filled `b` as a backing slice.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The `tensor` library will simply reuse the entire backing array so that fewer
    allocations are made. What this means is  you have to be careful when handling `b`.
    Modifying the contents of `b` afterward will change the content in the `tensor.Dense` as
    well. Given that `b` was created in the `prepareX` function, once the function
    has returned, there's no way to modify the contents of `b`. This is a good way
    to prevent accidental modification.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: From labels to one-hot vectors
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that neural networks built in Gorgonia only take `tensor.Tensor`s as
    inputs. Therefore, the labels will also have to be converted into `tensor.Tensor`.
    The function is quite similar to `prepareX`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'What we''re building here is a matrix with *N* rows and ten columns. The specifics
    of why we build a matrix of `(N,10)` will be explored in the next chapter, but
    for now let''s zoom into an imaginary row. Imagine the first label, `(int(N[i]))`, is `7`.
    The row will look like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This is called a one-hot vector encoding. It will be useful to us later, and
    will expanded upon in the next chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s also useful to have visualization when we are dealing with image data.
    Earlier we had converted our image pixels from a `byte` to a `float64` using `pixelWeight`.
    It''d be instructive to also have the reverse function:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here''s how to visualize 100 of the images:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The dataset is a huge slice of images. We need to figure out how many we want
    first;  hence, `N := rows * cols`. Having the number we want, we then slice using `data.Slice(makeRS(0,
    N), nil)`, which slices the tensor along the first axis. The sliced tensor is
    then reshaped into a four-dimensional array with `sliced.Reshape(rows, cols, 28,28)`.
    The way you can think about it is to have a stacked rows and columns of 28x28
    images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**A primer on slicing**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'A `*tensor.Dense` acts very much like a standard Go slice; just as you can
    slice `a[0:2]`, you can do the same with Gorgonia''s tensors. The `.Slice()` method
    for all tensors accepts a `tensor.Slice` descriptor, defined as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`type Slice interface {`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '`    Start() int`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '`    End() int`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '`    Step() int`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: As such, we would have to make our own data type that fulfills the `Slice` interface.
    It's defined in the `utils.go` file of this project. `makeRS(0, N)` simply reads
    as if we were doing `data[0:N]`. Details and reasoning for this API can be found
    on the Gorgonia tensor Godoc page.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Then a grayscale image is created using the built-in image package: `canvas
    := image.NewGray(rect)`. A `image.Gray` is essentially a slice of bytes and each
    byte is a pixel. What we need to do next is to fill up the pixels. Quite simply,
    we simply loop through the columns and rows in each patch, and we fill it up with
    the correct value extracted from the tensor. The `reversePixelWeight` function is
    used to convert the float into a byte, which is then converted into a `color.Gray`.
    The pixel in the canvas is then set using `canvas.Set(x, y, c)`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Following that, the canvas is encoded as a PNG. *Et voilà*, our visualization
    is done!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Now Calling the visualize in the main function as such:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This yields the following image:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0efea962-a169-49a2-a75e-a1c147214e9e.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Preprocessing
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we are going to do next is to "whiten" our data using a **Zero Phase Component
    Analysis** (**ZCA**). The definitions of ZCA is beyond the scope of this chapter,
    but briefly, ZCA is very much like **Principal Component Analysis** (**PCA**).
    In our 784-pixel slice, there is a high probability that the pixels are correlated
    with one another. What PCA does is it finds the set of pixels that are uncorrelated
    with one another. It does this by looking at all the images at once and figuring
    out how each column correlates with one another:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is a pretty large chunk of code. Let's go through the code. But first,
    let's understand the key ideas behind ZCA before going through the code that implements
    it..
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'First, recall what PCA does: it finds the set of inputs (columns and pixels,
    to be used interchangeably) that are least correlated with one another. What ZCA
    does is then to take the principal components found and multiply them by the inputs
    to transform the inputs so that they become less correlated with one another.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to subtract the row mean. To do that, we first make a clone
    of the data (we''ll see why later), then subtract the mean with this function:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After all the preceding spiel about efficiency of a flat slice versus a `[][]float64`,
    what I am going to suggest next is going to sound counter-intuitive. But please
    bear with me. `native.MatrixF64 `takes a `*tensor.Dense` and returns a `[][]float64`,
    which we call `nat`. `nat` shares the same allocation as the tensor `a`. No extra
    allocations are made, and any modification made to `nat` will show up in `a`.
    In this scenario, we should treat `[][]float64` as an easy way to iterate through
    the values in the tensor. This can be seen here:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Like in the `visualize` function, we first iterate through the columns, albeit
    for a different purpose. We want to find the mean of each column. We then store
    the mean of each column in the mean variable. This allows us to subtract the column
    mean:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This block of code uses the `vecf64` package that comes with Gorgonia to subtract
    a slice from another slice, element-wise. It''s rather the same as the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The only real reason to use `vecf64` is that it''s optimized to perform the
    operation with SIMD instructions: instead of doing `row[j] -= mean[j]` one at
    a time, it performs `row[j] -= mean[j]`, `row[j+1] -= mean[j+1]`, `row[j+2] -=
    mean[j+2]`, and `row[j+3] -= mean[j+3]` simultaneously.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'After we''ve subtracted the mean, we find its transpose and make a copy of
    it:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Typically, you would find the transpose of a `tensor.Tensor` by using something
    like `data2.T()`. But this does not return a copy of it. Instead, the `tensor.T` 
    function clones the data structure, then performs a transposition on it. The reason
    for that? We''re about to use both the tranpose and `data2` to find `Sigma` (more
    on matrix multiplication will be expounded in the next chapter):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After we have found `sigma`, we divide it by the number of columns-1\. This
    provides an unbiased estimator. The `tensor.UseUnsafe` option is used to indicate
    that the result should be stored back into the `sigma` tensor:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All this is done so that we can perform an SVD on `sigma`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Singular Value Decomposition, if you are not familiar with it, is a method
    among many that breaks down a matrix into its constituents. Why would you want
    to do so? For one, it makes parts of calculations of some things easier. What
    it does is to factorize  ![](img/e8e5b0cf-7095-4d02-bcea-adf293b141b7.png), a
    (M, N) matrix into a (M, N) matrix called ![](img/b3336713-7358-4b4f-81ea-59bac8d93aa8.png),
    a (M,M) matrix called ![](img/2ef3a6de-2db6-47a8-9325-d4ba67b13ab6.png), and a
    (N, N) matrix called ![](img/40514d0a-31c6-4a02-b0c3-5836f4d76358.png). To reconstruct
    A, the formula is simply:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/958010d7-bc97-44b5-9afb-6f620e3879f6.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: The decomposed parts will then be used. In our case, we're not particularly
    interested about the right singular values ![](img/8cbd12c1-85b2-42bc-894d-c37cfdbe30e8.png),
    so we'll ignore it for now. The decomposed parts are simply used to transform
    the images, which can be found in the tailend of the function body.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'After preprocessing, we can once more visualize the first 100 or so images:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/882ed7d0-2a36-45fd-a66b-45c9c5c04b30.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Building a neural network
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let''s build a neural network! We''ll be building a simple three-layer
    neural network with one hidden layer. A three-layer neural network has two weight
    matrices, so we can define the neural network as such:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`hidden` represents the weight matrix between the input layer and hidden layer,
    while `final` represents the weight matrix between the hidden layer and the final
    layer.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a graphical representation of our *NN data structure:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bba69b0-285c-4dc9-befe-501938ee971c.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: The input layer is the slice of 784 `float64` which is then fed forward (that
    is, a matrix multiplication followed by an activation function) to form the hidden
    layer. The hidden layer is then fed forward to form the final layer. The final
    layer is a vector of ten `float64`, which is exactly the one-hot encoding that
    we discussed earlier. You can think of them as pseud-probabilities,  because the
    values don't exactly sum up to 1.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: A key thing to note: `b0` and `b1` are bias values for the hidden layer and
    the final layer, respectively. They are not actually used mainly due to the mess; 
    it's quite difficult to get the correct differentiation. A challenge for the reader
    is to later incorporate the use of `b0` and `b1`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'And to create a new neural network, we have the `New` function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `fillRandom` function fills a `[]float64` with random values. In our case,
    we fill it up from random values drawn from a uniform distribution. Here, we use
    the `distuv` package from Gonum:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After the slices `r` and `r2` have been filled, the tensors `hiddenT` and `finalT` are
    created, and the `*NN` is returned.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Feed forward
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a conceptual idea of how the neural network works, let''s
    write the forward propagation function. We''ll call it `Predict` because, well,
    to predict, you merely need to run the function forward:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is fairly straightforward, except for a few control structures. I should
    first explain that the API of the tensor package is quite expressive in the sense
    in that it allows the user multiple ways of doing the same thing, albeit with
    different type signatures. Briefly, the patterns are the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor.BINARYOPERATION(a, b tensor.Tensor, opts ...tensor.FuncOpt) (tensor.Tensor,
    error)`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensor.UNARYOPERATION(a tensor.Tensor, opts ...tensor.FuncOpt)(tensor.Tensor,
    error)`'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(a *tensor.Dense) BINARYOPERATION (b *tensor.Dense, opts ...tensor.FuncOpt)
    (*tensor.Dense, error)`'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(a *tensor.Dense) UNARYOPERATION(opts ...tensor.FuncOpt) (*tensor.Dense, error)`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key things to note are package level operations (`tensor.Add`, `tensor.Sub` , and
    so on) take one or more `tensor.Tensor`s and return a `tensor.Tensor` and an `error`.
    There are multiple things that fulfill a `tensor.Tensor` interface, and the tensor package
    provides two structural types that fulfill the interface:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '`*tensor.Dense`: A representation of of a densely packed tensor'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*tensor.CS`: A memory-efficient representation of a sparsely packed tensor
    with the data arranged in compressed sparse columns/row format'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the most part, the most commonly used type of `tensor.Tensor` is the `*tensor.Dense` type.
    The `*tensor.CS` data structure is only used for very specific memory-constrained
    optimizations for specific algorithms. We shan't talk more about the `*tensor.CS` type
    in this chapter.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the package level operations, each specific type also has methods
    that they implement. `*tensor.Dense`'s methods (`.Add(...)`, `.Sub(...)`, and
    so on) take one or more `*tensor.Dense` and return `*tensor.Dense` and an error.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Handling errors with maybe
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that quick introduction out of the way, we can now talk about the `maybe` type.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the things you may have already noticed is that almost all the operations
    return an error. Indeed, there are very few functions and methods that do not
    return an error. The logic behind this is simple: most of the errors are actually
    recoverable and have suitable recovery strategies.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for this project, we have one error recovery strategy: bubble up the
    error to the `main` function, where a `log.Fatal `will be called and the error
    will be inspected for debugging.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I defined `maybe` as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This way, it is able to handle any function as long as it's wrapped within a
    closure.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do this? I personally do not enjoy this structure. I taught it to a few
    students of mine as a cool trick, and since then they claimed that the resulting
    code was more understandable than having blocks of:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: I can definitely empathize with this view. It is most useful in my opinion in
    the prototyping phase, especially when it is not clear yet when and where to handle
    the error (in our case, return early). Leaving the returning of an error until
    the end of the function can be useful. In production code though, I would prefer
    to be as explicit as possible about error-handling strategies.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be further augmented by abstracting common function calls into methods.
    For example, we see this line, `m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid,
    tensor.UseUnsafe()) }) `, twice in the preceding snippet. If we want to prioritize
    understandability while leaving the structure mostly intact, we could abstract
    it away by creating a new method:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: And we would just call `m.sigmoid(hidden)` instead. This is one of the many
    error-handling strategies that programmers can employ to help them. Remember,
    you're a programmer; you are allowed and even expected to program your way out!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the feed forward function
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all that done, let's walk through the the feed forward function, line by
    line.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'First, recall from the section, *Emulating a neural network*, that we can define
    a neural network as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We do the first matrix multiplication as part of calculating the first hidden
    layer: `hidden := m.do(func() (tensor.Tensor, error) { return nn.hidden.MatVecMul(a))
    })`. `MatVecMul` is used because we're multiplying a matrix by a vector.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Then we perform the second part of calculating a layer: `act0 := m.do(func()
    (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })`.
    Once again, the `tensor.UseUnsafe()` function option is used to tell the function
    to not allocate a new tensor. *Voila*! We've successfully calculated the first
    layer.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The same two steps are repeated for the final layer, and we get a one-hot-ish
    vector. Do note that for the first step, I used `tensor.MatVecMul(nn.final, act0)` instead
    of `nn.final.MatVecMul(act0)`. This was done to show that both functions are indeed
    the same, and they just take different types (the method takes a concrete type
    while the package function takes an abstract data type). They are otherwise identical
    in function.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the `affine` function is quite easy to read, whereas the other functions
    are quite difficult to read? Read through the section about `maybe` and see if
    you can come up with a way to write it in such a way that it reads more like affine.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Is there a way to abstract the function into a function like `affine` so that
    you could just call a single function and not repeat yourself?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Before we return the result, we need to perform a check to see if anything in
    the prece-ing steps have errored. Think about what are the errors that could happen.
    They would, in my experience, predominantly be shape related errors. In this specific
    project, a shape error should be considered a failure, so we return a nil result
    and the error.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we would have to check for errors at this point is because we
    are about to use `pred`. If `pred` is nil (which it would be if an error had occurred
    earlier), trying to access the `.Data()` function would cause a panic.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, after the check, we call the `.Data()` method, which returns the raw
    data as a flat slice. It's an `interface{}` type though, so we would have to convert
    it back to a `[]float64` before inspecting the data further. Because the result
    is a vector, it is no different in data layout from a `[]float64`, so we can directly
    call `argmax` on it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '`argmax` simply returns the index of the greatest value in the slice. It''s
    defined thus:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: And thus, we have managed to write a feed forward function for our neural network.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Costs
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having written a fairly straightforward feed forward function, let's now look
    at how to make the neural network learn.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that we said earlier that a neural network learns when you tell it that
    it''s made a mistake? More technically, we ask the question: what kind of cost
    function can we use so that it is able to convey to the neural network accurately
    about what the true value is.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The cost function we want to use for this project is the sum of squared errors.
    What is an error? Well, an error is simply the difference between the real value
    and the predicted value. Does this mean that if the real value is `7`, and the
    neural network predicted `2`, the cost would just be `7`-`2` ? No. This is because
    we should not treat the labels as numbers. They are labels.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: So what do we subtract? Recall the one-hot vector that we created earlier? If
    we peek inside the `Predict` function, we can see that `pred`, the result of the
    final activation is a slice of ten `float64`s. That's what we're going to subtract.
    Because both are slices of ten `float64`s, we would have to subtract them element-wise.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Merely subtracting the slices would not be useful; the results may be negative.
    Imagine if you were tasked to find the lowest possible costs for a product. If
    someone came up to you and told you that their product costs negative amounts
    and that they would pay you to use it, would you not use it? So to prevent that,
    we take the square of the errors.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the sum of squared errors, we simply square the result. Because
    we're training the neural network one image at a time, the sum is simply the squared
    errors of that one image.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The section on costs is a little sparse for good reason. Furthermore, there
    is a twist: we''re not going to entirely calculate the full cost function, mainly
    because we don''t need to for this specific case. Costs are heavily tied to the
    notion of backpropagation. Now we''re going to do some mathematical trickery.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that our cost was the sum of squared errors. We can write it like so:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f4c7e6a-9d36-4667-a1a1-31446b34ff28.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: 'Now what I am about to describe can sound very much like cheating, but it''s
    a valid strategy. The derivative with regard to prediction is this:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b363491e-6a94-4a6b-93cf-32484d500d7e.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: 'To make things a bit easier on ourselves, let''s redefine the cost as this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61dca636-79b3-48f2-8399-a2ddcc7fd9bb.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: It doesn't make a difference to the process of finding the lowest cost. Think
    about it; imagine a highest cost and a lowest cost. The difference between them
    if there is a ![](img/2a654360-2167-43c0-b2bb-1c1fee72bfcc.png) multiplier in
    front of them does not change the fact that the lowest cost is still lower than
    the highest cost. Take some time to work this out on your own to convince yourself
    that having a constant multiplier doesn't change the process.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of a `sigmoid` function is:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/175fea51-70db-4c72-9023-0335d29534c5.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'From there, we can work out the derivation of the cost function with regard
    to the weights matrix. How to work out the full backpropagation will be explained
    in the next chapter. For now, here is the code:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: And there we have it, the derivatives of the cost with regard to the inputs
    matrices.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'The thing to do with the derivatives is to use them as gradients to update
    the input matrices. To do that, use a simple gradient descent algorithm; we simply
    add the gradient to the values itself. But we don''t want to add the full value
    of the gradient. If we do that and our starting value is very close to the minima,
    we''d overshoot it. So we need to multiply the gradients by some small value,
    known as the learn rate:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And this is the training function in full:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are several observations to be made:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: You may note that parts of the body of the `Predict` method are repeated at
    the top of the `Train` method
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tensor.UseUnsafe()` function option is used a lot
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is going to be a pain point when we start scaling up into deeper networks.
    As such, in the next chapter, we will explore the possible solutions to these
    problems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural network
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our main looks like this so far:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here are the steps in brief:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Load image files.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load label files.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert image files into `*tensor.Dense.`
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert label files into `*tensor.Dense.`
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize 100 of the images.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform ZCA whitening on the images.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the whitened images.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a native iterator for the dataset.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the neural network with a 100 unit hidden layer.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a slice of the costs. This is so we can keep track of the average cost
    over time.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, slice the input into single image slices.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, slice the output labels into single slices.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, call `nn.Train()` with a learn rate of `0.1` and use the
    sliced single image and single labels as a training example.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train for five epochs.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would we know that the neural network has learned well? One way is to monitor
    the costs. If the neural network is learning, the average costs over time will
    drop. There may be bumps, of course, but the overall big picture should be that
    the cost does not end up higher than when the program first runs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way we could test how well the neural network is learning is to cross-validate.
    The neural network could learn very well on the training data, in essence, memorizing
    which collections of pixels will result in a particular label. However, to check
    that the machine learning algorithm generalizes well, we need to show the neural
    network some data it's never seen before.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code to do so:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that the code is largely the same as the code before in the `main` function.
    The exception is that instead of calling `nn.Train`, we call `nn.Predict`. Then
    we check to see whether the label is the same as what we predicted.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the tweakable parameters:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'After running (it takes 6.5 minutes), and tweaking various parameters, I ran
    the code and got the following results:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: A simple three-layer neural network leads to a 97% accuracy! This is, of course,
    not close to state of the art. We'll build one that goes up to 99.xx% in the next
    chapter, but requires a big shift of mindset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network takes time. It's often wise to want to save the result
    of the neural network. The `*tensor.Dense type` implements` gob.GobEncoder` and `gob.GobDecoder `and
    to save the neural network to disk, simply save the weights (`nn.hidden` and` nn.final`).
    For an additional challenge, write a gob encoder for those weight matrices and
    save/load the functionality.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, let''s have a look at a few of the things that was wrongly classified.
    In the preceding code, this snippet writes out five wrong predictions:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And here they are:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd98a558-7acd-4604-a721-9051bf245c7d.png) ![](img/4013c335-7f1a-48da-91d0-b51ce57444f9.png) ![](img/a046a761-ede0-40d8-ba20-dd8e66e956de.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: In the first image, the neural network classified it as a `0`, while the true
    value is `6`. As you can see, it is an easy mistake to make. The second image
    shows a `2`, and the neural network classified it as a `4`. You may be inclined
    to think that looks a bit like a `4`. And, lastly, if you are an American reader,
    the chances are you have been exposed to the Palmer handwriting method. If so,
    I'll bet that you might classify the last picture as a 7, instead of a `2`, which
    is exactly what the neural network predicts. Unfortunately, the real label is
    that it's a `2`. Some people just have terrible handwriting.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned how to write a simple neural network with one
    hidden layer that performs remarkably well. Along the way, we've learned how to
    perform ZCA whitening so that the data can be cleaned. There are some difficulties
    with this model, of course; you'd have to pre-calculate the derivatives by hand
    before you coded it.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway point is that a simple neural network can do a lot! While this
    version of the code is very Gorgonia's tensor-centric, the principles are exactly
    the same, even if using Gonum's mat. In fact, Gorgonia's tensor uses Gonum's awesome
    matrix multiplication library underneath.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will revisit the notion of a neural network on the same
    dataset to get a 99% accuracy, but our mindsets of how to approach a neural network
    will have to change. I would advise re-reading the section on linear algebra to
    get a stronger grasp on things.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
