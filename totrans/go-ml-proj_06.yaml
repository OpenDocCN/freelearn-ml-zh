- en: Neural Networks - MNIST Handwriting Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine, you were a postal worker. Your would be job to deliver letters. Most
    of the time, the recipient's name and address would be printed and quite legible,
    and your job becomes quite easy. But come Thanksgiving and Christmas, the number
    of envelopes with handwritten addresses increases as people give their personal
    touches and flourishes. And, to be frank, some people (me included) just have
    terrible handwriting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blame it on schools for no longer emphasizing cursive handwriting if you must,
    but the problem remains: handwriting is hard to read and interpret. God forbid
    you have to deliver a letter penned by a doctor (good luck doing that!).'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, instead, if you had built a machine learning system that allows you
    to read handwriting. That's what we will be doing this chapter and the next; we
    will be building a type of machine-learning algorithm known as an artificial neural
    network, and in the next chapter, we will be expanding on the concept with deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the basics of neural networks, see how it's inspired
    by biological neurons, find a better way of representing them, and finally apply
    neural networks on handwriting to recognize digits.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **neural network** can mean one of two things in modern parlance. The
    first refers to a network of neurons found in your brain. These neurons form specific
    networks and pathways and are vital to you understanding this very sentence. The
    second meaning of the term refers to an artificial neural network; that is, things
    we build in software to emulate a neural network in the brain.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, has led to very many unfortunate comparisons between a biological
    neural network and an artificial neural network. To understand why, we must start
    at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: From here on, I shall spell **neuron** with a British spelling denoting a real
    neuron cell, while the American spelling, **neuron,** will be reserved for the
    artificial variant.
  prefs: []
  type: TYPE_NORMAL
- en: 'This following diagram is of a neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ccfd1f8-ed10-48a7-beb1-2df575822988.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, a neuron typically consists of the soma (the general body of the
    cell that contains its nucleus), an optional axon covered in a kind of fatty tissue
    known as **myelin**, and dendrites. The latter two components ( the axon and dendrites)
    are particularly interesting because together they form a structure known as a
    synapse. Specifically, it's the end of an axon the terminal) that forms such synapses.
  prefs: []
  type: TYPE_NORMAL
- en: The vast majority of synapses in mammalian brains are between axon terminals
    and dendrites. The typical flow of signals (chemical or electrical impulses) goes
    from one neuron, travels along the axon, and deposits its signal onto the next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee48211-f139-4b97-9a5b-3ef35750c1fe.png)'
  prefs: []
  type: TYPE_IMG
- en: In the image above, we have three neurons, labelled A, B, and C. Imagine A receives
    a signal from an external source (like your eyes). It receives a signal that is
    strong enough that it passes the signal down the axon, which touches the dendrites
    of B via a synapse. B receives the signal and decides it doesn't warrant passing
    along the signal to C, so nothing goes down the axon of B.
  prefs: []
  type: TYPE_NORMAL
- en: And so we will now explore how you might emulate this.
  prefs: []
  type: TYPE_NORMAL
- en: Emulating a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s simplify the preceding diagram of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/affc082b-26f6-48c2-b27c-7a5950eab893.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll have a circle represent the body of the neuron, and we'll call it the
    **neuron**. The "dendrites" of the neuron receive inputs from other neurons (unshown)
    and add up all the inputs. Each input represents an input from another neuron;
    so, if you see three inputs, it means that this neuron is connected to three other
    neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the sum of the inputs exceeds a threshold value, then we can say the neuron
    "fires" or is activated. This simulates the activation potential of an actual
    neuron. For simplicity, let''s say if it fires, then the output will be 1; otherwise,
    it will be 0\. Here is a good emulation of it in Go code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is generally known as a **perceptron**, and it's a faithful emulation of
    how neurons work, if your knowledge of how neurons work is stuck in the 1940s
    and 1950s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a rather interesting anecdote: As I was writing this section, King
    Princess'' 1950 started playing in the background and I thought it would be rather
    apt to imagine ourselves in the 1950s, developing the perceptron. There remains
    a problem: the artificial network we emulated so far cannot learn! It is programmed
    to do whatever the inputs tell it to do.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean for an artificial neural network "to learn" exactly? There''s
    an idea that arose in neuroscience in the 1950s, called the **Hebbian Rule**,
    which can be briefly summed up as: *Neurons that fire together grow together*.
    This gives rise to an idea that some synapses are thicker; hence,they have stronger
    connections, and other synapses are thinner; hence, they  have weaker connections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To emulate this, we would need to introduce the concept of a weighted value,
    the weight of which corresponds to the strength of the input from another neuron.
    Here''s a good approximation of this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, if you are familiar with linear algebra, you might think to
    yourself that `total` is essentially a vector product. You would be absolutely
    correct. Additionally, if the threshold is 0, then you have simply applied a `heaviside`
    step function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In other words, we can summarize a single neuron in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note in the last two examples, I switched over from `int` to a more canonical `float64`.
    The point remains the same: a single neuron is simply a function applied to a
    vector product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A single neuron does not do much. But stack a bunch of them together and arrange
    them by layers like so, and then suddenly they start to do more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/357ea737-22c2-41a6-93c6-7bf87ee3c5c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we come to the part that requires a conceptual leap: if a neuron is essentially
    just a vector product, *stacking* the neurons simply makes it a matrix!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an image can be represented as a flat slice of `float64` , the `vectorDot` function
    is replaced with `matVecMul`, which is a function that multiplies a matrix and
    vector to return a vector. We can write a function representing the neural layer
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Linear algebra 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to take a detour to talk about linear algebra. It's featured quite a
    bit so far in this book, although it was scarcely mentioned by name. In fact linear
    algebra underlies every chapter we've done so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you have two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7132731e-26cc-4ef8-bb5e-5042f0bfa77b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say ![](img/08f454a0-6b2a-4c0b-88b5-cd421a39f6bb.png) and ![](img/8f907ffd-9c6a-4dc8-9f15-16fd50bfe601.png) is ![](img/2795f1bc-6f85-449c-a35c-4eeb72ace9c3.png) and ![](img/92bc7148-9b76-4912-b340-d2b1bb340fd8.png), respectively.
    We can now write the following equations as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e57f3c1-9194-4c7e-99fd-bf1aea04ff75.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can solve it using basic algebra (please do work it out on your own): ![](img/6a2280ac-9c8d-464b-8d66-b85679e23414.png) and ![](img/bd4f7b98-14cc-46c9-a203-c9a3433c88c8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you have three, four, or five simultaneous equations? It starts to
    get cumbersome to calculate these values. Instead, we invented a new notation:
    the matrix notation, which will allow us to solve simultaneous equations faster.'
  prefs: []
  type: TYPE_NORMAL
- en: It had been used for about 100 years without a name (it was first termed "matrix"
    by James Sylvester) and formal rules were being used until Arthur Cayley formalized
    the rules in 1858\. Nonetheless, the idea of grouping together parts of an equation
    into a bunch had been long used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by "factoring" out the equations into their parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a2fcb74-6071-44a3-92d4-ead6ddc3d1b6.png)'
  prefs: []
  type: TYPE_IMG
- en: The horizontal line indicates that it's two different equations, not that they
    are ratios. Of course, we realize that we've been making too many repetitions
    so we simplify the matrix of ![](img/42c27d9e-dee6-4c3f-88cb-2c7b7a5d15c7.png) and ![](img/9900269b-ee42-4ed5-b00f-83d1626aa677.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc172162-8e7b-4190-878a-134847ec9ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that ![](img/fd1741f7-280b-4b26-b879-2f85d5566a96.png) and ![](img/39e7e985-0ee3-413e-9821-7a30732beda3.png) is
    only ever written once. It''s rather unneat to write it the way we just wrote
    it, so instead we write it like so to be neater:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa7b5549-dad6-4e96-bbf6-f70d9b3aa10c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Not only do we write it like so, we give specific rule on how to read this
    notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43c4d6a6-d9c2-4fa6-accd-860c1c436f07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We should give the matrices names so we can refer to them later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62d17701-c859-467c-ad8c-2596e593f698.png)'
  prefs: []
  type: TYPE_IMG
- en: The bold indicates that the variable holds multiple values. An uppercase indicates
    a matrix (![](img/a768d36c-8050-4467-bb39-65ba0d19dd87.png)), and lowercase indicates
    a vector ( ![](img/babf308e-bdf6-464d-9744-e4c275f220ff.png) and ![](img/42393974-379f-47dc-9f4c-b991cd3bcaff.png).
    This is to distinguish it from scalar variables (variables that only hold one
    value), which are typically written without boldface (for example, ![](img/c248220b-8bfa-49d8-bb7f-bf56e777ef95.png) and ![](img/a5e79da2-e584-41e7-a42c-fddcd77de875.png)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the equations, the solution is simply this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2223ba92-4d76-4dea-a40c-60ce62525d48.png)'
  prefs: []
  type: TYPE_IMG
- en: The ![](img/bb1d7cdb-9e10-4977-9680-9a41e8341f59.png) superscript indicates
    an inverse is to be taken. This is rather consistent with normal algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a problem ![](img/d594d7b5-c706-4002-8aea-bc9f6c475e27.png) where you
    are asked to solve for ![](img/4606db1b-66e9-4465-a4a3-7ea25d8a7d28.png). The
    solution is simply ![](img/84bc32bb-efd6-4cc3-bd10-034f7a25b4c4.png). Or we can
    rewrite it as a series of multiplications as![](img/2877c3bd-3326-4c07-8d7e-f1ebddc3d4da.png).
    And what do we know about fractions where one is the numerator? They can simply
    be written as a power to the -1\. Hence, we arrive at this solution equation: ![](img/3287306b-cdd3-47c6-979e-a3a489b222bd.png)
  prefs: []
  type: TYPE_NORMAL
- en: Now if you squint very carefully, the scalar version of the equation looks very
    much like the matrix notation version of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: How to calculate the inverse of a matrix is not what this book aims to do. Instead,
    I encourage you to pick up a linear algebra text book. I highly recommend Sheldon
    Axler's *Linear Algebra Done Right* (Springer Books).
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, here are the main points:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication and notation were invented to solve simultaneous equations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve the simultaneous equation, we treat the equation as though the variables
    were scalar variables and use inverses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now comes the interesting part. Using the same two equations, we will turn
    the question around. What if we knew what ![](img/711bc8fc-a9f0-415b-8a8f-b44628ae2ae0.png) and ![](img/a93465f3-1ea7-4725-bd05-ab0aa6dabfbd.png) is
    instead? The equations would now look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64ca7973-411f-4c88-af64-a1df61a8a353.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Writing it in matrix form, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/993b3a8b-ad23-404d-a5d5-171116a8911a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Careful readers would have caught an error by now: there are *four* variables
    (![](img/9eca9944-2c2a-4642-a5c0-a79f3c6aa4c2.png), ![](img/8e698660-7b72-497f-a335-8e797a420e28.png), ![](img/9a666a8f-89b7-4038-b0e9-37dcf01f05ed.png),
    and ![](img/ddf46523-0dcc-437d-89d2-0d66c71ed1e0.png)), but only *two* equations.
    From high-school math, we learn that you can''t solve a system of equations where
    there are fewer equations than there are variables!'
  prefs: []
  type: TYPE_NORMAL
- en: The thing is, your high school math teacher kind of lied to you. It is sort
    of possible to solve this, and you've already done so yourself in [Chapter 2](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml),
    *Linear Regression - House Price Prediction*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, most machine learning problems can be re-expressed in linear algebra,
    specifically of this form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0736a77e-0079-4872-8275-c5b4c2de61b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this in my opinion, is the right way to think about artificial neural networks:
    a series of mathematical functions, not an analogue of biological neurons. We
    will explore this a bit more in the next chapter. In fact, this understanding
    is vital to the understanding of deep learning and why it works.'
  prefs: []
  type: TYPE_NORMAL
- en: For now, it suffices to follow on with the more common notion that an artificial
    neural network is similar in actions to a biologically inspired neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The thing about linear algebra is, it's linear. It is useful when the change
    of the output is proportional to the change in input. The real world is full of
    non-linear functions and equations. Solving non-linear equation is hard with a
    capital H. But we've got a trick. We can take a linear equation, and then add
    a non-linearity to it. This way, the function becomes non-linear!
  prefs: []
  type: TYPE_NORMAL
- en: Following from this view, you can view an artificial neural network as a generic
    version of all the previous chapters we've gone through so far.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the history of artificial neural networks, the community has favored
    particular activation functions in a fashionable way. In the early days, the Heaviside
    function was favored. Gradually, the community moved toward favoring differentiable,
    continuous functions, such as sigmoid and tanh. But lately, the pendulum of fashion
    has swung back toward the harder, seemingly discontinuous functions. The key is
    that we've learned new tricks on how to differentiate functions, such as the **rectified
    linear unit** (**ReLu**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the more popular activation functions over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bacc949b-f005-4866-b8f2-c595cafa4c97.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/de354d5c-72cd-4045-8481-84ba7681540d.png)'''
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2830b2-db5e-4508-aa69-2f478de83a22.png)'
  prefs: []
  type: TYPE_IMG
- en: One thing to note about these is that these functions are all nonlinear and
    they all have a hard limit on the y axis.
  prefs: []
  type: TYPE_NORMAL
- en: The vertical ranges of the activation functions are limited, but the horizontal
    ranges are not. We can use biases to adjust how our activation functions look.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that biases can be zero. It also means that we can omit biases.
    Most of the time, for more complex projects, this is fine, though adding biases
    will add to the accuracy of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want you to think about how you learn. Not the learning styles, mind; no,
    I want you to give your learning process a long hard deep thought. Think of the
    various ways you learn. Maybe you've touched a stove while it's hot once. Or if
    you ever learned a new language, maybe you started out by memorizing phrases before
    becoming fluent. Think about all the chapters that had preceded this. What do
    they have in common?
  prefs: []
  type: TYPE_NORMAL
- en: In broad strokes, learning is done by means of corrections. If you touched a
    stove while it's hot, you made a mistake. The correction is to never touch a stove
    when it's hot ever again. You've learned how not to touch the stove while it's
    hot.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the way a neural network learns is by means of correction. If we
    want to train a machine to learn to classify handwriting, we would need to provide
    some sample images, and tell the machine which are the correct labels. If the
    machine predicted the labels wrongly, we need to tell it to change something in
    the neural network and try again.
  prefs: []
  type: TYPE_NORMAL
- en: 'What can be changed? The weights of course. The inputs can''t be changed; they''re
    inputs. But we can always try different weights. Hence, the process of learning
    can be broken down into two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Telling the neural network that it is wrong when it made a mistake.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the weights so that the next try will yield a better result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When broken down like this, we have a good idea of how to proceed next. One
    way would be a binary determination mechanism: if the neural network predicted
    the correct answer, don''t update the weights. If it''s wrong, update the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: How to update the weights, then? Well, one way would be to completely replace
    the weight matrix with new values and try again. Since the weight matrix is filled
    from values pulled from a random distribution, the new weight matrix would be
    a new random matrix.
  prefs: []
  type: TYPE_NORMAL
- en: It should be quite obvious that these two methods, when combined, would take
    a very very long time before the neural network learns anything; it's as if we're
    simply guessing our way into the correct weight matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, modern neural networks use the concept of **backpropagation** to tell
    the neural network that it's made a mistake, and some form of **gradient descent** to
    update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specifics of backpropagation and gradient descent are outside the scope
    of this chapter (and book). I''ll, however, briefly run through the big ideas
    by sharing a story. I was having lunch with a couple of friends who also work
    in machine learning and that lunch ended with us arguing. This was because I had
    casually mentioned that backpropagation was "discovered", as opposed to "invented".
    My friends were adamant that backpropagation was invented, not discovered. My
    reasoning was simple: Mathematics is "discovered" if multiple people stumble upon
    it with the same formulation. Mathematics is "invented" if there were no parallel
    discovery of it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation, in various forms, has been constantly rediscovered over time.
    The first time backpropagation was discovered was in the invention of linear regression.
    I should note that it was a very specific form of backpropagation specific to
    linear regression: the sum of squared errors can be propagated back to its inputs
    by differentiating the result of the sum of squared errors with regard to the
    inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: We start with a cost. Remember how we have to tell the neural network that it's
    made a mistake. We do so by telling the neural network the cost of making a prediction.
    This is called a cost function. We can define a cost so that when the neural network
    makes a correct prediction, the cost is low, and when the neural network makes
    a wrong prediction, the cost is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine for now, that the cost function is ![](img/29e4aff9-eb8a-4967-8815-49f749fe2f4f.png).
    How do you know at what values of ![](img/68ac22cd-9ee1-4dbc-ad85-7c9461a01602.png) the
    cost will be lowest? From high- school math, we know that the solution is to differentiate ![](img/3ac8cdda-cb4c-49f4-9fca-cb77ddb71c50.png) with
    regard to ![](img/0676e8ac-1a8f-4534-b806-dcfb07833e16.png) and solve for the
    solution when it''s 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2bc33e9-f947-41d0-9f7b-681e8fc3ef30.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation takes the same cue. In short, backpropagtion is just a bunch
    of partial differentiations with regard to the weights. The main difference between
    our toy example and real backpropagation is that the derivation of our expression
    is easy to solve. For more complex mathematical expressions, it can be computationally
    too expensive to compute the solution. Instead, we rely on gradient descent to
    find the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent assumes we start our x somewhere and we update the x iteratively
    toward the lowest cost. In each iteration, we update the weights. The simplest
    form of gradient descent is to add the gradient of the weights to the weights
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway is the powerful notion that you can tell the inputs that an
    error has occurred by performing differentiation of the function and finding a
    point at which the derivatives are at its minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project we're embarking on is the one as mentioned in the opening paragraphs.
    The dataset which we are going to classify is a collection of handwritten numbers
    originally collected by the National Institute of Standards and Technology and
    later modified by Yann LeCun's team. Our goal is to classify the handwritten numbers
    as either one of 0, 1, 2... 9.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to build a basic neural network with the understanding that neural
    networks are applied linear algebra, and we'll be using Gorgonia for this and
    the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To install Gorgonia, simply run `go get -u gorgonia.org/gorgonia` and `go get
    -u gorgonia.org/tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: Gorgonia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gorgonia is a library that facilitates efficient mathematical operations for
    the purposes of building deep neural networks. It operates on the fundamental
    understanding that neural networks are mathematical expressions. As such it is
    quite easy to build neural networks using Gorgonia.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note on the chapters: Because Gorgonia is a relatively huge library, parts
    of this chapter will elide over some things about Gorgonia but will be expanded
    upon in the next chapter, as well as another Packt book,  *Hands On Deep Learning
    in Go*.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data for the MNIST data can be found in the repository for this chapter.
    In its original form, it's not in a standard image format. So, we will need to
    parse the data into an acceptable format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset comes in two parts: labels and images. So here are a couple of
    functions, designed to read and parse the MNIST file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: First, the functions read the file from a `io.Reader` and reading a set of `int32`s.
    These are the metadata of the file. The first `int32` is a magic number that is
    used to indicate if a file is a labels file or a file of images. `n` indicates
    the number of images or labels the file contains. `nrow` and `ncol` are metadata
    that exists in the file, and indicates how many rows/columns there are in each
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zooming into the `readImageFile` function, we can see that after all the metadata
    has been read, we know to create a `[]RawImage` of size `n`. The image format
    used in the MNIST dataset is essentially a slice of 784 bytes (28 columns and
    28 rows). Each byte therefore represents a pixel in the image. The value of each
    byte represents how bright the pixel is, ranging from 0 to 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c023a57-bc75-44bd-9a8b-8783b8f4e46e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is an example of an MNIST image blown up. At the top-left
    corner, the index of the pixel in a flat slice is 0\. At the top right corner,
    the index of the pixel in a flat slice is 27\. At the bottom-left corner, the
    index of the pixel in a flat slice is 755\. And, finally, at the bottom-right
    corner, the index is 727.  This is an important concept to keep in mind: A 2D
    image can be represented as a 1D slice.'
  prefs: []
  type: TYPE_NORMAL
- en: Acceptable format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is an acceptable format to represent the image? A slice of bytes is useful
    for reading and displaying the image, but it''s not particularly useful for doing
    any machine learning. Rather, we should want to represent the image as a slice
    of floating points. So, here''s a function to convert a byte into a `float64`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is essentially a scaling function that scales from 0-255 to between 0.0
    and 1.0\. There is an additional check; if the value is 1.0, we return 0.999 instead
    of 1\. This is mainly due to the fact that when values are 1.0, numerical instability
    tends to happen, as mathematical functions tend to act weirdly. So instead, replace
    1.0 with values that are very close to 1.
  prefs: []
  type: TYPE_NORMAL
- en: So now, we can make a `RawImage` into a `[]float64`. And because we have `N` images
    in the form of `[]RawImage`, we can make it into a `[][]float64`, or a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: From images to a matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we've established that we can convert a list of images in a special format
    in to a slice of slices of `float64`. Recall from earlier, that when you stack
    neurons together they form a matrix, and the activation of a neural layer is simply
    a matrix-vector multiplication. And when the inputs are stacked together, it's
    simply matrix-matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: We technically can build a neural network with just `[][]float64`. But the end
    result will be quite slow. Collectively as a species, we have had approximately
    40 years of developing algorithms for efficient linear algebra operations, such
    as matrix multiplication and matrix-vector multiplication. This collection of
    algorithms are generally known as BLAS (Basic Linear Algebra Subprograms).
  prefs: []
  type: TYPE_NORMAL
- en: We have been, up to this point in the book, using libraries built on top of
    a library that provide BLAS functions, namely  Gonum's BLAS library. If you had
    been following the book up to this point, you would have it installed already.
    Otherwise, run `go get -u gonum.org/v1/gonum/...`, which would install the entire
    suite of Gonum libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the way BLAS works in general, we need a better way of representing
    matrices than `[][]float64`. Here we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Gonum's `mat` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gorgonia's `tensor` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Gorgonia's `tensor`? The reason for `tensor` is quite simple. It plays well
    with Gorgonia itself, which requires multidimensional arrays. Gonum's `mat` only
    takes up to two dimensions, while in the next chapter we'll see a use of four-dimensional
    arrays.
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fundamentally tensors are very much like vectors. The idea is stolen from physics.
    Imagine pushing a box on a two-dimensional plane. If you push the box with a force
    of 1 Newton along the *x* axis, there is no force applied to the *y* axis. You
    would write the vector as such: `[1, 0]`. If the box were moving along the *x* axis
    with at a speed of 10 km/h and along the *y* axis with a speed of 2 km/h, you
    would write the vector as such: `[10, 2]`. Note that they are unitless: the first
    example was a vector of Newtons, the second example was a vector with km/h as
    its units.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, it is a representation of something (a force, a speed, or anything
    with magnitude and direction) applied to a direction. From this idea, computer
    science co-opted the name vector. But in Go, they're called a **slice**.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what is a tensor? Eliding a lot of the details but without a loss of generality,
    a tensor is like a vector. Except multidimensional. Imagine if you were to describe
    two speeds along the plane (imagine a silly putty being stretched in two directions
    at different speeds): `[1, 0]` and `[10, 2]`. You would write it as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '`⎡ 1 0⎤`'
  prefs: []
  type: TYPE_NORMAL
- en: '`⎣10 2⎦`'
  prefs: []
  type: TYPE_NORMAL
- en: This is also called a matrix (when it's two-dimensional). It's called a 3-Tensor
    when it's three-dimensional, 4-Tensor when its four-dimensional, and so on and
    so forth. Note that if you have a third speed (that is, the silly putty being
    stretched in a third direction), you wouldn't have a 3-Tensor. Instead you'd still
    have  a matrix, with three rows.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize a 3-Tensor while building on the previous example, imagine if you
    will, that the two directions that the silly putty was being pulled at was a slice
    in time. Then imagine another slice in time where the same silly putty is pulled
    in two directions again. So now you'd have two matrices. A 3-Tensor is what happens
    when you imagine stacking these matrices together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert a `[]RawImage` to a `tensor.Tensor`, the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Gorgonia may be a bit confusing to beginners. So let me explain the code line
    by line. But first, you must be aware that like Gonum matrices, Gorgonia tensors,
    no matter how many dimensions, are also internally represented as a flat slice.
    Gorgonia tensors are a little more flexible in the sense that they can take more
    than a flat slice of `float64` ( it takes slices of other types too). This is
    called the backing slice or array. This is one of the fundamental reasons why
    performing linear algebra operations is more efficient in Gonum and Gorgonia than
    using plain `[][]float64`.
  prefs: []
  type: TYPE_NORMAL
- en: '`rows := len(M)` and `cols := len(M[0])` are pretty self explanatory. We want
    to know the rows (that is, number of images) and columns (the number of pixels
    in the image).'
  prefs: []
  type: TYPE_NORMAL
- en: '`b := make([]float64, 0, rows*cols)` creates the backing array with a capacity
    of `rows * cols`. This backing array is called a backing *array* because throughout
    the lifetime of `b`, the size will not change. Here we start with a length of
    `0` because we want to use the `append` function later on.'
  prefs: []
  type: TYPE_NORMAL
- en: '`a := make([]T, 0, capacity)` is a good pattern to use to pre-allocate a slice.
    Consider a snippet that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a := make([]int, 0)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`    for i := 0; i < 10; i++ {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        a = append(a, i)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: During the first call to append, the Go runtime will look at the capacity of `a`,
    and find it's `0`. So it will allocate some memory to create a slice of size 1\.
    Then on the second call to append, the Go runtime will look at the capacity of `a` and
    find that it's `1`, which is insufficient. So it will allocate twice the current
    capacity of the slice. On the fourth iteration, it will find the capacity of `a` is insufficient
    for appending and once again allocates twice the current capacity of the slice.
  prefs: []
  type: TYPE_NORMAL
- en: The thing about allocation is that it is an expensive operation. Occasionally
    the Go runtime may not only have to allocate memory, but copy the memory to a
    new location. This adds to the cost of appending to a slice.
  prefs: []
  type: TYPE_NORMAL
- en: So instead, if we know the capacity of the slice upfront, it's best to allocate
    all of it in one shot. We can specify the length, but it's often a cause of indexing
    errors. So my recommendation is to allocate with the capacity and a length of
    `0`. That way, you can safely use append without having to worry about indexing
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: After creating a backing slice, we simply populate the backing slice with the
    values of the pixel, converted to a `float64` using the `pixelWeight` function
    that we described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call `tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))`, which
    returns a `*tensor.Dense`. The `tensor.WithShape(rows, cols)` construction option
    creates a `*tensor.Dense` with the specified shape while `tensor.WithBacking(b)` simply
    uses the already pre-allocated and pre-filled `b` as a backing slice.
  prefs: []
  type: TYPE_NORMAL
- en: The `tensor` library will simply reuse the entire backing array so that fewer
    allocations are made. What this means is  you have to be careful when handling `b`.
    Modifying the contents of `b` afterward will change the content in the `tensor.Dense` as
    well. Given that `b` was created in the `prepareX` function, once the function
    has returned, there's no way to modify the contents of `b`. This is a good way
    to prevent accidental modification.
  prefs: []
  type: TYPE_NORMAL
- en: From labels to one-hot vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that neural networks built in Gorgonia only take `tensor.Tensor`s as
    inputs. Therefore, the labels will also have to be converted into `tensor.Tensor`.
    The function is quite similar to `prepareX`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'What we''re building here is a matrix with *N* rows and ten columns. The specifics
    of why we build a matrix of `(N,10)` will be explored in the next chapter, but
    for now let''s zoom into an imaginary row. Imagine the first label, `(int(N[i]))`, is `7`.
    The row will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]`'
  prefs: []
  type: TYPE_NORMAL
- en: This is called a one-hot vector encoding. It will be useful to us later, and
    will expanded upon in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s also useful to have visualization when we are dealing with image data.
    Earlier we had converted our image pixels from a `byte` to a `float64` using `pixelWeight`.
    It''d be instructive to also have the reverse function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s how to visualize 100 of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is a huge slice of images. We need to figure out how many we want
    first;  hence, `N := rows * cols`. Having the number we want, we then slice using `data.Slice(makeRS(0,
    N), nil)`, which slices the tensor along the first axis. The sliced tensor is
    then reshaped into a four-dimensional array with `sliced.Reshape(rows, cols, 28,28)`.
    The way you can think about it is to have a stacked rows and columns of 28x28
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '**A primer on slicing**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `*tensor.Dense` acts very much like a standard Go slice; just as you can
    slice `a[0:2]`, you can do the same with Gorgonia''s tensors. The `.Slice()` method
    for all tensors accepts a `tensor.Slice` descriptor, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`type Slice interface {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`    Start() int`'
  prefs: []
  type: TYPE_NORMAL
- en: '`    End() int`'
  prefs: []
  type: TYPE_NORMAL
- en: '`    Step() int`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: As such, we would have to make our own data type that fulfills the `Slice` interface.
    It's defined in the `utils.go` file of this project. `makeRS(0, N)` simply reads
    as if we were doing `data[0:N]`. Details and reasoning for this API can be found
    on the Gorgonia tensor Godoc page.
  prefs: []
  type: TYPE_NORMAL
- en: Then a grayscale image is created using the built-in image package: `canvas
    := image.NewGray(rect)`. A `image.Gray` is essentially a slice of bytes and each
    byte is a pixel. What we need to do next is to fill up the pixels. Quite simply,
    we simply loop through the columns and rows in each patch, and we fill it up with
    the correct value extracted from the tensor. The `reversePixelWeight` function is
    used to convert the float into a byte, which is then converted into a `color.Gray`.
    The pixel in the canvas is then set using `canvas.Set(x, y, c)`.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, the canvas is encoded as a PNG. *Et voilà*, our visualization
    is done!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now Calling the visualize in the main function as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0efea962-a169-49a2-a75e-a1c147214e9e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we are going to do next is to "whiten" our data using a **Zero Phase Component
    Analysis** (**ZCA**). The definitions of ZCA is beyond the scope of this chapter,
    but briefly, ZCA is very much like **Principal Component Analysis** (**PCA**).
    In our 784-pixel slice, there is a high probability that the pixels are correlated
    with one another. What PCA does is it finds the set of pixels that are uncorrelated
    with one another. It does this by looking at all the images at once and figuring
    out how each column correlates with one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is a pretty large chunk of code. Let's go through the code. But first,
    let's understand the key ideas behind ZCA before going through the code that implements
    it..
  prefs: []
  type: TYPE_NORMAL
- en: 'First, recall what PCA does: it finds the set of inputs (columns and pixels,
    to be used interchangeably) that are least correlated with one another. What ZCA
    does is then to take the principal components found and multiply them by the inputs
    to transform the inputs so that they become less correlated with one another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to subtract the row mean. To do that, we first make a clone
    of the data (we''ll see why later), then subtract the mean with this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the preceding spiel about efficiency of a flat slice versus a `[][]float64`,
    what I am going to suggest next is going to sound counter-intuitive. But please
    bear with me. `native.MatrixF64 `takes a `*tensor.Dense` and returns a `[][]float64`,
    which we call `nat`. `nat` shares the same allocation as the tensor `a`. No extra
    allocations are made, and any modification made to `nat` will show up in `a`.
    In this scenario, we should treat `[][]float64` as an easy way to iterate through
    the values in the tensor. This can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Like in the `visualize` function, we first iterate through the columns, albeit
    for a different purpose. We want to find the mean of each column. We then store
    the mean of each column in the mean variable. This allows us to subtract the column
    mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This block of code uses the `vecf64` package that comes with Gorgonia to subtract
    a slice from another slice, element-wise. It''s rather the same as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The only real reason to use `vecf64` is that it''s optimized to perform the
    operation with SIMD instructions: instead of doing `row[j] -= mean[j]` one at
    a time, it performs `row[j] -= mean[j]`, `row[j+1] -= mean[j+1]`, `row[j+2] -=
    mean[j+2]`, and `row[j+3] -= mean[j+3]` simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After we''ve subtracted the mean, we find its transpose and make a copy of
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Typically, you would find the transpose of a `tensor.Tensor` by using something
    like `data2.T()`. But this does not return a copy of it. Instead, the `tensor.T` 
    function clones the data structure, then performs a transposition on it. The reason
    for that? We''re about to use both the tranpose and `data2` to find `Sigma` (more
    on matrix multiplication will be expounded in the next chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have found `sigma`, we divide it by the number of columns-1\. This
    provides an unbiased estimator. The `tensor.UseUnsafe` option is used to indicate
    that the result should be stored back into the `sigma` tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'All this is done so that we can perform an SVD on `sigma`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Singular Value Decomposition, if you are not familiar with it, is a method
    among many that breaks down a matrix into its constituents. Why would you want
    to do so? For one, it makes parts of calculations of some things easier. What
    it does is to factorize  ![](img/e8e5b0cf-7095-4d02-bcea-adf293b141b7.png), a
    (M, N) matrix into a (M, N) matrix called ![](img/b3336713-7358-4b4f-81ea-59bac8d93aa8.png),
    a (M,M) matrix called ![](img/2ef3a6de-2db6-47a8-9325-d4ba67b13ab6.png), and a
    (N, N) matrix called ![](img/40514d0a-31c6-4a02-b0c3-5836f4d76358.png). To reconstruct
    A, the formula is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/958010d7-bc97-44b5-9afb-6f620e3879f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The decomposed parts will then be used. In our case, we're not particularly
    interested about the right singular values ![](img/8cbd12c1-85b2-42bc-894d-c37cfdbe30e8.png),
    so we'll ignore it for now. The decomposed parts are simply used to transform
    the images, which can be found in the tailend of the function body.
  prefs: []
  type: TYPE_NORMAL
- en: 'After preprocessing, we can once more visualize the first 100 or so images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/882ed7d0-2a36-45fd-a66b-45c9c5c04b30.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let''s build a neural network! We''ll be building a simple three-layer
    neural network with one hidden layer. A three-layer neural network has two weight
    matrices, so we can define the neural network as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`hidden` represents the weight matrix between the input layer and hidden layer,
    while `final` represents the weight matrix between the hidden layer and the final
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a graphical representation of our *NN data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bba69b0-285c-4dc9-befe-501938ee971c.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer is the slice of 784 `float64` which is then fed forward (that
    is, a matrix multiplication followed by an activation function) to form the hidden
    layer. The hidden layer is then fed forward to form the final layer. The final
    layer is a vector of ten `float64`, which is exactly the one-hot encoding that
    we discussed earlier. You can think of them as pseud-probabilities,  because the
    values don't exactly sum up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: A key thing to note: `b0` and `b1` are bias values for the hidden layer and
    the final layer, respectively. They are not actually used mainly due to the mess; 
    it's quite difficult to get the correct differentiation. A challenge for the reader
    is to later incorporate the use of `b0` and `b1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'And to create a new neural network, we have the `New` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fillRandom` function fills a `[]float64` with random values. In our case,
    we fill it up from random values drawn from a uniform distribution. Here, we use
    the `distuv` package from Gonum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After the slices `r` and `r2` have been filled, the tensors `hiddenT` and `finalT` are
    created, and the `*NN` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Feed forward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a conceptual idea of how the neural network works, let''s
    write the forward propagation function. We''ll call it `Predict` because, well,
    to predict, you merely need to run the function forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This is fairly straightforward, except for a few control structures. I should
    first explain that the API of the tensor package is quite expressive in the sense
    in that it allows the user multiple ways of doing the same thing, albeit with
    different type signatures. Briefly, the patterns are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor.BINARYOPERATION(a, b tensor.Tensor, opts ...tensor.FuncOpt) (tensor.Tensor,
    error)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensor.UNARYOPERATION(a tensor.Tensor, opts ...tensor.FuncOpt)(tensor.Tensor,
    error)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(a *tensor.Dense) BINARYOPERATION (b *tensor.Dense, opts ...tensor.FuncOpt)
    (*tensor.Dense, error)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(a *tensor.Dense) UNARYOPERATION(opts ...tensor.FuncOpt) (*tensor.Dense, error)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key things to note are package level operations (`tensor.Add`, `tensor.Sub` , and
    so on) take one or more `tensor.Tensor`s and return a `tensor.Tensor` and an `error`.
    There are multiple things that fulfill a `tensor.Tensor` interface, and the tensor package
    provides two structural types that fulfill the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*tensor.Dense`: A representation of of a densely packed tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*tensor.CS`: A memory-efficient representation of a sparsely packed tensor
    with the data arranged in compressed sparse columns/row format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the most part, the most commonly used type of `tensor.Tensor` is the `*tensor.Dense` type.
    The `*tensor.CS` data structure is only used for very specific memory-constrained
    optimizations for specific algorithms. We shan't talk more about the `*tensor.CS` type
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the package level operations, each specific type also has methods
    that they implement. `*tensor.Dense`'s methods (`.Add(...)`, `.Sub(...)`, and
    so on) take one or more `*tensor.Dense` and return `*tensor.Dense` and an error.
  prefs: []
  type: TYPE_NORMAL
- en: Handling errors with maybe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that quick introduction out of the way, we can now talk about the `maybe` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the things you may have already noticed is that almost all the operations
    return an error. Indeed, there are very few functions and methods that do not
    return an error. The logic behind this is simple: most of the errors are actually
    recoverable and have suitable recovery strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for this project, we have one error recovery strategy: bubble up the
    error to the `main` function, where a `log.Fatal `will be called and the error
    will be inspected for debugging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I defined `maybe` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This way, it is able to handle any function as long as it's wrapped within a
    closure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do this? I personally do not enjoy this structure. I taught it to a few
    students of mine as a cool trick, and since then they claimed that the resulting
    code was more understandable than having blocks of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: I can definitely empathize with this view. It is most useful in my opinion in
    the prototyping phase, especially when it is not clear yet when and where to handle
    the error (in our case, return early). Leaving the returning of an error until
    the end of the function can be useful. In production code though, I would prefer
    to be as explicit as possible about error-handling strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be further augmented by abstracting common function calls into methods.
    For example, we see this line, `m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid,
    tensor.UseUnsafe()) }) `, twice in the preceding snippet. If we want to prioritize
    understandability while leaving the structure mostly intact, we could abstract
    it away by creating a new method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: And we would just call `m.sigmoid(hidden)` instead. This is one of the many
    error-handling strategies that programmers can employ to help them. Remember,
    you're a programmer; you are allowed and even expected to program your way out!
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the feed forward function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all that done, let's walk through the the feed forward function, line by
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, recall from the section, *Emulating a neural network*, that we can define
    a neural network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We do the first matrix multiplication as part of calculating the first hidden
    layer: `hidden := m.do(func() (tensor.Tensor, error) { return nn.hidden.MatVecMul(a))
    })`. `MatVecMul` is used because we're multiplying a matrix by a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Then we perform the second part of calculating a layer: `act0 := m.do(func()
    (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })`.
    Once again, the `tensor.UseUnsafe()` function option is used to tell the function
    to not allocate a new tensor. *Voila*! We've successfully calculated the first
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: The same two steps are repeated for the final layer, and we get a one-hot-ish
    vector. Do note that for the first step, I used `tensor.MatVecMul(nn.final, act0)` instead
    of `nn.final.MatVecMul(act0)`. This was done to show that both functions are indeed
    the same, and they just take different types (the method takes a concrete type
    while the package function takes an abstract data type). They are otherwise identical
    in function.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the `affine` function is quite easy to read, whereas the other functions
    are quite difficult to read? Read through the section about `maybe` and see if
    you can come up with a way to write it in such a way that it reads more like affine.
  prefs: []
  type: TYPE_NORMAL
- en: Is there a way to abstract the function into a function like `affine` so that
    you could just call a single function and not repeat yourself?
  prefs: []
  type: TYPE_NORMAL
- en: Before we return the result, we need to perform a check to see if anything in
    the prece-ing steps have errored. Think about what are the errors that could happen.
    They would, in my experience, predominantly be shape related errors. In this specific
    project, a shape error should be considered a failure, so we return a nil result
    and the error.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we would have to check for errors at this point is because we
    are about to use `pred`. If `pred` is nil (which it would be if an error had occurred
    earlier), trying to access the `.Data()` function would cause a panic.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, after the check, we call the `.Data()` method, which returns the raw
    data as a flat slice. It's an `interface{}` type though, so we would have to convert
    it back to a `[]float64` before inspecting the data further. Because the result
    is a vector, it is no different in data layout from a `[]float64`, so we can directly
    call `argmax` on it.
  prefs: []
  type: TYPE_NORMAL
- en: '`argmax` simply returns the index of the greatest value in the slice. It''s
    defined thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: And thus, we have managed to write a feed forward function for our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Costs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having written a fairly straightforward feed forward function, let's now look
    at how to make the neural network learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that we said earlier that a neural network learns when you tell it that
    it''s made a mistake? More technically, we ask the question: what kind of cost
    function can we use so that it is able to convey to the neural network accurately
    about what the true value is.'
  prefs: []
  type: TYPE_NORMAL
- en: The cost function we want to use for this project is the sum of squared errors.
    What is an error? Well, an error is simply the difference between the real value
    and the predicted value. Does this mean that if the real value is `7`, and the
    neural network predicted `2`, the cost would just be `7`-`2` ? No. This is because
    we should not treat the labels as numbers. They are labels.
  prefs: []
  type: TYPE_NORMAL
- en: So what do we subtract? Recall the one-hot vector that we created earlier? If
    we peek inside the `Predict` function, we can see that `pred`, the result of the
    final activation is a slice of ten `float64`s. That's what we're going to subtract.
    Because both are slices of ten `float64`s, we would have to subtract them element-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Merely subtracting the slices would not be useful; the results may be negative.
    Imagine if you were tasked to find the lowest possible costs for a product. If
    someone came up to you and told you that their product costs negative amounts
    and that they would pay you to use it, would you not use it? So to prevent that,
    we take the square of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the sum of squared errors, we simply square the result. Because
    we're training the neural network one image at a time, the sum is simply the squared
    errors of that one image.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The section on costs is a little sparse for good reason. Furthermore, there
    is a twist: we''re not going to entirely calculate the full cost function, mainly
    because we don''t need to for this specific case. Costs are heavily tied to the
    notion of backpropagation. Now we''re going to do some mathematical trickery.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that our cost was the sum of squared errors. We can write it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f4c7e6a-9d36-4667-a1a1-31446b34ff28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now what I am about to describe can sound very much like cheating, but it''s
    a valid strategy. The derivative with regard to prediction is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b363491e-6a94-4a6b-93cf-32484d500d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make things a bit easier on ourselves, let''s redefine the cost as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61dca636-79b3-48f2-8399-a2ddcc7fd9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: It doesn't make a difference to the process of finding the lowest cost. Think
    about it; imagine a highest cost and a lowest cost. The difference between them
    if there is a ![](img/2a654360-2167-43c0-b2bb-1c1fee72bfcc.png) multiplier in
    front of them does not change the fact that the lowest cost is still lower than
    the highest cost. Take some time to work this out on your own to convince yourself
    that having a constant multiplier doesn't change the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of a `sigmoid` function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/175fea51-70db-4c72-9023-0335d29534c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From there, we can work out the derivation of the cost function with regard
    to the weights matrix. How to work out the full backpropagation will be explained
    in the next chapter. For now, here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: And there we have it, the derivatives of the cost with regard to the inputs
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The thing to do with the derivatives is to use them as gradients to update
    the input matrices. To do that, use a simple gradient descent algorithm; we simply
    add the gradient to the values itself. But we don''t want to add the full value
    of the gradient. If we do that and our starting value is very close to the minima,
    we''d overshoot it. So we need to multiply the gradients by some small value,
    known as the learn rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is the training function in full:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several observations to be made:'
  prefs: []
  type: TYPE_NORMAL
- en: You may note that parts of the body of the `Predict` method are repeated at
    the top of the `Train` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tensor.UseUnsafe()` function option is used a lot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is going to be a pain point when we start scaling up into deeper networks.
    As such, in the next chapter, we will explore the possible solutions to these
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our main looks like this so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the steps in brief:'
  prefs: []
  type: TYPE_NORMAL
- en: Load image files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load label files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert image files into `*tensor.Dense.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert label files into `*tensor.Dense.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize 100 of the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform ZCA whitening on the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the whitened images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a native iterator for the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the neural network with a 100 unit hidden layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a slice of the costs. This is so we can keep track of the average cost
    over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, slice the input into single image slices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, slice the output labels into single slices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within each epoch, call `nn.Train()` with a learn rate of `0.1` and use the
    sliced single image and single labels as a training example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train for five epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would we know that the neural network has learned well? One way is to monitor
    the costs. If the neural network is learning, the average costs over time will
    drop. There may be bumps, of course, but the overall big picture should be that
    the cost does not end up higher than when the program first runs.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way we could test how well the neural network is learning is to cross-validate.
    The neural network could learn very well on the training data, in essence, memorizing
    which collections of pixels will result in a particular label. However, to check
    that the machine learning algorithm generalizes well, we need to show the neural
    network some data it's never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that the code is largely the same as the code before in the `main` function.
    The exception is that instead of calling `nn.Train`, we call `nn.Predict`. Then
    we check to see whether the label is the same as what we predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the tweakable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running (it takes 6.5 minutes), and tweaking various parameters, I ran
    the code and got the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: A simple three-layer neural network leads to a 97% accuracy! This is, of course,
    not close to state of the art. We'll build one that goes up to 99.xx% in the next
    chapter, but requires a big shift of mindset.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network takes time. It's often wise to want to save the result
    of the neural network. The `*tensor.Dense type` implements` gob.GobEncoder` and `gob.GobDecoder `and
    to save the neural network to disk, simply save the weights (`nn.hidden` and` nn.final`).
    For an additional challenge, write a gob encoder for those weight matrices and
    save/load the functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, let''s have a look at a few of the things that was wrongly classified.
    In the preceding code, this snippet writes out five wrong predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And here they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd98a558-7acd-4604-a721-9051bf245c7d.png) ![](img/4013c335-7f1a-48da-91d0-b51ce57444f9.png) ![](img/a046a761-ede0-40d8-ba20-dd8e66e956de.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first image, the neural network classified it as a `0`, while the true
    value is `6`. As you can see, it is an easy mistake to make. The second image
    shows a `2`, and the neural network classified it as a `4`. You may be inclined
    to think that looks a bit like a `4`. And, lastly, if you are an American reader,
    the chances are you have been exposed to the Palmer handwriting method. If so,
    I'll bet that you might classify the last picture as a 7, instead of a `2`, which
    is exactly what the neural network predicts. Unfortunately, the real label is
    that it's a `2`. Some people just have terrible handwriting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned how to write a simple neural network with one
    hidden layer that performs remarkably well. Along the way, we've learned how to
    perform ZCA whitening so that the data can be cleaned. There are some difficulties
    with this model, of course; you'd have to pre-calculate the derivatives by hand
    before you coded it.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway point is that a simple neural network can do a lot! While this
    version of the code is very Gorgonia's tensor-centric, the principles are exactly
    the same, even if using Gonum's mat. In fact, Gorgonia's tensor uses Gonum's awesome
    matrix multiplication library underneath.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will revisit the notion of a neural network on the same
    dataset to get a 99% accuracy, but our mindsets of how to approach a neural network
    will have to change. I would advise re-reading the section on linear algebra to
    get a stronger grasp on things.
  prefs: []
  type: TYPE_NORMAL
