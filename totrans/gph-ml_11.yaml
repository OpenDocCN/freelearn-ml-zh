- en: Chapter 8:Graph Analysis for Credit Card Transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analysis of financial data is one of the most common and important domains in
    big data and data analysis. Indeed, due to the increasing number of mobile devices
    and the introduction of a standard platform for online payment, the amount of
    transactional data that banks are producing and consuming is increasing exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, new tools and techniques are needed to exploit as much as
    we can from this huge amount of information in order to better understand customers'
    behavior and support data-driven decisions in business processes. Data can also
    be used to build better mechanisms to improve security in the online payment process.
    Indeed, as online payment systems are becoming increasingly popular due to e-commerce
    platforms, at the same time, cases of fraud are also increasing. An example of
    a fraudulent transaction is a transaction performed with a stolen credit card.
    Indeed, in this case, the fraudulent transactions will be different from the transactions
    made by the original owner of the credit card.
  prefs: []
  type: TYPE_NORMAL
- en: However, building automatic procedures to detect fraudulent transactions could
    be a complex problem due to the large number of variables involved.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will describe how we can represent credit card transaction
    data as a graph in order to automatically detect fraudulent transactions using
    machine learning algorithms. We will start processing the dataset by applying
    some of the techniques and algorithms we described in previous chapters to build
    a fraud detection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating a graph from credit card transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraction of properties and communities from the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of supervised and unsupervised machine learning algorithms to fraud
    classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using *Jupyter* notebooks with *Python* 3.8 for all of our exercises.
    The following is a list of Python libraries that will be installed for this chapter
    using `pip`. For example, run `pip install networkx==2.5` on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the rest of this book, unless clearly stated to the contrary, we will refer
    to `nx` as the results of the Python `import networkx as nx` command.
  prefs: []
  type: TYPE_NORMAL
- en: All code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter08](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset used in this chapter is the *Credit Card Transactions Fraud Detection
    Dataset* available on *Kaggle* at the following URL: [https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is made up of simulated credit card transactions containing legitimate
    and fraudulent transactions for the period January 1, 2019 – December 31, 2020\.
    It includes the credit cards of 1,000 customers performing transactions with a
    pool of 800 merchants. The dataset was generated using *Sparkov Data Generation*.
    More information about the generation algorithm is available at the following
    URL: [https://github.com/namebrandon/Sparkov_Data_Generation](https://github.com/namebrandon/Sparkov_Data_Generation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each transaction, the dataset contains 23 different features. In the following
    table, we will show only the information that will be used in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.1 – List of variables used in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 – List of variables used in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of our analysis, we will use the `fraudTrain.csv` file. As
    already suggested, take a look at the dataset by yourself. It is strongly suggested
    to explore and become as comfortable as possible with the dataset before starting
    any machine learning task. We also suggest that you investigate two other datasets
    that will not be covered in this chapter. The first one is the Czech Bank's Financial
    Analysis dataset, available at https://github.com/Kusainov/czech-banking-fin-analysis.
    This dataset came from an actual Czech bank in 1999, for the period covering 1993
    – 1998\. The data pertaining to clients and their accounts consists of directed
    relations. Unfortunately, there are no labels on the transactions, making it impossible
    to train a fraud detection engine using machine learning techniques. The second
    dataset is the paysim1 dataset, available at [https://www.kaggle.com/ntnu-testimon/paysim1](https://www.kaggle.com/ntnu-testimon/paysim1).
    This dataset comprises simulated mobile money transactions based on a sample of
    real transactions extracted from one month of financial logs from a mobile money
    service implemented in an African country. The original logs were provided by
    a multinational company, which is the provider of the mobile financial service
    and is currently running in more than 14 countries across the globe. This dataset
    also contains labels on fraudulent/genuine transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset and graph building using networkx
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step of our analysis will be to load the dataset and build a graph.
    Since the dataset represents a simple list of transactions, we need to perform
    several operations to build the final credit card transaction graph. The dataset
    is a simple CSV file; we can use `pandas` to load the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to help the reader deal with the dataset, we selected 20% of the genuine
    transactions and all of the fraudulent transactions. As a result, from a total
    of 1,296,675 transactions, we will only use 265,342 transactions. Moreover, we
    can also investigate the number of fraudulent and genuine transactions in our
    dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'By way of a result, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In other words, from a total of 265,342 transactions, only `7506` (2.83 %) are
    fraudulent transactions, while the others are genuine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be represented as a graph using the `networkx` library. Before
    starting with the technical description, we will start by specifying how the graph
    is built from the data. We used two different approaches to build the graph, namely,
    the bipartite and tripartite approaches, as described in the paper *APATE: A Novel
    Approach for Automated Credit Card Transaction Fraud Detection Using Network-Based
    Extensions*, available at https://www.scinapse.io/papers/614715210.'
  prefs: []
  type: TYPE_NORMAL
- en: For the **bipartite approach**, we build a weighted bipartite graph ![](img/B16069_08_001.png)
    where ![](img/B16069_08_002.png), where each node ![](img/B16069_08_003.png) represents
    a customer, and each node ![](img/B16069_08_004.png) represents a merchant. An
    edge ![](img/B16069_08_005.png) is created if a transaction exists from the customer,
    ![](img/B16069_08_006.png), to the merchant, ![](img/B16069_08_007.png). Finally,
    to each edge of the graph, we assign an (always positive) weight representing
    the amount (in US dollars) of the transaction. In our formalization, we allow
    both directed and undirected graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the dataset represents temporal transactions, multiple interactions can
    happen between a customer and a merchant. In both our formalizations, we decided
    to collapse all that information in a single graph. In other words, if multiple
    transactions are present between a customer and a merchant, we will build a single
    edge between the two nodes with its weight given by the sum of all the transaction
    amounts. A graphical representation of the direct bipartite graph is visible in
    *Figure 8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Bipartite graph generated from the input dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Bipartite graph generated from the input dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The bipartite graph we defined can be built using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is quite simple. To build the bipartite credit card transaction graph,
    we use different `networkx` functions. To go more in depth, the operations we
    performed in the code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We built a map to assign a `node_id` to each merchant or customer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiple transactions are aggregated in a single transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `networkx` function, `nx.from_edgelist`, is used to build the networkx graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two attributes, namely, `weight` and `label`, are assigned to each edge. The
    former represents the total number of transactions between the two nodes, whereas
    the latter indicates whether the transaction is genuine or fraudulent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we can also see from the code, we can select whether we want to build a
    directed or an undirected graph. We can build an undirected graph by calling the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instead build a direct graph by calling the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The only difference is given by the second parameter we pass in the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: The **tripartite approach** is an extension of the previous one, also allowing
    the transactions to be represented as a vertex. If, on the one hand, this approach
    drastically increases network complexity, on the other hand, it allows extra node
    embeddings to be built for merchants and cardholders and every transaction. Formally
    for this approach, we build a weighted tripartite graph, ![](img/B16069_08_008.png),
    where ![](img/B16069_08_009.png), where each node ![](img/B16069_08_010.png) represents
    a customer, each node ![](img/B16069_08_011.png) represents a merchant, and each
    node ![](img/B16069_08_012.png) is a transaction. Two edges ![](img/B16069_08_013.png)
    and ![](img/B16069_08_014.png) are created for each transaction, ![](img/B16069_08_015.png),
    from customer ![](img/B16069_08_016.png) to the merchant ![](img/B16069_08_017.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to each edge of the graph, we assign an (always positive) weight representing
    the amount (in US dollars) of the transaction. Since, in this case, we create
    a node for each transaction, we do not need to aggregate multiple transactions
    from a customer to a merchant. Moreover, as for the other approach, in our formalization,
    we allow both directed and undirected graphs. A graphical representation of the
    direct bipartite graph is visible in *Figure 8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Tripartite graph generated from the input dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Tripartite graph generated from the input dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The tripartite graph we defined can be built using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is quite simple. To build the tripartite credit card transaction graph,
    we use different `networkx` functions. To go more in depth, the operations we
    performed in the code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We built a map to assign a `node_id` to each merchant, customer, and transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `networkx` function, `nx.from_edgelist`, is used to build the networkx graph,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two attributes, namely, `weight` and `label`, are assigned to each edge. The
    former represents the total number of transactions between the two nodes, whereas
    the latter indicates whether the transaction is genuine or fraudulent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we can also see from the code, we can select whether we want to build a
    directed or an undirected graph. We can build an undirected graph by calling the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instead build a direct graph by calling the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The only difference is given by the second parameter we pass in the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: In the formalized graph representation that we introduced, the real transactions
    are represented as edges. According to this structure for both bipartite and tripartite
    graphs, the classification of fraudulent/genuine transactions is described as
    an edge classification task. In this task, the goal is to assign to a given edge
    a label (`0` for genuine, `1` for fraudulent) describing whether the transaction
    the edge represents is fraudulent or genuine.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this chapter, we use for our analysis both bipartite and tripartite
    undirected graphs, denoted by the Python variables `G_bu` and `G_tu`, respectively.
    We will leave it to you, as an exercise, an extension of the analyses proposed
    in this chapter to direct graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin our analysis with a simple check to validate whether our graph is
    a real bipartite graph using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As result, we get `True`. This check gives us the certainty that the two graphs
    are actually bipartite/tripartite graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, using the following command, we can get some basic statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'By way of a result, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the two graphs differ in both, the number of nodes and the number
    of edges. The bipartite undirected graph has 1,676, equal to the number of customers
    plus the number of merchants with a high number of edges (201,725). The tripartite
    undirected graph has 267,016, equal to the number of customers plus the number
    of merchants plus all the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, the number of nodes, as expected, is higher (530,680) compared
    to the bipartite graph. The interesting difference in this comparison is given
    by the average degree of the two graphs. Indeed, the average degree of the bipartite
    graph is higher compared to the tripartite graph, as expected. Indeed, since,
    in the tripartite graph, the connections are "split" by the presence of the transaction
    nodes, the average degree is lower.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe how we can now use the transaction graphs
    generated to perform a more complete statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Network topology and community detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to analyze some graph metrics to have a clear
    picture of the general structure of the graph. We will be using `networkx` to
    compute most of the useful metrics we have seen in [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014),
    *Getting Started with Graphs*. We will try to interpret the metrics to gain insights
    into the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Network topology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good starting point for our analysis is the extraction of simple graph metrics
    to have a general understanding of the main properties of bipartite and tripartite
    transaction graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by looking at the distribution of the degree for both bipartite and
    tripartite graphs using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'By way of a result, we get the plot in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Degree distribution for bipartite (left) and tripartite (right)
    graphs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Degree distribution for bipartite (left) and tripartite (right)
    graphs
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8.3*, it is possible to see how the distribution of nodes reflects
    the average degree we previously saw. In greater detail, the bipartite graph has
    a more variegate distribution, with a peak of around 300\. For the tripartite
    graph, the distribution has a big peak for degree 2, while the other part of the
    tripartite degree distribution is similar to the bipartite distribution. These
    distributions completely reflect the differences in how the two graphs were defined.
    Indeed, if bipartite graphs are made by connections from the customer to the merchant,
    in the tripartite graph, all the connections pass through the transaction nodes.
    Those nodes are the majority in the graph, and they all have a degree of 2 (an
    edge from a custom and an edge to a merchant). As a consequence, the frequency
    in the bin representing degree 2 is equal to the number of transaction nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue our investigation by analyzing the `edges weight` distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by computing the quantile distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By way of a result, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the same command as before, we can also plot (in log scale) the distribution
    of `edges weight`, cut to the 90th percentile. The result is visible in the following
    diagram:![Figure 8.4 – Edge weight distribution for bipartite (left) and tripartite
    (right) graphs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16069_08_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.4 – Edge weight distribution for bipartite (left) and tripartite (right)
    graphs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see how, due to the aggregation of the transaction having the same customer
    and merchant, the distribution of the bipartite graph is shifted to the right
    (high values) compared to the tripartite graph, where edge weights were not computed,
    aggregating multiple transactions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will now investigate the `betweenness centrality` metric. It measures how
    many shortest paths pass through a given node, giving an idea of how *central*
    that node is for the spreading of information inside the network. We can compute
    the distribution of node centrality by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As result, we get the following distributions:![Figure 8.5 – Betweenness centrality
    distribution for bipartite (left) and tripartite (right) graphs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16069_08_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.5 – Betweenness centrality distribution for bipartite (left) and tripartite
    (right) graphs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As expected, for both graphs, the betweenness centrality is low. This can be
    understood due to the large number of non-bridging nodes inside the network. Similar
    to what we saw for the degree distribution, the distribution of betweenness centrality
    values is different in the two graphs. Indeed, if the bipartite graph has a more
    variegate distribution with a mean of 0.00072, in the tripartite graph, the transaction
    nodes are the ones that mainly move the distribution values and lower the mean
    to 1.38e-05\. Also, in this case, we can see that the distribution for the tripartite
    graph has a big peak, representing the transaction nodes, and the rest of the
    distribution is quite similar to the bipartite distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can finally compute the assortativity of the two graphs using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By way of a result, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can observe how both graphs have a negative assortativity, likely showing
    that well-connected individuals associate with poor-connected individuals. For
    the bipartite graph, the value is low (-0.14), since customers who have a low
    degree are only connected with merchants who have high degrees due to the high
    number of incoming transactions. The assortativity is even lower (-0.81) for the
    tripartite graph. This behavior is expected due to the presence of the transaction
    nodes. Indeed, those nodes always have a degree of 2, and they are linked to customers
    and merchants represented by highly connected nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Community detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another interesting analysis we can perform is community detection. This analysis
    can help to identify specific fraudulent patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to perform community extraction is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we simply use the `community` library to extract the communities
    in the input graph. We then print the communities detected by the algorithms,
    sorted according to the number of nodes contained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the bipartite graph, we obtain the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the tripartite graph, we obtain the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Due to a large number of nodes in the tripartite graph, we found 106 communities
    (we reported just a subset of them), while, for the bipartite graph, only 12 communities
    were found. As consequence, to have a clear picture, for the tripartite graph,
    it is better to plot the distribution of the nodes contained in the different
    communities using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By way of a result, we get the following:![Figure 8.6 – Distribution of communities'
    node size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16069_08_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.6 – Distribution of communities' node size
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the diagram, it is possible to see how the peak is reached around 2,500\.
    This means that more than 30 large communities have more than 2,000 nodes. From
    the plot, it is also possible to see that a few communities have fewer than 1,000
    nodes and more than 3,000 nodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For each set of communities detected by the algorithm, we can compute the percentage
    of fraudulent transactions. The goal of this analysis is to identify specific
    sub-graphs where there is a high concentration of fraudulent transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code simply generates a node-induced subgraph by using the nodes contained
    in a specific community. The graph is used to compute the percentage of fraudulent
    transactions as a ratio of the number of fraudulent edges over the number of all
    the edges in the graph. We can also plot a node-induced subgraph detected by the
    community detection algorithm by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given a particular community index, `gId`, the code extracts the node-induced
    subgraph, using the node available in the `gId` community index, and plots the
    graph obtained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By running the two algorithms on the bipartite graph, we will obtain the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For each community, we have the percentage of its fraudulent edges. To have
    a better description of the subgraph, we can plot community 10 by executing the
    previous line of code using `gId=10`. As a result, we get the following:![Figure
    8.7 – Induced subgraph of community 10 for the bipartite graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16069_08_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.7 – Induced subgraph of community 10 for the bipartite graph
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The image of the induced subgraph allows us to better understand whether specific
    patterns are visible in the data. Running the same algorithms on the tripartite
    graph, we obtain the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Due to the large number of communities, we can plot the distribution of the
    fraudulent over genuine ratio with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By way of a result, we get the following:![Figure 8.8 – Distribution of communities'
    fraudulent/genuine edge ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16069_08_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.8 – Distribution of communities' fraudulent/genuine edge ratio
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the diagram, we can observe that a large part of the distribution is around
    communities having a ratio of between 2 and 4\. There are a few communities with
    a low ratio (<1) and with a high ratio (>5).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also, for the tripartite graph, we can plot community 6 (with a ratio of 6.86),
    made by 1,935 nodes, by executing the previous line of code using `gId=6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Induced subgraph of community 6 for the tripartite graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Induced subgraph of community 6 for the tripartite graph
  prefs: []
  type: TYPE_NORMAL
- en: As for the bipartite use case, in this image, we can see an interesting pattern
    that could be used to perform a deeper exploration of some important graph sub-regions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we perform some explorative tasks to better understand the
    graphs and their properties. We also gave an example describing how a community
    detection algorithm can be used to spot patterns in the data. In the next section,
    we will describe how machine learning can be used to automatically detect fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding for supervised and unsupervised fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will describe how the bipartite and tripartite graphs described
    previously can be used by graph machine learning algorithms to build automatic
    procedures for fraud detection using supervised and unsupervised approaches. As
    we already discussed at the beginning of this chapter, transactions are represented
    by edges, and we then want to classify each edge in the correct class: fraudulent
    or genuine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline we will use to perform the classification task is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A sampling procedure for the imbalanced task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of an unsupervised embedding algorithm to create a feature vector for
    each edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application of supervised and unsupervised machine learning algorithms to
    the feature space defined in the previous point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised approach to fraudulent transaction identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since our dataset is strongly imbalanced, with fraudulent transactions representing
    2.83% of total transactions, we need to apply some techniques to deal with unbalanced
    data. In this use case, we will apply a simple random undersampling strategy.
    Going into more depth, we will take a subsample of the majority class (genuine
    transactions) to match the number of samples of the minority class (fraudulent
    transactions). This is just one of the many techniques available in literature.
    It is also possible to use outlier detection algorithms, such as isolation forest,
    to detect fraudulent transactions as outliers in the data. We leave it to you,
    as an exercise, to extend the analyses using other techniques to deal with imbalanced
    data, such as random oversampling or using cost-sensitive classifiers for the
    classification task. Specific techniques for node and edge sampling that can be
    directly applied to the graph will be described in [*Chapter 10*](B16069_10_Final_JM_ePub.xhtml#_idTextAnchor150),
    *Novel Trends on Graphs*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we use for random undersampling is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code is straightforward. We applied the `resample` function of the `sklearn`
    package to filter the `downsample` function of the original data frame. We then
    build a graph using the function defined at the beginning of the chapter. To create
    the tripartite graph, the `build_graph_tripartite` function should be used. As
    the next step, we split the dataset into training and validation with a ratio
    of 80/20:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As before, also in this case, the code is straightforward since we simply apply
    the `train_test_split` function of the `sklearn` package.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now build the feature space using the `Node2Vec` algorithm as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `node2vec` results are used to build, as described in [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Unsupervised Graph Learning*, the edge embedding that will generate the final
    feature space used by the classifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The code to perform this task is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Different steps are performed compared to the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: For each `Edge2Vec` algorithm, the previously computed `Node2Vec` algorithm
    is used to generate the feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `RandomForestClassifier` from the `sklearn` Python library is trained on the
    feature set generated in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different performance metrics, namely, precision, recall, and F1-score, are
    computed on the validation test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can apply the code we previously described to both bipartite and tripartite
    graphs to solve the fraud detection task. In the following table, we report the
    performances for the bipartite graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.2 – Supervised fraud edge classification performances for a bipartite
    graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 – Supervised fraud edge classification performances for a bipartite
    graph
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we report the performances for the tripartite graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.3 – Supervised fraud edge classification performances for a tripartite
    graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.3 – Supervised fraud edge classification performances for a tripartite
    graph
  prefs: []
  type: TYPE_NORMAL
- en: In *Table 8.2* and *Table 8.3*, we reported the classification performances
    obtained using bipartite and tripartite graphs. As we can see from the results,
    the two methods, in terms of F1-score, precision, and recall, show significant
    differences. Since, for both graph types, Hadamard and average edge embedding
    algorithms give the most interesting results, we are going to focus our attention
    on those two. Going into more detail, the tripartite graph has a better precision
    compared to the bipartite graph (0.89 and 0.74 for the tripartite graph versus
    0.73 and 0.71 for the bipartite graph).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the bipartite graph has a better recall compared to the tripartite
    graph (0.76 and 0.79 for the bipartite graph versus 0.29 and 0.45 for the tripartite
    graph). We can therefore conclude that in this specific case, the use of a bipartite
    graph could be a better choice since it achieves high performances in terms of
    F1 with a smaller graph (in terms of nodes and edges) compared to the tripartite
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised approach to fraudulent transaction identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The same approach can also be applied in unsupervised tasks using k-means.
    The main difference is that the generated feature space will not undergo a train-validation
    split. Indeed, in the following code, we will compute the `Node2Vec` algorithm
    on the entire graph generated following the downsampling procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously defined for the supervised analysis, when building the node feature
    vectors, we can use different `Egde2Vec` algorithms to run the k-means algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Different steps are performed in the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: For each `Edge2Vec` algorithm, the previously computed `Node2Vec` algorithm
    on train and validation sets is used to generate the feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `KMeans` clustering algorithm from the `sklearn` Python library is fitted
    on the feature set generated in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different performance metrics, namely, adjusted **mutual information** (**MNI**),
    homogeneity, completeness, and v-measure scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can apply the code described previously to both bipartite and tripartite
    graphs to solve the fraud detection task using the unsupervised algorithm. In
    the following table, we report the performances for the bipartite graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.4 – Unsupervised fraud edge classification performances for the bipartite
    graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_041.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.4 – Unsupervised fraud edge classification performances for the bipartite
    graph
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we report the performances for the tripartite graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 8.5 – Unsupervised fraud edge classification performances for the tripartite
    graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_08_051.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.5 – Unsupervised fraud edge classification performances for the tripartite
    graph
  prefs: []
  type: TYPE_NORMAL
- en: In *Table 8.4* and *Table 8.5*, we reported the classification performances
    obtained using bipartite and tripartite graphs with the application of an unsupervised
    algorithm. As we can see from the results, the two methods show significant differences.
    It is also worth noticing that, in this case, the performances obtained with the
    Hadamard embedding algorithm clearly outperform all other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: As shown by *Table 8.4* and *Table 8.5*, also for this task, the performances
    obtained with the tripartite graph outstrip those obtained with the bipartite
    graph. In the unsupervised case, we can see how the introduction of the transaction
    nodes improves overall performance. We can assert, that, in the unsupervised setting,
    for this specific use case and using as a reference the results obtained in *Table
    8.4* and *Table 8.5*, use of the tripartite graph could be a better choice since
    it enables the attainment of superior performances compared with the bipartite
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described how a classical fraud detection task can be described
    as a graph problem and how the techniques described in the previous chapter can
    be used to tackle the problem. Going into more detail, we introduced the dataset
    we used and described the procedure to transform the transactional data into two
    types of graph, namely, bipartite and tripartite undirected graphs. We then computed
    local (along with their distributions) and global metrics for both graphs, comparing
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a community detection algorithm was applied to the graphs in order
    to spot and plot specific regions of the transaction graph where the density of
    fraudulent transactions is higher compared to the other communities.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we solved the fraud detection problem using supervised and unsupervised
    algorithms, comparing the performances of the bipartite and tripartite graphs.
    As the first step, since the problem was unbalanced with a higher presence of
    genuine transactions, we performed simple downsampling. We then applied different
    Edge2Vec algorithms in combination with a random forest, for the supervised task,
    and k-means for an unsupervised task, achieving good classification performances.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes the series of examples that are used to show how graph
    machine learning algorithms can be applied to problems belonging to different
    domains, such as social network analysis, text analytics, and credit card transaction
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will describe some practical uses for graph databases
    and graph processing engines that are useful for scaling out the analysis to large
    graphs.
  prefs: []
  type: TYPE_NORMAL
