- en: Chapter 11. Dimension Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing feature selection with FSelector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing dimension reduction with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the number of principal components using a scree test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the number of principal components using the Kaiser method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing multivariate data using biplot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing dimension reduction with MDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing dimensions with SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing images with SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing nonlinear dimension reduction with ISOMAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing nonlinear dimension deduction with Local Linear Embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most datasets contain features (such as attributes or variables) that are highly
    redundant. In order to remove irrelevant and redundant data to reduce the computational
    cost and avoid overfitting, you can reduce the features into a smaller subset
    without a significant loss of information. The mathematical procedure of reducing
    features is known as dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reduction of features can increase the efficiency of data processing. Dimension
    reduction is, therefore, widely used in the fields of pattern recognition, text
    retrieval, and machine learning. Dimension reduction can be divided into two parts:
    feature extraction and feature selection. Feature extraction is a technique that
    uses a lower dimension space to represent data in a higher dimension space. Feature
    selection is used to find a subset of the original variables.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of feature selection is to select a set of relevant features to
    construct the model. The techniques for feature selection can be categorized into
    feature ranking and feature selection. Feature ranking ranks features with a certain
    criteria and then selects features that are above a defined threshold. On the
    other hand, feature selection searches the optimal subset from a space of feature
    subsets.
  prefs: []
  type: TYPE_NORMAL
- en: In feature extraction, the problem can be categorized as linear or nonlinear.
    The linear method searches an affine space that best explains the variation of
    data distribution. In contrast, the nonlinear method is a better option for data
    that is distributed on a highly nonlinear curved surface. Here, we list some common
    linear and nonlinear methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common linear methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**: Principal component analysis maps data to a lower dimension, so that
    the variance of the data in a low dimension representation is maximized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MDS**: Multidimensional scaling is a method that allows you to visualize
    how near (pattern proximities) objects are to each other and can produce a representation
    of your data with lower dimension space. PCA can be regarded as the simplest form
    of MDS if the distance measurement used in MDS equals the covariance of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SVD**: Singular value decomposition removes redundant features that are linear
    correlated from the perspective of linear algebra. PCA can also be regarded as
    a specific case of SVD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some common nonlinear methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ISOMAP**: ISOMAP can be viewed as an extension of MDS, which uses the distance
    metric of geodesic distances. In this method, geodesic distance is computed by
    graphing the shortest path distances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLE**: Locally linear embedding performs local PCA and global eigen-decomposition.
    LLE is a local approach, which involves selecting features for each category of
    the class feature. In contrast, ISOMAP is a global approach, which involves selecting
    features for all features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will first discuss how to perform feature ranking and selection.
    Next, we will focus on the topic of feature extraction and cover recipes in performing
    dimension reduction with both linear and nonlinear methods. For linear methods,
    we will introduce how to perform PCA, determine the number of principal components,
    and its visualization. We then move on to MDS and SVD. Furthermore, we will introduce
    the application of SVD to compress images. For nonlinear methods, we will introduce
    how to perform dimension reduction with ISOMAP and LLE.
  prefs: []
  type: TYPE_NORMAL
- en: Performing feature selection with FSelector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `FSelector` package provides two approaches to select the most influential
    features from the original feature set. Firstly, rank features by some criteria
    and select the ones that are above a defined threshold. Secondly, search for optimum
    feature subsets from a space of feature subsets. In this recipe, we will introduce
    how to perform feature selection with the `FSelector` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to train the support vector machine. For those who have not prepared
    the dataset, please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform feature selection on a `churn` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the package, `FSelector`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can use `random.forest.importance` to calculate the weight for each
    attribute, where we set the importance type to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can use the `cutoff` function to obtain the attributes of the top
    five weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can make an evaluator to select the feature subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can find the optimum feature subset using a hill climbing search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we present how to use the `FSelector` package to select the
    most influential features. We first demonstrate how to use the feature ranking
    approach. In the feature ranking approach, the algorithm first employs a weight
    function to generate weights for each feature. Here, we use the random forest
    algorithm with the mean decrease in accuracy (where `importance.type = 1`) as
    the importance measurement to gain the weights of each attribute. Besides the
    random forest algorithm, you can select other feature ranking algorithms (for
    example, `chi.squared`, `information.gain`) from the `FSelector` package. Then,
    the process sorts attributes by their weight. At last, we can obtain the top five
    features from the sorted feature list with the `cutoff` function. In this case,
    `number_customer_service_calls`, `international_plan, total_day_charge`, `total_day_minutes`,
    and `total_intl_calls` are the five most important features.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we illustrate how to search for optimum feature subsets. First, we need
    to make a five-fold cross-validation function to evaluate the importance of feature
    subsets. Then, we use the hill climbing searching algorithm to find the optimum
    feature subsets from the original feature sets. Besides the hill-climbing method,
    one can select other feature selection algorithms (for example, `forward.search`)
    from the `FSelector` package. Lastly, we can find that `international_plan + voice_mail_plan
    + number_vmail_messages + total_day_minutes + total_day_calls + total_eve_minutes
    + total_eve_charge + total_intl_minutes + total_intl_calls + total_intl_charge
    + number_customer_service_calls` are optimum feature subsets.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also use the `caret` package to perform feature selection. As we have
    discussed related recipes in the model assessment chapter, you can refer to [Chapter
    7](part0083_split_000.html#page "Chapter 7. Model Evaluation"), *Model Evaluation*,
    for more detailed information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For both feature ranking and optimum feature selection, you can explore the
    package, `FSelector`, for more related functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing dimension reduction with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is the most widely used linear method
    in dealing with dimension reduction problems. It is useful when data contains
    many features, and there is redundancy (correlation) within these features. To
    remove redundant features, PCA maps high dimension data into lower dimensions
    by reducing features into a smaller number of principal components that account
    for most of the variance of the original features. In this recipe, we will introduce
    how to perform dimension reduction with the PCA method.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `swiss` dataset as our target to perform PCA.
    The `swiss` dataset includes standardized fertility measures and socio-economic
    indicators from around the year 1888 for each of the 47 French-speaking provinces
    of Switzerland.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform principal component analysis on the
    `swiss` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the `swiss` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exclude the first column of the `swiss` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then perform principal component analysis on the `swiss` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain a summary from the PCA results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can use the `predict` function to output the value of the principal
    component with the first row of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the feature selection method may remove some correlated but informative
    features, you have to consider combining these correlated features into a single
    feature with the feature extraction method. PCA is one of the feature extraction
    methods, which performs orthogonal transformation to convert possibly correlated
    variables into principal components. Also, you can use these principal components
    to identify the directions of variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of PCA is carried on by the following steps: firstly, find the
    mean vector, ![How it works...](img/00217.jpeg), where ![How it works...](img/00218.jpeg)
    indicates the data point, and *n* denotes the number of points. Secondly, compute
    the covariance matrix by the equation, ![How it works...](img/00219.jpeg). Thirdly,
    compute the eigenvectors,![How it works...](img/00220.jpeg), and the corresponding
    eigenvalues. In the fourth step, we rank and choose the top *k* eigenvectors.
    In the fifth step, we construct a *d x k* dimensional eigenvector matrix, U. Here,
    *d* is the number of original dimensions and *k* is the number of eigenvectors.
    Finally, we can transform data samples to a new subspace in the equation, ![How
    it works...](img/00221.jpeg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, it is illustrated that we can use two principal components,
    ![How it works...](img/00222.jpeg), and ![How it works...](img/00223.jpeg), to
    transform the data point from a two-dimensional space to new two-dimensional subspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A sample illustration of PCA
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe we use the `prcomp` function from the `stats` package to perform
    PCA on the `swiss` dataset. First, we remove the standardized fertility measures
    and use the rest of the predictors as input to the function, `prcomp`. In addition
    to this, we set `swiss` as an input dataset; the variable should be shifted to
    the zero center by specifying `center=TRUE`; scale variables into the unit variance
    with the option, `scale=TRUE`, and store the output in the variable, `swiss.pca`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, as we print out the value stored in `swiss.pca`, we can find the standard
    deviation and rotation of the principal component. The standard deviation indicates
    the square root of the eigenvalues of the covariance/correlation matrix. On the
    other hand, the rotation of the principal components shows the coefficient of
    the linear combination of the input features. For example, PC1 equals *Agriculture
    * 0.524 + Examination * -0.572 + Education * -0.492 + Catholic* 0.385 + Infant.Mortality
    * 0.092*. Here, we can find that the attribute, *Agriculture*, contributes the
    most for PC1, for it has the highest coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can use the `summary` function to obtain the importance of
    components. The first row shows the standard deviation of each principal component,
    the second row shows the proportion of variance explained by each component, and
    the third row shows the cumulative proportion of the explained variance. Finally,
    you can use the `predict` function to obtain principal components from the input
    features. Here, we input the first row of the dataset, and retrieve five principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another principal component analysis function is `princomp`. In this function,
    the calculation is performed by using eigen on a correlation or covariance matrix
    instead of a single value decomposition used in the `prcomp` function. In general
    practice, using `prcomp` is preferable; however, we cover how to use `princomp`
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use `princomp` to perform PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then obtain the summary information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the `predict` function to obtain principal components from the
    input features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to the `prcomp` and `princomp` functions from the `stats` package,
    you can use the `principal` function from the `psych` package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `psych` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `principal` function to retrieve the principal components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Determining the number of principal components using the scree test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we only need to retain the principal components that account for most of
    the variance of the original features, we can either use the Kaiser method, scree
    test, or the percentage of variation explained as the selection criteria. The
    main purpose of a scree test is to graph the component analysis results as a scree
    plot and find where the obvious change in the slope (elbow) occurs. In this recipe,
    we will demonstrate how to determine the number of principal components using
    a scree plot.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by generating a principal
    component object and save it in the variable, `swiss.pca`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to determine the number of principal components
    with the scree plot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can generate a bar plot by using `screeplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00225.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The scree plot in bar plot form
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also generate a line plot by using `screeplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00226.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The scree plot in line plot form
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use a scree plot to determine the number
    of principal components. In a scree plot, there are two types of plots, namely,
    bar plots and line plots. As both generated scree plots reveal, the obvious change
    in slope (the so-called elbow or knee) occurs at component 2\. As a result, we
    should retain component 1, where the component is in a steep curve before component
    2, which is where the flat line trend commences. However, as this method can be
    ambiguous, you can use other methods (such as the Kaiser method) to determine
    the number of components.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, if you use the `plot` function on a generated principal component
    object, you can also retrieve the scree plot. For more details on `screeplot`,
    please refer to the following document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use `nfactors` to perform parallel analysis and nongraphical solutions
    to the Cattell scree test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Non-graphical solution to scree test
  prefs: []
  type: TYPE_NORMAL
- en: Determining the number of principal components using the Kaiser method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the scree test, you can use the Kaiser method to determine the
    number of principal components. In this method, the selection criteria retains
    eigenvalues greater than `1`. In this recipe, we will demonstrate how to determine
    the number of principal components using the Kaiser method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by generating a principal
    component object and save it in the variable, `swiss.pca`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to determine the number of principal components
    with the Kaiser method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can obtain the standard deviation from `swiss.pca`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can obtain the variance from `swiss.pca`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select components with a variance above 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also use the scree plot to select components with a variance above
    1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00228.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Select component with variance above 1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also use the Kaiser method to determine the number of components. As
    the computed principal component object contains the standard deviation of each
    component, we can compute the variance as the standard deviation, which is the
    square root of variance. From the computed variance, we find both component 1
    and 2 have a variance above 1\. Therefore, we can determine the number of principal
    components as 2 (both component 1 and 2). Also, we can draw a red line on the
    scree plot (as shown in the preceding figure) to indicate that we need to retain
    component 1 and 2 in this case.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to determine which principal components to retain, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ledesma, R. D., and Valero-Mora, P. (2007). *Determining the Number of Factors
    to Retain in EFA: an easy-to-use computer program for carrying out Parallel Analysis*.
    *Practical Assessment, Research & Evaluation*, 12(2), 1-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing multivariate data using biplot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to find out how data and variables are mapped in regard to the principal
    component, you can use `biplot`, which plots data and the projections of original
    features on to the first two components. In this recipe, we will demonstrate how
    to use `biplot` to plot both variables and data on the same figure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by generating a principal
    component object and save it in the variable, `swiss.pca`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to create a biplot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a scatter plot using component 1 and 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00229.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The scatter plot of first two components from PCA result
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you would like to add features on the plot, you can create biplot using
    the generated principal component object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00230.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The biplot using PCA result
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use `biplot` to plot data and projections
    of original features on to the first two components. In the first step, we demonstrate
    that we can actually use the first two components to create a scatter plot. Furthermore,
    if you want to add variables on the same plot, you can use `biplot`. In `biplot`,
    you can see the provinces with higher indicators in the agriculture variable,
    lower indicators in the education variable, and examination variables scores that
    are higher in PC1\. On the other hand, the provinces with higher infant mortality
    indicators and lower agriculture indicators score higher in PC2.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides `biplot` in the `stats` package, you can also use `ggbiplot`. However,
    you may not find this package from CRAN; you have to first install `devtools`
    and then install `ggbiplot` from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The ggbiplot using PCA result
  prefs: []
  type: TYPE_NORMAL
- en: Performing dimension reduction with MDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Multidimensional scaling** (**MDS**) is a technique to create a visual presentation
    of similarities or dissimilarities (distance) of a number of objects. The *multi*
    prefix indicates that one can create a presentation map in one, two, or more dimensions.
    However, we most often use MDS to present the distance between data points in
    one or two dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: In MDS, you can either use a metric or a nonmetric solution. The main difference
    between the two solutions is that metric solutions try to reproduce the original
    metric, while nonmetric solutions assume that the ranks of the distance are known.
    In this recipe, we will illustrate how to perform MDS on the `swiss` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue using the `swiss` dataset as our input data
    source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform multidimensional scaling using the metric
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can perform metric MDS with a maximum of two dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then plot the `swiss` data in a two-dimension scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00232.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The 2-dimension scatter plot from cmdscale object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In addition, you can perform nonmetric MDS with `isoMDS`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also plot the data points in a two-dimension scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00233.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The 2-dimension scatter plot from isoMDS object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can then plot the data points in a two-dimension scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00234.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The Shepard plot from isoMDS object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MDS reveals the structure of the data by providing a visual presentation of
    similarities among a set of objects. In more detail, MDS places an object in an
    n-dimensional space, where the distances between pairs of points corresponds to
    the similarities among the pairs of objects. Usually, the dimensional space is
    a two-dimensional Euclidean space, but it may be non-Euclidean and have more than
    two dimensions. In accordance with the meaning of the input matrix, MDS can be
    mainly categorized into two types: metric MDS, where the input matrix is metric-based,
    nonmetric MDS, where the input matrix is nonmetric-based.'
  prefs: []
  type: TYPE_NORMAL
- en: Metric MDS is also known as principal coordinate analysis, which first transforms
    a distance into similarities. In the simplest form, the process linearly projects
    original data points to a subspace by performing principal components analysis
    on similarities. On the other hand, the process can also perform a nonlinear projection
    on similarities by minimizing the stress value, ![How it works...](img/00235.jpeg),
    where ![How it works...](img/00236.jpeg) is the distance measurement between the
    two points, ![How it works...](img/00237.jpeg) and ![How it works...](img/00238.jpeg),
    and ![How it works...](img/00239.jpeg) is the similarity measure of two projected
    points, ![How it works...](img/00240.jpeg) and ![How it works...](img/00241.jpeg).
    As a result, we can represent the relationship among objects in the Euclidean
    space.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to metric MDS, which use a metric-based input matrix, a nonmetric-based
    MDS is used when the data is measured at the ordinal level. As only the rank order
    of the distances between the vectors is meaningful, nonmetric MDS applies a monotonically
    increasing function, f, on the original distances and projects the distance to
    new values that preserve the rank order. The normalized equation can be formulated
    as ![How it works...](img/00242.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we illustrate how to perform metric and nonmetric MDS on the
    `swiss` dataset. To perform metric MDS, we first need to obtain the distance metric
    from the `swiss` data. In this step, you can replace the distance measure to any
    measure as long as it produces a similarity/dissimilarity measure of data points.
    You can use `cmdscale` to perform metric multidimensional scaling. Here, we specify
    `k = 2`, so the maximum generated dimensions equals `2`. You can also visually
    present the distance of the data points on a two-dimensional scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can perform nonmetric MDS with `isoMDS`. In nonmetric MDS, we do not
    match the distances, but only arrange them in order. We also set `swiss` as an
    input dataset with maximum dimensions of two. Similar to the metric MDS example,
    we can plot the distance between data points on a two-dimensional scatter plot.
    Then, we use a Shepard plot, which shows how well the projected distances match
    those in the distance matrix. As per the figure in step 4, the projected distance
    matches well in the distance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another visualization method is to present an MDS object as a graph. A sample
    code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The graph presentation of MDS object
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also compare differences between the generated results from MDS and
    PCA. You can compare their differences by drawing the projected dimensions on
    the same scatter plot. If you use a Euclidean distance on MDS, the projected dimensions
    are exactly the same as the ones projected from PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00244.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The comparison between MDS and PCA
  prefs: []
  type: TYPE_NORMAL
- en: Reducing dimensions with SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Singular value decomposition** (**SVD**) is a type of matrix factorization
    (decomposition), which can factorize matrices into two orthogonal matrices and
    diagonal matrices. You can multiply the original matrix back using these three
    matrices. SVD can reduce redundant data that is linear dependent from the perspective
    of linear algebra. Therefore, it can be applied to feature selection, image processing,
    clustering, and many other fields. In this recipe, we will illustrate how to perform
    dimension reduction with SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue using the dataset, `swiss`, as our input data
    source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform dimension reduction using SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can perform `svd` on the `swiss` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then plot the percentage of variance explained and the cumulative variance
    explained in accordance with the SVD column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00245.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The percent of variance explained
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00246.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Cumulative percent of variance explained
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can reconstruct the data with only one singular vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can compare the original dataset with the constructed dataset in
    an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00247.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The comparison between original dataset and re-constructed dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVD is a factorization of a real or complex matrix. In detail, the SVD of m
    x n matrix, A, is the factorization of A into the product of three matrices, ![How
    it works...](img/00248.jpeg). Here, U is an m x m orthonormal matrix, D has singular
    values and is an m x n diagonal matrix, and V^T is an n x n orthonormal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we demonstrate how to perform dimension reduction with SVD.
    First, you can apply the `svd` function on the `swiss` dataset to obtain factorized
    matrices. You can then generate two plots: one shows the variance explained in
    accordance to a singular vector, the other shows the cumulative variance explained
    in accordance to a singular vector.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows that the first singular vector can explain 80 percent
    of variance. We now want to compare the differences from the original dataset
    and the reconstructed dataset with a single singular vector. We, therefore, reconstruct
    the data with a single singular vector and use the `image` function to present
    the original and reconstructed datasets side-by-side and see how they differ from
    each other. The next figure reveals that these two images are very similar.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, PCA can be regarded as a specific case of SVD. Here,
    we generate the orthogonal vector from the `swiss` data from SVD and obtained
    the rotation from `prcomp`. We can see that the two generated matrices are the
    same:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compressing images with SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we demonstrated how to factorize a matrix with SVD and
    then reconstruct the dataset by multiplying the decomposed matrix. Furthermore,
    the application of matrix factorization can be applied to image compression. In
    this recipe, we will demonstrate how to perform SVD on the classic image processing
    material, Lenna.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you should download the image of Lenna beforehand (refer to
    [http://www.ece.rice.edu/~wakin/images/lena512.bmp](http://www.ece.rice.edu/~wakin/images/lena512.bmp)
    for this), or you can prepare an image of your own to see how image compression
    works.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to compress an image with SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load `bmp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then read the image of Lenna as a numeric matrix with the `read.bmp`
    function. When the reader downloads the image, the default name is `lena512.bmp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rotate and plot the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00249.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The picture of Lenna
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can perform SVD on the read numeric matrix and plot the percentage
    of variance explained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00250.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The percentage of variance explained
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can obtain the number of dimensions to reconstruct the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the point at which the singular vector can explain more than 90 percent
    of the variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also wrap the code into a function, `lenna_compression`, and you can
    then use this function to plot compressed Lenna:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you can use 18 vectors to reconstruct the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00251.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The reconstructed image with 18 components
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can obtain the point at which the singular vector can explain more than
    99 percent of the variance;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00252.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The reconstructed image with 92 components
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to compress an image with SVD. In the first
    step, we use the package, `bmp`, to load the image, Lenna, to an R session. Then,
    as the read image is rotated, we can rotate the image back and use the `plot`
    function to plot Lenna in R (as shown in the figure in step 3). Next, we perform
    SVD on the image matrix to factorize the matrix. We then plot the percentage of
    variance explained in regard to the number of singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Further, as we discover that we can use 18 components to explain 90 percent
    of the variance, we then use these 18 components to reconstruct Lenna. Thus, we
    make a function named `lenna_compression` with the purpose of reconstructing the
    image by matrix multiplication. As a result, we enter 18 as the input to the function,
    which returns a rather blurry Lenna image (as shown in the figure in step 8).
    However, we can at least see an outline of the image. To obtain a clearer picture,
    we discover that we can use 92 components to explain 99 percent of the variance.
    We, therefore, set the input to the function, `lenna_compression`, as 92\. The
    figure in step 9 shows that this generates a clearer picture than the one constructed
    using merely 18 components.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Lenna picture is one of the most widely used standard test images for compression
    algorithms. For more details on the Lenna picture, please refer to [http://www.cs.cmu.edu/~chuck/lennapg/](http://www.cs.cmu.edu/~chuck/lennapg/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing nonlinear dimension reduction with ISOMAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ISOMAP is one of the approaches for manifold learning, which generalizes linear
    framework to nonlinear data structures. Similar to MDS, ISOMAP creates a visual
    presentation of similarities or dissimilarities (distance) of a number of objects.
    However, as the data is structured in a nonlinear format, the Euclidian distance
    measure of MDS is replaced by the geodesic distance of a data manifold in ISOMAP.
    In this recipe, we will illustrate how to perform a nonlinear dimension reduction
    with ISOMAP.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `digits` data from `RnavGraphImageData` as our
    input source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform nonlinear dimension reduction with ISOMAP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `RnavGraphImageData` and `vegan` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then load the dataset, `digits`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rotate and plot the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00253.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A sample image from the digits dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can randomly sample 300 digits from the population:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transpose the selected digit data and then compute the dissimilarity between
    objects using `vegdist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use `isomap` to perform dimension reduction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00254.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A 2-dimension scatter plot from ISOMAP object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, you can overlay the scatter plot with the minimum spanning tree, marked
    in red;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00255.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A 2-dimension scatter plot overlay with minimum spanning tree
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ISOMAP is a nonlinear dimension reduction method and a representative of isometric
    mapping methods. ISOMAP can be regarded as an extension of the metric MDS, where
    pairwise the Euclidean distance among data points is replaced by geodesic distances
    induced by a neighborhood graph.
  prefs: []
  type: TYPE_NORMAL
- en: The description of the ISOMAP algorithm is shown in four steps. First, determine
    the neighbor of each point. Secondly, construct a neighborhood graph. Thirdly,
    compute the shortest distance path between two nodes. At last, find a low dimension
    embedding of the data by performing MDS.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to perform a nonlinear dimension reduction
    using ISOMAP. First, we load the digits data from `RnavGraphImageData`. Then,
    after we select one digit and plot its rotated image, we can see an image of the
    handwritten digit (the numeral 3, in the figure in step 3).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample 300 digits as our input data to ISOMAP. We then transpose
    the dataset to calculate the distance between each image object. Once the data
    is ready, we calculate the distance between each object and perform a dimension
    reduction. Here, we use `vegdist` to calculate the dissimilarities between each
    object using a Euclidean measure. We then use ISOMAP to perform a nonlinear dimension
    reduction on the `digits` data with the dimension set as `6`, number of shortest
    dissimilarities retained for a point as `8`, and ensure that you analyze the largest
    connected group by specifying `fragmentedOK` as `TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can use the generated ISOMAP object to make a two-dimension scatter
    plot (figure in step 6), and also overlay the minimum spanning tree with lines
    in red on the scatter plot (figure in step 7).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also use the `RnavGraph` package to visualize high dimensional data
    (digits in this case) using graphs as a navigational infrastructure. For more
    information, please refer to [http://www.icesi.edu.co/CRAN/web/packages/RnavGraph/vignettes/RnavGraph.pdf](http://www.icesi.edu.co/CRAN/web/packages/RnavGraph/vignettes/RnavGraph.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a description of how you can use `RnavGraph` to visualize high dimensional
    data in a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `RnavGraph` and `graph` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then create an `NG_data` object from the `digit` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an `NG_graph` object from `NG_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize the graph in the `tk2d` plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00256.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A 3-D Transition graph plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One can also view a 4D transition graph plot:![There's more...](img/00257.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A 4D transition graph plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Performing nonlinear dimension reduction with Local Linear Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Locally linear embedding** (**LLE**) is an extension of PCA, which reduces
    data that lies on a manifold embedded in a high dimensional space into a low dimensional
    space. In contrast to ISOMAP, which is a global approach for nonlinear dimension
    reduction, LLE is a local approach that employs a linear combination of the k-nearest
    neighbor to preserve local properties of data. In this recipe, we will give a
    short introduction of how to use LLE on an s-curve data.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use digit data from `lle_scurve_data` within the `lle`
    package as our input source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform nonlinear dimension reduction with LLE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install and load the package, `lle`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then load `ll_scurve_data` from `lle`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, perform `lle` on `lle_scurve_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the result with the `str` and `plot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00258.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A 2-D scatter plot of embedded data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, you can use `plot_lle` to plot the LLE result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00259.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A LLE plot of LLE result
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLE is a nonlinear dimension reduction method, which computes a low dimensional,
    neighborhood, preserving embeddings of high dimensional data. The algorithm of
    LLE can be illustrated in these steps: first, LLE computes the k-neighbors of
    each data point, ![How it works...](img/00218.jpeg). Secondly, it computes a set
    of weights for each point, which minimizes the residual sum of errors, which can
    best reconstruct each data point from its neighbors. The residual sum of errors
    can be described as ![How it works...](img/00260.jpeg), where ![How it works...](img/00261.jpeg)
    if ![How it works...](img/00262.jpeg) is not one of ![How it works...](img/00218.jpeg)''s
    k-nearest neighbor, and for each i, ![How it works...](img/00263.jpeg). Finally,
    find the vector, Y, which is best reconstructed by the weight, W. The cost function
    can be illustrated as ![How it works...](img/00264.jpeg), with the constraint
    that ![How it works...](img/00265.jpeg), and ![How it works...](img/00266.jpeg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we demonstrate how to perform nonlinear dimension reduction
    using LLE. First, we load `lle_scurve_data` from `lle`. We then perform `lle`
    with two dimensions and 12 neighbors, and list the dimensions for every data point
    by specifying `id =TRUE`. The LLE has three steps, including: building a neighborhood
    for each point in the data, finding the weights for linearly approximating the
    data in that neighborhood, and finding the low dimensional coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can examine the data using the `str` and `plot` functions. The `str`
    function returns X,Y, choice, and ID. Here, X represents the input data, Y stands
    for the embedded data, choice indicates the index vector of the kept data, while
    subset selection and ID show the dimensions of every data input. The `plot` function
    returns the scatter plot of the embedded data. Lastly, we use `plot_lle` to plot
    the result. Here, we enable the interaction mode by setting the inter equal to
    `TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful package for nonlinear dimension reduction is `RDRToolbox`, which
    is a package for nonlinear dimension reduction with ISOMAP and LLE. You can use
    the following command to install `RDRToolbox`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
