<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer079">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 class="chapterTitle" id="_idParaDest-108"><span class="koboSpan" id="kobo.2.1">Data Management for ML</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">As an ML solutions architecture practitioner, I often receive requests for guidance on designing data management platforms for ML workloads. </span><span class="koboSpan" id="kobo.3.2">Although data management platform architecture is typically treated as a separate technical discipline, it plays a crucial role in ML workloads. </span><span class="koboSpan" id="kobo.3.3">To create a comprehensive ML platform, ML solutions architects must understand the essential data architecture considerations for ML and be familiar with the technical design of a data management platform that caters to the needs of data scientists and automated ML pipelines. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">In this chapter, we will explore the intersection of data management and ML, discussing key considerations for designing a data management platform specifically tailored for ML. </span><span class="koboSpan" id="kobo.4.2">We will delve into the core architectural components of such a platform and examine relevant AWS technologies and services that can be used to build it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.5.1">The following topics will be covered:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.6.1">Data management considerations for ML</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.7.1">Data management architecture for ML</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">Hands-on exercise – data management for ML</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-109"><span class="koboSpan" id="kobo.9.1">Technical requirements</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.10.1">In this chapter, you will need access to an AWS account and AWS services such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.11.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.12.1">S3</span></strong><span class="koboSpan" id="kobo.13.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.14.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.15.1">Lake Formation</span></strong><span class="koboSpan" id="kobo.16.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.17.1">AWS</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.18.1">Glue</span></strong><span class="koboSpan" id="kobo.19.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.20.1">AWS</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.21.1">Lambda</span></strong><span class="koboSpan" id="kobo.22.1">. </span><span class="koboSpan" id="kobo.22.2">If you do not have an AWS account, follow the official AWS website’s instructions to create an account.</span></p>
<h1 class="heading-1" id="_idParaDest-110"><span class="koboSpan" id="kobo.23.1">Data management considerations for ML</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.24.1">Data management is a</span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.25.1"> broad and complex topic. </span><span class="koboSpan" id="kobo.25.2">Many organizations have dedicated data management teams and organizations to manage and </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.26.1">govern the various aspects of a data platform. </span><span class="koboSpan" id="kobo.26.2">Historically, data management primarily revolved around fulfilling the requirements of transactional systems and analytics systems. </span><span class="koboSpan" id="kobo.26.3">However, as ML solutions gain prominence, there are now additional business and technological factors to consider when it comes to data management platforms. </span><span class="koboSpan" id="kobo.26.4">The advent of ML introduces new requirements and challenges that necessitate an evolution in data management practices to effectively support these advanced solutions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.27.1">To understand where data management intersects with the ML workflow, let’s bring back the ML lifecycle, as</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.28.1"> illustrated in the following figure:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.29.1"><img alt="Figure 4.1 – Intersection of data management and the ML life cycle " src="../Images/B20836_04_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.30.1">Figure 4.1: Intersection of data management and the ML lifecycle</span></p>
<p class="normal"><span class="koboSpan" id="kobo.31.1">At a high level, data management intersects with the ML lifecycle in three stages: </span><em class="italic"><span class="koboSpan" id="kobo.32.1">data understanding and preparation</span></em><span class="koboSpan" id="kobo.33.1">, </span><em class="italic"><span class="koboSpan" id="kobo.34.1">model training and evaluation</span></em><span class="koboSpan" id="kobo.35.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.36.1">model deployment</span></em><span class="koboSpan" id="kobo.37.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.38.1">During the </span><em class="italic"><span class="koboSpan" id="kobo.39.1">data understanding and preparation </span></em><span class="koboSpan" id="kobo.40.1">stage, data scientists undertake several essential tasks. </span><span class="koboSpan" id="kobo.40.2">They begin by identifying relevant data sources that contain datasets suitable for their modeling tasks. </span><span class="koboSpan" id="kobo.40.3">Exploratory data analysis is then performed to gain insights into the dataset, including data statistics, correlations between features, and data sample distributions. </span><span class="koboSpan" id="kobo.40.4">Additionally, data preparation for model training and validation is crucial, involving a series of steps that typically include the following:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.41.1">Data validation</span></strong><span class="koboSpan" id="kobo.42.1">: The </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.43.1">data is checked for errors and anomalies to ensure its quality. </span><span class="koboSpan" id="kobo.43.2">This includes verifying the data range, distribution, and data types and identifying missing or null values.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.44.1">Data cleaning</span></strong><span class="koboSpan" id="kobo.45.1">: Any</span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.46.1"> identified data errors are fixed or corrected to ensure the accuracy and consistency of the dataset. </span><span class="koboSpan" id="kobo.46.2">This may involve removing duplicates, handling missing values, or resolving inconsistencies.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.47.1">Data enrichment</span></strong><span class="koboSpan" id="kobo.48.1">: Additional </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.49.1">value is derived from the data through techniques like joining different datasets or transforming the data. </span><span class="koboSpan" id="kobo.49.2">This helps generate new signals and insights that can enhance the modeling process.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.50.1">Data labeling</span></strong><span class="koboSpan" id="kobo.51.1">: For </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.52.1">supervised ML model training, training and testing datasets need to be labeled by human annotators or the ML model accurately. </span><span class="koboSpan" id="kobo.52.2">This critical step is necessary to guarantee the development and validation of high-quality models.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.53.1">The data management capabilities needed during this stage encompass the following aspects:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.54.1">Dataset discovery</span></strong><span class="koboSpan" id="kobo.55.1">: The </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.56.1">capability to search and locate curated datasets using relevant metadata like dataset name, description, field name, and data owner.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.57.1">Data access</span></strong><span class="koboSpan" id="kobo.58.1">: The</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.59.1"> ability to access both raw and processed datasets to perform exploratory data analysis. </span><span class="koboSpan" id="kobo.59.2">This ensures data scientists can explore and analyze the data effectively.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.60.1">Querying and retrieval</span></strong><span class="koboSpan" id="kobo.61.1">: The </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.62.1">capability to run queries against selected datasets to obtain details such as statistical information, data quality metrics, and data samples. </span><span class="koboSpan" id="kobo.62.2">Additionally, it includes the ability to retrieve data from the data management platform to a data science environment for further processing and feature engineering.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.63.1">Scalable data processing</span></strong><span class="koboSpan" id="kobo.64.1">: The </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.65.1">ability to execute data processing operations on large datasets efficiently. </span><span class="koboSpan" id="kobo.65.2">This ensures that data scientists can handle and process substantial amounts of data during model development and experimentation.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.66.1">During the stage of model training and validation, data scientists are responsible for generating a training and validation dataset to conduct formal model training. </span><span class="koboSpan" id="kobo.66.2">To facilitate this process, the following data management capabilities are essential:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">Data processing and automated workflows</span></strong><span class="koboSpan" id="kobo.68.1">: A data management platform should provide</span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.69.1"> robust data processing capabilities along with automated workflows. </span><span class="koboSpan" id="kobo.69.2">This enables the conversion of raw or curated datasets into training and validation datasets in various formats suitable for model training.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.70.1">Data repository and versioning</span></strong><span class="koboSpan" id="kobo.71.1">: An efficient </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.72.1">data management platform should offer a dedicated data repository to store and manage the training and validation datasets. </span><span class="koboSpan" id="kobo.72.2">Additionally, it should support versioning, allowing data scientists to keep track of different iterations and modifications made to the datasets, along with the versions of the code and trained ML models.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.73.1">Data labeling</span></strong><span class="koboSpan" id="kobo.74.1">: For </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.75.1">supervised ML model training, training and testing datasets need to be labeled by human annotators or the ML model accurately. </span><span class="koboSpan" id="kobo.75.2">This critical step is necessary to guarantee the development and validation of high-quality models. </span><span class="koboSpan" id="kobo.75.3">This is a highly labor-intensive task, requiring purpose-built software tools to do it at scale.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.76.1">ML features/embeddings generation and storage</span></strong><span class="koboSpan" id="kobo.77.1">: Some ML features/embeddings (e.g., averages, sums, and text embeddings) need to be pre-computed for</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.78.1"> one or more downstream model training tasks. </span><span class="koboSpan" id="kobo.78.2">These features/embeddings often need to be managed using purpose-built tools for efficient access and reuse. </span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.79.1">Dataset provisioning for model training</span></strong><span class="koboSpan" id="kobo.80.1">: The platform should provide mechanisms to</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.81.1"> serve the training and validation datasets to the model training infrastructure. </span><span class="koboSpan" id="kobo.81.2">This ensures that the datasets are accessible by the training environment, allowing data scientists to train models effectively.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.82.1">During the stage of model deployment, the focus shifts toward utilizing the trained models to serve predictions. </span><span class="koboSpan" id="kobo.82.2">To support this stage effectively, the following data management capabilities are crucial:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.83.1">Serving data for feature processing</span></strong><span class="koboSpan" id="kobo.84.1">: The data management platform should be capable of </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.85.1">serving the data required for feature processing as part of the input data when invoking the deployed models. </span><span class="koboSpan" id="kobo.85.2">This ensures that the models receive the relevant data inputs required for generating predictions.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.86.1">Serving pre-computed features/embeddings</span></strong><span class="koboSpan" id="kobo.87.1">: In some cases, pre-computed features/embeddings are utilized as inputs when invoking the deployed models. </span><span class="koboSpan" id="kobo.87.2">The </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.88.1">data management platform should have the capability to serve these pre-computed features seamlessly, allowing the models to incorporate them into the prediction process.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.89.1">In contrast to traditional data access patterns for transactional or business intelligence solutions, where developers can utilize non-production data in lower environments for development purposes, data scientists typically require access to production data for model development.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.90.1">Having explored the considerations for ML data management, we will now delve deeper into the data management architecture specifically designed for ML. </span><span class="koboSpan" id="kobo.90.2">It is important to understand that effective data management is crucial for success in applied ML. </span><span class="koboSpan" id="kobo.90.3">Organizations fail with ML not just due to poor algorithms or inaccurate models, but also due to problems with real-world data and production systems. </span><span class="koboSpan" id="kobo.90.4">Data management shortcomings can sink ML projects despite brilliant modeling.</span></p>
<h1 class="heading-1" id="_idParaDest-111"><span class="koboSpan" id="kobo.91.1">Data management architecture for ML</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.92.1">Depending </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.93.1">on the scale of your ML initiatives, it is important to consider different data management architecture patterns to effectively support them. </span><span class="koboSpan" id="kobo.93.2">The right architecture depends on the scale and scope of the ML initiatives within an organization in order to balance the business needs with engineering efforts.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.94.1">For </span><em class="italic"><span class="koboSpan" id="kobo.95.1">small-scale ML projects</span></em><span class="koboSpan" id="kobo.96.1"> characterized by limited data scope, a small team size, and minimal cross-functional </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.97.1">dependencies, a purpose-built data pipeline tailored to meet specific project requirements can be a suitable approach. </span><span class="koboSpan" id="kobo.97.2">For instance, if your project involves working with structured data sourced from an existing data warehouse and a publicly available dataset, you can consider developing a straightforward data pipeline. </span><span class="koboSpan" id="kobo.97.3">This pipeline would extract the necessary data from the data warehouse and public domain and store it in a dedicated storage location owned by the project team. </span><span class="koboSpan" id="kobo.97.4">This data extraction process can be scheduled as needed to facilitate further analysis and processing. </span><span class="koboSpan" id="kobo.97.5">The following diagram illustrates a simplified data management flow designed to support a small-scale ML project:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.98.1"><img alt="Figure 4.2 – Data architecture for an ML project with limited scope " src="../Images/B20836_04_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.99.1">Figure 4.2: Data architecture for an ML project with limited scope</span></p>
<p class="normal"><span class="koboSpan" id="kobo.100.1">For </span><em class="italic"><span class="koboSpan" id="kobo.101.1">large-scale ML initiatives</span></em><span class="koboSpan" id="kobo.102.1"> at the enterprise level, the data management architecture closely resembles that </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.103.1">of enterprise analytics. </span><span class="koboSpan" id="kobo.103.2">Both require robust support for data ingestion from diverse sources and centralized management of data for various processing and access requirements. </span><span class="koboSpan" id="kobo.103.3">While analytics data management primarily deals with structured data and often relies on an enterprise data warehouse as its core backend, ML data management needs to handle structured, semi-structured, and unstructured data for different ML tasks. </span><span class="koboSpan" id="kobo.103.4">Consequently, a data lake architecture is commonly adopted. </span><span class="koboSpan" id="kobo.103.5">ML data management is typically an integral part of the broader enterprise data management strategy, encompassing both analytics and ML initiatives.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.104.1">The following </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.105.1">diagram illustrates a logical enterprise data management architecture comprising key components such as data ingestion, data storage, data processing, data catalog, data security, and data access:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.106.1"><img alt="Figure 4.3 – Enterprise data management " src="../Images/B20836_04_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.107.1">Figure 4.3: Enterprise data management</span></p>
<p class="normal"><span class="koboSpan" id="kobo.108.1">In the following sections, we will delve into a detailed analysis of each key component of enterprise data management, providing an in-depth understanding of their functionalities and implications within a data management architecture built using AWS native services in the cloud. </span><span class="koboSpan" id="kobo.108.2">By exploring the specific characteristics and capabilities of these components, we will gain valuable insights into the overall structure and mechanics of an AWS-based data management architecture.</span></p>
<h2 class="heading-2" id="_idParaDest-112"><span class="koboSpan" id="kobo.109.1">Data storage and management</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.110.1">Data storage</span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.111.1"> and management is a fundamental component of the overall ML data management architecture. </span><span class="koboSpan" id="kobo.111.2">ML workloads often require data from diverse sources and in various formats, and the sheer volume of data can be substantial, particularly when dealing with unstructured data. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.112.1">To address these requirements, cloud object data storage solutions like Amazon S3 are commonly employed as the underlying storage medium. </span><span class="koboSpan" id="kobo.112.2">Conceptually, cloud object storage can be likened to a file storage system that accommodates files</span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.113.1"> of different formats. </span><span class="koboSpan" id="kobo.113.2">Moreover, the storage system allows for the organization of files using prefixes, which serve as virtual folders for enhanced object management. </span><span class="koboSpan" id="kobo.113.3">It is important to note that these prefixes do not correspond to physical folder structures. </span><span class="koboSpan" id="kobo.113.4">The</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.114.1"> term “object storage” stems from the fact that each file is treated as an independent object, bundled with metadata, and assigned a unique identifier. </span><span class="koboSpan" id="kobo.114.2">Object storage boasts features such as virtually unlimited storage capacity, robust object analytics based on metadata, API-based access, and cost-effectiveness.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.115.1">To efficiently handle the vast quantities of data stored in cloud object storage, it is advisable to</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.116.1"> implement a data lake architecture that leverages this storage medium. </span><span class="koboSpan" id="kobo.116.2">A data lake, tailored to encompass the entire enterprise or a specific line of business, acts as a centralized hub for data management and access. </span><span class="koboSpan" id="kobo.116.3">Designed to accommodate limitless data volumes, the data lake facilitates the organization of data across various lifecycle stages, including raw, transformed, curated, and ML feature data. </span><span class="koboSpan" id="kobo.116.4">Its primary purpose is to consolidate disparate data silos into a singular repository that enables centralized management and access for both analytics and ML requirements. </span><span class="koboSpan" id="kobo.116.5">Notably, a data lake can house diverse data formats, such as structured data from databases, unstructured data like documents, semi-structured data in JSON and XML formats, as well as binary formats encompassing images, videos, and audio files. </span><span class="koboSpan" id="kobo.116.6">This capability proves particularly invaluable for ML workloads, as ML often involves working with data in multiple formats.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.117.1">The data lake should be organized into different zones. </span><span class="koboSpan" id="kobo.117.2">For example, a </span><em class="italic"><span class="koboSpan" id="kobo.118.1">landing zone</span></em><span class="koboSpan" id="kobo.119.1"> should be </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.120.1">established as the target for the initial data ingestion from different sources. </span><span class="koboSpan" id="kobo.120.2">After data preprocessing and data quality management processing, the data can be moved to the raw data zone. </span><span class="koboSpan" id="kobo.120.3">Data in </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.121.1">the </span><em class="italic"><span class="koboSpan" id="kobo.122.1">raw data zone</span></em><span class="koboSpan" id="kobo.123.1"> can be further transformed and processed to meet different business and downstream consumption needs. </span><span class="koboSpan" id="kobo.123.2">To further ensure the reliability of the dataset for usage, the data can be curated and </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.124.1">stored in the </span><em class="italic"><span class="koboSpan" id="kobo.125.1">curated data zone</span></em><span class="koboSpan" id="kobo.126.1">. </span><span class="koboSpan" id="kobo.126.2">For ML tasks, ML features often need to be pre-computed and stored in an ML feature zone for reuse purposes.</span></p>
<h3 class="heading-3" id="_idParaDest-113"><span class="koboSpan" id="kobo.127.1">AWS Lake Formation</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.128.1">AWS Lake Formation is a </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.129.1">comprehensive data management service offered by AWS, which streamlines the process of building and maintaining a data lake on the AWS platform. </span><span class="koboSpan" id="kobo.129.2">The </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.130.1">following figure illustrates the core components of AWS Lake Formation:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.131.1"><img alt="Figure 4.4 – AWS Lake Formation " src="../Images/B20836_04_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.132.1">Figure 4.4: AWS Lake Formation</span></p>
<p class="normal"><span class="koboSpan" id="kobo.133.1">Overall, AWS Lake Formation </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.134.1">offers four fundamental capabilities to enhance data lake management:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.135.1">Data source crawler</span></strong><span class="koboSpan" id="kobo.136.1">: This</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.137.1"> functionality automatically examines data files within the data lake to infer their underlying structure, enabling efficient organization and categorization of the data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.138.1">Data catalog creation and maintenance</span></strong><span class="koboSpan" id="kobo.139.1">: AWS Lake Formation facilitates the creation and ongoing management of a data catalog, providing a centralized repository for metadata, enabling easy data discovery and exploration within the data lake.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.140.1">Data transformation processing</span></strong><span class="koboSpan" id="kobo.141.1">: With built-in data transformation capabilities, the service allows for the processing and transformation of data stored in the data lake, enabling data scientists and analysts to work with refined and optimized datasets.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.142.1">Data security and access control</span></strong><span class="koboSpan" id="kobo.143.1">: AWS Lake Formation ensures robust data security by providing comprehensive access control mechanisms and enabling</span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.144.1"> fine-grained permissions management, ensuring that data is accessed only by authorized individuals and teams.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.145.1">Lake Formation integrates with </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.146.1">AWS Glue, a serverless </span><strong class="keyWord"><span class="koboSpan" id="kobo.147.1">Extract, Transform, Load</span></strong><span class="koboSpan" id="kobo.148.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.149.1">ETL</span></strong><span class="koboSpan" id="kobo.150.1">) and data catalog service, to provide data catalog management and data ETL processing functionality. </span><span class="koboSpan" id="kobo.150.2">We will cover ETL and data catalog components separately in later sections.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.151.1">Lake Formation provides a centralized data access management capability for managing data access permissions for databases, tables, or different registered S3 locations. </span><span class="koboSpan" id="kobo.151.2">For databases and tables, the permission can be granularly assigned to individual tables and columns and database functions, such as creating tables and inserting records.</span></p>
<h2 class="heading-2" id="_idParaDest-114"><span class="koboSpan" id="kobo.152.1">Data ingestion</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.153.1">Data ingestion is </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.154.1">the bridge between data sources and data storage. </span><span class="koboSpan" id="kobo.154.2">It plays a crucial role in acquiring data from diverse sources, including structured, semi-structured, and unstructured formats, such as databases, knowledge graphs, social media, file storage, and IoT devices. </span><span class="koboSpan" id="kobo.154.3">Its primary responsibility is to store this data persistently in various storage solutions like object data storage (e.g., Amazon S3), data warehouses, or other data stores. </span><span class="koboSpan" id="kobo.154.4">Effective data ingestion patterns should incorporate both real-time streaming and batch ingestion mechanisms to cater to different types of data sources and ensure timely and efficient data acquisition.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.155.1">Various data ingestion technologies and tools cater to different ingestion patterns. </span><span class="koboSpan" id="kobo.155.2">For streaming data ingestion, popular choices</span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.156.1"> include Apache Kafka, Apache Spark Streaming, and </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.157.1">Amazon Kinesis/Kinesis Firehose. </span><span class="koboSpan" id="kobo.157.2">These tools</span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.158.1"> enable real-time data ingestion </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.159.1">and processing. </span><span class="koboSpan" id="kobo.159.2">On the other hand, for batch-oriented data ingestion, tools like </span><strong class="keyWord"><span class="koboSpan" id="kobo.160.1">Secure File Transfer Protocol</span></strong><span class="koboSpan" id="kobo.161.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.162.1">SFTP</span></strong><span class="koboSpan" id="kobo.163.1">) and AWS Glue are commonly used. </span><span class="koboSpan" id="kobo.163.2">AWS Glue, in particular, offers support for a wide range of data sources and targets, including Amazon RDS, MongoDB, Kafka, Amazon DocumentDB, S3, and any databases that support JDBC connections. </span><span class="koboSpan" id="kobo.163.3">This flexibility allows for seamless ingestion of data from various sources into the desired data storage or processing systems.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.164.1">When making </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.165.1">decisions on which tools to use for data ingestion, it is important to assess the tools and technologies based on practical needs. </span><span class="koboSpan" id="kobo.165.2">The following are some of the considerations when deciding on data ingestion tools:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.166.1">Data format, size, and scalability</span></strong><span class="koboSpan" id="kobo.167.1">: Take into account the various data formats, data size, and scalability needs. </span><span class="koboSpan" id="kobo.167.2">ML projects could be using data from different sources and different formats (e.g., </span><strong class="keyWord"><span class="koboSpan" id="kobo.168.1">CSV</span></strong><span class="koboSpan" id="kobo.169.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.170.1">Parquet</span></strong><span class="koboSpan" id="kobo.171.1">, JSON/XML, documents, or image/audio/video files). </span><span class="koboSpan" id="kobo.171.2">Determine whether the infrastructure can handle large data volumes efficiently when necessary and scale down to reduce costs during periods of low volume.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.172.1">Ingestion patterns</span></strong><span class="koboSpan" id="kobo.173.1">: Consider the different data ingestion patterns that need to be supported. </span><span class="koboSpan" id="kobo.173.2">The tool or combination of several tools should support both batch ingestion patterns (transferring bulk data at specific time intervals) and real-time streaming (processing data such as sensor data or website clickstreams in real time).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.174.1">Data preprocessing capability</span></strong><span class="koboSpan" id="kobo.175.1">: Evaluate whether the ingested data needs to be preprocessed before it is stored in the target data repository. </span><span class="koboSpan" id="kobo.175.2">Look for tools that offer built-in processing capability or seamless integration with external processing tools.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.176.1">Security</span></strong><span class="koboSpan" id="kobo.177.1">: Ensure that the selected tools provide robust security mechanisms for authentication and authorization to protect sensitive data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.178.1">Reliability</span></strong><span class="koboSpan" id="kobo.179.1">: Verify that the tools offer failure recovery mechanisms to prevent critical data loss during the ingestion process. </span><span class="koboSpan" id="kobo.179.2">If recovery capability is lacking, ensure there is an option to rerun ingestion jobs from the source.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.180.1">Support for different data sources and targets</span></strong><span class="koboSpan" id="kobo.181.1">: The chosen ingestion tools should be compatible with a wide range of data sources, including databases, files, and streaming sources. </span><span class="koboSpan" id="kobo.181.2">Additionally, they should provide an API for easy data ingestion.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.182.1">Manageability</span></strong><span class="koboSpan" id="kobo.183.1">: Another important factor to consider is the level of manageability. </span><span class="koboSpan" id="kobo.183.2">Does the tool require self-management, or is it a fully managed solution? </span><span class="koboSpan" id="kobo.183.3">Consider the trade-offs between cost and operational complexity before making a decision.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.184.1">AWS provides</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.185.1"> several services for data ingestion into a data lake on their platform. </span><span class="koboSpan" id="kobo.185.2">These services include Kinesis Data Streams, Kinesis Firehose, AWS Managed Streaming for Kafka, and AWS Glue Streaming, which cater to streaming data requirements. </span><span class="koboSpan" id="kobo.185.3">For batch ingestion, options such as AWS Glue, SFTP, and AWS </span><strong class="keyWord"><span class="koboSpan" id="kobo.186.1">Data Migration Service</span></strong><span class="koboSpan" id="kobo.187.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.188.1">DMS</span></strong><span class="koboSpan" id="kobo.189.1">) are available. </span><span class="koboSpan" id="kobo.189.2">In the upcoming section, we will delve into the usage of Kinesis Firehose and AWS Glue to manage data ingestion processes for data lakes. </span><span class="koboSpan" id="kobo.189.3">We will also discuss AWS Lambda, a serverless compute service, for a simple and lightweight data ingestion alternative. </span></p>
<h3 class="heading-3" id="_idParaDest-115"><span class="koboSpan" id="kobo.190.1">Kinesis Firehose</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.191.1">Kinesis Firehose is a</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.192.1"> service that streamlines the process of loading streaming data into a data lake. </span><span class="koboSpan" id="kobo.192.2">It is a fully managed solution, meaning you don’t have to worry about managing the underlying infrastructure. </span><span class="koboSpan" id="kobo.192.3">Instead, you can interact with the service’s API to handle the ingestion, processing, and delivery of your data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.193.1">Kinesis Firehose provides comprehensive support for various scalable data ingestion requirements, including:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.194.1">Seamless integration with diverse data sources such as websites, IoT devices, and video cameras. </span><span class="koboSpan" id="kobo.194.2">This is achieved using an ingestion agent or ingestion API.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.195.1">Versatility in delivering data to multiple destinations, including Amazon S3, Amazon Redshift (an AWS data warehouse service), Amazon OpenSearch (a managed search engine), and Splunk (a log aggregation and analysis product).</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.196.1">Seamless integration with AWS Lambda and Kinesis Data Analytics, offering advanced data processing capabilities. </span><span class="koboSpan" id="kobo.196.2">With AWS Lambda, you can leverage serverless computing to execute custom functions written in languages like Python, Java, Node.js, Go, C#, and Ruby. </span><span class="koboSpan" id="kobo.196.3">For more comprehensive information on the functionality of Lambda, please refer to the official AWS documentation.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.197.1">The following figure illustrates the data flow with Kinesis Firehose:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.198.1"><img alt="Figure 4.5 – Kinesis Firehose data flow " src="../Images/B20836_04_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.199.1">Figure 4.5: Kinesis Firehose data flow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.200.1">Kinesis </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.201.1">operates by establishing delivery streams, which are the foundational components in the Firehose architecture responsible for receiving streaming data from data producers. </span><span class="koboSpan" id="kobo.201.2">These delivery streams can be configured with various delivery destinations, such as S3 and Redshift. </span><span class="koboSpan" id="kobo.201.3">To accommodate the data volume generated by the producers, you can adjust the throughput of the data stream by specifying the number of shards. </span><span class="koboSpan" id="kobo.201.4">Each shard has the capacity to ingest 1 MB/sec of data and can support data reading at a rate of 2 MB/sec. </span><span class="koboSpan" id="kobo.201.5">Additionally, Kinesis Firehose offers APIs for increasing the number of shards and merging them when needed.</span></p>
<h3 class="heading-3" id="_idParaDest-116"><span class="koboSpan" id="kobo.202.1">AWS Glue</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.203.1">AWS Glue is a </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.204.1">comprehensive serverless ETL service that helps manage the data integration and ingestion process for data lakes. </span><span class="koboSpan" id="kobo.204.2">It seamlessly connects with various data sources, including transactional databases, data warehouses, and NoSQL databases, facilitating the movement of data to different destinations, such as Amazon S3. </span><span class="koboSpan" id="kobo.204.3">This movement can be scheduled or triggered by events. </span><span class="koboSpan" id="kobo.204.4">Additionally, AWS Glue offers the capability to process and transform data before delivering it to the target. </span><span class="koboSpan" id="kobo.204.5">It provides a range of processing options, such as the Python shell for executing Python scripts and Apache Spark for Spark-based data processing tasks. </span><span class="koboSpan" id="kobo.204.6">With AWS Glue, you can efficiently integrate and ingest data into your data lake, benefiting from its fully managed and serverless nature.</span></p>
<h3 class="heading-3" id="_idParaDest-117"><span class="koboSpan" id="kobo.205.1">AWS Lambda</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.206.1">AWS Lambda is</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.207.1"> AWS’s serverless computing platform. </span><span class="koboSpan" id="kobo.207.2">It seamlessly integrates with various AWS services, including Amazon S3. </span><span class="koboSpan" id="kobo.207.3">By leveraging Lambda, you can trigger the execution of functions in response to events, such as the creation of a new file in S3. </span><span class="koboSpan" id="kobo.207.4">These Lambda functions can be developed to move data from different sources, such as copying data from a source S3 bucket to a target landing bucket in a data lake.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.208.1">It’s important to note that AWS Lambda is not specifically designed for large-scale data movement or processing tasks, due to limitations such as memory size and maximum execution time allowed. </span><span class="koboSpan" id="kobo.208.2">However, for simpler data ingestion and processing jobs, it proves to be a highly efficient tool.</span></p>
<h2 class="heading-2" id="_idParaDest-118"><span class="koboSpan" id="kobo.209.1">Data cataloging</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.210.1">A </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.211.1">data catalog </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.212.1">plays a crucial role in enabling data analysts and scientists to discover and access data stored in a central data storage. </span><span class="koboSpan" id="kobo.212.2">It becomes particularly important during the data understanding and exploration phase of the ML lifecycle when scientists need to search and comprehend available data for their ML projects. </span><span class="koboSpan" id="kobo.212.3">When evaluating a data catalog tool, consider</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.213.1"> the following key factors:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.214.1">Metadata catalog</span></strong><span class="koboSpan" id="kobo.215.1">: The technology should support a central data catalog for effective management of data lake metadata. </span><span class="koboSpan" id="kobo.215.2">This involves handling metadata such as database names, table schemas, and table tags. </span><span class="koboSpan" id="kobo.215.3">The Hive metastore catalog is a popular standard for managing metadata catalogs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.216.1">Automated data cataloging</span></strong><span class="koboSpan" id="kobo.217.1">: The technology should have the capability to automatically discover and catalog datasets, as well as to infer data schemas from various data sources like Amazon S3, relational databases, NoSQL databases, and logs. </span><span class="koboSpan" id="kobo.217.2">Typically, this functionality is implemented through a crawler that scans data sources, identifies metadata elements (e.g., column names, data types), and adds them to the catalog.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.218.1">Tagging flexibility</span></strong><span class="koboSpan" id="kobo.219.1">: The technology should have the ability to assign custom attributes or tags to metadata entities like databases, tables, and fields. </span><span class="koboSpan" id="kobo.219.2">This flexibility supports enhanced data search and discovery capabilities within the catalog.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.220.1">Integration with other tools</span></strong><span class="koboSpan" id="kobo.221.1">: The technology should allow seamless integration of the data catalog with a wide range of data processing tools, enabling easy access to the underlying data. </span><span class="koboSpan" id="kobo.221.2">Additionally, native integration with data lake management platforms is advantageous.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.222.1">Search functionality</span></strong><span class="koboSpan" id="kobo.223.1">: The technology should have a robust search capability across diverse metadata attributes within the catalog. </span><span class="koboSpan" id="kobo.223.2">This includes searching by database, table, and field names, custom tags or descriptions, and data types.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.224.1">When it comes to</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.225.1"> building data catalogs, there are various technical options available. </span><span class="koboSpan" id="kobo.225.2">In this section, we first explore how AWS Glue can be utilized for data cataloging purposes. </span><span class="koboSpan" id="kobo.225.3">We will also discuss a </span><strong class="keyWord"><span class="koboSpan" id="kobo.226.1">Do-It-Yourself</span></strong><span class="koboSpan" id="kobo.227.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.228.1">DIY</span></strong><span class="koboSpan" id="kobo.229.1">) option for a data catalog using standard AWS services such as Lambda and OpenSearch. </span></p>
<h3 class="heading-3" id="_idParaDest-119"><span class="koboSpan" id="kobo.230.1">AWS Glue Data Catalog</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.231.1">AWS Glue offers a comprehensive solution for data cataloging, integrating seamlessly with AWS Lake Formation and other AWS services. </span><span class="koboSpan" id="kobo.231.2">The AWS Glue Data Catalog can be a drop-in replacement </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.232.1">for the Hive metastore catalog, so any Hive metastore-compatible applications can work with the AWS Glue Data Catalog. </span><span class="koboSpan" id="kobo.232.2">With AWS Glue, you can automatically discover, catalog, and organize your data assets, making them easily searchable and accessible to data analysts and scientists. </span><span class="koboSpan" id="kobo.232.3">Here are some key features and benefits of</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.233.1"> using AWS Glue for data cataloging:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.234.1">Automated data discovery</span></strong><span class="koboSpan" id="kobo.235.1">: AWS Glue provides automated data discovery capabilities. </span><span class="koboSpan" id="kobo.235.2">By using data crawlers, Glue can scan and analyze data from diverse structured and semi-structured sources such as Amazon S3, relational databases, NoSQL databases, and more. </span><span class="koboSpan" id="kobo.235.3">It identifies metadata information, including table schemas, column names, and data types, that is stored in the AWS Glue Data Catalog.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.236.1">Centralized metadata repository</span></strong><span class="koboSpan" id="kobo.237.1">: The AWS Glue Data Catalog serves as a centralized metadata repository for your data assets. </span><span class="koboSpan" id="kobo.237.2">It provides a unified view of your data, making it easier to search, query, and understand the available datasets.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.238.1">Metadata management</span></strong><span class="koboSpan" id="kobo.239.1">: AWS Glue allows you to manage and maintain metadata associated with your data assets. </span><span class="koboSpan" id="kobo.239.2">You can define custom tags, add descriptions, and organize your data using databases, tables, and partitions within the Data Catalog.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.240.1">The metadata hierarchy of the AWS Glue Data Catalog is organized using databases and tables. </span><span class="koboSpan" id="kobo.240.2">Databases </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.241.1">serve as containers for tables, which hold the actual data. </span><span class="koboSpan" id="kobo.241.2">Like traditional databases, a single database can house multiple tables, which can be sourced from various data stores. </span><span class="koboSpan" id="kobo.241.3">However, each table is exclusively associated with a single database. </span><span class="koboSpan" id="kobo.241.4">To query these databases and tables, one can utilize Hive metastore-compatible tools such as Amazon Athena to execute SQL queries. </span><span class="koboSpan" id="kobo.241.5">When collaborating with AWS Lake Formation, access permissions to the catalog’s databases and tables can be controlled through the Lake Formation entitlement layer.</span></p>
<h3 class="heading-3" id="_idParaDest-120"><span class="koboSpan" id="kobo.242.1">Custom data catalog solution</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.243.1">Another </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.244.1">option for building a data catalog is to create your own with a set of AWS services. </span><span class="koboSpan" id="kobo.244.2">Consider this option when you have specific requirements that are not met by the purpose-built products. </span><span class="koboSpan" id="kobo.244.3">The architecture for this DIY approach involves leveraging services like DynamoDB and Lambda, as depicted in the accompanying diagram:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.245.1"><img alt="         Comprehensive data catalog using AWS Lambda, DynamoDB,            and Amazon OpenSearch Service                " src="../Images/B20836_04_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.246.1">Figure 4.6: Custom data catalog solution</span></p>
<p class="normal"><span class="koboSpan" id="kobo.247.1">At a high level, AWS Lambda triggers are used to populate DynamoDB tables with object names and metadata when those objects are put into S3; Amazon OpenSearch Service is used to search for specific assets, related metadata, and data classifications.</span></p>
<h2 class="heading-2" id="_idParaDest-121"><span class="koboSpan" id="kobo.248.1">Data processing</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.249.1">The data processing</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.250.1"> functionality of a data lake encompasses the frameworks and compute resources necessary </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.251.1">for various data processing tasks, such as data correction, transformation, merging, splitting, and ML feature engineering. </span><span class="koboSpan" id="kobo.251.2">This component is a key step in the ML lifecycle as it helps prepare the data for downstream model training and inference steps. </span><span class="koboSpan" id="kobo.251.3">Common data processing frameworks include Python shell scripts using libraries such as pandas, NumPy, and Apache Spark. </span><span class="koboSpan" id="kobo.251.4">The essential</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.252.1"> requirements for data processing technology are as follows:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.253.1">Integration and compatibility with the underlying storage technology</span></strong><span class="koboSpan" id="kobo.254.1">: The ability to seamlessly work with the native storage system simplifies data access and movement between the storage and processing layers.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.255.1">Integration with the data catalog</span></strong><span class="koboSpan" id="kobo.256.1">: The capability to interact with the data catalog’s metastore to query databases and tables within the catalog.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.257.1">Scalability</span></strong><span class="koboSpan" id="kobo.258.1">: The capacity to scale compute resources up or down to accommodate changing data volumes and processing velocity requirements.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.259.1">Language and framework support</span></strong><span class="koboSpan" id="kobo.260.1">: Support for popular data processing libraries and frameworks, such as Python and Spark.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.261.1">Batch and real-time processing capabilities</span></strong><span class="koboSpan" id="kobo.262.1">: The capability to handle both real-time data streams and bulk data processing in batch mode.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.263.1">Now, let’s examine a selection of AWS services that offer data processing capabilities within a data lake architecture:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.264.1">AWS Glue ETL</span></strong><span class="koboSpan" id="kobo.265.1">: In </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.266.1">addition to supporting data movement and data catalogs, the ETL features of AWS Glue can be used for ETL and general-purpose data processing. </span><span class="koboSpan" id="kobo.266.2">AWS Glue ETL provides several built-in functions for data transformation, such as dropping the </span><code class="inlineCode"><span class="koboSpan" id="kobo.267.1">NULL</span></code><span class="koboSpan" id="kobo.268.1"> field (the </span><code class="inlineCode"><span class="koboSpan" id="kobo.269.1">NULL</span></code><span class="koboSpan" id="kobo.270.1"> field represents new data) and data filtering. </span><span class="koboSpan" id="kobo.270.2">It also provides general processing frameworks for Python and Spark to run Python scripts and Spark jobs. </span><span class="koboSpan" id="kobo.270.3">Glue ETL works natively with the AWS Glue Data Catalog to access the databases and tables in the catalog. </span><span class="koboSpan" id="kobo.270.4">Glue ETL can also access the Amazon S3 storage directly.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.271.1">Amazon Elastic MapReduce (EMR)</span></strong><span class="koboSpan" id="kobo.272.1">: </span><strong class="keyWord"><span class="koboSpan" id="kobo.273.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.274.1">EMR</span></strong><span class="koboSpan" id="kobo.275.1"> is a fully managed big data </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.276.1">processing platform on AWS. </span><span class="koboSpan" id="kobo.276.2">It is designed for large-scale data processing using the Spark framework and other Apache tools, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.277.1">Apache</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.278.1">Hive</span></strong><span class="koboSpan" id="kobo.279.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.280.1">Apache</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.281.1">Hudi</span></strong><span class="koboSpan" id="kobo.282.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.283.1">Presto</span></strong><span class="koboSpan" id="kobo.284.1">. </span><span class="koboSpan" id="kobo.284.2">It</span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.285.1"> integrates with the Glue Data Catalog </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.286.1">and Lake </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.287.1">Formation natively to access databases and tables in Lake Formation.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.288.1">AWS Lambda</span></strong><span class="koboSpan" id="kobo.289.1">: AWS Lambda </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.290.1">can be used for lightweight data processing tasks or as part of a larger data processing pipeline within the data lake architecture. </span><span class="koboSpan" id="kobo.290.2">Lambda can be triggered by real-time events, so it is a good option for real-time data processing.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.291.1">While efficient data processing prepares raw data for model training and consumption, robust data management must also ensure ML teams can track data provenance and access historical versions as needed through capabilities like data versioning.</span></p>
<h2 class="heading-2" id="_idParaDest-122"><span class="koboSpan" id="kobo.292.1">ML data versioning</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.293.1">To </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.294.1">establish a lineage for model training across training data and ML models, it is crucial to implement version control for the training, validation, and testing datasets. </span><span class="koboSpan" id="kobo.294.2">Data versioning control presents </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.295.1">challenges as it necessitates the use of appropriate tools and adherence to best practices by individuals. </span><span class="koboSpan" id="kobo.295.2">During the model building process, it is common for data scientists to obtain a copy of a dataset, perform cleansing and transformations specific to their needs, and save the modified data as a new version. </span><span class="koboSpan" id="kobo.295.3">This poses significant challenges in terms of data management, including duplication and establishing links between the data and its various upstream and downstream tasks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.296.1">Data versioning for the entire data lake is out of the scope of this book. </span><span class="koboSpan" id="kobo.296.2">Instead, we will focus on discussing a few architectural options specifically related to versioning control for training datasets.</span></p>
<h3 class="heading-3" id="_idParaDest-123"><span class="koboSpan" id="kobo.297.1">S3 partitions</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.298.1">In this </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.299.1">approach, each newly created or updated dataset is stored in a separate S3 partition with a unique prefix, typically derived from the name of the S3 folder. </span><span class="koboSpan" id="kobo.299.2">While this method can lead to data duplication, it offers a clear and simple approach to differentiate between different datasets intended for model training. </span><span class="koboSpan" id="kobo.299.3">To maintain data integrity, it is advisable to generate datasets through a controlled processing pipeline that enforces naming standards. </span><span class="koboSpan" id="kobo.299.4">The processing pipeline should also track data provenance and record the processing scripts used for data manipulation and feature engineering. </span><span class="koboSpan" id="kobo.299.5">Furthermore, the datasets should be configured as read-only for downstream applications, ensuring their immutability. </span><span class="koboSpan" id="kobo.299.6">The following example showcases an S3 partition structure, illustrating multiple versions of a training dataset:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.300.1">s3://project1/&lt;</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.301.1">date</span></span><span class="koboSpan" id="kobo.302.1">&gt;/&lt;unique version </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.303.1">id</span></span><span class="koboSpan" id="kobo.304.1"> 1&gt;/train_1.txt
s3://project1/&lt;</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.305.1">date</span></span><span class="koboSpan" id="kobo.306.1">&gt;/&lt;unique version </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.307.1">id</span></span><span class="koboSpan" id="kobo.308.1"> 1&gt;/train_2.txt
s3://project1/&lt;</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.309.1">date</span></span><span class="koboSpan" id="kobo.310.1">&gt;/&lt;unique version </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.311.1">id</span></span><span class="koboSpan" id="kobo.312.1"> 2&gt;/train_1.txt
s3://project1/&lt;</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.313.1">date</span></span><span class="koboSpan" id="kobo.314.1">&gt;/&lt;unique version </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.315.1">id</span></span><span class="koboSpan" id="kobo.316.1"> 2&gt;/train_2.txt
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.317.1">In this</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.318.1"> instance, the two versions of the dataset are segregated using distinct S3 prefixes. </span><span class="koboSpan" id="kobo.318.2">To effectively track these training files, it is recommended to employ a database for storing metadata pertaining to these training files. </span><span class="koboSpan" id="kobo.318.3">When utilizing these files, it is crucial to establish links between the training datasets, ML training jobs, ML training scripts, and the resulting ML models to establish a comprehensive lineage. </span></p>
<h3 class="heading-3" id="_idParaDest-124"><span class="koboSpan" id="kobo.319.1">Versioned S3 buckets</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.320.1">Amazon S3 offers</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.321.1"> versioning support for S3 buckets, which can be leveraged to manage different versions of training datasets when enabled. </span><span class="koboSpan" id="kobo.321.2">With this approach, each newly created or updated dataset is assigned a unique version ID at the S3 object level. </span><span class="koboSpan" id="kobo.321.3">Additionally, it is recommended to utilize a database to store all relevant metadata associated with each version of the training dataset. </span><span class="koboSpan" id="kobo.321.4">This enables the establishment of lineage, tracking the journey from data processing to ML model training. </span><span class="koboSpan" id="kobo.321.5">The metadata should capture essential information to facilitate comprehensive tracking and analysis.</span></p>
<h3 class="heading-3" id="_idParaDest-125"><span class="koboSpan" id="kobo.322.1">Purpose-built data version tools</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.323.1">Instead </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.324.1">of developing custom solutions for data version control, there are purpose-built tools available for efficient data version management. </span><span class="koboSpan" id="kobo.324.2">For example, these tools can be used to track and store different versions of ML training and validation datasets, which are important for repeatable experimentations and model training tasks. </span><span class="koboSpan" id="kobo.324.3">Here are a few notable options:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.325.1">Git LFS (Large File Storage)</span></strong><span class="koboSpan" id="kobo.326.1">: Git LFS </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.327.1">extends Git’s capabilities to handle large files, including datasets. </span><span class="koboSpan" id="kobo.327.2">It stores these files outside the Git repository while retaining versioning information. </span><span class="koboSpan" id="kobo.327.3">Git LFS seamlessly integrates with Git and is commonly used to version large files in data-centric projects.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.328.1">DataVersionControl (DVC)</span></strong><span class="koboSpan" id="kobo.329.1">: DVC is </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.330.1">an open-source tool designed specifically for data versioning and management. </span><span class="koboSpan" id="kobo.330.2">It integrates with Git and provides features for tracking and managing large datasets. </span><span class="koboSpan" id="kobo.330.3">DVC enables lightweight links to actual data files stored in remote storage, such as Amazon S3 or a shared file system. </span><span class="koboSpan" id="kobo.330.4">This approach maintains a history of changes and allows easy switching between different dataset versions, eliminating the need for data duplication.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.331.1">Pachyderm</span></strong><span class="koboSpan" id="kobo.332.1">: Pachyderm is</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.333.1"> an open-source data versioning and data lineage tool. </span><span class="koboSpan" id="kobo.333.2">It offers version control for data pipelines, enabling tracking of changes to data, code, and configuration files. </span><span class="koboSpan" id="kobo.333.3">Pachyderm supports distributed data processing frameworks like Apache Spark and provides features like reproducibility, data lineage, and data lineage-based branching.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.334.1">These</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.335.1"> purpose-built tools streamline the process of data versioning, ensuring efficient tracking and management of datasets.</span></p>
<h2 class="heading-2" id="_idParaDest-126"><span class="koboSpan" id="kobo.336.1">ML feature stores</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.337.1">In large </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.338.1">enterprises, it is beneficial to centrally manage common reusable ML features like curated customer profile data and standardized product sales data. </span><span class="koboSpan" id="kobo.338.2">This practice helps reduce the ML project lifecycle, particularly during the data understanding and data preparation stages. </span><span class="koboSpan" id="kobo.338.3">To achieve this, many organizations have built central ML feature stores, an architectural component for storing common reusable ML features, as part of the ML development architecture to meet the downstream model development, training, and model inference needs. </span><span class="koboSpan" id="kobo.338.4">Depending on the specific requirements, there are two main options for managing these reusable ML features.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.339.1">Firstly, you can build custom feature stores that fulfill the fundamental requirements of inserting and looking up organized features for ML model training. </span><span class="koboSpan" id="kobo.339.2">These custom feature stores can be tailored to meet the specific needs of the organization.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.340.1">Alternatively, you can opt for commercial-grade feature store products, such as Amazon SageMaker Feature Store, a ML service offered by AWS, which we will delve into in later chapters. </span><span class="koboSpan" id="kobo.340.2">It provides advanced capabilities such as online and offline functionality for training and inference, metadata tagging, feature versioning, and advanced search. </span><span class="koboSpan" id="kobo.340.3">These features enable efficient management and utilization of ML features in production-grade scenarios.</span></p>
<h2 class="heading-2" id="_idParaDest-127"><span class="koboSpan" id="kobo.341.1">Data serving for client consumption</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.342.1">The central </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.343.1">data management platform should offer various methods, such as APIs or Hive metastore-based approaches, to facilitate online access to the data for downstream tasks such as data discovery and model training. </span><span class="koboSpan" id="kobo.343.2">Additionally, it is important to consider data transfer tools that support the movement of data from the central data management platform to other data-consuming environments, catering to different data consumption patterns such as local access to the data in the consuming environment. </span><span class="koboSpan" id="kobo.343.3">It is advantageous to explore tools that either have built-in data serving capabilities or can be seamlessly integrated with external data serving tools, as building custom data serving features could be a challenging engineering undertaking.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">When supplying data to data science environments, there are multiple data serving patterns to consider. </span><span class="koboSpan" id="kobo.344.2">In the following discussion, we will explore two prominent data access patterns and their characteristics.</span></p>
<h3 class="heading-3" id="_idParaDest-128"><span class="koboSpan" id="kobo.345.1">Consumption via API</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.346.1">In this</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.347.1"> data serving pattern, consumption environments and applications have the capability to directly access data from the data lake. </span><span class="koboSpan" id="kobo.347.2">This can be achieved using Hive metastore-compliant tools or through direct access to S3, the underlying storage of the data lake. </span><span class="koboSpan" id="kobo.347.3">Amazon provides various services that facilitate this pattern, such as Amazon Athena, a powerful big data query tool, Amazon EMR, a robust big data processing tool, and Amazon Redshift Spectrum, a feature of Amazon Redshift.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.348.1">By leveraging these services, data lake data indexed in Glue catalogs can be queried without the need to make a separate copy of the data. </span><span class="koboSpan" id="kobo.348.2">This pattern is particularly suitable when only a subset of the data is required for downstream data processing tasks. </span><span class="koboSpan" id="kobo.348.3">It offers the advantage of avoiding data duplication while enabling efficient selection and processing of specific data subsets as part of the overall data workflow.</span></p>
<h3 class="heading-3" id="_idParaDest-129"><span class="koboSpan" id="kobo.349.1">Consumption via data copy</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.350.1">In this</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.351.1"> data serving pattern, a specific portion of the data stored in the data lake is replicated or copied to the storage of the consumption environment. </span><span class="koboSpan" id="kobo.351.2">This replication allows for tailored processing and consumption based on specific needs. </span><span class="koboSpan" id="kobo.351.3">For instance, the latest or most relevant data can be loaded into a data analytics environment such as Amazon Redshift. </span><span class="koboSpan" id="kobo.351.4">Similarly, it can be delivered to S3 buckets owned by a data science environment, enabling efficient access and utilization for data science tasks. </span><span class="koboSpan" id="kobo.351.5">By replicating the required data subsets, this pattern provides flexibility and optimized performance for different processing and consumption requirements in various environments.</span></p>
<h2 class="heading-2" id="_idParaDest-130"><span class="koboSpan" id="kobo.352.1">Special databases for ML </span></h2>
<p class="normal"><span class="koboSpan" id="kobo.353.1">Considering </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.354.1">emerging ML paradigms like graph neural networks and generative AI, specialized databases have been developed to cater to ML-specific tasks such as link prediction, cluster classification, and retrieval-augmented generation. </span><span class="koboSpan" id="kobo.354.2">In the following section, we will delve into two types of databases—vector databases and graph databases—and examine how they are utilized in ML tasks. </span><span class="koboSpan" id="kobo.354.3">We will explore their unique characteristics and applications in the context of ML.</span></p>
<h3 class="heading-3" id="_idParaDest-131"><span class="koboSpan" id="kobo.355.1">Vector databases</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.356.1">Vector databases, also</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.357.1"> known as vector similarity search engines or vector stores, are specialized databases designed to efficiently</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.358.1"> store, index, and query high-dimensional vectors. </span><span class="koboSpan" id="kobo.358.2">Examples of high-dimensional vectors include numerical vectors’ representation of images or text. </span><span class="koboSpan" id="kobo.358.3">These databases are particularly well suited for ML applications that rely on vector-based computations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.359.1">In ML, vectors are commonly used to represent data points, embeddings, or feature representations. </span><span class="koboSpan" id="kobo.359.2">These vectors capture essential information about the underlying data, enabling similarity search, clustering, classification, and other ML tasks. </span><span class="koboSpan" id="kobo.359.3">Vector databases provide powerful tools for handling these vector-based operations at scale.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.360.1">One of the key features of vector databases is their ability to perform fast similarity searches, allowing efficient retrieval of vectors that are most similar to a given query vector. </span><span class="koboSpan" id="kobo.360.2">This capability is essential in various ML use cases, such as recommender systems, content-based search, and anomaly detection.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.361.1">There are several vector database providers on the market, each offering its own unique features and </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.362.1">capabilities. </span><span class="koboSpan" id="kobo.362.2">Some of the prominent ones include:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.363.1">Facebook AI Similarity Search (FAISS)</span></strong><span class="koboSpan" id="kobo.364.1">: Developed by </span><strong class="keyWord"><span class="koboSpan" id="kobo.365.1">Facebook AI Research</span></strong><span class="koboSpan" id="kobo.366.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.367.1">FAIR</span></strong><span class="koboSpan" id="kobo.368.1">), FAISS is </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.369.1">an open-source library for efficient similarity search and clustering of dense vectors. </span><span class="koboSpan" id="kobo.369.2">It provides highly optimized algorithms and data structures for fast and scalable vector search.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.370.1">Milvus</span></strong><span class="koboSpan" id="kobo.371.1">: Milvus is </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.372.1">an open-source vector database designed for managing and serving large-scale vector datasets. </span><span class="koboSpan" id="kobo.372.2">It offers efficient similarity search, supports multiple similarity metrics, and provides scalability through distributed computing.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.373.1">Pinecone</span></strong><span class="koboSpan" id="kobo.374.1">: Pinecone </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.375.1">is a cloud-native vector database service that specializes in high-performance similarity search and recommendation systems. </span><span class="koboSpan" id="kobo.375.2">It offers real-time indexing and retrieval of vectors with low latency and high throughput.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.376.1">Elasticsearch</span></strong><span class="koboSpan" id="kobo.377.1">: Although</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.378.1"> primarily known as a full-text search and analytics engine, Elasticsearch also provides vector similarity search capabilities using plugins for efficient vector indexing and querying.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.379.1">Weaviate</span></strong><span class="koboSpan" id="kobo.380.1">: Weaviate is</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.381.1"> an open-source vector database. </span><span class="koboSpan" id="kobo.381.2">It allows you to store data objects and vector embeddings from your favorite ML models, and scale seamlessly into billions of data objects.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.382.1">These</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.383.1"> are just a few examples of vector database providers, and the landscape is continuously evolving with new solutions and advancements in the field. </span><span class="koboSpan" id="kobo.383.2">When choosing a vector database provider, it’s important to consider factors such as performance, scalability, ease of integration, and the specific requirements of your ML use case.</span></p>
<h3 class="heading-3" id="_idParaDest-132"><span class="koboSpan" id="kobo.384.1">Graph databases</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.385.1">Graph databases</span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.386.1"> are specialized databases designed to store, manage, and query graph-structured data. </span><span class="koboSpan" id="kobo.386.2">In a graph database, data is</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.387.1"> represented as nodes (entities) and edges (relationships) connecting these nodes, forming a graph-like structure. </span><span class="koboSpan" id="kobo.387.2">Graph databases excel at capturing and processing complex relationships and dependencies between entities, making them highly relevant for ML tasks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.388.1">Graph databases offer a powerful way to model and analyze data in domains where relationships play a crucial role, such as social networks, recommendation systems, fraud detection, knowledge graphs, and network analysis. </span><span class="koboSpan" id="kobo.388.2">They enable efficient traversal of the graph, allowing for queries that explore connections and patterns within the data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.389.1">In the </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.390.1">context of ML, graph databases have</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.391.1"> multiple applications. </span><span class="koboSpan" id="kobo.391.2">One key use case is graph-based feature engineering, where graphs are used to represent relationships between entities, and the graph structure is leveraged to derive features that can enhance the performance of ML models. </span><span class="koboSpan" id="kobo.391.3">For example, in a recommendation system, a graph database can represent user-item interactions and graph-based features can be derived to capture user similarities, item similarities, or collaborative filtering patterns.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.392.1">Graph databases also enable graph-based algorithms, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.393.1">graph convolutional networks</span></strong><span class="koboSpan" id="kobo.394.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.395.1">GCNs</span></strong><span class="koboSpan" id="kobo.396.1">), for</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.397.1"> tasks like node classification, link prediction, and graph clustering. </span><span class="koboSpan" id="kobo.397.2">These algorithms leverage the graph structure to propagate information across nodes and capture complex patterns in the data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.398.1">Furthermore, graph databases can be used to store and query graph embeddings, which are low-dimensional vector representations of nodes or edges. </span><span class="koboSpan" id="kobo.398.2">These embeddings capture the structural and semantic information of the graph and can be input to ML models for downstream tasks, such as node classification or recommendation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.399.1">Some of the notable</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.400.1"> graph databases include </span><strong class="keyWord"><span class="koboSpan" id="kobo.401.1">Neo4j</span></strong><span class="koboSpan" id="kobo.402.1">, a popular and widely used graph database that allows for efficient storage, retrieval, and querying of graph-structured data, and Amazon Neptune, a fully managed graph database service provided by AWS.</span></p>
<h2 class="heading-2" id="_idParaDest-133"><span class="koboSpan" id="kobo.403.1">Data pipelines</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.404.1">Data pipelines </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.405.1">streamline the flow of</span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.406.1"> data by automating tasks such as data ingestion, validation, transformation, and feature engineering. </span><span class="koboSpan" id="kobo.406.2">These pipelines ensure data quality and facilitate the creation of training and validation datasets for ML models. </span><span class="koboSpan" id="kobo.406.3">Numerous workflow tools are available for constructing data pipelines, and many data management tools offer built-in capabilities for building and managing these pipelines:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.407.1">AWS Glue workflows</span></strong><span class="koboSpan" id="kobo.408.1">: AWS Glue workflows</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.409.1"> provide </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.410.1">a native workflow management feature within AWS Glue, enabling the orchestration of various Glue jobs like data ingestion, processing, and feature engineering. </span><span class="koboSpan" id="kobo.410.2">Comprised of trigger and node components, a Glue workflow incorporates schedule triggers, event triggers, and on-demand triggers. </span><span class="koboSpan" id="kobo.410.3">Nodes within the workflow can be either crawler jobs or ETL jobs. </span><span class="koboSpan" id="kobo.410.4">Triggers initiate workflow runs, while event triggers are emitted after the completion of crawler or ETL jobs. </span><span class="koboSpan" id="kobo.410.5">By structuring a series of triggers and jobs, workflows facilitate the seamless execution of data pipelines within AWS Glue.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.411.1">AWS Step Functions</span></strong><span class="koboSpan" id="kobo.412.1">: AWS Step Functions</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.413.1"> is a powerful workflow orchestration tool that seamlessly integrates </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.414.1">with various AWS data processing services like AWS Glue and Amazon EMR. </span><span class="koboSpan" id="kobo.414.2">It enables the creation of robust workflows to execute diverse steps within a data pipeline, such as data ingestion, data processing, and feature engineering, ensuring smooth coordination and execution of these tasks.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.415.1">AWS Managed Workflows for Apache Airflow</span></strong><span class="koboSpan" id="kobo.416.1">: AWS </span><strong class="keyWord"><span class="koboSpan" id="kobo.417.1">Managed Workflows for Apache Airflow</span></strong><span class="koboSpan" id="kobo.418.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.419.1">MWAA</span></strong><span class="koboSpan" id="kobo.420.1">) is a fully managed service that</span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.421.1"> simplifies the deployment, configuration, and management of Apache Airflow, an open-source platform</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.422.1"> for orchestrating and scheduling data workflows. </span><span class="koboSpan" id="kobo.422.2">This service offers scalability, reliability, and easy integration with other AWS services, making it an efficient solution for managing complex data workflows in the cloud.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.423.1">Having explored</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.424.1"> the fundamental elements of ML data management architecture, the subsequent sections will delve into subjects related to security and governance.</span></p>
<h2 class="heading-2" id="_idParaDest-134"><span class="koboSpan" id="kobo.425.1">Authentication and authorization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.426.1">Authentication and </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.427.1">authorization are crucial for ensuring secure access to a data lake. </span><span class="koboSpan" id="kobo.427.2">Federated authentication, such</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.428.1"> as AWS </span><strong class="keyWord"><span class="koboSpan" id="kobo.429.1">Identity and Access Management</span></strong><span class="koboSpan" id="kobo.430.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.431.1">IAM</span></strong><span class="koboSpan" id="kobo.432.1">), verifies user identities for administration and data consumption purposes. </span><span class="koboSpan" id="kobo.432.2">AWS Lake Formation combines the built-in Lake Formation access control with AWS IAM to govern access to data catalog resources and underlying data storage. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.433.1">The built-in Lake Formation permission model utilizes commands like grant and revoke to control access to resources such as databases and tables, as well as actions like table creation. </span><span class="koboSpan" id="kobo.433.2">When a user requests access to a resource, both IAM policies and Lake Formation permissions are evaluated to verify and enforce access before granting it. </span><span class="koboSpan" id="kobo.433.3">This multi-layered approach enhances data lake security and governance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.434.1">There are </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.435.1">several personas involved in the </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.436.1">administration of the data lake and consumption of the data lake resources, including:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.437.1">Lake Formation administrator</span></strong><span class="koboSpan" id="kobo.438.1">: A </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.439.1">Lake Formation administrator has permission to manage all aspects of a Lake Formation data lake in an AWS account. </span><span class="koboSpan" id="kobo.439.2">Examples include granting/revoking permissions to access data lake resources for other users, registering data stores in S3, and creating/deleting databases. </span><span class="koboSpan" id="kobo.439.3">When setting up Lake Formation, you will need to register as an administrator. </span><span class="koboSpan" id="kobo.439.4">An administrator can be an AWS IAM user or IAM role. </span><span class="koboSpan" id="kobo.439.5">You can add more than one administrator to a Lake Formation data lake.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.440.1">Lake Formation database creator</span></strong><span class="koboSpan" id="kobo.441.1">: A Lake Formation database creator is granted </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.442.1">permission to create databases in Lake Formation. </span><span class="koboSpan" id="kobo.442.2">A database creator can be an IAM user or IAM role.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.443.1">Lake Formation database user</span></strong><span class="koboSpan" id="kobo.444.1">: A</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.445.1"> Lake Formation database user can be granted permission to perform different actions against a database. </span><span class="koboSpan" id="kobo.445.2">Example permissions include create table, drop table, describe table, and alter table. </span><span class="koboSpan" id="kobo.445.3">A database user can be an IAM user or IAM role.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.446.1">Lake Formation data user</span></strong><span class="koboSpan" id="kobo.447.1">: A </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.448.1">Lake Formation data user can be granted permission to perform different actions against database tables and columns. </span><span class="koboSpan" id="kobo.448.2">Example permissions include insert, select, describe, delete, alter, and drop. </span><span class="koboSpan" id="kobo.448.3">A data user can be an IAM user or an IAM role.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.449.1">Accessing and querying the database and tables in Lake Formation is facilitated through compatible AWS services like Amazon Athena and Amazon EMR. </span><span class="koboSpan" id="kobo.449.2">When performing queries using these services, Lake Formation verifies the principals (IAM users, groups, and roles) associated with them to ensure they have the necessary access permissions for the database, tables, and corresponding S3 data location. </span><span class="koboSpan" id="kobo.449.3">If access is granted, Lake Formation issues a temporary credential to the service, enabling it to execute the query securely and efficiently. </span><span class="koboSpan" id="kobo.449.4">This process ensures that only authorized services can interact with Lake Formation and perform queries on the data.</span></p>
<h2 class="heading-2" id="_idParaDest-135"><span class="koboSpan" id="kobo.450.1">Data governance</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.451.1">Having </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.452.1">secure access to trustworthy data is essential to the success of an ML initiative. </span><span class="koboSpan" id="kobo.452.2">Data governance encompasses essential practices to ensure the reliability, security, and accountability of data assets. </span><span class="koboSpan" id="kobo.452.3">Trustworthy data is achieved through the identification and documentation of data flows, as well as the measurement and reporting of data quality. </span><span class="koboSpan" id="kobo.452.4">Data protection and security involve classifying data and applying appropriate access permissions to safeguard its confidentiality and integrity. </span><span class="koboSpan" id="kobo.452.5">To maintain visibility of data activities, monitoring and auditing mechanisms should be implemented, allowing organizations to track and analyze actions performed on data, ensuring transparency and accountability in data management. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.453.1">A data catalog is one of the most important components of data governance. </span><span class="koboSpan" id="kobo.453.2">On AWS, the Glue Data Catalog is a fully managed service for data catalog management. </span><span class="koboSpan" id="kobo.453.3">You also have the option to build custom data catalogs using different foundational building blocks. </span><span class="koboSpan" id="kobo.453.4">For example, you can follow the reference architecture at </span><a href="https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html"><span class="url"><span class="koboSpan" id="kobo.454.1">https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html</span></span></a><span class="koboSpan" id="kobo.455.1"> for building a custom data catalog on AWS.</span></p>
<h3 class="heading-3" id="_idParaDest-136"><span class="koboSpan" id="kobo.456.1">Data lineage</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.457.1">To establish </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.458.1">and document data lineage during the ingestion and processing of data across different zones, it is important to capture specific data points. </span><span class="koboSpan" id="kobo.458.2">When utilizing data ingestion and processing tools like AWS Glue, AWS EMR, or AWS Lambda in a data pipeline, the following information can be captured to establish comprehensive data lineage:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.459.1">Data source details</span></strong><span class="koboSpan" id="kobo.460.1">: Include the name of the data source, its location, and ownership information to identify the origin of the data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.461.1">Data processing job history</span></strong><span class="koboSpan" id="kobo.462.1">: Capture the history and details of the data processing jobs involved in the pipeline. </span><span class="koboSpan" id="kobo.462.2">This includes information such as the job name, unique </span><strong class="keyWord"><span class="koboSpan" id="kobo.463.1">identifier</span></strong><span class="koboSpan" id="kobo.464.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.465.1">ID</span></strong><span class="koboSpan" id="kobo.466.1">), associated processing script, and owner of the job.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.467.1">Generated artifacts</span></strong><span class="koboSpan" id="kobo.468.1">: Document the artifacts generated because of the data processing jobs. </span><span class="koboSpan" id="kobo.468.2">For example, record the S3 URI or other storage location for the target data produced by the pipeline.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.469.1">Data metrics</span></strong><span class="koboSpan" id="kobo.470.1">: Track relevant metrics at different stages of data processing. </span><span class="koboSpan" id="kobo.470.2">This can include the number of records, data size, data schema, and feature statistics to provide insights into the processed data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.471.1">To store and </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.472.1">manage data lineage information and processing metrics, it is recommended to establish a central data operational data store. </span><span class="koboSpan" id="kobo.472.2">AWS DynamoDB, a fully managed NoSQL database, is an excellent technology choice for this purpose. </span><span class="koboSpan" id="kobo.472.3">With its capabilities optimized for low latency and high transaction access, DynamoDB provides efficient storage and retrieval of data lineage records and processing metrics. </span><span class="koboSpan" id="kobo.472.4">By capturing and documenting these data points, organizations can establish a comprehensive data lineage that provides a clear understanding of the data’s journey from its source through various processing stages. </span><span class="koboSpan" id="kobo.472.5">This documentation enables traceability, auditability, and better management of the data as it moves through the pipeline.</span></p>
<h3 class="heading-3" id="_idParaDest-137"><span class="koboSpan" id="kobo.473.1">Other data governance measures</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.474.1">In addition to managing</span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.475.1"> data lineage, there are several other important measures for effective data governance, including:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.476.1">Data quality</span></strong><span class="koboSpan" id="kobo.477.1">: Automated data quality checks should be implemented at different stages, and quality metrics should be reported. </span><span class="koboSpan" id="kobo.477.2">For example, after the source data is ingested into the landing zone, an AWS Glue quality check job can run to check the data quality using tools such as the open-source </span><code class="inlineCode"><span class="koboSpan" id="kobo.478.1">Deequ</span></code><span class="koboSpan" id="kobo.479.1"> library. </span><span class="koboSpan" id="kobo.479.2">Data quality metrics (such as counts, schema validation, missing data, the wrong data type, or statistical deviations from the baseline) and reports can be generated for reviews. </span><span class="koboSpan" id="kobo.479.3">Optionally, manual or automated operational data cleansing processes should be established to correct data quality issues.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.480.1">Data cataloging</span></strong><span class="koboSpan" id="kobo.481.1">: Create a central data catalog and run Glue crawlers on datasets in the data lake to automatically create an inventory of data and populate the central data catalog. </span><span class="koboSpan" id="kobo.481.2">Enrich the catalogs with additional metadata to track other information to support discovery and data audits, such as the business owner, data classification, and data refresh date. </span><span class="koboSpan" id="kobo.481.3">For ML workloads, data science teams also generate new datasets (for example, new ML features) from the existing datasets in the data lake for model training purposes. </span><span class="koboSpan" id="kobo.481.4">These datasets should also be registered and tracked in a data catalog, and different versions of the data should be retained and archived for audit purposes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.482.1">Data access provisioning</span></strong><span class="koboSpan" id="kobo.483.1">: A formal process should be established for requesting and granting access to datasets and Lake Formation databases and tables. </span><span class="koboSpan" id="kobo.483.2">An external ticketing system can be used to manage the workflow for requesting access and granting access.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.484.1">Monitoring and auditing</span></strong><span class="koboSpan" id="kobo.485.1">: Data access should be monitored, and access history should be maintained. </span><span class="koboSpan" id="kobo.485.2">Amazon S3 server access logging can be enabled </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.486.1">to track access to all S3 objects directly. </span><span class="koboSpan" id="kobo.486.2">AWS Lake Formation also records all accesses to Lake Formation datasets</span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.487.1"> in </span><strong class="keyWord"><span class="koboSpan" id="kobo.488.1">AWS</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.489.1">CloudTrail</span></strong><span class="koboSpan" id="kobo.490.1"> (AWS CloudTrail provides event history in an AWS account to enable governance, compliance, and operational auditing). </span><span class="koboSpan" id="kobo.490.2">With Lake Formation auditing, you can get details such as event source, event name, SQL queries, and data output location.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.491.1">By implementing these key data governance measures, organizations can establish a strong foundation for data management, security, and compliance, enabling them to maximize the value of their data assets while mitigating risks.</span></p>
<h1 class="heading-1" id="_idParaDest-138"><span class="koboSpan" id="kobo.492.1">Hands-on exercise – data management for ML</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.493.1">In this hands-on exercise, you will go through the process of constructing a simple data management </span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.494.1">platform for a fictional retail bank. </span><span class="koboSpan" id="kobo.494.2">This platform will serve as the foundation for an ML workflow, and we will leverage different AWS technologies to build it. </span><span class="koboSpan" id="kobo.494.3">If you don’t have an AWS account, you can easily create one by following the instructions at </span><a href="https://aws.amazon.com/console/"><span class="url"><span class="koboSpan" id="kobo.495.1">https://aws.amazon.com/console/</span></span></a><span class="koboSpan" id="kobo.496.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.497.1">The data management platform we create will have the following key components:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.498.1">A data lake environment for data management using Lake Formation</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.499.1">A data ingestion component for ingesting files to the data lake using Lambda</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.500.1">A data catalog component using the Glue Data Catalog</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.501.1">A data discovery and query component using the Glue Data Catalog and Athena</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.502.1">A data processing component using Glue ETL</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.503.1">A data pipeline component using a Glue pipeline</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.504.1">The following diagram shows the data management architecture we will build in this exercise:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.505.1"><img alt="Figure 4.6 – Data management architecture for the hands-on exercise " src="../Images/B20836_04_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.506.1">Figure 4.7: Data management architecture for the hands-on exercise</span></p>
<p class="normal"><span class="koboSpan" id="kobo.507.1">Let’s get started </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.508.1">with building out this architecture on AWS.</span></p>
<h2 class="heading-2" id="_idParaDest-139"><span class="koboSpan" id="kobo.509.1">Creating a data lake using Lake Formation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.510.1">We will build the </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.511.1">data lake architecture using AWS Lake Formation; it is the primary service for building data lakes on AWS. </span><span class="koboSpan" id="kobo.511.2">After you log on to the </span><strong class="keyWord"><span class="koboSpan" id="kobo.512.1">AWS</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.513.1">Management Console</span></strong><span class="koboSpan" id="kobo.514.1">, create an S3 bucket called </span><code class="inlineCode"><span class="koboSpan" id="kobo.515.1">MLSA-DataLake-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.516.1">. </span><span class="koboSpan" id="kobo.516.2">We will use this bucket as the storage for the data lake. </span><span class="koboSpan" id="kobo.516.3">If you get a message that the bucket name is already in use, try adding some random characters to the name to make it unique. </span><span class="koboSpan" id="kobo.516.4">If you are not familiar with how to create S3 buckets, follow the instructions at the following link: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html"><span class="url"><span class="koboSpan" id="kobo.517.1">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.518.1">After the bucket is created, follow these steps to get started with creating a data lake:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.519.1">Register Lake Formation administrators</span></strong><span class="koboSpan" id="kobo.520.1">: We need to add Lake Formation administrators to the data lake. </span><span class="koboSpan" id="kobo.520.2">The administrators will have full permission to manage all aspects of the data lake. </span><span class="koboSpan" id="kobo.520.3">To do this, navigate to the Lake Formation management console, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.521.1">Administrative roles and tasks</span></strong><span class="koboSpan" id="kobo.522.1"> link, and you should be prompted to add an administrator. </span><span class="koboSpan" id="kobo.522.2">Select </span><strong class="screenText"><span class="koboSpan" id="kobo.523.1">Add myself</span></strong><span class="koboSpan" id="kobo.524.1"> and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.525.1">Get started</span></strong><span class="koboSpan" id="kobo.526.1"> button. </span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.527.1">Register S3 storage</span></strong><span class="koboSpan" id="kobo.528.1">: Next, we need to register the S3 bucket (</span><code class="inlineCode"><span class="koboSpan" id="kobo.529.1">MLSA-DataLake-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.530.1">) you created earlier in Lake Formation, so it will be managed and accessible through Lake Formation. </span><span class="koboSpan" id="kobo.530.2">To do this, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.531.1">Dashboard</span></strong><span class="koboSpan" id="kobo.532.1"> link, expand </span><strong class="screenText"><span class="koboSpan" id="kobo.533.1">Data lake setup</span></strong><span class="koboSpan" id="kobo.534.1">, and then click on </span><strong class="screenText"><span class="koboSpan" id="kobo.535.1">Register Location</span></strong><span class="koboSpan" id="kobo.536.1">. </span><span class="koboSpan" id="kobo.536.2">Browse and select the bucket you created and click on </span><strong class="screenText"><span class="koboSpan" id="kobo.537.1">Register Location</span></strong><span class="koboSpan" id="kobo.538.1">. </span><span class="koboSpan" id="kobo.538.2">This S3 bucket will be used by Lake Formation to store data for the databases and manage its access permissions.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.539.1">Create database</span></strong><span class="koboSpan" id="kobo.540.1">: Now, we are ready to set up a database called </span><code class="inlineCode"><span class="koboSpan" id="kobo.541.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.542.1"> for managing retail customers. </span><span class="koboSpan" id="kobo.542.2">Before we register the database, let’s first create a folder called the </span><code class="inlineCode"><span class="koboSpan" id="kobo.543.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.544.1"> folder under the </span><code class="inlineCode"><span class="koboSpan" id="kobo.545.1">MLSA-DataLake-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.546.1"> bucket. </span><span class="koboSpan" id="kobo.546.2">This folder will be used to store data files associated with the database. </span><span class="koboSpan" id="kobo.546.3">To do this, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.547.1">Create database</span></strong><span class="koboSpan" id="kobo.548.1"> button on the Lake Formation dashboard and follow the instructions on the screen to create the database.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.549.1">You have now</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.550.1"> successfully created a data lake powered by Lake Formation and created a database for data management. </span><span class="koboSpan" id="kobo.550.2">With this data lake created, we are now ready to build additional data management components. </span><span class="koboSpan" id="kobo.550.3">Next, we will create a data ingestion pipeline to move files into the data lake.</span></p>
<h2 class="heading-2" id="_idParaDest-140"><span class="koboSpan" id="kobo.551.1">Creating a data ingestion pipeline</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.552.1">Now</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.553.1"> that the database is prepared, we can proceed to ingest data into this newly created database. </span><span class="koboSpan" id="kobo.553.2">As mentioned earlier, there are various data sources available, including databases like Amazon RDS, streaming platforms like social media feeds, and logs such as CloudTrail. </span><span class="koboSpan" id="kobo.553.3">Additionally, AWS offers a range of services for building data ingestion pipelines, such as AWS Glue, Amazon Kinesis, and AWS Lambda. </span><span class="koboSpan" id="kobo.553.4">In this phase of the exercise, we will focus on creating an AWS Lambda function job that will facilitate the ingestion of data from other S3 buckets into our target database. </span><span class="koboSpan" id="kobo.553.5">As mentioned earlier, Lambda functions can be used for lightweight data ingestion and processing tasks:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.554.1">Create a source S3 bucket and download data files</span></strong><span class="koboSpan" id="kobo.555.1">: Let’s create another S3 bucket, called </span><code class="inlineCode"><span class="koboSpan" id="kobo.556.1">customer-data-source</span></code><span class="koboSpan" id="kobo.557.1">, to represent the data source where we will ingest the data from.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.558.1">Create a Lambda function</span></strong><span class="koboSpan" id="kobo.559.1">: Now, we will create the Lambda function that ingests </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.560.1">data from the </span><code class="inlineCode"><span class="koboSpan" id="kobo.561.1">customer-data-source</span></code><span class="koboSpan" id="kobo.562.1"> bucket to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.563.1">MLSA-DataLake-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.564.1"> bucket:</span><ol class="romanList" style="list-style-type: lower-roman;">
<li class="romanList" value="1"><span class="koboSpan" id="kobo.565.1">To get started, navigate to the AWS Lambda management console, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.566.1">Functions</span></strong><span class="koboSpan" id="kobo.567.1"> link in the left pane, and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.568.1">Create Function</span></strong><span class="koboSpan" id="kobo.569.1"> button in the right pane. </span><span class="koboSpan" id="kobo.569.2">Choose </span><strong class="screenText"><span class="koboSpan" id="kobo.570.1">Author from scratch</span></strong><span class="koboSpan" id="kobo.571.1">, then enter </span><code class="inlineCode"><span class="koboSpan" id="kobo.572.1">datalake-s3-ingest</span></code><span class="koboSpan" id="kobo.573.1"> for the function name, and select the latest Python version (e.g., 3.10) as the runtime. </span><span class="koboSpan" id="kobo.573.2">Keep the default for the execution role, which will create a new IAM role for this Lambda function. </span><span class="koboSpan" id="kobo.573.3">Click on </span><strong class="screenText"><span class="koboSpan" id="kobo.574.1">Create function</span></strong><span class="koboSpan" id="kobo.575.1"> to continue. </span></li>
<li class="romanList"><span class="koboSpan" id="kobo.576.1">On the next screen, click on </span><strong class="screenText"><span class="koboSpan" id="kobo.577.1">Add trigger</span></strong><span class="koboSpan" id="kobo.578.1">, select </span><strong class="screenText"><span class="koboSpan" id="kobo.579.1">S3</span></strong><span class="koboSpan" id="kobo.580.1"> as the trigger, and select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.581.1">customer-data-source</span></code><span class="koboSpan" id="kobo.582.1"> bucket as the source. </span><span class="koboSpan" id="kobo.582.2">For </span><strong class="screenText"><span class="koboSpan" id="kobo.583.1">Event Type</span></strong><span class="koboSpan" id="kobo.584.1">, choose the </span><strong class="screenText"><span class="koboSpan" id="kobo.585.1">Put</span></strong><span class="koboSpan" id="kobo.586.1"> event and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.587.1">Add</span></strong><span class="koboSpan" id="kobo.588.1"> button to complete the step. </span><span class="koboSpan" id="kobo.588.2">This trigger will allow the Lambda function to be invoked when there is an S3 bucket event, such as saving a file into the bucket.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.589.1">After you add the trigger, you will be brought back to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.590.1">Lambda-&gt;function-&gt; datalake-s3-ingest</span></code><span class="koboSpan" id="kobo.591.1"> screen. </span><span class="koboSpan" id="kobo.591.2">Next, let’s create the function by replacing the default function template with the following code block. </span><span class="koboSpan" id="kobo.591.3">Replace the </span><code class="inlineCode"><span class="koboSpan" id="kobo.592.1">desBucket</span></code><span class="koboSpan" id="kobo.593.1"> variable with the name of the actual bucket:
            </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.594.1">import</span></span><span class="koboSpan" id="kobo.595.1"> json
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.596.1">import</span></span><span class="koboSpan" id="kobo.597.1"> boto3
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.598.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.599.1">lambda_handler</span></span><span class="koboSpan" id="kobo.600.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.601.1">event, context</span></span><span class="koboSpan" id="kobo.602.1">):
     s3 = boto3.resource(</span><span class="hljs-string"><span class="koboSpan" id="kobo.603.1">'s3'</span></span><span class="koboSpan" id="kobo.604.1">)
     </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.605.1">for</span></span><span class="koboSpan" id="kobo.606.1"> record </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.607.1">in</span></span><span class="koboSpan" id="kobo.608.1"> event[</span><span class="hljs-string"><span class="koboSpan" id="kobo.609.1">'Records'</span></span><span class="koboSpan" id="kobo.610.1">]:
           srcBucket = record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.611.1">'s3'</span></span><span class="koboSpan" id="kobo.612.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.613.1">'bucket'</span></span><span class="koboSpan" id="kobo.614.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.615.1">'name'</span></span><span class="koboSpan" id="kobo.616.1">]
           srckey = record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.617.1">'s3'</span></span><span class="koboSpan" id="kobo.618.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.619.1">'object'</span></span><span class="koboSpan" id="kobo.620.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.621.1">'key'</span></span><span class="koboSpan" id="kobo.622.1">]
           desBucket = </span><span class="hljs-string"><span class="koboSpan" id="kobo.623.1">"MLSA-DataLake-&lt;your initials&gt;"</span></span><span class="koboSpan" id="kobo.624.1">
           desFolder = srckey[</span><span class="hljs-number"><span class="koboSpan" id="kobo.625.1">0</span></span><span class="koboSpan" id="kobo.626.1">:srckey.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.627.1">'.'</span></span><span class="koboSpan" id="kobo.628.1">)]
           desKey = </span><span class="hljs-string"><span class="koboSpan" id="kobo.629.1">"bank_customer_db/"</span></span><span class="koboSpan" id="kobo.630.1"> + desFolder + </span><span class="hljs-string"><span class="koboSpan" id="kobo.631.1">"/"</span></span><span class="koboSpan" id="kobo.632.1"> + srckey
           source= { </span><span class="hljs-string"><span class="koboSpan" id="kobo.633.1">'Bucket'</span></span><span class="koboSpan" id="kobo.634.1"> : srcBucket,</span><span class="hljs-string"><span class="koboSpan" id="kobo.635.1">'Key'</span></span><span class="koboSpan" id="kobo.636.1">:srckey}
           dest ={ </span><span class="hljs-string"><span class="koboSpan" id="kobo.637.1">'Bucket'</span></span><span class="koboSpan" id="kobo.638.1"> : desBucket,</span><span class="hljs-string"><span class="koboSpan" id="kobo.639.1">'Key'</span></span><span class="koboSpan" id="kobo.640.1">:desKey}
           s3.meta.client.copy(source, desBucket, desKey)
     </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.641.1">return</span></span><span class="koboSpan" id="kobo.642.1"> {
           </span><span class="hljs-string"><span class="koboSpan" id="kobo.643.1">'statusCode'</span></span><span class="koboSpan" id="kobo.644.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.645.1">200</span></span><span class="koboSpan" id="kobo.646.1">,
           </span><span class="hljs-string"><span class="koboSpan" id="kobo.647.1">'body'</span></span><span class="koboSpan" id="kobo.648.1">: json.dumps(</span><span class="hljs-string"><span class="koboSpan" id="kobo.649.1">'files ingested'</span></span><span class="koboSpan" id="kobo.650.1">)
     }
</span></code></pre>
</li>
<li class="romanList"><span class="koboSpan" id="kobo.651.1">The </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.652.1">new function will also need S3 permission to copy files (</span><em class="italic"><span class="koboSpan" id="kobo.653.1">objects</span></em><span class="koboSpan" id="kobo.654.1">) from one bucket to another. </span><span class="koboSpan" id="kobo.654.2">For simplicity, just add the </span><code class="inlineCode"><span class="koboSpan" id="kobo.655.1">AmazonS3FullAccess</span></code><span class="koboSpan" id="kobo.656.1"> policy to the </span><strong class="screenText"><span class="koboSpan" id="kobo.657.1">execution IAM role</span></strong><span class="koboSpan" id="kobo.658.1"> associated with the function. </span><span class="koboSpan" id="kobo.658.2">You can find the IAM role by clicking on the </span><strong class="screenText"><span class="koboSpan" id="kobo.659.1">Permission</span></strong><span class="koboSpan" id="kobo.660.1"> tab for the Lambda function.</span></li>
</ol>
</li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.661.1">Trigger data ingestion</span></strong><span class="koboSpan" id="kobo.662.1">: Now, download the sample data files from the following link: </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip"><span class="url"><span class="koboSpan" id="kobo.663.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip</span></span></a></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.664.1">Then, save the file to your local machine. </span><span class="koboSpan" id="kobo.664.2">Extract the archived files. </span><span class="koboSpan" id="kobo.664.3">There should be two files (</span><code class="inlineCode"><span class="koboSpan" id="kobo.665.1">customer_data.csv</span></code><span class="koboSpan" id="kobo.666.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.667.1">churn_list.csv</span></code><span class="koboSpan" id="kobo.668.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.669.1">You can now trigger the data ingestion process by uploading the </span><code class="inlineCode"><span class="koboSpan" id="kobo.670.1">customer_detail.csv</span></code><span class="koboSpan" id="kobo.671.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.672.1">churn_list.csv</span></code><span class="koboSpan" id="kobo.673.1"> files to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.674.1">customer-data-source</span></code><span class="koboSpan" id="kobo.675.1"> bucket and verify the process completion by checking the </span><code class="inlineCode"><span class="koboSpan" id="kobo.676.1">MLSA-DataLake-&lt;your initials&gt;/bank_customer_db</span></code><span class="koboSpan" id="kobo.677.1"> folder for the two files.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.678.1">You have now successfully created an AWS Lambda-based data ingestion pipeline to automatically move data from a source S3 bucket to a target S3 bucket. </span><span class="koboSpan" id="kobo.678.2">With this simple ingestion pipeline created and data moved, we are now ready to implement components to support the discovery of these data files. </span><span class="koboSpan" id="kobo.678.3">Next, let’s create an AWS Glue Data Catalog using the Glue crawler.</span></p>
<h2 class="heading-2" id="_idParaDest-141"><span class="koboSpan" id="kobo.679.1">Creating a Glue Data Catalog</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.680.1">To allow</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.681.1"> discovery and querying of the data in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.682.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.683.1"> database, we need to create a data catalog. </span><span class="koboSpan" id="kobo.683.2">As discussed earlier, Glue Data Catalog is a managed data catalog on AWS. </span><span class="koboSpan" id="kobo.683.3">It comes with a utility called an AWS Glue crawler that can help discover data and populate the catalog.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.684.1"> Here, we will use an AWS Glue crawler to crawl the files in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.685.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.686.1"> S3 folder and generate the catalog:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.687.1">Grant permission for Glue</span></strong><span class="koboSpan" id="kobo.688.1">:</span><ol class="romanList" style="list-style-type: lower-roman;">
<li class="romanList" value="1"><span class="koboSpan" id="kobo.689.1">First, let’s </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.690.1">grant permission for AWS Glue to access the </span><code class="inlineCode"><span class="koboSpan" id="kobo.691.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.692.1"> database. </span><span class="koboSpan" id="kobo.692.2">We will create a new IAM role for the Glue service to assume on your behalf. </span><span class="koboSpan" id="kobo.692.3">To do this, create a new IAM service role called </span><code class="inlineCode"><span class="koboSpan" id="kobo.693.1">AWSGlueServiceRole_data_lake</span></code><span class="koboSpan" id="kobo.694.1">, and attach the </span><code class="inlineCode"><span class="koboSpan" id="kobo.695.1">AWSGlueServiceRole</span></code><span class="koboSpan" id="kobo.696.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.697.1">AmazonS3FullAccess</span></code><span class="koboSpan" id="kobo.698.1"> IAM-managed policies to it. </span><span class="koboSpan" id="kobo.698.2">Make sure you select </span><strong class="screenText"><span class="koboSpan" id="kobo.699.1">Glue</span></strong><span class="koboSpan" id="kobo.700.1"> as the service when you create the role. </span><span class="koboSpan" id="kobo.700.2">If you are not familiar with how to create a role and attach a policy, follow the instructions at the following link: </span><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide"><span class="url"><span class="koboSpan" id="kobo.701.1">https://docs.aws.amazon.com/IAM/latest/UserGuide</span></span></a></li>
<li class="romanList"><span class="koboSpan" id="kobo.702.1">After the role is created, click on </span><strong class="screenText"><span class="koboSpan" id="kobo.703.1">Data lake permission</span></strong><span class="koboSpan" id="kobo.704.1"> in the left pane of the Lake Formation management console and then click the </span><strong class="screenText"><span class="koboSpan" id="kobo.705.1">Grant</span></strong><span class="koboSpan" id="kobo.706.1"> button in the right pane.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.707.1">On the next screen, select </span><code class="inlineCode"><span class="koboSpan" id="kobo.708.1">AWSGlueServiceRole_data_lake</span></code><span class="koboSpan" id="kobo.709.1"> for </span><strong class="screenText"><span class="koboSpan" id="kobo.710.1">IAM users and role</span></strong><strong class="keyWord"> </strong><span class="koboSpan" id="kobo.711.1">and </span><code class="inlineCode"><span class="koboSpan" id="kobo.712.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.713.1"> under </span><strong class="screenText"><span class="koboSpan" id="kobo.714.1">Named data catalog resources</span></strong><span class="koboSpan" id="kobo.715.1">, choose </span><strong class="screenText"><span class="koboSpan" id="kobo.716.1">Super</span></strong><span class="koboSpan" id="kobo.717.1"> for both </span><strong class="screenText"><span class="koboSpan" id="kobo.718.1">Database permissions</span></strong><span class="koboSpan" id="kobo.719.1"> and </span><strong class="screenText"><span class="koboSpan" id="kobo.720.1">Grantable permissions</span></strong><span class="koboSpan" id="kobo.721.1">, and finally click on </span><strong class="screenText"><span class="koboSpan" id="kobo.722.1">Grant</span></strong><span class="koboSpan" id="kobo.723.1">. </span><span class="koboSpan" id="kobo.723.2">The </span><strong class="screenText"><span class="koboSpan" id="kobo.724.1">Super</span></strong><span class="koboSpan" id="kobo.725.1"> permission allows the service role to have access to create databases and grant permission as part of the automation. </span><code class="inlineCode"><span class="koboSpan" id="kobo.726.1">AWSGlueServiceRole_data_lake</span></code><span class="koboSpan" id="kobo.727.1"> will be used later to configure the Glue crawler job.</span></p> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2"><strong class="keyWord"><span class="koboSpan" id="kobo.728.1">Configure the Glue crawler job</span></strong><span class="koboSpan" id="kobo.729.1">:</span><ol class="romanList" style="list-style-type: lower-roman;">
<li class="romanList" value="1"><span class="koboSpan" id="kobo.730.1">Launch the Glue crawler by clicking on the </span><strong class="screenText"><span class="koboSpan" id="kobo.731.1">Crawler</span></strong><span class="koboSpan" id="kobo.732.1"> link in the Lake Formation management console. </span><span class="koboSpan" id="kobo.732.2">A new browser tab for Glue will open. </span><span class="koboSpan" id="kobo.732.3">Click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.733.1">Create Crawler</span></strong><span class="koboSpan" id="kobo.734.1"> button to get started. </span><span class="koboSpan" id="kobo.734.2">Enter </span><code class="inlineCode"><span class="koboSpan" id="kobo.735.1">bank_customer_db_crawler</span></code><span class="koboSpan" id="kobo.736.1"> as the name of the crawler. </span><span class="koboSpan" id="kobo.736.2">Click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.737.1">Add a data source</span></strong><span class="koboSpan" id="kobo.738.1"> button, select </span><strong class="screenText"><span class="koboSpan" id="kobo.739.1">S3</span></strong><span class="koboSpan" id="kobo.740.1">, and enter </span><code class="inlineCode"><span class="koboSpan" id="kobo.741.1">s3://MLSA-DataLake-&lt;your initials&gt;/bank_customer_db/churn_list/</span></code><span class="koboSpan" id="kobo.742.1"> for the </span><strong class="screenText"><span class="koboSpan" id="kobo.743.1">include path</span></strong><span class="koboSpan" id="kobo.744.1"> field.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.745.1">Click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.746.1">Add another data source</span></strong><span class="koboSpan" id="kobo.747.1"> button again. </span><span class="koboSpan" id="kobo.747.2">This time, enter </span><code class="inlineCode"><span class="koboSpan" id="kobo.748.1">s3://MLSA-DataLake-&lt;your initials&gt;/bank_customer_db/customer_data/</span></code><span class="koboSpan" id="kobo.749.1">.</span></li>
</ol>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.750.1">On the next screen, </span><strong class="screenText"><span class="koboSpan" id="kobo.751.1">Configure security settings</span></strong><span class="koboSpan" id="kobo.752.1">, select </span><code class="inlineCode"><span class="koboSpan" id="kobo.753.1">AWSGlueServiceRole_data_lake</span></code><span class="koboSpan" id="kobo.754.1"> for the existing IAM role, which you used earlier:</span><ol class="romanList" style="list-style-type: lower-roman;">
<li class="romanList" value="1"><span class="koboSpan" id="kobo.755.1">On the next </span><strong class="screenText"><span class="koboSpan" id="kobo.756.1">Set output and scheduling</span></strong><span class="koboSpan" id="kobo.757.1"> screen, select </span><code class="inlineCode"><span class="koboSpan" id="kobo.758.1">bank_customer_db</span></code><span class="koboSpan" id="kobo.759.1"> as the target database, and choose </span><strong class="screenText"><span class="koboSpan" id="kobo.760.1">on demand</span></strong><span class="koboSpan" id="kobo.761.1"> as the frequency for the crawler schedule. </span></li>
<li class="romanList"><span class="koboSpan" id="kobo.762.1">On the next </span><strong class="screenText"><span class="koboSpan" id="kobo.763.1">Review and create</span></strong><span class="koboSpan" id="kobo.764.1"> screen, select </span><strong class="screenText"><span class="koboSpan" id="kobo.765.1">Finish</span></strong><span class="koboSpan" id="kobo.766.1"> on the final screen to complete the setup.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.767.1">On the </span><strong class="screenText"><span class="koboSpan" id="kobo.768.1">Crawler</span></strong><span class="koboSpan" id="kobo.769.1"> screen, select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.770.1">bank_customer_db_crawler</span></code><span class="koboSpan" id="kobo.771.1"> job you just created, click on </span><strong class="screenText"><span class="koboSpan" id="kobo.772.1">Run crawler</span></strong><span class="koboSpan" id="kobo.773.1">, and wait for the status to say </span><strong class="screenText"><span class="koboSpan" id="kobo.774.1">Ready</span></strong><span class="koboSpan" id="kobo.775.1">.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.776.1">Navigate back to the Lake Formation management console and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.777.1">Tables</span></strong><span class="koboSpan" id="kobo.778.1"> link. </span><span class="koboSpan" id="kobo.778.2">You will now see two new tables created (</span><code class="inlineCode"><span class="koboSpan" id="kobo.779.1">churn_list</span></code><span class="koboSpan" id="kobo.780.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.781.1">customer_data</span></code><span class="koboSpan" id="kobo.782.1">).</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.783.1">You have now successfully configured an AWS Glue crawler that automatically discovers table schemas from data files and creates data catalogs for the new data.</span></li>
</ol>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.784.1">You have </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.785.1">successfully created the Glue Data Catalog for the newly ingested data. </span><span class="koboSpan" id="kobo.785.2">With that, we now have the proper component to support data discovery and query. </span><span class="koboSpan" id="kobo.785.3">Next, we will use Lake Formation and Athena to discover and query the data in the data lake.</span></p>
<h2 class="heading-2" id="_idParaDest-142"><span class="koboSpan" id="kobo.786.1">Discovering and querying data in the data lake</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.787.1">To facilitate the</span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.788.1"> data discovery and data understanding phase of the ML workflow, it is essential to incorporate data discovery and data query capabilities within the data lake.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.789.1">By default, Lake Formation already provides a list of tags, such as data type classification (for example, CSV), for searching tables in the database. </span><span class="koboSpan" id="kobo.789.2">Let’s add a few more tags for each table to make it more discoverable:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.790.1">Grant </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.791.1">permission to edit the database tables by granting your current user ID </span><strong class="screenText"><span class="koboSpan" id="kobo.792.1">Super</span></strong><span class="koboSpan" id="kobo.793.1"> permission for both the </span><code class="inlineCode"><span class="koboSpan" id="kobo.794.1">customer_data</span></code><span class="koboSpan" id="kobo.795.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.796.1">churn_list</span></code><span class="koboSpan" id="kobo.797.1"> tables.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.798.1">Let’s add some metadata to the table fields. </span><span class="koboSpan" id="kobo.798.2">Select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.799.1">customer_data</span></code><span class="koboSpan" id="kobo.800.1"> table, click on </span><strong class="screenText"><span class="koboSpan" id="kobo.801.1">Edit Schema</span></strong><span class="koboSpan" id="kobo.802.1">, select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.803.1">creditscore</span></code><span class="koboSpan" id="kobo.804.1"> field, click on </span><strong class="screenText"><span class="koboSpan" id="kobo.805.1">Edit</span></strong><span class="koboSpan" id="kobo.806.1"> and </span><strong class="screenText"><span class="koboSpan" id="kobo.807.1">Add</span></strong><span class="koboSpan" id="kobo.808.1"> to add a column property, and enter the following, where </span><code class="inlineCode"><span class="koboSpan" id="kobo.809.1">description</span></code><span class="koboSpan" id="kobo.810.1"> is the key and the actual text is the value:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.811.1">description:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.812.1">credit</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.813.1">score</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.814.1">is</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.815.1">the</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.816.1">FICO</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.817.1">score</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.818.1">for</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.819.1">each</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.820.1">customer</span></span>
</code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.821.1">Follow the same previous steps and add the following column property for the </span><code class="inlineCode"><span class="koboSpan" id="kobo.822.1">exited</span></code><span class="koboSpan" id="kobo.823.1"> field in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.824.1">churn_list</span></code><span class="koboSpan" id="kobo.825.1"> table:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.826.1">description:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.827.1">churn</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.828.1">flag</span></span>
</code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.829.1">We are </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.830.1">now ready to do some searches using metadata inside the Lake Formation management console. </span><span class="koboSpan" id="kobo.830.2">Try typing the following words separately in the text box for </span><strong class="keyWord"><span class="koboSpan" id="kobo.831.1">Find table by properties</span></strong><span class="koboSpan" id="kobo.832.1"> to search for tables and see what’s returned:</span><ul>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.833.1">FICO</span></code></li>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.834.1">csv</span></code></li>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.835.1">churn flag</span></code></li>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.836.1">creditscore</span></code></li>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.837.1">customerid</span></code></li>
</ul>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.838.1">Now that </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.839.1">you have found the table you are looking for, let’s query the table and see the actual data to learn how to query the data interactively, which is an important task performed by data scientists for data exploration and understanding. </span><span class="koboSpan" id="kobo.839.2">Select the table you want to query and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.840.1">View data</span></strong><span class="koboSpan" id="kobo.841.1"> button in the </span><strong class="screenText"><span class="koboSpan" id="kobo.842.1">Actions</span></strong><span class="koboSpan" id="kobo.843.1"> drop-down menu. </span><span class="koboSpan" id="kobo.843.2">This should bring you to the </span><strong class="screenText"><span class="koboSpan" id="kobo.844.1">Amazon Athena</span></strong><span class="koboSpan" id="kobo.845.1"> screen. </span><span class="koboSpan" id="kobo.845.2">You should see a </span><strong class="screenText"><span class="koboSpan" id="kobo.846.1">Query</span></strong><span class="koboSpan" id="kobo.847.1"> tab already created, and the query already executed. </span><span class="koboSpan" id="kobo.847.2">The results are displayed at the bottom of the screen. </span><span class="koboSpan" id="kobo.847.3">If you get a warning message stating that you need to provide an output location, select the </span><strong class="screenText"><span class="koboSpan" id="kobo.848.1">Settings</span></strong><span class="koboSpan" id="kobo.849.1"> tab, and then click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.850.1">Manage</span></strong><span class="koboSpan" id="kobo.851.1"> button to provide an S3 location as the output location. </span><span class="koboSpan" id="kobo.851.2">You can run any other SQL query to explore the data further, such as joining the </span><code class="inlineCode"><span class="koboSpan" id="kobo.852.1">customer_data</span></code><span class="koboSpan" id="kobo.853.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.854.1">churn_list</span></code><span class="koboSpan" id="kobo.855.1"> tables with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.856.1">customerid</span></code><span class="koboSpan" id="kobo.857.1"> field:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.858.1">SELECT</span></span> <span class="hljs-operator"><span class="koboSpan" id="kobo.859.1">*</span></span> <span class="hljs-key ord"><span class="koboSpan" id="kobo.860.1">FROM</span></span><span class="koboSpan" id="kobo.861.1"> "bank_customer_db"."customer_data", " bank_customer_db"."churn_list" </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.862.1">where</span></span><span class="koboSpan" id="kobo.863.1"> "bank_customer_db"."customer_data"."customerid" </span><span class="hljs-operator"><span class="koboSpan" id="kobo.864.1">=</span></span><span class="koboSpan" id="kobo.865.1"> "bank_customer_db"."churn_list"."customerid" ;
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.866.1">You have now learned how to discover the data in Lake Formation and run queries against the data in a Lake Formation database and tables. </span><span class="koboSpan" id="kobo.866.2">Next, let’s run a data processing job using the Amazon Glue ETL service to make the data ready for ML tasks.</span></p>
<h2 class="heading-2" id="_idParaDest-143"><span class="koboSpan" id="kobo.867.1">Creating an Amazon Glue ETL job to process data for ML</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.868.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.869.1">customer_data</span></code><span class="koboSpan" id="kobo.870.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.871.1">churn_list</span></code><span class="koboSpan" id="kobo.872.1"> tables contain features that are useful for ML. </span><span class="koboSpan" id="kobo.872.2">However, they </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.873.1">need to be joined and processed so they can be used to train ML models. </span><span class="koboSpan" id="kobo.873.2">One option is for the data scientists to download these datasets and process them in a Jupyter notebook for model training. </span><span class="koboSpan" id="kobo.873.3">Another option is to process the data using a separate processing engine so that the data scientists can work with the processed data directly. </span><span class="koboSpan" id="kobo.873.4">Here, we will set up an AWS Glue job to process the data in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.874.1">customer_data</span></code><span class="koboSpan" id="kobo.875.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.876.1">churn_list</span></code><span class="koboSpan" id="kobo.877.1"> tables and transform them into new ML features that are ready for model training directly:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.878.1">First, create a new S3 bucket called </span><code class="inlineCode"><span class="koboSpan" id="kobo.879.1">MLSA-DataLake-Serving-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.880.1">. </span><span class="koboSpan" id="kobo.880.2">We will use this bucket to store the output training datasets from the Glue job.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.881.1">Using the Lake Formation console, grant </span><code class="inlineCode"><span class="koboSpan" id="kobo.882.1">AWSGlueService_Role</span></code> <strong class="screenText"><span class="koboSpan" id="kobo.883.1">Super</span></strong><span class="koboSpan" id="kobo.884.1"> access to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.885.1">customer_data</span></code><span class="koboSpan" id="kobo.886.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.887.1">churn_list</span></code><span class="koboSpan" id="kobo.888.1"> tables. </span><span class="koboSpan" id="kobo.888.2">We will use this role to run the Glue job.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.889.1">To start creating the Glue job, go to the Glue console and click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.890.1">ETL Jobs</span></strong><span class="koboSpan" id="kobo.891.1"> link on the Glue console. </span><span class="koboSpan" id="kobo.891.2">Click on </span><strong class="screenText"><span class="koboSpan" id="kobo.892.1">Script editor</span></strong><span class="koboSpan" id="kobo.893.1"> and then click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.894.1">Create script</span></strong><span class="koboSpan" id="kobo.895.1"> button.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.896.1">On the script editor screen, change the job name from </span><strong class="screenText"><span class="koboSpan" id="kobo.897.1">Untitled job</span></strong><span class="koboSpan" id="kobo.898.1"> to </span><code class="inlineCode"><span class="koboSpan" id="kobo.899.1">customer_churn_process</span></code><span class="koboSpan" id="kobo.900.1"> for easy tracking.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.901.1">On the </span><strong class="screenText"><span class="koboSpan" id="kobo.902.1">Job details</span></strong><span class="koboSpan" id="kobo.903.1"> tab, select </span><code class="inlineCode"><span class="koboSpan" id="kobo.904.1">AWSGlueService_Role</span></code><span class="koboSpan" id="kobo.905.1"> as the IAM role. </span><span class="koboSpan" id="kobo.905.2">Add a new </span><code class="inlineCode"><span class="koboSpan" id="kobo.906.1">Job</span></code><span class="koboSpan" id="kobo.907.1"> parameter called </span><code class="inlineCode"><span class="koboSpan" id="kobo.908.1">target_bucket</span></code><span class="koboSpan" id="kobo.909.1"> under </span><code class="inlineCode"><span class="koboSpan" id="kobo.910.1">Advanced Properties</span></code><span class="koboSpan" id="kobo.911.1"> and enter the value of your target bucket for the output files.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.912.1">On the </span><strong class="screenText"><span class="koboSpan" id="kobo.913.1">Script tab</span></strong><span class="koboSpan" id="kobo.914.1"> screen, copy the following code blocks to the code section. </span><span class="koboSpan" id="kobo.914.2">Make sure to replace </span><code class="inlineCode"><span class="koboSpan" id="kobo.915.1">default_bucket</span></code><span class="koboSpan" id="kobo.916.1"> with your own bucket in the code. </span><span class="koboSpan" id="kobo.916.2">The following code block first joins the </span><code class="inlineCode"><span class="koboSpan" id="kobo.917.1">churn_list</span></code><span class="koboSpan" id="kobo.918.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.919.1">customer_data</span></code><span class="koboSpan" id="kobo.920.1"> tables using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.921.1">customerid</span></code><span class="koboSpan" id="kobo.922.1"> column as the key, then transforms the </span><code class="inlineCode"><span class="koboSpan" id="kobo.923.1">gender</span></code><span class="koboSpan" id="kobo.924.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.925.1">geo</span></code><span class="koboSpan" id="kobo.926.1"> columns with an index, creates a new DataFrame with only the relevant columns, and finally saves the output file to an S3 location using the date and generated version ID as partitions. </span><span class="koboSpan" id="kobo.926.2">The code uses default values for the target bucket and prefix variables and generates a date partition and version partition for the S3 location. </span><span class="koboSpan" id="kobo.926.3">The job can also accept input arguments for these parameters.
    </span><p class="normal"><span class="koboSpan" id="kobo.927.1">The following code block sets up default configurations, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.928.1">SparkContext</span></code><span class="koboSpan" id="kobo.929.1"> and a default bucket:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.930.1">import</span></span><span class="koboSpan" id="kobo.931.1"> sys
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.932.1">from</span></span><span class="koboSpan" id="kobo.933.1"> awsglue.utils </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.934.1">import</span></span><span class="koboSpan" id="kobo.935.1"> getResolvedOptions
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.936.1">from</span></span><span class="koboSpan" id="kobo.937.1"> awsglue.transforms </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.938.1">import</span></span><span class="koboSpan" id="kobo.939.1"> Join
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.940.1">from</span></span><span class="koboSpan" id="kobo.941.1"> pyspark.context </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.942.1">import</span></span><span class="koboSpan" id="kobo.943.1"> SparkContext
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.944.1">from</span></span><span class="koboSpan" id="kobo.945.1"> awsglue.context </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.946.1">import</span></span><span class="koboSpan" id="kobo.947.1"> GlueContext
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.948.1">from</span></span><span class="koboSpan" id="kobo.949.1"> awsglue.job </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.950.1">import</span></span><span class="koboSpan" id="kobo.951.1"> Job
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.952.1">import</span></span><span class="koboSpan" id="kobo.953.1"> pandas </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.954.1">as</span></span><span class="koboSpan" id="kobo.955.1"> pd
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.956.1">from</span></span><span class="koboSpan" id="kobo.957.1"> datetime </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.958.1">import</span></span><span class="koboSpan" id="kobo.959.1"> datetime
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.960.1">import</span></span><span class="koboSpan" id="kobo.961.1"> uuid
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.962.1">from</span></span><span class="koboSpan" id="kobo.963.1"> pyspark.ml.feature </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.964.1">import</span></span><span class="koboSpan" id="kobo.965.1"> StringIndexer
glueContext = GlueContext(SparkContext.getOrCreate())
logger = glueContext.get_logger()
current_date = datetime.now()
default_date_partition = </span><span class="hljs-string"><span class="koboSpan" id="kobo.966.1">f"</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.967.1">{current_date.year}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.968.1">-</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.969.1">{current_date.month}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.970.1">-</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.971.1">{current_date.day}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.972.1">"</span></span><span class="koboSpan" id="kobo.973.1">   
default_version_id = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.974.1">str</span></span><span class="koboSpan" id="kobo.975.1">(uuid.uuid4())
default_bucket = </span><span class="hljs-string"><span class="koboSpan" id="kobo.976.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.977.1">&lt;your default bucket name&gt;"</span></span><span class="koboSpan" id="kobo.978.1">
default_prefix = </span><span class="hljs-string"><span class="koboSpan" id="kobo.979.1">"ml-customer-churn"</span></span><span class="koboSpan" id="kobo.980.1">
target_bucket = </span><span class="hljs-string"><span class="koboSpan" id="kobo.981.1">""</span></span><span class="koboSpan" id="kobo.982.1">
prefix = </span><span class="hljs-string"><span class="koboSpan" id="kobo.983.1">""</span></span><span class="koboSpan" id="kobo.984.1">
day_partition =</span><span class="hljs-string"><span class="koboSpan" id="kobo.985.1">""</span></span><span class="koboSpan" id="kobo.986.1">
version_id = </span><span class="hljs-string"><span class="koboSpan" id="kobo.987.1">""</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.988.1">try</span></span><span class="koboSpan" id="kobo.989.1">:
     args = getResolvedOptions(sys.argv,[</span><span class="hljs-string"><span class="koboSpan" id="kobo.990.1">'JOB_NAME'</span></span><span class="koboSpan" id="kobo.991.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.992.1">'target_bucket'</span></span><span class="koboSpan" id="kobo.993.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.994.1">'prefix'</span></span><span class="koboSpan" id="kobo.995.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.996.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.997.1">day_partition'</span></span><span class="koboSpan" id="kobo.998.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.999.1">'version_id'</span></span><span class="koboSpan" id="kobo.1000.1">])
     target_bucket = args[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1001.1">'target_bucket'</span></span><span class="koboSpan" id="kobo.1002.1">]
     prefix = args[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1003.1">'prefix'</span></span><span class="koboSpan" id="kobo.1004.1">]
     day_partition = args[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1005.1">'day_partition'</span></span><span class="koboSpan" id="kobo.1006.1">]
     version_id = args[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1007.1">'version_id'</span></span><span class="koboSpan" id="kobo.1008.1">]
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.1009.1">except</span></span><span class="koboSpan" id="kobo.1010.1">:
     logger.error(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1011.1">"error occured with getting arguments"</span></span><span class="koboSpan" id="kobo.1012.1">)
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.1013.1">if</span></span><span class="koboSpan" id="kobo.1014.1"> target_bucket == </span><span class="hljs-string"><span class="koboSpan" id="kobo.1015.1">""</span></span><span class="koboSpan" id="kobo.1016.1">:
     target_bucket = default_bucket
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.1017.1">if</span></span><span class="koboSpan" id="kobo.1018.1"> prefix == </span><span class="hljs-string"><span class="koboSpan" id="kobo.1019.1">""</span></span><span class="koboSpan" id="kobo.1020.1">:
     prefix = default_prefix
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.1021.1">if</span></span><span class="koboSpan" id="kobo.1022.1"> day_partition == </span><span class="hljs-string"><span class="koboSpan" id="kobo.1023.1">""</span></span><span class="koboSpan" id="kobo.1024.1">:
     day_partition = default_date_partition
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.1025.1">if</span></span><span class="koboSpan" id="kobo.1026.1"> version_id == </span><span class="hljs-string"><span class="koboSpan" id="kobo.1027.1">""</span></span><span class="koboSpan" id="kobo.1028.1">:
     version_id = default_version_id
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1029.1">The</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.1030.1"> following code joins the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1031.1">customer_data</span></code><span class="koboSpan" id="kobo.1032.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1033.1">churn_list</span></code><span class="koboSpan" id="kobo.1034.1"> tables into a single table using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1035.1">customerid</span></code><span class="koboSpan" id="kobo.1036.1"> column as the key:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1037.1"># catalog: database and table names</span></span><span class="koboSpan" id="kobo.1038.1">
db_name = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1039.1">"bank_customer_db"</span></span><span class="koboSpan" id="kobo.1040.1">
tbl_customer = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1041.1">"customer_data"</span></span><span class="koboSpan" id="kobo.1042.1">
tbl_churn_list = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1043.1">"churn_list"</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.1044.1"># Create dynamic frames from the source tables</span></span><span class="koboSpan" id="kobo.1045.1">
customer = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=tbl_customer)
churn = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=tbl_churn_list)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1046.1"># Join the frames to create customer churn dataframe</span></span><span class="koboSpan" id="kobo.1047.1">
customer_churn = Join.apply(customer, churn, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1048.1">'customerid'</span></span><span class="koboSpan" id="kobo.1049.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1050.1">'customerid'</span></span><span class="koboSpan" id="kobo.1051.1">)
customer_churn.printSchema()
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1052.1">The</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.1053.1"> following code block transforms several data columns from string labels to label indices and writes the final file to an output location in S3:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1054.1"># ---- Write out the combined file ----</span></span><span class="koboSpan" id="kobo.1055.1">
current_date = datetime.now()
str_current_date = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1056.1">f"</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1057.1">{current_date.year}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1058.1">-</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1059.1">{current_date.month}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1060.1">-</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1061.1">{current_date.day}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1062.1">"</span></span><span class="koboSpan" id="kobo.1063.1">   
random_version_id = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1064.1">str</span></span><span class="koboSpan" id="kobo.1065.1">(uuid.uuid4())
output_dir = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1066.1">f"s3://</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1067.1">{target_bucket}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1068.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1069.1">{prefix}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1070.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1071.1">{day_partition}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1072.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1073.1">{version_id}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1074.1">"</span></span><span class="koboSpan" id="kobo.1075.1">
s_customer_churn = customer_churn.toDF()
gender_indexer = StringIndexer(inputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1076.1">"gender"</span></span><span class="koboSpan" id="kobo.1077.1">, outputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1078.1">"genderindex"</span></span><span class="koboSpan" id="kobo.1079.1">)
s_customer_churn = gender_indexer.fit(s_customer_churn).transform(s_customer_churn)
geo_indexer = StringIndexer(inputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1080.1">"geography"</span></span><span class="koboSpan" id="kobo.1081.1">, outputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1082.1">"geographyindex"</span></span><span class="koboSpan" id="kobo.1083.1">)
s_customer_churn = geo_indexer.fit(s_customer_churn).transform(s_customer_churn)
s_customer_churn = s_customer_churn.select(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1084.1">'geographyindex'</span></span><span class="koboSpan" id="kobo.1085.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1086.1">'estimatedsalary'</span></span><span class="koboSpan" id="kobo.1087.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1088.1">'hascrcard'</span></span><span class="koboSpan" id="kobo.1089.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1090.1">'numofproducts'</span></span><span class="koboSpan" id="kobo.1091.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1092.1">'balance'</span></span><span class="koboSpan" id="kobo.1093.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1094.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1095.1">age'</span></span><span class="koboSpan" id="kobo.1096.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1097.1">'genderindex'</span></span><span class="koboSpan" id="kobo.1098.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1099.1">'isactivemember'</span></span><span class="koboSpan" id="kobo.1100.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1101.1">'creditscore'</span></span><span class="koboSpan" id="kobo.1102.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1103.1">'tenure'</span></span><span class="koboSpan" id="kobo.1104.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1105.1">'exited'</span></span><span class="koboSpan" id="kobo.1106.1">)
s_customer_churn = s_customer_churn.coalesce(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1107.1">1</span></span><span class="koboSpan" id="kobo.1108.1">)
s_customer_churn.write.option(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1109.1">"header"</span></span><span class="koboSpan" id="kobo.1110.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1111.1">"true"</span></span><span class="koboSpan" id="kobo.1112.1">).</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1113.1">format</span></span><span class="koboSpan" id="kobo.1114.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1115.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1116.1">csv"</span></span><span class="koboSpan" id="kobo.1117.1">).mode(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1118.1">'Overwrite'</span></span><span class="koboSpan" id="kobo.1119.1">).save(output_dir)
logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1120.1">"output_dir:"</span></span><span class="koboSpan" id="kobo.1121.1"> + output_dir)
</span></code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7"><span class="koboSpan" id="kobo.1122.1">Click on </span><strong class="screenText"><span class="koboSpan" id="kobo.1123.1">Save</span></strong><span class="koboSpan" id="kobo.1124.1"> and then the </span><strong class="screenText"><span class="koboSpan" id="kobo.1125.1">Run job</span></strong><span class="koboSpan" id="kobo.1126.1"> button to run the job. </span><span class="koboSpan" id="kobo.1126.2">Check the job running status by clicking on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1127.1">ETL jobs</span></strong><span class="koboSpan" id="kobo.1128.1"> link in the Glue console, and then click on </span><strong class="screenText"><span class="koboSpan" id="kobo.1129.1">Job run monitoring</span></strong><span class="koboSpan" id="kobo.1130.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1131.1">After the job completes, check the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1132.1">s3://MLSA-DataLake-Serving-&lt;your initials&gt;/ml-customer-churn/&lt;date&gt;/&lt;guid&gt;/</span></code><span class="koboSpan" id="kobo.1133.1"> location in S3 and see whether a new CSV file was generated. </span><span class="koboSpan" id="kobo.1133.2">Open the file and see whether you see the new processed dataset in the file.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.1134.1">You have now </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.1135.1">successfully built an AWS Glue job for data processing and feature engineering for ML. </span><span class="koboSpan" id="kobo.1135.2">With this, you can automate data processing and feature engineering, which is critical to achieve reproducibility and governance. </span><span class="koboSpan" id="kobo.1135.3">Try creating a crawler to crawl the newly processed data in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1136.1">MLSA-DataLake-Serving-&lt;your initials&gt;</span></code><span class="koboSpan" id="kobo.1137.1"> bucket to make it available in the Glue catalog and run some queries against it. </span><span class="koboSpan" id="kobo.1137.2">You should see a new table created with multiple partitions (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.1138.1">ml-customer-churn</span></code><span class="koboSpan" id="kobo.1139.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.1140.1">date</span></code><span class="koboSpan" id="kobo.1141.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1142.1">GUID</span></code><span class="koboSpan" id="kobo.1143.1">) for the different training datasets. </span><span class="koboSpan" id="kobo.1143.2">You can query the data by using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1144.1">GUID</span></code><span class="koboSpan" id="kobo.1145.1"> partition as a query condition.</span></p>
<h2 class="heading-2" id="_idParaDest-144"><span class="koboSpan" id="kobo.1146.1">Building a data pipeline using Glue workflows</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1147.1">Next, we will </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.1148.1">construct a pipeline that executes a data ingestion job, followed by the creation of a database catalog for the data. </span><span class="koboSpan" id="kobo.1148.2">Finally, a data processing job will be initiated to generate the training dataset. </span><span class="koboSpan" id="kobo.1148.3">This pipeline will automate the flow of data from the source to the desired format, ensuring seamless and efficient data processing for ML model training:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1149.1">To start, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1150.1">Workflows (orchestration)</span></strong><span class="koboSpan" id="kobo.1151.1"> link in the left pane of the Glue</span><strong class="keyWord"> </strong><span class="koboSpan" id="kobo.1152.1">management console.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1153.1">Click on </span><strong class="screenText"><span class="koboSpan" id="kobo.1154.1">Add workflow</span></strong><span class="koboSpan" id="kobo.1155.1"> and enter a name for your workflow on the next screen. </span><span class="koboSpan" id="kobo.1155.2">Then, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1156.1">Create workflow</span></strong><span class="koboSpan" id="kobo.1157.1"> button.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1158.1">Select the workflow you just created and click on </span><strong class="screenText"><span class="koboSpan" id="kobo.1159.1">Add trigger</span></strong><span class="koboSpan" id="kobo.1160.1">. </span><span class="koboSpan" id="kobo.1160.2">Select the </span><strong class="screenText"><span class="koboSpan" id="kobo.1161.1">Add New</span></strong><span class="koboSpan" id="kobo.1162.1"> tab, and then enter a name for the trigger and select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1163.1">on-demand</span></code><span class="koboSpan" id="kobo.1164.1"> trigger type.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1165.1">On the workflow UI designer, you will see a new </span><strong class="screenText"><span class="koboSpan" id="kobo.1166.1">Add Node</span></strong><span class="koboSpan" id="kobo.1167.1"> icon show up. </span><span class="koboSpan" id="kobo.1167.2">Click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1168.1">Add Node</span></strong><span class="koboSpan" id="kobo.1169.1"> icon, select the </span><strong class="screenText"><span class="koboSpan" id="kobo.1170.1">Crawler</span></strong><span class="koboSpan" id="kobo.1171.1"> tab, and select </span><code class="inlineCode"><span class="koboSpan" id="kobo.1172.1">bank_customer_db_crawler</span></code><span class="koboSpan" id="kobo.1173.1">, then click on </span><strong class="screenText"><span class="koboSpan" id="kobo.1174.1">Add</span></strong><span class="koboSpan" id="kobo.1175.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1176.1">On the workflow UI designer, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1177.1">Crawler</span></strong><span class="koboSpan" id="kobo.1178.1"> icon, and you will see a new </span><strong class="screenText"><span class="koboSpan" id="kobo.1179.1">Add Trigger</span></strong><span class="koboSpan" id="kobo.1180.1"> icon show up. </span><span class="koboSpan" id="kobo.1180.2">Click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1181.1">Add Trigger</span></strong><span class="koboSpan" id="kobo.1182.1"> icon, select the </span><strong class="screenText"><span class="koboSpan" id="kobo.1183.1">Add new</span></strong><span class="koboSpan" id="kobo.1184.1"> tab, and select </span><strong class="screenText"><span class="koboSpan" id="kobo.1185.1">Start after ANY event</span></strong><span class="koboSpan" id="kobo.1186.1"> as the trigger logic, and then click on </span><strong class="keyWord"><span class="koboSpan" id="kobo.1187.1">Add</span></strong><span class="koboSpan" id="kobo.1188.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1189.1">On the</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.1190.1"> workflow UI designer, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.1191.1">Add Node</span></strong><span class="koboSpan" id="kobo.1192.1"> icon, select the </span><strong class="screenText"><span class="koboSpan" id="kobo.1193.1">Jobs</span></strong><span class="koboSpan" id="kobo.1194.1"> tab, and select the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1195.1">customer_churn_process</span></code><span class="koboSpan" id="kobo.1196.1"> job.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1197.1">On the workflow UI designer, the final workflow should look like the following diagram:
    </span><figure class="mediaobject"><span class="koboSpan" id="kobo.1198.1"><img alt="Figure 4.7 – Glue data flow design " src="../Images/B20836_04_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1199.1">Figure 4.8: Glue data flow design</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="8"><span class="koboSpan" id="kobo.1200.1">Now, you are ready to run the workflow. </span><span class="koboSpan" id="kobo.1200.2">Select the workflow and select </span><strong class="screenText"><span class="koboSpan" id="kobo.1201.1">Run</span></strong><span class="koboSpan" id="kobo.1202.1"> from the </span><strong class="screenText"><span class="koboSpan" id="kobo.1203.1">Actions</span></strong><span class="koboSpan" id="kobo.1204.1"> dropdown. </span><span class="koboSpan" id="kobo.1204.2">You can monitor the running status by selecting the </span><strong class="screenText"><span class="koboSpan" id="kobo.1205.1">Run ID</span></strong><span class="koboSpan" id="kobo.1206.1"> and clicking on </span><strong class="screenText"><span class="koboSpan" id="kobo.1207.1">View run details</span></strong><span class="koboSpan" id="kobo.1208.1">. </span><span class="koboSpan" id="kobo.1208.2">You should see something similar to the following screenshot:
    </span><figure class="mediaobject"><span class="koboSpan" id="kobo.1209.1"><img alt="Figure 4.8 – Glue workflow execution " src="../Images/Image2513.jpg"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1210.1">Figure 4.9: Glue workflow execution</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="9"><span class="koboSpan" id="kobo.1211.1">Try deleting the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1212.1">customer_data</span></code><span class="koboSpan" id="kobo.1213.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1214.1">churn_list</span></code><span class="koboSpan" id="kobo.1215.1"> tables and re-run the workflow. </span><span class="koboSpan" id="kobo.1215.2">See whether the new tables are created again. </span><span class="koboSpan" id="kobo.1215.3">Check the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1216.1">s3://MLSA-DataLake-Serving-&lt;your initials&gt;/ml-customer-churn/&lt;date&gt;/</span></code><span class="koboSpan" id="kobo.1217.1"> S3 location to verify a new folder is created with a new dataset.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.1218.1">Congratulations! </span><span class="koboSpan" id="kobo.1218.2">You have completed the hands-on lab and learned how to build a simple data lake and its supporting components to allow data cataloging, data querying, and data processing. </span><span class="koboSpan" id="kobo.1218.3">You should now be able to apply some of the skills learned to real-world</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.1219.1"> design and the implementation of a data management platform on AWS to support the ML development lifecycle.</span></p>
<h1 class="heading-1" id="_idParaDest-145"><span class="koboSpan" id="kobo.1220.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1221.1">In this chapter, we delved into the considerations for managing data in the context of ML and explored the architecture of an enterprise data management platform for ML. </span><span class="koboSpan" id="kobo.1221.2">We examined the intersection of data management with the ML lifecycle and learned how to design a data lake architecture on AWS. </span><span class="koboSpan" id="kobo.1221.3">To apply these concepts, we went through the process of building a data lake using AWS Lake Formation. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.1222.1">Through hands-on experience, we practiced data ingestion, processing, and cataloging for data discovery, querying, and ML tasks. </span><span class="koboSpan" id="kobo.1222.2">Additionally, we gained proficiency in using AWS data management tools such as AWS Glue, AWS Lambda, and Amazon Athena. </span><span class="koboSpan" id="kobo.1222.3">In the next chapter, our focus will shift to the architecture and technologies involved in constructing data science environments using open-source tools.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.1223.1">Leave a review!</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1224.1">Enjoying this book? </span><span class="koboSpan" id="kobo.1224.2">Help readers like you by leaving an Amazon review. </span><span class="koboSpan" id="kobo.1224.3">Scan the QR code below to get a free eBook of your choice.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1225.1"><img alt="" role="presentation" src="../Images/Review_Copy.png"/></span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.1226.1">*Limited Offer</span></em></p>
</div>
</body></html>