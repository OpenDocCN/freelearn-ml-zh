- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview of Differential Privacy Algorithms and Applications of Differential
    Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of differential privacy holds great significance in the realm of
    data privacy and its importance continues to grow as more and more data is collected
    and analyzed. Differential privacy algorithms offer a means to safeguard individual
    privacy while still allowing for valuable insights to be derived from this data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will gain an overview of differential privacy algorithms,
    along with a comprehension of crucial concepts such as sensitivity and clipping
    in the context of differential privacy. Additionally, we will explore how aggregates
    are generated through the use of differential privacy, including in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following main topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Laplace algorithm for differential privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gaussian algorithm for differential privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating aggregates using differential privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensitivity and its role in aggregate generation algorithms using differential
    privacy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Queries using differential privacy: Count, sum, and mean Count, sum, and mean'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clipping:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding its significance within the realm of differential privacy and
    examples
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of real-world applications that make use of differential privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differential privacy algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential privacy is a fundamental concept that‚Äôs designed to safeguard individual
    privacy while enabling the statistical analysis of sensitive data. It establishes
    a mathematical framework that guarantees the preservation of individual privacy
    during data analysis and sharing processes. Differential privacy algorithms play
    a vital role by introducing random noise into the data, making it challenging
    to identify specific records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding chapter, we learned that the core idea behind differential
    privacy is to introduce random noise into the data analysis process. This noise
    makes it difficult for an attacker to determine whether a particular individual‚Äôs
    data was included in the analysis, thus preserving privacy. The fundamental concept
    is that the inclusion or exclusion of any individual‚Äôs data should not significantly
    impact the results of the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 ‚Äì Illustrating the primary concept of differential privacy](img/B16573_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 ‚Äì Illustrating the primary concept of differential privacy
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy can be defined mathematically. For two datasets, *D1* and
    *D2*, which differ by a single record, the probability that the randomized algorithm
    or mechanism, *L* (such as count, sum, average, and so on), applied to *D1* yields
    a result in subset *S* is bounded by the exponential of epsilon (*Œµ*) multiplied
    by the probability that *L* applied to *D2* yields a result in S, plus a delta
    (*Œ¥*) term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(L[D1] ‚àà S) ‚â§ exp(Œµ) * P(L[D2] ‚àà S) + Œ¥
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* denotes probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D1* and *D2* represent the two datasets that differ by a single record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S* encompasses all subsets of the randomized algorithm or mechanism, *L*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Œµ* (epsilon) is a positive real number that controls the privacy loss and
    is referred to as the privacy budget. It determines the extent to which the algorithm
    can differ between the two databases and quantifies the privacy loss incurred
    when the algorithm is applied to the database. If Œµ is zero, the queries will
    yield similar answers, which compromises privacy to a lesser extent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure differential privacy, random noise (in the form of a random number)
    is added to the results. This noise prevents adversaries from determining whether
    the returned results are genuine or have noise added, and if so, the magnitude
    of the added noise. The added noise guarantees that the data cannot be utilized
    to identify individuals while still allowing useful statistical properties to
    be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the random number distribution depends on how to select the right
    random number. Next, we‚Äôll explore two well-known distributions, the Laplace distribution
    and the Gaussian distribution, to assess their suitability for the privacy parameter,
    Œµ, and to guarantee differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Laplace distribution, also referred to as the double exponential distribution,
    is a continuous probability distribution named after the renowned mathematician
    Pierre-Simon Laplace. It arises from merging two exponential distributions, one
    positive and one negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better comprehend the Laplace distribution, let‚Äôs understand the exponential
    distribution. The exponential distribution is defined by this function:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = e -x
  prefs: []
  type: TYPE_NORMAL
- en: When x equals 0, f(x) is equal to 1, and as x increases, f(x) gradually approaches
    0.
  prefs: []
  type: TYPE_NORMAL
- en: To observe this behavior, you can try a simple Python program to analyze the
    exponential distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try out a simple Python program and test this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Exponential_Laplace_Guassian_Clipping.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 ‚Äì Exponential distribution](img/B16573_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 ‚Äì Exponential distribution
  prefs: []
  type: TYPE_NORMAL
- en: Laplace distribution ‚Äì mathematical definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Laplace distribution‚Äôs probabilistic density function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16573_04_Formula_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, x is a random variable, Œº is the location parameter that determines the
    center of the distribution, and b > 0 is the scale parameter that controls the
    spread or variability of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace distribution has several important properties. It is symmetric around
    its mean (Œº), with a peak at Œº. The tails of the distribution are heavy, meaning
    that it has a higher probability of observing extreme values compared to a Gaussian
    (normal) distribution. This property makes the Laplace distribution suitable for
    modeling data with outliers or heavy-tailed behavior. The Laplace distribution
    finds applications in various fields, including statistics, signal processing,
    image processing, and machine learning. It is particularly useful in scenarios
    where robustness to outliers is desired or when modeling data with a Laplacian
    noise assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'If Œº equals 0 and b equals 1, then the equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ùî£(x) = 1¬†_¬†2 `e-x`
  prefs: []
  type: TYPE_NORMAL
- en: 'This is nothing but half of the exponential distribution. Let‚Äôs implement Laplace
    in simple Python code and understand it further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 ‚Äì Laplace distribution with different means and scaling parameters](img/B16573_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 ‚Äì Laplace distribution with different means and scaling parameters
  prefs: []
  type: TYPE_NORMAL
- en: From this graph, we can see that as the scaling parameter (b) increases in the
    Laplace distribution, the tails contribute a greater proportion compared to the
    central point. This characteristic can be leveraged to enhance privacy by utilizing
    larger scaling parameters.
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the scaling parameter, the Laplace distribution‚Äôs tails become
    heavier, resulting in a higher likelihood of extreme values occurring. This increased
    variability makes it more challenging for adversaries to extract sensitive information
    or identify individual data points accurately. Therefore, larger scaling parameters
    can be employed as a privacy-enhancing measure as they contribute to a broader
    spread of the distribution and provide a greater level of privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the Laplace distribution one of the algorithms of choice for differential
    privacy?
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following example, which shows that Laplace distribution
    is one of the right algorithms to implement for differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have taken a simple dataset that consists of numbers from 90 to 110
    in increments of 0.2\. The average of these numbers is close to 100, which is
    the query result ‚Äì that is, the query finds the average of these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find the Laplace distribution for this dataset with the mean as the query
    result and the scaling factor (b) set to 2, which is **1 / Œµ** in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 ‚Äì Distributions using Laplace](img/B16573_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 ‚Äì Distributions using Laplace
  prefs: []
  type: TYPE_NORMAL
- en: The query result can come from any of these possible distributions. As per the
    differential privacy formula (DP), query results from one distribution should
    be less than or equal to eŒµ times the second distribution when they are compared.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular instance, the resultant value of 100 from the query is derived
    from distribution 1\. However, it‚Äôs not possible to definitively exclude the possibility
    that it originates from the second distribution, particularly if the probability
    of the outcome is less than or equal to eŒµ times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The result is *0.25 0.15163266492815836*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The result is 0.25 0.25.
  prefs: []
  type: TYPE_NORMAL
- en: This proves that the Laplace distribution can be used as one of the mechanisms
    for differential privacy and that the b scaling parameter can be set to 1/Œµ, which
    is the privacy budget.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gaussian distribution, also known as normal distribution, is a continuous
    probability distribution that is widely used in statistical analysis to model
    real-world phenomena. It is characterized by its bell-shaped curve, which is symmetrical
    around the mean value.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution ‚Äì mathematical definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaussian distribution is also called normal distribution and it is a continuous
    probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for Gaussian distribution is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16573_04_Formula_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, x is the random variable in the Gaussian distribution; Œº is the location
    parameter, as in, it will be the mean of the data; œÉ is the standard deviation;
    and œÉ2 is the variance of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is an implementation of a Gaussian function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs generate plots for both the Gaussian distribution and the Laplace distribution
    using the random dataset we‚Äôve created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 ‚Äì Gaussian distribution with different means and standard deviation](img/B16573_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 ‚Äì Gaussian distribution with different means and standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: Based on this graph, we can see that the Laplace distribution has a sharper
    peak compared to the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs determine if the Gaussian mechanism complies with Œµ differential
    privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Gaussian mechanism does not satisfy Œµ differential privacy. Let‚Äôs check
    whether it satisfies (Œµ, Œ¥) differential privacy by adding a Œ¥ value in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This shows that the Gaussian mechanism supports (Œµ, Œ¥) differential privacy,
    which is sometimes called approximate differential privacy due to the addition
    of delta (Œ¥).
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of noise-adding algorithms to apply differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table provides a high-level comparison of three noise-adding
    mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Exponential Mechanism** | **Laplace Mechanism** | **Gaussian Mechanism**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Usage | Mostly used to select a specific record or choose the best element,
    specifically with non-numeric or categorical data | Widely used in numerical aggregates
    due to its balance between privacy and utility and its ease of implementation
    | Used when dealing with a smaller scale of data and where differential privacy
    needs to be applied repeatedly |'
  prefs: []
  type: TYPE_TB
- en: '| Noise type | Exponential noise | Laplace (double exponential) noise | Gaussian
    noise |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy | High privacy level due to the exponential distribution of output
    probabilities | Random noise might significantly change outcomes, compromising
    data utility | Achieves either delta privacy or epsilon-delta privacy, which is
    a trade-off for better utility |'
  prefs: []
  type: TYPE_TB
- en: '| Implementation | Complex to implement due to the nature of exponential distribution
    | Relatively easy to implement due to the feature of the Laplace distribution
    | Complex to implement but generally suitable when data sensitivity is low |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptability | Mainly used for categorical data | Used mostly with numerical
    data | Suitable for low-sensitivity and multi-query data |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 - High-level comparison of three noise-adding mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Generating aggregates using differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Differential privacy is a technique that‚Äôs used to protect the privacy of individuals
    when they‚Äôre collecting and sharing data. One of the ways that differential privacy
    can be used is in the generation of aggregates, which are statistical summaries
    of data that can provide useful insights while also protecting the privacy of
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Once the noise has been added to the data, statistical aggregates can be calculated
    and released without compromising individual privacy. Examples of statistical
    aggregates that can be generated using differential privacy include means, medians,
    and histograms. One of the challenges of using differential privacy to generate
    aggregates is balancing privacy and accuracy. The amount of noise that is added
    to the data will affect the accuracy of the statistical aggregates that are generated.
    Therefore, there is often a trade-off between the amount of privacy protection
    and the accuracy of the results. Overall, the use of differential privacy to generate
    aggregates is a powerful tool for protecting individual privacy while still allowing
    useful statistical properties to be calculated. However, it requires careful consideration
    of the specific use case and the trade-offs between privacy and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sensitivity plays a crucial role in the realm of differential privacy. It refers
    to the maximum amount by which the output of a function or computation can change
    when a single individual‚Äôs data point is added or removed from a dataset. Sensitivity
    provides a measure of the privacy risk associated with performing computations
    on sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs look at an example of a real-life scenario that illustrates the need to
    measure the impact of changing a dataset using sensitivity analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario ‚Äì financial risk assessment model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose a financial institution develops a machine learning model to assess
    the credit risk of loan applicants. The model takes various features, such as
    income, credit history, employment status, and outstanding debt, into account
    to predict the likelihood of loan default. The institution wants to ensure that
    the model is robust and not overly sensitive to the presence or absence of any
    individual in the dataset. In this case, sensitivity analysis can be employed
    to evaluate the impact of changing the dataset, specifically by removing or modifying
    the data of certain individuals. The institution wants to determine whether the
    exclusion or modification of data for specific individuals significantly alters
    the model‚Äôs predictions or introduces bias, and measure the privacy risk associated
    with this change.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs break this down in mathematical terms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a function, f, that takes a dataset, D, as input and
    returns a numerical output, then the sensitivity of f is defined as the maximum
    absolute difference between the outputs of f on two datasets, D and D‚Äô, that differ
    by the presence or absence of a single individual‚Äôs data point.
  prefs: []
  type: TYPE_NORMAL
- en: So, here, the sensitivity of f is sensitivity(f) = max_{D, D‚Äô} ||f(D) - f(D‚Äô)||.
  prefs: []
  type: TYPE_NORMAL
- en: The sensitivity of a function is a fundamental concept in differential privacy
    because it determines the amount of noise that needs to be added to the output
    of the function to guarantee privacy. Naturally, functions with higher sensitivity
    are more prone to leaking information about individuals in the dataset and therefore
    require more noise to be added to their outputs to maintain privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand sensitivity, let‚Äôs consider the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: X = [ 0, 1,2,3,4,5,6,7,8,9,10,11,‚Ä¶..N]
  prefs: []
  type: TYPE_NORMAL
- en: 'Two functions/queries are defined in this dataset: ùî£1(x) = x `and` f2(x) =
    x3`.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs define another function/query that calculates the following difference:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚àÜ f(xa, xb) = | f(xa) ‚àí f(xb) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs calculate f1, f2, ‚àÜ f1, ‚àÜ f2 and observe the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| XÔÉ† | X0 | X1 | X2 | X3 | X4 | X5 | X6 | X7 | ‚Ä¶. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | ‚Ä¶ |'
  prefs: []
  type: TYPE_TB
- en: '| f1(xi)=x | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | ‚Ä¶ |'
  prefs: []
  type: TYPE_TB
- en: '| f2(xi)=x3 | 0 | 1 | 8 | 27 | 64 | 125 | 216 | 343 | ‚Ä¶ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ‚àÜf1(xi, xi+1) | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ‚Ä¶. |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ‚àÜf2(xi, xi+1) | 1 | 7 | 21 | 37 | 61 | 91 | 127 | ‚Ä¶ | ... |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 - Sensitivity analysis of functions/queries on datasets
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity refers to the impact a change in the underlying dataset can have
    on the result of a query.
  prefs: []
  type: TYPE_NORMAL
- en: Let xA, xB be any dataset from all possible datasets of X differing by at most
    one element.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the sensitivity is calculated with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity = max | f(xa) ‚àí f(xb) ||, where Xa,Xb ‚äÜ X
  prefs: []
  type: TYPE_NORMAL
- en: Based on this definition, it is very clear that the sensitivity for f1(x) =
    x is 1 because the maximum difference is constant. With the second query, f2(x)=x3,
    the difference is not constant and it is unbounded, growing based on the number
    of data elements in the dataset. To calculate the sensitivity of the second function/query,
    we need to specify the lower bounds and upper bounds ‚Äì for example, from x5 to
    x10, x0 to x5, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs define another query that is bounded within the input values of 5 to
    10 or 0 to 5 using the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: f3(x) = x3, where 0<=x<=5
  prefs: []
  type: TYPE_NORMAL
- en: ‚àÜ f(xa, xb) = | f(xa) ‚àí f(xb) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| XÔÉ† | X0 | X1 | X2 | X3 | X4 | X5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| fx(xi)=x3 | 0 | 1 | 8 | 27 | 64 | 125 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ‚àÜf2(xi, xi+1) | 1 | 7 | 21 | 37 | 61 | 91 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 - Sensitivity analysis of query bounded within input values
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the sensitivity will be max | 1+7+21+37+61+91 | = 216:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The sensitivity of queries can be further classified into global sensitivity
    and local sensitivity, depending on whether the sensitivity is calculated using
    universal datasets or local datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Global sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Global sensitivity refers to the maximum possible change in query results when
    considering all possible datasets. It is calculated based on the entire population
    or a universal dataset. Global sensitivity provides an upper bound on the potential
    privacy risk and captures the worst-case scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Local sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local sensitivity, on the other hand, focuses on the sensitivity of a query
    based on a specific local dataset. It measures the maximum change in query results
    when a single data point is added or removed from this local dataset. Local sensitivity
    provides a more fine-grained and context-specific measure of privacy risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this, we can formulate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Query/Function** | **Global Sensitivity** | **Comments** |'
  prefs: []
  type: TYPE_TB
- en: '| ùëì(ùë•) = ùë• | 1 | Changing ùë• by 1 will result in changes of ùëì(ùë•) by 1 |'
  prefs: []
  type: TYPE_TB
- en: '| ùëì(ùë•) = ùë• 2 | Unbounded | A change in ùëì(ùë•) depends on the value of ùë• |'
  prefs: []
  type: TYPE_TB
- en: '| ùëì(ùë•) = ùë• + ùë• | 2 | Changing ùë• by 1 changes ùëì(ùë•) by 2 |'
  prefs: []
  type: TYPE_TB
- en: '| ùëì( ùë•) = 5 ‚àó ùë• | 5 | Changing ùë• by 1 changes ùëì(ùë•) by 5 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.4 - Local sensitivity analysis of query results
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy is achieved by adding noise to the query result so that
    privacy is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The amount of noise that needs to be added in differential privacy depends
    on four key parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The privacy parameter (Œµ)**: The privacy parameter, Œµ, controls the level
    of privacy protection provided by differential privacy. A smaller value of Œµ implies
    stronger privacy guarantees but may result in higher noise being added to the
    query result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The privacy budget (ŒîŒµ)**: The privacy budget represents the total amount
    of privacy that can be expended over multiple queries or operations. It is typically
    divided equally between the individual queries to ensure that the overall privacy
    guarantees are maintained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity (Œîf)**: Sensitivity refers to the maximum amount by which the
    query result can change when a single data point is added or removed. It quantifies
    the impact of individual data on the query result and helps determine the amount
    of noise to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data size (N)**: The size of the dataset being analyzed also influences the
    amount of noise added in differential privacy. Larger datasets generally allow
    for less noise to be added, resulting in improved accuracy, while smaller datasets
    may require more noise to preserve privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By carefully choosing appropriate values for these parameters and considering
    the trade-off between privacy and accuracy, the noise can be tailored to provide
    effective privacy protection while still yielding useful and reliable query results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 ‚Äì Parameters that influence the addition of noise](img/B16573_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 ‚Äì Parameters that influence the addition of noise
  prefs: []
  type: TYPE_NORMAL
- en: Queries that use differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Queries that use differential privacy allow analysts and data scientists to
    retrieve aggregated information from a dataset while ensuring that the privacy
    of individual data points is protected. These queries are designed to add a controlled
    amount of noise to the query results, making it difficult to discern the contribution
    of any particular individual in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various types of queries can be performed using differential privacy. Some
    commonly used ones include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Count queries**: These queries aim to determine the number of records that
    satisfy certain conditions in a dataset while preserving privacy. The query result
    is perturbed by adding noise to the true count, ensuring that individual contributions
    cannot be accurately determined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sum queries**: Sum queries involve calculating the sum of specific values
    in a dataset while maintaining privacy ‚Äì for example, computing the total income
    of a group of individuals while protecting the confidentiality of individual income
    values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average queries**: Average queries compute the average value of a specific
    attribute in a dataset while ensuring privacy. Noise is added to the computed
    average to protect individual data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-K queries**: Top-K queries aim to identify the K largest values or records
    in a dataset while preserving privacy. Noise is added to the query result to hide
    the specific contributions of individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range queries**: Range queries involve retrieving records or values within
    a specific range from a dataset while maintaining privacy. Noise is added to the
    query result to prevent individual data points that have been identified falling
    within the range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve differential privacy, various mechanisms are used to add noise to
    the query results. These mechanisms include the Laplace mechanism, the Gaussian
    mechanism, and randomized response, among others. The choice of mechanism depends
    on the specific requirements of the query and the desired privacy guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the level of privacy protection provided by differential
    privacy is quantifiable through a privacy parameter called epsilon (Œµ). A smaller
    value of Œµ provides stronger privacy guarantees but may introduce more noise into
    the query results, potentially reducing their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Queries that use differential privacy offer a balance between data utility and
    privacy protection. They allow for meaningful analysis and insights to be derived
    from sensitive datasets while safeguarding the confidentiality of individuals‚Äô
    information. By incorporating differential privacy techniques into data analysis
    processes, organizations can ensure compliance with privacy regulations and build
    trust with their users or customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs deep dive into these queries one by one with the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Age** | **Salary** | **Gender** | **Title** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 32 | 200K | M | Staff Engineer |'
  prefs: []
  type: TYPE_TB
- en: '| B | 44 | 300K | F | Manager |'
  prefs: []
  type: TYPE_TB
- en: '| C | 55 | 400K | F | Director |'
  prefs: []
  type: TYPE_TB
- en: '| D | 66 | 500K | M | VP |'
  prefs: []
  type: TYPE_TB
- en: Table 4.6 - Differential privacy queries analysis with the provided dataset
  prefs: []
  type: TYPE_NORMAL
- en: Count queries using differential privacy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Analyze the results of the following queries and figure out how sensitivity
    operates:'
  prefs: []
  type: TYPE_NORMAL
- en: How many total employees are there in this dataset? The answer is 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many people have below 300K salary? The answer is 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many male employees are working? The answer is 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure the answer includes noise and guarantees differential privacy, it
    is essential to determine the sensitivity of count queries and the appropriate
    value to use for sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: Count queries have a sensitivity of 1, meaning that if one record is added,
    the query result will increase by 1, or if one row is removed, the query result
    will decrease by 1\. By understanding the sensitivity of count queries, we can
    accurately incorporate noise while preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: We will utilize the NumPy library to generate differentially private results
    for the sample count queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy provides Laplace noise generation based on the given sensitivity, mean,
    and epsilon values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Ouput:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Sum queries using differential privacy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let‚Äôs analyze the following queries and understand how sensitivity works in
    this case:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the total salaries of all employees in this dataset? The total sum of
    salaries is reported as 1,400K ( without applying differential privacy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the total age of employees? The total sum of ages is reported as 197
    (without applying differential privacy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure the answers include the noise that was added by guaranteed differential
    privacy, it is important to determine the sensitivity for the sum queries and
    identify the appropriate value to use for sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike count queries, sensitivity for sum queries is not straightforward because
    the addition of a new row will increase the output by the added value.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, sensitivity in this case is unbounded since it depends on the data
    of the record being added.
  prefs: []
  type: TYPE_NORMAL
- en: For the query regarding the age of a person, we can estimate the upper bound
    by considering the historical maximum age to which a person has lived, which is
    around 126\. Thus, we can assume an upper bound of 126 for this query.
  prefs: []
  type: TYPE_NORMAL
- en: However, determining the real upper bound for the salary query is challenging.
    It could be 1M, 10M, 100M, or even higher. As a result, in sum queries, the sensitivity
    depends on the lower and upper bounds provided to the queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'To automatically calculate the lower and upper bounds, a technique called clipping
    can be employed, which we will explore in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Guaranteeing accuracy and privacy in the results of differential privacy are
    heavily influenced by their sensitivity, which is determined by the lower and
    upper bounds specified for sum queries.
  prefs: []
  type: TYPE_NORMAL
- en: Average queries using differential privacy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Analyze the following queries and figure out how sensitivity works in this
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the average salary? The answer is 350 K.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the average age? The answer is 49.26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the answers for average queries include the noise added by guaranteed
    differential privacy, it is important to determine the sensitivity for these queries
    and identify the appropriate value to use for the sensitivity parameter in the
    differential privacy algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to considering average queries is to treat them as two separate
    queries: a summation query and a count query. The sensitivity of count queries
    can be calculated using known methods; the sensitivity of sum queries can be determined
    similarly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By understanding the sensitivity of both the count and sum queries, we can
    then provide the differentially private result for average queries. This involves
    incorporating noise in a way that preserves privacy while ensuring the accuracy
    of the average calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The noisy average salary with differential privacy will be distorted due to
    the added noise, while the average salary without differential privacy remains
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs implement the same use case using differential privacy techniques
    such as sensitivity, clipping, and noise. We‚Äôll use the Laplace mechanism to inject
    noise into the average calculation.
  prefs: []
  type: TYPE_NORMAL
- en: While the current discussion aims to provide an understanding of the base concepts,
    it‚Äôs important to note that production-grade frameworks that implement differential
    privacy take into account various intricacies in more detail. These frameworks
    address factors such as handling overflows of numbers during calculations and
    employing appropriate algorithms to ensure accurate and privacy-preserving computations.
    These frameworks prioritize robustness and reliability to meet the demands of
    real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Clipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, unbounded queries have an infinite sensitivity value,
    which cannot be directly utilized to provide results with differential privacy.
    One approach to addressing this issue is to transform unbounded queries into bounded
    ones by specifying their lower and upper bounds.
  prefs: []
  type: TYPE_NORMAL
- en: In differential privacy, clipping is a technique that‚Äôs used to bind the sensitivity
    of a function by constraining its output within a specific range. The fundamental
    concept is to clip or limit the output of a function to fall into a predetermined
    range, such as [-c, c], where c is a positive constant. Afterward, noise is introduced
    to the clipped output to ensure privacy guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clipping procedure involves two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling the function‚Äôs output**: The output of the function is scaled by
    dividing it by a scaling factor, denoted as s. This scaling ensures that the absolute
    value of the scaled output is less than or equal to the clipping threshold, c.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mathematically, this can be represented as f‚Äô(D) = f(D) / s, where |f‚Äô(D)| <=
    c.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Adding noise**: Once the output has been clipped, noise is incorporated to
    uphold differential privacy. This noise can be generated from a Laplace or Gaussian
    distribution, with appropriate parameters depending on the desired level of privacy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The clipping technique serves to restrict the influence of individual data points
    on the function‚Äôs output and facilitates the addition of sufficient noise to preserve
    privacy effectively. By bounding the sensitivity using clipping, differential
    privacy mechanisms can maintain data confidentiality while providing accurate
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs revisit the simple dataset that we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Age** | **Salary** | **Gender** | **Title** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 32 | 200K | M | Staff Engineer |'
  prefs: []
  type: TYPE_TB
- en: '| B | 44 | 300K | F | Manager |'
  prefs: []
  type: TYPE_TB
- en: '| C | 55 | 400K | F | Director |'
  prefs: []
  type: TYPE_TB
- en: '| D | 66 | 500K | M | VP |'
  prefs: []
  type: TYPE_TB
- en: Table 4.7 - Sensitivity bounding analysis using clipping technique
  prefs: []
  type: TYPE_NORMAL
- en: Given the actual query, ‚ÄúHow much is the total salary of all employees in this
    dataset?‚Äù and the actual answer, which is 1,400K, providing the total salary with
    differential privacy requires the addition of noise using a noise mechanism such
    as a Laplace or Gaussian mechanism. This noise mechanism relies on the sensitivity
    of the query.
  prefs: []
  type: TYPE_NORMAL
- en: To proceed, we must establish the lower and upper bounds as arbitrary fixed
    values. By calculating the difference between the lower and upper bounds, we obtain
    the sensitivity of the query. However, to delve deeper into this process, we will
    apply the clip function to the input data.
  prefs: []
  type: TYPE_NORMAL
- en: By applying the clip function, we constrain the input data within a specific
    range, which aids in managing the sensitivity of the query. This technique helps
    limit the influence of individual data points and facilitates the addition of
    noise in a controlled manner. By employing clipping, we can strike a balance between
    data privacy and utility, ensuring accurate results while preserving the confidentiality
    of sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs play with the clip function on a dataset and understand this further.
  prefs: []
  type: TYPE_NORMAL
- en: Clipping example 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs consider the following example use case: a social media platform wants
    to determine the average number of likes per user for a given post. The platform
    wants to protect the privacy of its users while still obtaining an accurate estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an implementation in Python without differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: DP_End_to_End.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Average likes per user (without differential* *privacy): 51.146*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs implement the same use case using differential privacy techniques
    such as sensitivity, clipping, and noise. We‚Äôll use the Laplace mechanism to inject
    noise into the average calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we set the sensitivity to 1 since the average calculation has
    a sensitivity of 1 (changing one user‚Äôs likes can affect the average by, at most,
    1). Then, we clipped the likes per user to ensure they fall within a specific
    range (in this case, 0 to 100).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we calculated the clipped average likes and defined the privacy budget
    and epsilon value. The privacy budget represents the total amount of privacy protection
    available and the epsilon value controls the level of privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we calculated the scale parameter for the Laplace distribution based
    on the sensitivity, privacy budget, and epsilon. We added noise to the clipped
    average likes using the Laplace mechanism, considering the scale parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting noisy_average_likes value provides an estimate of the average
    number of likes per user while incorporating differential privacy techniques to
    protect users‚Äô privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Clipping example 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is another example of clipping:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source code :* *DP_End_to_End.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, if we apply the `clip` function provided by `Pandas` with a range
    of 0 and 9,999, then the salaries don‚Äôt change because of the lower and upper
    bound values in the `clip` function.
  prefs: []
  type: TYPE_NORMAL
- en: Clipping example 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is another example of clipping:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: DP_End_to_End.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this case, when we set the upper bound to 400 and the lower bound to 250,
    the clip function changed salaries above 400 to 400 and below 250 to 250 (for
    example, 200 becomes 250 and 500 becomes 400).
  prefs: []
  type: TYPE_NORMAL
- en: pandas‚Äô `clip` function will enforce that the input data is within the specified
    clipping bounds. If one of the values is too small, it will be converted into
    the lower clipping bound, and if a value is too large, it will be converted into
    the upper clipping bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema illustrates this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Clip (250, 200, 300, 400, 500, **400**)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 ‚Äì Illustrating clipping functionality](img/B16573_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 ‚Äì Illustrating clipping functionality
  prefs: []
  type: TYPE_NORMAL
- en: Since it is a toy dataset, we can look at the data and come up with proper lower
    and upper bounds for clipping the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs add noise to the clipped inputs‚Äô sum with sensitivity (the clipped range
    difference) and epsilon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: By employing appropriate lower and upper bounds, the results that were obtained
    using differential privacy are closer to the actual values, resulting in increased
    accuracy compared to using random bounds. In this example, the actual total salary
    is 1,400K, but with added noise, the returned value is 1,693K.
  prefs: []
  type: TYPE_NORMAL
- en: However, one potential drawback of using specific lower and upper bounds is
    that they may inadvertently reveal the actual salaries of individuals present
    in the dataset, thereby compromising privacy.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is to initialize the lower bound with a value of zero
    or a suitably low value and gradually increase the upper bound. The process involves
    observing the query result and monitoring when it stops changing or becomes stable.
    At this point, the value of the upper bound is stopped, ensuring that the query
    result does not reveal sensitive information and preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: This iterative approach allows for a balance between accuracy and privacy as
    it dynamically determines the bounds based on the characteristics of the data.
    It helps prevent the disclosure of specific salary information while still providing
    reasonably accurate results within the scope of differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs explore this approach with the toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: DP_End_to_End.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 ‚ÄìTotal Salary vs Clipping ranges for Salary](img/B16573_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 ‚ÄìTotal Salary vs Clipping ranges for Salary
  prefs: []
  type: TYPE_NORMAL
- en: Given the small size of the dataset, it‚Äôs relatively straightforward to identify
    the optimal clipping value for the query results ‚Äì in this scenario, the sum of
    the salaries doesn‚Äôt increase beyond 500\. However, with larger datasets, the
    noise can sometimes become too dominant to analyze for all different sensitivity
    or clipping values.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of real-life applications of differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will go through a high-level summary of a few real-world
    applications that make use of differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy usage at Uber
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uber Technologies, Inc. provides mobility as a service ‚Äì that is, ride-hailing
    services. It has to protect the privacy of the customers who make use of its services
    but at the same time analyze its data to provide a better service. It has developed
    an open source framework to achieve differential privacy with SQL queries, which
    it made to analyze and derive insights. All SQL queries are transformed using
    Uber‚Äôs differential privacy framework, which executes SQL queries and provides
    the query results with differential privacy guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: It can be found on GitHub at [https://github.com/uber-archive/sql-differential-privacy](https://github.com/uber-archive/sql-differential-privacy).
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy usage at Apple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apple, Inc. has incorporated local differential privacy on a large scale in
    its products with the aim of understanding and enhancing the user experience.
    They‚Äôve employed a private count mean sketch algorithm, which utilizes count,
    mean, sketch, and a privacy budget (epsilon). For each key feature, they‚Äôve established
    a privacy budget and the maximum number of records (differentially private records)
    that can be transmitted to a remote server after sensitive features such as IP
    addresses have been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key Feature** | **Epsilon (**ùú∫**)** | **Number of Records per Day for**
    **Each Device** |'
  prefs: []
  type: TYPE_TB
- en: '| Emoji suggestions | 4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Lookup hints | 4 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| QuickType | 8 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| ‚Ä¶. |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 4.8 - Implementation of local differential privacy by Apple, Inc.
  prefs: []
  type: TYPE_NORMAL
- en: It injects noise after encoding the input vector using a hash function and flips
    each vector element with a probability of 1/(1 + ùëí ùúÄ/2), where Œµ is the privacy
    budget.
  prefs: []
  type: TYPE_NORMAL
- en: More details about this algorithm can be found at [https://machinelearning.apple.com/research/learning-with-privacy-at-scale](https://machinelearning.apple.com/research/learning-with-privacy-at-scale).
  prefs: []
  type: TYPE_NORMAL
- en: 'Jun Tang, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, Xiaofeng Wang,
    et al. have undertaken a detailed analysis of Apple‚Äôs differential privacy implementation
    by debugging the client-side code and published a research article in Arxiv. I
    strongly recommend that you read this paper to understand Apple‚Äôs implementation
    better and the additional details that were considered: *Privacy Loss in Apple‚Äôs
    Implementation of Differential Privacy on MacOS* *10.12* ([https://arxiv.org/abs/1709.02753](https://arxiv.org/abs/1709.02753)).'
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy usage in the US Census
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The US Census Bureau used differential privacy to apply noise to the 2020 census
    data to protect respondents‚Äô confidentiality in the collected and shared census
    data.
  prefs: []
  type: TYPE_NORMAL
- en: More details can be found at [https://www.census.gov/data/academy/webinars/2021/disclosure-avoidance-series/differential-privacy-101.html](https://www.census.gov/data/academy/webinars/2021/disclosure-avoidance-series/differential-privacy-101.html).
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy at Google
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google uses differential privacy in its Chrome browser to find out about frequently
    visited pages; Google Maps and Google Assistant also have this functionality.
    The differential privacy system that it has developed is called **Randomized Aggregatable
    Privacy-Preserving Ordinal Response** (**RAPPOR**) and provides an epsilon value
    of 2 as the lower limit and 8 to 9 as the upper limit.
  prefs: []
  type: TYPE_NORMAL
- en: Google has open sourced the implementation of RAPPOR. You can learn more about
    this at [https://github.com/google/rappor](https://github.com/google/rappor).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the concept of differential privacy and explored
    how the Laplace and Gaussian mechanisms can generate noise to ensure privacy while
    generating aggregate query results. We discussed the significance of parameters
    such as epsilon, delta, and sensitivity, and how they are used to calculate noise
    using Laplace or Gaussian distributions. Additionally, we learned about the process
    of determining upper and lower bounds using the clipping technique. Finally, we
    provided a summary of how differential privacy is used in real-world applications
    at Apple and Uber and by the US Census Bureau.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into open source frameworks for differential
    privacy. We will explore how to develop applications using these frameworks and
    dive into the realm of machine learning with differential privacy in detail. This
    will provide a comprehensive understanding of how to implement differential privacy
    in practical scenarios and how to leverage its benefits in the context of machine
    learning applications.
  prefs: []
  type: TYPE_NORMAL
