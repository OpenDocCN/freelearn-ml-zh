["```py\n    import torch\n    from transformers import BertModel, BertTokenizer\n    model_name = \"bert-base-cased\"\n    tokenizer =BertTokenizer.from_pretrained(model_name,\n                                             torchscript = True)\n    bert = BertModel.from_pretrained(model_name, torchscript=True)\n    ```", "```py\n    max_length = 128\n    tokenizer_out = tokenizer(text,\n                              padding = \"max_length\",\n                              max_length = max_length,\n                              truncation = True,\n                              return_tensors = \"pt\", )\n    attention_mask = tokenizer_out.attention_mask\n    input_ids = tokenizer_out.input_ids\n    ```", "```py\n    model.eval()\n    traced_script_module = torch.jit.trace(model,\n                                           [ input_ids,\n                                           attention_mask ])\n    traced_script_module.save(\"bert_model.pt\")\n    ```", "```py\n    vocab_file = open(\"vocab.txt\", \"w\")\n    for i, j in tokenizer.get_vocab().items():\n        vocab_file.write(f\"{i} {j}\\n\")\n    vocab_file.close()\n    ```", "```py\n    #include <torch/torch.h>\n    #include <string>\n    #include <unordered_map>\n    class Tokenizer {\n     public:\n      Tokenizer(const std::string& vocab_file_path,\n                int max_len = 128);\n      std::pair<torch::Tensor, torch::Tensor> tokenize(\n          const std::string text);\n     private:\n      std::unordered_map<std::string, int> vocab_;\n      int max_len_{0};\n    }\n    ```", "```py\n    Tokenizer::Tokenizer(const std::string& vocab_file_path,\n                         int max_len)\n        : max_len_{max_len} {\n      auto file = std::ifstream(vocab_file_path);\n      std::string line;\n      while (std::getline(file, line)) {\n        auto sep_pos = line.find_first_of(' ');\n        auto token = line.substr(0, sep_pos);\n        auto id = std::stoi(line.substr(sep_pos + 1));\n        vocab_.insert({token, id});\n      }\n    }\n    ```", "```py\n    std::pair<torch::Tensor, torch::Tensor> Tokenizer::tokenize(const std::string text) {\n      std::string pad_token = \"[PAD]\";\n      std::string start_token = \"[CLS]\";\n      std::string end_token = \"[SEP]\";\n      auto pad_token_id = vocab_[pad_token];\n      auto start_token_id = vocab_[start_token];\n      auto end_token_id = vocab_[end_token];\n    ```", "```py\n      std::vector<int> input_ids(max_len_, pad_token_id);\n      std::vector<int> attention_mask(max_len_, 0);\n      input_ids[0] = start_token_id;\n      attention_mask[0] = 1;\n    ```", "```py\n      std::string word;\n      std::istringstream ss(text);\n    ```", "```py\n      int input_id = 1;\n      while (getline(ss, word, ' ')) {\n      // search token in the vocabulary and increment input_id\n      if (input_id == max_len_ - 1) {\n        break;\n      }\n    }\n    ```", "```py\n      size_t start = 0;\n    while (start < word.size()) {\n      size_t end = word.size();\n      std::string token;\n      bool has_token = false;\n      while (start < end) {\n        // search the prefix in the vocabulary\n        end--;\n      }\n      if (input_id == max_len_ - 1) {\n        break;\n      }\n      if (!has_token) {\n        break;\n      }\n      start = end;\n    }\n    ```", "```py\n      auto token = word.substr(start, end - start);\n    if (start > 0) \n      token = \"##\" + token;\n    auto token_iter = vocab_.find(token);\n    if (token_iter != vocab_.end()) {\n      attention_mask[input_id] = 1;\n      input_ids[input_id] = token_iter->second;\n      ++input_id;\n      has_token = true;\n      break;\n    }\n    ```", "```py\n      attention_mask[input_id] = 1;\n      input_ids[input_id] = end_token_id;\n      auto input_ids_tensor = torch::tensor(\n                                 input_ids).unsqueeze(0);\n      auto attention_masks_tensor = torch::tensor(\n                                      attention_mask).unsqueeze(0);\n    return std::make_pair(input_ids_tensor,\n                          attention_masks_ tensor);\n    ```", "```py\n    #include <string>\n    #include <vector>\n    class ImdbReader {\n     public:\n        ImdbReader(const std::string& root_path);\n        size_t get_pos_size() const;\n        size_t get_neg_size() const;\n        const std::string& get_pos(size_t index) const;\n        const std::string& get_neg(size_t index) const;\n    private:\n    using Reviews = std::vector<std::string>;\n    void read_ directory(const std::string& path,\n                         Reviews& reviews);\n     private:\n        Reviews pos_samples_;\n        Reviews neg_samples_;\n        size_t max_size_{0};\n    };\n    ```", "```py\n    int main(int argc, char** argv) {\n      if (argc > 0) {\n        auto root_path = fs::path(argv[1]);\n        … ImdbReader train_reader(root_path / \"train\");\n        ImdbReader test_reader(root_path / \"test\");\n      }\n    }\n    ```", "```py\n    namespace fs = std::filesystem;\n    ImdbReader::ImdbReader(const std::string& root_path) {\n      auto root = fs::path(root_path);\n      auto neg_path = root / \"neg\";\n      auto pos_path = root / \"pos\";\n      if (fs::exists(neg_path) && fs::exists(pos_path)) {\n        auto neg = std::async(std::launch::async, [&]() {\n          read_directory(neg_path, neg_samples_);\n        });\n        auto pos = std::async(std::launch::async, [&]() {\n          read_directory(pos_path, pos_samples_);\n        });\n        neg.get();\n        pos.get();\n      } else {\n    throw std::invalid_argument(\"ImdbReader incorrect \n                                 path\");\n      }\n    }\n    ```", "```py\n    void ImdbReader::read_directory(const std::string& path,\n                                    Reviews& reviews) {\n      for (auto& entry : fs::directory_iterator(path)) {\n        if (fs::is_regular_file(entry)) {\n          std::ifstream file(entry.path());\n          if (file) {\n            std::stringstream buffer;\n            buffer << file.rdbuf();\n            reviews.push_back(buffer.str());\n          }\n        }\n      }\n    }\n    ```", "```py\n    using ImdbData = std::pair<torch::Tensor, torch::Tensor>;\n    using ImdbExample = torch::data::Example<ImdbData,\n                                             torch::Tensor>;\n    ```", "```py\n    class ImdbDataset : public torch::data::Dataset<ImdbDataset, ImdbExample> {\n     public:\n        ImdbDataset(const std::string& dataset_path,\n                    std::shared_ptr<Tokenizer> tokenizer);\n        // torch::data::Dataset implementation\n        ImdbExample get(size_t index) override;\n        torch::optional<size_t> size() const override;\n     private:\n        ImdbReader reader_;\n        std::shared_ptr<Tokenizer> tokenizer_;\n    };\n    ```", "```py\n    torch::optional<size_t> ImdbDataset::size() const {\n        return reader_.get_pos_size() + reader_.get_neg_size();\n    }\n    ```", "```py\n    ImdbExample ImdbDataset::get(size_t index) {\n      torch::Tensor target;\n      const std::string* review{nullptr};\n      if (index < reader_.get_pos_size()) {\n        review = &reader_.get_pos(index);\n        target = torch::tensor(1, torch::dtype(torch::kLong));\n      } else {\n        review =\n            &reader_.get_neg(index - reader_.get_pos_size());\n        target = torch::tensor(0, torch::dtype(torch::kLong));\n      }\n      // encode text\n      auto tokenizer_out = tokenizer_->tokenize(*review);\n      return {tokenizer_out, target.squeeze()};\n    }\n    ```", "```py\n    torch::data::transforms::Collation<ImdbExample> {\n      ImdbExample apply_batch(std::vector<ImdbExample> examples)\n          override {\n        std::vector<torch::Tensor> input_ids;\n        std::vector<torch::Tensor> attention_masks;\n        std::vector<torch::Tensor> labels;\n        input_ids.reserve(examples.size());\n        attention_masks.reserve(examples.size());\n        labels.reserve(examples.size());\n        for (auto& example : examples) {\n          input_ids.push_back(std::move(example.data.first));\n          attention_masks.push_back(\n              std::move(example.data.second));\n          labels.push_back(std::move(example.target));\n        }\n        return {{torch::stack(input_ids),\n                 torch::stack(attention_masks)},\n                torch::stack(labels)};\n      }\n    }\n    ```", "```py\n    #include <torch/script.h>\n    #include <torch/torch.h>\n    class ModelImpl : public torch::nn::Module {\n     public:\n        ModelImpl() = delete;\n        ModelImpl(const std::string& bert_model_path);\n        torch::Tensor forward(at::Tensor input_ids,\n                              at::Tensor attention_masks);\n     private:\n        torch::jit::script::Module bert_;\n        torch::nn::Dropout dropout_;\n        torch::nn::Linear fc1_;\n        torch::nn::Linear fc2_;\n    };\n    TORCH_MODULE(Model);\n    ```", "```py\n    ModelImpl::ModelImpl(const std::string& bert_model_path)\n        : dropout_(register_module(\n              \"dropout\",\n              torch::nn::Dropout(\n                  torch::nn::DropoutOptions().p(0.2)))),\n          fc1_(register_module(\n              \"fc1\",\n              torch::nn::Linear(\n                  torch::nn::LinearOptions(768, 512)))),\n          fc2_(register_module(\n              \"fc2\",\n              torch::nn::Linear(\n                  torch::nn::LinearOptions(512, 2)))) {\n      bert_ = torch::jit::load(bert_model_path);\n    }\n    ```", "```py\n    torch::Tensor ModelImpl::forward(\n        at::Tensor input_ids,\n        at::Tensor attention_masks) {\n      std::vector<torch::jit::IValue> inputs = {\n          input_ids, attention_masks};\n      auto bert_output = bert_.forward(inputs);\n      auto pooler_output =\n          bert_output.toTuple()->elements()[1].toTensor();\n      auto x = fc1_(pooler_output);\n      x = torch::nn::functional::relu(x);\n      x = dropout_(x);\n      x = fc2_(x);\n      x = torch::softmax(x, /*dim=*/1);\n      return x;\n    }\n    ```", "```py\nauto tokenizer = std::make_shared<Tokenizer>(vocab_path);\nImdbDataset train_dataset(dataset_path / \"train\", tokenizer);\n```", "```py\nint batch_size = 8;\nauto train_loader = torch::data::make_data_loader(\n    train_dataset.map(Stack()),\n    torch::data::DataLoaderOptions()\n        .batch_size(batch_size)\n        .workers(8));\n```", "```py\ntorch::DeviceType device = torch::cuda::is_available()\n    ? torch::DeviceType::CUDA\n    : torch::DeviceType::CPU;\nModel model(model_path);\nmodel->to(device);\n```", "```py\ntorch::optim::AdamW optimizer(model->parameters(),\n    torch::optim::AdamWOptions(1e-5));\n```", "```py\nfor (int epoch = 0; epoch < epochs; ++epoch) {\n  model->train();\n  for (auto& batch : (*train_loader)) {\n    optimizer.zero_grad();\n    auto batch_label = batch.target.to(device);\n    auto batch_input_ids =\n        batch.data.first.squeeze(1).to(device);\n    auto batch_attention_mask =\n        batch.data.second.squeeze(1).to(device);\n    auto output =\n        model(batch_input_ids, batch_attention_mask);\n    torch::Tensor loss =\n        torch::cross_entropy_loss(output, batch_label);\n    loss.backward();\n    torch::nn::utils::clip_grad_norm_(model->parameters(),\n                                      1.0);\n    optimizer.step();\n  }\n}\n```"]