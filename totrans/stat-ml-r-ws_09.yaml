- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Algebra in R
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter introduced an efficient and effective reporting approach
    using R Markdown. *Part 1* of this book essentially covered the practical aspects
    of getting things done using R. *Part 2* of this book goes back to the fundamentals,
    covering two essential pillars of mathematics: linear algebra and calculus. Understanding
    these basics will better prepare us to appreciate and work with common mathematical
    operations to the point that these operations feel natural to us. *Part 2* aims
    to develop that level of literacy, starting with a fundamental review of linear
    algebra with R in this chapter.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the basic concepts of
    linear algebra, including vectors, matrices, and the system of equations. You
    will also be able to interpret basic notations in linear algebra and work with
    common matrices using R.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear algebra
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common matrix operations and properties
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving system of linear equations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_7/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_7/working.R).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear algebra
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter delves into one of the most important branches of mathematics:
    **linear algebra**. Linear algebra deals with linear operations of mathematical
    objects, including vectors, matrices, and tensors (high-dimensional matrices),
    the most common forms of data. For example, the typical table we use to store
    data in Excel consists of a series of columns. Each column is called a vector,
    which stores a specific number of elements and takes the form of a column instead
    of a row by default. A collection of these column vectors forms a matrix, a two-dimensional
    Excel table, or DataFrame, as we used to call it in the previous chapters. We
    can also view the same table as a collection of row vectors, where each vector
    lives in the form of a row.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put these in context. The following code snippet loads the sleep dataset
    and prints out the first six rows and three columns. We use A to denote this 6x3
    matrix in the following exposition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s introduce some notation here. We use a bold lowercase letter, a, to denote
    a column vector, say the `extra` variable, which consists of six elements in the
    vector. In other words, a is a six-dimensional vector. The column vector, a, can
    be *transposed* into a row vector, denoting a T. Since a T is a row vector, we
    can write a T = [0.7, − 1.6, − 0.2, − 1.2, − 0.1, 3.4], which is also the typical
    way of writing row vectors (or transposed column vectors) in many books. Similarly,
    a row vector can be transposed into the (original) column vector, giving us (a T) T
    = a.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'A graphical illustration will help here. *Figure 7**.1* depicts the process
    of transposing the column vector, a, into a row vector, a T, then transposing
    it again into the original column vector, (a T) T = a. Therefore, we can view
    the matrix, A, as a collection of column vectors concatenated horizontally, or
    a collection of row vectors concatenated vertically:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Transposing the column vector into a row vector, and then transposing
    it again into the original column vector](img/B18680_07_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Transposing the column vector into a row vector, and then transposing
    it again into the original column vector
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Note that the rows are also referred to as observations and that the columns
    are referred to as features or attributes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We will start by working with vectors in the following section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Working with vectors
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector in R chains together data elements of the same type. Multiple vectors
    join hands, often side by side, to form a matrix. Therefore, it is the building
    block in linear algebra, and we must start from there.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to create a vector. We will explore a few such options
    in the following exercise.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1 – creating a vector
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise will introduce three common ways of creating a vector. Let’s
    get started:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a vector of integers from 1 to 6 using the `c()` function, where `c`
    stands for concatenation:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: One more function we can use to create a sequence of integers is the `seq()`
    function, which generates a sequence of numbers equally spaced based on the `by`
    argument.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the same vector using `seq()`:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These three implementations create the same vector.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In addition, we can use the `rep()` function to create a list of repeated numbers.
    The following code snippet produces a 6-element vector whose data is all ones:'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A vector in R is mutable, meaning we can change the value in the vector’s specific
    position(s). Let’s look at how to achieve this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2 – modifying a vector
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modifying the contents of a vector is a common operation, especially in the
    context of processing a DataFrame using column-wise filtering conditions. The
    algebra of vectors follows mostly the same principle as scalars. To see this,
    we must first index the desired elements in the vector, followed by using an assignment
    operation to override the value of the specific element. Let’s take a look:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the value of the second element in the vector, `x`, to `20`:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, we can access an element in a vector by wrapping the position in a squared
    bracket. We can also perform bulk operations that exert the same effect on all
    elements in a vector.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Double all elements in the vector, `x`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, the multiplier, `2`, is broadcast to each element in the vector to perform
    the respective multiplication. The same broadcasting mechanism applies to the
    addition operation and more.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Increment all elements in `x` by one:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Of course, we can obtain the same result by adding `x` to `y`, a vector of
    ones with the same length:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we’ve reviewed the basics of working with vectors, we can enter the
    realm of matrices.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Working with matrices
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A matrix is a stack of vectors superimposed together. An m x n matrix can be
    considered n m-dimensional column vectors stacked horizontally, or m n-dimensional
    row vectors stacked vertically. For each of the DataFrames that we have worked
    with, each row is an observation, and each column represents a feature.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to create matrices by completing an exercise.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3 – creating a matrix
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `matrix()` function to create a matrix. This function accepts
    three arguments: the data to be passed into the matrix via `data`, the number
    of rows via `nrow`, and the number of columns via `ncol`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 3x2 matrix, `X`, filled with the number `2`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note the use of the broadcasting mechanism here. As we only pass in a single
    number, it is replicated across all the cells in the matrix. Also, observe the
    indexing pattern in the result. The position on the left indexes each row before
    the comma and each column by the position on the right after the comma. We can
    verify this observation by indexing the element located in the second row and
    the first column:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s verify the class of this object:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result shows that `X` is both a matrix and (multi-dimensional) array.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can also create a matrix based on a vector.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a 3x2 matrix using the `x` variable. Fill the matrix by rows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, filling by rows means sequentially placing each element in the original
    vector, `x`, row-wise, beginning from the first row from left to right and jumping
    to the next row when hitting the end of the current row. We can also design the
    filling by columns.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the matrix by columns instead:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As with the vector, we can change a specific element in a matrix by first locating
    the element via indexing and assigning the new value, as shown here:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A common operation in linear algebra is to multiply a matrix by a vector. Such
    an operation gives rise to many interesting and important interpretations regarding
    matrix manipulation. We’ll look at this operation in the following section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Matrix vector multiplication
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An overarching rule when multiplying a matrix by a vector is the equality of
    the inner dimension. In other words, the column dimension (meaning the number
    of columns) of the matrix, when multiplied by a vector on its right, has to be
    equal to the row dimension of the multiplying vector. For example, given an m
    x n matrix, it can only multiply a column vector of size n x 1, and such multiplication
    results in another vector of size m x 1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a concrete example here. Recall that X is a 3x2 matrix. Multiplying
    it by a 2x1 vector, y, should produce a 3x1 vector, z. The transition from the
    original vector, y, to the new vector, z, has an extra meaning here: y has changed
    space and now lives in a three-dimensional world instead of two! We can also say
    the old vector, y, is projected and stretched to the new vector, z, because of
    the projection matrix, X. Such a projection, or transformation, constitutes the
    majority of operations in modern neural networks. By projecting matrices (also
    called representations) across different layers, a neural network can *learn*
    different levels of abstraction.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume y is a vector of ones. The rule of matrix-vector multiplication
    says that each entry in the resulting vector, z, is the inner product of the corresponding
    vectors. The inner product between two vectors, also called the dot product, is
    the sum of the products of the corresponding elements in each vector. For example,
    the first entry in z is 12\. Its positional index, which is [1,1], says that it
    requires the first row in X, which is [10, 2], and the first column in y, which
    is [1, 1], to enter the inner product operation. Multiplying the corresponding
    elements in each vector and summing the results gives us 10*1+2*1=12\. Similarly,
    the second entry is calculated as 2*1+2*1=4\. *Figure 7**.2* summarizes this matrix-vector
    multiplication:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Illustrating the matrix-vector multiplication process](img/B18680_07_002.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Illustrating the matrix-vector multiplication process
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at how to perform matrix-vector multiplication in R.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.4 – applying matrix-vector multiplication
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The matrix-vector multiplication operation comes via the `%*%` symbol in R.
    The `%*%` notation is very different from the `*` notation alone, which stands
    for element-wise multiplication. The following exercise illustrates such a difference:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply the previous matrix, X, by a 2x1 vector of ones:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result shows that the generated vector is 3x1\. Note that the multiplication
    will not go through if the dimensions do not check:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The error says that the dimensions of the vector, `c(1,1,1)`, do not match those
    in the `X` matrix.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can verify the calculation of each cell in the resulting vector.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the first entry in the resulting vector separately:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result shows that the first entry is the dot product of the first row, `X[1,]`,
    and the first (and only) column, `c(1,1)`. We can also re-express the dot product
    as explicit element-wise multiplication and summation.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-express the previous dot product as explicit element-wise multiplication
    and summation:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that both results return the same value but assume a different data structure:
    the dot product operation returns a matrix, while the re-expressed operation returns
    a vector.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s verify the same calculation using the second entry in the resulting vector:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can also examine the behavior of multiplying a matrix with a scalar.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Double each element in the matrix, X:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that when switching to the element-wise multiplication symbol, `*`, the
    dimension of the resulting matrix remains unchanged. In addition, the broadcasting
    mechanism is in play here, where a scalar of 2 is multiplied by each element in
    the matrix, X. The effect is as if we’re multiplying X by another matrix of the
    same dimensions:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, let’s cover matrix-matrix multiplication. We will refer to it as matrix
    multiplication for short.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Matrix multiplication is the most widely used form of operation in many domains.
    Take the neural network model, for example. *Figure 7**.3* shows a simple network
    architecture called a fully connected neural network, where all neurons are fully
    connected. The input data, represented by the input layer with 10 rows and 3 columns,
    will enter a series of matrix multiplications (plus nonlinear transformations,
    which are ignored here) to learn useful representations (Z 1) and, therefore,
    accurate predictions (Z 2). There are two matrix multiplications here. The first
    matrix multiplication happens between the 10x3 input data, X, and the weight matrix
    W 1, which produces a 10x4 hidden representation, Z 1, in the hidden layer. The
    second matrix multiplication transforms Z 1 into the final 10x2 output, Z 2, using
    another 4x2 weight matrix, W 2\. We can also interpret a series of matrix multiplications
    as transforming/projecting the input data, X, to the hidden representation, Z 1,
    and then to the output, Z 2:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Schematic of a simple two-layer fully connected neural network](img/B18680_07_003.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Schematic of a simple two-layer fully connected neural network
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 7**.3*, the input data, represented by the input
    layer with 10 rows and 3 columns, will enter a series of matrix multiplications
    (plus nonlinear transformations, which are ignored here) to learn useful representations
    (Z 1) and, therefore, accurate predictions (Z 2). The first matrix multiplication
    happens between the 10x3 input data, X, and the weight matrix, W 1, resulting
    in a 10x4 hidden representation, Z 1\. The second matrix multiplication transforms
    Z 1 into the final 10x2 output, Z 2, using another 4x2 weight matrix, W 2.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: There is more to these matrix multiplications. Let’s assume the input data has
    many features. By applying these matrix multiplications with the automatically
    learned weight matrices, these features can be weighed correspondingly to produce
    accurate predictions in the output layer. Automatic feature learning, including
    those nodes in the hidden layer, is a distinguishing characteristic of modern
    neural networks compared to traditional manual feature learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a single matrix multiplication in more detail. *Figure 7**.4*
    illustrates the calculation process of multiplying a 2x2 matrix, [1 3 2 4], by
    another 2x2 matrix, [2 2 2 2]. We can also view each column in the resulting matrix
    as a result of matrix-vector multiplication, as covered earlier; these columns
    are then concatenated to form the new output matrix:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看单个矩阵乘法。*图7.4*说明了将2x2矩阵[1 3 2 4]与另一个2x2矩阵[2 2 2 2]相乘的计算过程。我们还可以将结果矩阵中的每一列视为前面提到的矩阵-向量乘法的结果；然后这些列被连接起来形成新的输出矩阵：
- en: '![Figure 7.4 – Breaking down the matrix multiplication process](img/B18680_07_004.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 矩阵乘法过程的分解](img/B18680_07_004.jpg)'
- en: Figure 7.4 – Breaking down the matrix multiplication process
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 矩阵乘法过程的分解
- en: Let’s go through an exercise to put the practical part of things into context.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来将实际部分的内容置于上下文中。
- en: Exercise 7.5 – working with matrix multiplication
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.5 – 矩阵乘法操作
- en: 'Matrix multiplication still relies on the `%*%` sign in R. In this exercise,
    we will reproduce the example in *Figure 7**.4*, with a bit of stretch in terms
    of the order of multiplication:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，矩阵乘法仍然依赖于`%*%`符号。在这个练习中，我们将重新生成*图7.4*中的示例，在乘法顺序上略有扩展：
- en: 'Reproduce the previous matrix multiplication example:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新生成之前的矩阵乘法示例：
- en: '[PRE21]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, we use the `matrix()` function to create two matrices, the first generated
    by converting a vector and the second by duplicating a scalar value via broadcasting.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`matrix()`函数创建了两个矩阵，第一个是通过转换向量生成的，第二个是通过广播重复标量值生成的。
- en: The order of multiplication is of key importance in matrix algebra; shuffling
    the order will produce a different result in most cases. Let me switch these two
    matrices.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在矩阵代数中，乘法的顺序至关重要；改变顺序在大多数情况下会产生不同的结果。让我交换这两个矩阵。
- en: 'Switch the order of multiplication for these two matrices:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交换这两个矩阵的乘法顺序：
- en: '[PRE22]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Please verify the result of the matrix multiplication and appreciate the importance
    of order in multiplying two matrices: matrix multiplication is *not* commutative.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请验证矩阵乘法的结果，并欣赏乘法顺序在两个矩阵乘法中的重要性：矩阵乘法*不是*交换的。
- en: Additionally, element-wise multiplication happens using the `*` operator.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，元素级联乘法是通过`*`运算符进行的。
- en: 'Double every element in the previous matrix on the left:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将左侧前一个矩阵中的每个元素都乘以2：
- en: '[PRE23]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Several special matrices are of particular interest. We’ll review them in the
    next section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 几种特殊的矩阵特别引人关注。我们将在下一节中回顾它们。
- en: The identity matrix
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单位矩阵
- en: There are many special types of matrices. The first type of special matrix is
    the **identity matrix**, which assumes a value of 1 at the on-diagonal positions
    and 0 elsewhere. The biggest characteristic of an identity matrix is the identity
    preservation of the original multiplying matrix – that is, any matrix that can
    successfully multiply by an identity matrix obtains the same result as itself.
    This sounds like multiplying by one, and we are indeed doing so when the identity
    matrix is 1x1.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵有很多特殊类型。第一种特殊矩阵是**单位矩阵**，其对角线位置上的值为1，其他位置为0。单位矩阵的最大特点是保持原始乘法矩阵的恒等性——也就是说，任何能够成功与单位矩阵相乘的矩阵都会得到与自身相同的结果。这听起来像是乘以1，而当我们处理1x1的单位矩阵时，我们确实是这样做的。
- en: Exercise 7.6 – working with the identity matrix
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.6 – 单位矩阵操作
- en: 'Let’s go through an exercise to see how identity matrices work:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来看看单位矩阵是如何工作的：
- en: 'Create a 2x2 identity matrix using the `diag()` function:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`diag()`函数创建一个2x2的单位矩阵：
- en: '[PRE24]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To create an identity matrix, we simply need to pass the number of ones in
    the diagonal of the matrix. A diagonal matrix can also be created from a vector:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要创建一个单位矩阵，我们只需要传递矩阵对角线上的1的数量。对角矩阵也可以从一个向量中创建：
- en: '[PRE25]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, the elements in the vector are used to fill in the diagonal positions
    of the diagonal matrix while leaving all the rest of the cells as 0.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，向量中的元素用于填充对角矩阵的对角线位置，而将所有其他单元格保留为0。
- en: Let’s multiply the 2x2 identity matrix by the previous running matrix.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们将2x2的单位矩阵与之前的运行矩阵相乘。
- en: 'Multiply this identity matrix by the previous 2x2 matrix numbered 1 to 4:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个单位矩阵与之前编号为1到4的2x2矩阵相乘：
- en: '[PRE26]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can see that there is no change in the resulting matrix. Let’s verify this
    again:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到结果矩阵没有发生变化。让我们再次验证这一点：
- en: '[PRE27]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Lastly, let’s verify that the result is still the same after switching the order
    of multiplication.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，让我们验证在交换乘法顺序后结果是否仍然相同。
- en: 'Move the multiplying identity matrix to the left and perform matrix multiplication:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The result shows no change to the original matrix. If we use X to denote the
    original matrix and I to denote the identity matrix, we will get XI = IX = X.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Other operations can derive new matrices based on the original one, such as
    transposing or inversing a matrix.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Transposing a matrix
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already have some experience transposing a vector, which is a special case
    of transposing a matrix. Transposing a matrix means flipping the original matrix,
    X, to generate a new one, X T, whose columns and rows are now switched. Transposing
    a transposed matrix, X T, gives back the original matrix, (X T) T = X.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: A special type of matrix is called the **symmetric matrix**, such that X T =
    X. A symmetric matrix is also a **square matrix**; otherwise, the transposed dimensions
    will not check.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see this in practice.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.7 – transposing a matrix
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will transpose a matrix using the `t()` function and transpose
    it again to see whether it matches the original matrix:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a square matrix, `X`, with elements one to four filled by column:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Transpose the matrix using `t()`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we can see that the transposed matrix has its rows and columns flipped
    from the original matrix. The diagonal elements stay the same, though.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transpose the transposed matrix again and verify whether it is equal to the
    original matrix:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'A visual inspection shows that it is indeed the same original matrix. However,
    we can also use the built-in `all.equal()` function to perform a systematic check:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The result shows that these two matrices are equal to each other, thus being
    the same matrix.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s review the last type of operation: inversing a matrix.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Inverting a matrix
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inversing a scalar number is intuitive. For any number, x, its inverse (or reciprocal)
    is 1 _ x  if x ≠ 0\. This condition ensures that x is invertible. Similarly, not
    all matrices are invertible.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we say that a matrix, X, is invertible if it can, when multiplied
    by its inverse, X −1, produce an identity matrix, I. In other words, X X −1 =
    X −1 X = I.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some interesting properties of an invertible matrix as well. For
    example, inverting an inverse matrix gives us the original matrix: (X −1) −1 =
    X. Also, since the product of an identity matrix with itself is an identity matrix,
    the inverse of an identity matrix is thus the identity matrix itself, a special
    case compared to others.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The inverse of a matrix can be obtained using the `solve()` function in R, which
    will give us an error if the matrix is not invertible. Let’s practice this via
    the following exercise.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.8 – inverting a matrix
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will invert both an identity matrix and a standard square
    matrix using the `solve()` function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Invert a two-dimensional identity matrix:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The result shows that the inverse of an identity matrix is itself.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Invert the running matrix, X, from the previous example:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can verify the validity of this inverse matrix by multiplying it by the original
    matrix and checking whether it gives us an identity matrix.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply by the original matrix to verify it based on the definition of matrix
    inversion:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The result shows that `Xinv` is indeed the inverse matrix of `X`.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following section, we will look at matrix-vector multiplication from
    the perspective of solving a system of linear equations, which is an essential
    task in many machine learning algorithms.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Solving a system of linear equations
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The matrix-vector multiplication operation gives rise to a system of equations.
    In a typical machine learning algorithm, data comes in the form of a matrix, X,
    and the target outcome is a vector, y. When the model that’s used is a straightforward
    linear model, we assume the input-output relationship as Xw = y, where w represents
    the vector of features/coefficients. An n x p matrix of input data multiplies
    a p x 1 vector, w, of features to produce, as expected, an n x 1 output vector,
    y. The essence of linear regress is thus to solve for the exact values in w such
    that the system of linear equations in Xw = y are satisfied.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The equivalence between matrix-vector multiplication and the system of linear
    equations may take some time to become noticeable. Let’s pause and look at this
    equivalence.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: System of linear equations
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are already familiar with the process of calculating a matrix-vector multiplication
    operation. A 2x2 matrix, X, when multiplying a 2x1 vector, w, will result in a
    2x1 vector, y. The first element in y, positioned as (1, 1), comes from the dot
    product (weighted sum) between the first row in X and the first column in y. Similarly,
    the second element in y, positioned as (2, 1), comes from the dot product between
    the second row in X and the first column in y. The positional index of each element
    in y determines the ingredients used in the respective dot product operation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Such an interpretation, however, is a raw and low-level one. A more advanced
    interpretation lies in the column space of X and the associated linear combination
    of the column vectors in X weighted by the weights in w in the same matrix-vector
    multiplication. A concrete example will help here.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a simple matrix, X = [1 3 2 4], and a vector, w T = [1,1].
    The output vector is y = [1 * 1 + 3 * 1 2 * 1 + 4 * 1] = [4 6] based on the usual
    method of calculation. The column view gives another process of calculation: y
    = 1 *[1 2] + 1 *[3 4] = [1 2] + [3 4] = [4 6]. Here, the two columns, [1 2] and
    [3 4], are weighted by each element in w, respectively. In other words, these
    two columns are linearly combined to produce the output column vector, y. This
    forms the basis of the system of linear equations. *Figure 7**.5* summarizes these
    two different perspectives of matrix-vector multiplication for this example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Illustrating the two different ways of calculating matrix-vector
    multiplication](img/B18680_07_005.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Illustrating the two different ways of calculating matrix-vector
    multiplication
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Now, assume the weight vector, w, is unknown, with two unknown elements, w 1
    and w 2 – in other words, w T = [ w 1, w 2]. Also, assume that the input matrix,
    X, and the output vector, y, are known. This is a common situation where we are
    given the input-output data pairs and asked to estimate the coefficients of the
    linear model such that Xw = y. Now comes the system of linear equations. Plugging
    in the previous weighted combination of column vectors gives us
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'w 1 *[1 2] + w 2 *[3 4] = [ w 1 2 w 1] + [3 w 2 4 w 2] = [ w 1 + 3 w 2 2 w 1
    + 4 w 2] = [4 6]. We have a system of equations, as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  2 w 1 + 4 w 2 = 6'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Solving this system of linear equations gives us w 1 = w 2 = 1\. The equivalence
    of matrix-vector multiplication and the system of linear equations is manifested
    here.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to appreciate the equivalence in terms of code.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.9 – understanding the system of linear equations
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will first use the usual matrix-vector multiplication
    procedure to calculate the result, followed by providing a detailed and manual
    column-view implementation of the same operation. We will see that both give the
    same result:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 2x2 matrix, `X`, that consists of four numbers, 1 to 4, and is filled
    by column:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a vector, `w`, of length two, both filled with a value of 1:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that a vector is a column vector by default. When the 2x2 matrix, `X`,
    multiplies the 2x1 vector, `w`, as shown in the following code, the vector, `w`,
    is expressed as a column vector before entering the multiplication operation.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply `X` and `w` and save the result in `y`:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, the `y` vector is displayed as an explicit column vector. Lastly, let’s
    verify the calculation using the column view.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform the same matrix-vector multiplication using the column view, meaning
    a weighted sum of column vectors:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, we use `X[,1]` and `X[,2]` to access the first and second columns, respectively.
    The result agrees with the previous one, although following a different format
    as it is displayed as a row vector now.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Matrix-vector multiplication gives rise to a system of linear equations. However,
    this system of linear equations may or may not have a solution. Even if there
    is a solution, it may not be unique. In the following section, we will examine
    the potential solution to matrix-vector equations.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The solution to matrix-vector equations
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s continue with the example where a 2x2 matrix, X, multiplies a 2x1 weight
    vector, w, to produce a 2x1 output vector, y. We will discuss three cases here:
    without a solution, with a unique solution, and with infinitely many solutions.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the second case. Recall our previous values in X, w, and
    y. In [1 3 2 4] x [1 1] = [4 6], we multiplied the former two (X and w) to produce
    the third one (y) and showed how to derive the values of w given X and y by solving
    a system of linear equations in [1 3 2 4] x [w 1 w 2] = [4 6]. In other words,
    the solution is unique, and we can solve it analytically by writing out the explicit
    system of linear equations and solving the unknowns. It will be more involved
    when we have more unknowns to solve, but the process stays the same. In a typical
    linear regression setting, when the number of unknowns (the length of the weight
    vector, w) is the same as the number of equations (the number of rows in X), and
    provided that the input matrix, X, is a nice one (for example, if its columns
    are uncorrelated), we can find a unique solution to the system of linear equations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s tweak the input matrix, X, so that no solution can be found. One
    way to do this is to set the second row vector of X to zero, giving us X = [1 3 0 0].
    Thus, we have the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 0 0]x[w 1 w 2] = [4 6]'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '{w 1 + 3 w 2 = 4 0 = 6'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: It is obvious that the second equation fails, and therefore this system of linear
    equations cannot have a solution. This is called an *inconsistent* system since
    0 cannot be 6\. Note that in the first equation, there are infinitely many pairs
    of ( w 1, w 2) that satisfy the equation. This also leads to our third case.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose we change the output vector slightly and make the second entry
    of y zero, giving us y = [4 0]. Thus, we have the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 0 0]x[w 1 w 2] = [4 0]'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '{w 1 + 3 w 2 = 4 0 = 0'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Now, this system of linear equations has at least one solution and is *consistent*,
    but with infinitely many solutions. There is not much we can take away from solving
    this system of equations; since there are infinitely many solutions, we are unable
    to evaluate which one is the best to report. Real-world optimization problems
    are typically concerned with finding *the* single most optimal solution or the
    best sub-optimal solution that is closest to the empirically unattainable optimal
    solution.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.6* summarizes these three cases:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Three different cases when solving a system of linear equations](img/B18680_07_006.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Three different cases when solving a system of linear equations
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that we can also view the process of solving a system of linear
    equations from a geometric perspective. Let’s dive in.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Geometric interpretation of solving a system of linear equations
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, let’s start with the case where we have a unique solution for the
    system of linear equations with two unknowns (w 1 and w 2) and two equations:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 2 4] x [w 1 w 2] = [4 6]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  2 w 1 + 4 w 2 = 6'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'If we introduce a two-dimensional coordinate system and put w 1 and w 2 on
    the *x* axis and *y* axis, respectively, we will see two lines on the coordinate
    system, each representing a linear equation in the system. Now, we can re-express
    the previous system of equations as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '{w 2 = −  w 1 _ 3  +  4 _ 3   w 2 = −  w 1 _ 2  +  3 _ 2'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, solving the system of linear equations corresponds with finding
    the intersection point of these two lines, since it is only at the intersection
    point that both equations are satisfied. The following codes help us plot the
    two lines:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, we use the `plot()` function to draw a two-dimensional coordinate system
    with a circle representing the point (1,1). We also add two lines using the `abline()`
    function, which accepts the intercept and slope of the line as input arguments.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Running this code generates *Figure 7**.7*. We can see that these two lines
    happen to meet at point (1,1), which is not a coincidence!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Plotting the system of linear equations as intersecting lines](img/B18680_07_007.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Plotting the system of linear equations as intersecting lines
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the case with no solution for the system of equations.
    The previous example shows an inconsistent system of equations, where the second
    equation simply does not stand. With the geometric interpretation in the coordinate
    system, we can make another line parallel to the first one, such as by shifting
    the line up by one unit, to produce a case with no solution. Specifically, we
    can have the following system of equations:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  w 1 + 3 (w 2 − 1) = 4'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we minus 1 from w 2 to shift the line upward by one unit. Again, we can
    express these equations as a function of w 1:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '{w 2 = −  w 1 _ 3  +  4 _ 3   w 2 = −  w 1 _ 3  +  7 _ 3'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot these two lines on the coordinate system via the following code.
    The only change is the bigger intercept in the second line compared to the first
    one:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Running this code generates *Figure 7**.8*. Since these two lines are parallel,
    there will be no intersection between the two lines, so we’ll end up with no solution
    to the system of linear equations:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Plotting two parallel lines](img/B18680_07_008.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Plotting two parallel lines
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to the case with infinitely many solutions. As you may have
    guessed, we just need to make the second equation the same as the first one, thus
    creating two overlapping lines in the coordinate system. Any point on these two
    overlapping lines is a valid solution, and there is an infinite number of such
    points on the line(s).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Working with a system of linear equations may not guarantee a solution. Even
    if there are as many unknown variables as there are rows in the system, this isn’t
    guaranteed to give us a solution. In the following section, we’ll look at under
    what conditions we are guaranteed to get a unique solution.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a unique solution to a system of linear equations
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall our analytical framework: Xw = y, where we are given input-output pairs
    (X, y) and would like to solve the unknown vector, w. If things were simple and
    all these variables were scalars, we would simply divide both sides by X and obtain
    the solution, w = X −1 y, provided that X is invertible. Note that we would multiply
    both sides by X −1 on the left to perform the division. This brings us to the
    first condition: the matrix, X, needs to have a corresponding inverse matrix;
    that is, it is invertible.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.9* illustrates the correspondence between simple scalar calculations
    in regular algebra and matrix manipulations in matrix algebra. Note that by definition
    of matrix inverse, we have X −1 X = I. We used this result in the derivation.
    The identity matrix is special; any matrix multiplying an identity matrix will
    stay unchanged. Thus, we have Iw = w. Again, we assume the matrix, X, is invertible:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Comparing scalar and matrix manipulations when solving the system
    of linear equations](img/B18680_07_009.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Comparing scalar and matrix manipulations when solving the system
    of linear equations
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The second condition is that the *determinant* of the matrix, X, cannot be
    zero. The determinant is a summary measure of the size of the matrix; we will
    discuss this in more detail in the next chapter. The third condition is what we
    alluded to earlier: the rows and columns of the matrix, X, can form a *basis*
    for the corresponding row and column space. There should be no correlated rows
    or columns in the matrix. A basis of a vector space is a set of vectors that can
    uniquely generate all the other vectors in the same vector space via linear combinations.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we would need a somewhat trivial condition: when Xw = 0, we have
    w = 0.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the inverse and determinant of a matrix is straightforward in R.
    In the following code, we are calculating the inverse of the matrix, X, using
    the `solve()` function and its determinant using the `det()` function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The next chapter will touch on the matrix determinant, norm, trace, and other
    special properties in more detail.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Now that we can calculate the matrix inverse, let’s learn how to obtain the
    solution to the system of linear equations using the inverse of the input matrix,
    X, and output, vector y.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.10 – obtaining the unique solution to a system of linear equations
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will make use of the matrix inverse to obtain the solution
    to a system of linear equations:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct the input matrix, `X`, and output vector, `y`, based on the previous
    example:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that the data we chose here is based on the previous running example, where
    the solution to the unknown variables is w = [1,1].
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the solution to the system of linear equations:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As we can see, the solution, which can be called the coefficient vector, matches
    the true result exactly. We can also multiply the input matrix by this solution
    to obtain an estimated output and check whether it is the same as the given output
    vector.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the output via matrix-vector multiplication between the input matrix
    and the estimated coefficient vector:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The result matches the given output vector.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solving a system of linear equations with a square input matrix is a simple
    setting. When working with real data, the input matrix will likely be non-square,
    with more rows or columns. When there are more rows than columns, we have a limited
    number of unknown variables that need to satisfy more equations. This is an **overdetermined
    system**. On the other hand, when there are more unknown variables to satisfy
    fewer equations, we have an **underdetermined system**. We will discuss these
    two situations in the following section.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Overdetermined and underdetermined systems of linear equations
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example we covered earlier, which consisted of two equations and two unknown
    variables to solve, gives a solution that passes through both lines on the coordinate
    system. This is called **interpolation**, where the solution perfectly satisfies
    both equations without any errors. In the language of machine learning, interpolation
    means the trained model can score 100% on the training dataset. This is not necessarily
    a good thing since the model will likely run into the risk of overfitting – that
    is, it will do pretty well on the training set but not so well on the test set.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Complex models such as neural networks and other nonlinear models tend to have
    very low or even zero training errors – that is, they are likely to interpolate
    the training data. The zero training cost occurs when the model becomes sufficiently
    complex to interpolate the training data. Perfect prediction in the training data
    is likely to happen when the number of coefficients, p, used by the model is equal
    to or larger than the number of observations, n, in the training input. In the
    case of linear regression, when p = n, we solve a system of linear equations where
    the number of free variables, whose optimal values we are solving for, is equal
    to the number of linear equations in the system.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: A simple example is a two-dimensional training set with two observation points
    on a coordinate system, as we saw earlier. When only one parameter is available,
    the model is reflected as a straight horizontal line in the coordinate system.
    This univariate model is too simple to fit these two points unless they happen
    to live along the fitted line. The resulting system of equations is said to be
    overdetermined and under-parameterized since we have more linear equations than
    the unknown variables in the system.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have two parameters to fit the model, the problem can be solved with
    an exact solution: two linear equations and two unknowns, a straightforward exercise.
    We can fit a line anywhere in the coordinate system by adjusting its intercept
    and slope. So, the problem becomes fitting a line that passes through the two
    points. We can train a model that passes through the two points and produces zero
    training error, thus interpolating between these two points.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'When there are more than two unknown variables – say we have three weights
    in the model – the problem becomes underdetermined and over-parameterized. The
    solution, when solvable, will be infinite. Having more than two parameters corresponds
    to fitting two points with a curved line. Any curve that passes through the two
    points produces zero training error, yet the curve can be arbitrarily wiggly and
    complex, depending on the number of parameters used. *Figure 7**.10* summarizes
    these three scenarios:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Three scenarios of model complexity](img/B18680_07_010.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Three scenarios of model complexity
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.10* shows fitting two two-dimensional observations using different
    model complexities. The left plot contains a 0th degree polynomial model that
    contains only one parameter, thus being a horizontal line. The middle plot has
    an equal number of parameters to the number of observations, making the solution
    unique and exact. The right plot contains models with more than two parameters,
    where the solution is non-unique due to being an underdetermined system.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Perfect interpolation occurs when p = n – that is, the number of features is
    equal to the number of observations. The perfect interpolation continues with
    p > n when the system of equations becomes underdetermined and over-parameterized,
    leading to infinitely many solutions that correspond to models of an arbitrary
    shape. All these models pass through the observed data points and thus give zero
    training error.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the basics of linear algebra, including working
    with vectors and matrices and performing matrix-vector multiplication. We highlighted
    a few special matrices, such as the identity matrix, and common operations, such
    as transposing and inverting a matrix.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used matrix-vector multiplication to solve a system of linear equations
    under different settings. We introduced the geometric interpretation that corresponds
    to the system of linear equations, along with how to obtain the solution using
    matrix inverse and multiplication operations.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we touched upon common settings of the input matrix in the machine learning
    context, covering both underdetermined and overdetermined systems. Developing
    such an understanding will be crucial when we delve into statistical modeling
    and machine learning in the third part of this book.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss slightly more advanced concepts in matrix
    algebra and implementations in R.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
