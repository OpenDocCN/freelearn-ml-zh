- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Algebra in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter introduced an efficient and effective reporting approach
    using R Markdown. *Part 1* of this book essentially covered the practical aspects
    of getting things done using R. *Part 2* of this book goes back to the fundamentals,
    covering two essential pillars of mathematics: linear algebra and calculus. Understanding
    these basics will better prepare us to appreciate and work with common mathematical
    operations to the point that these operations feel natural to us. *Part 2* aims
    to develop that level of literacy, starting with a fundamental review of linear
    algebra with R in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the basic concepts of
    linear algebra, including vectors, matrices, and the system of equations. You
    will also be able to interpret basic notations in linear algebra and work with
    common matrices using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear algebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common matrix operations and properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving system of linear equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_7/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_7/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter delves into one of the most important branches of mathematics:
    **linear algebra**. Linear algebra deals with linear operations of mathematical
    objects, including vectors, matrices, and tensors (high-dimensional matrices),
    the most common forms of data. For example, the typical table we use to store
    data in Excel consists of a series of columns. Each column is called a vector,
    which stores a specific number of elements and takes the form of a column instead
    of a row by default. A collection of these column vectors forms a matrix, a two-dimensional
    Excel table, or DataFrame, as we used to call it in the previous chapters. We
    can also view the same table as a collection of row vectors, where each vector
    lives in the form of a row.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put these in context. The following code snippet loads the sleep dataset
    and prints out the first six rows and three columns. We use A to denote this 6x3
    matrix in the following exposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s introduce some notation here. We use a bold lowercase letter, a, to denote
    a column vector, say the `extra` variable, which consists of six elements in the
    vector. In other words, a is a six-dimensional vector. The column vector, a, can
    be *transposed* into a row vector, denoting a T. Since a T is a row vector, we
    can write a T = [0.7, − 1.6, − 0.2, − 1.2, − 0.1, 3.4], which is also the typical
    way of writing row vectors (or transposed column vectors) in many books. Similarly,
    a row vector can be transposed into the (original) column vector, giving us (a T) T
    = a.
  prefs: []
  type: TYPE_NORMAL
- en: 'A graphical illustration will help here. *Figure 7**.1* depicts the process
    of transposing the column vector, a, into a row vector, a T, then transposing
    it again into the original column vector, (a T) T = a. Therefore, we can view
    the matrix, A, as a collection of column vectors concatenated horizontally, or
    a collection of row vectors concatenated vertically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Transposing the column vector into a row vector, and then transposing
    it again into the original column vector](img/B18680_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Transposing the column vector into a row vector, and then transposing
    it again into the original column vector
  prefs: []
  type: TYPE_NORMAL
- en: Note that the rows are also referred to as observations and that the columns
    are referred to as features or attributes.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by working with vectors in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Working with vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector in R chains together data elements of the same type. Multiple vectors
    join hands, often side by side, to form a matrix. Therefore, it is the building
    block in linear algebra, and we must start from there.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to create a vector. We will explore a few such options
    in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1 – creating a vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise will introduce three common ways of creating a vector. Let’s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a vector of integers from 1 to 6 using the `c()` function, where `c`
    stands for concatenation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: One more function we can use to create a sequence of integers is the `seq()`
    function, which generates a sequence of numbers equally spaced based on the `by`
    argument.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the same vector using `seq()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These three implementations create the same vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In addition, we can use the `rep()` function to create a list of repeated numbers.
    The following code snippet produces a 6-element vector whose data is all ones:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A vector in R is mutable, meaning we can change the value in the vector’s specific
    position(s). Let’s look at how to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2 – modifying a vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modifying the contents of a vector is a common operation, especially in the
    context of processing a DataFrame using column-wise filtering conditions. The
    algebra of vectors follows mostly the same principle as scalars. To see this,
    we must first index the desired elements in the vector, followed by using an assignment
    operation to override the value of the specific element. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the value of the second element in the vector, `x`, to `20`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, we can access an element in a vector by wrapping the position in a squared
    bracket. We can also perform bulk operations that exert the same effect on all
    elements in a vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Double all elements in the vector, `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the multiplier, `2`, is broadcast to each element in the vector to perform
    the respective multiplication. The same broadcasting mechanism applies to the
    addition operation and more.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Increment all elements in `x` by one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Of course, we can obtain the same result by adding `x` to `y`, a vector of
    ones with the same length:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve reviewed the basics of working with vectors, we can enter the
    realm of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Working with matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A matrix is a stack of vectors superimposed together. An m x n matrix can be
    considered n m-dimensional column vectors stacked horizontally, or m n-dimensional
    row vectors stacked vertically. For each of the DataFrames that we have worked
    with, each row is an observation, and each column represents a feature.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to create matrices by completing an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3 – creating a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `matrix()` function to create a matrix. This function accepts
    three arguments: the data to be passed into the matrix via `data`, the number
    of rows via `nrow`, and the number of columns via `ncol`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 3x2 matrix, `X`, filled with the number `2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note the use of the broadcasting mechanism here. As we only pass in a single
    number, it is replicated across all the cells in the matrix. Also, observe the
    indexing pattern in the result. The position on the left indexes each row before
    the comma and each column by the position on the right after the comma. We can
    verify this observation by indexing the element located in the second row and
    the first column:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s verify the class of this object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that `X` is both a matrix and (multi-dimensional) array.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can also create a matrix based on a vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a 3x2 matrix using the `x` variable. Fill the matrix by rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, filling by rows means sequentially placing each element in the original
    vector, `x`, row-wise, beginning from the first row from left to right and jumping
    to the next row when hitting the end of the current row. We can also design the
    filling by columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the matrix by columns instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As with the vector, we can change a specific element in a matrix by first locating
    the element via indexing and assigning the new value, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A common operation in linear algebra is to multiply a matrix by a vector. Such
    an operation gives rise to many interesting and important interpretations regarding
    matrix manipulation. We’ll look at this operation in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix vector multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An overarching rule when multiplying a matrix by a vector is the equality of
    the inner dimension. In other words, the column dimension (meaning the number
    of columns) of the matrix, when multiplied by a vector on its right, has to be
    equal to the row dimension of the multiplying vector. For example, given an m
    x n matrix, it can only multiply a column vector of size n x 1, and such multiplication
    results in another vector of size m x 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a concrete example here. Recall that X is a 3x2 matrix. Multiplying
    it by a 2x1 vector, y, should produce a 3x1 vector, z. The transition from the
    original vector, y, to the new vector, z, has an extra meaning here: y has changed
    space and now lives in a three-dimensional world instead of two! We can also say
    the old vector, y, is projected and stretched to the new vector, z, because of
    the projection matrix, X. Such a projection, or transformation, constitutes the
    majority of operations in modern neural networks. By projecting matrices (also
    called representations) across different layers, a neural network can *learn*
    different levels of abstraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume y is a vector of ones. The rule of matrix-vector multiplication
    says that each entry in the resulting vector, z, is the inner product of the corresponding
    vectors. The inner product between two vectors, also called the dot product, is
    the sum of the products of the corresponding elements in each vector. For example,
    the first entry in z is 12\. Its positional index, which is [1,1], says that it
    requires the first row in X, which is [10, 2], and the first column in y, which
    is [1, 1], to enter the inner product operation. Multiplying the corresponding
    elements in each vector and summing the results gives us 10*1+2*1=12\. Similarly,
    the second entry is calculated as 2*1+2*1=4\. *Figure 7**.2* summarizes this matrix-vector
    multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Illustrating the matrix-vector multiplication process](img/B18680_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Illustrating the matrix-vector multiplication process
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at how to perform matrix-vector multiplication in R.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.4 – applying matrix-vector multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The matrix-vector multiplication operation comes via the `%*%` symbol in R.
    The `%*%` notation is very different from the `*` notation alone, which stands
    for element-wise multiplication. The following exercise illustrates such a difference:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply the previous matrix, X, by a 2x1 vector of ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result shows that the generated vector is 3x1\. Note that the multiplication
    will not go through if the dimensions do not check:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The error says that the dimensions of the vector, `c(1,1,1)`, do not match those
    in the `X` matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can verify the calculation of each cell in the resulting vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the first entry in the resulting vector separately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the first entry is the dot product of the first row, `X[1,]`,
    and the first (and only) column, `c(1,1)`. We can also re-express the dot product
    as explicit element-wise multiplication and summation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-express the previous dot product as explicit element-wise multiplication
    and summation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that both results return the same value but assume a different data structure:
    the dot product operation returns a matrix, while the re-expressed operation returns
    a vector.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s verify the same calculation using the second entry in the resulting vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also examine the behavior of multiplying a matrix with a scalar.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Double each element in the matrix, X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that when switching to the element-wise multiplication symbol, `*`, the
    dimension of the resulting matrix remains unchanged. In addition, the broadcasting
    mechanism is in play here, where a scalar of 2 is multiplied by each element in
    the matrix, X. The effect is as if we’re multiplying X by another matrix of the
    same dimensions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s cover matrix-matrix multiplication. We will refer to it as matrix
    multiplication for short.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Matrix multiplication is the most widely used form of operation in many domains.
    Take the neural network model, for example. *Figure 7**.3* shows a simple network
    architecture called a fully connected neural network, where all neurons are fully
    connected. The input data, represented by the input layer with 10 rows and 3 columns,
    will enter a series of matrix multiplications (plus nonlinear transformations,
    which are ignored here) to learn useful representations (Z 1) and, therefore,
    accurate predictions (Z 2). There are two matrix multiplications here. The first
    matrix multiplication happens between the 10x3 input data, X, and the weight matrix
    W 1, which produces a 10x4 hidden representation, Z 1, in the hidden layer. The
    second matrix multiplication transforms Z 1 into the final 10x2 output, Z 2, using
    another 4x2 weight matrix, W 2\. We can also interpret a series of matrix multiplications
    as transforming/projecting the input data, X, to the hidden representation, Z 1,
    and then to the output, Z 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Schematic of a simple two-layer fully connected neural network](img/B18680_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Schematic of a simple two-layer fully connected neural network
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 7**.3*, the input data, represented by the input
    layer with 10 rows and 3 columns, will enter a series of matrix multiplications
    (plus nonlinear transformations, which are ignored here) to learn useful representations
    (Z 1) and, therefore, accurate predictions (Z 2). The first matrix multiplication
    happens between the 10x3 input data, X, and the weight matrix, W 1, resulting
    in a 10x4 hidden representation, Z 1\. The second matrix multiplication transforms
    Z 1 into the final 10x2 output, Z 2, using another 4x2 weight matrix, W 2.
  prefs: []
  type: TYPE_NORMAL
- en: There is more to these matrix multiplications. Let’s assume the input data has
    many features. By applying these matrix multiplications with the automatically
    learned weight matrices, these features can be weighed correspondingly to produce
    accurate predictions in the output layer. Automatic feature learning, including
    those nodes in the hidden layer, is a distinguishing characteristic of modern
    neural networks compared to traditional manual feature learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a single matrix multiplication in more detail. *Figure 7**.4*
    illustrates the calculation process of multiplying a 2x2 matrix, [1 3 2 4], by
    another 2x2 matrix, [2 2 2 2]. We can also view each column in the resulting matrix
    as a result of matrix-vector multiplication, as covered earlier; these columns
    are then concatenated to form the new output matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Breaking down the matrix multiplication process](img/B18680_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Breaking down the matrix multiplication process
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to put the practical part of things into context.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.5 – working with matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Matrix multiplication still relies on the `%*%` sign in R. In this exercise,
    we will reproduce the example in *Figure 7**.4*, with a bit of stretch in terms
    of the order of multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reproduce the previous matrix multiplication example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `matrix()` function to create two matrices, the first generated
    by converting a vector and the second by duplicating a scalar value via broadcasting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The order of multiplication is of key importance in matrix algebra; shuffling
    the order will produce a different result in most cases. Let me switch these two
    matrices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Switch the order of multiplication for these two matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Please verify the result of the matrix multiplication and appreciate the importance
    of order in multiplying two matrices: matrix multiplication is *not* commutative.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, element-wise multiplication happens using the `*` operator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Double every element in the previous matrix on the left:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Several special matrices are of particular interest. We’ll review them in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: The identity matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many special types of matrices. The first type of special matrix is
    the **identity matrix**, which assumes a value of 1 at the on-diagonal positions
    and 0 elsewhere. The biggest characteristic of an identity matrix is the identity
    preservation of the original multiplying matrix – that is, any matrix that can
    successfully multiply by an identity matrix obtains the same result as itself.
    This sounds like multiplying by one, and we are indeed doing so when the identity
    matrix is 1x1.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.6 – working with the identity matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s go through an exercise to see how identity matrices work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 2x2 identity matrix using the `diag()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create an identity matrix, we simply need to pass the number of ones in
    the diagonal of the matrix. A diagonal matrix can also be created from a vector:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the elements in the vector are used to fill in the diagonal positions
    of the diagonal matrix while leaving all the rest of the cells as 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s multiply the 2x2 identity matrix by the previous running matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply this identity matrix by the previous 2x2 matrix numbered 1 to 4:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that there is no change in the resulting matrix. Let’s verify this
    again:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Lastly, let’s verify that the result is still the same after switching the order
    of multiplication.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Move the multiplying identity matrix to the left and perform matrix multiplication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows no change to the original matrix. If we use X to denote the
    original matrix and I to denote the identity matrix, we will get XI = IX = X.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Other operations can derive new matrices based on the original one, such as
    transposing or inversing a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Transposing a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already have some experience transposing a vector, which is a special case
    of transposing a matrix. Transposing a matrix means flipping the original matrix,
    X, to generate a new one, X T, whose columns and rows are now switched. Transposing
    a transposed matrix, X T, gives back the original matrix, (X T) T = X.
  prefs: []
  type: TYPE_NORMAL
- en: A special type of matrix is called the **symmetric matrix**, such that X T =
    X. A symmetric matrix is also a **square matrix**; otherwise, the transposed dimensions
    will not check.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see this in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.7 – transposing a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will transpose a matrix using the `t()` function and transpose
    it again to see whether it matches the original matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a square matrix, `X`, with elements one to four filled by column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transpose the matrix using `t()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that the transposed matrix has its rows and columns flipped
    from the original matrix. The diagonal elements stay the same, though.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transpose the transposed matrix again and verify whether it is equal to the
    original matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A visual inspection shows that it is indeed the same original matrix. However,
    we can also use the built-in `all.equal()` function to perform a systematic check:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that these two matrices are equal to each other, thus being
    the same matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s review the last type of operation: inversing a matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Inverting a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inversing a scalar number is intuitive. For any number, x, its inverse (or reciprocal)
    is 1 _ x  if x ≠ 0\. This condition ensures that x is invertible. Similarly, not
    all matrices are invertible.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we say that a matrix, X, is invertible if it can, when multiplied
    by its inverse, X −1, produce an identity matrix, I. In other words, X X −1 =
    X −1 X = I.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some interesting properties of an invertible matrix as well. For
    example, inverting an inverse matrix gives us the original matrix: (X −1) −1 =
    X. Also, since the product of an identity matrix with itself is an identity matrix,
    the inverse of an identity matrix is thus the identity matrix itself, a special
    case compared to others.'
  prefs: []
  type: TYPE_NORMAL
- en: The inverse of a matrix can be obtained using the `solve()` function in R, which
    will give us an error if the matrix is not invertible. Let’s practice this via
    the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.8 – inverting a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will invert both an identity matrix and a standard square
    matrix using the `solve()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invert a two-dimensional identity matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the inverse of an identity matrix is itself.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Invert the running matrix, X, from the previous example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can verify the validity of this inverse matrix by multiplying it by the original
    matrix and checking whether it gives us an identity matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply by the original matrix to verify it based on the definition of matrix
    inversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that `Xinv` is indeed the inverse matrix of `X`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following section, we will look at matrix-vector multiplication from
    the perspective of solving a system of linear equations, which is an essential
    task in many machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Solving a system of linear equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The matrix-vector multiplication operation gives rise to a system of equations.
    In a typical machine learning algorithm, data comes in the form of a matrix, X,
    and the target outcome is a vector, y. When the model that’s used is a straightforward
    linear model, we assume the input-output relationship as Xw = y, where w represents
    the vector of features/coefficients. An n x p matrix of input data multiplies
    a p x 1 vector, w, of features to produce, as expected, an n x 1 output vector,
    y. The essence of linear regress is thus to solve for the exact values in w such
    that the system of linear equations in Xw = y are satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: The equivalence between matrix-vector multiplication and the system of linear
    equations may take some time to become noticeable. Let’s pause and look at this
    equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: System of linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are already familiar with the process of calculating a matrix-vector multiplication
    operation. A 2x2 matrix, X, when multiplying a 2x1 vector, w, will result in a
    2x1 vector, y. The first element in y, positioned as (1, 1), comes from the dot
    product (weighted sum) between the first row in X and the first column in y. Similarly,
    the second element in y, positioned as (2, 1), comes from the dot product between
    the second row in X and the first column in y. The positional index of each element
    in y determines the ingredients used in the respective dot product operation.
  prefs: []
  type: TYPE_NORMAL
- en: Such an interpretation, however, is a raw and low-level one. A more advanced
    interpretation lies in the column space of X and the associated linear combination
    of the column vectors in X weighted by the weights in w in the same matrix-vector
    multiplication. A concrete example will help here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a simple matrix, X = [1 3 2 4], and a vector, w T = [1,1].
    The output vector is y = [1 * 1 + 3 * 1 2 * 1 + 4 * 1] = [4 6] based on the usual
    method of calculation. The column view gives another process of calculation: y
    = 1 *[1 2] + 1 *[3 4] = [1 2] + [3 4] = [4 6]. Here, the two columns, [1 2] and
    [3 4], are weighted by each element in w, respectively. In other words, these
    two columns are linearly combined to produce the output column vector, y. This
    forms the basis of the system of linear equations. *Figure 7**.5* summarizes these
    two different perspectives of matrix-vector multiplication for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Illustrating the two different ways of calculating matrix-vector
    multiplication](img/B18680_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Illustrating the two different ways of calculating matrix-vector
    multiplication
  prefs: []
  type: TYPE_NORMAL
- en: Now, assume the weight vector, w, is unknown, with two unknown elements, w 1
    and w 2 – in other words, w T = [ w 1, w 2]. Also, assume that the input matrix,
    X, and the output vector, y, are known. This is a common situation where we are
    given the input-output data pairs and asked to estimate the coefficients of the
    linear model such that Xw = y. Now comes the system of linear equations. Plugging
    in the previous weighted combination of column vectors gives us
  prefs: []
  type: TYPE_NORMAL
- en: 'w 1 *[1 2] + w 2 *[3 4] = [ w 1 2 w 1] + [3 w 2 4 w 2] = [ w 1 + 3 w 2 2 w 1
    + 4 w 2] = [4 6]. We have a system of equations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  2 w 1 + 4 w 2 = 6'
  prefs: []
  type: TYPE_NORMAL
- en: Solving this system of linear equations gives us w 1 = w 2 = 1\. The equivalence
    of matrix-vector multiplication and the system of linear equations is manifested
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to appreciate the equivalence in terms of code.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.9 – understanding the system of linear equations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will first use the usual matrix-vector multiplication
    procedure to calculate the result, followed by providing a detailed and manual
    column-view implementation of the same operation. We will see that both give the
    same result:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a 2x2 matrix, `X`, that consists of four numbers, 1 to 4, and is filled
    by column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a vector, `w`, of length two, both filled with a value of 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that a vector is a column vector by default. When the 2x2 matrix, `X`,
    multiplies the 2x1 vector, `w`, as shown in the following code, the vector, `w`,
    is expressed as a column vector before entering the multiplication operation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply `X` and `w` and save the result in `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, the `y` vector is displayed as an explicit column vector. Lastly, let’s
    verify the calculation using the column view.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform the same matrix-vector multiplication using the column view, meaning
    a weighted sum of column vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use `X[,1]` and `X[,2]` to access the first and second columns, respectively.
    The result agrees with the previous one, although following a different format
    as it is displayed as a row vector now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Matrix-vector multiplication gives rise to a system of linear equations. However,
    this system of linear equations may or may not have a solution. Even if there
    is a solution, it may not be unique. In the following section, we will examine
    the potential solution to matrix-vector equations.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to matrix-vector equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s continue with the example where a 2x2 matrix, X, multiplies a 2x1 weight
    vector, w, to produce a 2x1 output vector, y. We will discuss three cases here:
    without a solution, with a unique solution, and with infinitely many solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the second case. Recall our previous values in X, w, and
    y. In [1 3 2 4] x [1 1] = [4 6], we multiplied the former two (X and w) to produce
    the third one (y) and showed how to derive the values of w given X and y by solving
    a system of linear equations in [1 3 2 4] x [w 1 w 2] = [4 6]. In other words,
    the solution is unique, and we can solve it analytically by writing out the explicit
    system of linear equations and solving the unknowns. It will be more involved
    when we have more unknowns to solve, but the process stays the same. In a typical
    linear regression setting, when the number of unknowns (the length of the weight
    vector, w) is the same as the number of equations (the number of rows in X), and
    provided that the input matrix, X, is a nice one (for example, if its columns
    are uncorrelated), we can find a unique solution to the system of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s tweak the input matrix, X, so that no solution can be found. One
    way to do this is to set the second row vector of X to zero, giving us X = [1 3 0 0].
    Thus, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 0 0]x[w 1 w 2] = [4 6]'
  prefs: []
  type: TYPE_NORMAL
- en: '{w 1 + 3 w 2 = 4 0 = 6'
  prefs: []
  type: TYPE_NORMAL
- en: It is obvious that the second equation fails, and therefore this system of linear
    equations cannot have a solution. This is called an *inconsistent* system since
    0 cannot be 6\. Note that in the first equation, there are infinitely many pairs
    of ( w 1, w 2) that satisfy the equation. This also leads to our third case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose we change the output vector slightly and make the second entry
    of y zero, giving us y = [4 0]. Thus, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 0 0]x[w 1 w 2] = [4 0]'
  prefs: []
  type: TYPE_NORMAL
- en: '{w 1 + 3 w 2 = 4 0 = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Now, this system of linear equations has at least one solution and is *consistent*,
    but with infinitely many solutions. There is not much we can take away from solving
    this system of equations; since there are infinitely many solutions, we are unable
    to evaluate which one is the best to report. Real-world optimization problems
    are typically concerned with finding *the* single most optimal solution or the
    best sub-optimal solution that is closest to the empirically unattainable optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.6* summarizes these three cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Three different cases when solving a system of linear equations](img/B18680_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Three different cases when solving a system of linear equations
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that we can also view the process of solving a system of linear
    equations from a geometric perspective. Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric interpretation of solving a system of linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, let’s start with the case where we have a unique solution for the
    system of linear equations with two unknowns (w 1 and w 2) and two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 3 2 4] x [w 1 w 2] = [4 6]'
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  2 w 1 + 4 w 2 = 6'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we introduce a two-dimensional coordinate system and put w 1 and w 2 on
    the *x* axis and *y* axis, respectively, we will see two lines on the coordinate
    system, each representing a linear equation in the system. Now, we can re-express
    the previous system of equations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{w 2 = −  w 1 _ 3  +  4 _ 3   w 2 = −  w 1 _ 2  +  3 _ 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, solving the system of linear equations corresponds with finding
    the intersection point of these two lines, since it is only at the intersection
    point that both equations are satisfied. The following codes help us plot the
    two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the `plot()` function to draw a two-dimensional coordinate system
    with a circle representing the point (1,1). We also add two lines using the `abline()`
    function, which accepts the intercept and slope of the line as input arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Running this code generates *Figure 7**.7*. We can see that these two lines
    happen to meet at point (1,1), which is not a coincidence!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Plotting the system of linear equations as intersecting lines](img/B18680_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Plotting the system of linear equations as intersecting lines
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the case with no solution for the system of equations.
    The previous example shows an inconsistent system of equations, where the second
    equation simply does not stand. With the geometric interpretation in the coordinate
    system, we can make another line parallel to the first one, such as by shifting
    the line up by one unit, to produce a case with no solution. Specifically, we
    can have the following system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '{ w 1 + 3 w 2 = 4  w 1 + 3 (w 2 − 1) = 4'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we minus 1 from w 2 to shift the line upward by one unit. Again, we can
    express these equations as a function of w 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '{w 2 = −  w 1 _ 3  +  4 _ 3   w 2 = −  w 1 _ 3  +  7 _ 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot these two lines on the coordinate system via the following code.
    The only change is the bigger intercept in the second line compared to the first
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code generates *Figure 7**.8*. Since these two lines are parallel,
    there will be no intersection between the two lines, so we’ll end up with no solution
    to the system of linear equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Plotting two parallel lines](img/B18680_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Plotting two parallel lines
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to the case with infinitely many solutions. As you may have
    guessed, we just need to make the second equation the same as the first one, thus
    creating two overlapping lines in the coordinate system. Any point on these two
    overlapping lines is a valid solution, and there is an infinite number of such
    points on the line(s).
  prefs: []
  type: TYPE_NORMAL
- en: Working with a system of linear equations may not guarantee a solution. Even
    if there are as many unknown variables as there are rows in the system, this isn’t
    guaranteed to give us a solution. In the following section, we’ll look at under
    what conditions we are guaranteed to get a unique solution.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a unique solution to a system of linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall our analytical framework: Xw = y, where we are given input-output pairs
    (X, y) and would like to solve the unknown vector, w. If things were simple and
    all these variables were scalars, we would simply divide both sides by X and obtain
    the solution, w = X −1 y, provided that X is invertible. Note that we would multiply
    both sides by X −1 on the left to perform the division. This brings us to the
    first condition: the matrix, X, needs to have a corresponding inverse matrix;
    that is, it is invertible.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.9* illustrates the correspondence between simple scalar calculations
    in regular algebra and matrix manipulations in matrix algebra. Note that by definition
    of matrix inverse, we have X −1 X = I. We used this result in the derivation.
    The identity matrix is special; any matrix multiplying an identity matrix will
    stay unchanged. Thus, we have Iw = w. Again, we assume the matrix, X, is invertible:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Comparing scalar and matrix manipulations when solving the system
    of linear equations](img/B18680_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Comparing scalar and matrix manipulations when solving the system
    of linear equations
  prefs: []
  type: TYPE_NORMAL
- en: 'The second condition is that the *determinant* of the matrix, X, cannot be
    zero. The determinant is a summary measure of the size of the matrix; we will
    discuss this in more detail in the next chapter. The third condition is what we
    alluded to earlier: the rows and columns of the matrix, X, can form a *basis*
    for the corresponding row and column space. There should be no correlated rows
    or columns in the matrix. A basis of a vector space is a set of vectors that can
    uniquely generate all the other vectors in the same vector space via linear combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we would need a somewhat trivial condition: when Xw = 0, we have
    w = 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the inverse and determinant of a matrix is straightforward in R.
    In the following code, we are calculating the inverse of the matrix, X, using
    the `solve()` function and its determinant using the `det()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The next chapter will touch on the matrix determinant, norm, trace, and other
    special properties in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we can calculate the matrix inverse, let’s learn how to obtain the
    solution to the system of linear equations using the inverse of the input matrix,
    X, and output, vector y.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.10 – obtaining the unique solution to a system of linear equations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will make use of the matrix inverse to obtain the solution
    to a system of linear equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct the input matrix, `X`, and output vector, `y`, based on the previous
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the data we chose here is based on the previous running example, where
    the solution to the unknown variables is w = [1,1].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the solution to the system of linear equations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the solution, which can be called the coefficient vector, matches
    the true result exactly. We can also multiply the input matrix by this solution
    to obtain an estimated output and check whether it is the same as the given output
    vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the output via matrix-vector multiplication between the input matrix
    and the estimated coefficient vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result matches the given output vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solving a system of linear equations with a square input matrix is a simple
    setting. When working with real data, the input matrix will likely be non-square,
    with more rows or columns. When there are more rows than columns, we have a limited
    number of unknown variables that need to satisfy more equations. This is an **overdetermined
    system**. On the other hand, when there are more unknown variables to satisfy
    fewer equations, we have an **underdetermined system**. We will discuss these
    two situations in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Overdetermined and underdetermined systems of linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example we covered earlier, which consisted of two equations and two unknown
    variables to solve, gives a solution that passes through both lines on the coordinate
    system. This is called **interpolation**, where the solution perfectly satisfies
    both equations without any errors. In the language of machine learning, interpolation
    means the trained model can score 100% on the training dataset. This is not necessarily
    a good thing since the model will likely run into the risk of overfitting – that
    is, it will do pretty well on the training set but not so well on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Complex models such as neural networks and other nonlinear models tend to have
    very low or even zero training errors – that is, they are likely to interpolate
    the training data. The zero training cost occurs when the model becomes sufficiently
    complex to interpolate the training data. Perfect prediction in the training data
    is likely to happen when the number of coefficients, p, used by the model is equal
    to or larger than the number of observations, n, in the training input. In the
    case of linear regression, when p = n, we solve a system of linear equations where
    the number of free variables, whose optimal values we are solving for, is equal
    to the number of linear equations in the system.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example is a two-dimensional training set with two observation points
    on a coordinate system, as we saw earlier. When only one parameter is available,
    the model is reflected as a straight horizontal line in the coordinate system.
    This univariate model is too simple to fit these two points unless they happen
    to live along the fitted line. The resulting system of equations is said to be
    overdetermined and under-parameterized since we have more linear equations than
    the unknown variables in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have two parameters to fit the model, the problem can be solved with
    an exact solution: two linear equations and two unknowns, a straightforward exercise.
    We can fit a line anywhere in the coordinate system by adjusting its intercept
    and slope. So, the problem becomes fitting a line that passes through the two
    points. We can train a model that passes through the two points and produces zero
    training error, thus interpolating between these two points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When there are more than two unknown variables – say we have three weights
    in the model – the problem becomes underdetermined and over-parameterized. The
    solution, when solvable, will be infinite. Having more than two parameters corresponds
    to fitting two points with a curved line. Any curve that passes through the two
    points produces zero training error, yet the curve can be arbitrarily wiggly and
    complex, depending on the number of parameters used. *Figure 7**.10* summarizes
    these three scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Three scenarios of model complexity](img/B18680_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Three scenarios of model complexity
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.10* shows fitting two two-dimensional observations using different
    model complexities. The left plot contains a 0th degree polynomial model that
    contains only one parameter, thus being a horizontal line. The middle plot has
    an equal number of parameters to the number of observations, making the solution
    unique and exact. The right plot contains models with more than two parameters,
    where the solution is non-unique due to being an underdetermined system.'
  prefs: []
  type: TYPE_NORMAL
- en: Perfect interpolation occurs when p = n – that is, the number of features is
    equal to the number of observations. The perfect interpolation continues with
    p > n when the system of equations becomes underdetermined and over-parameterized,
    leading to infinitely many solutions that correspond to models of an arbitrary
    shape. All these models pass through the observed data points and thus give zero
    training error.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the basics of linear algebra, including working
    with vectors and matrices and performing matrix-vector multiplication. We highlighted
    a few special matrices, such as the identity matrix, and common operations, such
    as transposing and inverting a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used matrix-vector multiplication to solve a system of linear equations
    under different settings. We introduced the geometric interpretation that corresponds
    to the system of linear equations, along with how to obtain the solution using
    matrix inverse and multiplication operations.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we touched upon common settings of the input matrix in the machine learning
    context, covering both underdetermined and overdetermined systems. Developing
    such an understanding will be crucial when we delve into statistical modeling
    and machine learning in the third part of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss slightly more advanced concepts in matrix
    algebra and implementations in R.
  prefs: []
  type: TYPE_NORMAL
