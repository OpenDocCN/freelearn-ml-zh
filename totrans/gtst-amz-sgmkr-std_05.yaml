- en: '*Chapter 3*: Data Preparation with SageMaker Data Wrangler'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With SageMaker Data Wrangler, you can perform exploratory data analysis and
    data preprocessing for ML modeling with a point and click experience. You will
    be able to quickly iterate through data transformation and quick modeling to see
    if your transform recipe improves model performance, learning if there is implicit
    bias in the data against sensitive groups, and having a clear record of what transformation
    has been done on the processed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning how to use **SageMaker Data Wrangler**
    in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with SageMaker Data Wrangler for customer churn prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing data from sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data with visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting data for ML training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need to access materials in [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter03](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter03).
    You need to make sure your IAM execution role has the AmazonAthenaFullAccess policy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with SageMaker Data Wrangler for customer churn prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customer churn is a serious problem for businesses. Losing a customer is definitely
    not something you want to see if you are a business owner. You want to your customers
    to be happy with your product or service and continue to use them for, well, forever.
    Customer churn is always going to happen but being able to understand how and
    why a customer leaves the service or why a customer is not buying your product
    anymore is critical for your business. Being able to predict ahead of time would
    be even better.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will perform exploratory data analysis and data transformation
    with SageMaker Data Wrangler, and at the end of the chapter, we will be training
    an ML model using the **XGBoost algorithm** on the wrangled data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to take a synthetic `chapter03/1-prepare_data.ipynb` notebook
    and execute the it. You will get a copy of the data, then perform these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into three data frames, `customer_info`, `account_info`, and
    `utility`, so that we can demonstrate joining in SageMaker Data Wrangler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mask out values randomly to create missingness in the data so that we can demonstrate
    functionalities of SageMaker Data Wrangler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the three data frames in an S3 bucket and make `utility` available in Amazon
    Athena so that we can simulate importing data from multiple sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launching SageMaker Data Wrangler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can access SageMaker Data Wrangler in any of the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Click through **File** | **New** | **Data Wrangler Flow** (*Figure 3.1*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the Launcher, click on **New data flow** (*Figure 3.1*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1 â€“ Creating a new Data Wrangler flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 â€“ Creating a new Data Wrangler flow
  prefs: []
  type: TYPE_NORMAL
- en: From the left sidebar, **SageMaker resources**, choose Data Wrangler in the
    drop-down menu and click **New flow** (*Figure 3.2*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.2 â€“ Creating a new Data Wrangler flow file from the registry. You
    can find all the flow files you have here too'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 â€“ Creating a new Data Wrangler flow file from the registry. You can
    find all the flow files you have here too
  prefs: []
  type: TYPE_NORMAL
- en: Notably, from `untitled.flow`, created in the current working directory. A *data
    flow file*, with the extension `.flow`, is a file that records all the steps you
    do with SageMaker Data Wrangler from the UI. It is a JSON-based file that can
    be easily transferred and reused. SageMaker Studio and Data Wrangler can interpret
    the content of the JSON file and render the transformations and analyses you do
    for the dataset. What's happening behind the scenes during this wait time is SageMaker
    Studio is launching a data wrangler *KernelGateway* app with a dedicated *ml.m5.4xlarge*
    instance to support the activities we are going to perform inside SageMaker Data
    Wrangler and to avoid contention with other notebook kernels. Once it's ready,
    you should see the view presented in *Figure 3.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 â€“ Starting point of a data wrangling journey with SageMaker Data
    Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 â€“ Starting point of a data wrangling journey with SageMaker Data
    Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, let's rename the flow file to `wrangling-customer-churn.flow`
    or something to your liking by right-clicking on the file in the file explorer
    and selecting **Rename**.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's get started with SageMaker Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data from sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in the data preparation journey is to import data from a source(s).
    There are four options from which data can be imported: `chapter03/1-prepare_data.ipynb`
    notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: Importing from S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please follow the next steps to import the CSV files into the S3 bucket. We
    want to load the `customer_info` and `account_info` tables:'
  prefs: []
  type: TYPE_NORMAL
- en: From the view in *Figure 3.3*, select **Amazon S3** as the source. You should
    see a list of S3 buckets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the data following the path of the SageMaker default bucket that has
    the naming convention `sagemaker-<region>-<accountid>`. Then descend into the
    `sagemaker-studio-book/chapter03/data/` folder to find the CSV files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `telco_churn_customer_info.csv` and inspect the data. Make sure the file
    type is CSV and `ml.m5.4xlarge` instance with 16 vCPUs and 64 GiB of RAM. Sampling
    can be helpful to make sure the dataset fits into the memory when you have a large
    dataset. Click **Import**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1â€“3 for `telco_churn_account_info.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 â€“ Data flow after two CSV files are imported'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 â€“ Data flow after two CSV files are imported
  prefs: []
  type: TYPE_NORMAL
- en: Once the two CSV files are loaded, you should see the view in *Figure 3.4* in
    the `utility`.
  prefs: []
  type: TYPE_NORMAL
- en: Importing from Athena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As our `utility` table is being registered as an Amazon Athena table, we can
    import it from Athena with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Import** tab and select **Amazon Athena** as the source. You
    should see the view shown in *Figure 3.5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the two drop-down options, select **AwsDataCatalog** for **Data catalog**
    and select **telco_db** for **Database**. And for **Advanced configuration**,
    you can check/uncheck **Enable sampling**. As shown in **Location of query results**,
    you can find the output of the query in the location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 â€“ Importing data from Amazon Athena'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 â€“ Importing data from Amazon Athena
  prefs: []
  type: TYPE_NORMAL
- en: After you select the database, you will see the available tables on the right
    side in the `telco_churn_utility` table in our Amazon Athena database. You can
    click on the eye icon to preview the table so that we know how the table looks,
    as in *Figure 3.6*, and how to form a more complex query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6 â€“ Previewing the table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 â€“ Previewing the table
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get all the data through a query. Please put the following query statement
    into the query box. Then click **Run**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will find the query result below the query box. We get all the rows and
    columns with the previous statement. Inspect the data and click on the **Import**
    button at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a dataset name, such as `telco_churn_utility`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see all three tables being loaded into the data flow in the **Data
    Flow** tab. By clicking on the plus sign when you hover over any of the rightmost
    nodes, you will see actions that you can perform on such tables, as shown in *Figure
    3.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 â€“ Actions after tables are imported'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 â€“ Actions after tables are imported
  prefs: []
  type: TYPE_NORMAL
- en: Next, we should check the data types, or the schema of the tables, to make sure
    that they are being inferred correctly during the import process.
  prefs: []
  type: TYPE_NORMAL
- en: Editing the data type
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data type dictates how each data column is read by Data Wrangler and how
    it should be processed. There are **Long**, **Float**, **Boolean**, **String**,
    and **Date** types in Data Wrangler. **Long** holds data that is in integer form.
    **Float** allows floating points in the data. **Boolean** represents binary values
    such as *0/1* and *Yes/No*. **String** makes the data a text-based entry. **Date**
    holds data that is in the form of text (*dd-MM-yyyy*) but is interpreted as a
    date instead of a string and allows date-related operations and comparison.
  prefs: []
  type: TYPE_NORMAL
- en: The types of transformation that can be applied to data depends on the data
    type. For example, you can only apply a numerical operation on columns of the
    `Long` and `Float` types. Therefore, it is important to get the data types correctly
    defined before proceeding even though Data Wrangler does infer data types while
    importing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s check and edit the data types of the imported tables in Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: From the view shown in *Figure 3.7*, click on the plus sign next to `telco_churn_account_info.csv`
    and select **Edit data types**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown in *Figure 3.8*, `Long` integer type. To change it, in **CONFIGURE
    TYPES** in the right panel, click on **Type** for the **Account Length** column,
    and select **Long**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`String`. But they should be of the Boolean type to conserve memory. Change
    them to `Boolean` by selecting `Boolean` in **CONFIGURE TYPES**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview** to see how the data looks after the data type change. See
    *Figure 3.8*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 â€“ Editing data types in Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 â€“ Editing data types in Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: We can see that **Account Length** is now of the **Long** type with integer
    values intact and that **Int'l Plan** and **Vmail Plan** are **Boolean** with
    yes/no converted to true/false, as shown in the table. Data type conversion does
    not result in data loss or anything so we can proceed to apply the edit.
  prefs: []
  type: TYPE_NORMAL
- en: Click `String` type to `Boolean` type. This is because the period, **.**, in
    the values would invalidate the conversion. You can try changing it and preview
    the change. You will see the whole column being erased. We will deal with this
    column with transformation later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ve changed and confirmed the data type for the first table. We should do
    the same for the other two tables:'
  prefs: []
  type: TYPE_NORMAL
- en: Click **Back to data flow** to return to the data flow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the plus sign next to `telco_churn_customer_info.csv` and select **Edit
    data types**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change `Long` to `String`. Though this column has integer values, they should
    be treated as `locality` rather than numeric features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview**, then **Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Back to data flow** to return to the data flow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the plus sign next to the last table, `telco_churn_utility`, then select
    **Edit data types**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change `cust_serv_calls` from `Float` to `Long`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview**, then **Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Back to data flow** to return to the data flow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We've verified and fixed the data type for the three tables. Now it is time
    to join them together as one table.
  prefs: []
  type: TYPE_NORMAL
- en: Joining tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Joining tables is one of the most common steps when you are working with multiple
    data sources and the most important step to enrich your features when you are
    building an ML model. Think on relational database terms. Your tables maintain
    some sort of relationship that allows you to put them all together to get a big
    picture. We will be joining the three tables by `customerID` column with Data
    Wrangler. Please follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the plus sign next to `telco_churn_account_info.csv` and select **Join**.
    You should see the view shown in *Figure 3.9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 â€“ Joining tables in SageMaker Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 â€“ Joining tables in SageMaker Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: '`telco_churn_account_info.csv` is chosen as `telco_churn_customer_info.csv`
    as **Right**. You should see the linkage between the two tables, as shown in *Figure
    3.10*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 â€“ Joining tables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 â€“ Joining tables
  prefs: []
  type: TYPE_NORMAL
- en: Click **Configure** to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown in *Figure 3.11*, select `join` type as we expect to get all the data
    in, then select **CustomerID** for both **Left** and **Right** as the key to join.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 â€“ Joining tables with Full outer and select keys'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 â€“ Joining tables with Full outer and select keys
  prefs: []
  type: TYPE_NORMAL
- en: Click `CustomerID_0` and `CustomerID_1`. We will deal with this later in the
    *Applying transformation* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Add** in the top right to complete the join.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we need to join the last table. Click on the plus sign next to the joined
    table and select **Join**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `telco_churn_utility` as **Right**, then click **Configure**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, select `CustomerID_0` for `customer_id` for **Right** to join.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Apply** to preview the joined dataset. Yes, the tables are joined, but
    with the keys duplicated, which can be addressed later in the *Applying transformation*
    section. No worries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Add** in the top right to complete the join. You will be brought back
    to the data flow. You should see the flow, as shown in *Figure 3.12*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 â€“ Data flow after joining three tables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 â€“ Data flow after joining three tables
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you find anything that was done wrong, don't worry, just click on the plus
    sign on the node that has the mistake and select **Delete** to remove the node.
    But do keep in mind that if you delete a node that is not the last node, all the
    downstream nodes will be deleted too.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are ready to move on to the next phase: getting to explore the dataset!'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data with visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exploratory data analysis** (**EDA**) provides insights into the data at
    hand and helps us strategize the data transformation so that ML modeling can be
    the most performant. Analyzing and visualizing data with programming is robust
    and scalable but it requires lots of coding and development. Using SageMaker Data
    Wrangler, you can easily create charts and figures in the UI. Currently, SageMaker
    Data Wrangler supports the following types of chart and analysis that do not require
    coding: **histogram**, **scatter plot**, **bias report**, **multicollinearity**,
    **quick model**, **target leakage**, and **table summary**. Let''s take a look
    at how they work one by one.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the frequency distribution with a histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The histogram helps us understand the frequency distribution of a variable
    whose values are bucketed into discrete intervals with a bar graph. We can use
    the histogram function in SageMaker Data Wrangler to see, for example, how long
    callers spend making calls in the daytime. To do this, please follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the plus sign next to the **2nd Join** node and select **Add analysis**.
    You should see the view shown in *Figure 3.13*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.13 â€“ Adding an analysis in SageMaker Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 â€“ Adding an analysis in SageMaker Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: Fill in a name for the analysis in `day_mins_histogram`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **day_mins** for **X axis**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview** to see the chart, as shown in *Figure 3.14*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.14 â€“ Histogram of minutes of call time in the daytime'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 â€“ Histogram of minutes of call time in the daytime
  prefs: []
  type: TYPE_NORMAL
- en: 'This is great! You created your first visualization in SageMaker Data Wrangler
    to see the frequency distribution of call time in the daytime among all customers.
    We see that most customers'' calls are shorter than 8 minutes and few calls are
    longer than 12 minutes. But this is an overall view. As a data scientist, you
    might want to know how customers who left the service behave differently from
    the customers who continue to use the service. We should slice and dice the data
    based on the target status: **Churn?**. We can do it through the **Facet by**
    option. We will proceed to modify the chart and not save the current chart.'
  prefs: []
  type: TYPE_NORMAL
- en: Choose **Churn?** for **Facet by** and click **Preview**. You should see an
    updated chart, as in *Figure 3.15*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.15 â€“ Histogram of the day_mins variable by target'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 â€“ Histogram of the day_mins variable by target
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that customers who left the service (the **True.** chart) most
    frequently make calls for around 6-10 minutes while the customers who stayed with
    the service (the **False.** chart) talk less on calls. What an interesting observation.
    Let's save the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save** to save and return to the page where all analyses are saved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **All Analyses** view, you can see charts and analyses you created for
    each node at any given state. We have created a histogram. Let's go on to create
    another chart.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A data scientist might be wondering if customers who call more in the daytime
    also call often in the evening. Or you might be curious if any correlation exists
    between the customer''s account length and call time. You can use a **scatter
    plot** to visualize this characteristic. Let''s create a scatter plot for the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Analysis** page, click **Create new analysis** at the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose `AccountLength_CallTime_Scatter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Account Length** for **X axis** and **day_mins** for **Y axis**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview**. You should see a chart, as shown in *Figure 3.16*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16 â€“ Scatter plot of Account Length versus day_mins'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 â€“ Scatter plot of Account Length versus day_mins
  prefs: []
  type: TYPE_NORMAL
- en: There does not seem to be any correlation visually between the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms and scatter plots are the two most common tools for EDA that you
    probably are familiar with. With SageMaker Data Wrangler, you can use ML-oriented
    analyses such as Quick Model to help you determine your data transformation strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Previewing ML model performance with Quick Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quick Model is another tool that helps you quickly get a sense of whether your
    data provides any predictive power with the variables presented in the data. This
    tool is useful and can be used frequently. Let''s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Analysis** page, click **Create new analysis** in the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Quick Model** for **Analysis type**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a name in `first_quickmodel`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Churn?** for **Label** and click **Preview**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.17 â€“ Quick Model result that shows the F1 score of the model performance
    on a test set and feature importance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 â€“ Quick Model result that shows the F1 score of the model performance
    on a test set and feature importance
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Data Wrangler takes a minute or so and returns a bar chart, as shown
    in *Figure 3.17*, showing the feature importance and an F1 score on a randomly
    split test set from the given dataset. We have not applied any transformation
    or data cleaning, as you can see in the following data table. SageMaker Data Wrangler
    employs a popular algorithm called **random forest classification** to train a
    model and test it out on a hold-out test set. We can see a preliminary result
    of a 0.851 F1 score, with **night_charge** being the most important feature in
    predicting customer churn status. We can also see that there are features that
    do not provide much predictive power, such as **Int'l Plan** and **VMail Plan**.
    And there are redundant features such as **CustomerID_*** that should not have
    been included in the modeling. This gives us hints to make sure to include **night_charge**
    and other high-importance features in the actual modeling and that we can leave
    out **Int'l Plan** and **VMail Plan** if we are restricted by the number of features
    we can use. Let's ink the analysis on the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save** to save the analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we just did our first quick modeling, to get a sense of the model performance
    we are getting, it is also a good idea to test whether we are running into any
    data leakage or target leakage problems.
  prefs: []
  type: TYPE_NORMAL
- en: Revealing target leakage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Target leakage means that there are features in the data that are highly correlated
    or basically a proxy representation of the target variable. For example, if our
    dataset contains a column that records the date of termination for each churned
    customer, then this column is going to contain those who churned, resulting in
    an extremely high modeling accuracy if we include it in the modeling. The problem
    in this example is that come prediction time in the real world, it is very unlikely
    to have the date of termination when the job of the model is to predict future
    churn. Let''s see if our dataset contains any target leakage:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Analysis** page, click **Create new analysis** in the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Target Leakage** for **Analysis type**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a name in `churn_target_leakage`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input `25` for **Max features** because we have 24 columns in the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **classification** for **Problem Type**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Churn?** for **Target** and click **Preview**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.18 â€“ Target leakage result showing features that are safe and that
    are possibly redundant (color-coded with a legend to the right of the chart)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 â€“ Target leakage result showing features that are safe and that
    are possibly redundant (color-coded with a legend to the right of the chart)
  prefs: []
  type: TYPE_NORMAL
- en: 'The target leakage analysis computes the cross-validated area under the ROC
    for each individual feature against the target, as explained in the text above
    the chart in *Figure 3.18*. This analysis shows that no feature is determined
    as potential target leakage, which is a good sign. The result also confirms the
    conclusions we learned from the quick modeling exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: a) `night_charge` is important in predicting churn and provides a high level
    of predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: b) `VMail Plan` is providing little predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: c) `CustomerID_*` is redundant in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's save the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save** to save the analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned about feature predictive power through the last two analyses. We
    should also take a look at how we can create a custom visualization with SageMaker
    Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker Data Wrangler uses **Altair** ([https://altair-viz.github.io/](https://altair-viz.github.io/))
    to create visualizations programmatically. We can create any custom visualization
    with code in SageMaker Data Wrangler as well for greater flexibility. For example,
    we can create a boxplot for **night_charge** by **Churn?** status to understand
    the statistical distribution of the two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **All Analyses** page, click **Create new analysis** in the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Code** tab right next to **Configure**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a name, such as `boxplot_night_charge_by_churn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input the following code in the coding area. Be sure to import the `altair`
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click **Preview**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see a box plot representing the distribution of `Churn?` status,
    as shown in *Figure 3.19*. If you hover over the box plot, you can see the descriptive
    statistics of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 â€“ Creating a custom boxplot using the Altair library'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 â€“ Creating a custom boxplot using the Altair library
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save** to save the custom visualization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's worth noting is that these analyses and visualizations are saved as part
    of the flow file so that you can have full visibility of how you wrangle the data.
  prefs: []
  type: TYPE_NORMAL
- en: With these analyses, we now have a good understanding of how we should transform
    and wrangle the data.
  prefs: []
  type: TYPE_NORMAL
- en: Applying transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can easily apply data transformation using SageMaker Data Wrangler because
    there are numerous built-in transformations you can use out of the box without
    any coding. So far, we have observed the following from the analyses that we need
    to handle next in order to build up an ML dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing data in some features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Churn?` column is now in string format with `True.` and `False.` as values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundant `CustomerID_*` columns after joins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features that are not providing predictive power, including but not limited
    to `Phone`, `VMail Plan`, and `Int'l Plan`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also would like to perform the following transformations for ML purposes
    because we want to train an XGBoost model to predict the `Churn?` status afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical variables, that is, `State` and `Area Code` features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: In the **Data Flow** tab, click on the plus sign next to the **2nd Join** node,
    and select **Add transform**. You should see the view shown in *Figure 3.20,*
    with a table on the left and a list of transformations on the right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 â€“ A workspace to transform your data. You can expand each transform
    on the right side to see options'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 â€“ A workspace to transform your data. You can expand each transform
    on the right side to see options
  prefs: []
  type: TYPE_NORMAL
- en: To drop **CustomerID_***, click **Manage columns** to expand the transform,
    select **Drop column** in **Transform**, and select **CustomerID_0** for **Column
    to drop**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `CustomerID_0` is now gone, as shown in *Figure 3.21*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 â€“ Dropping columns in SageMaker Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 â€“ Dropping columns in SageMaker Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: Click **Add** to put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2â€“4 to drop `CustomerID_1` and `customer_id`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If done correctly, you should see four steps applied on the **Previous steps**
    tab to the right, as shown in *Figure 3.22*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 â€“ Reviewing previous steps in the Previous steps tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 â€“ Reviewing previous steps in the Previous steps tab
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you realize you did anything incorrectly and want to revert to a previous
    transformation, you can **Remove** steps from the last one, one at a time, as
    shown in *Figure 3.22*.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to handling missing data in `cust_serv_calls`, expand `Account Length`
    for **Input column**, and **Approximate Median** for **Imputing strategy**. We
    can leave **Output column** empty to instruct SageMaker Data Wrangler to overwrite
    the existing column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `102,` as shown in *Figure 3.23*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 â€“ Account Length is filled with the median value, 102'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 â€“ Account Length is filled with the median value, 102
  prefs: []
  type: TYPE_NORMAL
- en: Click **Add** to put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 6â€“8 for `cust_serv_calls`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are features that do not provide much predictive capability based on the
    quick model and target leakage analyses worth dropping too. `Phone` is one of
    the features that is shown to contain little to no useful information. Also, as
    is common knowledge, we know phone numbers are mostly randomly assigned when you
    sign up for a service. On the other hand, even though `VMail Plan` and `Int'l
    Plan` provide no predictive information, they are of the simple `Boolean` type
    and do have real meaning. It might not hurt as much to carry these features into
    modeling. So, let's drop the `Phone` feature.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 2â€“4 to drop `Phone`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving on to transforming categorical features, we have `State` and `Area Code`,
    which represent the location of a customer. We could apply one-hot encoding to
    transform them. However, we may risk the `Area Code`, the next best action would
    be to drop it. Let's perform one-hot encoding to `State` and drop `Area Code`.
  prefs: []
  type: TYPE_NORMAL
- en: Expand **Encode categorical**, choose **One-hot encode** for **Transform**,
    select **State** for **Input column**, select **Columns** for **Output style**,
    and leave other options as their defaults.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `State` column is replaced with `State_*` sparse features, with each representing
    whether customers are of a particular state (0 for false and 1 for true).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Add** to put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2â€“4 to drop `Area Code`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Last but not least, the target feature, `Churn?,` needs some wrangling. It has
    a weird period that messed up the data type conversion previously. Furthermore,
    the SageMaker built-in XGBoost algorithm we are going to use for modeling later
    requires the target feature to be in the first column. Let's apply a text operation
    and move the column.
  prefs: []
  type: TYPE_NORMAL
- en: Expand `Churn?` for `.` (a period) for **Symbols**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `Churn?` has been removed, as shown in *Figure 3.24*. Click **Add** to
    put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.24 â€“ Ending period removed in the Churn? column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 â€“ Ending period removed in the Churn? column
  prefs: []
  type: TYPE_NORMAL
- en: We can now use the data type parser to convert the True/False into a Boolean
    representation. Expand **Parse column as type**, choose **Churn?** for **Column**,
    and select **Boolean** in the **To** drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `Boolean` type. Click **Add** to put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To move **Churn?** to the front, expand **Manage columns**, select **Move column**
    for **Transform**, select **Move to start** for **Move type**, and choose **Churn?**
    for **Column to move**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview** to see the transformation. Now the **Churn?** column becomes
    the first feature. Click **Add** to put the transformation into effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We've just applied eleven transformations to the dataset. We can run a quick
    modeling analysis to make sure we are on the right track in terms of modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring performance while wrangling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can always add an analysis at any point in time while wrangling the data
    in SageMaker Data Wrangler. This allows you to analyze the data after key transformation
    and verify the predictive power with Quick Model. Let''s add an analysis for the
    wrangled data:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Analysis** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Quick Model** for **Analysis type**, add a name in **Analysis name**,
    and select **Churn?** for **Label**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Preview** to see the modeling result, as shown in *Figure 3.25*. The
    model's F1 score has improved from *0.851* to *0.871*. We are on the right track.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.25 â€“ Quick modeling after all transformations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_03_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 â€“ Quick modeling after all transformations
  prefs: []
  type: TYPE_NORMAL
- en: Click **Add** to put the analysis on the canvas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, we have used SageMaker Data Wrangler to analyze the telco churn dataset
    in depth and wrangled the data according to the findings from the analyses. Quick
    Model is showing an improved F1 score in predicting customer churn. We should
    move on to see what options we have with this work.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting data for ML training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker Data Wrangler supports the following export options: **Save to S3**,
    **Pipeline**, **Python Code**, and **Feature Store**. The data transformations
    we have applied so far are not really applied to the data yet. The transformation
    steps need to be executed to get the final transformed data. When we export our
    flow file with the preceding options, SageMaker Data Wrangler automatically generates
    code and notebooks to guide you through the execution process so that we do not
    have to write any code, but it leaves flexibility for us to customize the code.'
  prefs: []
  type: TYPE_NORMAL
- en: The four export options satisfy many use cases. **Save to S3** is an obvious
    one and offers lots of flexibility. If you would like to get the transformed data
    in an S3 bucket so that you can train an ML model in Amazon SageMaker, you can
    also download it locally from S3 and import it to other tools if you need to.
    The **Pipeline** option creates a SageMaker pipeline that can easily be called
    a repeatable workflow. Such workflows can be configured as event-triggered or
    time-triggered so that you can automate the data transformation as a pipeline.
    We will learn more about SageMaker Pipelines in [*Chapter 10*](B17447_10_ePub_RK.xhtml#_idTextAnchor134),
    *Monitoring ML Models in Production with SageMaker Model Monitor*. **Python Code**
    offers the most visibility and flexibility. You can see how each transformation
    is implemented by Amazon SageMaker, run the code in a Spark environment, and get
    the data processed. With the **Feature Store** option, you get an automatically
    generated Jupyter notebook that will process the data and create a feature group
    in SageMaker Feature Store. We will learn more about SageMaker Feature Store in
    [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077), *Building and Training
    ML Models with SageMaker Studio IDE*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, I''d like to show you the option **Save to S3**, which includes
    ML training in the automatically generated notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: First, save the flow file so that the exported resource will pick up the latest
    change. In the menu bar, select **File**->**Save Data Wrangler Flow**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Export** tab, click the **Steps** node, and select the last step,
    **Move column**, in the list of transformations. By clicking a step, all the steps
    leading to the step chosen will be selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Export step** in the top right, and click **Save to S3**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new Python Jupyter notebook should pop out. This notebook contains code to
    process the SageMaker Data Wrangler flow file using SageMaker Processing and to
    save the processed data in S3\. This is our first encounter with SageMaker Processing
    in action. In short, it allows us to use appropriate compute resources to perform
    data processing, model evaluation, and statistical analysis. With SageMaker Processing,
    you are no longer bound by the compute resource available locally in the Studio
    notebook environment; instead, the processing script and Data Wrangler flow file
    can be run on a right-size compute instance(s). You can see things in action in
    the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Please execute all the cells before **(Optional) Next Steps** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may configure the notebook in the section where you see ðŸ’¡ **Configurable
    Settings**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The SageMaker Processing job may take a couple of minutes. At the end of the
    processing job, the processed data is available in an S3 bucket. You should see
    the following output from the cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The following optional sections are the interesting modeling part. Let's run
    these steps to train an ML model to predict churn using SageMaker's built-in XGBoost
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reassign the value of `run_optional_steps` to `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The default objective metric, `reg:squarederror`, for XGBoost is for regression
    use cases. Change it to `binary:logistic` because we have a binary classification
    use case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Execute all the remaining cells in the notebook to start a training job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The training job will take a minute or two to finish. You can see the actions
    behind the scenes printed out as output in the last cell. We will learn more about
    SageMaker training and training algorithms in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)*,
    Building and Training ML Models with SageMaker Studio IDE*. Once finished, the
    model is saved in S3 as well, which can be used in hosting in Amazon SageMaker
    or the model can be used locally. We will learn more about hosting options in
    [*Chapter 7*](B17447_07_ePub_RK.xhtml#_idTextAnchor099), *Hosting ML Models in
    the Cloud: Best Practices*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed how to use SageMaker Data Wrangler using a telco
    customer churn dataset. We learned how to import data from various sources, join
    tables, analyze with advanced ML-based analyses, and create visualizations with
    SageMaker Data Wrangler. We then applied transformations easily with built-in
    transforms available out of the box from SageMaker Data Wrangler without any code.
    At the end of the chapter, we showed how to export the transformed data to an
    S3 bucket and how to easily train an ML model using the automatically generated
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the concept of a feature store in a
    machine learning project, and how to set up a feature store using **SageMaker
    Feature Store**. SageMaker Feature Store unifies the features across teams so
    that teams can remove redundant feature engineering pipelines. It also serves
    as a central repository for both model training and model serving use cases because
    of its unique design pattern to have an offline store for easy querying for selecting
    training datasets and an online store for low latency transactions required in
    the model serving environment.
  prefs: []
  type: TYPE_NORMAL
