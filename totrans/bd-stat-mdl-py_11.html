<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-167"><a id="_idTextAnchor174"/>11</h1>
<h1 id="_idParaDest-168"><a id="_idTextAnchor175"/>ARIMA Models</h1>
<p><a id="_idTextAnchor176"/>In this chapter, we will discuss univariate time series models. These are models that only consider a single variable and create forecasts based only on the previous samples in the time series. We will start by looking at models for stationary time series data and then progress to models for non-stationary time series data. We will also discuss how to identify appropriate models based on the characteristics of time series. This will provide a powerful set of models for forecasting time series.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Models for stationary time series</li>
<li>Models for non-stationary time series</li>
<li>More on model evaluation</li>
</ul>
<h1 id="_idParaDest-169"><a id="_idTextAnchor177"/>Technical requirements</h1>
<p>In this chapter, we use two additional Python libraries for time series analysis: <code>sktime</code> and <code>pmdarima</code>. Please install the following versions of these libraries to run the provided code. Instructions for installing libraries can be found in <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Sampling </em><em class="italic">and Generalization</em>.</p>
<ul>
<li><code>sktime==0.15.0</code></li>
<li><code>pmdarima==2.02</code></li>
</ul>
<p>More information about <code>sktime</code> can be found at this link: <a href="https://www.sktime.org/en/stable/get_started.xhtml">https://www.sktime.org/en/stable/get_started.xhtml</a></p>
<p>More information about <code>pmdarima</code> can be found at this link: <a href="http://alkaline-ml.com/pmdarima/">http://alkaline-ml.com/pmdarima/</a></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor178"/>Models for stationary time series</h1>
<p>In this section, we will discuss <strong class="bold">Autoregressive</strong> (<strong class="bold">AR</strong>), <strong class="bold">Moving Average</strong> (<strong class="bold">MA</strong>), and <strong class="bold">Autoregressive Moving Average</strong> (<strong class="bold">ARMA</strong>) models that are useful for stationary data. These models are useful when modeling patterns and variance around process means that output over time. <em class="italic">When we have data that does not exhibit autocorrelation, we can use statistical and machine learning models that do not make assumptions about time, such as Logistic Regression or Naïve Bayes, so long as the data supports such </em><em class="italic">use cases</em>.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor179"/>Autoregressive (AR) models</h2>
<h3>The AR(p) model</h3>
<p>In <a href="B18945_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a>, <em class="italic">Introduction to Time Series</em> we <a id="_idIndexMarker804"/>considered how the <strong class="bold"><a id="_idIndexMarker805"/></strong><strong class="bold">Partial Auto-Correlation Function</strong> (<strong class="bold">PACF</strong>) correlates one data point to another lag, controlling for those lags between. We also discussed how inspection of the PACF plot is a frequently used method for assessing the ordering of an autoregressive model. Thereto, the autoregressive model is one that considers specific points in the past to be directly correlated to the value of a given point at lag zero. Suppose we have a process y t with random, normally distributed white noise, ϵ t, where t = ± 1, ± 2, …. If – using real constants of ϕ 1, ϕ 2, … , ϕ p where ϕ p ≠ 0 – we can formulate the process in the following way:</p>
<p>y t − μ − ϕ 1(y t−1 − μ) − ϕ 2(y t−2 − μ) − … − ϕ p(y p − μ) = ϵ t</p>
<p>Letting μ represent the overall process sample mean (in our examples, we will consider <strong class="bold">zero-mean</strong> processes), we can consider this to be an autoregressive process of order <em class="italic">p</em>, or AR(p) [<em class="italic">1</em>]. We can define the autocorrelation for the AR(p) model as follows:</p>
<p>ρ k = ϕ 1 |k|</p>
<p>There is also this example:</p>
<p>ρ k = ϕ 1 ρ k−1 + … + ϕ p ρ k−p</p>
<p>In the preceding example, where ρ k is the lag <em class="italic">k</em> autocorrelation. ϕ 1 is both the slope and autocorrelation for an AR(1) process.</p>
<p class="callout-heading">AR(p) model structure and components</p>
<p class="callout">To prevent confusion, note that with the equation y t − μ − ϕ 1(y t−1 − μ) − ϕ 2(y t−2 − μ) − … − ϕ p(y p − μ) = ϵ t we are attempting to build a mathematical model that represents the process such that if perfectly modeled, all that remains is the random, normally distributed white noise, ϵ t. This effectively means the model leaves zero residual error (in other words, a perfect fit). Each y t−k term – where <em class="italic">k</em> is a lag in time – represents the value at that point in time and each corresponding value of ϕ is the coefficient value required for the y t−k such that when taken in combination with all other values of <em class="italic">y</em>, the model statistically approximates zero error.</p>
<h3>The AR(1) model</h3>
<p>The <strong class="bold">backshift operator notation</strong>, or simply <strong class="bold">operator notation</strong>, is a<a id="_idIndexMarker806"/> simplified, shorthand <a id="_idIndexMarker807"/>method of formulating models. It is <a id="_idIndexMarker808"/>called “backshift” because it shifts time back one lag from <em class="italic">t</em> to <em class="italic">t-1</em>. The purpose is to avoid the necessitation of writing the subscript (y t−k) following every φ coefficient and instead writing B k−1 while including only y t once, which is handy when writing AR(p) models with high orders of <em class="italic">p</em>. In the following equation, the zero-mean form of an AR(1) follows the following structure:</p>
<p>y t − μ − ϕ 1(y t−1 − μ) = ϵ t</p>
<p>The equation reduces to the following example:</p>
<p>y t − ϕ 1(y t−1) = ϵ t</p>
<p>In backshift operator notation, we can say this:</p>
<p>( 1 − ϕ 1 B)y t = 𝝐 t</p>
<p class="callout-heading">Note on |𝝓 1| in an AR(1)</p>
<p class="callout">It’s worth noting at this point that an AR(1) process is stationary if |ϕ 1| &lt; 1. That is, when the absolute value of the lag-one autocorrelation is &lt; 1, the AR(1) process is stationary. When |ϕ 1| = 1, an ARIMA model may still be useful, but when |ϕ 1| &gt; 1, the process is considered to be explosive and should not be modeled. This is because a value |ϕ 1| &lt; 1 means the root is outside of, and not bounded by, the unit circle. A value of |ϕ 1| = 1 is on the unit circle but can be differenced to remove the unit root. The root for the case of an AR(1) can be calculated as z = 1/ϕ 1. A set of data producing |ϕ 1| &gt; 1 cannot be filtered in a way that puts its root outside the unit circle.</p>
<p class="callout">When all roots of an AR(p) are outside the unit circle, the given realization (one time series sampled from a stochastic process) will converge to the mean, have constant variance, and be independent of time. This is an ideal scenario for time series data.</p>
<p>Let’s walk <a id="_idIndexMarker809"/>through an example of an AR(1) process with |ϕ 1| &lt; 1 and therefore a stationary root. Assume we have identified the following first-order autoregressive process:</p>
<p>y t − 0.5 y t−1 = ϵ t</p>
<p>This is converted to operator notation:</p>
<p>(1 − 0.5B) y t = ϵ t</p>
<p>When looking for roots, we can use the following notation:</p>
<p>(1 − 0.5z) = 0</p>
<p>This gives us a root of <em class="italic">z</em>:</p>
<p>z =  1 _ ϕ 1  =  1 _ 0.5  = 2</p>
<p>Therefore, since the root is greater than 1 and thus outside the unit circle, the AR(1) representation of the process is stationary. In Python, we can build this process using the upcoming code. First, we build the AR(1) parameters, which we want to have a 0.5. Because we substitute 0.5 into the model X t − ϕ 1(y t−1) = ϵ t, we insert <em class="italic">0.5</em> and not <em class="italic">-0.5</em> for <code>arparams</code>. Also, note based on ρ k = ϕ 1 |k| that <em class="italic">0.5</em> is the lag-1 autocorrelation. The process we build will have an arbitrary sample size of <code>nsample=200</code>. We build the (1 − 0.5B) component of (1 − 0.5B) y t = ϵ t using the <code>np.r_[1, -</code><code>arparams]</code> step:</p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
import statsmodels.api as sm
import numpy as np
arparams = np.array([0.5])
ar = np.r_[1, -arparams]
ar_process = sm.tsa.ArmaProcess(ar)
y = ar_process.generate_sample(nsample=200)</pre>
<p>Now that we have the code to create the AR(1) we looked at the equation for, let’s see the roots and compare it to our manually calculated <em class="italic">z</em>:</p>
<pre class="source-code">
ar_process.arroots</pre>
<p><code>array([2.])</code></p>
<p>We can<a id="_idIndexMarker810"/> see the Python output, <code>2.</code> is the same as the calculation we performed. We know that since the absolute value of the root is greater than 1, the AR(1) process is stationary, but let’s confirm with Python:</p>
<pre class="source-code">
ar_process.isstationary</pre>
<p><code>True</code></p>
<p>We can observe by looking at the PACF that this is an autoregressive of order p = 5. We can also observe by looking at the ACF that the value of ϕ 1 is approximately 0.5. For an autoregressive model, the PACF is used to identify the number of significant lags to include as the order of the AR, and the ACF is used to determine the values of the coefficients, ϕ k, included in that order. It is simple to observe the values for an AR(1) using the ACF, but less obvious when p &gt; 1 since ACF does not control for individual lags when compared to the most recent point (lag zero) as the PACF does. Let’s generate the plots with Python:</p>
<pre class="source-code">
fig, ax = plt.subplots(1,3, figsize=(20,5))
ax[0].set_title('Realization')
ax[0].plot(y)
plot_acf(y, alpha=0.05, lags=50, ax=ax[1])
ax[1].set_title('ACF')
plot_pacf(y, alpha=0.05, lags=50, ax=ax[2])
ax[2].set_title('PACF')</pre>
<div><div><img alt="Figure 11.1 – The AR(1) process" height="349" src="img/B18945_11_001.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – The AR(1) process</p>
<p>Common<a id="_idIndexMarker811"/> for AR(1) processes, we see in <em class="italic">Figure 11</em><em class="italic">.1</em> a single significant partial autocorrelation in the PACF plot, excluding lag zero. Note there is some significance as we near lag 45, but because of the insignificance between lag 1 and those points, including those lags and constructing something such as an AR(50) would result in extreme overfitting; the coefficients from lag 2 through roughly lag 45 would fall between roughly 0 and ± 0.15. As referenced in <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, <em class="italic">Parametric Tests</em>, the correlation between about ± 0.1 and ± 0.3 is generally considered a weak correlation.</p>
<h3>The AR(2) model</h3>
<p>Let’s<a id="_idIndexMarker812"/> look at the following stationary AR(2) process:</p>
<p>y t − 0.8 y t−1 − 0.48 y t−2 = ϵ t</p>
<p>Converted to backshift operator notation, we have the following:</p>
<p>(1 − 0.8B − 0.48 B 2) y t = ϵ t</p>
<p>We also have this:</p>
<p>(1 − 0.8z − 0.48 z 2) = 0</p>
<p>Since we’re focusing on Python in this book, we won’t walk through the steps, but it may be useful to know second-order polynomials – such as AR(2) – follow the quadratic equation, a x 2 + bx + c (− 0.48 z 2 − 0.8z + 1 for our process). Therefore, we can use the quadratic formula:</p>
<p> − b ± √ _ b 2 − 4ac   ___________ 2a </p>
<p>This is what we’ll use to find the roots. In Python, we can find the roots of this model using the following:</p>
<pre class="source-code">
arparams = np.array([-0.8, -0.48])
ar = np.r_[1, -arparams]
ar_process = sm.tsa.ArmaProcess(ar)
print('AR(2) Roots: ', ar_process.arroots)
print('AR(2) Stationarity: ', ar_process.isstationary)</pre>
<p>Here we can see the unit roots identified using the <code>statmodels</code> ArmaProcess function:</p>
<p><code>AR(2) Roots:  [-</code><code>0.83333333-1.1785113j -0.83333333+1.1785113j]</code></p>
<p><code>AR(2) Stationarity:  </code><code>True</code></p>
<p>We can<a id="_idIndexMarker813"/> observe the roots are in complex conjugate form a ± bi. When a process has roots in complex conjugate form, it is expected the autocorrelations will exhibit an oscillatory pattern, which we can see in the ACF plot in <em class="italic">Figure 11</em><em class="italic">.2</em>. We can also observe the two significant lags in the PACF, which support the case of order p=2:</p>
<div><div><img alt="Figure 11.2 – AR(2) with complex conjugate roots" height="349" src="img/B18945_11_002.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – AR(2) with complex conjugate roots</p>
<p>To mathematically test that complex conjugate roots are stationary (outside the unit circle), we take the magnitude of the vector of each root’s real and imaginary parts and check if it is greater than 1. The magnitude for complex conjugate roots, <em class="italic">z</em>, following form a ± bi is the following equation:</p>
<p>‖z‖ = √ _ a 2 + b 2 </p>
<p>The magnitude of our roots is this:</p>
<p>√ _________________  − 0.8333 2± 1.1785 2  = 1.4433</p>
<p>Since 1.4433 &gt; 1, we <a id="_idIndexMarker814"/>know our AR(2) model is stationary.</p>
<p class="callout-heading">Identifying order p for AR models using the PACF</p>
<p class="callout">When<a id="_idIndexMarker815"/> identifying a lag order p for an autoregressive process, AR(p), based on the PACF plot, we take the maximum lag where significant partial autocorrelation exists as the order for p. In observing <em class="italic">Figure 11</em><em class="italic">.3</em>, because the PACF dampens after lag 4 and through about lag 30, we will cut off order consideration after lag 4 because using more lags (consider them as features for a time series model) will likely result in overfitting. The order selected using PACF is based on the last significant lag before the partial autocorrelations dampen. While lags 2 and 3 seem to be small and may not be significant, lag 4 is. Therefore, we may get the best model using an AR of order 4. Typically, we test our assumptions using errors with information criteria, such as AIC or BIC.</p>
<div><div><img alt="Figure 11.3 – AR(p) order identification" height="489" src="img/B18945_11_003.jpg" width="717"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – AR(p) order identification</p>
<h3>AR(p) end-to-end example</h3>
<p>Let us <a id="_idIndexMarker816"/>walk through an end-to-end example <a id="_idIndexMarker817"/>of AR(p) modeling in Python. First, we need to generate a dataset produced by an AR(4) process. We will use this data as the process we will attempt to model:</p>
<pre class="source-code">
arparams = np.array([1.59, -0.544, -0.511, 0.222])
ar = np.r_[1, -arparams]
ar_process = sm.tsa.ArmaProcess(ar)
y = ar_process.generate_sample(nsample=200)</pre>
<p>For the <a id="_idIndexMarker818"/>following steps, let us assume data <code>y</code> is the <a id="_idIndexMarker819"/>output of a machine about which we know nothing.</p>
<h4>Step 1 - visual inspection</h4>
<p>We first<a id="_idIndexMarker820"/> visualize the original data and its ACF and PACF plots using the code we provided earlier in the chapter:</p>
<div><div><img alt="Figure 11.4 – Step 1 in model development: visual inspection" height="347" src="img/B18945_11_004.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Step 1 in model development: visual inspection</p>
<p>We can see based on the PACF plot we have what appears to be an AR(2), but possibly an AR(4). After lag 4, the partial autocorrelations lose statistical significance at the 5% level of significance. We can see, however, when considering the statistical significance of lag 4 in the PACF, however slight, lag 4 in the ACF is significant. While the value at lag 4 is not the value of the coefficient, its significance is useful in helping determine order p. Nonetheless, an AR(4) may overfit and fail to<a id="_idIndexMarker821"/> generalize as well as an AR(2). Next, we will use <strong class="bold">Aikake Information Criterion</strong> (<strong class="bold">AIC</strong>) and <strong class="bold">Bayesian Information Criterion</strong> (<strong class="bold">BIC</strong>) to help <a id="_idIndexMarker822"/>make our determination.</p>
<p>Based on the constant mean and the fact we do not have an exponentially dampening (which would also need to be significant) ACF, there does not appear to be a trend.</p>
<h4>Step 2 - selecting the order of AR(p)</h4>
<p>Since <a id="_idIndexMarker823"/>we are uncertain based on the visual inspection of the order we should use for the AR(p) model, we will use AIC and BIC to help our decision. The AIC and BIC process will fit models using order zero up to the <code>max_ar</code> value provided in the upcoming code. The models will fit the entire dataset. The order with the lowest error is generally the best. Their error calculations are as follows:</p>
<p>AIC = 2k − 2ln( ˆ L )</p>
<p>BIC = kln(n) − 2ln( ˆ L )</p>
<p>This is where <em class="italic">k</em> is the number of lags – up to the maximum order tested – for the data,  ˆ L  is the maximum likelihood estimate, and <em class="italic">n</em> is the sample size (or length of the dataset being tested). For both tests, the lower error is better.</p>
<p>We will import <code>arma_order_select_ic</code> from <code>Statsmodels</code> and test it using up to a maximum of 4 lags based on our observation in the PACF plot in <em class="italic">Figure 11</em><em class="italic">.4</em>. As noted, based on our visual inspection, we do not appear to have a trend. However, we can verify this statistically with an OLS-based unit root test called<a id="_idIndexMarker824"/> the <strong class="bold">Dickey-Fuller test</strong>. The <strong class="bold">null hypothesis</strong> of <a id="_idIndexMarker825"/>the Dickey-Fuller test is that a unit root (and therefore, trend) is present at some point in the maximum number of lags tested (maxlag). The alternative hypothesis is that there is no unit root (no trend) in the data. For reference, the alternative hypothesis states that the data is an order zero - <strong class="bold">I(0)</strong> - integrated process while the null hypothesis states that the data is an order one - <strong class="bold">I(1)</strong> - integrated process. If the absolute value of the test statistic is greater than the critical value or the p-value is significant, we can conclude there is no trend present (no unit root).</p>
<p>The Dickey-Fuller test considers each data point out to the number of lags included in the regression test. We will need to analyze the ACF plot for this; because we want to consider as far out as a trend could be possible, we must choose the longest lag that has significance. The idea is that if we have a strong trend in our data, such as growth, for the most part, each sequential value will lead to another increasing subsequent value for as long as the trend exists. In our case, the maximum significant lag in the ACF plot is approximately 25. The Dickey-Fuller test has relatively low statistical power (prone to Type II error or failing to reject the null when the null should be rejected) so a high order of lags is not concerning so long as it is practical; the risk is failing to include enough lags.</p>
<p class="callout-heading">Dickey-Fuller unit roots</p>
<p class="callout">The Dickey-Fuller tests only if there is a trend unit root, but not if there is a seasonal unit root. We discuss the difference between trend and seasonal unit roots in the ARIMA section of this chapter.</p>
<p>In the upcoming <a id="_idIndexMarker826"/>code block, we add <code>maxlag=25</code> for our 25 lags from the ACF plot in <em class="italic">Figure 11</em><em class="italic">.4</em>. We will also include <code>regression='c'</code>, which adds a constant (or intercept) into the OLS it performs; we will not need to manually add the constant in that case:</p>
<pre class="source-code">
from statsmodels.tsa.stattools import adfuller
dicky_fuller = adfuller(y, maxlag=25, regression='c')
print('Dickey-Fuller p-value: ', dicky_fuller[1])
print('Dickey-Fuller test statistic: ', dicky_fuller[0])
print('Dickey-Fuller critical value: ', dicky_fuller[4].get('5%'))</pre>
<p>We can see based on the Dickey-Fuller test that we should reject the null hypothesis and conclude the process is order-zero integrated and therefore does not have trend:</p>
<p><code>Dickey-Fuller p-value:  </code><code>1.6668842047161513e-06</code></p>
<p><code>Dickey-Fuller test statistic:  -</code><code>5.545206445371327</code></p>
<p><code>Dickey-Fuller critical value:  -</code><code>2.8765564361715534</code></p>
<p>We can therefore insert into our <code>arma_order_select_ic</code> function that <code>trend='n'</code> (otherwise, we may want to difference the data, which we will show in the ARIMA section of the chapter):</p>
<pre class="source-code">
from statsmodels.tsa.stattools import arma_order_select_ic
model_ar = arma_order_select_ic(y=y, max_ar=4, max_ma=0,
                                ic=['aic','bic'], trend='n')
print('AIC Order Selection: ', model_ar.aic_min_order)
print('AIC Error: ', round(model_ar.aic.min()[0], 3))
print('BIC Order Selection: ', model_ar.bic_min_order)
print('BIC Error: ', round(model_ar.bic.min()[0], 3))</pre>
<p>Here we can see the AR and MA orders identified to produce a fit with the lowest overall error according to our AIC and BIC tests:</p>
<p><code>AIC Order Selection:  (</code><code>4, 0)</code></p>
<p><code>AIC Error:  </code><code>586.341</code></p>
<p><code>BIC Order Selection:  (</code><code>2, 0)</code></p>
<p><code>BIC Error:  </code><code>597.642</code></p>
<p>We can <a id="_idIndexMarker827"/>see AIC selected an AR(4) and BIC selected an AR(2). It is preferable that both tests select the same term orders. However, as we noted already, the AR(2) may be less likely to overfit. Since the best order isn’t completely clear, we will proceed to test both models (using AR(2) and AR(4)) by comparing their errors and log-likelihood estimates.</p>
<h4>Step 3 - building the AR(p) model</h4>
<p>In this step, we <a id="_idIndexMarker828"/>can add our arguments to the <code>statsmodels’</code> ARIMA function and fit it to the data with our prescribed AR(4). To be clear, an AR(4) is the same as an ARIMA(4,0,0). We want to include <code>enforce_stationarity=True</code> to ensure our model will produce useful results. If not, we will receive a warning and need to address the issue by either differencing, using a different model – such as SARIMA, changing our sampling method, changing our time binning (from days to weeks, for example), or abandoning time series modeling for the data altogether:</p>
<pre class="source-code">
from statsmodels.tsa.arima.model import ARIMA
ar_aic = ARIMA(y, order=(4,0,0),
               enforce_stationarity=True).fit()
print(ar_aic.summary())</pre>
<p>In our model output, we can see the <em class="italic">SARIMAX Results</em> title and <em class="italic">Model: ARIMA(4,0,0)</em>. This can be disregarded. A SARIMAX with no seasonal component and no exogenous variables (in our case) is simply an ARIMA. Further, an ARIMA of order (4,0,0) is an <em class="italic">AR(4)</em>:</p>
<div><div><img alt="Figure 11.5 – AR(4) model results" height="528" src="img/B18945_11_005.jpg" width="688"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – AR(4) model results</p>
<p>The AR(4) process <a id="_idIndexMarker829"/>we modeled (the simulated process we built prior to step 1 is </p>
<p>y t − 1.59 y t−1 + 0.544 y t−2 + 0.511 y t−3 − 0.222 y t−4 = ϵ t</p>
<p>and the AR(4) model we produced using the data from the input process is the following:</p>
<p>y t − 1.6217 y t−1 + 0.6877 y t−2 + 0.3066 y t−3 − 0.1158 y t−4 = ϵ t</p>
<p>In backshift operator notation, we have the following equation:</p>
<p>(1 − 1.6217B + 0.6877 B 2 + 0.3066 B 3 − 0.1158 B 4) y t = ϵ t</p>
<p>Notably, the term for lag 4 is not significant and the confidence interval contains 0. Therefore, including this term is a known risk for overfitting and something worth weighing if considering alternative models. It would be prudent to compare an AR(2) and even an AR(3) to our AR(4) based on AIC and BIC and choose a different model if the results are much improved, but we will skip this process for the sake of time.</p>
<p>Regarding the model summary metrics, we discussed <a id="_idIndexMarker830"/>the <strong class="bold">Ljung-Box test</strong> in the last chapter so will not cover the details here, but the high p-value <em class="italic">(Prob(Q)</em>) for that test indicates there is not correlated error at lag 1. Typically, if there is serial correlation in the residuals of a model fit, the residuals will have lag 1 autocorrelation. The <strong class="bold">Jarque-Bera test</strong> assumes<a id="_idIndexMarker831"/> the errors are normally distributed under the null hypothesis and not normally distributed under the alternative hypothesis. The high p-value <em class="italic">(Prob(JB)</em>) for that test suggests the error is normally distributed. The test for <strong class="bold">heteroskedasticity</strong> tests<a id="_idIndexMarker832"/> the null that the residuals are constant (homoscedastic) with the alternative hypothesis being that they are non-constant, which is an issue for a time series regression fit. Here, our Heteroskedasticity p-value <em class="italic">(Prob(H)</em>) is high so we can assume our model’s residuals <a id="_idIndexMarker833"/>have constant variance. A <strong class="bold">skew</strong> score between [-0.5, 0.5] is <a id="_idIndexMarker834"/>considered not skewed, whereas between [-1, -0.5] or [0.5, 1] is moderately skewed and &gt; ±2 is high. A perfect <a id="_idIndexMarker835"/>score for <strong class="bold">kurtosis</strong> is 3. Kurtosis &gt; ±7 is high. Because our skew is 0.04 and our kurtosis score is 2.58, we can assume our residuals are normally distributed.</p>
<h4>Step 4 - test forecasting</h4>
<p>Another <a id="_idIndexMarker836"/>method for validating a model is to forecast existing points using data leading up to those points. Here, we use the model to forecast the last 5 points using the full dataset excluding those last 5 points. We then compare to get an idea of model performance. Note that we generated 200 samples and the index for those samples starts at 0. Therefore, our 200th sample is at positional index 199:</p>
<pre class="source-code">
df_pred = ar_aic.get_prediction(start=195, end=199).summary_frame(alpha=0.05)
df_pred.index=[195,196,197,198,199] # reindexing for 0 index</pre>
<p>In the following table, the <em class="italic">mean</em> column is the forecast. We manually appended the <em class="italic">actuals</em> column with the last 5 values in our data to compare to the forecast. <em class="italic">mean_se</em> is our mean squared error for our estimates compared to actuals. <em class="italic">y</em> is our index and the <em class="italic">ci</em> columns are for our 95% forecast confidence interval since we used <code>alpha=0.05</code> in the previous code.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-6">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">y</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_se</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_ci_lower</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_ci_upper</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">actuals</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>195</p>
</td>
<td class="No-Table-Style">
<p>24.70391</p>
</td>
<td class="No-Table-Style">
<p>0.99906</p>
</td>
<td class="No-Table-Style">
<p>22.74579</p>
</td>
<td class="No-Table-Style">
<p>26.662035</p>
</td>
<td class="No-Table-Style">
<p>25.5264</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>196</p>
</td>
<td class="No-Table-Style">
<p>19.36453</p>
</td>
<td class="No-Table-Style">
<p>0.99906</p>
</td>
<td class="No-Table-Style">
<p>17.4064</p>
</td>
<td class="No-Table-Style">
<p>21.322652</p>
</td>
<td class="No-Table-Style">
<p>18.8797</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>197</p>
</td>
<td class="No-Table-Style">
<p>7.525904</p>
</td>
<td class="No-Table-Style">
<p>0.99906</p>
</td>
<td class="No-Table-Style">
<p>5.567779</p>
</td>
<td class="No-Table-Style">
<p>9.484028</p>
</td>
<td class="No-Table-Style">
<p>7.4586</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>198</p>
</td>
<td class="No-Table-Style">
<p>-5.8744</p>
</td>
<td class="No-Table-Style">
<p>0.99906</p>
</td>
<td class="No-Table-Style">
<p>-7.83252</p>
</td>
<td class="No-Table-Style">
<p>-3.916274</p>
</td>
<td class="No-Table-Style">
<p>-7.1316</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>199</p>
</td>
<td class="No-Table-Style">
<p>-19.5785</p>
</td>
<td class="No-Table-Style">
<p>0.99906</p>
</td>
<td class="No-Table-Style">
<p>-21.5366</p>
</td>
<td class="No-Table-Style">
<p>-17.620356</p>
</td>
<td class="No-Table-Style">
<p>-17.9268</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – AR(4) model outputs versus actuals</p>
<p>We can<a id="_idIndexMarker837"/> see based on our mean squared error (0.999062) that our model provides a reasonable fit across a forecast horizon of 5 points on test data. Using the following code, we plot our test forecast against the corresponding actuals:</p>
<pre class="source-code">
fig, ax = plt.subplots(1,1,figsize=(20,5))
ax.plot(y, marker='o', markersize=5)
ax.plot(df_pred['mean'], marker='o', markersize=4)
ax.plot(df_pred['mean_ci_lower'], color='g')
ax.plot(df_pred['mean_ci_upper'], color='g')
ax.fill_between(df_pred.index, df_pred['mean_ci_lower'], df_pred['mean_ci_upper'], color='g', alpha=0.1)
ax.set_title('Test Forecast for AR(4)')</pre>
<div><div><img alt="Figure 11.7 – The AR(4) test forecast" height="347" src="img/B18945_11_007.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – The AR(4) test forecast</p>
<h4>Step 5 - building a forecast</h4>
<p>Determining a <a id="_idIndexMarker838"/>reasonable forecast horizon is highly dependent on at least the data and the process it represents, and the lag used for modeling, in addition to model error. The time series practitioner should weigh all factors of model performance and business needs versus risks before providing forecasting to stakeholders. Adding the following code, we re-run the plot to see our true forecast with a 5-point horizon:</p>
<pre class="source-code">
df_forecast = ar_aic.get_prediction(start=200, end=204).summary_frame(alpha=0.05)
df_forecast.index=[200, 201, 202, 203, 204]
forecast = np.hstack([np.repeat(np.nan, len(y)), df_pred['mean']])</pre>
<div><div><img alt="Figure 11.8 – The AR(4) forecast horizon = 5" height="347" src="img/B18945_11_008.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – The AR(4) forecast horizon = 5</p>
<p>We will cover additional steps in the model evaluation section of this chapter.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor180"/>Moving average (MA) models</h2>
<h3>The MA(q) model</h3>
<p>Whereas<a id="_idIndexMarker839"/> the AR(p) model is a direct function<a id="_idIndexMarker840"/> of the correlation between lag zero and specific individual lags of order <em class="italic">p</em> over time, the moving average model of order <em class="italic">q</em>, MA(q), is a function of autocorrelation between lag zero and all previous lags included in order <em class="italic">q</em>. It acts as a low-pass filter that models errors to provide a useful fit to data.</p>
<p>Let us take a process, y t, that has a mean of zero and a random, normally distributed white noise component, ϵ t, where t = ± 1, ± 2, …. If we can write this process as</p>
<p>y t − μ = ϵ t − ϴ 1 ϵ t−1 − … − ϴ q ϵ t−q</p>
<p>and ϴ 1, ϴ 2, … , ϴ q are real constants and ϴ q ≠ 0, then we can call this a moving average process having order <em class="italic">q</em>, or MA(q). In backshift operator notation, we have the following:</p>
<p>y t − μ = (1 − ϴ 1 B − … − ϴ q B q) ϵ t</p>
<p>We can define the autocorrelations (ρ k) for the MA(q) model thusly:</p>
<p>ρ k =  − ϴ k + ∑ j=1 q−k ϴ j ϴ j+k  _____________  1 + ∑ j=1 q  ϴ j 2 </p>
<p>For all lags <em class="italic">k</em> in 1,2, … , q. Where k &gt; q, we have ρ k = 0.</p>
<p>When <a id="_idIndexMarker841"/>discussing the AR(p) models, we explained how the roots of the AR model must be outside the unit circle. When considering MA(q) models, we have<a id="_idIndexMarker842"/> the concept of invertibility. <strong class="bold">Invertibility</strong> essentially ensures a <em class="italic">logical and stable correlation with the past</em>. Typically, this means the current point in time is more closely related to nearby points in the past than those more distant. The inability to model a process using invertible roots means we cannot ensure our model provides a unique solution for the set of model autocorrelations. If points in the distant past are more relevant to the current point than those nearby, we have a randomness in the process that cannot be reliably modeled or forecasted.</p>
<p class="callout-heading">Identifying MA(q) model invertibility</p>
<p class="callout">For a moving <a id="_idIndexMarker843"/>average model to be invertible, all roots must be outside the unit circle and non-imaginary; <em class="italic">all</em> roots must be greater than 1. For an MA(1) process, it is invertible when |ϴ 1| &lt; 1. An invertible MA(q) process is equivalent to an infinite-order, converging AR(p). An AR(p) model converges if its coefficients converge to zero as lags k approach p. If an MA(q) is invertible, we can say y t = ϴ(B) ϵ t and ϴ −1(B) y t = ϵ t [<em class="italic">1</em>].</p>
<h3>The MA(1) model</h3>
<p>For an MA(q) model <a id="_idIndexMarker844"/>of order 1, we have the following process:</p>
<p>ρ 0 = 1</p>
<p>ρ 1 =  ϴ 1 _ 1 + ϴ 1 2 </p>
<p>ρ k&gt;1 = 0</p>
<p>For a MA(q) model with zero autocorrelation, the pattern of whose process we are attempting to model is random, normally distributed white noise variance, which can – at best – only be modeled by its mean. It is important to note that as ϴ 1 → 0, ρ 1 → 0 and for an MA(1) process, this means the process can be approximated by white noise.</p>
<p>Let us consider the following MA(1) zero-mean model following the form y t − μ = ϵ t − ϴ 1 ϵ t−1:</p>
<p>y t − 0 = a t − 0.8 ϵ t−1</p>
<p>In backshift notation we have the following:</p>
<p>y t = (1 − 0.8B) ϵ t</p>
<p>We know this process is invertible because |ϴ 1| &lt; 1. Let us confirm this with Python using the <code>ArmaProcess</code> function from the <code>statsmodels</code> <code>tsa</code> module:</p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
import statsmodels.api as sm
import numpy as np
maparams = np.array([0.8])
ma = np.r_[1, -maparams]
ma_process = sm.tsa.ArmaProcess(ma=ma)
print('MA(1) Roots: ', ma_process.maroots)
print('MA(1) Invertibility: ', ma_process.isinvertible)</pre>
<p><code>MA(1) Roots:  [</code><code>1.25]</code></p>
<p><code>MA(1) Invertibility:  </code><code>True</code></p>
<p>Contrary to the AR(p) model, the MA(q) model’s order is identified using the ACF plot. Because the ACF does not control for lags and is a composite autocorrelation measure across all lags up to the lag whose autocorrelation measure is considered, the function is used to identify the relevant lag order for the moving average component. In <em class="italic">Figure 11</em><em class="italic">.9</em>, we can <a id="_idIndexMarker845"/>see the significant correlation at lag 1 for our MA(1) process. There are two additional significant correlations at lags 6 and 7, but using lags this far out typically results in overfitting, especially when taken alongside the fact that lags 2 through 5 are not significant at the 5% level of significance:</p>
<div><div><img alt="Figure 11.9 – MA(1) plots" height="349" src="img/B18945_11_009.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – MA(1) plots</p>
<p class="callout-heading">Dampening ACFs and PACFs</p>
<p class="callout">For an invertible moving average model, we can observe that the ACF will cut off at the order of significance, but the PACF will typically continue and dampen <em class="italic">overall</em> to statistical zero over time. It is not necessarily expected to happen smoothly and all at once as all data sets are different, but it is expected that over time, more and more lags will dampen to zero. We explain the reason for this behavior from the ACFs and PACFs in the ARMA section of this chapter, but it is worth noting this is to be expected for invertible processes. Conversely, stationary autoregressive processes are expected to cut off at the order of significance in the PACF plots while the ACFs dampen to zero over time.</p>
<h3>The MA(2) model</h3>
<p>For an MA(q) model <a id="_idIndexMarker846"/>of order 2, we have this:</p>
<p>ρ 0 = 1</p>
<p>ρ 1 =  − ϴ 1 + ϴ 1 ϴ 2  _ 1 + ϴ 1 2 + ϴ 2 2 </p>
<p>ρ 2 =  − ϴ 2 _ 1 + ϴ 1 2 + ϴ 2 2 </p>
<p>ρ k&gt;2 = 0</p>
<p>The following is an MA(2) example we will look at:</p>
<p>y t = (1 − 1.6B + 0.9 B 2) ϵ t</p>
<p>Using the <a id="_idIndexMarker847"/>model’s polynomial in the quadratic equation form, we can find the approximate roots using the quadratic formula:</p>
<p>0.888 ± 0.567i</p>
<p>Because we have two complex conjugate roots, we can take the same L 2 norm we did for the AR(p) process, using the form of a ± bi:</p>
<p>√ _____________  0.888 2 + 0.567 2  ≈ 1.054</p>
<p>Because 1.054 is greater than 1, we can confirm the MA(2) has invertible roots and is thus capable of producing a unique solution and a model whose values are logically serially correlated to past values. Let us perform the same analysis in Python:</p>
<pre class="source-code">
maparams = np.array([1.6, -0.9])
ma = np.r_[1, -maparams]
ma_process = sm.tsa.ArmaProcess(ma=ma)
print('MA(2) Roots: ', ma_process.maroots)
print('MA(2) Invertibility: ', ma_process.isinvertible)</pre>
<p>The output we see highlighted in green confirms our calculated findings and the fact that since the magnitude of the complex conjugate roots is greater than 0, we have an invertible MA(2) process:</p>
<p><code>MA(2) Roots:  [</code><code>0.88888889-0.56655772j 0.88888889+0.56655772j]</code></p>
<p><code>MA(2) Invertibility:  </code><code>True</code></p>
<p>We can see in the ACF in <em class="italic">Figure 11</em><em class="italic">.10</em> that this is a second-order moving average process:</p>
<div><div><img alt="Figure 11.10 – MA(2) plots" height="349" src="img/B18945_11_010.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – MA(2) plots</p>
<p>The <a id="_idIndexMarker848"/>process and code for identifying model ordering, building the model, and generating a forecast are the same for the MA(q) model as it is for the AR(p) model. We have discussed that the order selection based on visual inspection for the MA(q) is performed using the ACF, whereas for the AR(p) this is done using the PACF, and that it is the only major difference in the process between the two models. Aside from that, <code>enforce_invertibility</code> should be set to equal <code>True</code> for MA(q) models in place of <code>enforce_stationarity=True</code>. Providing a <code>max_ar</code> or <code>max_ma</code> order higher or lower than useful in the <code>arma_order_select_ic</code> function may result in a <em class="italic">convergence warning or an invertibility warning</em>. One reason for these warnings is there was a higher order provided than possible to fit (such as when there is no order possible). Another reason is the presence of a <strong class="bold">unit root</strong>. If<a id="_idIndexMarker849"/> there is an apparent trend in the data, it <em class="italic">must be removed</em> before modeling. If there is no trend, it is possible to receive this error due to seasonality in the data, which presents a different order of unit root. We will discuss modeling in the case of unit roots associated with trend and seasonality in the ARIMA and seasonal ARIMA section of this chapter. It is also worth specifying that the Dickey-Fuller test can be used for moving average data since moving average processes can be influenced by trends.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor181"/>Autoregressive moving average (ARMA) models</h2>
<p>In the<a id="_idIndexMarker850"/> autoregressive<a id="_idIndexMarker851"/> model section, we discussed how an AR(p) model is used to model process output values using autocorrelation controlling for individual lags. The goal of the AR(p) model is to estimate exact values for points corresponding to lags in the future using the values for the same specific lags in the context of a past horizon. For example, the value at two points in the future is strongly <a id="_idIndexMarker852"/>correlated with the value at two points in the past. In the moving average model section, we discussed how MA(q) models act as low-pass filters that help a model explain noise in a process. Rather than seeking to model exact points, we use the MA(q) to model variance around the process.</p>
<p>Consider<a id="_idIndexMarker853"/> an example of a four-cylinder car engine that produces constant output. Let us assume we have a worn-down motor mount near the fourth cylinder. We can expect consistent output vibration related to each cylinder firing, but the vibration will increase slightly for each stroke that is closer to the worn motor mount. Using an AR-only model would assume each cylinder vibrates a certain amount and be able to account for that, but we would be missing information. Adding a MA component would be able to model the fact that starting at cylinder one, each subsequent stroke up through cylinder four will have additional vibration related to the worn motor mount and thus explain much more of the overall process. This would reasonably be an ARMA(4,4) model. Suppose we replace the worn-down motor mount with a mount of equal wear compared to the other mounts; we would then have an ARMA(4,0) (or AR(4)) process.</p>
<p>In many cases, we find we have significant peaks in both autocorrelation and partial autocorrelation. Rather than using only MA(q) or AR(p) modeling, respectively, we can combine the two. This combination, represented as an ARMA(p,q), enables us to model the process as well as any noise component around the process that may correlate to specific lags. Because an ARMA(p,q) typically has fewer parameters (lower order) for each AR and MA component <a id="_idIndexMarker854"/>than AR or MA models do, the ARMA is considered a <strong class="bold">parsimonious model</strong>, which is a model that uses as few explanatory variables (in this case, time lags) as possible to achieve the desired level of performance. When y t is an invertible and stationary process, we can define it as an ARMA(p,q):</p>
<p>y t − μ = Φ 1(y t−1 − μ) − … − Φ p(y t−p − μ) = ϵ t − ϴ 1 ϵ t−1 − … − ϴ q ϵ t−q</p>
<p>Where Φ p ≠ 0 and ϴ q ≠ 0, we can re-write the equation for ARMA(p,q) in backshift operator notation:</p>
<p>ΦB(y t − μ) = ϴ(B) ϵ t</p>
<p>Practically speaking, we can expect that for an invertible moving average process, we see significant lags in the ACF up to the magnitude of the order <em class="italic">q</em>, but then the PACF will taper off thereafter, typically in lags beyond the order of the moving average process identified in the ACF. This is because a finite moving average process can be represented as an infinite-order autoregressive process. Conversely, because the moving average process that has this behavior is invertible, the inverse must also be true; that a finite autoregressive process can be represented as an infinite-order moving average process. Therefore, the PACF will dampen to zero for an invertible moving average process, and for a <a id="_idIndexMarker855"/>stationary autoregressive process, the <a id="_idIndexMarker856"/>ACF will dampen to zero. Because invertibility is a requirement of an ARMA process, it allows us to re-write the equation as an infinite-order MA process in general linear form:</p>
<p>y t = Φ −1(B)ϴ(B) ϵ t</p>
<p>It also allows us to do so as an infinite-order AR process:</p>
<p>ϴ −1(B)Φ(B) y t = ϵ t</p>
<p>Let us walk through an example in Python. First, using the same imports we did before in this chapter, let us generate an invertible and stationary ARMA(2,1) process dummy dataset that satisfies the following equation:</p>
<p>(1 − 1.28B + 0.682 B 2) y t = (1 − 0.58B) ϵ t</p>
<pre class="source-code">
arparams = np.array([1.2, -0.6])
ar = np.r_[1, -arparams]
maparams = np.array([0.5])
ma = np.r_[1, -maparams]
arma_process = sm.tsa.ArmaProcess(ar=ar, ma=ma)</pre>
<p>Let us confirm stationary and invertibility:</p>
<pre class="source-code">
print('AR(2) Roots: ', arma_process.arroots)
print('AR(2) Invertibility: ', arma_process.isstationary)
print('MA(1) Roots: ', arma_process.maroots)
print('MA(1) Invertibility: ', arma_process.isinvertible)</pre>
<p>We can use the quadratic formula to test, but we can trust the code to confirm:</p>
<p><code>AR(2) Roots:  [</code><code>1.-0.81649658j 1.+0.81649658j]</code></p>
<p><code>AR(2) Stationarity:  </code><code>True</code></p>
<p><code>MA(1) Roots:  [</code><code>2.]</code></p>
<p><code>MA(1) Invertibility:  </code><code>True</code></p>
<p>Now that <a id="_idIndexMarker857"/>we have a <a id="_idIndexMarker858"/>stationary and invertible process, let us generate 200 samples from it:</p>
<pre class="source-code">
y = arma_process.generate_sample(nsample=200)</pre>
<h4>Step 1 – Visual inspection</h4>
<p>Let us <a id="_idIndexMarker859"/>take a look at the plots we have been using to build intuition about the process that generated the data:</p>
<div><div><img alt="Figure 11.11 – ARMA(p,q) process sample data" height="460" src="img/B18945_11_011.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – ARMA(p,q) process sample data</p>
<p>We can see using the ACF that we have what appears to be an MA(1) component. Based on the PACF, it looks as if we could have either an AR(2) or an AR(4). The realization appears to be a process that satisfies stationarity.</p>
<h4>Step 2 – Select order of ARMA(p,q)</h4>
<p>Before<a id="_idIndexMarker860"/> we decide on an order for our ARMA model, let us use the Dickey-Fuller test to check if our data has any trend:</p>
<pre class="source-code">
from statsmodels.tsa.stattools import adfuller
dicky_fuller = adfuller(y, maxlag=25, regression='c')
print('Dickey-Fuller p-value: ', dicky_fuller[1])
print('Dickey-Fuller test statistic: ', dicky_fuller[0])
print('Dickey-Fuller critical value: ', dicky_fuller[4].get('5%'))</pre>
<p>We can see the statistical significance that confirms we do not have a unit root in the lags provided with <code>maxlag</code> (remember, H o : <em class="italic">the data has a unit root</em> and H a : <em class="italic">the data does not have a unit root</em>). Therefore, we can use the ARMA model without any first-order differencing, which would require at least an ARIMA:</p>
<p><code>Dickey-Fuller p-value:  </code><code>6.090665062133195e-16</code></p>
<p><code>Dickey-Fuller test statistic:  -</code><code>9.40370671340928</code></p>
<p><code>Dickey-Fuller critical value:  -</code><code>2.876401960790147</code></p>
<p>Now let us use <code>statmodels</code> <code>arma_order_select_ic</code> to see what AIC and BIC select for the ARMA(p,q) order. We know the maximum order for an MA(q) is one, but since we are not sure if this is an AR(2) or an AR(4), we can use <code>max_ar=4</code>:</p>
<pre class="source-code">
from statsmodels.tsa.stattools import arma_order_select_ic
model_arma = arma_order_select_ic(y=y, max_ar=4, max_ma=1, ic=['aic','bic'], trend='n')
print('AIC Order Selection: ', model_arma.aic_min_order)
print('AIC Error: ', round(model_arma.aic.min()[0], 3))
print('BIC Order Selection: ', model_arma.bic_min_order)
print('BIC Error: ', round(model_arma.bic.min()[0], 3))</pre>
<p>We can see that AIC selected an ARMA(4,1) and BIC selected an ARMA(2,1):</p>
<p><code>AIC Order Selection:  (</code><code>4, 1)</code></p>
<p><code>AIC Error:  </code><code>548.527</code></p>
<p><code>BIC Order Selection:  (</code><code>2, 1)</code></p>
<p><code>BIC Error:  </code><code>565.019</code></p>
<p>The ARMA(4,1) has a<a id="_idIndexMarker861"/> lower error, but we know from this and previous chapters in the book that models with lower error on training data may be more likely to have more variance and thus be more likely to overfit. However, let us use ARMA(4,1).</p>
<h4>Step 3 – Building the AR(p) model</h4>
<p>Now let us<a id="_idIndexMarker862"/> build our ARMA(4,1) model. Note the 0 is for the integrated first-order difference for an ARIMA(p,d,q) model. Since we do not have a trend-based unit root, we do not need to difference to remove any trend. Thus, d=0:</p>
<pre class="source-code">
from statsmodels.tsa.arima.model import ARIMA
arma_aic = ARIMA(y, order=(4,0,1),
                 enforce_stationarity=True, enforce_invertibility=True).fit()
print(arma_aic.summary())</pre>
<p>Here, we can see the model provides a reasonable fit based on the model metrics. However, there is one issue; our first three AR coefficients have no statistical significance (high p-values and confidence intervals containing zero). This is a big problem and confirms our model is overfitted. Our model includes terms it is not getting benefit from. Therefore, our model would most certainly fail to generalize well on unseen data and should be reconstructed:</p>
<div><div><img alt="Figure 11.12 – ARIMA(4,0,1) model results" height="556" src="img/B18945_11_012.jpg" width="687"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – ARIMA(4,0,1) model results</p>
<p>Let us <a id="_idIndexMarker863"/>re-run this as an ARMA(2,1), which is the same as an ARIMA(2,0,1) since there is no differencing to be integrated. This is what we visually identified, and BIC selected the following code:</p>
<pre class="source-code">
from statsmodels.tsa.arima.model import ARIMA
arma_aic = ARIMA(y, order=(2,0,1),
                 enforce_stationarity=True, enforce_invertibility=True).fit()
print(arma_aic.summary())</pre>
<p>We can see now a much better fit for our variables. All coefficients are significant, and the model metrics remain sufficient. The model we have identified corresponds to the following equation:</p>
<p>(1 − 1.2765B + 0.6526 B 2) y t = (1 + 0.58B) ϵ t</p>
<p>We can compare that to our dummy process:</p>
<p>(1 − 1.28B + 0.682 B 2) y t = (1 − 0.58B) ϵ t</p>
<div><div><img alt="Figure 11.13 – ARIMA(2,0,1) model results" height="506" src="img/B18945_11_013.jpg" width="690"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – ARIMA(2,0,1) model results</p>
<h4>Step 4 – Test forecasting</h4>
<p>Now, let us <a id="_idIndexMarker864"/>cross-validate our model by training the model on data up through the last five points, then forecasting the last five points so that we can compare them to the actuals. Recall that our indexing starts at 0 so our dataset ends at index 199:</p>
<pre class="source-code">
df_pred = arma_aic.get_prediction(start=195, end=199).summary_frame(alpha=0.05)
df_pred.index=[195,196,197,198,199]</pre>
<p>We can see our predicted values in the <em class="italic">mean</em> column in the following table:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">y</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_se</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_ci_lower</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">mean_ci_upper</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">actuals</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>195</p>
</td>
<td class="No-Table-Style">
<p>-0.01911</p>
</td>
<td class="No-Table-Style">
<p>0.932933</p>
</td>
<td class="No-Table-Style">
<p>-1.84762</p>
</td>
<td class="No-Table-Style">
<p>1.80940631</p>
</td>
<td class="No-Table-Style">
<p>0.559875</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>196</p>
</td>
<td class="No-Table-Style">
<p>0.58446</p>
</td>
<td class="No-Table-Style">
<p>0.932933</p>
</td>
<td class="No-Table-Style">
<p>-1.24406</p>
</td>
<td class="No-Table-Style">
<p>2.412975242</p>
</td>
<td class="No-Table-Style">
<p>0.778127</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>197</p>
</td>
<td class="No-Table-Style">
<p>0.479364</p>
</td>
<td class="No-Table-Style">
<p>0.932933</p>
</td>
<td class="No-Table-Style">
<p>-1.34915</p>
</td>
<td class="No-Table-Style">
<p>2.307879057</p>
</td>
<td class="No-Table-Style">
<p>1.695218</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>198</p>
</td>
<td class="No-Table-Style">
<p>0.914009</p>
</td>
<td class="No-Table-Style">
<p>0.932933</p>
</td>
<td class="No-Table-Style">
<p>-0.91451</p>
</td>
<td class="No-Table-Style">
<p>2.74252465</p>
</td>
<td class="No-Table-Style">
<p>2.041826</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>199</p>
</td>
<td class="No-Table-Style">
<p>0.80913</p>
</td>
<td class="No-Table-Style">
<p>0.932933</p>
</td>
<td class="No-Table-Style">
<p>-1.01939</p>
</td>
<td class="No-Table-Style">
<p>2.637645206</p>
</td>
<td class="No-Table-Style">
<p>0.578695</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – AR(4) model outputs versus actuals</p>
<p>Let’s print out <a id="_idIndexMarker865"/>our model’s <strong class="bold">Average Squared </strong><strong class="bold">Error</strong> (<strong class="bold">ASE</strong>):</p>
<pre class="source-code">
print('Average Squared Error: ', np.mean((df_pred['mean'] - y[195:])**2))</pre>
<p>Here we see the ASE:</p>
<p><code>Average Squared Error:  </code><code>0.6352208223437921</code></p>
<p>Our test <a id="_idIndexMarker866"/>forecast plot is shown in <em class="italic">Figure 11</em><em class="italic">.15</em>. Note that our estimate appears conservative. Using ARMA(4,1) may have produced a closer, but less generalizable fit. One method for improving forecasting would be to build the model using only recent points (relative to subject matter knowledge of the process). Including a larger set of data will produce a fit that generalizes more to the overall process rather than to a possibly more relevant timeframe:</p>
<div><div><img alt="Figure 11.15 – ARMA(2,1) test forecast" height="349" src="img/B18945_11_015.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – ARMA(2,1) test forecast</p>
<h4>Step 5 – Building a forecast</h4>
<p>Now, let <a id="_idIndexMarker867"/>us forecast five ahead:</p>
<pre class="source-code">
df_forecast = arma_aic.get_prediction(start=200, end=204).summary_frame(alpha=0.05)
df_forecast.index=[200, 201, 202, 203, 204]
forecast = np.hstack([np.repeat(np.nan, len(y)), df_pred['mean']])</pre>
<div><div><img alt="Figure 11.16 – ARMA(2,1) forecast horizon = 5" height="349" src="img/B18945_11_016.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – ARMA(2,1) forecast horizon = 5</p>
<p>On a final note, regarding ARMA models, <em class="italic">we always assume process stationarity</em>. If stationarity cannot be assumed, neither autoregressive nor moving average models can be used. In the next section of this chapter, we will discuss integrating into ARMA models first-order differencing as a method for conditionally stationarizing a process to overcome limitations of non-stationarity.</p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor182"/>Models for non-stationary time series</h1>
<p>In the previous section, we <a id="_idIndexMarker868"/>discussed ARMA models for stationary time series data. In this section, we will look at non-stationary time series data and extend our model to work with non-stationary data. Let us start by taking a look at some sample data (shown in <em class="italic">Figure 11</em><em class="italic">.17</em>). There are two series: US GDP (left) and airline passenger volume (right).</p>
<div><div><img alt="Figure 11.17 – US GDP (left) and airline passenger (right) time series" height="845" src="img/B18945_11_017.jpg" width="905"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – US GDP (left) and airline passenger (right) time series</p>
<p>The US GDP series appears to exhibit an upward trend with some variations in the series. The airline passenger volume series also exhibits an upward trend, but there also appears to be a repeated pattern in the series. The repeated pattern in the airline series is called <strong class="bold">seasonality</strong>. Both <a id="_idIndexMarker869"/>series are non-stationary because of the apparent trend. Additionally, the airline passenger volume series appears to exhibit non-constant variance. We will <a id="_idIndexMarker870"/>model the GDP series with ARIMA, and we will model the seasonal ARIMA. Let’s take a look at these models.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor183"/>ARIMA models</h2>
<p><strong class="bold">ARIMA</strong> is <a id="_idIndexMarker871"/>an acronym for <strong class="bold">AutoRegressive Integrated Moving Average</strong>. This model is a generalization of the ARMA model that can be applied to non-stationary time series data. The new part added to this <a id="_idIndexMarker872"/>model is “integrated,” which is<a id="_idIndexMarker873"/> a <strong class="bold">differencing</strong> operation applied to the time series to <strong class="bold">stationarize</strong> (to make stationary) the time series. After the time series is stationarized, we can fit an ARMA model to the differenced data. Let’s take a look at the mathematics of this model. We will start with understanding how differencing works, and then put the whole model ARIMA model together.</p>
<h3>Differencing</h3>
<p>Differencing data <a id="_idIndexMarker874"/>is computing the difference between consecutive data points. The resulting data from differencing represents the <em class="italic">change</em> between each data point. We can write the difference as such:</p>
<p>y ′ t = y t − y t−1</p>
<p>This equation is the first-order difference, meaning it is the first difference between the data points. It may be necessary to make additional differences between the data points to stationarize the series. The second difference represents the <em class="italic">change of changes</em> between the data points. The second-order difference can be written thusly:</p>
<p>y ″ t = y ′  t − y ′  t−1 = (y t − y t−1) − (y t−1 − y t−2)</p>
<p>The “order” is simply the number of times a difference operation is applied.</p>
<h3>The ARIMA model</h3>
<p>As mentioned earlier, the ARIMA model is ARMA with the addition of differencing to make the time series stationary (stationarize the time series). Then we can express an ARIMA model mathematically as follows, where y′ t is the differenced series, differenced d times until it is stationary:</p>
<p>y′ t = c + ϕ 1 y′ t−1 + … + ϕ p y ′  t−p + ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q</p>
<p>The ARIMA model has three orders, which are denoted ARIMA(p,d,q):</p>
<ul>
<li>p – the autoregressive order</li>
<li>d – the differencing order</li>
<li>q – the moving average order</li>
</ul>
<p>With more<a id="_idIndexMarker875"/> complicated models such as ARIMA, we will tend to describe them with backshift notation since it is easier to express these models with backshift notation. An ARIMA model will take the following form using backshift notation:</p>
<p>(1 − ϕ 1 B − … − ϕ p B p) (1 − B) d y t = c + (1 + θ 1 B + … + θ q B q) 𝝐 t</p>
<p>                            ↑          ↑          ↑</p>
<p>     AR(p) d differences                MA(q)</p>
<p>Notice the term for the differences in the equation: (1 − B) d. In the previous section, we discussed roots as related to stationary models. In that context, the roots were always outside of the unit circle. With an ARIMA model, we add unit roots to the model. To understand the impact of a unit root, let’s simulate an AR(1) model and see what happens as the root of the model is moved toward one. These simulations are shown in <em class="italic">Figure 11</em><em class="italic">.18</em>.</p>
<div><div><img alt="Figure 11.18 – AR(1) simulations with root approaching 1" height="1291" src="img/B18945_11_018.jpg" width="1075"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – AR(1) simulations with root approaching 1</p>
<p>We can make two observations from the simulations shown in <em class="italic">Figure 11</em><em class="italic">.18</em>. The first observation is that the time series appear to exhibit more wandering behavior as the root increases <a id="_idIndexMarker876"/>toward one. For instance, the middle-time series shows more wandering from the mean than the top-time series. The bottom time series (with a root of one), does not appear to regress toward a mean such as the other two simulations. The second observation is about the autocorrelations. As the root of the AR(1) approaches 1, the autocorrelations get stronger and decrease slower over the lags. These two observations are characteristic of a series with a root near or at one. Additionally, the presence of unit roots will dominate the time series behavior, making it easy to recognize from the autocorrelation plot.</p>
<h3>Fitting an ARIMA model</h3>
<p>There are two <a id="_idIndexMarker877"/>steps to fit an ARIMA model: (1) stationarize the series from differencing to determine the difference order and (2) fit an ARMA model to the resulting series. In the previous section, we discussed how to fit an ARMA model so in this section, we will focus on the first step.</p>
<p>At the beginning of this section, we showed a time series of US GDP values. We will use that time series as a case study for fitting an ARIMA model. First, let’s take a look at the series and its autocorrelations again. The series and autocorrelations are shown in <em class="italic">Figure 11</em><em class="italic">.19</em>.</p>
<div><div><img alt="Figure 11.19 – US GDP time series and autocorrelations" height="513" src="img/B18945_11_019.jpg" width="1048"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – US GDP time series and autocorrelations</p>
<p>From the plots shown in <em class="italic">Figure 11</em><em class="italic">.19</em>, it appears that the time series of US GPD data is non-stationary time series. The time series exhibits wandering behavior, and the autocorrelations are strong and decrease slowly. As we discussed, this is characteristic behavior of <a id="_idIndexMarker878"/>unit roots. For secondary evidence, we can use the Dickey-Fuller test for unit root. The null hypothesis of the Dickey-Fuller test is that a unit root is present in the time series. The following code shows how to use the Dickey-Fuller test from <code>pmdarima</code>. The test returns a p-value of 0.74 indicating we cannot reject the null hypothesis, meaning that the time series should be differenced:</p>
<pre class="source-code">
Import pmdarima as pm
from sktime import datasets
y_macro_economic = datasets.load_macroeconomic()
adf_test = pm.arima.ADFTest()
adf_test.should_diff(y_macro_economic.realgdp.values)
# (0.7423236714537164, True)</pre>
<p>We can take the first difference of time series using the <code>diff</code> function from <code>numpy</code>:</p>
<pre class="source-code">
first_diff = np.diff(y_macro_economic.realgdp.values, n=1)</pre>
<p>Taking the first difference, we arrive at a new time series as shown in <em class="italic">Figure 11</em><em class="italic">.20</em>:</p>
<div><div><img alt="Figure 11.20 – The first difference of US GDP time series" height="505" src="img/B18945_11_020.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – The first difference of US GDP time series</p>
<p>The first difference of the US GDP time series shown in <em class="italic">Figure 11</em><em class="italic">.20</em> appears to be stationary. In fact, it appears to be consistent with an AR(2) model. We double-check whether we need to take an additional difference using the Dickey-Fuller test on the first differenced data:</p>
<pre class="source-code">
first_diff = np.diff(y_macro_economic.realgdp.values, n=1)
adf_test.should_diff(first_diff)
# (0.01, False)</pre>
<p>The Dickey-Fuller test <a id="_idIndexMarker879"/>returns a p-value of 0.01 for the first differenced data, which means we can reject the null hypothesis and we can stop differencing the data. That means that our ARIMA model for this data will have a difference order of 1 (d = 1).</p>
<p>After finding the difference order, we can fit an ARMA model to the differenced data. Since we have already discussed fitting ARMA models, we will use an automated fitting method provided by <code>pmdarima</code>. <code>pm.auto_arima</code> is a function for automatically fitting an ARIMA model to data, however, in this case, we will use it to fit the ARMA portion from the differenced series. The output of <code>pm.auto_arima</code> for the first difference data is shown in the following code block:</p>
<pre class="source-code">
pm.auto_arima(
    first_diff, error_action='ignore', trace=True,
    suppress_warnings=True, maxiter=5, seasonal=False,
    test='adf'
)
Performing stepwise search to minimize aic
 ARIMA(2,0,2)(0,0,0)[0]             : AIC=2207.388, Time=0.03 sec
 ARIMA(0,0,0)(0,0,0)[0]             : AIC=2338.346, Time=0.01 sec
 ARIMA(1,0,0)(0,0,0)[0]             : AIC=2226.760, Time=0.02 sec
 ARIMA(0,0,1)(0,0,0)[0]             : AIC=2284.220, Time=0.01 sec
 ARIMA(1,0,2)(0,0,0)[0]             : AIC=2206.365, Time=0.02 sec
 ARIMA(0,0,2)(0,0,0)[0]             : AIC=2253.267, Time=0.02 sec
 ARIMA(1,0,1)(0,0,0)[0]             : AIC=2203.917, Time=0.01 sec
 ARIMA(2,0,1)(0,0,0)[0]             : AIC=2208.521, Time=0.02 sec
 ARIMA(2,0,0)(0,0,0)[0]             : AIC=2208.726, Time=0.02 sec
 ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=2193.482, Time=0.04 sec
 ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=2208.669, Time=0.03 sec
 ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=2195.212, Time=0.02 sec
 ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=2191.810, Time=0.03 sec
 ARIMA(2,0,0)(0,0,0)[0] intercept   : AIC=2190.196, Time=0.02 sec
 ARIMA(3,0,0)(0,0,0)[0] intercept   : AIC=2191.589, Time=0.03 sec
 ARIMA(3,0,1)(0,0,0)[0] intercept   : AIC=2193.567, Time=0.03 sec
Best model:  ARIMA(2,0,0)(0,0,0)[0] intercept
Total fit time: 0.349 seconds</pre>
<p>Since the ARMA fit <a id="_idIndexMarker880"/>for the differenced data is ARMA(2,0), the ARIMA orders for the original time series would be ARIMA(2,1,0). Next, we will look at forecasting from an ARIMA model.</p>
<h3>Forecasting with ARIMA</h3>
<p>Once we<a id="_idIndexMarker881"/> have a fit model, we can forecast with that model. As mentioned in previous chapters, when making predictions we should create a train-test split, so we have data to compare with the predictions. The model should only fit the training data to avoid data leakage. We can use the <code>train_test_split</code> function from <code>pmdarima</code> to split the data. Then we proceed with the usual steps: split, train, and predict. The code for this is shown in the following code block:</p>
<pre class="source-code">
from pmdarima.model_selection import train_test_split
train, test =
    train_test_split(y_macro_economic.realgdp.values,
    train_size=0.9
)
arima = pm.auto_arima(
    train, out_of_sample_size=10,
    suppress_warnings=True, error_action='ignore',
    test='adf'
)
preds, conf_int = arima.predict(
    n_periods=test.shape[0], return_conf_int=True
)</pre>
<p>The preceding code fits an ARIMA model with <code>auto_arima</code> and then forecasts the size of the test set using the <code>predict</code> method of the ARIMA object. The forecasts for the series generated by the code are shown in <em class="italic">Figure 11</em><em class="italic">.21</em>:</p>
<div><div><img alt="Figure 11.21 – US GDP ARIMA forecast over test split" height="481" src="img/B18945_11_021.jpg" width="506"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – US GDP ARIMA forecast over test split</p>
<p>The<a id="_idIndexMarker882"/> forecast of the US GDP in <em class="italic">Figure 11</em><em class="italic">.21</em> appears to follow the trend of the data but does not capture the small variations in the series. However, the variation is captured in the prediction interval (labeled as “interval”). This model appears to provide a reasonably good prediction of the test data. Note that the interval increases over time. This is because predictions become more uncertain farther in the future. Generally, shorter forecasts are more likely to be accurate.</p>
<p>In this section, we built on the ARMA model and extended it to non-stationary data using differencing, which formed the ARIMA model. In the next section, we will look at non-stationary time series that include seasonal effects and make a further extension to the ARIMA model.</p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor184"/>Seasonal ARIMA models</h1>
<p>Let’s look at another <a id="_idIndexMarker883"/>characteristic of time series called <strong class="bold">seasonality</strong>. Seasonality<a id="_idIndexMarker884"/> is the presence of a pattern in a time series that repeats at regular intervals. Seasonal time series are common in nature. For example, yearly weather patterns and daily sunshine patterns are seasonal patterns. Back at the start of the non-stationary section, we showed an example of a non-stationary time series with seasonality. This time series is shown again in <em class="italic">Figure 11</em><em class="italic">.22</em> along with its ACF plot.</p>
<div><div><img alt="Figure 11.22 – Airline volume data and ACF plot" height="504" src="img/B18945_11_022.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.22 – Airline volume data and ACF plot</p>
<p>The time series shown in <em class="italic">Figure 11</em><em class="italic">.22</em> is the monthly total of international airline passengers from 1949 to 1960 [<em class="italic">3</em>]. There is a definite repeated pattern in this time series. To model this type of data, we will need to an additional term to the ARIMA model to account for seasonality.</p>
<h3>Seasonal differencing</h3>
<p>We will use a <a id="_idIndexMarker885"/>similar approach for modeling this type of time series as we did with ARIMA. We will start by using differencing to stationarize the data, then fit an ARMA model to the differenced data. With seasonal time series, we will need to use seasonal differencing to remove the seasonal effects, which we can show mathematically:</p>
<p>y ′ t = y t − y t−T</p>
<p>Where T is the period of the season. For example, the time series in <em class="italic">Figure 11</em><em class="italic">.22</em> exhibits monthly seasonality and each data point represents one month; therefore, the T = 12 for the airline volume data. Then, for the airline data, we would use the following difference equation to remove seasonality:</p>
<p>y ′ t = y t − y t−12</p>
<p>We can also identify the seasonality by observing where peaks occur in the ACF plot. The ACF plot in <em class="italic">Figure 11</em><em class="italic">.22</em> shows a peak at 12, indicating a seasonal period of 12, which is consistent with our knowledge of the time series.</p>
<p>We will see how to apply seasonal differences later in this section using <code>pmdarima</code>. Let’s take a look at how seasonality is included in the model.</p>
<h3>Seasonal ARIMA</h3>
<p>As mentioned in <a id="_idIndexMarker886"/>the ARIMA section, we will be differencing the original series, then fitting a stationary model to the differenced data. Then our time series would be described by the following equation where y′ t is the differenced series (including seasonal and sequential differences):</p>
<p>y′ t = c + ϕ 1 y′ t−1 + … + ϕ p y ′  t−p + ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q</p>
<p>We can express the whole model with backshift notation:</p>
<p>(1 − ϕ 1 B − … − ϕ p B p) (1 − B) d (1 − B s)y t = c + (1 + θ 1 B + … + θ q B q) 𝝐 t</p>
<p>               ↑     ↑    ↑                 ↑   </p>
<p>             AR(p)         d diff   seasonal diff                               MA(q)</p>
<p>We have a <a id="_idIndexMarker887"/>new term in the equation that accounts for seasonality: (1 − B s). We are adding a new order parameter to the model: s. This model is typically denoted ARUMA(p,d,q,s).</p>
<p class="callout-heading">SARIMA models</p>
<p class="callout">In this section, we <a id="_idIndexMarker888"/>are only covering seasonal differencing. There are more complex models that allow for moving average seasonality and autoregressive seasonality called SARIMA and denoted SARIMA(p,d,q)(P,D,Q)[m]. These models are beyond the scope of this chapter. However, we would encourage the reader to explore these models further after mastering the topics found in this chapter and the next chapter. The ARIMA model covered in this chapter is a subset of the SARIMA model, which accounts for seasonal differencing, which is the “D” order of SARIMA(p,d,q)(P,D,Q)[m].</p>
<p>Just as the (1 − B) d term we added for ARIMA, the (1 − B s) term adds roots to the unit circle. However, unlike the roots from (1 − B) d, the roots from (1 − B s) are distributed uniformly around the unit circle. These roots can be calculated and plotted programmatically with <code>numpy</code> and <code>matplotlib</code> or automatically with computational intelligence tools such as <a id="_idIndexMarker889"/>Wolfram Alpha (<a href="https://www.wolframalpha.com/">https://www.wolframalpha.com/</a>).</p>
<h3>Fitting an ARIMA model with seasonality</h3>
<p>We will take the <a id="_idIndexMarker890"/>following steps to fit an ARIMA model with seasonality:</p>
<ul>
<li>Remove seasonality with differencing.</li>
<li>Remove additional non-stationarity with differencing.</li>
<li>Fit a stationary model to the resulting series.</li>
</ul>
<p>This is essentially the same process we used to fit an ARIMA model, but there is an additional step to handle the seasonal component. Let us walk through an example with the airline data.</p>
<p>We will start <a id="_idIndexMarker891"/>with using differencing to remove the seasonal component of the time series. Recall that the seasonal period of the airline time series is 12, meaning that we need to perform differencing at lag 12 as shown with this equation:</p>
<p>y ′ t = y t − y t−12</p>
<p>We can perform this difference using the <code>diff</code> function from <code>pmdarima</code>. The following code shows how to perform the 12th lagged difference on the airline data:</p>
<pre class="source-code">
import pmdarima as pm
from sktime import datasets
y_airline = datasets.load_airline()
series = pm.utils.diff( y_airline.values, lag=12)</pre>
<p>After performing the seasonal difference, we get the differenced series shown in <em class="italic">Figure 11</em><em class="italic">.23</em> along with the ACF plot. The seasonal portion of the time series appears to be completely removed. The differenced series does not appear to exhibit any repeating patterns. Additionally, the ACF plot does not show the seasonal peak that was present in the ACF plot of the original data:</p>
<div><div><img alt="Figure 11.23 – Airline data after seasonal difference" height="439" src="img/B18945_11_023.jpg" width="895"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.23 – Airline data after seasonal difference</p>
<p>With the seasonal portion of the time series removed, we need to determine whether we need to take any additional differences to stationarize the new time series. The differenced series in <em class="italic">Figure 11</em><em class="italic">.23</em> appears to exhibit a trend. The original data also exhibited a trend. As<a id="_idIndexMarker892"/> before, we can use the Dickey-Fuller test to get additional evidence on whether we should apply additional differences. Running the Dickey-Fuller test on this series will result in a p-value of 0.099, which suggests that we should take a difference in the series to account for a unit root:</p>
<pre class="source-code">
adf_test = pm.arima.ADFTest()
adf_test.should_diff(series)
# (0.09898694171553156, True)</pre>
<p>Taking the first difference of the series will result in the series shown in <em class="italic">Figure 11</em><em class="italic">.24</em>. After taking these two differences the series appears to be sufficiently stationarized.</p>
<div><div><img alt="Figure 11.24 – Airline data after seasonal and first difference" height="499" src="img/B18945_11_024.jpg" width="1025"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.24 – Airline data after seasonal and first difference</p>
<p>The series in <em class="italic">Figure 11</em><em class="italic">.24</em> shows the stationarized version of the airline data. Based on the ACF plot, we should be able to fit a relatively simple ARMA model to the stationarized series. We will use <code>auto_arima</code> function to make an automatic fit as we did in the ARIMA section:</p>
<pre class="source-code">
pm.auto_arima(
    series, error_action='ignore', trace=False,
    suppress_warnings=True, maxiter=5, seasonal=False,
    test='adf'
)
# ARIMA(1,0,0)(0,0,0)[0]</pre>
<p>Fitting the <a id="_idIndexMarker893"/>differenced data with <code>auto_arima</code> returns an AR(1) model. A simple model as we expected.</p>
<p>Putting this all together our resulting model is an ARUMA(1,1,0,12). As with the previous ARIMA example, we could have fit this model with <code>auto_arima</code>, but we walked through the differencing steps here to help build intuition for what each difference element does to the series. Let’s take a look at the direct fit from <code>auto_arima</code> now:</p>
<pre class="source-code">
pm.auto_arima(
    y_airline.values, error_action='ignore', trace=True,
    suppress_warnings=True, maxiter=5, seasonal=True, m=12,
    test='adf'
)
# Best model:  ARIMA(1,1,0)(0,1,0)[12]</pre>
<p>We see that <code>auto_arima</code> found the same model that we did using manual differencing. Note the model is denoted in SARIMA format (see earlier callout about SARIMA). The (0,1,0)[12] means seasonality of 12 when one difference for the seasonality. Now that we have a fit model, let’s look at forecasting for our seasonal model.</p>
<h3>Forecasting ARIMA with seasonality</h3>
<p>Once <a id="_idIndexMarker894"/>we have a fit model, we can forecast with that model. As mentioned in the section on forecasting with ARIMA, when should we make a train-test split so we have data to compare with the predictions? We will use the same procedure: split the data, train the model, and forecast over the test set size:</p>
<pre class="source-code">
train, test = train_test_split(y_airline.values, train_size=0.9)
sarima = pm.auto_arima(
    train, error_action='ignore', trace=True,
    suppress_warnings=True, maxiter=5, seasonal=True, m=12,
    test='adf'
)
preds, conf_int = sarima.predict(n_periods=test.shape[0], return_conf_int=True)</pre>
<p>The preceding <a id="_idIndexMarker895"/>code fits a full SARIMA model with <code>auto_arima</code> and then forecasts the size of the test set using the <code>predict</code> method. The forecasts for the series generated by the code are shown in <em class="italic">Figure 11</em><em class="italic">.25</em>.</p>
<div><div><img alt="Figure 11.25 – SARIMA forecast of the airline data" height="1050" src="img/B18945_11_025.jpg" width="1077"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.25 – SARIMA forecast of the airline data</p>
<p>The forecast <a id="_idIndexMarker896"/>of the airline data in <em class="italic">Figure 11</em><em class="italic">.25</em> appears to capture the variation of the data very well. This is likely due to the strength of the seasonality component in the time series. Note that the prediction intervals increase over time just as with the ARIMA prediction intervals, but the intervals follow the general pattern of the series. This is an impact of the additional knowledge of seasonality in the series.</p>
<p>In this section, we discussed ARIMA models with seasonality and showed how to remove seasonal components. We also looked at forecasting a model with seasonality. In the next section, we will take a closer look at validating time series models.</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor185"/>More on model evaluation</h1>
<p>In the previous<a id="_idIndexMarker897"/> sections, we discussed other methods to prepare data, test and validate models. In this section, we will discuss how to validate time series models and introduce several methods for validating time series models. We will cover the following methods for model evaluation: <strong class="bold">resampling</strong>, <strong class="bold">shifting</strong>, <strong class="bold">optimized persistence forecasting,</strong> and <strong class="bold">rolling </strong><strong class="bold">window forecasting</strong>.</p>
<p>The real-world dataset considered in this section is Coca Cola stock data collected from Yahoo Finance databases from 01/19/1962 to 12/19/2021 for stock price prediction. This is a time series analysis to forecast the future stock value of a given stock. The reader can download the dataset from the Kaggle platform for this analysis. To motivate the study, we first go to explore the Coco Cola stock dataset:</p>
<pre class="source-code">
data = pd.read_csv("COCO COLA.csv", parse_dates=["Date"], index_col="Date")</pre>
<div><div><img alt="Figure 11.26 – Coco Cola dataset" height="224" src="img/B18945_11_026.jpg" width="547"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.26 – Coco Cola dataset</p>
<p>The Date index is related to 15096 trading days from 01/19/1962 to 12/19/2021. The <code>High</code> and <code>Low</code> columns here refer to the maximum and minimum prices on each trading day. <code>Open</code> and <code>Close</code> refer to the stock prices when the market was open and closed on the same trading day. The total amount of trading stocks in each day refers to the <code>Volume</code> column and the last column (<code>Adj Close</code>) refers to adjusted values (combining with stock splits, dividends, etc.). To illustrate how resampling, shifting, rolling windows, and expanding windows perform, we narrow down to use only the <code>Open</code> column from the year 2016:</p>
<pre class="source-code">
data= data[data.index&gt;='2016-01-01'][['Open']]</pre>
<p>The data was collected by trading dates. However, we will perform the study monthly.  The <strong class="bold">resampling</strong> technique <a id="_idIndexMarker898"/>is used to aggregate data from days to months. This idea motivates us to introduce this technique.</p>
<h4>Resampling</h4>
<p><code>resample()</code> function to change time frequencies. The following code illustrates the resampling technique in Python:</p>
<pre class="source-code">
fig, ax = plt.subplots(4,1, figsize=(12,8))
ax[0].set_title('Original stock price from 2016')
ax[0].plot(data)
ax[1].plot(data.resample('7D').mean())
ax[1].set_title('7 days - Downsampling stock price from 2016')
ax[2].plot(data.resample('M').mean())
ax[2].set_title('Monthly Downsampling  stock price from 2016')
ax[3].plot(data.resample('Y').mean())
ax[3].set_title('Yearly Downsampling stock price from 2016')
fig.tight_layout(pad=5.0)
plt.show()</pre>
<p>Here is the <a id="_idIndexMarker902"/>output of the previous code:</p>
<div><div><img alt="Figure 11.27 – Resampling for Coco Cola dataset" height="490" src="img/B18945_11_027.jpg" width="778"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.27 – Resampling for Coco Cola dataset</p>
<p>We observe that the plots become smoother when time frequencies decrease. Next, we discuss the shifting method used in time series.</p>
<h4>Shifting</h4>
<p>In<a id="_idIndexMarker903"/> time series analysis, it is not uncommon to <code>shift()</code> function to create new<a id="_idIndexMarker904"/> features:</p>
<pre class="source-code">
data["price_lag_1"] = data["Open"].shift(1)
data.head()</pre>
<div><div><img alt="Figure 11.28 – First five rows of Coco Cola stock data with price shifted once" height="225" src="img/B18945_11_028.jpg" width="282"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.28 – First five rows of Coco Cola stock data with price shifted once</p>
<p>Observe that the first row of the <code>price_lag_1</code> column is filled with a NaN value. We can replace the missing value with the <code>fill_value</code> parameter:</p>
<pre class="source-code">
data["price_lag_1"] = data["Open"].shift(1, fill_value = data['Open'].mean())</pre>
<p>Finally, we discuss the forecasting methods such as <strong class="bold">optimized persistence</strong> and <strong class="bold">rolling window forecasting</strong>. Another resource related to these methods can be found in [<em class="italic">3</em>].</p>
<h4>Optimized persistence forecasting</h4>
<p>We will <a id="_idIndexMarker905"/>convert the Coco Cola stock<a id="_idIndexMarker906"/> price time frequency to monthly frequency from 2016 using resampling and then we apply an optimized persistence forecasting technique to predict the future value using the previous observation. RMSE scores are considered to evaluate persistence models:</p>
<pre class="source-code">
from sklearn.metrics import mean_squared_error
import math
train, test = data.resample('M').mean()['Open'][0:-24], data.resample('M').mean()['Open'][-24:]
persistence = range(1, 25)
RMSE_scores = []
for p in persistence:
    history = [x for x in train]
    pred = []
    for i in range(len(test)):
    # Prediction on test set
        yhat = history[-p]
        pred.append(yhat)
    history.append(test[i])
    # RMSE score performance
    rmse = math.sqrt(mean_squared_error(test, pred))
    RMSE_scores.append(rmse)
    print(f'p={p} RMSE={rmse}')</pre>
<p>The <a id="_idIndexMarker907"/>output<a id="_idIndexMarker908"/> is as follows:</p>
<div><div><img alt="Figure 11.29 – RMSE scores for Optimized persistence forecasting" height="418" src="img/B18945_11_029.jpg" width="258"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.29 – RMSE scores for Optimized persistence forecasting</p>
<p>We observe<a id="_idIndexMarker909"/> that when p=6, the RMSE <a id="_idIndexMarker910"/>score is the smallest:</p>
<div><div><img alt="Figure 11.30 – Optimized Persistence Forecasting, test versus prediction" height="278" src="img/B18945_11_030.jpg" width="387"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.30 – Optimized Persistence Forecasting, test versus prediction</p>
<p>Running the<a id="_idIndexMarker911"/> persistence test again <a id="_idIndexMarker912"/>with p=6, we can see the following:</p>
<pre class="source-code">
history = [x for x in train]
pred = []
for i in range(len(test)):
    # Prediction
    yhat = history[-6]
    pred.append(yhat)
    history.append(test[i])
# Plots
plt.plot(list(test))
plt.plot(pred)
plt.show()</pre>
<p>Then, we can produce a visualization:</p>
<div><div><img alt="Figure 11.31 – Optimized Persistence Forecasting" height="327" src="img/B18945_11_031.jpg" width="503"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.31 – Optimized Persistence Forecasting</p>
<p>The blue<a id="_idIndexMarker913"/> curve<a id="_idIndexMarker914"/> is the test value, and the orange curve is for the prediction.</p>
<h4>Rolling window forecast</h4>
<p>This <a id="_idIndexMarker915"/>technique creates a <strong class="bold">rolling window</strong> with a<a id="_idIndexMarker916"/> specified window size and then performs a statistic calculation in this window, using it for forecasting which rolls through the data used in a study. We conduct a similar study as in the last part on the Coco Cola stock price dataset from 2016 using a monthly resampling dataset:</p>
<pre class="source-code">
from numpy import mean
train, test = data.resample('M').mean()['Open'][0:-24], data.resample('M').mean()['Open'][-24:]
window = range(1, 25)
RMSE_scores = []
for w in window:
    history = [x for x in train]
    pred = []
    for i in range(len(test)):
    # Prediction on test set
        yhat = mean(history[-w:])
        pred.append(yhat)
    history.append(test[i])
    # RMSE score performance
    rmse = math.sqrt(mean_squared_error(test, pred))
    RMSE_scores.append(rmse)
    print(f'w={w} RMSE={rmse}')
plt.plot(window, RMSE_scores)
plt.title('Rolling Forecasting')
plt.xlabel('Windows sizes')
plt.ylabel('RMSE scores')
plt.show()</pre>
<div><div><img alt="Figure 11.32 – Rolling window forecasting" height="275" src="img/B18945_11_032.jpg" width="439"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.32 – Rolling window forecasting</p>
<p>With<a id="_idIndexMarker917"/> window size = 9, the RMSE of 3.808 is the smallest. Run <a id="_idIndexMarker918"/>the Python code again with window size = 9 we produce a similar visualization as with the Optimized Persistence Forecast.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor186"/>Summary</h1>
<p>In this chapter, we discussed various methods for modeling univariate time series data from stationary time series models such as ARMA to non-stationary models such as ARIMA. We started with stationary models and discussed how to identify modeling approaches based on the characteristics of time series. Then we built on the stationary models by adding a term in the model to stationarize time series. Finally, we talked about seasonality and how to account for seasonality in an ARIMA model. While these methods are powerful for forecasting, they do not incorporate potential information from other external variables. As in the previous chapter, we will see that external variables can help improve forecasts. In the next chapter, we will look at multivariate methods for time series data to take advantage of other explanatory variables.</p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor187"/>References</h1>
<p>Please refer to the final word file for how the references should look.</p>
<ol>
<li><em class="italic">APPLIED TIME SERIES ANALYSIS WITH R</em>, W. Woodward, H. Gray, A. Elliott. Taylor &amp; Francis Group, LLC. 2017.</li>
<li>Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976) <em class="italic">Time Series Analysis, Forecasting and Control</em>. Third Edition. Holden-Day. Series G.</li>
<li>Brownlee, J, (2017) <em class="italic">Simple Time Series Forecasting Models to Test So That You Don’t Fool </em><em class="italic">Yourself</em> (<a href="https://machinelearningmastery.com/simple-time-series-forecasting-models/">https://machinelearningmastery.com/simple-time-series-forecasting-models/</a>).</li>
</ol>
</div>
</div></body></html>