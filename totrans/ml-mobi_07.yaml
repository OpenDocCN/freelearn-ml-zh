- en: Spam Message Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will provide you with an overview of **natural language processing**
    (**NLP**) and discuss how NLP can be combined with machine learning to provide
    solutions to problems. Then, the chapter will take a real-world use case of doing
    spam message detection by utilizing NLP, combined with the linear SVM classification
    model. The program will be implemented as a mobile application using Core ML for
    iOS.
  prefs: []
  type: TYPE_NORMAL
- en: To handle text in machine learning algorithms, we will go through the various
    NLP techniques that will be used on the text data to make it ready for learning
    algorithms. Once the text is prepared, we will see how we can classify it using
    the linear SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem definition**: The bulk SMS message data is provided, and these messages
    need to be classified as spam or non-spam messages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the linear SVM algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solving the problem using linear SVM in Core ML:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create the model file using scikit-learn
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing the scikit-learn model into the Core ML project
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing an iOS mobile application, using the scikit-learn model in it, and doing
    spam message detection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is a huge topic, and it is beyond the scope of this book to go into detail
    on the subject. However, in this section, we will go through the high-level details
    of NLP and try to understand the key concepts required to prepare and process
    the textual data using NLP, in order to make it ready for consumption by machine
    learning algorithms for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Huge, unstructured textual data is getting generated on a daily basis. Social
    media, websites such as Twitter and Facebook, and communication apps, such as
    WhatsApp, generate an enormous volume of this unstructured data daily—not to mention
    the volume created by blogs, news articles, product reviews, service reviews,
    advertisements, emails, and SMS. So, to summarize, there is **huge data **(in
    TBS).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is not possible for a computer to get any insight from this data
    and to carry out specific actions based on the insights, directly from this huge
    data, because of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is unstructured
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data cannot be understood directly without preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This data cannot be directly fed in an unprocessed form into any ML algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make this data more meaningful and to derive information from it, we use
    NLP. The field of study that focuses on the interactions between human language
    and computers is called **NLP**. NLP is a branch of data science that is closely
    related to computational linguistics. It deals with the science of the computer –
    analyzing, understanding, and deriving information from human natural language-based
    data, which is usually unstructured like text, speech, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Through NLP, computers can analyze and derive meaning from human language and
    do many useful things. By utilizing NLP, many complex tasks, such as an automatic
    summary of huge documents, translations, relationship extraction between a different
    mass of unstructured data, sentiment analysis, and speech recognition, can be
    accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: 'For computers to understand and analyze human language, we need to analyze
    the sentence in a more structured manner and understand the core of it. In any
    sentence, we need to understand three core things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic information**: This relates to the meaning of the sentence. This
    is the specific meaning of the words in the sentence, for example, *The k**ite
    flies*. Here, we don''t know whether the kite is man-made or a bird.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntactic information**: This relates to the structure of the sentence. This
    is the specific syntactic meaning of the words in a sentence. *Sreeja saw Geetha
    with candy*. Here, we are not sure who has the candy: Sreeja or Geetha?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pragmatic information (context)**: This relates to the context (linguistic
    or non-linguistic) of the sentence. This is the specific context in which the
    words in the sentence are used. For example, *He is out* in the context of baseball
    and healthcare is different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, computers cannot analyze and recognize sentences as humans do. Therefore,
    there is a well-defined way to enable computers to perform text processing. Here
    are the main steps involved in that exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: This step deals with removing all the noise from the sentence,
    so the only information critical in the context of the sentence is retained for
    the next step. For example, language stop words ("noise"), such as *is*, *the*,
    or *an*, can be removed from the sentence for further processing. When processing
    the sentence, the human brain doesn''t take into consideration the noise that''s
    present in the language. Similarly, the computer can be fed with noiseless text
    for further processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature engineering**: For the computer to process the preprocessed text,
    it needs to know the key features of the sentence. This is what is accomplished
    through the feature engineering step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**NLP processing**: With the human language converted into a feature matrix,
    the computer can perform NLP processing, which could either be classification,
    sentiment analysis, or text matching.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's try to understand the high-level activities that would be performed
    in each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Text-preprocessing techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can process text, it needs to be preprocessed. Preprocessing would
    deal with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing noise from the text under consideration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be additional steps, such as a grammar check or spellcheck, based
    on the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Removing noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any text present in the sentence that may not be relevant to the context of
    the data can be termed noise.
  prefs: []
  type: TYPE_NORMAL
- en: For example, this can include language stop words (commonly used words in a
    language – *is*, *am*, *the*, *of*, and *in*), URLs or links, social media entities
    (mentions, hashtags), and punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: To remove the noise from the sentence, the general approach is to maintain a
    dictionary of noise words and then iterate through the tokens of the sentence
    under consideration against this dictionary and remove matching stop words. The
    dictionary of noise words is updated frequently to cover all possible noise.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The disparities of words in sentences are converted into a normalized form.
    The words in a sentence may vary, such as *sing*, *singer*, *sang*, or *singing*,
    but they all would more or less fit into the same context and could be standardized.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to normalize sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming: **A basic rule-based process of stripping the suffixes (*-ing*,
    *-ly*, *-es*, *-s*) from a word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lemmatization: **The more sophisticated procedure to identify the root form
    of a word. It involves a more complex process of verifying the semantics and syntax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step involves standardizing the sentence to make sure it contains tokens
    that are from the standard language dictionary only and not anything else, such
    as hashtags, colloquial words, and so on. All these are removed in this step.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the text has been processed, the next step to arrange the features
    from the text so that they can be fed into any machine learning algorithm to carry
    out classification, clustering, and so on. There are various methods to convert
    the text into a feature matrix, and we will go through some of them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Entity extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, the key entities from the sentence that would be used for NLP processing
    are extracted. **Named entity recognition** (**NER**) is one such method, where
    the entities could be named entities, such as that of a place, person, or monument.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is another method, where the topics are identified from the corpus of text.
    The topics can be single words, patterns of words, or sequences of co-occurring
    words. Based on a number of words in the topic, these could be called **N-Gram.** So,
    based on context and repeatability, bigrams and trigrams could be used as features.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A bag-of-words model is a representation of text that describes the occurrence
    of words within a document. It involves the representation of known words and
    a measure of the presence of known words in the document. The model is more centered
    around the occurrence of known words in the document, and not about the order
    of words or the structure of words in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data can also be represented as numerical values using various techniques.
    **Term Frequency-Inverse Document Frequency** (**TF-IDF**) for a huge corpus of
    text documents is an important technique in this class.
  prefs: []
  type: TYPE_NORMAL
- en: TF–IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TF-IDF is a weighted model  that's used to convert the text documents into vector
    models on the basis of the occurrence of words in the documents without considering
    the exact ordering of text in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a set of N text documents and any one document to be D. Then,
    we define the following.
  prefs: []
  type: TYPE_NORMAL
- en: TF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This measures how frequently a term occurs in a document. Since every document
    is a different length, it is possible that a term would appear more in long documents
    than shorter ones. Thus, the TF is often divided by the document length to normalize
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(t) = (Number of times term t appears in a document(D))/(Total number of
    terms in the document(N))*.'
  prefs: []
  type: TYPE_NORMAL
- en: Inverse Document Frequency (IDF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This measures how important a term is for the corpus. While computing TF, all
    terms are considered equally important. However, it is common thinking that stop
    words occur more often, but they are less important as far as NLP is concerned.
    Thus, there is a need to bring down the importance of common terms and bring up
    the importance of rare terms, hence the IDF, which is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF(t) = log_e(Total number of documents/Number of documents with term t in
    it)*'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TF IDF formula gives the relative importance of a term in a corpus (list
    of documents), given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f33724e-a938-4bcc-a947-1a91644caded.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf*[*i,j*] = number of occurence of *i* in *j*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*df[i]* = number of documents containing *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* = total number of document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a document that contains 1,000 words, wherein the word rat appears
    3 times. The **term frequency** (**TF**) for rat is then (3/1000=) 0.003\. Now,
    in 10,000 documents, the word cat appears in 1,000 of them. Therefore, the **inverse
    document frequency** (**IDF**) is calculated as log(10000/1000) = 1\. Thus, the
    TF-IDF weight is the product of these quantities is 0.003 * 1 = 0.12.
  prefs: []
  type: TYPE_NORMAL
- en: The words or features in the text corpus could also be organized as feature
    vectors for easy feeding into the next step of NLP processing.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying/clustering the text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last step is to actually carry out classification or clustering using the
    feature engineered matrix or word vectors. We could use any classification algorithm
    and feed the feature vector to carry out classification or clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to carrying out the clustering, different similarity measures could
    be used, such as Cosine Distance or Levenshtein distance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding linear SVM algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml), *Supervised and
    Unsupervised Learning Algorithms*, we covered the SVM algorithm and now have an
    idea of how the SVM model works. A linear support vector machine or linear SVM is
    a linear classifier that tries to find a hyperplane with the largest margin that
    splits the input space into two regions.
  prefs: []
  type: TYPE_NORMAL
- en: A hyperplane is a generalization of a plane. In one dimension, a hyperplane
    is called a **point**. In two dimensions, it is a line. In three dimensions, it
    is a plane. In more dimensions, you can call it a hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw, the goal of SVM is to identify the hyperplane that tries to find
    the largest margin that splits the input space into two regions. If the input
    space is linearly separable, it is easy to separate them. However, in real life,
    we find that the input space is very non-linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53a98c4a-8f3d-4517-9e7e-c004a1a91794.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding scenario, the SVM can help us separate the red and blue balls
    by using what is called a **Kernel Trick**, which is the method of using a linear
    classifier to solve a non-linear problem.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function is applied to each data instance to map the original non-linear
    observations into a higher-dimensional space in which they become separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular kernel functions available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The linear kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The polynomial kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RBF (Gaussian) kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The string kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linear kernel is often recommended for text classification, as most text
    classification problems need to be categorized into two classes. In our example,
    we also want to classify the SMS messages into spam and non-spam.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the problem using linear SVM in Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to look at how we can solve the spam message detection
    problem using all the concepts we have gone through in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to take a bunch of SMS messages and attempt to classify them as
    spam or non-spam. This is a classification problem and we will use the linear
    SVM algorithm to perform this, considering the advantages of using this algorithm
    for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use NLP techniques to convert the data-SMS messages into a feature
    vector to feed into the linear SVM algorithm. We are going to use the scikit-learn
    vectorizer methods to transform the SMS messages into the TF-IDF vector, which
    could be fed into the linear SVM model to perform SMS spam detection (classification
    into spam and non-spam).
  prefs: []
  type: TYPE_NORMAL
- en: About the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data that we are using to create the model that detects the spam messages
    is taken from [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/), which
    contains 747 spam message samples, along with 4,827 non-spam messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'These messages are taken from different sources and labeled with the category
    of spam and non-spam. If you open the downloaded file in Notepad or any text editor,
    it will be in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding sample, we can see that every line starts with the category
    name and is followed by the actual message.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create a model to classify a message as spam or non-spam, we need a library
    that is capable of doing so. Here, we've selected scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'To write this application, you need to have the Python3+ version installed
    on your desktop, and Xcode 9+ must be installed on your Mac machine. If you don''t
    have either of these, please check the appendix of this book to learn how to get
    them. Once you have installed Python in your machine, execute the following commands
    to get the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding code, we installed scikit-learn to get access to the algorithms
    and NumPy as the scikit-learn requires it, and pandas (*pandas* is an open source,
    BSD-licensed library providing high-performance, easy-to-use data structures and
    data analysis tools for the Python programming) to read the model from the file
    and core-ML tools to generate a Core ML model file.
  prefs: []
  type: TYPE_NORMAL
- en: Now, download `SMSSpamCollection.txt`, a plain text file from the model link
    stated in the preceding section, onto your disk and put it in your `project` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Model file using Scikit Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In your project folder, create a python file with the following code to create
    a model file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the fitted model, we can append the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Upon executing the preceding program, it will show you whether the given message
    is spam or non-spam.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the scikit-learn model into the Core ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we created our model to classify the messages as spam and
    non-spam. Now, let's convert that into the Core ML model so that we can use that
    in an IOS app.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a core-ML model, append the following lines to the preceding code
    and run them. This will create a `.mlmodel` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can take the generated `SpamMessageClassifier.mlmodel` file and use
    this in your Xcode.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the iOS application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can get the code for the iOS project in our GitHub repository ([https://github.com/PacktPublishing/Machine-Learning-for-Mobile](https://github.com/PacktPublishing/Machine-Learning-for-Mobile)).
    Once you download the project and open the project in Xcode, you will find the
    directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/130e69e7-19e1-4d52-baf7-dd837444d45e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this, I want to explain the important files to you. Main. Storyboard is
    having the UI design for the app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7fdf840-e9a4-4f6d-84c3-ac32757e3fc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have two labels, one button, and one text box. The two labels are
    a heading label and on result label. Button to submit the input and get the result.
    And we have a textbox to give a message as input. Here, the main processing is
    written in the `controller.swift` view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the app in the simulator of Xcode, it will generate the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d8d52c5-36b8-456b-8ee5-9dab19a0155f.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through many things, such as, understanding NLP at
    a high level. There are various steps involved in NLP, such as text preprocessing,
    as well as techniques to carry this out, such as feature engineering and methods
    to perform feature engineering and classification or clustering of the feature
    vectors. We also looked into the linear SVM algorithm in which we went through
    the details of the SVM algorithm, the kernel function, and how it is more applicable
    to text classification.
  prefs: []
  type: TYPE_NORMAL
- en: We solved our problem using linear SVM in Core ML and we also saw a practical
    example of performing spam message detection using the linear SVM algorithm model
    that we developed in scikit learn and converted into a Core ML model. We wrote
    an iOS application using the converted Core ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be introduced to another ML framework, Fritz, which
    tries to solve the common problems that we see in model deployment and upgrades,
    and the unification of handling ML models across mobile OS platforms.
  prefs: []
  type: TYPE_NORMAL
