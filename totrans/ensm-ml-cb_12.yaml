- en: Homogenous Ensemble for Multiclass Classification Using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models to classify fashion products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many studies have been done in classification problems to find out how to obtain
    better classification accuracy. This problem tends to be more complex when there's
    a large number of classes on which to make a prediction. In the case of multiclass
    classification, it's assumed that each class in the target variable are independent
    of each other. A multiclass classification technique involves training one or
    more models to classify a target variable that can take more than two classes.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of homogeneous models to classify fashion products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we''ll use the Fashion-MNIST dataset. This dataset has 60,000
    images of fashion products from ten categories. The target variable can be classified
    into ten classes:'
  prefs: []
  type: TYPE_NORMAL
- en: T-shirt/top
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trouser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pullover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sandal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shirt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sneakers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ankle boot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each image is a 28 x 28 grayscale image. We will proceed by reading the data
    to build a few homogeneous models over a few iterations to see whether the ensemble
    can deliver a higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use Google Colab to train our models. Google Colab comes with TensorFlow
    installed, so we don't have to install it separately in our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required libraries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We load our data from the datasets that come with `tf.keras`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the dimensions of the train and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7c78596-8409-45b0-9b07-88787f16e63b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We take note of the unique values in the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that there are 10 classes labelled from 0 to 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adb643ea-43b7-4070-967a-1a60c4a811c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can take a quick glimpse at the first few observations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we plot the first 15 images, along with the associated
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a825ef7-c292-420e-8539-a852303b3206.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll now move on to training our models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we''ll create multiple homogeneous models over
    a few iterations using `tf.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We mention seven iterations and 10 epochs in each iteration. In the following
    screenshot, we can see the progress as the model gets trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/facfdce9-3e8f-43c7-91ba-7b757885f4c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the code in *Step 1*, we collate the accuracy, precision, and recall for
    every iteration on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see how the preceding three metrics change
    in each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66b6e227-2ba8-4c36-bc2b-8232068e5a6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll form a DataFrame with the predictions that are returned by all of the
    models in each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the type into an integer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform max-voting to identify the most predicted class for each observation.
    We simply use `mode` to find out which class was predicted the most times for
    an observation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the accuracy of the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate the confusion matrix with the required labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We plot the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix plot appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e7a1a0d-f59b-4d09-b8fd-ccb1c8cf2eae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We create a DataFrame with all of the iteration numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We then combine the accuracy, precision, and recall in one single table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see the structure that holds the metrics
    from each of the models and the ensemble model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/033e6f18-7e95-4b79-95c9-c433448cce9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We plot the accuracy returned by each iteration and the accuracy from max-voting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot. We notice that the accuracy returned by the
    max-voting method is the highest compared to individual models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de750f3d-1583-4900-aa77-1e788d9ac40d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also plot the precision and recall for each model and the ensemble:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bb4181e-f699-495d-9bf5-d5aab4558174.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, we notice that the precision and recall improve
    for an ensemble model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Getting ready* section, we imported our required libraries. Note that
    we''ve imported the `TensorFlow` library. We can directly access the datasets
    by importing the `tf.keras.datasets` module. This module comes with various built-in
    datasets, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boston_housing`: Boston housing price regression dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cifar10`: CIFAR10 small images classification dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fashion_mnist`: Fashion-MNIST dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imdb`: IMDB sentiment classification dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mnist`: MNIST handwritten digits dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reuters`: Reuters topic classification dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the `fashion_mnist` dataset from this module. We loaded the pre-shuffled
    train and test data and checked the shape of the train and test subsets.
  prefs: []
  type: TYPE_NORMAL
- en: We noticed, in the G*etting* *ready* section, that the shape of the training
    subset is (60000, 28, 28), which means that we have 60,000 images that are of
    28 X 28 pixel in size.
  prefs: []
  type: TYPE_NORMAL
- en: We checked the distinct levels in the target variable with the `unique()` method.
    We saw that there were 10 classes from 0 to 9.
  prefs: []
  type: TYPE_NORMAL
- en: We also took a quick look at some of the images. We defined the number of columns
    and rows that we required. Running an iteration, we plotted the images with `matplotlib.pyplot.imshow()`
    in grayscale. We also printed the actual class labels against each of the images using `matplotlib.pyplot.title()`.
  prefs: []
  type: TYPE_NORMAL
- en: In the *How to do it...* section, in *Step 1*, we created multiple homogeneous
    models using the `tf.keras` module. In each iteration, we used the `resample()`
    method to create bootstrap samples. We passed `replace=True` to the `resample()`
    method to ensure that we have samples with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we also defined the model architecture. We added layers to the
    model using `tf.keras.layers`. In each layer, we defined the number of units.
  prefs: []
  type: TYPE_NORMAL
- en: '"Model architecture" refers to the overall neural network structure, which
    includes groups of units called layers. These layers are arranged in a chain-like
    structure. Each layer is a function of its previous layer. Determining the model
    architecture is key to neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We ran through a few iterations in our example. We set the number of iterations.
    In each iteration, we compiled the model and fit it to our training data. We made
    predictions on our test data and captured the following metrics in a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've used `Rectified Linear Units (RELU)` as the activation function for the
    hidden layers. ReLU is represented by the `f(x) = max{0, x}`. In neural networks,
    ReLU is recommended as the default activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the last layer of the model architecture, we've used softmax as
    the activation function. The softmax function can be considered a generalization
    of the sigmoid function. While the sigmoid function is used to represent a probability
    distribution of a dichotomous variable, the softmax function is used to represent
    a probability distribution of a target variable with more than two classes. When
    the softmax function is used for multi-class classification, it returns a probability
    value between 0 and 1 for each class. The sum of all probabilities will be equal
    to one.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 2*, we checked the structure of the accuracy DataFrame that we created
    in *Step 1*. We noticed that we had three columns for accuracy, precision, and
    recall and the metrics for each iteration were captured. In *Step 3*, we converted
    the datatypes in the DataFrame into an integer.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we performed max-voting using `stats.mode()` for each observation.
    Since we ran seven iterations, we had seven predictions for each observation.
    `stats.mode()` returned the prediction with the maximum occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we checked the accuracy of the model with the max-voted predictions.
    In *Step 6* and *Step 7*, we generated the confusion matrix to visualize the correct
    predictions. The diagonal elements in the plot were the correct predictions, while
    the off-diagonal elements were the misclassifications. We saw that there was a
    higher number of correct classifications compared to misclassifications.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 8* and *Step 9*, we proceeded to create a structure to hold the performance
    metrics (accuracy, precision, and recall), along with the labels for each iteration
    and the ensemble. We used this structure to plot our charts for the performance
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we plotted the accuracy for each iteration and the max-voted predictions.
    Similarly, in *Step 11*, we plotted the precision and recall for each iteration
    and the max-voted predictions.
  prefs: []
  type: TYPE_NORMAL
- en: From the plots we generated in *Step 10* and *Step 11*, we noticed how the accuracy,
    precision, and recall improved for the max-voted predictions.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.keras` module provides us with TensorFlow-specific functionality, such
    as eager-execution, data pipelines, and estimators. You can take a look at the
    various options the `tf.keras` module provides us.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we used the built-in optimizer classes provided by the `tf.keras.optimizer`
    module. We used the **Adam** **optimizer** in our example, but there are other
    optimizers you can use, such as Adadelta, Adagrad, Adamax, RMSprop, or SGD.
  prefs: []
  type: TYPE_NORMAL
- en: In the present day, the Adam optimizer is one of the best optimizers. It's an
    extension of **Stochastic Gradient Descent** (**SGD**). SGD considers a single
    learning rate for all weight updates and the learning rate remains unchanged during
    the model training process. The Adam algorithm considers adaptive learning rates
    methods to compute individual learning rates for each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.keras.losses` module provides us with various options so that we can choose
    our loss function. We used `sparse_categorical_crossentropy`. Depending on your
    task, you might opt for other options, such as `binary_crossentropy`, `categorical_crossentropy`, `mean_squared_error`,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of multiclass classification, if the target variable is one-hot
    encoded, use `categorical_crossentropy`. If the classes in the target variable
    are represented as integers, use `sparse_categorical_crossentropy`.
  prefs: []
  type: TYPE_NORMAL
- en: You can get more detailed information about the other hyperparameters that can
    be used with `tf.keras` at [https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras).
  prefs: []
  type: TYPE_NORMAL
