- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using H2O AutoML with Other Technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few chapters, we have been exploring how we can use H2O AutoML in
    production. We saw how we can use H2O models as POJOs and MOJOs as portable objects
    that can make predictions. However, in actual production environments, you will
    often be using multiple technologies to meet various technical requirements. The
    collaboration of such technologies plays a big role in the seamless functionality
    of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it is important to know how we can use H2O models in collaboration with
    other commonly used technologies in the ML domain. In this chapter, we shall explore
    and implement H2O with some of these technologies and see how we can build systems
    that can work together to provide a collaborative benefit.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will investigate how we can host an H2O prediction service as a web
    service using the **Spring Boot** application. Then, we will explore how we can
    perform real-time prediction using H2O with **Apache Storm**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using H2O AutoML and Spring Boot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using H2O AutoML and Apache Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a better understanding of how you
    can use models trained using H2O AutoML with different technologies to make predictions
    in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of your preferred web browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Integrated Development Environment** (**IDE**) of your choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the experiments conducted in this chapter have been performed using IntelliJ
    IDE on an Ubuntu Linux system. You are free to follow along using the same setup
    or perform the same experiments using IDEs and operating systems that you are
    comfortable with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s jump right into the first section, where we’ll learn how to host models
    trained using H2O AutoML on a web application created using Spring Boot.
  prefs: []
  type: TYPE_NORMAL
- en: Using H2O AutoML and Spring Boot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s times, most software services that are created are hosted on the
    internet, where they can be made accessible to all internet users. All of this
    is done using web applications hosted on web servers. Even prediction services
    that use ML can be made available to the public by hosting them on web applications.
  prefs: []
  type: TYPE_NORMAL
- en: The **Spring Framework** is one of the most commonly used open source web application
    frameworks to create websites and web applications. It is based on the Java platform
    and, as such, can be run on any system with a JVM. **Spring Boot** is an extension
    of the Spring Framework that provides a preconfigured setup for your web application
    out of the box. This helps you quickly set up your web application without the
    need to implement the underlying pipelining needed to configure and host your
    web service.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s dive into the implementation by understanding the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume you are working for a wine manufacturing company. The officials
    have a requirement where they want to automate the process of calculating the
    quality of wine and its color. The service should be available as a web service
    where the quality assurance executive can provide some information about the wine’s
    attributes, and the service uses these details and an underlying ML model to predict
    the quality of the wine as well as its color.
  prefs: []
  type: TYPE_NORMAL
- en: So, technically, we will need two models to make the full prediction. One will
    be a regression model that predicts the quality of the wine, while the other will
    be a classification model that predicts the color of the wine.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a combination of the Red Wine Quality and White Wine Quality datasets
    and run H2O AutoML on it to train the models. You can find the datasets at https://archive.ics.uci.edu/ml/datasets/Wine+Quality.
    The combined dataset is already present at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot/h2o_spring_boot](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot/h2o_spring_boot).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a sample of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Wine quality and color dataset ](img/B17298_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Wine quality and color dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fixed acidity**: This feature explains the amount of acidity that is non-volatile,
    meaning it does not evaporate over a certain period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**volatile acidity**: This feature explains the amount of acidity that is volatile,
    meaning it will evaporate over a certain period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**citric acid**: This feature explains the amount of citric acid present in
    the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**residual sugar**: This feature explains the amount of residual sugar present
    in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chlorides**: This feature explains the number of chlorides present in the
    wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**free sulfur dioxide**: This feature explains the amount of free sulfur dioxide
    present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**total sulfur dioxide**: This feature explains the amount of total sulfur
    dioxide present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**density**: This feature explains the density of the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pH**: This feature explains the pH value of the wine, with 0 being the most
    acidic and 14 being the most basic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sulphates**: This feature explains the number of sulfates present in the
    wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alcohol**: This feature explains the amount of alcohol present in the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**quality**: This is the response column, which notes the quality of the wine.
    0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**color**: This feature represents the color of the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the problem statement and the dataset that we will be
    working with, let’s design the architecture to show how this web service will
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive deep into the implementation of the service, let’s look at the
    overall architecture of how all of the technologies should work together. The
    following is the architecture diagram of the wine quality and color prediction
    web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Architecture of the wine quality and color prediction web service
    ](img/B17298_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Architecture of the wine quality and color prediction web service
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the various components of this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client**: This is the person – or in this case, the wine quality assurance
    executive – who will be using the application. The client communicates with the
    web application by making a POST request to it, passing the attributes of the
    wine, and getting the quality and color of the wine as a prediction response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spring Boot Application**: This is the web application that runs on a web
    server and is responsible for performing the computation processes. In our scenario,
    this is the application that will be accepting the POST request from the client,
    feeding the data to the model, getting the prediction results, and sending the
    results back to the client as a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tomcat Web server**: The web server is nothing but the software and hardware
    that handles the HTTP communication over the internet. For our scenario, we shall
    be using the Apache Tomcat web server. Apache Tomcat is a free and open source
    HTTP web server written in Java. The web server is responsible for forwarding
    client requests to the web application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h2o-genmodel` library to make predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H2O server**: Models will be trained using the H2O server. As we saw in [*Chapter
    1*](B17298_01.xhtml#_idTextAnchor017), *Understanding H2O AutoML Basics*, we can
    run H2O AutoML on an H2O server. We shall do the same for our scenario by starting
    an H2O server, training the models using H2O AutoML, and then downloading the
    trained models as POJOs so that we can load them into the Spring Boot application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset**: This is the wine quality dataset that we are using to train our
    models. As stated in the previous section, this dataset is a combination of the
    Red Wine Quality and White Wine Quality datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a good understanding of how we are going to create our wine
    quality and color prediction web service, let’s move on to its implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Working on the implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This service has already been built and is available on GitHub. The code base
    can be found at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_spring_boot).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into the code, make sure your system meets the following minimum
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Java version 8 and above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Maven, preferably version 3.8.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python version 3.7 and above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O Python library installed using pip3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git installed on your system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we will clone the GitHub repository, open it in our preferred IDE, and
    go through the files to understand the whole process. The following steps have
    been performed on *Ubuntu 22.04 LTS* and we are using **IntelliJ IDEA** *version
    2022.1.4* as the IDE. Feel free to use any IDE of your choice that supports Maven
    and the Spring Framework for better support.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, clone the GitHub repository and navigate to `Chapter 13/h2o_spring_boot/`.
    Then, you start your IDE and open the project. Once you have opened the project,
    you should get a directory structure similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Directory structure of h2o_wine_predictor ](img/B17298_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Directory structure of h2o_wine_predictor
  prefs: []
  type: TYPE_NORMAL
- en: 'The directory structure consists of the following important files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pom.xml`: A **Project Object Model** (**POM**) is the fundamental unit of
    the Maven build automation tool. It is an XML file that contains all the information
    about all the dependencies needed, as well as the configurations needed to correctly
    build the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`script.py`: This is the Python script that we will use to train our models
    on the wine quality dataset. The script starts an H2O server instance, imports
    the dataset, and then runs AutoML to train the models. We shall look at it in
    more detail later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/java/com.h2o_wine_predictor.demo/api/PredictionController.java`:
    This is the controller file that has the request mapping to direct the POST request
    to execute the mapped function. The function eventually calls the actual business
    logic where predictions are made using the ML models and the response is sent
    back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/java/com.h2o_wine_predictor.demo/service/PredictionService.java`:
    This is the actual file where the business logic of making predictions resides.
    This function imports the POJO models and the h2o-genmodel library and uses them
    to predict the data received from the controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/java/com.h2o_wine_predictor.demo/Demo`: This is the main function
    of the Spring Boot application. If you want to start the Spring Boot application,
    you must execute this main function, which starts the Apache Tomcat server that
    hosts the web application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/resources/winequality-combined.csv`: This is where the actual CSV
    dataset is stored. The Python script that trains the H2O models picks the dataset
    from this path and starts training the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed that we don’t have the model POJO files anywhere in the
    directory. So, let’s build those. Refer to the `script.py` Python file and let’s
    understand what is being done line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `script.py` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script starts by importing the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once importing is done, the script initializes the H2O server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the H2O server is up and running, the script imports the dataset from
    the `src/main/resources` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the column color is categorical, the script sets it to `factor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you will need a training and validation DataFrame to train and validate
    your model during training. Therefore, the script also splits the DataFrame into
    a 70/30 ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the DataFrames are ready, we can begin the training process for training
    the first model, which is the classification model to classify the color of the
    wine. So, the script sets the label and features, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the training data is ready, we can create the H2O AutoML object and
    begin the model training. The following script does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When initializing the `H2OautoML` object, we set the `exclude_algos` parameter
    with the `StackedEnsemble` value. This is done as stacked ensemble models are
    not supported by POJOs, as we learned in [*Chapter 10*](B17298_10.xhtml#_idTextAnchor196),
    *Working with Plain Old Java Objects (POJOs)*.
  prefs: []
  type: TYPE_NORMAL
- en: This starts the AutoML model training process. Some `print` statements will
    help you observe the progress and results of the model training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training process is done, the script will retrieve the leader
    model and download it as a POJO with the correct name – that is, `WineColorPredictor`
    – and place it in the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the script will do the same for the next model – that is, the regression
    model – to predict the quality of the wine. It slightly tweaks the label and sets
    it to `quality`. The rest of the steps are the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the training is finished, the script will extract the leader model, name
    it `WineQualityPredictor`, and download it as a POJO in the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have both model POJOs downloaded, we need to move them to the `src/main/java/com.h2o_wine_predictor.demo/model/`
    directory. But before we do that, we will also need to add the POJOs to the `com.h2o.wine_predictor.demo`
    package so that the `PredictionService.java` file can import the models. So, the
    script does this by creating a new file, adding the package inclusion instruction
    line to the file, appending the rest of the original POJO file, and saving the
    file in the `src/main/java/com.h2o_wine_predictor.demo/model/` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It does the same for the `WineQualityPredictor` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, it deletes the `tmp` directory to clean everything up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, let’s run this script and generate our models. You can do so by executing
    the following command in your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This should generate the respective model POJO files in the `src/main/java/com.h2o_wine_predictor.demo/model/`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s observe the `PredictionService` file in the `src/main/java/com.h2o_wine_predictor.demo/service`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PredictionService` class inside the `PredictionService` file has the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wineColorPredictorModel`: This is an attribute of the `EasyPredictModelWrapper`
    type. It is a class from the h2o-genmodel library that is imported by the `PredictionService`
    file. We use this attribute to load the `WineColorPredictor` model that we just
    generated using `script.py`. We shall use this attribute to make predictions on
    the incoming request later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wineQualityPredictorModel`: Similar to `wineColorPredictorModel`, this is
    the wine quality equivalent attribute that uses the same `EasyPredictModelWrapper`.
    This attribute will be used to load the `WineQualityPredictor` model and use it
    to make predictions on the quality of the wine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we understand the attributes of this file, let’s check out the methods,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`createJsonResponse()`: This function is pretty straightforward in the sense
    that it takes the binomial classification prediction result from the `WineColorPredictor`
    model and the regression prediction result from the `WineQualityPredictor` model
    and combines them into a JSON response that the web application sends back to
    the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictColor()`: This function uses the `wineColorPredictorModel` attribute
    of the `PredictionService` class to make predictions on the data. It outputs the
    prediction result of the color of the wine as a `BinomialModelPrediction` object,
    which is a part of the h2o-genmodel library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictQuality()`: This function uses the `wineQualityPredictorModel` attribute
    of the `PredictionService` class to make predictions on the data. It outputs the
    prediction result of the quality of the wine as a `RegressionModelPrediction`
    object, which is part of the h2o-genmodel library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fillRowDataFromHttpRequest()`: This function is responsible for converting
    the feature values received from the POST request into a `RowData` object that
    will be passed to `wineQualityPredictorModel` and `wineColorPredictorModel` to
    make predictions. `RowData` is an object from the h2o-genmodel library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getPrediction()`: This is called by `PredictionController`, which passes the
    feature values as a map to make predictions on. This function internally calls
    all the previously mentioned functions and orchestrates the entire prediction
    process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It gets the feature values from the POST request as input. It passes these values,
    which are in the form of `Map` objects, to `fillRowDataFromHttpRequest()`, which
    converts them into the `RowData` type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it passes this `RowData` to the `predictColor()` and `predictQuality()`
    functions to get the prediction values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, it passes these results to the `createJsonResponse()` function to
    create an appropriate JSON response with the prediction values and returns the
    JSON to `PredictionController`, where the controller returns it to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have had a chance to go through the important parts of the whole
    project, let’s go ahead and run the application so that we can have the web service
    running locally on our machines. Then, we will run a simple `cURL` command with
    the wine quality feature values and see if we get the predictions as a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the application, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using IntelliJ IDE, then you can directly click on the green play
    button in the top-right corner of the IDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, you can directly run it from your command line by executing
    the following command inside the project directory where the `pom.xml` file is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If everything is working fine, then you should get an output similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Successful Spring Boot application run output ](img/B17298_13_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Successful Spring Boot application run output
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Spring Boot application is running, the only thing remaining is
    to test this out by making a POST request call to the web service running on `localhost:8082`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open another Terminal and execute the following `curl` command to make a prediction
    request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The request should go to the web application, where the application will extract
    the feature values, convert them into the `RowData` object type, pass `RowData`
    to the prediction function, get the prediction results, convert the prediction
    results into an appropriate `JSON`, and get the `JSON` back as a response. This
    should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Prediction result from the Spring Boot web application ](img/B17298_13_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Prediction result from the Spring Boot web application
  prefs: []
  type: TYPE_NORMAL
- en: From the JSON response, you can see that the predicted color of the wine is
    `white` and that its quality is `5.32`.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have just implemented an ML prediction service on a Spring
    Boot web application. You can further expand this service by adding a frontend
    that takes the feature values as input and a button that, upon being clicked,
    creates a POST body of all those values and sends the API request to the backend.
    Feel free to experiment with this project as there is plenty of scope for how
    you can use H2O model POJOs on a web service.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll learn how to make real-time predictions using H2O
    AutoML, along with another interesting technology called Apache Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Using H2O AutoML and Apache Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Storm** is an open source data analysis and computation tool for processing
    large amounts of stream data in real time. In the real world, you will often have
    plenty of systems that continuously generate large amounts of data. You may need
    to make some computations or run some processes on this data to extract useful
    information as it is generated in real time.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Apache Storm?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take the example of a **log system** in a very heavily used web service.
    Assuming that this web service receives millions of requests per second, it is
    going to generate tons of logs. And you already have a system in place that stores
    these logs in your database. Now, this log data will eventually pile up and you
    will have petabytes of log data stored in your database. Querying all this historical
    data to process it in one go is going to be very slow and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: What you can do is process the data as it is generated. This is where Apache
    Storm comes into play. You can configure your Apache Storm application to perform
    the needed processing and direct your log data to flow through it and then store
    it in your database. This will streamline the processing, making it real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Storm can be used for multiple use cases, such as real-time analytics,
    **Extract-Transform-Load** (**ETL**) data in data pipelines, and even ML. What
    makes Apache Storm the go-to solution for real-time processing is because of how
    fast it is. A benchmarking test performed by the Apache Foundation found Apache
    Storm to process around a million tuples per second per node. Apache Storm is
    also very scalable and fault-tolerant, which guarantees that it will process all
    the incoming real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s dive deep into the architecture of Apache Storm to understand how
    it works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of Apache Storm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apache Storm uses cluster computing, similar to how **Hadoop** and even H2O
    work. Consider the following architectural diagram of Apache Storm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Architecture of Apache Storm ](img/B17298_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Architecture of Apache Storm
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Storm distinguishes the nodes in its cluster into two categories – a
    master node and a worker node. The features of these nodes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master Node**: The master node runs a special daemon called **Nimbus**. The
    Nimbus daemon is responsible for distributing the data among all the worker nodes
    in the cluster. It also monitors failures and will resend the data to other nodes
    once a failure is detected, ensuring that no data is left out of processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker Node**: The worker nodes run a daemon called the **Supervisor**. The
    Supervisor daemon is the service that is constantly listening for work and starts
    or stops the underlying processes as necessary for the computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The communication between the master node and the worker nodes using their respective
    daemons is done using the **Zookeeper cluster**. In short, the Zookeeper cluster
    is a centralized service that maintains configuration and synchronization services
    for stateless groups. In this scenario, the master node and the worker nodes are
    stateless and fast-failing services. All the state details are stored in the Zookeeper
    cluster. This is beneficial as keeping the nodes stateless helps with fault tolerance
    as the nodes can be brought back to life and they will start working as if nothing
    had happened.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in understanding the various concepts and technicalities
    of Zookeeper, then feel free to explore it in detail at [https://zookeeper.apache.org/](https://zookeeper.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the implementation part of Apache Storm, we need to be
    aware of certain concepts that are important to understand how Apache Storm works.
    The different concepts are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tuples**: Apache Storm uses a data model called Tuple as its primary unit
    of data that is to be processed. It is a named list of values and can be an object
    of any type. Apache Storm supports all primitive data types out of the box. But
    it can also support custom objects, which can be deserialized into primitive types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: Streams are unbounded sequences of tuples. A stream represents
    the path from where your data flows from one transformation to the next. The basic
    primitives that Apache Storm provides for doing these transformations are spouts
    and bolts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spouts**: A spout is a source for a stream. It is at the start of the stream
    from where it reads the data from the outside world. It takes this data from the
    outside world and sends it to a bolt.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bolt**: A bolt is a process that consumes data from single or multiple streams,
    transforms or processes it, and then outputs the result. You can link multiple
    bolts one after the other while feeding the output of one bolt as input to the
    next to perform complex processing. Bolts can run functions, filter data, perform
    aggregation, and even store data in databases. You can perform any kind of functionality
    you want on a bolt.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topologies**: The entire orchestration of how data will be processed in real
    time using streams, spouts, and bolts in the form of a **Directed Acyclic Graph**
    (**DAG**) is called a **topology**. You need to submit this topology to the Nimbus
    daemon using the main function of Apache Storm. The topology graph contains nodes
    and edges, just like a regular graph structure. Each node contains processing
    logic and each edge shows how data is to be transferred between two nodes. Both
    the Nimbus and the topology are **Apache Thrift** structures, which are special
    type systems that allow programmers to use native types in any programming language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about Apache Thrift by going to [https://thrift.apache.org/docs/types](https://thrift.apache.org/docs/types).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a better understanding of what Apache Storm is and the various
    concepts involved in its implementation, we can move on to the implementation
    part of this section, starting with installing Apache Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Apache Storm is a very powerful and sophisticated system. It has plenty of applicability
    outside of just machine learning and also has plenty of features and support.
    If you want to learn more about Apache Storm, go to [https://storm.apache.org/](https://storm.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Apache Storm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by noting down the basic requirements for installing Apache Storm.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Java version greater than Java 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Maven, preferably version 3.8.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, make sure these basic requirements are already installed on your system.
    Now, let’s start by downloading the Apache Storm repo. You can find the repo at
    [https://github.com/apache/storm](https://github.com/apache/storm).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, execute the following command to clone the repository to your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once the download is finished, you can open the `storm` folder to get a glimpse
    of its contents. You will notice that there are tons of files, so it can be overwhelming
    when you’re trying to figure out where to start. Don’t worry – we’ll work on very
    simple examples that should be enough to give you a basic idea of how Apache Storm
    works. Then, you can branch out from there to get a better understanding of what
    Apache Storm has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, open your Terminal and navigate to the cloned repo. You will need to locally
    build Apache Storm itself before you can go about implementing any of the Apache
    Storm features. You need to do this as locally building Apache Storm generates
    important JAR files that get installed in your `$HOME/.m2/repository` folder.
    This is the folder where Maven will pick up the JAR dependencies when you build
    your Apache Storm application.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, locally build Apache Storm by executing the following command at the root
    of the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The build might take some time, considering that Maven will be building several
    JAR files that are important dependencies to your application. So, while that
    is happening, let’s understand the problem statement that we will be working on.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume you are working for a medical company. The medical officials have
    a requirement, where they want to create a system that predicts whether the person
    is likely to suffer from any complications after surviving a heart failure or
    whether they are safe to be discharged. The catch is that this prediction service
    will be used by all the hospitals in the country, and they need immediate prediction
    results so that the doctors can decide whether to keep the patient admitted for
    a few days to monitor their health or decide to discharge them.
  prefs: []
  type: TYPE_NORMAL
- en: So, the machine learning problem is that there will be streams of data that
    our system will need to make immediate predictions. We can set up a Apache Storm
    application that streams all the data into the prediction service and deploys
    model POJOs trained using H2O AutoML to make the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We can train the models on the Heart Failure Clinical dataset, which can be
    found at [https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records](https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows some sample content from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Heart Failure Clinical dataset ](img/B17298_13_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Heart Failure Clinical dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**age**: This feature indicates the age of the patient in years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates yes and `0` indicates no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates yes and `0` indicates no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**creatinine phosphokinase**: This feature indicates the level of the CPK enzyme
    in the blood in **micrograms per liter** (**mcg/L**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates yes and `0` indicates no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ejection fraction**: This feature indicates the percentage of blood leaving
    the heart at each contraction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**platelets**: This feature indicates the platelets in the blood in kilo platelets
    per **milliliter** (**ml**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates the patient is a woman and `0` indicates the patient is a man'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**serum creatinine**: This feature indicates the level of serum creatinine
    in the blood in **milligrams per deciliter** (**mg/dL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**serum sodium**: This feature indicates the level of serum sodium in the blood
    **milliequivalent per liter** (**mEq/L**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates yes and `0` indicates no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**time**: This feature indicates the number of follow-ups in days'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` indicates yes and `0` indicates no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the problem statement and the dataset that we will be
    working with, let’s design the architecture of how we can use Apache Storm and
    H2O AutoML to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at the overall architecture of how all the technologies should work
    together. Refer to the following architecture diagram of the heart failure complication
    prediction service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Architecture diagram of using H2O AutoML with Apache Storm
    ](img/B17298_13_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Architecture diagram of using H2O AutoML with Apache Storm
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the various components of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '`script.py`: From an architectural point of view, the solution is pretty simple.
    First, we train the models using H2O AutoML, which can be easily done by using
    this script, which imports the dataset, sets the label and features, and runs
    AutoML. The leader model can then be extracted as a POJO, which we can later use
    in Apache Storm to make predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Spout**: We will have a spout in Apache Storm that constantly reads
    data and passes it to the **Prediction Bolt** in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction Bolt**: This bolt contains the prediction service that imports
    the trained model POJO and uses it to make predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification Bolt**: The results from the Prediction Bolt are passed to
    this bolt. This bolt classifies the results as potential complications and no
    complications based on the binary classification result from the Prediction Bolt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have designed a simple and good solution, let’s move on to its implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Working on the implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This service is already available on GitHub. The code base can be found at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_apache_storm/h2o_storm](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%2013/h2o_apache_storm/h2o_storm).
  prefs: []
  type: TYPE_NORMAL
- en: So, download the repo and navigate to `/Chapter 13/h2o_apache_storm/h2o_storm/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see that we have two folders. One is the `storm-starter` directory,
    while the other is the `storm-streaming` directory. Let’s focus on the `storm-streaming`
    directory first. Start your IDE and open the `storm-streaming` project. Once you
    open the project, you should see a directory structure similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – storm_streaming directory structure ](img/B17298_13_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – storm_streaming directory structure
  prefs: []
  type: TYPE_NORMAL
- en: 'This directory structure consists of the following important files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scripty.py`: This is the Python script that we will use to train our models
    on the heart failure complication dataset. The script starts an H2O server instance,
    imports the dataset, and then runs AutoML to train the models. We shall look at
    this in more detail later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`H2ODataSpout.java`: This is the Java file that contains the Apache Storm spout
    and its functionality. It reads the data from the `live_data.csv` file and forwards
    individual observations one at a time to the bolts, simulating the real-time flow
    of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`H2OStormStarter.java`: This is a Java file that contains the Apache Storm
    topology with the two bolts – the Prediction Bolt and Classification Bolt classes.
    We shall start our Apache Storm service using this file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_data.csv`: This is the dataset that contains a part of the heart
    failure complication data that we will be using to train our models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`live_data.csv`: This is the dataset that contains the heart failure complication
    data that we will be using to simulate the real-time inflow of data into our Apache
    Storm application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the previous experiments, where we made changes in a separate application
    repository, for this experiment, we shall make changes in Apache Storm’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: The following steps have been performed on *Ubuntu 22.04 LTS*; *IntelliJ IDEA
    version 2022.1.4* has been used as the IDE. Feel free to use any IDE of your choice
    that supports the Maven framework for better support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by understanding the model training script, `script.py`. The code
    for `script.py` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the script imports the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once importing is done, the H2O server is initialized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the H2O server is up and running, the script imports the `training_data.csv`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the DataFrame has been imported, we can begin the training process
    for training the models using AutoML. So, the script sets the label and features,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create the H2O AutoML object and begin the model training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since POJOs are not supported for stacked ensemble models, we set the `exclude_algos`
    parameter with the `StackedEnsemble` value.
  prefs: []
  type: TYPE_NORMAL
- en: This starts the AutoML model training process. Some `print` statements are in
    here that will help you observe the progress and results of the model training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training process is done, the script retrieves the leader model
    and downloads it as a POJO with the correct name – that is, `HeartFailureComplications`
    – and places it in the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, let’s run this script and generate our model. Executing the following command
    in your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This should generate the respective model POJO files in the `tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s investigate the next file in the repository: `H2ODataSpout.java`.
    The `H2ODataSpout` class in the Java file has a few attributes and functions that
    are important for building the Apache Storm applications. We won’t focus on them
    much, but let’s have a look at the functions that do play a bigger role in the
    business logic of the applications. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nextTuple()`: This function contains the logic of reading the data from the
    `live_data.csv` file and emits the data row by row to the Prediction Bolt. Let’s
    have a quick look at the code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, you have the sleep timer. Apache Storm, as we know, is a super-fast
    real-time data processing system. Observing our live data flowing through the
    system will be difficult for us, so the `sleep` function ensures that there is
    a delay of 1,000 milliseconds so that we can easily observe the flow of data and
    see the results:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function then instantiates the `live_data.csv` file into the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then declares the `observation` variable. This is nothing but the
    individual row data that will be read and stored in this variable by the spout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have the logic where the spout program reads the row in the data.
    Which row to read is decided by the `_cnt` atomic integer, which gets incremented
    as the spout reads and emits the row to the Prediction Bolt in an infinite loop.
    This infinite loop simulates the continuous flow of data, despite `live_data.csv`
    containing only limited data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have the atomic number increment so that the next iteration picks
    up the next row in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have the `_collector.emit()` function, which emits the row data
    so that it’s stored in `_collector`, which, in turn, is consumed by the Prediction
    Bolt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`declareOutputFields()`: In this method, we declare the headers of our data.
    We can extract and use the headers from our trained AutoML model POJO using its
    `NAMES` attribute:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Other miscellaneous functions*: The remaining `open()`, `close()`, `ack()`,
    `fail()`, and `getComponentConfiguration()` functions are supportive functions
    for error handling and preprocessing or postprocessing activities that you might
    want to do in the spout. To keep this experiment simple, we won’t dwell on them
    too much.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moving on, let’s investigate the `H2OStormStarter.java` file. This file contains
    both bolts that are needed for performing the predictions and classification,
    as well as the `h2o_storm()` function, which builds the Apache Storm topology
    and passes it onto the Apache Storm cluster. Let’s dive deep into the individual
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class PredictionBolt`: This is the `Bolt` class and is responsible for obtaining
    the class probabilities of the heart failure complication dataset. It imports
    the H2O model POJO and uses it to calculate the class probabilities of the incoming
    row data. It has three functions – `prepare()`, `execute()` and `declareOutputFields()`.
    We shall only focus on the `execute` function since it contains the execution
    logic of the bolt; the rest are supportive functions. The `execute` function contains
    the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The very first thing this function does is import the H2O model POJO:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, it extracts the input tuple values from its parameter variables and stores
    them in the `raw_data` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code categorically maps all the categorical data in the row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the code gets the prediction and emits the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the code acknowledges the tuple so that the spout is informed about
    its consumption and won’t resend the tuple for retry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`Bolt` class also has some supportive functions, along with the main `execute()`
    function. Let’s dive deep into this to understand what is going on in the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function simply computes if there is a possibility of *Possible Complication*
    or *No Complications* based on the `_threshold` value and emits the result back:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`h2o_storm()`: This is the main function of the application and builds the
    topology using `H2ODataSpout` and the two bolts – Prediction Bolt and Classifier
    Bolt. Let’s have a deeper look into its functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, the function instantiates `TopologyBuilder()`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using this object, it builds the topology by setting the spout and the bolts,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Apache Storm also needs some configuration data to set up its cluster. Since
    we are creating a simple example, we can just use the default configurations,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it creates a cluster and submits the topology it created, along with
    the configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, there are some functions to wrap the whole experiment together.
    The `Util.sleep()` function is used to pause for an hour so that Apache Storm
    can loop over the functionality indefinitely while simulating a continuous flow
    of real-time data. The `cluster.killTopology()` function kills the `HeartComplicationPredictor`
    topology, which stops the simulation in the cluster. Finally, the `cluster.shutdown()`
    function brings down the Apache Storm cluster, freeing up the resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a better understanding of the contents of the files and how
    we are going to be running our service, let’s proceed and look at the contents
    of the `storm-starter` project. The directory structure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – storm-starter directory structure ](img/B17298_13_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – storm-starter directory structure
  prefs: []
  type: TYPE_NORMAL
- en: The `src` directory contains several different types of Apache Storm topology
    samples that you can choose to experiment with. I highly recommend that you do
    so as that will help you get a better understanding of how versatile Apache Storm
    is when it comes to configuring your streaming service for different needs.
  prefs: []
  type: TYPE_NORMAL
- en: However, we shall perform this experiment in the `test` directory to keep our
    files isolated from the ones in the `src` directory. So, let’s see how we can
    run this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to build and run the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `storm-streaming` directory, run the `script.py` file to generate the
    H2O model POJO. The script should run H2O AutoML and generate a leaderboard. The
    leader model will be extracted, renamed `HeartFailureComplications`, and downloaded
    as a POJO. Run the following command in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `HeartFailureComplications` POJO will be imported by the other files in
    the `storm-starter` project, so to ensure that it can be correctly imported by
    files in the same package, we need to add this POJO to that same package. So,
    modify the POJO file to add the `storm.starter` package as the first line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, move the `HeartFailureComplications` POJO file, the `H2ODataSpout.java`
    file, and the `H2OStormStarted.java` file inside the `storm-starter` repository
    inside its `storm-starter/test/jvm/org.apache.storm.starter` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to import the `h2o-model.jar` file into the `storm-starter` project.
    We can do so by adding the following dependency to the `pom.xml` file of the experiment,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your directory should now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – storm-starter directory structure after file transfers ](img/B17298_13_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – storm-starter directory structure after file transfers
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will run this project by right-clicking on the `H2OStormStarter.java`
    file and running it. You should get a stream of constant output that demonstrates
    your spout and bolt in action. This can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Heart complication prediction output in Apache Storm ](img/B17298_13_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Heart complication prediction output in Apache Storm
  prefs: []
  type: TYPE_NORMAL
- en: 'If you observe the results closely, you should see that there are executors
    in the logs; all the Apache Storm spouts and bolts are internal executor processes
    that run on the cluster. You will also see the prediction probabilities besides
    each tuple. This should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Heart complication prediction result ](img/B17298_13_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Heart complication prediction result
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – we have just covered another design pattern that shows us
    how we can use models trained using H2O AutoML to make real-time predictions on
    streaming data using Apache Storm. This concludes the last experiment of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on how we can implement models that have been trained
    using H2O AutoML in different scenarios using different technologies to make predictions
    on different kinds of data.
  prefs: []
  type: TYPE_NORMAL
- en: We started by implementing an AutoML leader model in a scenario where we tried
    to make predictions on data over a web service. We created a simple web service
    that was hosted on localhost using Spring Boot and the Apache Tomcat web server.
    We trained the model on data using AutoML, extracted the leader model as a POJO,
    and loaded that POJO as a class in the web application. By doing this, the application
    was able to use the model to make predictions on the data that it received as
    a POST request, responding with the prediction results.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we looked into another design pattern where we aimed to make predictions
    on real-time data. We had to implement a system that can simulate the real-time
    flow of data. We did this with Apache Storm. First, we dived deep into understanding
    what Apache Storm is, its architecture, and how it works by using spouts and bolts.
    Using this knowledge, we built a real-time data streaming application. We deployed
    our AutoML trained model in a Prediction Bolt where the Apache Storm application
    was able to use the model to make predictions on the real-time streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the final chapter of this book. There are still innumerable features,
    concepts, and design patterns that we can work with while using H2O AutoML. The
    more you experiment with this technology, the better you will get at implementing
    it. Thus, it is highly recommended that you keep experimenting with this technology
    and discover new ways of solving ML problems while automating your ML workflows.
  prefs: []
  type: TYPE_NORMAL
