<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying Computer Vision Applications on Jetson TX1</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span>The previous chapter described the installation of OpenCV and CUDA on a Jetson TX1 development board. This chapter will describe how to use these features on the board. The properties of the Jetson TX1 GPU that make it useful for parallel processing will be described in detail. The chapter will also describe how we can execute the CUDA and C++ codes, seen earlier in this book, on Jetson TX1. It will also demonstrate the performance of the Jetson TX1 GPU in executing CUDA code. The primary motive of this chapter will be to demonstrate the use of Jetson TX1 in deploying image- and video-processing applications. Basic image-processing applications such as image reading, displaying, addition, thresholding, and filtering are taken as examples to demonstrate the use of Jetson TX1 for computer vision applications. Moreover, camera interfacing is important for the deployment of the board in real-life scenarios. This chapter will describe the procedure to use the onboard camera or USB camera for video-capturing and processing applications. How to deploy some advanced applications, like face detection and background subtraction, will be explained in the last part of the chapter.  </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Device properties of a Jetson TX1 board</li>
<li>Running CUDA programs on a Jetson TX1 board</li>
<li>Image processing on a Jetson TX1 board</li>
<li>Interfacing cameras with a Jetson TX1 development board</li>
<li>Advanced applications such as face detection, eye detection, and background subtraction on a Jetson TX1 development board</li>
</ul>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>This chapter requires a good understanding of the OpenCV, CUDA, and any programming language. It also requires any Nvidia GPU development board, like Jetson TK1, TX1, or TX2. The code files used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. </span></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2xDtHhm">http://bit.ly/2xDtHhm</a></p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Device properties of Jetson TX1 GPU</h1>
                </header>
            
            <article>
                
<p><span>CUDA provides a simple interface to determine the</span> <span>capabilities of a GPU device, which is Tegra X1 present on a Jetson TX1 board.  It is important to find out the properties of the device that will help in writing optimal programs for it. The program to find the properties of the device is available in the CUDA sample programs installed with JetPack in the home folder. You can also run the program we developed in the second chapter to find out the device properties.</span></p>
<p><span>The output of the program on an Nvidia Tegra X1 GPU is as follows:</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-641 image-border" src="assets/3691873c-0729-4c74-a7db-d8605b91b509.png" style="" width="917" height="722"/></div>
<p>The JetPack 3.3 installs the CUDA 9.0 runtime version. The global memory for the GPU device is around 4 GB, with a GPU clock speed of around 1 GHz. This clock speed is slower than the GeForce 940 GPU mentioned earlier in this book. The memory clock speed is only 13 MHz compared to 2.505 GHz on GeForce 940, which makes Jetson TX1 slower. The L2 cache is 256 KB compared to 1 MB on GeForce 940. Most of the other properties are similar to GeForce 940.</p>
<p>The maximum number of threads that can be launched per block in <em>X</em>, <em>Y</em>, and <em>Z</em> directions are 1,024, 1,024, and 64 respectively. These numbers should be used while determining the number of parallel threads to be launched from a program. The same care should be taken while launching the number of parallel blocks per grid. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To summarize, we have seen the device properties of the Tegra X1 GPU available on a Jetson TX1 development board. It is an embedded board so memory is available and the clock rate is comparatively slower than for GPU devices like GeForce 940 that comes with a laptop. Still, it is way faster than embedded platforms like Arduino and Raspberry Pi. It can be easily used in deploying computer vision applications that require high computational power. Now that we have seen the device properties, we will start by developing the first program using CUDA on Jetson TX1. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic CUDA program on Jetson TX1</h1>
                </header>
            
            <article>
                
<p>In this section, the example of adding two large arrays is taken to demonstrate the use of a Jetson TX1 development board in executing CUDA programs. The performance of the program is also measured using CUDA events.</p>
<p>The kernel function for adding two large arrays with 50,000 elements is as follows:</p>
<pre><br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>//Defining number of elements in Array<br/>#define N 50000<br/>//Defining Kernel function for vector addition<br/>__global__ void gpuAdd(int *d_a, int *d_b, int *d_c) {<br/> //Getting Thread index of current kernel<br/> int tid = threadIdx.x + blockIdx.x * blockDim.x;<br/> while (tid &lt; N)<br/> {<br/> d_c[tid] = d_a[tid] + d_b[tid];<br/> tid += blockDim.x * gridDim.x;<br/> }<br/>}</pre>
<p>The kernel function takes two device pointers, which point to input arrays as input, and one device pointer, which points to output arrays in the device memory as arguments.  The thread ID of the current kernel execution is calculated and array elements indexed by the thread index are added by the kernel. If the number of kernels launched is less than the number of array elements, then the same kernel will add <kbd>Array</kbd> elements offset by the block dimension as shown in the  <kbd>while</kbd> loop. The <kbd>main</kbd> function for adding two arrays is as follows:</p>
<pre>int main(void) <br/>{<br/> //Defining host arrays<br/> int h_a[N], h_b[N], h_c[N];<br/> //Defining device pointers<br/> int *d_a, *d_b, *d_c;<br/> cudaEvent_t e_start, e_stop;<br/> cudaEventCreate(&amp;e_start);<br/> cudaEventCreate(&amp;e_stop);<br/> cudaEventRecord(e_start, 0);<br/> // allocate the memory<br/> cudaMalloc((void**)&amp;d_a, N * sizeof(int));<br/> cudaMalloc((void**)&amp;d_b, N * sizeof(int));<br/> cudaMalloc((void**)&amp;d_c, N * sizeof(int));<br/> //Initializing Arrays<br/> for (int i = 0; i &lt; N; i++) {<br/> h_a[i] = 2 * i*i;<br/> h_b[i] = i;<br/> }<br/> // Copy input arrays from host to device memory<br/> cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);<br/> cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);<br/> //Calling kernels passing device pointers as parameters<br/> gpuAdd &lt;&lt; &lt;1024, 1024 &gt;&gt; &gt;(d_a, d_b, d_c);<br/> //Copy result back to host memory from device memory<br/> cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);<br/> cudaDeviceSynchronize();<br/> cudaEventRecord(e_stop, 0);<br/> cudaEventSynchronize(e_stop);<br/> float elapsedTime;<br/> cudaEventElapsedTime(&amp;elapsedTime, e_start, e_stop);<br/> printf("Time to add %d numbers: %3.1f ms\n",N, elapsedTime);<br/><br/></pre>
<p>Two host arrays are defined and memory is allocated to them using the  <kbd>cudaMalloc</kbd> function. They are initialized to some random values and uploaded to the device memory. Two CUDA events are created to measure the performance of the CUDA program. The kernel is launched with 1,024 blocks in parallel, with each block having 1,024 threads. These numbers are taken from the device properties, as explained in the last section. The result from the kernel function is transferred to the host memory. The time taken by the kernel function is recorded by the <kbd>e_start</kbd> and <kbd>e_stop</kbd> events, before and after the kernel launch. The time taken by the function is displayed on the console.</p>
<p>The following code is added to verify the correctness of the result, computed by the GPU, and to clean up the memory used by the program:</p>
<pre> int Correct = 1;<br/> printf("Vector addition on GPU \n");<br/> //Printing result on console<br/> for (int i = 0; i &lt; N; i++) {<br/> if ((h_a[i] + h_b[i] != h_c[i]))<br/> {<br/>  Correct = 0;<br/> }<br/><br/> }<br/> if (Correct == 1)<br/> {<br/> printf("GPU has computed Sum Correctly\n");<br/> }<br/> else<br/> {<br/> printf("There is an Error in GPU Computation\n");<br/> }<br/> //Free up memory<br/> cudaFree(d_a);<br/> cudaFree(d_b);<br/> cudaFree(d_c);<br/> return 0;<br/>}</pre>
<p>The same array addition operation is performed on the CPU and compared with the result obtained from the GPU to verify whether the GPU has computed the result correctly or not. This is also displayed on the console. All the memory used by the program is freed up by using the <kbd>cudaFree</kbd> function. </p>
<p>The following two commands need to be run from the Terminal to execute the program. The program should be in the current working directory:</p>
<pre><strong>$ nvcc 01_performance_cuda_events.cu -o gpu_add</strong><br/><strong>$ ./gpu_add</strong></pre>
<p>The <kbd>nvcc</kbd> command is used to compile CUDA code with an Nvidia CUDA compiler. The file name is passed as an argument to the command. The name of the object file, which will be created by the compiler, is specified with the <kbd>-o</kbd> option. This filename will be used to execute the program. This is done by the second command. The output of the program is as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-642 image-border" src="assets/adb67283-4e75-494c-978d-b230fc682b6c.png" style="" width="733" height="109"/></div>
<p>As can be seen from the result, Jetson TX1 takes <kbd>3.4ms</kbd> to compute the sum of two arrays with 50,000 elements, which is slower than GeForce 940 used in the third chapter of this book, but still it is faster than sequential execution on a CPU. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To summarize, this section demonstrated the use of a Jetson TX1 development board in the execution of CUDA programs. The syntax is the same as we have seen earlier in this book. So all CUDA programs developed earlier in the book can be executed on Jetson TX1 without much modification. The procedure to execute the program is also described. The next section will describe the use of Jetson TX1 for image-processing applications.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image processing on Jetson TX1 </h1>
                </header>
            
            <article>
                
<p>This section will demonstrate the use of Jetson TX1 in the deployment of image-processing applications. We will again use OpenCV and CUDA for accelerating computer vision applications on Jetson TX1. In the last chapter, we saw the installation procedure for JetPack 3.3, which contains OpenCV and CUDA. But in the latest JetPack, OpenCV is not compiled with CUDA support nor has it GStreamer support, which is needed for accessing the camera from the code. So, it is a good idea to remove the OpenCV installation that comes with JetPack and to compile the new version of OpenCV with CUDA and GStreamer support. The next section will demonstrate the procedure to do that.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Compiling OpenCV with CUDA support (if necessary)</h1>
                </header>
            
            <article>
                
<p>Though OpenCV which comes with JetPack, can work with a new OpenCV installation, it is a good idea to remove the old installation first and then start a new one. This will avoid unnecessary confusion. To accomplish that, the following steps have to be performed:</p>
<ol>
<li> Run the following command from the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt-get purge libopencv*</strong></pre>
<ol start="2">
<li>Make sure that all the packages installed are the latest versions. If that is not the case, then you can update them by running the following two commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt-get update</strong><br/><strong>$ sudo apt-get dist-upgrade</strong></pre>
<ol start="3">
<li>The latest versions of cmake and gcc compiler are needed to compile OpenCV from the source so they can be installed by running the following two commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt-get install --only-upgrade gcc-5 cpp-5 g++-5</strong><br/><strong>$ sudo apt-get install build-essential make cmake cmake-curses-gui libglew-dev libgtk2.0-dev</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>There are some dependencies that need to be installed to compile OpenCV with GStreamer support. This can be done by the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt-get install libdc1394-22-dev libxine2-dev libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev</strong></pre>
<ol start="5">
<li>Download the source for the latest version of OpenCV and extract it in a folder by executing the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://github.com/opencv/opencv/archive/3.4.0.zip -O opencv.zip</strong><br/><strong>$ unzip opencv.zip</strong></pre>
<ol start="6">
<li>Now go inside the  <kbd>opencv</kbd> folder and create the <kbd>build</kbd> directory. Then go inside this newly created <kbd>build</kbd> directory. These can be done by executing the following commands from Command Prompt.</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd opencv</strong><br/><strong>$ mkdir build</strong><br/><strong>$ cd build</strong> </pre>
<ol start="7">
<li>The <kbd>cmake</kbd><span> </span>command is used to compile<span> </span><kbd>opencv</kbd><span> </span>with CUDA support. Make sure the <kbd>WITH_CUDA</kbd> flag is set to <kbd>ON</kbd> in this command. Note  <kbd>CUDA_ARCH_BIN</kbd> should be set to <kbd>5.3</kbd> for a Jetson TX1 development board and <kbd>6.2</kbd> for Jetson TX2. The examples are not built to save time and space. The entire<span> </span><kbd>cmake</kbd><span> </span>command is as follows:</li>
</ol>
<pre style="padding-left: 60px" class="crayon-line crayon-striped-line"><strong>cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local \</strong><br/><strong>  -D WITH_CUDA=ON -D CUDA_ARCH_BIN="5.3" -D CUDA_ARCH_PTX="" \</strong><br/><strong>  -D WITH_CUBLAS=ON -D ENABLE_FAST_MATH=ON -D CUDA_FAST_MATH=ON \</strong><br/><strong>  -D ENABLE_NEON=ON -D WITH_LIBV4L=ON -D BUILD_TESTS=OFF \</strong><br/><strong>  -D BUILD_PERF_TESTS=OFF -D BUILD_EXAMPLES=OFF \</strong><br/><strong>  -D WITH_QT=ON -D WITH_OPENGL=ON ..</strong></pre>
<ol start="8">
<li>It will start the configuration and creation of <kbd>makefile</kbd>. The <kbd>cmake</kbd><span> command </span><span>will create  <kbd>makefile</kbd> in the <kbd>build</kbd> directory after successful configuration.</span></li>
<li><span>To compile OpenCV using <kbd>makefile</kbd> execute the  <kbd>make -j4</kbd> command from the command window.</span></li>
<li>After successful compilation, to install OpenCV you have to execute the command <kbd>sudo make install</kbd> <span>from the command line. </span></li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If these steps are executed successfully then OpenCV 3.4.0 will be installed with CUDA and GStreamer support on Jetson TX1, and any computer vision application made using OpenCV can be deployed on it. The next section will demonstrate simple image-processing operations on the board.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reading and displaying images</h1>
                </header>
            
            <article>
                
<p>The first basic operation needed for any computer vision application is that of reading and displaying images that are stored on the disk. This section will demonstrate a simple code to do this operation on Jetson TX1. The OpenCV syntax will not change much as we move from the GPU on the computer to the Jetson TX1 development board. A few minor changes will be there. The code for reading and displaying images on Jetson TX1 is as follows:</p>
<pre>#include &lt;opencv2/opencv.hpp&gt;<br/>#include &lt;iostream&gt;<br/><br/>using namespace cv;<br/>using namespace std;<br/><br/>int main()<br/>{<br/> Mat img = imread("images/cameraman.tif",0);<br/> if (img.empty()) <br/> {<br/> cout &lt;&lt; "Could not open an image" &lt;&lt; endl;<br/> return -1;<br/> }<br/> imshow("Image Read on Jetson TX1"; , img); <br/> waitKey(0); <br/> return 0;<br/>}</pre>
<p>The necessary OpenCV libraries are included in the code. The image is read using the <kbd>imread</kbd> function inside the <kbd>Main</kbd> function. The image is read as a grayscale image because the second argument to the <kbd>imread</kbd> command is specified as <kbd>0</kbd>. To read an image as a color image, it can be specified as <kbd>1</kbd>. The <kbd>if</kbd> statement checks whether the image is read or not, and if it is not then the code is terminated after displaying an error on the console. When the name of the image is incorrect or the image is not stored in the specified path, then an error in reading an image can happen. This error is handled by the <kbd>if</kbd> statement. The image is displayed using the <kbd>imshow</kbd> command. The <kbd>waitKey</kbd> function is used to display the image until any key is pressed on the keyboard.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding code shown can be saved as the <kbd>image_read.cpp</kbd> file and executed using the following command from the Terminal. Make sure that the program file is stored in the current working directory of the Terminal:</p>
<pre>For compilation:<br/><strong>$ g++ -std = c++11 image_read.cpp 'pkg_config --libs --cflags opencv' -o image_read</strong><br/>For execution:<br/><strong>$./image_read</strong></pre>
<p>The output of the program is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-643 image-border" src="assets/e845b1c3-7dcf-4e95-98ba-48b43c16ec77.png" style="" width="363" height="330"/></div>
<p>This section demonstrated the procedure to read and display an image on Jetson TX1. In the next section, we will see some more image-processing operations and also try to measure the performance of them on Jetson TX1.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image addition</h1>
                </header>
            
            <article>
                
<p>This section will demonstrate the use of Jetson TX1 for simple image-processing applications like image addition. The intensities of pixels at the same location are added to construct the new image after addition. Suppose in two images, the pixel at (0,0) has intensity values 50 and 150 respectively, then the intensity value in the resultant image will be 200, which is the addition of the two intensity values. OpenCV addition is a saturated operation, which means that if an answer of addition goes above 255 then it will be saturated at 255. The code to perform addition on Jetson TX1 is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>#include "opencv2/core/cuda.hpp"<br/><br/>int main (int argc, char* argv[])<br/>{<br/> //Read Two Images <br/> cv::Mat h_img1 = cv::imread("images/cameraman.tif");<br/> cv::Mat h_img2 = cv::imread("images/circles.png");<br/> int64 work_begin = cv::getTickCount(); <br/> //Create Memory for storing Images on device<br/> cv::cuda::GpuMat d_result1,d_img1, d_img2;<br/> cv::Mat h_result1;<br/> //Upload Images to device <br/> d_img1.upload(h_img1);<br/> d_img2.upload(h_img2);<br/><br/> cv::cuda::add(d_img1,d_img2, d_result1);<br/> //Download Result back to host<br/> d_result1.download(h_result1);<br/> cv::imshow("Image1 ", h_img1);<br/> cv::imshow("Image2 ", h_img2);<br/> cv::imshow("Result addition ", h_result1);<br/> int64 delta = cv::getTickCount() - work_begin;<br/> //Frequency of timer<br/> double freq = cv::getTickFrequency();<br/> double work_fps = freq / delta;<br/> std::cout&lt;&lt;"Performance of Addition on Jetson TX1: " &lt;&lt;std::endl;<br/> std::cout &lt;&lt;"Time: " &lt;&lt; (1/work_fps) &lt;&lt;std::endl;<br/> std::cout &lt;&lt;"FPS: " &lt;&lt;work_fps &lt;&lt;std::endl;<br/><br/> cv::imshow("result_add.png", h_result1);<br/> cv::waitKey();<br/> return 0;<br/>}</pre>
<p>One thing to be kept in mind while doing image addition is that both the images should be of the same size. If it is not the case, then they should be resized before addition. In the preceding code, two images of the same size are read from the disk and uploaded to the device memory for addition on a GPU. The <kbd>add</kbd> function from the <kbd>cv::cuda</kbd> module is used to perform image addition on the device. The resultant image is downloaded to the host and displayed on the console.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output of the program is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-644 image-border" src="assets/060e8009-68ad-4fe0-a469-d40c3aff8e01.png" style="" width="803" height="291"/></div>
<p>The performance of image addition is also measured using the <kbd>cv::getTickCount()</kbd> and <kbd>cv::getTickFrequency()</kbd> functions. The time taken by the addition operation is displayed on the console as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-645 image-border" src="assets/9e92015f-18e5-499d-998d-3a42d70b0232.png" style="" width="544" height="71"/></div>
<p>As can be seen from the preceding screenshot, it takes around <kbd>0.26ms</kbd> to add two images of the size 256 x 256 on Jetson TX1. This is a very good performance for an embedded platform. It should be noted that performance should be measured before the <kbd>imshow</kbd> function to measure the accurate time for the addition operation. The <kbd>imshow</kbd> function takes more time to display an image, so the time measured will not be an accurate estimation of time taken to do an add operation.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image thresholding</h1>
                </header>
            
            <article>
                
<p><span>This section will demonstrate the use of Jetson TX1 for more computationally intensive computer vision applications, like image thresholding. Image thresholding is a very simple image segmentation technique used to extract important regions from a grayscale image, based on certain intensity values. In this technique, if the pixel value is greater than a certain threshold value then it is assigned one value, or else it is assigned another value.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">OpenCV provides different types of thresholding techniques, and it is decided by the last argument of the function. These thresholding types are:</p>
<ul>
<li class="mce-root"><kbd>cv:.THRES H_BINARY</kbd>: If the intensity of the pixel is greater than the threshold, then set the pixel intensity equal to the <kbd>maxVal</kbd> constant, or else set the pixel intensity to zero.</li>
<li class="mce-root"><kbd>cv::THRESH_BINARY_INV</kbd>: If the intensity of the pixel is greater than the threshold, then set the pixel intensity equal to zero, or else set the pixel intensity to the <kbd>maxVal</kbd> constant.</li>
<li class="mce-root"><kbd>cv::THRESH_TRUNC</kbd>: This is basically a truncation operation. If the intensity of the pixel is greater than the threshold, then set the pixel intensity equal to the threshold, or else keep the intensity value as it is.</li>
<li class="mce-root"><kbd>cv::THRESH_TOZERO</kbd>: If the intensity of the pixel is greater than the threshold, then keep the pixel intensity as it is, or else set the pixel intensity to zero.</li>
<li class="mce-root"><kbd>cv::THRESH_TOZERO_INV</kbd>: If the intensity of the pixel is greater than the threshold, then set that pixel intensity equal to zero, or else keep the pixel intensity as it is.</li>
</ul>
<p class="mce-root">The program to implement all these thresholding techniques using OpenCV and CUDA on Jetson TX1 is shown as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>using namespace cv;<br/>int main (int argc, char* argv[])<br/>{<br/> cv::Mat h_img1 = cv::imread("images/cameraman.tif", 0);<br/> cv::cuda::GpuMat d_result1,d_result2,d_result3,d_result4,d_result5, d_img1;<br/> //Measure initial time ticks<br/> int64 work_begin = getTickCount(); <br/> d_img1.upload(h_img1);<br/> cv::cuda::threshold(d_img1, d_result1, 128.0, 255.0, cv::THRESH_BINARY);<br/> cv::cuda::threshold(d_img1, d_result2, 128.0, 255.0, cv::THRESH_BINARY_INV);<br/> cv::cuda::threshold(d_img1, d_result3, 128.0, 255.0, cv::THRESH_TRUNC);<br/> cv::cuda::threshold(d_img1, d_result4, 128.0, 255.0, cv::THRESH_TOZERO);<br/> cv::cuda::threshold(d_img1, d_result5, 128.0, 255.0, cv::THRESH_TOZERO_INV);<br/><br/> cv::Mat h_result1,h_result2,h_result3,h_result4,h_result5;<br/> d_result1.download(h_result1);<br/> d_result2.download(h_result2);<br/> d_result3.download(h_result3);<br/> d_result4.download(h_result4);<br/> d_result5.download(h_result5);<br/> //Measure difference in time ticks<br/> int64 delta = getTickCount() - work_begin;<br/> double freq = getTickFrequency();<br/> //Measure frames per second<br/> double work_fps = freq / delta;<br/> std::cout &lt;&lt;"Performance of Thresholding on GPU: " &lt;&lt;std::endl;<br/> std::cout &lt;&lt;"Time: " &lt;&lt; (1/work_fps) &lt;&lt;std::endl;<br/> std::cout &lt;&lt;"FPS: " &lt;&lt;work_fps &lt;&lt;std::endl;<br/> return 0;<br/>}</pre>
<p>The function used for image thresholding in OpenCV and CUDA on a GPU is <kbd>cv::cuda::threshold</kbd>. This function has many arguments. The first argument is the source image, which should be a grayscale image. The second argument is the destination at which the result is to be stored. The third argument is the threshold value, which is used to segment the pixel values. The fourth argument is the <kbd>maxVal</kbd> constant, which represents the value to be given if the pixel value is more than the threshold value. The final argument is the thresholding methods discussed earlier. The output of the program that shows the original image and the output of five thresholding techniques is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-646 image-border" src="assets/97d3d2e4-1bbe-4207-a477-4016201eaa19.png" style="" width="813" height="598"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The performance of image thresholding is also measured using the <kbd>cv::getTickCount()</kbd> and <kbd>cv::getTickFrequency()</kbd> functions. The time taken by five thresholding operations is displayed on the console, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-647 image-border" src="assets/95e2352a-1d63-44ca-bcb5-702a117b41ad.png" style="" width="250" height="56"/></div>
<p>It takes <kbd>0.32ms</kbd> to do five thresholding operations on Jetson TX1, which is again a very good performance for an image segmentation task on embedded platforms. The next section will describe the filtering operations on Jetson TX1.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image filtering on Jetson TX1</h1>
                </header>
            
            <article>
                
<p>Image filtering is a very important step in image preprocessing and feature extractions. Low pass filters, like averaging, Gaussian, and median filters, are used to remove different types of noise in an image, while high pass filters, like Sobel, Scharr, and Laplacian, are used to detect edges in an image. Edges are important features that can be used for computer vision tasks like object detection and classification. Image filtering is explained in detail earlier in this book.</p>
<p>This section describes the procedure to apply low pass and high pass filters on an image on Jetson TX1. The code for this is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;string&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/><br/>int main()<br/>{<br/> Mat h_img1;<br/> cv::cuda::GpuMat d_img1,d_blur,d_result3x3;<br/> h_img1 = imread("images/blobs.png",1);<br/><br/> int64 start = cv::getTickCount();<br/> d_img1.upload(h_img1);<br/> cv::cuda::cvtColor(d_img1,d_img1,cv::COLOR_BGR2GRAY);<br/> cv::Ptr&lt;cv::cuda::Filter&gt; filter3x3;<br/> filter3x3 = cv::cuda::createGaussianFilter(CV_8UC1,CV_8UC1,cv::Size(3,3),1);<br/> filter3x3-&gt;apply(d_img1, d_blur);<br/>  <br/> cv::Ptr&lt;cv::cuda::Filter&gt; filter1;<br/> filter1 = cv::cuda::createLaplacianFilter(CV_8UC1,CV_8UC1,1);<br/> filter1-&gt;apply(d_blur, d_result3x3);<br/>  <br/> cv::Mat h_result3x3,h_blur;<br/> d_result3x3.download(h_result3x3);<br/> d_blur.download(h_blur);<br/>  <br/> double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/> std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/> imshow("Laplacian", h_result3x3);<br/> imshow("Blurred", h_blur);<br/> cv::waitKey();<br/> return 0;<br/>}</pre>
<p>Laplacian is a second-order derivative used to extract both vertical and horizontal images from an image. It is highly sensitive to noise so sometimes it is necessary to remove noise using a low pass filter, like the Gaussian blur, and then apply a Laplacian filter. So in the code, the Gaussian filter of size 3 x 3 is applied to an input image with a standard deviation equal to <kbd>1</kbd>. The filter is created using the <kbd>cv::cuda::createGaussianFilter</kbd> function of OpenCV. The Laplacian filter is then applied to the Gaussian blurred image. The Laplacian filter is created using the <kbd>cv::cuda::createLaplacianFilter</kbd> function of OpenCV. The output of the Gaussian blurring and Laplacian filter is downloaded back to the host memory for display on the console. The performance of the filtering operations is also measured in the code. The output of the program is shown in the following screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-648 image-border" src="assets/fb538e16-9cee-4b60-ab1b-808e7831baae.png" style="" width="656" height="300"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As can be seen from the output, the Laplacian filter on a blurred image will remove false edges from an image. It will also remove Gaussian noise present in an input image. If an input image is distorted by salt and pepper noise then the median filter should be used as a preprocessing step to a Laplacian filter for edge detection. </p>
<p>To summarize, we have seen different image-processing functions such as image addition, image thresholding and image filtering on Jetson TX1. We have also seen that the performance of these operations on Jetson TX1 is much better than the performance of the same code on a CPU. The next section will describe the interfacing of the camera with a Jetson TX1 so that it can be used in real-life situations.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interfacing cameras with Jetson TX1</h1>
                </header>
            
            <article>
                
<p>Jetson TX1 can be interfaced with USB cameras or CSI cameras. The development board comes with one camera of 5 MP already interfaced with Jetson TX1. This camera can be used to capture video just like a webcam on a laptop. Camera interfacing is an important feature that makes the Jetson TX1 development board useful in real-time situations. It supports up to six-lane cameras. The detailed list of cameras supported by Jetson TX1 can be found at the following link: <a href="https://elinux.org/Jetson_TX1">https://elinux.org/Jetson_TX1 .</a></p>
<p>This section will demonstrate the procedure to capture videos using a camera interfaced with Jetson TX1 and how these videos can be used to develop computer vision applications, like face detection and background subtraction.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reading and displaying video from onboard camera</h1>
                </header>
            
            <article>
                
<p>This section will describe the method used to capture video from a USB camera or onboard camera interfaced with Jetson TX1. For this, OpenCV should be compiled with GStreamer support; otherwise, the format of the captured video will not be supported by OpenCV.</p>
<p>The following code can be used to capture video from a camera and display it on the screen:</p>
<pre>#include &lt;opencv2/opencv.hpp&gt;<br/>#include &lt;iostream&gt;<br/>#include &lt;stdio.h&gt;<br/>using namespace cv;<br/>using namespace std;<br/><br/>int main(int, char**)<br/>{<br/> Mat frame;<br/> // open the default camera using default API<br/> VideoCapture cap("nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)I420, framerate=(fraction)24/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format=(string)BGR ! appsink"); <br/> if (!cap.isOpened()) {<br/> cout &lt;&lt; "Unable to open camera\n";<br/> return -1;<br/> }<br/> while (1)<br/> {<br/> int64 start = cv::getTickCount();<br/> cap.read(frame);<br/> // check if we succeeded<br/> if (frame.empty()) {<br/>  cout &lt;&lt; "Can not read frame\n";<br/>  break;<br/> }<br/> double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/> std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/><br/> imshow("Live", frame);<br/> if (waitKey(30) == 'q')<br/>  break;<br/> }<br/> <br/> return 0;<br/>}</pre>
<p>The code is more or less similar to a code used for capturing video from a webcam on a desktop. Instead of using a device ID as an argument to capture an object, the string that specifies GStreamer pipeline is used. This is shown as follows:</p>
<pre>VideoCapture cap("nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)I420, framerate=(fraction)24/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format=(string)BGR ! appsink");</pre>
<p>The width and height of the captured video are specified as 1,280 and 720 pixels. The frame rate is also specified. These values will change according to formats supported by interfaced cameras. Use  <kbd>nvvidconv</kbd> to convert video to BGR format that is supported by OpenCV. It is also used for image scaling and flipping. To flip the captured video, the flip method can be specified as an integer value other than zero.  </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>cap.isOpened</kbd> property is used to check whether capturing from the camera has started or not. Then the frames are read one by one using the read method and displayed on the screen until <kbd>q</kbd> is pressed by the user. The rate of frame capturing is also measured in the code. </p>
<p>The output of the live video is captured by the camera for two different frames, and the frame rate is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-649 image-border" src="assets/2993e7b3-e147-4021-a841-5ed772f34066.png" style="" width="960" height="286"/></div>
<p>To summarize, in this section we have seen the procedure to capture video from a camera interfaced with a Jetson TX1 development board. This captured video can be used to develop useful real-time computer vision applications as described in the next section.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Advanced applications on Jetson TX1</h1>
                </header>
            
            <article>
                
<p>This section will describe the use of a Jetson TX1 embedded platform in the deployment of advanced computer vision applications, like face detection, eye detection, and background subtraction. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Face detection using Haar cascades</h1>
                </header>
            
            <article>
                
<p><span>A Haar cascade uses rectangular features to detect an object. It uses rectangles of different sizes to calculate different line and edge features. </span>The idea behind the Haar-like feature detection algorithm is to compute the difference between the sum of white pixels and the sum of black pixels inside the rectangle.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The main advantage of this method is the fast sum computation using the integral image approach. This makes the Haar cascade ideal for real-time object detection. It requires less time for processing an image than other algorithms used for object detection. The Haar cascade is ideal for deployment on embedded systems like Jetson TX1 because of its low computational complexity and low memory footprint.<span> So in this section, this algorithm is used to deploy face detection applications on Jetson TX1.</span></p>
<p>The code for face detection from a video captured by a camera interfaced with Jetson TX1 is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;opencv2/opencv.hpp&gt;<br/>using namespace cv;<br/>using namespace std;<br/><br/>int main()<br/>{<br/> VideoCapture cap("images/output.avi");<br/>//cv::VideoCapture cap("nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)I420, framerate=(fraction)24/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format=(string)BGR ! appsink"); <br/> if (!cap.isOpened()) {<br/>   cout &lt;&lt; "Can not open video source";<br/>   return -1;<br/> }<br/> std::vector&lt;cv::Rect&gt; h_found;<br/> cv::Ptr&lt;cv::cuda::CascadeClassifier&gt; cascade = cv::cuda::CascadeClassifier::create("haarcascade_frontalface_alt2.xml");<br/> cv::cuda::GpuMat d_frame, d_gray, d_found;<br/> while(1)<br/> {<br/> Mat frame;<br/> if ( !cap.read(frame) ) {<br/>   cout &lt;&lt; "Can not read frame from webcam";<br/>   return -1;<br/> }<br/> int64 start = cv::getTickCount();<br/> d_frame.upload(frame);<br/> cv::cuda::cvtColor(d_frame, d_gray, cv::COLOR_BGR2GRAY);<br/><br/> cascade-&gt;detectMultiScale(d_gray, d_found);<br/> cascade-&gt;convert(d_found, h_found);<br/> <br/> for(int i = 0; i &lt; h_found.size(); ++i)<br/> {<br/>   rectangle(frame, h_found[i], Scalar(0,255,255), 5);<br/> }<br/> double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/> std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/> imshow("Result", frame);<br/> if (waitKey(1) == 'q') {<br/>   break;<br/> }<br/> }<br/><br/> return 0;<br/>}</pre>
<p>The Haar cascade is an algorithm which needs to be trained to do a particular task. It is difficult to train a Haar cascade from scratch for a particular application, so OpenCV provides some trained XML files which can be used to detect objects. These XML files are provided in the <kbd>\usr\local\opencv\data\haarcascades_cuda</kbd> directory of the OpenCV and CUDA installations.</p>
<p><span>The webcam is initialized, and frames from the webcam are captured one by one. The frame is uploaded to the device memory for processing on the GPU. </span>OpenCV and CUDA provide the <kbd>CascadeClassifier</kbd> class that can be used for implementing the Haar cascade. The create method is used to create an object of that class. It requires the filename of the trained XML file to be loaded. </p>
<p><span>Inside the <kbd>while</kbd> loop,the <kbd>detectMultiscale</kbd> method is applied to every frame so that faces of different sizes can be detected in each frame. The detected location is converted to a rectangle vector, using the convert method. Then, this vector is iterated using the <kbd>for</kbd> loop so that a bounding box can be drawn using a rectangle function on all the detected faces. This procedure is repeated for every frame captured from the webcam. The performance of the algorithm is also measured in terms of frames per second.</span></p>
<p><span> The output of the program is as follows: </span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-650 image-border" src="assets/203b7c76-1671-45d1-97b5-5b4cd0fc34bd.png" style="" width="1057" height="286"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As can be seen from the output, the face is correctly localized in two different frames of the webcam at different positions. The second frame is a little bit blurred,  but it is not affecting the algorithm. The performance of the algorithm on Jetson TX1 is also shown in the right image. The algorithm works at around five frames per second.</p>
<p>To summarize, this section demonstrates the use of Jetson TX1 in detecting faces from a live video captured from a webcam. This application can be used for person identification, face locking, attendance monitoring, and so on.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Eye detection using Haar cascades</h1>
                </header>
            
            <article>
                
<p><span>This section will describe the use of Haar cascades in detecting </span>the eyes<span> of humans. The XML file for a trained Haar cascade for eye detection is provided in the OpenCV installation directory. This file is used to detect eyes. The code for it is as follows:</span></p>
<pre>#include "opencv2/objdetect/objdetect.hpp"<br/>#include "opencv2/highgui/highgui.hpp"<br/>#include "opencv2/imgproc/imgproc.hpp"<br/>#include "opencv2/cudaobjdetect.hpp" <br/>#include &lt;iostream&gt;<br/>#include &lt;stdio.h&gt;<br/> <br/>using namespace std;<br/>using namespace cv;<br/> <br/>int main( )<br/>{<br/>  Mat h_image;<br/>  h_image = imread("images/lena_color_512.tif", 0); <br/>  Ptr&lt;cuda::CascadeClassifier&gt; cascade =       cuda::CascadeClassifier::create("haarcascade_eye.xml");<br/>  cuda::GpuMat d_image;<br/>  cuda::GpuMat d_buf;<br/>  int64 start = cv::getTickCount();<br/>  d_image.upload(h_image);<br/>  cascadeGPU-&gt;setMinNeighbors(0);<br/>  cascadeGPU-&gt;setScaleFactor(1.02);<br/>  cascade-&gt;detectMultiScale(d_image, d_buf);<br/>  std::vector&lt;Rect&gt; detections;<br/>  cascade-&gt;convert(d_buf, detections);<br/> if (detections.empty())<br/>   std::cout &lt;&lt; "No detection." &lt;&lt; std::endl;<br/>   cvtColor(h_image,h_image,COLOR_GRAY2BGR);<br/> for(int i = 0; i &lt; detections.size(); ++i)<br/> {<br/>   rectangle(h_image, detections[i], Scalar(0,255,255), 5);<br/> }<br/> double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/> std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/> imshow("Result image on Jetson TX1", h_image);<br/>  <br/> waitKey(0); <br/> return 0;<br/>}</pre>
<p><span>The code is similar to the code for face detection. This is the advantage of using Haar cascades. If an XML file for a trained Haar cascade on a given object is available then the same code will work in all applications. Just the name of the XML file needs to change while creating an object of the <kbd>CascadeClassifier</kbd> class. In the preceding code, <kbd>haarcascade_eye.xml</kbd> , which is the trained XML file for eye detection, is used. The other code is self-explanatory. The scale factor is set at <kbd>1.02</kbd> so that image size will be reduced by <kbd>1.02</kbd> at every scale. The output of the eye detection program is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-652 image-border" src="assets/a440675f-7e4d-4361-9348-7813b5d8500e.png" style="" width="447" height="233"/></div>
<p>Now that we have detected objects from video and images using a Haar cascade, the captured video can also be used to detect and track objects using the background subtraction method as described in the next section. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Background subtraction using Mixture of Gaussian (MoG)</h1>
                </header>
            
            <article>
                
<p>Background subtraction is an important preprocessing step for object detection and tracking applications. It can also be used for unusual activity detection from  CCTV footage. This section demonstrates the use of Jetson TX1 in a background subtraction application. The camera interfaced with Jetson TX1 is mounted in a room for activity detection inside the room. The background of the room is initialized in the first frame. </p>
<p>The MoG, which is a widely used background subtraction method used for separating the foreground from the background based on Gaussian mixtures, is used for activity detection. The background is continuously updated from the sequence of frames. A mixture of K Gaussian distribution is used to categorize pixels as foreground or background. The time sequence of the frame is also weighted to improve background modeling. The intensities that are continuously changing are categorized as foreground, and intensities that are static are categorized as background.</p>
<p>The code for activity monitoring using MoG is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;string&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/>int main()<br/>{<br/>  <br/> VideoCapture cap("nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)I420, framerate=(fraction)24/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format=(string)BGR ! appsink");<br/> if (!cap.isOpened())<br/> {<br/> cout &lt;&lt; "Can not open camera or video file" &lt;&lt; endl;<br/> return -1;<br/> }<br/> Mat frame;<br/> cap.read(frame);<br/> GpuMat d_frame;<br/> d_frame.upload(frame);<br/> Ptr&lt;BackgroundSubtractor&gt; mog = cuda::createBackgroundSubtractorMOG();<br/> GpuMat d_fgmask,d_fgimage,d_bgimage;<br/> Mat h_fgmask,h_fgimage,h_bgimage;<br/> mog-&gt;apply(d_frame, d_fgmask, 0.01);<br/> namedWindow("image", WINDOW_NORMAL);<br/> namedWindow("foreground mask", WINDOW_NORMAL);<br/> namedWindow("foreground image", WINDOW_NORMAL);<br/> namedWindow("mean background image", WINDOW_NORMAL);<br/><br/> while(1)<br/> {<br/> cap.read(frame);<br/> if (frame.empty())<br/>  break;<br/> d_frame.upload(frame);<br/> int64 start = cv::getTickCount();<br/> mog-&gt;apply(d_frame, d_fgmask, 0.01);<br/> mog-&gt;getBackgroundImage(d_bgimage);<br/> double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/> std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/> d_fgimage.create(d_frame.size(), d_frame.type());<br/> d_fgimage.setTo(Scalar::all(0));<br/> d_frame.copyTo(d_fgimage, d_fgmask);<br/> d_fgmask.download(h_fgmask);<br/> d_fgimage.download(h_fgimage);<br/> d_bgimage.download(h_bgimage);<br/> imshow("image", frame);<br/> imshow("foreground mask", h_fgmask);<br/> imshow("foreground image", h_fgimage);<br/> imshow("mean background image", h_bgimage);<br/> if (waitKey(1) == 'q')<br/>  break;<br/> }<br/><br/> return 0;<br/>}</pre>
<p>The camera interfaced with Jetson TX1 is initialized with the GStreamer pipeline. The <kbd>createBackgroundSubtractorMOG</kbd> class is used to create an object for MoG implementation. The <kbd>apply</kbd> method of the created object is used to create a foreground mask from the first frame. It requires an input image, an <kbd>image</kbd> array to store foreground mask, and learning rate as the input. The image of the room without any activity is initialized as a background for the MoG. So, any activity that will happen will be categorized as foreground by the algorithm.</p>
<p>This foreground mask and the background image are continuously updated after every frame inside the <kbd>while</kbd> loop. The <kbd>getBackgroundImage</kbd> function is used to fetch<span> </span>the current<span> </span>background model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The foreground mask is used to create a foreground image, which indicates which objects are currently moving. It is basically logical and operates between the original frame and foreground mask. The foreground mask, foreground image, and the modeled background are downloaded to the host memory after every frame, for displaying on the screen.</p>
<p>The output of two different frames from the video is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-653 image-border" src="assets/cf472126-4abb-4203-a695-d60f675917bb.png" style="" width="973" height="549"/></div>
<p>The first row indicates the background of the room without any activity. When someone moves a hand in front of the camera, it will be detected as foreground, as shown in the second frame result. In the same way, if someone puts a cell phone in front of the camera, that will also be categorized as foreground, as shown in the third frame. The performance of the code in terms of frames per second is shown in the following screenshot:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-654 image-border" src="assets/6c0f9dae-08fd-438f-96ab-fa6067dd7a20.png" style="" width="413" height="421"/></div>
<p><span>The technique works at around 60-70 frames per second, which can easily be used to take a real-time decision. Though the demonstration in this section is very trivial, this application can be used in many real-life situations. The activity inside a room can be used to control the appliances present in the room. This will help in saving electricity when no person is present. This application can also be used at an ATM for monitoring activity inside it. It can also be used for other video surveillance applications in public places. Python can also be used as the programming language on Jetson TX1, which will be explained in the next section.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Computer vision using Python and OpenCV on Jetson TX1</h1>
                </header>
            
            <article>
                
<p>Up till now, we have developed all computer vision applications using C/C++, OpenCV, and CUDA. Jetson TX1 also supports the Python programming language for computer vision applications. When OpenCV is compiled on Jetson TX1, it also installs Python binaries for OpenCV. So programmers who are comfortable in Python programming language can use a Python interface for OpenCV in developing computer vision applications and deploying them on Jetson TX1. Python also comes preinstalled with Jetson TX1 as is the case for all Linux operating systems. Windows users can install Python separately. The installation procedure and advantages of Python are explained in the next chapter.</p>
<p>One disadvantage of using Python is that OpenCV <span>Python interface</span> is still not greatly benefited by CUDA acceleration. Still, the ease of learning Python and the wide range of applications in which it can be used have encouraged many software developers to use Python for computer vision applications. The sample code for reading and displaying images using Python and OpenCV is as follows: </p>
<pre>import numpy as np<br/>import cv2<br/>img = cv2.imread('images/cameraman.tif',0)<br/>cv2.imshow("Image read in Python", img)<br/>k = cv2.waitKey(0) &amp; 0xFF<br/>if k == 27: # wait for ESC key to exit<br/> cv2.destroyAllWindows()</pre>
<p>In Python,the <kbd>import</kbd> command is used to include a library in a file. So the <kbd>cv2</kbd> library is included by using the <kbd>import cv2</kbd> command. Images are stored as <kbd>numpy</kbd> arrays so <kbd>numpy</kbd> is also imported in a file. The <kbd>imread</kbd> function is used to read an image in the same way as C++. All OpenCV functions have to be prefixed with <kbd>cv2.</kbd> in Python. The <kbd>imshow</kbd> function is used to display an image. All OpenCV functions have a similar signature and functionality in Python as C++. </p>
<p>The following command can be used to execute the code from the Terminal:</p>
<pre># For Python2.7<br/><strong>$ python image_read.py</strong><br/># For Python 3<br/><strong>$ python image_read.py</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output of the program is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-655 image-border" src="assets/79d8b542-c442-41a4-927e-6f40e2831558.png" style="" width="257" height="283"/></div>
<p>This section is just included to make you aware that Python can also be used as a programming language for developing computer vision applications using OpenCV and deploying it on Jetson TX1.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter described the use of Jetson TX1 in the deployment of CUDA and OpenCV code. The properties of the GPU device present on a TX1 board that make it ideal for deploying computationally complex applications are explained in detail. The performance of Jetson TX1 for CUDA applications such as adding two large arrays is measured and compared with GPUs present on laptops. The procedure to work with images on Jetson TX1 is explained in detail in this chapter. The image-processing applications like image addition, image thresholding, and image filtering are deployed on Jetson TX1 and performance is measured for them.</p>
<p>The best part of Jetson TX1 is that multiple cameras can be interfaced with it in an embedded environment, and videos from that camera can be processed to design complex computer vision applications. The procedure to capture video from an onboard or USB camera interfaced with Jetson TX1 is explained in detail.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The chapter also described the deployment of advanced computer vision applications like face detection, eye detection, and background subtraction on Jetson TX1. The Python language can also be used to deploy computer vision applications on Jetson TX1. This concept is explained in the last part of the chapter. So far, we have seen how the C/C++ language can leverage the advantages of CUDA and GPU acceleration.</p>
<p>The next couple of chapters will demonstrate the use of CUDA and GPU acceleration for the Python language using the PyCUDA module.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Compare the performance of a GPU device on Jetson TX1 with a GeForce 940 GPU seen earlier in the book.</li>
<li>State True or False: All CUDA programs seen earlier in the book can be executed on Jetson TX1 without modification.</li>
<li>What is the need for recompiling OpenCV on Jetson TX1?</li>
<li>State True or False: OpenCV can't capture video from a camera connected to the USB port.</li>
<li>State True or False: It is better to use a CSI camera for computationally intensive applications than a USB camera.</li>
<li>If you are developing computationally intensive computer vision applications using OpenCV, which language would you prefer for faster performance?</li>
<li>Is there a need to install separate OpenCV Python binding or Python interpreter on Jetson TX1?</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>