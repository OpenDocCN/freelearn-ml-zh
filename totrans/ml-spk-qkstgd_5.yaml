- en: Unsupervised Learning Using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will train and evaluate unsupervised machine learning models
    applied to a variety of real-world use cases, again using Python, Apache Spark,
    and its machine learning library, `MLlib`. Specifically, we will develop and interpret
    the following types of unsupervised machine learning models and techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As described in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, in unsupervised learning, the goal is to uncover
    hidden relationships, trends, and patterns given only the input data, *x[i]*,
    with no output, *y[i]*. In other words, our input dataset will be of the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f2330fc-359c-4772-824c-3ba8d26bb77d.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering is a well-known example of a class of unsupervised learning algorithms
    where the goal is to segment data points into groups, where all of the data points
    in a specific group share similar features or attributes in common. By the nature
    of clustering, however, it is recommended that clustering models are trained on
    large datasets to avoid over fitting. The two most commonly used clustering algorithms
    are **hierarchical clustering** and **k-means clustering**, which are differentiated
    from each other by the processes by which they construct clusters. We shall study
    both of these algorithms in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By definition, in order to cluster data points into groups, we require an understanding
    of the *distance* between two given data points. A common measure of distance
    is the **Euclidean distance**, which is simply the straight-line distance between
    two given points in *k*-dimensional space, where *k* is the number of independent
    variables or features. Formally, the Euclidean distance between two points, *p*
    and *q*, given *k* independent variables or dimensions is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1555c487-d45f-4a1d-ac70-8be68fef47a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Other common measures of distance include the **Manhattan distance**, which
    is the sum of the absolute values instead of squares ( ![](img/c9a63cea-d2fb-4e72-b7fc-9c7471329d7a.png))
    and the **maximum coordinate distance**, where measurements are only considered
    for those data points that deviate the most. For the remainder of this chapter,
    we will measure the Euclidean distance. Now that we have an understanding of distance,
    we can define the following measures between two clusters, as illustrated in *Figure
    5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: The *minimum distance* between clusters is the distance between the two points
    that are the closest to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *maximum distance* between clusters is the distance between the two points
    that are furthest away from each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *centroid distance* between clusters is the distance between the centroids
    of each cluster, where the centroid is defined as the average of all data points
    in a given cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f379f7d2-1234-405f-8932-d9bcbccef7bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Cluster distance measures'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In hierarchical clustering, each data point starts off in its own self-defined
    cluster—for example, if you have 10 data points in your dataset, then there will
    initially be 10 clusters. The two *nearest* clusters, as defined by the Euclidean
    centroid distance, for example, are then combined. This process is then repeated
    for all distinct clusters until eventually all data points belong in the same
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process can be visualized using a **dendrogram**, as illustrated in *Figure
    5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66d40dfe-b419-4837-8425-c7650889885f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Hierarchical clustering dendrogram'
  prefs: []
  type: TYPE_NORMAL
- en: A dendrogram helps us to decide when to stop the hierarchical clustering process.
    It is generated by plotting the original data points on the *x *axis and the distance
    between clusters on the *y *axis. As new parent clusters are created, by combining
    the nearest clusters together, a horizontal line is plotted between those child
    clusters. Eventually, the dendrogram ends when all data points belong in the same
    cluster. The aim of the dendrogram is to tell us when to stop the hierarchical
    clustering process. We can deduce this by drawing a dashed horizontal line across
    the dendrogram, placed at a position that maximizes the vertical distance between
    this dashed horizontal line and the next horizontal line (up or down). The final
    number of clusters at which to stop the hierarchical clustering process is then
    the number of vertical lines the dashed horizontal line intersects. In *Figure
    5.2*, we would end up with two clusters containing the data points {5, 2, 7} and
    {8, 4, 10, 6, 1, 3, 9} respectively. However, make sure that the final number
    of clusters makes sense in the context of your use case.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In k-means clustering, a different process is followed in order to segment
    data points into clusters. First, the final number of clusters, *k*, must be defined
    upfront based on the context of your use case. Once defined, each data point is
    randomly assigned to one of these *k* clusters, after which the following process
    is employed:'
  prefs: []
  type: TYPE_NORMAL
- en: The centroid of each cluster is computed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points are then reassigned to those clusters that have the closest centroid
    to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The centroids of all clusters are then recomputed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points are then reassigned once more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is repeated until no data points can be reassigned—that is, until
    there are no further improvements to be had and all data points belong to a cluster
    that has the closest centroid to them. Therefore, since the centroid of a cluster
    is defined as the mean average of all data points in a given cluster, k-means
    clustering effectively partitions the data points into *k* clusters with each
    data point assigned to a cluster with a mean average that is closest to it.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in both clustering processes (hierarchical and k-means), a measure
    of distance needs to be computed. However, distance scales differently based on
    the type and units of the independent variables involved—for example, height and
    weight. Therefore, it is important to normalize your data first (sometimes called
    feature scaling) before training a clustering model so that it works properly.
    To learn more about normalization, please visit [https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling).
  prefs: []
  type: TYPE_NORMAL
- en: Case study – detecting brain tumors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s apply k-means clustering to a very important real-world use case: detecting
    brain tumors from **magnetic resonance imaging** (**MRI**) scans. MRI scans are
    used across the world to generate detailed images of the human body, and can be
    used for a wide range of medical applications, from detecting cancerous cells
    to measuring blood flow. In this case study, we will use grayscale MRI scans of
    a healthy human brain as the input for a k-means clustering model. We will then
    apply the trained k-means clustering model to an MRI scan of another human brain
    to see if we can detect suspicious growths and tumors.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the images we will use in this case study are relatively simple, in
    that any suspicious growths that are present will be visible to the naked eye.
    The fundamental purpose of this case study is to show how Python may be used to
    manipulate images, and how `MLlib` may be used to natively train k-means clustering
    models via its k-means estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Feature vectors from images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first challenge for us is to convert images into numerical feature vectors
    in order to train our k-means clustering model. In our case, we will be using
    grayscale MRI scans. A grayscale image in general can be thought of as a matrix
    of pixel-intensity values between 0 (black) and 1 (white), as illustrated in *Figure
    5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f66f2887-1962-4aad-8f70-9dd144bde421.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Grayscale image mapped to a matrix of pixel-intensity values'
  prefs: []
  type: TYPE_NORMAL
- en: The dimensions of the resulting matrix is equal to the height (*m*) and width
    (*n*) of the original image in pixels. The input into our k-means clustering model
    will therefore be (*m* x *n*) observations across one independent variable—the
    pixel-intensity value. This can subsequently be represented as a single vector
    containing (*m* x *n*) numerical elements—that is, (0.0, 0.0, 0.0, 0.2, 0.3, 0.4,
    0.3, 0.4, 0.5 …).
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have derived feature vectors from our grayscale MRI image, our k-means
    clustering model will assign each pixel-intensity value to one of the *k* clusters
    when we train it on our MRI scan of a healthy human brain. In the context of the
    real world, these *k* clusters represent different substances in the brain, such
    as grey matter, white matter, fatty tissue, and cerebral fluids, which our model
    will partition based on color, a process called image segmentation. Once we have
    trained our k-means clustering model on a healthy human brain and identified *k*
    distinct clusters, we can then apply those defined clusters to MRI brain scans
    of other patients in an attempt to identify the presence and volume of suspicious
    growths.
  prefs: []
  type: TYPE_NORMAL
- en: K-means cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the challenges when using the k-means clustering algorithm is how to
    choose a suitable value for *k* upfront, especially if it is not obvious from
    the wider context of the use case in question. One method to help us is to plot
    a range of possible values of *k* on the *x *axis against the output of the k-means
    cost function on the *y *axis. The k-means cost function computes the total sum
    of the squared distance of every point to its corresponding cluster centroid for
    that value of *k*. The goal is to choose a suitable value of *k* that minimizes
    the cost function, but that is not so large that it increases the computational
    complexity of generating the clusters with only a small return in the reduction
    in cost. We will demonstrate how to generate this plot, and hence choose a suitable
    value of *k*, when we develop our Spark application for image segmentation in
    the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MRI brain scans that we will use for our k-means clustering model have been
    downloaded from **The Cancer Imaging Archive** (**TCIA**), a service that anonymizes
    and hosts a large archive of medical images of cancer for public download, and
    that may be found at [http://www.cancerimagingarchive.net/](http://www.cancerimagingarchive.net/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MRI scan of our healthy human brain may be found in the GitHub repository
    accompanying this book, and is called `mri-images-data`/`mri-healthy-brain.png`.
    The MRI scan of the test human brain is called `mri-images-data`/`mri-test-brain.png`.
    We will use both in the following Spark application when training our k-means
    clustering model and applying it to image segmentation. Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp05-01-kmeans-clustering.ipynb`.
    It can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open the grayscale MRI scan of the healthy human brain and take a look
    at it! We can achieve this using the `scikit-learn` machine learning library for
    Python as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The rendered image is illustrated in *Figure 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/641d2583-8db8-43a3-a049-0d373649e56e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Original MRI scan rendered using scikit-learn and matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to turn this image into a matrix of decimal point pixel-intensity
    values between 0 and 1\. Conveniently, this function is provided out of the box
    by `scikit-learn` using the `img_as_float` method, as shown in the following code.
    The dimensions of the resulting matrix are 256 x 256, implying an original image
    of 256 x 256 pixels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to flatten this matrix into a single vector of 256 x 256 elements,
    where each element represents a pixel-intensity value. This can be thought of
    as another matrix of dimensions 1 x (256 x 256) = 1 x 65536\. We can achieve this
    using the `numpy` Python library. First, we convert our original 256 x 256 matrix
    into a 2-dimensional `numpy` array. We then use `numpy`''s `ravel()` method to
    flatten this 2-dimensional array into a 1-dimensional array. Finally, we represent
    this 1-dimensional array as a specialized array, or matrix, of dimensions 1 x
    65536 using the `np.matrix` command, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our single vector, represented as a matrix of 1 x 65536 in
    dimension, we need to convert it into a Spark dataframe. To achieve this, we firstly
    transpose the matrix using numpy''s `reshape()` method so that it is 65536 x 1\.
    We then use the `createDataFrame()` method, exposed by Spark''s SQLContext, to
    create a Spark dataframe containing 65536 observations/rows and 1 column, representing
    65536 pixel-intensity values, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to generate `MLlib` feature vectors using `VectorAssembler`,
    a method that we have seen before. The `feature_columns` for `VectorAssembler`
    will simply be the sole pixel-intensity column from our Spark dataframe. The output
    of applying `VectorAssembler` to our Spark dataframe via the `transform()` method
    will be a new Spark dataframe called `mri_healthy_brain_features_df`, containing
    our 65536 `MLlib` feature vectors, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compute and plot the output of the k-means cost function for a range
    of *k* in order to determine the best value of *k* for this use case. We achieve
    this by using `MLlib`''s `KMeans()` estimator in the Spark dataframe containing
    our feature vectors, iterating over values of `k` in the `range(2, 20)`. We can
    then plot this using the `matplotlib` Python library, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the resulting plot, as illustrated in *Figure 5.5*, a value of *k* of
    either 5 or 6 would seem to be ideal. At these values, the k-means cost is minimized with
    little return gained thereafter, as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac60c7e-4c77-4411-b9b6-b4910e063ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: K-means cost function'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to train our k-means clustering model! Again, we will use
    `MLlib`''s `KMeans()` estimator, but this time using a defined value for *k* (5,
    in our case, as we decided in step 6). We will then apply it, via the `fit()`
    method, to the Spark dataframe containing our feature vectors and study the centroid
    values for each of our 5 resulting clusters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will apply our trained k-means model to the Spark dataframe containing
    our feature vectors so that we may assign each of the 65536 pixel-intensity values
    to one of the five clusters. The result will be a new Spark dataframe containing
    our feature vectors mapped to a prediction, where in this case the prediction
    is simply a value between 0 and 4, representing one of the five clusters. Then,
    we convert this new dataframe into a 256 x 256 matrix so that we can visualize the
    segmented image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting segmented image, rendered using `matplotlib`, is illustrated
    in *Figure 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cf03ae5-c26a-4983-84cf-70d8e7ce9315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Segmented MRI scan'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our five defined clusters, we can apply our trained k-means
    model to a *new* image in order to segment it, also based on the same five clusters.
    First, we load the new grayscale MRI brain scan belonging to the test patient
    using the `scikit-learn` library, as we did before using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have loaded the new MRI brain scan image, we need to follow the same
    process to convert it into a Spark dataframe containing feature vectors representing
    the pixel-intensity values of the new test image. We then apply the trained k-means
    model, via the `transform()` method, to this test Spark dataframe in order to
    assign its pixels to one of the five clusters. Finally, we convert the Spark dataframe
    containing the test image predictions in to a matrix so that we can visualize
    the segmented test image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting segmented image belonging to the test patient, again rendered
    using `matplotlib`, is illustrated in *Figure 5.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8354900b-5a2b-47cd-a4b9-9518b991bd69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Segmented MRI scan belonging to the test patient'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare the two segmented images side by side (as illustrated in *Figure
    5.8*), we will see that, as a result of our k-means clustering model, five different
    colors have been rendered representing the five different clusters. In turn, these
    five different clusters represent different substances in the human brain, partitioned
    by color. We will also see that, in the test MRI brain scan, one of the colors
    takes up a substantially larger area compared to the healthy MRI brain scan, pointing
    to a suspicious growth that may potentially be a tumor requiring further analysis,
    as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f612788f-acd9-49f0-969c-dcc79a3844d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Comparison of segmented MRI scans'
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are numerous real-world use cases where the number of features available
    that may potentially be used to train a model is very large. A common example
    is economic data, and using its constituent stock price data, employment data,
    banking data, industrial data, and housing data together to predict the **gross
    domestic product** (**GDP**). Such types of data are said to have high dimensionality.
    Though they offer numerous features that can be used to model a given use case,
    high-dimensional datasets increase the computational complexity of machine learning
    algorithms, and more importantly may also result in over fitting. Over fitting
    is one of the results of the **curse of dimensionality**, which formally describes
    the problem of analyzing data in high-dimensional spaces (which means that the
    data may contain many attributes, typically hundreds or even thousands of dimensions/features),
    but where that analysis no longer holds true in a lower-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Informally, it describes the value of additional dimensions at the cost of model
    performance. **Principal component analysis** (**PCA**)is an *unsupervised* technique
    used to preprocess and reduce the dimensionality of high-dimensional datasets
    while preserving the original structure and relationships inherent to the original
    dataset so that machine learning models can still learn from them and be used
    to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – movie recommendation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To better understand PCA, let's study a movie recommendation use case. Our aim
    is to build a system that can make personalized movie recommendations to users
    based on historic user-community movie ratings (note that user viewing history
    data could also be used for such a system, but this is beyond the scope of this
    example).
  prefs: []
  type: TYPE_NORMAL
- en: The historic user-community movie ratings data that we will use for our case
    study has been downloaded from GroupLens, a research laboratory based at the University
    of Minnesota that collects movie ratings and makes them available for public download
    at [https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/).
    For the purposes of this case study, we have transformed the individual *movies*
    and *ratings* datasets into a single pivot table where the 300 rows represent
    300 different users, and the 3,000 columns represent 3,000 different movies. This
    transformed, pipe-delimited dataset can be found in the GitHub repository accompanying
    this book, and is called `movie-ratings-data/user-movie-ratings.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample of the historic user-community movie ratings dataset that we will
    study looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Movie #1****Toy Story** | **Movie #2****Monsters Inc.** | **Movie #3****Saw**
    | **Movie #4****Ring** | **Movie #5****Hitch** |'
  prefs: []
  type: TYPE_TB
- en: '| **User #1** | 4 | 5 | 1 | NULL | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **User #2** | 5 | NULL | 1 | 1 | NULL |'
  prefs: []
  type: TYPE_TB
- en: '| **User #3** | 5 | 4 | 3 | NULL | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| **User #4** | 5 | 4 | 1 | 1 | NULL |'
  prefs: []
  type: TYPE_TB
- en: '| **User #5** | 5 | 5 | NULL | NULL | 3 |'
  prefs: []
  type: TYPE_TB
- en: In this case, each movie is a different feature (or dimension), and each different
    user is a different instance (or observation). This sample table, therefore, represents
    a dataset containing 5 features. However, our actual dataset contains 3,000 different
    movies, and therefore 3,000 features/dimensions. Furthermore, in a real-life representation,
    not all users would have rated all the movies, and so there will be a significant
    number of missing values. Such a dataset, and the matrix used to represent it,
    is described as *sparse*. These issues would pose a problem for machine learning
    algorithms, both in terms of computational complexity and the likelihood of over
    fitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, take a closer look at the previous sample table. It
    seems that users that rated Movie #1 highly (Toy Story) generally also rated Movie
    #2 highly (Monsters Inc.) as well. We could say, for example, that User #1 is
    *representative* of all fans of computer-animated children''s films, and so we
    could recommend to User #2 the other movies that User #1 has historically rated
    highly (this type of recommendation system where we use data from other users
    is called **collaborative filtering**). At a high level, this is what PCA does—it
    identifies *typical representations*, called **principal components**, within
    a high-dimensional dataset so that the dimensions of the original dataset can
    be reduced while preserving its underlying structure and still be representative
    in *lower* dimensions! These reduced datasets can then be fed into machine learning
    models to make predictions as normal, without the fear of any adverse effects
    from reducing the raw size of the original dataset. Our formal definition of PCA
    can therefore now be extended so that we can define PCA as the identification
    of a linear subspace of lower dimensionality where the largest variance in the
    original dataset is maintained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our historic user-community movie ratings dataset, instead of
    eliminating Movie #2 entirely, we could seek to create a new feature that combines
    Movie #1 and Movie #2 in some manner. Extending this concept, we can create new
    features where each new feature is based on all the old features, and thereafter
    order these new features by how well they help us in predicting user movie ratings.
    Once ordered, we can drop the least important ones, thereby resulting in a reduction
    in dimensionality. So how does PCA achieve this? It does so by performing the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we standardize the original high-dimensional dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we take the standardized data and compute a covariance matrix that provides
    a means to measure how all our features relate to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the covariance matrix, we then find its *eigenvectors* and corresponding
    *eigenvalues*. Eigenvectors represent the principal components and provide a means
    to understand the direction of the data. Corresponding eigenvalues represent how
    much variance there is in the data in that direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The eigenvectors are then sorted in descending order based on their corresponding
    eigenvalues, after which the top *k* eigenvectors are selected representing the
    most important representations found in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new matrix is then constructed with these *k* eigenvectors, thereby reducing
    the original *n*-dimensional dataset into reduced *k* dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Covariance matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In mathematics, **variance** refers to a measure of how spread out a dataset
    is, and is calculated by the sum of the squared distances of each data point, *x[i]*,
    from the mean *x-bar*, divided by the total number of data points, *N*. This is
    represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/787eabe6-db53-4369-9e63-43761b34b1d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Covariance** refers to a measure of how strong the correlation between two
    or more random variables is (in our case, our independent variables), and is calculated
    for variables *x* and *y* over *i* dimensions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21109685-3428-485a-8bfc-2c75ae5a0a1d.png)'
  prefs: []
  type: TYPE_IMG
- en: If the covariance is positive, this implies that the independent variables are
    positively correlated. If the covariance is negative, this implies that the independent
    variables are negatively correlated. Finally, a covariance of zero implies that
    there is no correlation between the independent variables. You may note that we
    described correlation in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, when discussing multivariate linear regression.
    At that time, we computed the one-way covariance mapping between the dependent
    variable to all its independent variables. Now we are computing the covariance
    between all variables.
  prefs: []
  type: TYPE_NORMAL
- en: A **covariance matrix** is a symmetric square matrix where the general element
    (*i*, *j*) is the covariance, *cov(i, j)*, between independent variables *i* and
    *j* (which is the same as the symmetric covariance between *j* and *i*). Note
    that the diagonal in a covariance matrix actually represents just the *variance*
    between those elements, by definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **x** | **y** | **z** |'
  prefs: []
  type: TYPE_TB
- en: '| **x** | var(x) | cov(x, y) | cov(x, z) |'
  prefs: []
  type: TYPE_TB
- en: '| **y** | cov(y, x) | var(y) | cov(y, z) |'
  prefs: []
  type: TYPE_TB
- en: '| **z** | cov(z, x) | cov(z, y) | var(z) |'
  prefs: []
  type: TYPE_TB
- en: Identity matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An identity matrix is a square matrix in which all the elements along the main
    diagonal are 1 and the remaining elements are 0\. Identity matrices are important
    for when we need to find all of the eigenvectors for a matrix. For example, a
    3 x 3 identity matrix looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15e61c67-15f7-474d-b0f3-f1b47599e4bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Eigenvectors and eigenvalues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In linear algebra, eigenvectors are a special set of vectors whose *direction*
    remains unchanged when a linear transformation is applied to it, and only changes
    by a *scalar* factor. In the context of dimensionality reduction, eigenvectors
    represent the principal components and provide a means to understand the direction
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a matrix, *A*, of dimensions (*m* x *n*). We can multiply *A* by a
    vector, *x* (of dimensions *n* x 1 by definition), which results in a new vector, *b*
    (of dimensions *m* x 1), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a360801-1a58-40ec-acce-35f7e62c63c7.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, ![](img/ec5d7f90-b023-49b0-bf87-81f10dce29d7.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in some cases, the resulting vector, *b*, is actually a scaled version
    of the original vector, *x*. We call this scalar factor *λ*, in which case the
    formula above can be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1492866-60c5-45cf-bd49-c9612a240212.png)'
  prefs: []
  type: TYPE_IMG
- en: We say that *λ* is an *eigenvalue* of matrix *A*, and *x* is an *eigenvector*
    associated with *λ*. In the context of dimensionality reduction, eigenvalues represent
    how much variance there is in the data in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find all the eigenvectors for a matrix, we need to solve the following
    equation for each eigenvalue, where *I* is an identity matrix with the same dimensions
    as matrix *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c433abbc-2457-45b1-8721-857bfe6dd7da.png)'
  prefs: []
  type: TYPE_IMG
- en: The process by which to solve this equation is beyond the scope of this book.
    However, to learn more about eigenvectors and eigenvalues, please visit [https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).
  prefs: []
  type: TYPE_NORMAL
- en: Once all of the eigenvectors for the covariance matrix are found, these are
    then sorted in descending order by their corresponding eigenvalues. Since eigenvalues
    represent the amount of variance in the data for that direction, the first eigenvector
    in the ordered list represents the principal component that captures the most
    variance in the original variables from the original dataset, and so on. For example,
    as illustrated in *Figure 5.9*, if we were to plot a dataset with two dimensions
    or features, the first eigenvector (which will be the first principal component
    in order of importance) would represent the direction of most variation between
    the two features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second eigenvector (the second principal component in order of importance)
    would represent the direction of second-most variation between the two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f81a5743-0cc2-4b65-8cc9-ff48c27cf675.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Principal components across two dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: 'To help choose the number of principal components, *k*, to select from the
    top of the ordered list of eigenvectors, we can plot the number of principal components
    on the *x *axis against the cumulative explained variance on the *y *axis, as
    illustrated in *Figure 5.10*, where the explained variance is the ratio between
    the variance of that principal component and the total variance (that is, the
    sum of all eigenvalues):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8097f834-824c-4dfb-9fc0-663de2082ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Cumulative explained variance'
  prefs: []
  type: TYPE_NORMAL
- en: Using *Figure 5.10* as an example, we would select around the first 300 principal
    components, as these describe the most variation within the data out of the 3,000
    in total. Finally, we construct a new matrix by projecting the original dataset
    into *k*-dimensional space represented by the eigenvectors selected, thereby reducing
    the dimensionality of the original dataset from 3,000 dimensions to 300 dimensions.
    This preprocessed and reduced dataset can then be used to train machine learning
    models as normal.
  prefs: []
  type: TYPE_NORMAL
- en: PCA in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now return to our transformed pipe-delimited user-community movie ratings
    dataset, `movie-ratings-data/user-movie-ratings.csv`, which contains ratings by
    300 users covering 3,000 movies. We will develop an application in Apache Spark
    that seeks to reduce the dimensionality of this dataset while preserving its structure
    using PCA. To do this, we will go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp05-02-principal-component-analysis.ipynb`.
    This can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the transformed, pipe-delimited user-community movie ratings
    dataset into a Spark dataframe using the following code. The resulting Spark dataframe
    will have 300 rows (representing the 300 different users) and 3,001 columns (representing
    the 3,000 different movies plus the user ID column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now generate `MLlib` feature vectors containing 3,000 elements (representing
    the 3,000 features) using `MLlib`''s `VectorAssembler`, as we have seen before.
    We can achieve this using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can reduce the dimensionality of the dataset using PCA, we first
    need to standardize the features that we described previously. This can be achieved
    using `MLlib`''s `StandardScaler` estimator and fitting it to the Spark dataframe
    containing our feature vectors, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert our scaled features into a `MLlib` `RowMatrix`instance. A
    `RowMatrix` is a distributed matrix with no index, where each row is a vector.
    We achieve this by converting our scaled features data frame into an RDD and mapping
    each row of the RDD to the corresponding scaled feature vector. We then pass this
    RDD to `MLlib`''s `RowMatrix()` (as shown in the following code), resulting in
    a matrix of standardized feature vectors of dimensions 300 x 3,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our standardized data in matrix form, we can easily compute
    the top *k* principal components by invoking the `computePrincipalComponents()`
    method exposed by `MLlib`''s `RowMatrix`. We can compute the top 300 principal
    components as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have identified the top 300 principal components, we can project
    the standardized user-community movie ratings data from 3,000 dimensions to a
    linear subspace of only 300 dimensions while preserving the largest variances
    from the original dataset. This is achieved by using matrix multiplication and
    multiplying the matrix containing the standardized data by the matrix containing
    the top 300 principal components, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The resulting matrix now has dimensions of 300 x 300, confirming the reduction
    in dimensionality from the original 3,000 to only 300! We can now use this projected
    matrix and its PCA feature vectors as the input into subsequent machine learning
    models as normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use `MLlib`''s `PCA()` estimator directly on the dataframe
    containing our standardized feature vectors to generate a new dataframe with a
    new column containing the PCA feature vectors, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, this new dataframe and its PCA feature vectors can then be used to train
    subsequent machine learning models as normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can extract the explained variance for each principal component
    from our PCA model by accessing its `explainedVariance` attribute as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The resulting vector (of 300 elements) shows that, in our example, the first
    eigenvector (and therefore the first principal component) in the ordered list
    of principal components explains 8.2% of the variance, the second explains 4%,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we have demonstrated how we can reduce the dimensionality
    of the user-community movie ratings dataset from 3,000 dimensions to only 300
    dimensions while preserving its structure using PCA. The resulting reduced dataset
    can then be used to train machine learning models as normal, such as a hierarchical
    clustering model for collaborative filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have trained and evaluated various unsupervised machine
    learning models and techniques in Apache Spark using a variety of real-world use
    cases, including partitioning the various substances found in the human brain
    using image segmentation and helping to develop a movie recommendation system
    by reducing the dimensionality of a high-dimensional user-community movie ratings
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will develop, test, and evaluate some common algorithms
    that are used in **natural language processing** (**NLP**) in an attempt to train
    machines to automatically analyze and understand human text and speech!
  prefs: []
  type: TYPE_NORMAL
