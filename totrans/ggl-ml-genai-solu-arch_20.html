<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer220" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-346"><a id="_idTextAnchor408" class="calibre6 pcalibre pcalibre1"/>17</h1>
<h1 id="_idParaDest-347" class="calibre5"><a id="_idTextAnchor409" class="calibre6 pcalibre pcalibre1"/>Generative AI on Google Cloud</h1>
<p class="calibre3">Now that we’ve covered many important topics in the world of generative AI, this chapter will explore generative AI specifically in Google Cloud. We will discuss Google’s proprietary models, such as the Gemini, PaLM, Codey, Imagen, and MedLM APIs, which are each designed for different kinds of tasks, from language processing to <span>medical analysis.</span></p>
<p class="calibre3">We will also review open source and third-party models available on Google Cloud via repositories such as Vertex AI Model Garden and <span>Hugging Face.</span></p>
<p class="calibre3">Continuing from our discussion of vector databases in the previous chapter, we will explore various vector database options in Google Cloud, as well as potential use cases for <span>each option.</span></p>
<p class="calibre3">Finally, we will use the information we’ll cover in this chapter to start building generative AI solutions in <span>Google Cloud.</span></p>
<p class="calibre3">Specifically, this chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Overview of generative AI in <span>Google Cloud</span></li>
<li class="calibre8">Detailed exploration of Google Cloud <span>generative AI</span></li>
<li class="calibre8">Implementing generative AI solutions in <span>Google Cloud</span></li>
</ul>
<p class="calibre3">Let’s begin with a high-level overview of generative AI in <span>Google Cloud.</span></p>
<h1 id="_idParaDest-348" class="calibre5"><a id="_idTextAnchor410" class="calibre6 pcalibre pcalibre1"/>Overview of generative AI in Google Cloud</h1>
<p class="calibre3">The <a id="_idIndexMarker2019" class="calibre6 pcalibre pcalibre1"/>pace of generative AI development in Google Cloud is nothing short of astonishing. It seems like almost every day, a new model version, service, or feature is announced. In this section, I’ll introduce the various models and products at a high level, which will set the stage for deeper dives later in <span>this chapter.</span></p>
<p class="calibre3">Overall, the <a id="_idIndexMarker2020" class="calibre6 pcalibre pcalibre1"/>models and products are categorized by modality, such as text, code, image, and video. Let’s begin our journey with a discussion of Google’s various generative <span>AI models.</span></p>
<h2 id="_idParaDest-349" class="calibre9"><a id="_idTextAnchor411" class="calibre6 pcalibre pcalibre1"/>Google’s generative AI models</h2>
<p class="calibre3">Google has created many generative<a id="_idIndexMarker2021" class="calibre6 pcalibre pcalibre1"/> AI models over the past few years, with a dramatic acceleration of new models and model versions launched in the past year. This section discusses the Google first-party foundation models that are currently available on Google Cloud in early 2024, starting with the quite <span>famous “Gemini.”</span></p>
<h3 class="calibre11">Gemini API</h3>
<p class="calibre3">As of early 2024, Gemini is <a id="_idIndexMarker2022" class="calibre6 pcalibre pcalibre1"/>Google’s largest and most capable series of AI models. It<a id="_idIndexMarker2023" class="calibre6 pcalibre pcalibre1"/> comes in three <span>model families:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Gemini Ultra</strong>: This <a id="_idIndexMarker2024" class="calibre6 pcalibre pcalibre1"/>is the biggest model in the Gemini family and is intended for highly complex tasks. At the time of its announced launch in December 2023, it exceeded current state-of-the-art results on 30 of the 32 widely used academic benchmarks in <span>LLM research.</span></li>
<li class="calibre8"><strong class="bold">Gemini Pro</strong>: While this<a id="_idIndexMarker2025" class="calibre6 pcalibre pcalibre1"/> is not the largest model, it is considered Google’s best model for scaling across a wide range of tasks. Its most recent version at the time of writing this in early 2024 is version 1.5, which introduced a one-million-token context window – the largest context window of any model in the industry at the time of its launch. This enables us to send very large amounts of data in a single prompt, opening new use cases and solving significant limitations of <span>earlier LLMs.</span></li>
<li class="calibre8"><strong class="bold">Gemini Nano</strong>: This is <a id="_idIndexMarker2026" class="calibre6 pcalibre pcalibre1"/>Google’s most efficient Gemini model family and is intended to be loaded into the memory of a single device for performing on-device tasks. It was originally launched with two variants of slightly different sizes, Nano-1 <span>and Nano-2.</span></li>
</ul>
<p class="calibre3">There are also <a id="_idIndexMarker2027" class="calibre6 pcalibre pcalibre1"/>multimodal variants – for example, <strong class="bold">Gemini Ultra Vision</strong> and <strong class="bold">Gemini Pro Vision</strong> – that <a id="_idIndexMarker2028" class="calibre6 pcalibre pcalibre1"/>are trained on multiple kinds of input data, including text, code, image, audio, and video. This means that we can mix the modalities in our interactions with Gemini, such as sending an image in our prompt, asking<a id="_idIndexMarker2029" class="calibre6 pcalibre pcalibre1"/> questions (via text) about the contents of the photo, and receiving <span>textual outputs.</span></p>
<h3 class="calibre11">PaLM API models</h3>
<p class="calibre3">PaLM is Google’s <strong class="bold">Pathways Language Model</strong>, based on the “Pathways” system created by Google to<a id="_idIndexMarker2030" class="calibre6 pcalibre pcalibre1"/> build<a id="_idIndexMarker2031" class="calibre6 pcalibre pcalibre1"/> models that could perform more than one task (as opposed to traditional, single-purpose models). The current suite of<a id="_idIndexMarker2032" class="calibre6 pcalibre pcalibre1"/> PaLM models is based on the PaLM 2 release, and there are multiple offerings within this suite, such as PaLM 2 for Text, PaLM 2 for Chat, and Embeddings for Text, each with variants I’ll <span>describe next.</span></p>
<h4 class="calibre20">PaLM 2 for Text</h4>
<p class="calibre3">As the name suggests, this family of<a id="_idIndexMarker2033" class="calibre6 pcalibre pcalibre1"/> models is designed for text use cases. There are currently three variants of this <span>model family:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">text-bison</strong>: This can<a id="_idIndexMarker2034" class="calibre6 pcalibre pcalibre1"/> be used for multiple text-based language use<a id="_idIndexMarker2035" class="calibre6 pcalibre pcalibre1"/> cases, such as summarization, classification, and extraction. It can handle a maximum input of 8,192 tokens and a maximum output of <span>1,024 tokens.</span></li>
<li class="calibre8"><strong class="bold">text-unicorn</strong>: The most <a id="_idIndexMarker2036" class="calibre6 pcalibre pcalibre1"/>advanced model in the PaLM family for <a id="_idIndexMarker2037" class="calibre6 pcalibre pcalibre1"/>complex natural language tasks. While it’s a more advanced model than text-bison, it has the same input and output <span>token limits.</span></li>
<li class="calibre8"><strong class="bold">text-bison-32k</strong>: This <a id="_idIndexMarker2038" class="calibre6 pcalibre pcalibre1"/>variant is similar to the aforementioned<a id="_idIndexMarker2039" class="calibre6 pcalibre pcalibre1"/> text-bison, but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768 tokens (combined input <span>and output).</span></li>
</ul>
<p class="calibre3">When we send a prompt to <strong class="bold">PaLM 2 for Text</strong> models, the model generates text as a response, and each interaction is independent. However, we can build external logic to chain multiple interactions. Next, we’ll discuss a set of Google Cloud products that are designed for interactive <span>prompt integrations.</span></p>
<h4 class="calibre20">PaLM 2 for Chat</h4>
<p class="calibre3">With <strong class="bold">PaLM 2 for Chat</strong>, we can <a id="_idIndexMarker2040" class="calibre6 pcalibre pcalibre1"/>engage in a <strong class="bold">multi-turn</strong> conversation<a id="_idIndexMarker2041" class="calibre6 pcalibre pcalibre1"/> with the PaLM models. This product family consists of <span>two variants:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">chat-bison</strong>: This <a id="_idIndexMarker2042" class="calibre6 pcalibre pcalibre1"/>variant is designed for interactive conversation<a id="_idIndexMarker2043" class="calibre6 pcalibre pcalibre1"/> use cases. It can handle a maximum input of 8,192 tokens and a maximum output of <span>2,048 tokens.</span></li>
<li class="calibre8"><strong class="bold">chat-bison-32k</strong>: This<a id="_idIndexMarker2044" class="calibre6 pcalibre pcalibre1"/> variant is similar to the aforementioned <a id="_idIndexMarker2045" class="calibre6 pcalibre pcalibre1"/>chat-bison, but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768 tokens (combined input <span>and output).</span></li>
</ul>
<p class="calibre3">Given the nature of these product variants, they are well suited for use cases in which we want to maintain context across multiple prompts to provide a natural, human-like conversation experience. We can also use PaLM models to generate text embeddings. We’ll take a look at those <span>models next.</span></p>
<h4 class="calibre20">PaLM 2 embedding models</h4>
<p class="calibre3">We’ve already generically discussed <a id="_idIndexMarker2046" class="calibre6 pcalibre pcalibre1"/>models that can be used to create embeddings. In this section, we’ll look at Google’s PaLM 2 embedding models, which come in <span>two variants:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">textembedding-gecko</strong>: The name “gecko” refers to smaller models. Considering that we are <a id="_idIndexMarker2047" class="calibre6 pcalibre pcalibre1"/>using these models to create <a id="_idIndexMarker2048" class="calibre6 pcalibre pcalibre1"/>embeddings for text, we generally don’t need a very large model for that use case, so these smaller and more efficient models make more sense in this context. This particular model focuses on words in the <span>English language.</span></li>
<li class="calibre8"><strong class="bold">textembedding-gecko-multilingual</strong>: This variant is similar to the aforementioned<a id="_idIndexMarker2049" class="calibre6 pcalibre pcalibre1"/> textembedding-gecko, but<a id="_idIndexMarker2050" class="calibre6 pcalibre pcalibre1"/> this model can support over <span>100 languages.</span></li>
</ul>
<p class="calibre3">So far, all of the PaLM 2 models I’ve described focus on natural human language. Next, we’ll discuss models that are designed for use cases involving computer <span>programming languages.</span></p>
<h3 class="calibre11">Codey API models</h3>
<p class="calibre3">Codey<a id="_idIndexMarker2051" class="calibre6 pcalibre pcalibre1"/> is a fun name for models<a id="_idIndexMarker2052" class="calibre6 pcalibre pcalibre1"/> that are designed to implement code use <a id="_idIndexMarker2053" class="calibre6 pcalibre pcalibre1"/>cases. Codey models can generate code based on natural language requests and can even convert code from one programming language into another. Like the text-based PaLM 2 models, Codey models come in different variants based on whether we simply want the models to generate distinct responses to independent prompts, or we want to implement an interactive, chat-based use case. Let’s dive into each in <span>more detail.</span></p>
<h4 class="calibre20">Codey for Code Generation</h4>
<p class="calibre3">This set of offerings is designed for<a id="_idIndexMarker2054" class="calibre6 pcalibre pcalibre1"/> independent prompts (although, again, we can build logic to chain them together if we wish), and they come in <span>two variants:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">code-bison</strong>: This <a id="_idIndexMarker2055" class="calibre6 pcalibre pcalibre1"/>model can generate code based on a<a id="_idIndexMarker2056" class="calibre6 pcalibre pcalibre1"/> natural language prompt – for example, <strong class="source-inline">write a Java function to fetch the customer_id and account_balance fields from the 'accounts' table in the 'customer' MySQL database.</strong> This variant can handle a maximum input of 6,144 tokens and a maximum output of <span>1,024 tokens.</span></li>
<li class="calibre8"><strong class="bold">code-bison-32k</strong>: This variant is similar to the aforementioned code-bison, but it can handle a <a id="_idIndexMarker2057" class="calibre6 pcalibre pcalibre1"/>maximum<a id="_idIndexMarker2058" class="calibre6 pcalibre pcalibre1"/> output of 8,192 tokens and an overall maximum of 32,768 tokens (combined input <span>and output).</span></li>
</ul>
<p class="calibre3">Next, let’s discuss model variants for interactive coding <span>use cases.</span></p>
<h4 class="calibre20">Codey for Code Chat</h4>
<p class="calibre3">This product family enables us to engage<a id="_idIndexMarker2059" class="calibre6 pcalibre pcalibre1"/> in multi-turn conversations regarding code. These models also come in <span>two variants:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">codechat-bison</strong>: This<a id="_idIndexMarker2060" class="calibre6 pcalibre pcalibre1"/> model can be used to implement <a id="_idIndexMarker2061" class="calibre6 pcalibre pcalibre1"/>chatbot conversations involving code-related questions. Like the code-bison model, it can handle a maximum input of 6,144 tokens and a maximum output of <span>1,024 tokens.</span></li>
<li class="calibre8"><strong class="bold">codechat-bison-32k</strong>: This <a id="_idIndexMarker2062" class="calibre6 pcalibre pcalibre1"/>is similar to the aforementioned <a id="_idIndexMarker2063" class="calibre6 pcalibre pcalibre1"/>codechat-bison, but it can handle a maximum output of 8,192 tokens and an overall maximum of 32,768 tokens (combined input <span>and output).</span></li>
</ul>
<p class="calibre3">In addition to the Code Generation and Code Chat models, there is also a version of Codey that’s used for code completion, which I’ll <span>describe next.</span></p>
<h4 class="calibre20">Codey for Code Completion</h4>
<p class="calibre3">The code completion variant of <a id="_idIndexMarker2064" class="calibre6 pcalibre pcalibre1"/>Codey is intended to be used<a id="_idIndexMarker2065" class="calibre6 pcalibre pcalibre1"/> in <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>). There is one variant of this model, called <strong class="bold">code-gecko</strong>, which<a id="_idIndexMarker2066" class="calibre6 pcalibre pcalibre1"/> is designed to act as a helpful coding assistant to help developers write effective code in real time. This means that it can suggest snippets of code within the IDE as developers are writing their code. We’re all familiar with predictive text and auto-correct features on our mobile phones. This is a similar concept, but for code, and it helps developers get their work done more quickly <span>and effectively.</span></p>
<p class="calibre3">Now, let’s switch our discussion to another <span>modality: images.</span></p>
<h3 class="calibre11">Imagen API models</h3>
<p class="calibre3">Imagen<a id="_idIndexMarker2067" class="calibre6 pcalibre pcalibre1"/> is the name of the<a id="_idIndexMarker2068" class="calibre6 pcalibre pcalibre1"/> suite of <a id="_idIndexMarker2069" class="calibre6 pcalibre pcalibre1"/>Google models that are designed for working with images, where each model in the suite can be used for different types of image-related use cases, such as <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Image generation</strong>: The <strong class="bold">imagegeneration</strong> model, as its name suggests, can be used to generate <a id="_idIndexMarker2070" class="calibre6 pcalibre pcalibre1"/>images based <a id="_idIndexMarker2071" class="calibre6 pcalibre pcalibre1"/>on natural language prompts. For example, the image in <span><em class="italic">Figure 17</em></span><em class="italic">.1</em> was generated by the prompt, “an impressionistic painting of a woman fishing late on a summer evening in a small boat on a placid river with reeds and trees in the background.” We can also interactively edit our pictures to refine them based on our <span>desired outcomes:</span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer208">
<img alt="Figure 17.1: Image generated by Imagen" src="image/B18143_17_1.jpg" class="calibre194"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.1: Image generated by Imagen</p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Image captioning</strong>: We can send an image in our prompt to the <strong class="bold">imagetext</strong> model, and it will <a id="_idIndexMarker2072" class="calibre6 pcalibre pcalibre1"/>generate<a id="_idIndexMarker2073" class="calibre6 pcalibre pcalibre1"/> descriptive text based on the contents of that image. This can be useful for many kinds of business use cases, such as generating product descriptions based on images in a product catalog on a retail website, generating captions for images in news articles, or generating “alt text” for images <span>on websites.</span></li>
<li class="calibre8"><strong class="bold">Visual Question Answering (VQA)</strong>: In <a id="_idIndexMarker2074" class="calibre6 pcalibre pcalibre1"/>addition to generating captions for our pictures, the <strong class="bold">imagetext</strong> model also allows us to interactively ask questions about the <a id="_idIndexMarker2075" class="calibre6 pcalibre pcalibre1"/>contents <span>of images.</span></li>
<li class="calibre8"><strong class="bold">Multimodal embeddings</strong>: While we can use the <strong class="bold">textembedding-gecko</strong> models<a id="_idIndexMarker2076" class="calibre6 pcalibre pcalibre1"/> to generate text embeddings, the <strong class="bold">multimodalembedding</strong> model<a id="_idIndexMarker2077" class="calibre6 pcalibre pcalibre1"/> can generate embeddings for both images <span>and text.</span></li>
</ul>
<p class="calibre3">In addition to the general-purpose models I’ve just described, Google also provides models that have been designed to focus specifically on medical use cases. The next section briefly describes <span>those models.</span></p>
<h3 class="calibre11">MedLM API models</h3>
<p class="calibre3">There are <a id="_idIndexMarker2078" class="calibre6 pcalibre pcalibre1"/>two model variants in <a id="_idIndexMarker2079" class="calibre6 pcalibre pcalibre1"/>this suite of models, <strong class="bold">medlm-medium</strong> and <strong class="bold">medlm-large</strong>, both of which are HIPAA-compliant and can be<a id="_idIndexMarker2080" class="calibre6 pcalibre pcalibre1"/> used <a id="_idIndexMarker2081" class="calibre6 pcalibre pcalibre1"/>for summarizing medical documents and helping healthcare practitioners with <span>medical questions.</span></p>
<p class="calibre3">We can expect many more models and variants to be added by Google continuously to support an ever-growing plethora of use cases. In addition to Google’s first-party models described in the previous subsections, Google Cloud supports and embraces open source development, something we will <span>explore next.</span></p>
<h2 id="_idParaDest-350" class="calibre9"><a id="_idTextAnchor412" class="calibre6 pcalibre pcalibre1"/>Open source and third-party generative AI models on Google Cloud</h2>
<p class="calibre3">Google is a well-established<a id="_idIndexMarker2082" class="calibre6 pcalibre pcalibre1"/> contributor to open source communities, having created and contributed critically important inventions such as Android, Angular, Apache Beam, Go (programming language), Kubernetes, TensorFlow, and many others. In this section, we will explore Google’s open source generative AI models, as well as third-party (both open source and proprietary) generative AI models that we can easily use on Google Cloud. We’ll begin by discussing the Google Cloud Vertex AI Model Garden, which enables us to access <span>such models.</span></p>
<h3 class="calibre11">Google Cloud Vertex AI Model Garden</h3>
<p class="calibre3">Vertex AI Model Garden<a id="_idIndexMarker2083" class="calibre6 pcalibre pcalibre1"/> is a <a id="_idIndexMarker2084" class="calibre6 pcalibre pcalibre1"/>centralized library that provides a “one-stop shop” that makes it easy for us to find, customize, and deploy pre-trained <span>AI models.</span></p>
<p class="calibre3">Within Model Garden, we can access foundation models, such as Gemini and PaLM 2, as well as open source models, such as Gemma (described shortly) and Llama 2, and third-party models, such as Anthropic’s Claude 3. We can also access task-specific models that cater to use cases such as content classification and sentiment analysis, among <span>many more.</span></p>
<p class="calibre3">Vertex AI Model Garden is closely integrated with the rest of the Google Cloud and Vertex AI ecosystem, which enables us to easily build enterprise-grade solutions by providing access to all of the data processing and solution-building tools we’ve discussed in previous chapters, as well as many more that are beyond the scope of <span>this book.</span></p>
<p class="calibre3">In addition to making models accessible via Vertex AI Model Garden, Google Cloud has established a strategic partnership with Hugging Face, which I’ll <span>describe next.</span></p>
<h3 class="calibre11">Hugging Face</h3>
<p class="calibre3">Hugging Face<a id="_idIndexMarker2085" class="calibre6 pcalibre pcalibre1"/> is both a <a id="_idIndexMarker2086" class="calibre6 pcalibre pcalibre1"/>company and a community that makes it easy to share ML models, tools, and datasets. While it started as a chatbot company, it rapidly grew in popularity due to its Model Hub and Transformer libraries, which amassed a broad community of contributors and made it easy to access and use large, pre-trained models. With the relatively new direct partnership between Google Cloud and Hugging Face, Google Cloud customers can now easily avail of the immense variety of models, tools, and datasets from within their Google Cloud environments running on services such as Vertex AI <a id="_idIndexMarker2087" class="calibre6 pcalibre pcalibre1"/>and <strong class="bold">Google Kubernetes </strong><span><strong class="bold">Engine</strong></span><span> (</span><span><strong class="bold">GKE</strong></span><span>).</span></p>
<p class="calibre3">Considering that this is a book about Google ML and generative AI, the descriptive discussions here will focus on Google’s models; I’ll refer you to the external documentation for the non-Google open source and third-party models. With that in mind, in the next section, I will introduce Google’s suite of open source models, <span>named “Gemma.”</span></p>
<h3 class="calibre11">Gemma</h3>
<p class="calibre3">Gemma is a family<a id="_idIndexMarker2088" class="calibre6 pcalibre pcalibre1"/> of state-of-the-art and open source, lightweight models from Google (DOI citation: 10.34740/KAGGLE/M/3301) that were built from the same technology and research <a id="_idIndexMarker2089" class="calibre6 pcalibre pcalibre1"/>that was used to create the Gemini models. These are decoder-only, text-to-text LLMs with pre-trained, instruction-tuned variants and open weights, which were trained on data from a wide variety of sources, including web pages, documents, code, and mathematical texts. They are suitable for many different kinds of text-generation use cases, such as summarization or question answering, and their open weights mean that they can be customized for specific use cases. Also, since they are relatively lightweight, they don’t require specialized or large-scale computing resources to run, so we can use them in many environments, including a local laptop, for example, which makes it easy for developers to start experimenting <span>with them.</span></p>
<p class="callout-heading">Fun fact</p>
<p class="callout">Gemma happens to be my sister’s name, so I was pleasantly surprised when that name was chosen for this suite of models, although I had no involvement whatsoever in the naming of <span>these models.</span></p>
<p class="calibre3">We will look at the Gemma models in more detail later. In <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, I described the importance of embeddings and vector databases in generative AI. Now that we’ve explored the various generative AI models in Google Cloud, let’s discuss vector databases in <span>Google Cloud.</span></p>
<h2 id="_idParaDest-351" class="calibre9"><a id="_idTextAnchor413" class="calibre6 pcalibre pcalibre1"/>Vector databases in Google Cloud</h2>
<p class="calibre3">This section provides a brief introduction to <a id="_idIndexMarker2090" class="calibre6 pcalibre pcalibre1"/>vector databases in Google Cloud, and we’ll dive into them in more detail later in this chapter. I’ll begin this section with a simple question: “<em class="italic">Which Google Cloud database service provides vector database functionality?</em>” Quite simply, the answer is, “<em class="italic">Pretty much all of them, with just a </em><span><em class="italic">few exceptions!</em></span><span>”</span></p>
<p class="calibre3">The <a id="_idIndexMarker2091" class="calibre6 pcalibre pcalibre1"/>next question, then, might be, “<em class="italic">Which one should I use?</em>” The answer to that question is a bit more nuanced. We’ll explore the options in more detail in this chapter, starting with a couple of easy decisions. Firstly, the reason almost all Google Cloud database services provide vector database functionality is that Google wants to make it as easy as possible for you to access this functionality. If you already use AlloyDB to manage your application’s operational data, you can easily go ahead and use AlloyDB AI for your vector database needs. If you use BigQuery for your analytical needs, go ahead and use BigQuery Vector Search, although bear in mind that BigQuery is designed primarily for large-scale data processing rather than for optimizing latency. If you have strict low-latency requirements, your workload may be better suited for one of the other Google Cloud options we’ll <span>cover later.</span></p>
<p class="calibre3">If you’re starting fresh in Google Cloud and want to set up a vector database, begin your journey by exploring the offerings under Vertex AI. For a fully managed platform that enables developers to build Google-quality search experiences for websites, structured and unstructured data, or to integrate your applications with generative AI and search functionality that’s grounded in your enterprise data, start with Vertex AI Search and Conversation. If you want to create and store your vectors and rapidly search billions of semantically related items, look into Vertex AI <span>Vector Search.</span></p>
<p class="calibre3">In the next section, we will dive deeper into Google Cloud’s generative AI offerings, starting with a hands-on exploration of the models, and then covering the various vector database offerings in <span>more detail.</span></p>
<h1 id="_idParaDest-352" class="calibre5"><a id="_idTextAnchor414" class="calibre6 pcalibre pcalibre1"/>A detailed exploration of Google Cloud generative AI</h1>
<p class="calibre3">In this section, we’ll start interacting with Google Cloud’s generative AI offerings. To set the stage, I’ll begin by briefly introducing Google Cloud Vertex <span>AI Studio.</span></p>
<h2 id="_idParaDest-353" class="calibre9"><a id="_idTextAnchor415" class="calibre6 pcalibre pcalibre1"/>Google Cloud Vertex AI Studio</h2>
<p class="calibre3">Vertex AI Studio can be <a id="_idIndexMarker2092" class="calibre6 pcalibre pcalibre1"/>accessed within the Google Cloud console UI, and it provides an interface to easily start using all of the generative AI models I described in the <span>previous section.</span></p>
<p class="calibre3">To access the Google Cloud Vertex AI Studio UI, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the Google Cloud services menu and choose <span><strong class="bold">Vertex AI</strong></span><span>.</span></li>
<li class="calibre8">Under the <strong class="bold">Get started with Vertex AI</strong> section, click <strong class="bold">ENABLE ALL RECOMMENDED API</strong>, as <a id="_idIndexMarker2093" class="calibre6 pcalibre pcalibre1"/>depicted in <span><em class="italic">Figure 17</em></span><span><em class="italic">.2</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer209">
<img alt="Figure 17.2: Enabling the recommended APIs" src="image/B18143_17_2.jpg" class="calibre195"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.2: Enabling the recommended APIs</p>
<ol class="calibre7">
<li value="3" class="calibre8">Give it a few minutes for the APIs to enable. Once the APIs have been enabled, you can start using them. You can interact with each of the models I described in the previous section by clicking on each of the modalities – such as <strong class="bold">Language</strong>, <strong class="bold">Vision</strong>, <strong class="bold">Speech</strong>, and <strong class="bold">Multimodal</strong> – in the menu on the left-hand side of <span>the screen.</span></li>
<li class="calibre8">For example, if we click on <strong class="bold">Language</strong>, a screen similar to what’s shown in <span><em class="italic">Figure 17</em></span><em class="italic">.3</em> will <span>be displayed:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer210">
<img alt="Figure 17.3: The Language section" src="image/B18143_17_3.jpg" class="calibre196"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.3: The Language section</p>
<ol class="calibre7">
<li value="5" class="calibre8">From here, we can <a id="_idIndexMarker2094" class="calibre6 pcalibre pcalibre1"/>click on the various links and buttons to try out the different models. For example, clicking the <strong class="bold">TEXT CHAT</strong> button will present us with a chat interface into which we can type our prompts, as shown in <span><em class="italic">Figure 17</em></span><span><em class="italic">.4</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer211">
<img alt="Figure 17.4: Text chat" src="image/B18143_17_4.jpg" class="calibre197"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.4: Text chat</p>
<p class="calibre3">The parameters we can <a id="_idIndexMarker2095" class="calibre6 pcalibre pcalibre1"/>configure can be found on the right-hand side of the screen. Let’s take a <span>closer look:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Model</strong>: The model to which we want to send <span>a prompt.</span></li>
<li class="calibre8"><strong class="bold">Region</strong>: The region in which we want to run <span>our prompt.</span></li>
<li class="calibre8"><strong class="bold">Temperature</strong>: This parameter configures the level of creativity or randomness in the model’s responses. This can be seen as the level of imagination we want the model to use when generating responses. If we consider the use case of an LLM predicting the next word in a sequence, the temperature parameter influences the overall probability distribution of the next word, such that a higher temperature can result in more creative outputs, while a lower temperature guides the model to provide more conservative and predictable outcomes. If we want the model to create imaginative art or text, for example, we could configure a high temperature, whereas if we want more formal, factual responses, we would configure a <span>low temperature.</span></li>
<li class="calibre8"><strong class="bold">Output token limit</strong>: The maximum number of tokens we want the model to generate in <span>its response.</span></li>
</ul>
<p class="calibre3">We can click <a id="_idIndexMarker2096" class="calibre6 pcalibre pcalibre1"/>on the <strong class="bold">Advanced</strong> section at the bottom of the screen to expand it. Here, we will see additional parameters, such as <strong class="bold">Max responses</strong>, which configures the maximum number of example responses we want the model to return (note that this parameter is only valid for some types of interactions – for example, it is not applicable to chat interactions because chat will always provide one response in each turn of the conversation), as well as <strong class="bold">Top-K</strong> and <strong class="bold">Top-P</strong>. Like <strong class="bold">Temperature</strong>, <strong class="bold">Top-K</strong> and <strong class="bold">Top-P</strong> can be used to control the creativity and randomness of the model’s generated outputs, but they work in slightly different ways. Consider the case of an LLM selecting the next word (or token) in a sequence when generating a response. It does this by predicting the probability of the next word being the most likely to occur in the given sequence. If the LLM always only selects the word with the highest probability, then it does not allow for much flexibility when generating responses. However, we can use <strong class="bold">Top-K</strong> and <strong class="bold">Top-P</strong> to add a level of flexibility by understanding how they work. So, let’s take a <span>closer look:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Top-K</strong>: This uses a probability distribution to predict the probabilities for all possible next words based on the current prompt and context. K is the number of top-probability words that we want the model to select from. For example, if the value of K is 3, then the model will pick the next word from the top three most likely words (that is, the top three words with the highest probabilities). If the value of K is 1, then the model will pick only the most likely (that is, highest probability) word. This is a specific case we<a id="_idIndexMarker2097" class="calibre6 pcalibre pcalibre1"/> refer to as <span><strong class="bold">greedy selection</strong></span><span>.</span></li>
<li class="calibre8"><strong class="bold">Top-P</strong>: This is a bit more complex to explain, but only slightly. Rather than picking a specific number of the highest-probability words to choose from, the model will calculate the <strong class="bold">cumulative</strong> probability<a id="_idIndexMarker2098" class="calibre6 pcalibre pcalibre1"/> for all possible next words – that is, it will add up the probabilities of the most likely words until the sum of the probabilities reaches the specified threshold (P), at which point it will randomly select a word from the pool of words that fall under the <span>probability threshold.</span></li>
</ul>
<p class="calibre3">In the case <a id="_idIndexMarker2099" class="calibre6 pcalibre pcalibre1"/>of both <strong class="bold">Top-K</strong> and <strong class="bold">Top-P</strong>, lower values guide the outputs toward conservative responses, while higher values allow some wiggle room for more creative responses, similar to (and in conjunction with) the <span><strong class="bold">Temperature</strong></span><span> parameter.</span></p>
<p class="calibre3">In the <strong class="bold">Advanced</strong> section, for some models, we can also enable <strong class="bold">Grounding</strong>, in which case we can ground our responses based on data we have stored in Vertex AI Search and Conversation. We also have the option to stream responses, which refers to printing responses as they <span>are generated.</span></p>
<p class="calibre3">I encourage you to explore the different modalities and types of models in Vertex AI Studio. If you need help thinking of creative prompts to write, you can ask one of the text-based models to suggest some examples for you <span>to use!</span></p>
<p class="calibre3">In addition to accessing the various models through the Vertex AI Studio UI, you can also interact with the models programmatically via the REST APIs <span>and SDK.</span></p>
<p class="calibre3">Next, let’s explore the vector database options available in <span>Google Cloud.</span></p>
<h2 id="_idParaDest-354" class="calibre9"><a id="_idTextAnchor416" class="calibre6 pcalibre pcalibre1"/>A detailed exploration of Google Cloud vector database options</h2>
<p class="calibre3">Earlier in this chapter, I mentioned that almost all of Google Cloud’s database services provide vector database support. In this section, we’ll take a look at each one in more detail, starting with Vertex AI Search <span>and Conversation.</span></p>
<h3 class="calibre11">Vertex AI Search and Conversation</h3>
<p class="calibre3">While the main purpose of the<a id="_idIndexMarker2100" class="calibre6 pcalibre pcalibre1"/> Vertex AI Search and<a id="_idIndexMarker2101" class="calibre6 pcalibre pcalibre1"/> Conversation product suite, as its name suggests, is to build search and conversation applications, we can also use it as a vector database for<a id="_idIndexMarker2102" class="calibre6 pcalibre pcalibre1"/> implementing <strong class="bold">retrieval augmented generation</strong> (<strong class="bold">RAG</strong>) solutions. As we discussed earlier in this chapter, if you’re getting started with RAG on Google Cloud, unless you have a specific need to control the chunking, embedding, and indexing processes, or you have a specific operational need to link all of your data with another Google Cloud database service, Vertex AI Search and Conversation should be your first choice to consider. This is because it performs all of the chunking, embedding, and indexing processes for you, and it abstracts away all of those steps behind a simple and convenient orchestration interface, saving you a lot of time and effort. You can also use data connectors to ingest data from third-party applications, such as <a id="_idIndexMarker2103" class="calibre6 pcalibre pcalibre1"/>JIRA, Salesforce, <span>and</span><span><a id="_idIndexMarker2104" class="calibre6 pcalibre pcalibre1"/></span><span> Confluence.</span></p>
<p class="calibre3">If you need to specifically control the chunking, embedding, and indexing processes, you can choose from other Google Cloud database offerings, as <span>described next.</span></p>
<h3 class="calibre11">Vertex AI Vector Search</h3>
<p class="calibre3">As we’ve seen throughout this book, Vertex AI <a id="_idIndexMarker2105" class="calibre6 pcalibre pcalibre1"/>provides an entire ecosystem of tools and<a id="_idIndexMarker2106" class="calibre6 pcalibre pcalibre1"/> services for pretty much everything we could wish to do in the realm of ML and artificial intelligence. So, it makes sense that it would include a vector database, and that vector database is called Vertex AI Vector Search. It enables us to store and search through massive collections of embeddings to find the most similar or relevant items with high speed and <span>low latency.</span></p>
<p class="calibre3">Although we need to create the chunks, embeddings, and indexes (unlike when using Vertex AI Search and Conversation), it still provides a fully managed service in that we don’t need to worry about infrastructure or scaling because Vertex AI handles all of that for us. The fact that we need to create the chunks, embeddings, and indexes means that we can implement more granular control over those processes if needed, such as the models used to create our embeddings (note that this is also true for all of the other vector database options that we will discuss in the remainder of <span>this chapter).</span></p>
<p class="calibre3">Vertex AI Vector Search also integrates closely with the rest of the Vertex AI and Google Cloud ecosystem to serve as part of larger orchestration solutions requiring the use of embeddings. On the topic of the broader ecosystem, let’s consider additional Google Cloud vector databases beyond Vertex AI, starting <span>with BigQuery.</span></p>
<h3 class="calibre11">BigQuery Vector Search</h3>
<p class="calibre3">BigQuery Vector Search is a <a id="_idIndexMarker2107" class="calibre6 pcalibre pcalibre1"/>feature within Google Cloud BigQuery that allows us to <a id="_idIndexMarker2108" class="calibre6 pcalibre pcalibre1"/>store and find embeddings. We can send a query embedding to BigQuery by using the BigQuery <strong class="bold">VECTOR_SEARCH</strong> function, which will quickly find similar embeddings within the vector database. This is a major benefit for users who are familiar with SQL syntax, and it enables semantic search and similarity-based analysis directly within your BigQuery <span>data warehouse.</span></p>
<p class="calibre3">We can also choose to create a vector index to speed up the embedding retrieval process. When we use a vector index, an <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search<a id="_idIndexMarker2109" class="calibre6 pcalibre pcalibre1"/> is used to return approximate results, which is much quicker than doing a brute-force search to find an exact match. If we do not create a vector index, then a brute-force search will be performed. We also have the option to explicitly implement a brute-force search even when a vector index exists if we want to get an <span>exact result.</span></p>
<p class="calibre3">BigQuery <a id="_idIndexMarker2110" class="calibre6 pcalibre pcalibre1"/>Vector Search is a great choice for data science teams who already <a id="_idIndexMarker2111" class="calibre6 pcalibre pcalibre1"/>store a lot of data in BigQuery, and it also provides the benefits of BigQuery’s automatic and enormous scalability. Continuing the theme of scalability, another database service in Google Cloud that is renowned for its impressive scalability is Google Cloud Spanner. Let’s take a <span>closer look.</span></p>
<h3 class="calibre11">Spanner Vector Search</h3>
<p class="calibre3">Google Cloud Spanner is a fully <a id="_idIndexMarker2112" class="calibre6 pcalibre pcalibre1"/>managed and highly performant database service<a id="_idIndexMarker2113" class="calibre6 pcalibre pcalibre1"/> that is capable of global scale. It’s often referred to as a “NewSQL” database because it combines the scalability and flexibility of non-relational (NoSQL) databases with the familiar SQL interface of relational databases. It can easily handle petabytes of data, and unlike many NoSQL databases, it can guarantee strong consistency across transactions, even when <span>distributed globally.</span></p>
<p class="calibre3">With all of this in mind, Spanner is suitable for applications that require highly distributed and strongly consistent data, such as online banking use cases. In early 2024, Spanner also launched support for cosine distance and Euclidean distance comparisons, and we can use these vector distance functions to perform <strong class="bold">K-nearest neighbors</strong> (<strong class="bold">KNN</strong>) vector searches<a id="_idIndexMarker2114" class="calibre6 pcalibre pcalibre1"/> for use cases such as similarity search or RAG. This means that this highly distributed database service that is commonly used for enterprise-scale, business-critical workloads now also supports vector <span>database functionality.</span></p>
<p class="calibre3">While Spanner combines the best of NoSQL and relational database functionality, I’ll describe relational database options next before covering NoSQL <span>options explicitly.</span></p>
<h3 class="calibre11">Cloud SQL and pgvector</h3>
<p class="calibre3">Cloud SQL<a id="_idIndexMarker2115" class="calibre6 pcalibre pcalibre1"/> is a fully managed relational database service within Google Cloud that provides support<a id="_idIndexMarker2116" class="calibre6 pcalibre pcalibre1"/> for PostgreSQL, MySQL, and SQL Server. According to the Google Cloud documentation, “<em class="italic">More than 95% of Google Cloud’s top 100 customers use Cloud SQL to run their businesses.</em>” It is often seen as the default relational database service to use in Google Cloud, and considering that it’s fully managed, we don’t need to worry about server provisioning, operating system updates, or database patches because Google handles all of those activities <span>for us.</span></p>
<p class="calibre3">While Cloud SQL doesn’t natively <a id="_idIndexMarker2117" class="calibre6 pcalibre pcalibre1"/>support vector database capabilities, <strong class="bold">pgvector</strong> is an open source PostgreSQL extension that’s designed to provide <a id="_idIndexMarker2118" class="calibre6 pcalibre pcalibre1"/>vector similarity search functionality. It includes new data types, operators, and functions that enable us to store, search, and analyze high-dimensional vectors directly within our PostgreSQL database. It also includes specialized index types such as <strong class="bold">Hierarchical Navigable Small Worlds</strong> (<strong class="bold">HNSW</strong>) for<a id="_idIndexMarker2119" class="calibre6 pcalibre pcalibre1"/> fast ANN searches, and it Includes functions for vector addition, subtraction, normalization, and other operations, allowing us to manipulate embeddings within our database, as well as a similarity operator (<strong class="source-inline">&lt;-&gt;</strong>) to calculate distance metrics such as cosine similarity between vectors to help us find the most similar embeddings. What’s great is that it allows us to use SQL for all of those vector operations, which makes it easy to use for people who are familiar <span>with PostgreSQL.</span></p>
<p class="calibre3">In addition to using <strong class="source-inline">pgvector</strong> with Cloud SQL for PostgreSQL, we can also use it with AlloyDB, which I’ll <span>describe next.</span></p>
<h3 class="calibre11">AlloyDB AI Vector Search</h3>
<p class="calibre3">AlloyDB is a fully <a id="_idIndexMarker2120" class="calibre6 pcalibre pcalibre1"/>managed PostgreSQL-compatible database service on Google<a id="_idIndexMarker2121" class="calibre6 pcalibre pcalibre1"/> Cloud that’s designed for high-performance enterprise workloads. It significantly improves performance and scalability compared to standard PostgreSQL and provides advanced features such as auto-scaling and automatic failover, as well as built-in database backups and updates. It’s also suitable for hybrid transactional and analytical workloads, and recently, it has added multiple <span>ML-related features.</span></p>
<p class="calibre3">AlloyDB AI now <a id="_idIndexMarker2122" class="calibre6 pcalibre pcalibre1"/>also includes vector similarity search as it uses <strong class="source-inline">pgvector</strong> natively<a id="_idIndexMarker2123" class="calibre6 pcalibre pcalibre1"/> within <span>the database.</span></p>
<p class="calibre3">Next, we’ll discuss NoSQL database options for vector similarity search in <span>Google Cloud.</span></p>
<h3 class="calibre11">NoSQL database options for vector similarity search in Google Cloud</h3>
<p class="calibre3">Firestore is a <a id="_idIndexMarker2124" class="calibre6 pcalibre pcalibre1"/>NoSQL <a id="_idIndexMarker2125" class="calibre6 pcalibre pcalibre1"/>database within the Google Firebase ecosystem. Firebase itself is a Google Cloud framework that provides a collection of tools and services for developing mobile and web applications. In addition to the first-party functionality provided by Firebase, there’s<a id="_idIndexMarker2126" class="calibre6 pcalibre pcalibre1"/> also <strong class="bold">Firebase Extensions Hub</strong>, which provides a way for users to add more functionality to their solutions. While Firestore does not provide vector database support out-of-the-box, that functionality can be added via an extension named <strong class="bold">Semantic Search with Vertex AI</strong>, which adds text similarity search to a Firestore application by integrating<a id="_idIndexMarker2127" class="calibre6 pcalibre pcalibre1"/> with Vertex AI <span>Vector Search.</span></p>
<p class="calibre3">Another highly popular NoSQL database in Google Cloud is Bigtable. Similar to Firestore, Bigtable does not currently provide native support for vector similarity search use cases, but it can integrate with Vertex AI Vector Search to help build a solution for <span>that purpose.</span></p>
<p class="calibre3">Now that we’ve covered both the relational and NoSQL database options that provide vector database support in Google Cloud, it’s important to describe one additional type of data store, called <strong class="bold">Memorystore</strong>, which is<a id="_idIndexMarker2128" class="calibre6 pcalibre pcalibre1"/> used to implement caching solutions for extremely <span>low-latency workloads.</span></p>
<h3 class="calibre11">Memorystore for Redis</h3>
<p class="calibre3">Redis<a id="_idIndexMarker2129" class="calibre6 pcalibre pcalibre1"/> is an open source <a id="_idIndexMarker2130" class="calibre6 pcalibre pcalibre1"/>in-memory data store, meaning that, unlike traditional databases that store data on disk, Redis primarily stores data in RAM, giving it ultra-fast performance. Memorystore for Redis is a fully managed Redis service in Google Cloud that provides scalable, highly available, and secure Redis instances without requiring us to manage the instance or the <span>related infrastructure.</span></p>
<p class="calibre3">Memorystore for Redis<a id="_idIndexMarker2131" class="calibre6 pcalibre pcalibre1"/> now also supports ANN and KNN vector search, enabling us to implement an in-memory vector store cache for applications that require <a id="_idIndexMarker2132" class="calibre6 pcalibre pcalibre1"/>extremely <span>low-latency responses.</span></p>
<p class="calibre3">At this point, we’ve covered most of the main generative AI offerings on Google Cloud that are available. Now, it’s time to put some of our knowledge into action as we begin to implement generative AI solutions on <span>Google Cloud.</span></p>
<h1 id="_idParaDest-355" class="calibre5"><a id="_idTextAnchor417" class="calibre6 pcalibre pcalibre1"/>Implementing generative AI solutions on Google Cloud</h1>
<p class="calibre3">In this section, we <a id="_idIndexMarker2133" class="calibre6 pcalibre pcalibre1"/>will create a Vertex AI Search and Conversation application that will enable us to ask questions about the contents of a set of documents. We will use some publicly available medical study reports as examples, but this pattern can be applied to many different kinds of use cases. The following are some other popular applications of <span>this pattern:</span></p>
<ul class="calibre16">
<li class="calibre8">Enabling users of a retail website to get information about products on the site via a question-and-answer interface, in which they can ask human language questions and get factual responses about products in <span>the catalog.</span></li>
<li class="calibre8">Asking questions about financial documents. For example, employees in a company’s finance department could upload the company’s financial documents, such as quarterly earnings reports, and ask natural language questions about the contents of the reports to identify trends or other <span>important information.</span></li>
<li class="calibre8">Providing natural language search over a company’s vast corpus of internal documentation. For example, given a simple search interface, an employee could ask, “How can I modify my retirement contributions?” and they can get a response that tells them how to do so, along with a link to the internal documentation that describes the process <span>in detail.</span></li>
</ul>
<p class="calibre3">We will use Vertex AI Search and Conversation to build this application, as described in the <span>following subsections.</span></p>
<h2 id="_idParaDest-356" class="calibre9"><a id="_idTextAnchor418" class="calibre6 pcalibre pcalibre1"/>Building a Vertex AI Search and Conversation application</h2>
<p class="calibre3">At a high level, we will <a id="_idIndexMarker2134" class="calibre6 pcalibre pcalibre1"/>perform three main activities to build our Vertex AI Search and <span>Conversation application:</span></p>
<ol class="calibre7">
<li class="calibre8">Enable the Vertex AI Search and <span>Conversation API.</span></li>
<li class="calibre8">Create a data store to store the data we will use in <span>our application.</span></li>
<li class="calibre8">Create the Vertex AI Search and Conversation application so that it can interact with our <span>data store.</span></li>
</ol>
<p class="calibre3">Let’s begin by enabling the Vertex AI Search and <span>Conversation API.</span></p>
<h3 class="calibre11">Enabling the Vertex AI Search and Conversation API</h3>
<p class="calibre3">To enable the Vertex AI <a id="_idIndexMarker2135" class="calibre6 pcalibre pcalibre1"/>Search and Conversation API, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, type <strong class="source-inline">search</strong> into the search box and select <strong class="bold">Search </strong><span><strong class="bold">and Conversation</strong></span><span>.</span></li>
<li class="calibre8">If this is your first time using this service, a page similar to what’s shown in <span><em class="italic">Figure 17</em></span><em class="italic">.5</em> will <span>be displayed:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer212">
<img alt="Figure 17.5: Activating Vertex AI Search and Conversation" src="image/B18143_17_5.jpg" class="calibre198"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.5: Activating Vertex AI Search and Conversation</p>
<ol class="calibre7">
<li value="3" class="calibre8">Click <strong class="bold">CONTINUE AND ACTIVATE THE API</strong> (this checkbox is optional – you can either select it or leave <span>it blank).</span></li>
<li class="calibre8">After a<a id="_idIndexMarker2136" class="calibre6 pcalibre pcalibre1"/> few seconds, your environment will be created. A page similar to what’s shown in <span><em class="italic">Figure 17</em></span><em class="italic">.6</em> will <span>be displayed:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer213">
<img alt="Figure 17.6: Vertex AI Search and Conversation environment page" src="image/B18143_17_6.jpg" class="calibre199"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.6: Vertex AI Search and Conversation environment page</p>
<p class="calibre3">Now that we’ve <a id="_idIndexMarker2137" class="calibre6 pcalibre pcalibre1"/>enabled the API, we can start the next prerequisite step to set up our application, which is to create a <span>data store.</span></p>
<h3 class="calibre11">Creating a data store for Vertex AI Search and Conversation</h3>
<p class="calibre3">At the time<a id="_idIndexMarker2138" class="calibre6 pcalibre pcalibre1"/> of writing this, in March 2024, Vertex AI<a id="_idIndexMarker2139" class="calibre6 pcalibre pcalibre1"/> Search and Conversation can support the following data <span>store sources:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Website URLs</strong>: Automatically crawl website content from a list of domains <span>you define</span></li>
<li class="calibre8"><strong class="bold">BigQuery</strong>: Import data from your <span>BigQuery table</span></li>
<li class="calibre8"><strong class="bold">Cloud Storage</strong>: Import data from your <span>storage bucket</span></li>
<li class="calibre8"><strong class="bold">API</strong>: Import data manually by calling <span>the API</span></li>
</ul>
<p class="calibre3">We’re going to <a id="_idIndexMarker2140" class="calibre6 pcalibre pcalibre1"/>use Google Cloud Storage<a id="_idIndexMarker2141" class="calibre6 pcalibre pcalibre1"/> as our data store, so we’ll begin by setting up a location and uploading our data there. The next subsection describes <span>the process.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In <span><em class="italic">Chapter 4</em></span> of this book, we created a local directory in our Cloud Shell environment and cloned our GitHub repository into that directory. If you did not perform those steps, please reference those instructions now. They can be found in the <em class="italic">Creating a directory and cloning our GitHub </em><span><em class="italic">repository</em></span><span> section.</span></p>
<p class="callout">Once you have ensured that the GitHub repository for this book has been cloned into the local directory in your Cloud Shell environment, continue with the <span>following steps.</span></p>
<h4 class="calibre20">Storing our data in Google Cloud Storage</h4>
<p class="calibre3">The easiest <a id="_idIndexMarker2142" class="calibre6 pcalibre pcalibre1"/>way to stage our data is to use Google Cloud Shell. To<a id="_idIndexMarker2143" class="calibre6 pcalibre pcalibre1"/> set up our data store, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Click the <strong class="bold">Cloud Shell</strong> symbol in the top-right corner of the screen, as shown in <span><em class="italic">Figure 17</em></span><span><em class="italic">.7</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer214">
<img alt="Figure 17.7: Activating Cloud Shell" src="image/B18143_17_7.jpg" class="calibre200"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.7: Activating Cloud Shell</p>
<ol class="calibre7">
<li value="2" class="calibre8">This will activate the Cloud Shell environment, which will appear at the bottom of your screen. It will take a few seconds for your environment to activate. Once your environment has been activated, you can paste and run the commands mentioned in the following steps into <span>Cloud Shell.</span></li>
<li class="calibre8">Run the following commands to set up environment variables so that you can store your preferred region, your Cloud Storage bucket name, and the data path (<strong class="bold">replace YOUR-REGION with your preferred region, such as us-central1, and YOUR-BUCKET-NAME with your </strong><span><strong class="bold">bucket name</strong></span><span>):</span><pre class="source-code">
export REGION=YOUR-REGION
export BUCKET=YOUR-BUCKET-NAME
export BUCKET_PATH=${BUCKET}/data/Chapter-17/</pre></li> <li class="calibre8">Verify the bucket path (you should also copy the contents of the response from this command and keep it for reference to be used in a <span>later step):</span><pre class="source-code">
echo $BUCKET_PATH</pre></li> <li class="calibre8">Create <a id="_idIndexMarker2144" class="calibre6 pcalibre pcalibre1"/>the bucket <a id="_idIndexMarker2145" class="calibre6 pcalibre pcalibre1"/>if it doesn’t <span>already exist:</span><pre class="source-code">
gsutil mb -l $REGION gs://${BUCKET}</pre></li> <li class="calibre8">Change directories to the location at which the data for this chapter <span>is stored:</span><pre class="source-code">
cd ~/packt-ml-sa/Google-Machine-Learning-for-Solutions-Architects/Chapter-17/data</pre></li> <li class="calibre8">Upload the files to the bucket (this also creates the path within <span>the bucket):</span><pre class="source-code">
for i in *; do gsutil cp $i gs://${BUCKET_PATH}; done</pre></li> <li class="calibre8">Verify that the files have been uploaded (the following command should list <span>the files):</span><pre class="source-code">
gsutil ls gs://${BUCKET_PATH}</pre></li> </ol>
<p class="calibre3">Now that we’ve uploaded the files to our Cloud Storage bucket, we can create the Vertex AI Search and Conversation data store for our application, as described in the <span>next subsection.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The citations for the documents used in this exercise can be found in the <strong class="source-inline1">document_citations.txt</strong> file in the <strong class="source-inline1">Chapter-17</strong> directory of this book’s GitHub <span>repository: </span><a href="https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt" class="calibre6 pcalibre pcalibre1"><span>https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-17/document_citations.txt</span></a><span>.</span></p>
<h4 class="calibre20">Creating the Vertex AI Search and Conversation data store</h4>
<p class="calibre3">Within Vertex AI <a id="_idIndexMarker2146" class="calibre6 pcalibre pcalibre1"/>Search and Conversation, we will define a data store associated with the files we uploaded to Cloud Storage. To do that, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Vertex AI Search and Conversation console UI, select <strong class="bold">Data Stores</strong> |<strong class="bold"> </strong><strong class="bold">Create data store</strong> | <span><strong class="bold">Cloud Storage</strong></span><span>.</span></li>
<li class="calibre8">A screen similar to the one shown in <span><em class="italic">Figure 17</em></span><em class="italic">.8</em> will <span>be displayed:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer215">
<img alt="Figure 17.8: Import data from Cloud Storage" src="image/B18143_17_8.jpg" class="calibre201"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.8: Import data from Cloud Storage</p>
<ol class="calibre7">
<li value="3" class="calibre8">Enter the path to the files you uploaded (this is the value you specified for the <strong class="source-inline">BUCKET_PATH</strong> environment variable in your Cloud Shell environment in the <span>previous section).</span></li>
<li class="calibre8">We will use unstructured documents in this example, so the <strong class="bold">Unstructured documents</strong> option should <span>remain selected.</span></li>
<li class="calibre8">Click <strong class="bold">Continue</strong>. A <a id="_idIndexMarker2147" class="calibre6 pcalibre pcalibre1"/>screen similar to what’s shown in <span><em class="italic">Figure 17</em></span><em class="italic">.9</em> will <span>be displayed:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer216">
<img alt="Figure 17.9: Configure your data store" src="image/B18143_17_9.jpg" class="calibre202"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.9: Configure your data store</p>
<ol class="calibre7">
<li value="6" class="calibre8">Select <strong class="bold">global (Global)</strong> <span>under </span><span><strong class="bold">Multi-region</strong></span><span>.</span></li>
<li class="calibre8">Select <strong class="bold">Digital Parser</strong> under <strong class="bold">Default </strong><span><strong class="bold">document parser</strong></span><span>.</span></li>
<li class="calibre8">Enter<a id="_idIndexMarker2148" class="calibre6 pcalibre pcalibre1"/> a name for the data store, such as <strong class="source-inline">AIML-SA-DS</strong>, and <span>click </span><span><strong class="bold">CREATE</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Now that the data store has been created, it’s time to create the Vertex AI Search and <span>Conversation application.</span></p>
<h3 class="calibre11">Creating the Vertex AI Search and Conversation application</h3>
<p class="calibre3">To create the Vertex AI Search and<a id="_idIndexMarker2149" class="calibre6 pcalibre pcalibre1"/> Conversation application, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Vertex AI Search and Conversation console UI, select <strong class="bold">Apps</strong> | <strong class="bold">Create a new app</strong> | <span><strong class="bold">Search</strong></span><span>.</span></li>
<li class="calibre8">On the next screen that appears, ensure that the <strong class="bold">Enterprise edition features</strong> and <strong class="bold">Advanced LLM features</strong> checkboxes are enabled, as shown in <span><em class="italic">Figure 17</em></span><em class="italic">.10</em> (read the descriptions for each option to understand <span>their features):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer217">
<img alt="Figure 17.10: Enabling Search and Conversation features" src="image/B18143_17_10.jpg" class="calibre203"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.10: Enabling Search and Conversation features</p>
<ol class="calibre7">
<li value="3" class="calibre8">Next, enter <a id="_idIndexMarker2150" class="calibre6 pcalibre pcalibre1"/>an application name and a company name, select the location for the application, and select <strong class="bold">CONTINUE</strong>, as shown in <span><em class="italic">Figure 17</em></span><em class="italic">.11</em> (note the recommendation to choose a global location if you do not have compliance or regulatory reasons to locate your data in a <span>particular multi-region):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer218">
<img alt="Figure 17.11: Application details" src="image/B18143_17_11.jpg" class="calibre204"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.11: Application details</p>
<ol class="calibre7">
<li value="4" class="calibre8">Next, select<a id="_idIndexMarker2151" class="calibre6 pcalibre pcalibre1"/> the data store we created that you wish to integrate with our application, as shown in <span><em class="italic">Figure 17</em></span><span><em class="italic">.12</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer219">
<img alt="Figure 17.12: Selecting a data store" src="image/B18143_17_12.jpg" class="calibre205"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 17.12: Selecting a data store</p>
<p class="calibre3">It will take some time for the data for our application to be processed. Periodically refresh the page to check its status. Once it’s complete, a list of files will be displayed. At that point, we are ready to configure how we want our application to behave, as described in the <span>following subsection.</span></p>
<h3 class="calibre11">Configuring our newly created application</h3>
<p class="calibre3">There are <a id="_idIndexMarker2152" class="calibre6 pcalibre pcalibre1"/>numerous ways in which our new Search and Conversation application can work, and we can specify how we want it to work by modifying its configuration. To do that, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Vertex AI Search and Conversation console UI, select <strong class="bold">Apps</strong> from the menu on the left-hand side of <span>the page.</span></li>
<li class="calibre8">Click on our newly <span>created app.</span></li>
<li class="calibre8">Select <strong class="bold">Configurations</strong> from the menu on the left-hand side of <span>the page.</span><p class="calibre3">For the <strong class="bold">Search type</strong> configuration, there are <span>three options:</span></p><ol class="calibre77"><li class="alphabets"><strong class="bold">Search</strong>: Simply respond to the search query with a list of <span>relevant results.</span></li><li class="alphabets"><strong class="bold">Search with an answer</strong>: Provide a generative summary above the list of <span>search results.</span></li><li class="alphabets"><strong class="bold">Search with follow-ups</strong>: Provide conversational search with generative summaries and support for <span>follow-up questions.</span></li></ol></li>
<li class="calibre8">Select the <strong class="bold">Search with </strong><span><strong class="bold">follow-ups</strong></span><span> option.</span></li>
<li class="calibre8">Scroll down to the <strong class="bold">Large Language Models for summarization</strong> section and select the latest version <span>of Gemini.</span></li>
<li class="calibre8">You can leave all other options at their default values unless you have any specific requirements to <span>change them.</span></li>
<li class="calibre8">Click <strong class="bold">SAVE </strong><span><strong class="bold">AND PUBLISH</strong></span><span>.</span></li>
</ol>
<p class="calibre3">At this point, we are ready to start using our application, as described in the <span>following subsection.</span></p>
<h3 class="calibre11">Using our newly created application</h3>
<p class="calibre3">Now that we’ve<a id="_idIndexMarker2153" class="calibre6 pcalibre pcalibre1"/> created our application, we can start using it. To do that, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Vertex AI Search and Conversation console UI, select the <strong class="bold">Preview</strong> section for our newly <span>created application.</span></li>
<li class="calibre8">We will be presented with a search box in which we can ask questions about the contents of the documents in our <span>data store.</span></li>
<li class="calibre8">Begin with the following question: <strong class="source-inline">What are the effects of cinnamon </strong><span><strong class="source-inline">on health?</strong></span></li>
<li class="calibre8">Click <strong class="bold">Ask a follow-up</strong> and enter the following question: <strong class="source-inline">What specific properties contribute to its ability to regulate </strong><span><strong class="source-inline">glucose levels?</strong></span><p class="calibre3">Note that the follow-up question simply mentions “it” and does not mention cinnamon directly. However, the model uses the context from the previous question to understand <span>our intent.</span></p><p class="calibre3">Also, note that the generated answers may contain references, and the referenced documents that were used to generate the answer are listed in <span>the response.</span></p></li>
<li class="calibre8">Try out the following <span>additional questions:</span><ol class="calibre77"><li class="alphabets"><strong class="source-inline">How does the daily dosage of cinnamon used in the study compare to typical dietary </strong><span><strong class="source-inline">cinnamon intake?</strong></span></li><li class="alphabets"><strong class="source-inline">Were there any observed long-term effects of cinnamon supplementation beyond the 4-week </strong><span><strong class="source-inline">trial period?</strong></span></li><li class="alphabets"><strong class="source-inline">How might cinnamon supplementation interact with other dietary or lifestyle interventions for </strong><span><strong class="source-inline">prediabetes management?</strong></span></li><li class="alphabets"><strong class="source-inline">Could the study's findings on cinnamon's glucose-regulating effects extend to individuals with types 1 or </strong><span><strong class="source-inline">2 diabetes?</strong></span></li></ol></li>
<li class="calibre8">Next, let’s ask some questions that will cause the model to respond based on other documents<a id="_idIndexMarker2154" class="calibre6 pcalibre pcalibre1"/> in the collection. Try out the <span>following questions:</span><ol class="calibre77"><li class="alphabets"><strong class="source-inline">What are the main challenges FIM programs face when attempting to use EHR data </strong><span><strong class="source-inline">for evaluation?</strong></span></li><li class="alphabets"><strong class="source-inline">How can FIM programs overcome the barriers to accessing and utilizing EHR </strong><span><strong class="source-inline">data effectively?</strong></span></li><li class="alphabets"><strong class="source-inline">What alternative data sources can FIM programs use to evaluate health outcomes and healthcare utilization, apart from </strong><span><strong class="source-inline">EHR data?</strong></span></li><li class="alphabets"><strong class="source-inline">How do the privacy and liability concerns of healthcare partners impact the sharing of EHR data with </strong><span><strong class="source-inline">FIM programs?</strong></span></li><li class="alphabets"><strong class="source-inline">What role does albumin play in the nutritional status of children, and how are red bean cookies effective in </strong><span><strong class="source-inline">improving it?</strong></span></li><li class="alphabets"><strong class="source-inline">What specific dietary interventions have shown promise in modifying the gut microbiome to improve outcomes for patients </strong><span><strong class="source-inline">with diseases?</strong></span></li><li class="alphabets"><strong class="source-inline">How does the gut microbiome's interaction with the body impact mental health disorders, and what mechanisms </strong><span><strong class="source-inline">are involved?</strong></span></li><li class="alphabets"><strong class="source-inline">Can changes in the gut microbiome serve as early indicators for the development of chronic </strong><span><strong class="source-inline">kidney disease?</strong></span></li></ol></li>
</ol>
<p class="calibre3">These are just some examples – feel free to play around with additional documents and questions, and think about how this pattern can be extended to pretty much any use case due to the vast knowledge of these <span>wondrous models.</span></p>
<p class="calibre3">That’s it! You have successfully built your first generative AI application in Google Cloud! As you can see, Vertex AI Search and Conversation makes it very easy for us to do this. Behind the scenes, this can be seen as a RAG solution because we are interacting with the Gemini model and getting it to generate responses that are grounded in the contents of the documents we uploaded to our data store, although Vertex AI Search and Conversation abstracts away and manages all of the complexities and steps required to implement <span>the solution.</span></p>
<p class="calibre3">In the next chapter, we will build some additional and more complex use cases. First, however, let’s summarize what we’ve learned in <span>this chapter.</span></p>
<h1 id="_idParaDest-357" class="calibre5"><a id="_idTextAnchor419" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we dived into generative AI in Google Cloud, exploring Google’s native generative AI models, such as the Gemini, PaLM, Codey, Imagen, and MedLM APIs. We discussed the multiple versions of each model and some example use cases for each. Then, we introduced Vertex AI Studio and discussed open source and third-party models available on Google Cloud via repositories such as Vertex AI Model Garden and <span>Hugging Face.</span></p>
<p class="calibre3">Next, we discussed vector databases in Google Cloud, covering various options available, such as Vertex AI Search and Conversation, Vertex AI Vector Search, BigQuery Vector Search, Spanner Vector Search, <strong class="source-inline">pgvector</strong>, and AlloyDB AI Vector Search, including some decision factors for choosing one solution over another. It is these kinds of decision points that are often most important in the role of a solutions architect, and the decisions will vary based on the specific needs of the customer or project, including the cost of each solution. I recommend always consulting the latest pricing information for each product and factoring that into your <span>decision process.</span></p>
<p class="calibre3">Finally, we put some of this chapter’s topics into action and built a generative AI application in Google Cloud – specifically, we built a Vertex AI Search and Conversation application that enabled us to ask natural language questions about the contents of a collection <span>of documents.</span></p>
<p class="calibre3">In the next chapter, we will continue with this theme of using the topics we’ve covered throughout this book to build solutions in Google Cloud. Join me there to start diving <span>in further!</span></p>
</div>
</div></body></html>