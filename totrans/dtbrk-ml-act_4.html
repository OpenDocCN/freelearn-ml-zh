<html><head></head><body>
		<div id="_idContainer134">
			<h1 class="chapter-number" id="_idParaDest-82"><a id="_idTextAnchor180"/>4</h1>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor181"/>Getting to Know Your Data</h1>
			<p class="author-quote">“Truth, like gold, is to be obtained not by its growth, but by washing away from it all that is not gold.”</p>
			<p class="author-quote">―Leo Tolstoy<a id="_idTextAnchor182"/></p>
			<p><a id="_idTextAnchor183"/>In this chapter, we explore features within the Databricks DI Platform that help improve and monitor data quality and facilitate data exploration. There are numerous approaches to getting to know your data better with Databricks. First, we cover how to oversee data quality<a id="_idIndexMarker209"/> with <strong class="bold">Delta Live Tables</strong> (<strong class="bold">DLT</strong>) to catch quality issues early and prevent the contamination of entire pipelines. We’ll take our first close look at Lakehouse Monitoring, which helps us analyze data changes over time and can alert us to changes that concern us. Lakehouse Monitoring is a big time-saver, allowing you to focus on mitigating or responding to data changes rather than creating notebooks that calculate <span class="No-Break">standard metrics.</span><a id="_idTextAnchor184"/></p>
			<p>Moving on to data exploration, we will look at a couple of low-code approaches: Databricks Assistant <a id="_idIndexMarker210"/>and <strong class="bold">AutoML</strong>. Finally, we will touch on embeddings. We created embeddings from chunks of text in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, and you’ll learn how to use Databricks <strong class="bold">Vector Search</strong> (<strong class="bold">VS</strong>) to <a id="_idIndexMarker211"/>explore <span class="No-Break">your embeddings<a id="_idTextAnchor185"/>.</span></p>
			<p>Here is what you will learn as part of <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Improving data integrity <span class="No-Break">with DLT</span></li>
				<li>Monitoring data quality with Databricks <span class="No-Break">Lakehouse Monitoring</span></li>
				<li>Exploring data with <span class="No-Break">Databricks Assistant</span></li>
				<li>Generating data profiles <span class="No-Break">with AutoML</span></li>
				<li>Preparing data for vector search and <span class="No-Break">database indexing</span></li>
				<li>Enhancing data retrieval with Databricks <span class="No-Break">Vector Search</span></li>
				<li>Applying <span class="No-Break">our learning</span></li>
			</ul>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor186"/>Improving data integrity with DLT</h1>
			<p>In the<a id="_idIndexMarker212"/> last chapter, we introduced DLT as a helpful tool for streaming data and pipeline development. Here, we focus on how to use DLT as your go-to tool for actively tracking data quality. Generally, datasets are dynamic, not neat, and tidy like they often are in school and training. You can use code<a id="_idIndexMarker213"/> to clean data, of course, but there is a feature that makes the cleaning process even easier: DLT’s expectations. DLT’s expectations catch incoming data quality issues and automatically validate that incoming data passes specified rules and quality checks. For example, you might expect your customer data to have positive values for age or that dates follow a specific format. When data does not meet these expectations, it can negatively impact downstream data pipelines. With expectations implemented, you ensure that your pipelines <span class="No-Break">won’t suffer.</span></p>
			<p>Implementing expectations gives us more control over data quality, alerting us to unusual data requiring attention and action. There are several options when dealing with erroneous data in DLT: </p>
			<ol>
				<li>First, we can set a warning, which will report the number of records that failed an expectation as a metric but still write those invalid records to the destination dataset; see <strong class="source-inline">@dlt.expect()</strong> in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> for an example. </li>
				<li>Second, we can drop invalid records so that the final dataset only contains records that meet our expectations; see <strong class="source-inline">@dlt.expect_or_drop()</strong> in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>. </li>
				<li>Third, we can fail the operation entirely, so nothing new is written (note that this option requires manual re-triggering of the pipeline). </li>
				<li>Finally, we can quarantine the invalid data to another table to investigate it further. The following code should look familiar to the DLT code in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, but now with the addition <span class="No-Break">of expectations.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer100">
					<img alt="Figure 4.1 – Using DLT expectations to enforce data quality" src="image/B16865_04_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Using DLT expectations to enforce data quality</p>
			<p>Let’s look at our streaming transactions project as an example. In the <em class="italic">Applying our learning</em> section in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, we used DLT to write the transaction data to a table. Utilizing the same DLT code, we will save ourselves the manual effort of cleaning the <strong class="source-inline">CustomerID</strong> column by adding an expectation to the original code to drop any records with a <strong class="source-inline">null</strong> <strong class="source-inline">CustomerID</strong>. We will set another expectation to warn us if the <strong class="source-inline">Product</strong> field <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">null</strong></span><span class="No-Break">.</span></p>
			<p>Now, when<a id="_idIndexMarker214"/> we call <strong class="source-inline">generate_table()</strong>, the DLT pipeline will automatically clean up our table by dropping any null <strong class="source-inline">CustomerID</strong> values and flagging records without a <strong class="source-inline">Product</strong> value. Moreover, DLT will automatically build helpful visualizations to immediately investigate the <span class="No-Break">data’s quality.</span></p>
			<p>To try this<a id="_idIndexMarker215"/> yourself, update the DLT code from <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> (here’s the path to the notebook as a reminder: <span class="No-Break"><strong class="source-inline">Chapter 3</strong></span><strong class="source-inline">: Building </strong><strong class="source-inline">Out Our</strong><strong class="source-inline"> Bronze Layer/Project: Streaming Transactions/delta_live_tables/</strong>) to match <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>, and then rerun the DLT pipeline as you did before. Once the pipeline is complete, it generates the DAG. Click on the <strong class="source-inline">synthetic_transactions_silver</strong> table, then click the <strong class="bold">Data Quality</strong> tab from the table details. This will display information about the records processed, such as how many were written versus dropped for failing a given expectation, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer101">
					<img alt="Figure 4.2 – The DLT data quality visualizations" src="image/B16865_04_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – The DLT data quality visualizations</p>
			<p>These insights illustrate how expectations help automatically clean up our tables and flag information that might be useful for data scientists using this table downstream. In this example, we see that all records passed the <strong class="source-inline">valid_CustomerID</strong> expectation, so now we know <a id="_idIndexMarker216"/>we don’t have to worry about null customer IDs in the table. Additionally, almost 80% of records are missing a <strong class="source-inline">Product</strong> value, which may be relevant for data science and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) projects that<a id="_idIndexMarker217"/> use <span class="No-Break">this data.</span></p>
			<p>Just as we’ve<a id="_idIndexMarker218"/> considered the correctness and consistency of incoming data, we also want to consider how we can expand our data quality oversight to include data drift, for example, when your data’s distribution changes over time. Observing data drift is where Databricks Lakehouse Monitoring emerges as a vital complement to DLT, offering a configurable framework to consistently observe and verify the statistical properties and quality of <span class="No-Break">inpu<a id="_idTextAnchor187"/>t data.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor188"/>Monitoring data quality with Databricks Lakehouse Monitoring</h1>
			<p>Use <a id="_idIndexMarker219"/>Databricks Lakehouse Monitoring to proactively detect and respond to any deviations in your data distribution. Over time, your data may undergo changes in its underlying patterns. This could be feature drift, where the distribution of feature data changes over time, or concept drift, where the relationship between inputs and outputs of your model changes. Both types of drift can cause model quality to suffer. These changes can occur slowly or rapidly in your production environment, which is why monitoring your data even before it becomes an input into your ML models and data products <span class="No-Break">is ess<a id="_idTextAnchor189"/>ential.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor190"/>Mechanics of Lakehouse Monitoring</h2>
			<p>To monitor <a id="_idIndexMarker220"/>a table in Databricks, you create a monitor attached to that table. To monitor the performance of a ML model, you attach the monitor to an inference table that holds the model’s inputs and corresponding predictions. Databricks Lakehouse Monitoring provides the following profile types of analysis: snapshot, time series, <span class="No-Break">and inference.</span></p>
			<p>In addition to selecting the table to be monitored, called <a id="_idIndexMarker221"/>the <strong class="bold">primary table</strong>, you can optionally specify a baseline table to reference for measuring drift or the change in values over time. A baseline table is useful when you have a sample of what you expect your data to look like, such as the data with which your model was trained. Lakehouse Monitoring automatically computes drift relative to expected data values and distributions of the <span class="No-Break">baseline table.</span></p>
			<p>Creating a <a id="_idIndexMarker222"/>table monitor automatically creates two metric tables, <strong class="source-inline">profile_metrics</strong> and <strong class="source-inline">drift_metrics</strong>. Lakehouse Monitoring computes metric values on the table for the time windows and data subsets or “slices” you specify when you create the monitor. You can also add your own custom metrics; see <em class="italic">Further reading</em> <span class="No-Break">f<a id="_idTextAnchor191"/>or details.</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor192"/>Visualization and alerting</h2>
			<p>Lakehouse Monitoring generates<a id="_idIndexMarker223"/> an SQL dashboard automatically for every monitor. These dashboards provide a crucial platform for examining<a id="_idIndexMarker224"/> metrics and acting on results. Databricks’ alert system serves as a vigilant guardian, promptly notifying you of the significant shifts in data quality or distributions you subscribe to. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> shows how all the Lakehouse Monitoring components work together, using the data from the primary table and optional baseline table to generate a profile metrics table and drift table, which then populate the dashboard and <span class="No-Break">power alerts.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer102">
					<img alt="Figure 4.3 – Relationship between the input tables, the metric tables, the monitor, and the dashboard" src="image/B16865_04_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Relationship between the input tables, the metric tables, the monitor, and the dashboard</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor193"/>Creating a monitor</h2>
			<p>You can<a id="_idIndexMarker225"/> create a Databricks Lakehouse Monitor using the <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) or, for a more flexible and programmable approach, you can use the API. The API method is particularly advantageous when you want to script the creation of monitors to integrate them into your automated data pipelines. The following is a high-level summary of the steps to create a Lakehouse Monitor using <span class="No-Break">the API:</span></p>
			<ol>
				<li><strong class="bold">Choose the profile type</strong>: Decide on the profile type parameter that best suits your monitoring needs. The available types are <strong class="source-inline">Snapshot</strong>, <strong class="source-inline">TimeSeries</strong>, and <strong class="source-inline">Inference</strong>. Each type is suitable for different monitoring scenarios, with the <strong class="source-inline">TimeSeries</strong> and <strong class="source-inline">Inference</strong> types requiring a timestamp column. The inference profile also requires <strong class="source-inline">prediction_col</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">model_id_col</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Create the monitor</strong>: Use the <strong class="source-inline">lakehouse_monitoring</strong> module to call the <strong class="source-inline">create_monitor</strong> function, providing your table’s catalog schema, table name, the chosen profile type, and the output schema for the <span class="No-Break">monitor’s results.</span></li>
				<li><strong class="bold">Set the schedule</strong>: If you wish to run the monitor at regular intervals, you can specify a schedule using the <strong class="source-inline">MonitorCronSchedule</strong> object, which takes a cron expression and a time <span class="No-Break">zone ID.</span></li>
				<li><strong class="bold">Control access</strong>: After you create the monitor, you can manage access to the resulting metrics tables and dashboard using Unity <span class="No-Break">Catalog privileges.</span></li>
				<li><strong class="bold">Refresh and review results</strong>: Use the <strong class="source-inline">run_refresh</strong> function to refresh and update the metric tables. You can also check the status of specific runs with the <strong class="source-inline">get_refresh</strong> function and list all refreshes associated with a monitor <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">list_refreshes</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Review monitor settings</strong>: The <strong class="source-inline">get_monitor</strong> function allows you to retrieve your monitor’s current settings <span class="No-Break">for review.</span></li>
			</ol>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> shows an example of how to use the Lakehouse Monitoring API to create a <strong class="source-inline">TimeSeries</strong> <span class="No-Break">profile monitor:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer103">
					<img alt="Figure 4.4 – Creating a simple TimeSeries table monitor" src="image/B16865_04_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Creating a simple TimeSeries table monitor</p>
			<p>After <a id="_idIndexMarker226"/>establishing a robust framework for data quality monitoring with Databricks Lakehouse Monitoring, we can focus on enhancing our data exploration. This leads us to Databricks Assistant, a feature dedicated to helping developers be more productive <span class="No-Break">i<a id="_idTextAnchor194"/>n Databricks.</span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor195"/>Exploring data with Databricks Assistant</h1>
			<p>Databricks Assistant is <a id="_idIndexMarker227"/>a feature designed to boost your productivity in Databricks. It has many capabilities, including generating SQL from English, explaining code, helping troubleshoot errors, and optimizing code. Databricks Assistant is <a id="_idIndexMarker228"/>an exciting feature to watch as more capabilities emerge, and we want to give you a taste of the possibilities. Since this chapter is about exploring and monitoring data, let’s see how you can use Databricks Assistant as a low-code solution to explore your data. Suppose you are analyzing the Favorita Sales Forecasting data. You are looking to uncover insights into retail store distributions across various regions. You have a specific query in mind: you want to understand the store landscape in the Guayas region. However, SQL queries aren’t your strong suit, and maybe crafting the perfect query seems daunting. In order to explore your data regardless, you can use Databricks Assistant. There is no notebook in the project repo for this section, but we encourage you to try Databricks Assistant on the Favorita Forecasting project tables. Any of the <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> Favorita notebooks would be a great place to access Databricks Assistant. To access it, click on the icon shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em>, found in the left-hand sidebar of a notebook. Clicking on the icon will open the chat interface to the left of the notebook, where we will type in <span class="No-Break">our questions.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 4.5 – The Databricks Assistant icon" src="image/B16865_04_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – The Databricks Assistant icon</p>
			<p>First, we <a id="_idIndexMarker229"/>ask Databricks Assistant how many stores and store types are in the state of Guayas in the <strong class="source-inline">favorita_stores</strong> table (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.6</em>). Note that Databricks Assistant does not require Unity Catalog, but using Unity Catalog provides table information. The additional information makes Databricks Assistant’s responses more helpful and specific to the table you’re working with. This should sound similar to<a id="_idIndexMarker230"/> the <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>) project. We are augmenting the generation of answers by providing relevant information. Now, let’s see whether it can help us write the SQL query we need. Keep in mind that Databricks Assistant is powered by <strong class="bold">generative AI</strong>, so you <a id="_idIndexMarker231"/>may see different outputs when using <span class="No-Break">it yourself.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 4.6 – Question and response interaction with Databricks Assistant" src="image/B16865_04_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Question and response interaction with Databricks Assistant</p>
			<p>Nice! Databricks Assistant gave us some SQL code to paste into a notebook and run directly. However, a quick scan of the query shows us that we will only get a distinct count of stores and types rather than looking at the store type distribution we want. Let’s refine<a id="_idIndexMarker232"/> our question and try again (<span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="Figure 4.7 – The updated question submitted to Databricks Assistant and the results" src="image/B16865_04_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – The updated question submitted to Databricks Assistant and the results</p>
			<p>After submitting <a id="_idIndexMarker233"/>the second question in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.7</em>, Databricks Assistant provides a new query that accurately captures what we want to know. To make sure, let’s copy the SQL code provided, paste it into a notebook, and <span class="No-Break">run it.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 4.8 – Results from Databricks Assistant-generated SQL query" src="image/B16865_04_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Results from Databricks Assistant-generated SQL query</p>
			<p>Now we<a id="_idIndexMarker234"/> see the distribution of stores by type in the Guayas region, just as we wanted. Databricks Assistant is a handy tool, and we can confirm that playing with it is also fun! We encourage you to try it out on your own to see how you can use English to explore <span class="No-Break">your data.</span></p>
			<p>Databricks Assistant also generates comment suggestions for your tables and fields. From the <strong class="bold">Unity Catalog Explorer</strong> page, navigate to the same <strong class="source-inline">favorita_stores</strong> table we explored previously. We see in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.9</em> that Databricks Assistant has a suggestion for a table comment to help others understand the <span class="No-Break">table’s contents.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 4.9 – Databricks Assistant suggests a descriptive table comment" src="image/B16865_04_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Databricks Assistant suggests a descriptive table comment</p>
			<p>Databricks Assistant can also generate descriptive comments for each field in the table by selecting the <strong class="bold">AI Generate</strong> button on the right-hand side of the table’s <strong class="bold">Columns</strong> tab page. You<a id="_idIndexMarker235"/> can accept or edit the suggested comments. You can toggle off the suggestions by selecting the <strong class="bold">Hide AI suggestions</strong> button, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="Figure 4.10 – Databricks Assistant suggests comments for the columns in a table" src="image/B16865_04_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Databricks Assistant suggests comments for the columns in a table</p>
			<p>The generated comments might seem like they fall outside the scope of getting to know your data, but documentation is vital to making your data more easily discoverable for everyone else (and let’s hope others use generated comments so you can explore their datasets more easily too). The faster you understand a dataset, the easier it is to further explore <span class="No-Break">that data.</span></p>
			<p>Databricks Assistant is a great way to analyze your data when you prefer to work in English rather than code directly, and when you have specific questions in mind. Now let’s discuss another method for <a id="_idIndexMarker236"/>broader <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>): autogenerated notebo<a id="_idTextAnchor196"/>oks <span class="No-Break">using AutoML.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor197"/>Generating data profiles with AutoML</h1>
			<p>We <a id="_idIndexMarker237"/>introduced Databricks AutoML in <a href="B16865_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. This tool automates ML development and augments data science workflows. AutoML is<a id="_idIndexMarker238"/> best known for generating models, but we’ll get to modeling in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Since we’re talking about getting to know your data, we first want to focus on one extremely useful feature built into AutoML that often flies under the radar: autogenerated Python notebooks. AutoML provides a notebook for data exploration in addition to the notebook code for every experiment it runs. We will jump right into creating an AutoML experiment, view the data exploration code, and then return to explore the modeling <span class="No-Break">portion later.</span></p>
			<p>We’ll cover how to create an AutoML experiment via an API in the Favorita project notebooks. We encourage you to follow the instructions here to set up a simple regression experiment with the AutoML UI, so that we can take a look at the data profile created. Before you begin, make sure you have a DBR ML 9.1+ cluster running (you can use the <strong class="bold">DBR ML 14.2</strong> cluster set up in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">):</span></p>
			<ol>
				<li><strong class="bold">Start the experiment</strong>: Navigate to the <strong class="bold">Experiments</strong> tab on the platform’s left-hand navigation bar. Click the <strong class="bold">AutoML Experiment</strong> button to initiate a <span class="No-Break">new experiment.</span></li>
				<li><strong class="bold">Configure the </strong><span class="No-Break"><strong class="bold">AutoML experiment</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break"><strong class="bold">Compute configuration</strong></span><span class="No-Break">:</span><ol><li class="lower-roman"><strong class="bold">Cluster</strong>: Select the <strong class="bold">DBR ML 14.2</strong> cluster set up in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, or any other cluster with an ML runtime with a version greater than 9.1. Note that the cluster won’t appear in the drop-down menu unless <span class="No-Break">it’s running.</span></li></ol></li><li><span class="No-Break"><strong class="bold">Experiment configuration</strong></span><span class="No-Break">:</span><ol><li class="lower-roman" value="1"><strong class="bold">ML problem type</strong>: Select the type of ML problem you’re addressing. Select <strong class="bold">Regression</strong>, which is suitable for predicting continuous outcomes such as sales figures <span class="No-Break">or temperature.</span></li><li class="lower-roman"><strong class="bold">Input training dataset</strong>: Specify the dataset you will use to train the model. You can use the Favorita project’s <strong class="source-inline">ml_in_action.favorita_forecasting.favorite_train_set</strong> table, or your own data (just make sure to update the problem type if <strong class="source-inline">Regression</strong> does <span class="No-Break">not apply).</span></li><li class="lower-roman"><strong class="bold">Prediction target</strong>: Choose the specific column in your dataset that you want to predict. The AutoML process will use the other columns in your dataset to try and predict the values in this target column. If you’re following along with the Favorita scenario, <span class="No-Break">select </span><span class="No-Break"><strong class="bold">sales</strong></span><span class="No-Break">.</span></li></ol></li></ul></li>
			</ol>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em> shows the<a id="_idIndexMarker239"/> configuration of an AutoML experiment using the <strong class="source-inline">favorite_train_set</strong> training table. It illustrates how you can <a id="_idIndexMarker240"/>customize the AutoML process to fit the specific requirements of your ML task. By selecting the appropriate problem type, dataset, and prediction target, you’re instructing the AutoML system on how to approach the <span class="No-Break">model-building process.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set table" src="image/B16865_04_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set table</p>
			<p>Once you’ve filled out the UI and specified the Favorita table (or another table of your choice) as your input training dataset, click <strong class="bold">Start AutoML</strong> at the bottom of the screen. AutoML experiments may run for up to several hours as improvements in the evaluation metric continue, although you can set a shorter time limit for the experiment by changing the <strong class="bold">Timeout</strong> value (found under <strong class="bold">Advanced</strong> <strong class="bold">Configuration</strong>). As the experiment begins, a new page will open with progress updates. Once the experiment is complete, you will see links to two data artifacts: the notebook containing the code for the best <a id="_idIndexMarker241"/>model, labeled <strong class="bold">View notebook for</strong> <strong class="bold">best model</strong>, and the data exploration notebook, labeled <strong class="bold">View data exploration notebook</strong>, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 4.12 – The AutoML experiment page with the links to view the notebook for the best model and the data exploration notebook" src="image/B16865_04_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – The AutoML experiment page with the links to view the notebook for the best model and the data exploration notebook</p>
			<p>The <a id="_idIndexMarker242"/>exploration notebook uses <strong class="source-inline">ydata-profiling</strong>, formerly referred to as pandas’ profiler library, to generate statistics and summarize data for all the fields in the table. It also provides alerts on fields with high correlation issues, which could negatively impact models. These warnings are also available in the MLflow experiment UI, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 4.13 – AutoML warnings called out during the experiment run" src="image/B16865_04_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – AutoML warnings called out during the experiment run</p>
			<p>Look through the data exploration notebook for an overview of your data, from summary statistics to thorough profiles for <span class="No-Break">each variable.</span></p>
			<p>We have now explored two <a id="_idIndexMarker243"/>tools for data exploration: Databricks Assistant and the <strong class="source-inline">ydata-profiling</strong> library. These are great places to start<a id="_idIndexMarker244"/> for many classical ML projects. Next, we’ll discuss a more advanced data format and how you can use the DI Platform to explore it: data embeddings and vector search. Embeddings are advanced transformations that translate complex, unstructured data into a numerical format conducive to ML algorithms, capturing intricate relationships within the data that are pivotal for<a id="_idTextAnchor198"/> <span class="No-Break">sophisticated models.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor199"/>Using embeddings to understand unstructured data</h1>
			<p>So far, we’ve focused on how to explore your structured data. What about unstructured data, such as images or text? Recall that we converted PDF text chunks into a specific format <a id="_idIndexMarker245"/>called embeddings in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>’s RAG chatbot project work. We require embeddings, meaning numerical vector representations of the data, to perform a similarity (or hybrid) search between chunks of text. That way, when someone asks our chatbot a question, such as “What are the economic impacts of automation technologies using LLMs?” the chatbot will be able to search through the stored chunks of text from the arXiv articles, retrieve the most relevant chunks, and use those to better answer the question. For more visual readers, see the data preparation workflow in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.14</em>. We completed the <strong class="bold">Data Preparation</strong> step in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. We’ll run through the remaining setup steps in the <span class="No-Break">workflow now.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 4.14 – Vector database setup is the prerequisite process supporting RAG’s retrieval step" src="image/B16865_04_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Vector database setup is the prerequisite process supporting RAG’s retrieval step</p>
			<p>Embeddings<a id="_idIndexMarker246"/> are an essential part of building any chatbot. Pay attention to your embedding model to ensure it is re<a id="_idTextAnchor200"/>levant to the task. You wouldn’t want to build a chatbot designed to answer questions in French but use an embedding model that only knows English – your chatbot’<a id="_idTextAnchor201"/>s response quality will <span class="No-Break">definitely suffer!</span></p>
			<p>Embeddings that<a id="_idIndexMarker247"/> capture the nuances of language are essential for the chatbot to understand and generate contextually relevant responses. Equally important are the searching and filtering techniques you apply to the<a id="_idTextAnchor202"/> <strong class="bold">vector database</strong> itself. A <a id="_idIndexMarker248"/>vector database is similar to a SQL database, but instead of storing tabular data, it stores vector embeddings. A search algorithm can then search the embeddings. In the final flow of a RAG project, a user’s question is also converted into embeddings, and the search algorithm uses those embeddings to find similar embeddings stored in the vector database. The chatbot receives the most similar embeddings from the vector database to help it craft a response to the <span class="No-Break">user’s question.</span></p>
			<p>Let’s consider the <a id="_idIndexMarker249"/>requirements of a good vector <span class="No-Break">database solution:</span></p>
			<ul>
				<li><strong class="bold">Quality of the retrievals</strong>: The correctness and completeness of embeddings returned as relevant by the <span class="No-Break">search algorithm</span></li>
				<li><strong class="bold">Scalability of the solution</strong>: The ability to scale per the number of requests coming to the application with <span class="No-Break">dynamic traffic</span></li>
				<li><strong class="bold">Accessibility</strong>: The ability to easily access, read, write to, and use the application in <span class="No-Break">real time</span></li>
				<li><strong class="bold">Governance</strong>: The ability to govern vector storage with the same access controls as the original sources used to create the vector embeddings <span class="No-Break">and models</span></li>
				<li><strong class="bold">Integration</strong>: The ability to easily integrate with current market technologies and eliminate time spent stitching technologies and <span class="No-Break">solutions together</span></li>
			</ul>
			<p>Using embeddings <a id="_idIndexMarker250"/>and vector search is a powerful way to improve a variety of ML projects. There are many uses for vector databases, the most common of which <a id="_idIndexMarker251"/>are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">RAG systems</strong>: Vector search facilitates efficient data retrieval, which is then used to augment a <strong class="bold">Large Language Model</strong> (<strong class="bold">LLM</strong>)’s response. Augmenting an LLM with results from vector search leads to more accurate chatbot responses and <a id="_idIndexMarker252"/>minimizes errors such as hallucinations in <span class="No-Break">LLM outputs.</span></li>
				<li><strong class="bold">Recommendation systems</strong>: E-commerce and streaming platforms use vector search for efficient nearest-neighbor searches, matching user behavior with <span class="No-Break">relevant suggestions.</span></li>
				<li><strong class="bold">Image and video recognition</strong>: Vector search facilitates quick searches for similar features in images <span class="No-Break">and videos.</span></li>
				<li><strong class="bold">Bioinformatics</strong>: Vector search can be applied to tasks such as DNA sequence alignment and protein structure similarity search <a id="_idTextAnchor203"/>to improve <span class="No-Break">clinical research.</span></li>
			</ul>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor204"/>Enhancing data retrieval with Databricks Vector Search</h1>
			<p>Databricks VS is <a id="_idIndexMarker253"/>transforming how we refine and retrieve data for LLMs. Functioning as a serverless similarity search engine, VS enables the storage of vector embeddings and metadata in a dedicated <a id="_idIndexMarker254"/>vector database. Through VS, you can generate dynamic vector search indices from <strong class="bold">Delta</strong> tables<a id="_idIndexMarker255"/> overseen by Unity Catalog. Using a straightforward API, you can retrieve the most similar vectors <span class="No-Break">through queries.</span></p>
			<p>Here are some of <a id="_idIndexMarker256"/>Databricks VS’s <span class="No-Break">key benefits:</span></p>
			<ul>
				<li><strong class="bold">Seamless integration</strong>: VS works harmoniously within Databricks’ ecosystem, particularly Delta tables. This integration ensures that your data is always up to date, making it model-ready for ML applications. With VS, you can create a vector search index from a source Delta table and set the index to sync when the source table <span class="No-Break">is updated.</span></li>
				<li><strong class="bold">Streamlined operations</strong>: VS significantly simplifies operational complexity by eliminating the need to manage third-party vector<a id="_idTextAnchor205"/> databases. VS runs on serverless compute, meaning Databricks handles the infrastructure management <span class="No-Break">for you.</span></li>
				<li><strong class="bold">Enhanced scalability</strong>: Unlike standalone vector libraries, VS offers unparalleled scalability. VS handles large-scale data effortlessly, scaling automatically<a id="_idIndexMarker257"/> to meet the demands of your data and query load. This scalability is crucial for organizations with vast amounts of data and complex <span class="No-Break">search requirements.</span></li>
				<li><strong class="bold">Unified data asset governance</strong>: VS integrates with Unity Catalog; Unity Catalog handles data governance and access control lists. To prevent productional <a id="_idIndexMarker258"/>data leakage, you<a id="_idIndexMarker259"/> can manage access to the Databricks VS API and the underlying databases with <span class="No-Break">Unity Catalog.</span></li>
				<li><strong class="bold">Model Serving integration</strong>: Model Serving automates querying of the model serving endpoint for embedding generat<a id="_idTextAnchor206"/>ion without any overhead <span class="No-Break">from users.</span></li>
			</ul>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor207"/>Flexibility in embedding model support</h2>
			<p>One of VS’s<a id="_idIndexMarker260"/> major strengths is its support for any of the embedding models of your choice. VS supports hosted or fully managed embeddings. Hosted embeddings are self-managed. You create the embeddings and save them on a Delta table. For fully managed embeddings, the prepared text is saved in a Delta table, and embeddings are created by Databricks Model Serving. Model Serving will convert your incoming data into embeddings with the model of your choice. Databricks VS can support any model through the Databricks Model Serving endpoints, the Foundation Model API (as mentioned in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>), or external models. External models include SaaS models, for example, OpenAI’s ChatGPT and Anthropic’s PaLM. You can connect external models through the Databricks unified model serving gateway. See <em class="italic">External models in Databricks Model Serving</em> in <em class="italic">F<a id="_idTextAnchor208"/>urther reading</em> for <span class="No-Break">more information.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor209"/>Setting up a vector search</h2>
			<p>As<a id="_idIndexMarker261"/> mentioned, VS is a serverless product. Hence, we require a real-time connection with our chatbot application to the relevant content stored in the vector database. We’ll cover this again in the <em class="italic">Applying our learning</em>, <em class="italic">Project – RAG chatbot</em> section, but if you’re ready to set up your own endpoint, you just need a few lines of code, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 4.15 – Creating a Databricks VS endpoint" src="image/B16865_04_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Creating a Databricks VS endpoint</p>
			<p>Once you create an endpoint, you can host multiple vector search indices under one endpoint. The endpoint will scale according to the demand. The code to host your first index is presented in the <em class="italic">Apply our </em><span class="No-Break"><em class="italic">learning</em></span><span class="No-Break"> section).</span></p>
			<p>There are limitations on the number of endpoints you can create indices per endpoint, and the embedding dimensions. Please review the documentation linked in <em class="italic">Further reading</em>, as Databricks may remove or update limits as the <span class="No-Break">product evolves.</span></p>
			<p>Technology moves fast, especially in the world of generative AI. Databricks VS was built even as we wrote this book, and we expect it to continue evolving to search not only through text but also images and audio with a more robust hybrid search engine in <span class="No-Break">the future.</span></p>
			<p>We’ve walked through various Databricks products and features geared toward helping you understand your data. Get ready to follow along in your own Databricks workspace as we work through the <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> code by project <a id="_idTextAnchor210"/>and put these concepts<a id="_idTextAnchor211"/><a id="_idTextAnchor212"/> <span class="No-Break">into practice.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor213"/>Applying our learning</h1>
			<p>It’s time to apply these concepts to our example projects. We will use what we have learned to explore each project dataset, from using Databricks Assistant to AutoML, to creating a vector <a id="_idTextAnchor214"/>s<a id="_idTextAnchor215"/>earch index and exploring <span class="No-Break">image data.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor216"/>Technical requirements</h2>
			<p>Before you begin, review, and prepare the technical requirements necessary for the hands-on work in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>We use the <strong class="source-inline">missingno</strong> library to address missing numbers in our synthetic transactions project <span class="No-Break">data: </span><a href="https://pypi.org/project/missingno/"><span class="No-Break">https://pypi.org/project/missingno/</span></a></li>
				<li>For the RAG project, you will need to install the following either on your cluster or in the <strong class="source-inline">CH4-01-Creating_VectorDB</strong> notebook. If you choose to install them in the notebook, the code is included <span class="No-Break">for you:</span><ul><li><span class="No-Break"><strong class="source-inline">typing_extensions==4.7.1</strong></span></li><li><span class="No-Break"><strong class="source-inline">transformers==4.30.2</strong></span></li><li><span class="No-Break"><strong class="source-inline">llama-index==0.9.3</strong></span></li><li><span class="No-Break"><strong class="source-inline">langchain==0<a id="_idTextAnchor217"/>.<a id="_idTextAnchor218"/>0.319</strong></span></li><li><span class="No-Break"><strong class="source-inline">unstructured[pdf,docx]==0.10.30</strong></span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor219"/>Project – Favorita Store Sales – time-series forecasting</h2>
			<p>For the<a id="_idIndexMarker262"/> Favorita Store Sales project, we use many simple DBSQL queries for data exploration and to understand the relationships between datasets. Additionally, we use the <strong class="source-inline">ydata_profiling</strong> library to produce data profiles in HTML format, as shown in<em class="italic"> </em><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.19</em>. To follow along in your own workspace, please refer to the <span class="No-Break">following notebooks:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH4-01-Exploring_Favorita_Sales_Data</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH4-02-Exploring_Autogenerated_Notebook</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH4-03-Imputing_Oil_Data</strong></span></li>
			</ul>
			<p>In the last chapter, we created tables of the Favorita Sales Forecasting dataset from Kaggle. Now it’s time to explore! Open up the first notebook, <strong class="source-inline">CH4-01-Exploring_Favorita_Sales_Data</strong>, to do some initial data exploration <span class="No-Break">in SQL:</span></p>
			<ol>
				<li>The first cell is a simple <strong class="source-inline">select *</strong> SQL command. After running the cell, focus on the in-cell <strong class="bold">Data Profile</strong> and <span class="No-Break"><strong class="bold">Visualization</strong></span><span class="No-Break"> options.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 4.16 – You can create visualizations and a data profile of a SQL query result in a notebook" src="image/B16865_04_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – You can create visualizations and a data profile of a SQL query result in a notebook</p>
			<ol>
				<li value="2">Choose <strong class="bold">Data Profile</strong> and investigate the information autogenerated about <span class="No-Break">the data.</span></li>
				<li>Choose <strong class="bold">Visualization</strong>. You can create a line chart with the date (month) versus the sum<a id="_idIndexMarker263"/> of sales and grouped by family. Notice that not all data is used to <a id="_idTextAnchor220"/>produce the visualization; we only see January at first. Once you save the visualization, the chart will display in the cell. The <strong class="bold">Truncated data</strong> message should be present at the bottom of the visualization. To increase the number of records in the chart, select <strong class="bold">Aggregate over </strong><span class="No-Break"><strong class="bold">more data</strong></span><span class="No-Break">.</span></li>
				<li>Continue to play with the options. In the next cell, we filter to the top-performing product families. Investigate how different or similar the <span class="No-Break">charts appear.</span></li>
			</ol>
			<p>Now that we’ve done some initial exploration of the Favorita data, we can run a Databricks AutoML experiment to generate a baseline model. AutoML can be launched in the UI, which we demonstrated earlier in this chapter in the <em class="italic">Generating data profiles with AutoML</em> section (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em>), or you can create an experiment via an API as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.17</em>. For this project, we will launch both a regression experiment and a forecasting experiment. Let’s start with the <span class="No-Break">regression run.</span></p>
			<p>Notice that at the top of the notebook, the default language is SQL rather than Python. Therefore, when we want to execute Python code, we need to include <strong class="source-inline">%python</strong> at the top of the cell. We use Python for AutoML in the last cell of the first notebook. We’ve set the <strong class="source-inline">timeout_minutes</strong> variable to <strong class="source-inline">30</strong>, so the experiment will run for up to 30 minutes. However, AutoML stops training models if the validation metric is no longer improving. In that case, the experiment will finish in less time. Once the run is complete, the notebook UI will display links to the MLflow experiment where the model versions are accessible, the <a id="_idIndexMarker264"/>best trial notebook with the best model’s code, and the data exploration notebook. Since this chapter focuses on exploring data, we will only open the data exploration notebook <span class="No-Break">for now.</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="Figure 4.17 – Creatin﻿g an AutoML experiment from a notebook" src="image/B16865_04_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Creatin<a id="_idTextAnchor221"/>g an AutoML experiment from a notebook</p>
			<h3>AutoML for data exploration</h3>
			<p>When <a id="_idIndexMarker265"/>you executed the final cell in <strong class="source-inline">CH4-01-Exploring_Favorita_Sales_Data</strong>, you received several links in the results, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.17</em>. Click on the link to the data exploration notebook (you can also open <strong class="source-inline">CH4-02-Exploring_Autogenerated_Notebook</strong> for a version that was autogenerated when we ran the AutoML experiment ourselves). Let’s look at <span class="No-Break">this notebook.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="Figure 4.18 – EDA notebook created with AutoML" src="image/B16865_04_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – EDA notebook created with AutoML</p>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.18</em> shows a <a id="_idIndexMarker266"/>portion of the automatically generated data exploration notebook. This notebook imports libraries and points to the training data. It also automatically converts the datetime columns to <span class="No-Break">pandas datetime.</span></p>
			<p>The notebook uses a pandas-based library, so the notebook limits the data to <span class="No-Break">10,000 rows.</span></p>
			<p>Next, the notebook imports the <strong class="source-inline">ydata_profiling</strong> library, along with three additional correlation calculations added in this case. The <strong class="source-inline">ydata_profiling</strong> library provides <a id="_idTextAnchor222"/>similar data to what we would get using <strong class="source-inline">summary()</strong> functions, such as details about missing and correlated data. The library is easily imported into a notebook for exploring data. Once finished, you can export the details to HTML or PDF for easy sharing. It’s a great timesaver when exploring <span class="No-Break">new datasets.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 4.19 – ydata_profiling in the AutoML-created notebook" src="image/B16865_04_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – ydata_profiling in the AutoML-created notebook</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.17</em>, we <a id="_idIndexMarker267"/>launched a regression experiment via the A<a id="_idTextAnchor223"/>PI by calling <strong class="source-inline">automl.regress()</strong>. Now, we create another experiment with a forecasting problem type using <span class="No-Break">the UI.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer119">
					<img alt="Figure 4.20 – Creating an A﻿utoML forecasting experiment in the UI" src="image/B16865_04_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20 – Creating an A<a id="_idTextAnchor224"/>utoML forecasting experiment in the UI</p>
			<p>When we create an AutoML forecasting experiment, we can incorporate a country’s holidays into the model under the <strong class="bold">Country Holidays</strong> option in the <strong class="bold">Advanced Configuration</strong> section. We will choose <strong class="bold">NONE</strong> for this project, because although many countries are present in the drop-down menu, Ecuador is not among them. <strong class="bold">NONE</strong> will ensure holidays are not included as a feature in <span class="No-Break">the model.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer120">
					<img alt="Figure 4.21 – AutoML forecasting experiment advanced configurations" src="image/B16865_04_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.21 – AutoML forecasting experiment advanced configurations</p>
			<p>Once all the<a id="_idIndexMarker268"/> fields are completed, click <strong class="bold">Start AutoML</strong> at the bottom of the screen to run the experiment (adjust the <strong class="bold">Timeout</strong> variable down if you want to ensure the run finishes within a given amount of time). When the experiment has finished running, you will see a new screen with a list of <span class="No-Break">generated models.</span></p>
			<p>We have now created an AutoML experiment in a notebook, used AutoML to generate a notebook for EDA, and used AutoML for time-series analysis. Databricks AutoML does a lot of the heavy lifting with minimal code for ML <span class="No-Break">and EDA.</span></p>
			<p>The Favorita sales dataset contains oil prices, which we think could impact sales, so let’s augment the autogenerated data exploration with some of our own to explore the oil<a id="_idTextAnchor225"/> price data and see how we can <span class="No-Break">use it.</span></p>
			<h3>Exploring and cleaning oil price data</h3>
			<p>To get <a id="_idIndexMarker269"/>started with the oil data, open the <strong class="source-inline">CH4-03-Imputing_Oil_Data</strong> notebook. We use the pandas API on Spark to <a id="_idIndexMarker270"/>view the data (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.22</em>). We reindex the data because it’s out of order and data for some dates is missing. Notice that initially, the data starts in May 2015 rather than January 2013. Also, the dates May 9th and May 10th appear to <span class="No-Break">be missing.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer121">
					<img alt="Figure 4.22 – View the oil price data provided by Kaggle in the Favorita Sales dataset" src="image/B16865_04_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.22 – View the oil price data provided by Kaggle in the Favorita Sales dataset</p>
			<p>If we add the <strong class="source-inline">date</strong> column<a id="_idIndexMarker271"/> as the index column, as shown in the notebook, you’ll see that the rows are then in order. However, January 5th and January 6th are missing. This occurs because stock prices are <a id="_idIndexMarker272"/>not given for holidays <span class="No-Break">or weekends.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer122">
					<img alt="Figure 4.23 – Reindex to include all dates an﻿d fill forward missing prices" src="image/B16865_04_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.23 – Reindex to include all dates an<a id="_idTextAnchor226"/>d fill forward missing prices</p>
			<p>We use the <strong class="source-inline">reindex()</strong> command to create an updated index based on the minimum and maximum dates in the oil price data. The reindex creates rows for all missing dates. When new rows are created for the missing dates, the prices are NaNs. We use the <strong class="source-inline">ffill()</strong> function, or forward fill, to update the DataFrame by filling in dates without prices with the price from the day before. January 1st, 2013, doesn’t have a previous day to fill from, so it <span class="No-Break">remains </span><span class="No-Break"><strong class="source-inline">NaN</strong></span><span class="No-Break">.</span></p>
			<p>Now, we have a clean and consistent silver table of oil prices that we can pull into our Favorita time-series models. In the next chapter, we will do feature e<a id="_idTextAnchor227"/>ngineering with the Favorita <span class="No-Break">sales data.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor228"/>Project – streaming transactions</h2>
			<p>The synthetic <a id="_idIndexMarker273"/>dataset does not need cleaning (since we created it), but we can still explore the data to understand it better. To follow along in your own workspace, please refer to the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH4-01-Exploring_Synthetic_Transactions_Data</strong></span></li>
			</ul>
			<p>Open the notebook to generate visualizations with the <strong class="source-inline">seaborn</strong> library, as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.24</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer123">
					<img alt="Figure 4.24 – Data visualizations of the synthetic transactions data" src="image/B16865_04_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.24 – Data visualizations of the synthetic transactions data</p>
			<p>Feel free to explore the transaction data further.<a id="_idTextAnchor229"/> Next, we’ll move on to the <span class="No-Break">RAG chatbot.</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor230"/>Project – RAG chatbot</h2>
			<p>In the last chapter, we<a id="_idIndexMarker274"/> extracted chunks of text from our PDF documents. We converted those chunks into <a id="_idTextAnchor231"/>embeddings using the BGE embedding model. As a result, we are now ready to take the next step in preparing our data for retrieval. To follow along in your own workspace, please refer to the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH4-01-Creating_VectorDB</strong></span></li>
			</ul>
			<p>We explained in the <em class="italic">Enhancing data retrieval with Databricks Vector Search</em> section that Databricks VS is a serverless managed solution. In other words, we need to create the VS endpoint to host <span class="No-Break">our indices.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer124">
					<img alt="Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot" src="image/B16865_04_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot</p>
			<p>As a reminder, we will ingest the source table we prepared in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> into the index for retrieval search within the <span class="No-Break">chatbot application.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer125">
					<img alt="Figure 4.26 – Reading our source table containing text from the documentation" src="image/B16865_04_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.26 – Reading our source table containing text from the documentation</p>
			<p>We import functions from the project’s <strong class="source-inline">utils</strong> folder, <strong class="source-inline">mlia_utils.rag_funcs</strong>. The index only needs to be created once. Going forward, we will only write into it or read from it. We use an <strong class="source-inline">if</strong>/<strong class="source-inline">else</strong> clause to check whether an index with the name <strong class="source-inline">catalog.database.docs_vsc_idx_cont</strong> exists or not. If it exists, we just synchronize <a id="_idIndexMarker275"/>our <strong class="source-inline">catalog.database.pdf_documentation_text</strong> source table <span class="No-Break">to it.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer126">
					<img alt="Figure 4.27 – Check whether the index exists and create it if needed" src="image/B16865_04_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.27 – Check whether the index exists and create it if needed</p>
			<p>There are a few key pieces we must make sure are in place before proceeding with the rest of our <span class="No-Break">chatbot project:</span></p>
			<ul>
				<li><strong class="bold">Source data preparation</strong>: <strong class="source-inline">catalog.database.pdf_documentation_text</strong> is the Delta table you have prepared with your self-managed embedding functions as the primary data repository for their vector <span class="No-Break">search index.</span></li>
				<li><strong class="bold">Vector search index creation</strong>: An index named <strong class="source-inline">catalog.database.docs_vsc_idx_cont</strong> using Databricks VS. This index is linked to the source Delta table. The index is set on a trigger base update (<strong class="source-inline">pipeline_type="TRIGGERED"</strong>), which means that any modifications or new additions to the historical texts should be synced manually to your vector search index. If you wish to have changes reflected in the vector search <a id="_idIndexMarker276"/>index automatically, choose the <strong class="source-inline">CONTINUOUS</strong> mode instead. This continuous update mechanism ensures the data is always current and ready <span class="No-Break">for analysis.</span></li>
				<li><strong class="bold">Embedding model integration</strong>: To transform the text data into a vector format, we are using self-managed embeddings. This means no model for embedding conversion is provided. However, you may want to choose managed embeddings, where <strong class="source-inline">text_chunks</strong> is automatically converted into embeddings with a model that is served through Databricks Model Serving. This can be specified in the setup under the <strong class="source-inline">embedding_model_endpoint_name</strong> parameter. This integration guarantees that the textual data is efficiently converted into vectors, making it suita<a id="_idTextAnchor232"/>ble for advanced <span class="No-Break">similarity searches.</span></li>
			</ul>
			<p>We import the <strong class="source-inline">wait_for_index_to_be_ready()</strong> function from the <strong class="source-inline">utils</strong> folder. We run the code in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.28</em> to repeatedly check the index’s status until it is in an “online” state. As the VS index only needs to be created once, this function can take some time before the embeddings reach your VS index. Proceed once your index is in a “<span class="No-Break">ready” state.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer127">
					<img alt="Figure 4.28 – The wait for index function checks for the index status until it is online and ready" src="image/B16865_04_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.28 – The wait for index function checks for the index status until it is online and ready</p>
			<p>Once the index is online, we call <strong class="source-inline">get_index()</strong> to perf<a id="_idTextAnchor233"/>orm a similarity search. We also call a <strong class="source-inline">describe()</strong> method to show you the broader API options <span class="No-Break">you have.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer128">
					<img alt="Figure 4.29 – Using get_index with the describe method shows the index configuration" src="image/B16865_04_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.29 – Using get_index with the describe method shows the index configuration</p>
			<p>Because we<a id="_idIndexMarker277"/> use self-managed embeddings, our VS index is not connected to any model that can convert our input text query into embeddings to be mapped to the one under a database, so we need to convert them first! Here we are again leveraging the BGE embedding model from the Foundational Model Serving API and then passing embedded text to our index for <span class="No-Break">similarity search:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer129">
					<img alt="Figure 4.30 – Converting our query text into embeddings and passing to the index for similarity search" src="image/B16865_04_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.30 – Converting our query text into embeddings and passing to the index for similarity search</p>
			<p>Here is the result of the retrieved queries (only the first found one with two columns <strong class="source-inline">pdf_name</strong> <span class="No-Break">and content):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer130">
					<img alt="Figure 4.31– Sample of result of the retrieved query" src="image/B16865_04_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.31– Sample of result of the retrieved query</p>
			<p>Now, our VS index is<a id="_idIndexMarker278"/> ready to be used in our chatbot. We will explore how to connect all the tools together to generate a prop<a id="_idTextAnchor234"/>er human-readable answer in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">!</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor235"/>Project – multilabel image classification</h2>
			<p>In the last chapter, we <a id="_idIndexMarker279"/>saved our image data into volumes. Next, we will explore our data. To follow along in your own workspace, please refer to the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH4-01-Exploring_Dataset</strong></span></li>
			</ul>
			<p>We create our training dataset of images. Print the labels and a sample of the training data for a look at what we want our model <span class="No-Break">to classify.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer131">
					<img alt="Figure 4.32 – Load and view the training dataset" src="image/B16865_04_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.32 – Load and view the training dataset</p>
			<p>We can use <strong class="source-inline">display_image</strong> to view<a id="_idIndexMarker280"/> a few pictures from <span class="No-Break">our volumes.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer132">
					<img alt="Figure 4.33 – Display images within the notebook" src="image/B16865_04_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.33 – Display images within the notebook</p>
			<p>It’s good to have an idea of the proportion of the different labels of our data. This is how you can view the proportion of data in our training dataset. Wherever you perform a multilabel<a id="_idIndexMarker281"/> classification, make sure you have a good distribution of labels. Otherwise, consider training individual models that might be combined or augment <span class="No-Break">missing labels!</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer133">
					<img alt="Figure 4.34 – View the proportion of labels in the training dataset" src="image/B16865_04_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.34 – View the proportion of labels in the training dataset</p>
			<p>Now we have done some high-level exploration of our image classification dataset. For this project, no transformations are needed, so our next ste<a id="_idTextAnchor236"/>p with this data will be in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor237"/>Summary</h1>
			<p>It is critical to understand your data before using it. This chapter highlighted a variety of methods to explore and analyze our data within the <span class="No-Break">Databricks ecosystem.</span></p>
			<p>We began by revisiting DLT, this time focusing on how we use a feature called <strong class="bold">expectations</strong> to monitor and improve our data quality. We also introduced Databricks Lakehouse Monitoring as another tool for observing data quality. Among its many capabilities, Lakehouse Monitoring detects shifts in data distribution and alerts users to anomalies, thus preserving data integrity throughout its life cycle. We used Databricks Assistant to explore data with ad hoc queries written in English and showed why AutoML is an extremely useful tool for data exploration by automatically creating comprehensive data exploration notebooks. Together, all of these tools create a strong foundation to understand and explore your data. Finally, the chapter delved into Databricks VS and how using it to find similar documents can improve <span class="No-Break">chatbot responses.</span></p>
			<p>We have now set the foundation for the next phase of our data journey. <a href="B16865_05.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> will focus on how to build upon our bronze-layer data to create rich sets of featu<a id="_idTextAnchor238"/>res for data science and <span class="No-Break">ML projects.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor239"/>Questions</h1>
			<p>The following questions are meant to solidify key points to remember as well as tie the content back to your <span class="No-Break">own experience:</span></p>
			<ol>
				<li>What are some low-code options for data exploration that we discussed in <span class="No-Break">this chapter?</span></li>
				<li>When might you use Databricks Assistant for data exploration, and when might you use AutoML’s data <span class="No-Break">profile notebook?</span></li>
				<li>How and why would you set expectations on <span class="No-Break">your data?</span></li>
				<li>When would you use a regular database versus a vector database? What are some <a id="_idTextAnchor240"/>common use cases for <span class="No-Break">vector databases?</span></li>
			</ol>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor241"/>Answers</h1>
			<p>After putting thought into the questions, compare your answers <span class="No-Break">to ours:</span></p>
			<ol>
				<li>Some low-code data exploration options include using the <strong class="source-inline">ydata</strong> library, in-cell data profile, Databricks Assistant, <span class="No-Break">and AutoML.</span></li>
				<li>Databricks Assistant is useful for data exploration when you have a good idea of the analyses you want to build and you want code assistance. Databricks Assistant is a great way to speed up the coding process or augment your SQL knowledge. On the other hand, AutoML is very useful for automatically creating a profile notebook that broadly covers <span class="No-Break">your dataset.</span></li>
				<li>We would use Delta Live Tables to set expectations. Expectations are a way to flexibly handle data abnormalities and give the options to report bad data, drop that data, or fail the <span class="No-Break">pipeline entirely.</span></li>
				<li>Regular databases, or relational databases, are designed for data in tabular form, typically organized in rows or columns. A vector database is designed to store vector data, such as embeddings and high-dimensional data. Vector databases are optimized for operations based on vector space models, similarity searches, image and video analysis, and other ML problems. Some common use cases include <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>) systems, recommendation sys<a id="_idTextAnchor242"/>tems, and image and <span class="No-Break">video recognition.</span></li>
			</ol>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor243"/>Further reading</h1>
			<p>In this chapter, we pointed out specific technologies, technical features, and options. Please take a look at these resources to get deeper into the areas that interest <span class="No-Break">you most:</span></p>
			<ul>
				<li><em class="italic">Enabling visualizations with Aggregations in </em><span class="No-Break"><em class="italic">DBSQL</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization"><span class="No-Break">https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization</span></a></li>
				<li>Using the ydata profiler to explore <span class="No-Break">data: </span><a href="https://ydata-profiling.ydata.ai/docs/master/index.html"><span class="No-Break">https://ydata-profiling.ydata.ai/docs/master/index.html</span></a></li>
				<li><em class="italic">Advancing Spark - Meet the new Databricks </em><span class="No-Break"><em class="italic">Assistant</em></span><span class="No-Break">: </span><a href="https://youtu.be/Tv8D72oI0xM"><span class="No-Break">https://youtu.be/Tv8D72oI0xM</span></a></li>
				<li><em class="italic">Introducing Databricks Assistant, a context-aware AI </em><span class="No-Break"><em class="italic">assistant</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/introducing-databricks-assistant"><span class="No-Break">https://www.databricks.com/blog/introducing-databricks-assistant</span></a></li>
				<li><em class="italic">Model monitoring custom metrics </em><span class="No-Break"><em class="italic">creation</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html"><span class="No-Break">https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html</span></a></li>
				<li>Databricks Vector <span class="No-Break">Search: </span><a href="https://docs.databricks.com/en/generative-ai/vector-search.html"><span class="No-Break">https://docs.databricks.com/en/generative-ai/vector-search.html</span></a></li>
				<li>External models in Databricks Model <span class="No-Break">Serving: </span><a href="https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/"><span class="No-Break">https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/</span></a></li>
			</ul>
		</div>
	</body></html>