<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up"><div class="book" id="2QJ5E2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13" class="calibre1"/>Chapter 13. Scaling Up</h1></div></div></div><p class="calibre8">Up until now, we have reviewed a steady stream of pertinent topics concerning <span class="strong"><em class="calibre9">statistics</em></span> and specifically, <span class="strong"><em class="calibre9">predictive analytics</em></span>. In this chapter, we look to provide a tutorial dedicated to applying those concepts and practices to very large datasets. First, we'll begin by defining the phrase very large – at least as it is used to describe data defined (that we want to train our predictive models on or run our statistical algorithms against). Next, we will review the list of the challenges imposed by using bigger data sources, and finally, we will offer some ideas for meeting these challenges.</p><p class="calibre8">Our chapter is broken down into the following sections:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Getting started</li><li class="listitem">The phases of an analytics project</li><li class="listitem">Experience and data of scale</li><li class="listitem">The characteristics of big data</li><li class="listitem">Training models at scale</li><li class="listitem">The specific challenges (of big data)</li><li class="listitem">A path forward</li></ul></div></div>

<div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up">
<div class="book" title="Starting the project"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch13lvl1sec88" class="calibre1"/>Starting the project</h1></div></div></div><p class="calibre8">The phases <a id="id931" class="calibre1"/>of a general purpose predictive analytics project may be straightforward and perhaps easy (it's the practice of carrying out each of these phases effectively that is challenging).</p><div class="mediaobject"><img src="../images/00212.jpeg" alt="Starting the project" class="calibre10"/><div class="caption"><p class="calibre28">The Phases of a predictive analytics project</p></div></div><p class="calibre11"> </p><p class="calibre8">These <a id="id932" class="calibre1"/>phases are:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre2">Define</strong></span> (the data).</li><li class="listitem" value="2"><span class="strong"><strong class="calibre2">Profile &amp; Prepare</strong></span> (the data).</li><li class="listitem" value="3"><span class="strong"><strong class="calibre2">Determine the Question</strong></span> (what to predict).</li><li class="listitem" value="4"><span class="strong"><strong class="calibre2">Choose</strong></span> the algorithm.</li><li class="listitem" value="5"><span class="strong"><strong class="calibre2">Apply</strong></span> the model.</li></ol><div class="calibre13"/></div></div></div>

<div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up">
<div class="book" title="Starting the project">
<div class="book" title="Data definition"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch13lvl2sec106" class="calibre1"/>Data definition</h2></div></div></div><p class="calibre8">An <a id="id933" class="calibre1"/>interesting thought:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre29">"…Once you have enough data, you start to see patterns," he said. "You can build a model of how these data work. Once you build a model, you can predict…"</p><p class="calibre29">                                                                                         – Bertolucci, 2013</p></blockquote></div><p class="calibre8">At the beginning of any (and every) analytics project, data is defined – reviewed and analyzed: source, format, state, interval, and so on (some refer to this as the process of investigating the breadth and depth of available data).</p><p class="calibre8">One exercise demanded is to perform what is referred to as profiling the data source, or to establishing your data's profile by determining its characteristics, relationships, and patterns (and context). This process will, hopefully, produce a clearer view of the content and quality the data to be used in the project – that is, the data profile.</p><p class="calibre8">Then, after the exercise of profiling is completed, one would most likely proceed with performing some form of data scrubbing (this is also sometimes referred to as cleansing or in <a id="id934" class="calibre1"/>some cases preparing) in an effort to improve its level of quality. During the process of cleansing or scrubbing your data, you would most likely perform tasks such as aggregation, appending, merging, reformatting fields, changing variable types or adding missing values, and so on.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note51" class="calibre1"/>Note</h3><p class="calibre8">Data profiling techniques can include specific analysis types such as a <span class="strong"><em class="calibre9">univariate analysis</em></span> which involves frequency analysis for categorical variables and understanding distribution and summary statistics for continuous variables. This aids in missing value treatment, understanding distribution, and outlier treatment.</p></div></div></div></div>

<div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up">
<div class="book" title="Starting the project">
<div class="book" title="Experience"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch13lvl2sec107" class="calibre1"/>Experience</h2></div></div></div><p class="calibre8">When <a id="id935" class="calibre1"/>soliciting advice from a <span class="strong"><strong class="calibre2">subject matter expert</strong></span> (<span class="strong"><strong class="calibre2">SME</strong></span>), one would probably likely agree that an individual with more experience <a id="id936" class="calibre1"/>most likely will be able to provide a better service. With predictive analytics projects, the objective is not what the data can tell us, but what the data can tell us about an objective or problem, therefore, the size or amount of the data source (the amount of experience) available for the project becomes much more important. Typically, the more the data, the better.</p><p class="calibre8">So, at what point is it acceptable to say you have enough data for your predictive project? The politically correct answer to this question is that it depends. Some types of data science and predictive analysis projects require more specific data requirements than others will, effectively setting what the minimal data volumes might be.</p><p class="calibre8">In an extreme case, predicting may require data spanning many years or even many decades – as larger amounts of data can yield a breadth of patterns surrounding behaviors <a id="id937" class="calibre1"/>and decisions, and so on. Why? Because typically, analyzing (or training a model with) more data develops a more comprehensive understanding or a <span class="strong"><strong class="calibre2">better prediction</strong></span>.</p><p class="calibre8">With this in mind, perhaps a general rule of thumb is to collect as much data as possible (depending upon the objective or type of application). Some experts might suggest collecting at least three years ', and preferably five years', worth of data before beginning any predictive analysis project. Of course, years may not be the appropriate measure depending upon the type of application. For example, cases might be more appropriate or lines of text, and so on.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note52" class="calibre1"/>Note</h3><p class="calibre8">In practice, if an application was built around hospital visits, the more patient cases (typically millions) the better; a word predictor application would want to have as many text sentences or word phrases (tens of millions) as it could (to be effective).</p></div><p class="calibre8">Another predictive analytics data controversy might be understanding the idea of <span class="strong"><em class="calibre9">sufficient</em></span> versus <span class="strong"><em class="calibre9">enough</em></span>.</p><p class="calibre8">In some cases, given a shortage of volume or <span class="strong"><em class="calibre9">quantity</em></span>, the wise data scientist would always focus on the <span class="strong"><em class="calibre9">quality</em></span> or suitability of the data. This means that even though the volume of <a id="id938" class="calibre1"/>data is less than hoped for, the quality of the data, based upon the objective of the project, is deemed sufficient.</p><p class="calibre8">Given an understanding of all of the preceding points, it is important to gauge your data to determine if the volume of your data has reached the tipping point – that point where typical analytical activities begin to become onerous to perform.</p><p class="calibre8">In the next section, we will cover how to establish that data volume tipping point as it is always better to understand and expect challenges before you begin your heavy model training rather than finding out the hard way, after you've already begun.</p></div></div></div>

<div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up">
<div class="book" title="Starting the project">
<div class="book" title="Data of scale – big data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch13lvl2sec108" class="calibre1"/>Data of scale – big data</h2></div></div></div><p class="calibre8">When we <a id="id939" class="calibre1"/>use the phrase, data of scale we are not referring to the statistical measurement scales of interval, ordinal, nominal, and dichotomous. We are using the phrase loosely to convey the size, volume, or complexity of the data source to be used in your analytics project.</p><p class="calibre8">The, by <a id="id940" class="calibre1"/>now, well-known buzz word, <span class="strong"><strong class="calibre2">big data</strong></span> might (loosely) fit here, so let us take pause here to define how we are using the term big data.</p><p class="calibre8">A large assemblage of data, datasets that are so large or complex that traditional data processing applications are inadequate, and data about every aspect of our lives have all been used to define or refer to big data.</p><p class="calibre8">The following diagram illustrates big data's three v's:</p><div class="mediaobject"><img src="../images/00213.jpeg" alt="Data of scale – big data" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In 2001, then Gartner analyst <span class="strong"><em class="calibre9">Doug Laney</em></span> introduced the 3Vs concept to describe the occurrence of big data. The 3Vs, according to <span class="strong"><em class="calibre9">Laney</em></span>, are volume, variety, and velocity. The Vs <a id="id941" class="calibre1"/>make up the dimensionality of big data: volume (or the measurable amount of data), variety (meaning the number of types of data), and <a id="id942" class="calibre1"/>velocity (referring to the speed of processing or dealing with that data).</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note53" class="calibre1"/>Note</h3><p class="calibre8">Laney's explanation can be reviewed here: <a class="calibre1" href="http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf">http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf</a>).</p></div><p class="calibre8">Using the <span class="strong"><em class="calibre9">volume</em></span>, <span class="strong"><em class="calibre9">variety</em></span>, and <span class="strong"><em class="calibre9">velocity</em></span> concept, it is easier to foresee how a <span class="strong"><em class="calibre9">big data</em></span> source can be or quickly become increasingly challenging to work with, and as these dimensions' increase or expand they will only encumber the ability to effectively train predictive models on the data further.</p></div></div></div>

<div class="book" title="Chapter&#xA0;13.&#xA0;Scaling Up">
<div class="book" title="Starting the project">
<div class="book" title="Using Excel to gauge your data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch13lvl2sec109" class="calibre1"/>Using Excel to gauge your data</h2></div></div></div><p class="calibre8">Microsoft <a id="id943" class="calibre1"/>Excel is not a tool to be used to determine if your data qualifies as big data.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">If your data is too big for Microsoft Excel it still really doesn't necessarily qualify as big data.</strong></span> In fact, gigabytes of data, still manageable with various techniques, enterprise, and even open source tools, especially with the lower cost of storage today.</p><p class="calibre8">It is important to be able to realistically size or scale the data technology (keeping in mind expected data growth rates) you will be using in your predictive project before selecting an approach or even beginning any profiling or preparing work effort. This time is well spent as it will save time later that may be lost due to performance bottlenecks or rewriting scripts to use a different approach (one that can handle bigger data sources).</p><p class="calibre8">So, the question becomes, how do you gauge your data – is it really big data? Is it manageable? Or does it fall into that category that will require special handling or pre-processing before it can be effectively used for your predictive analytics objective?</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13lvl1sec89" class="calibre1"/>Characteristics of big data</h1></div></div></div><p class="calibre8">For you <a id="id944" class="calibre1"/>to determine if your data source qualifies as big data or as needing special handling, you can start by examining your data source in the following areas:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">The volume (amount) of data.</li><li class="listitem" value="2">The variety of data.</li><li class="listitem" value="3">The number of different sources and spans of the data.</li></ol><div class="calibre13"/></div><p class="calibre8">Let's <a id="id945" class="calibre1"/>examine each of these areas.</p></div>

<div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Volume"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch13lvl2sec110" class="calibre1"/>Volume</h2></div></div></div><p class="calibre8">If you <a id="id946" class="calibre1"/>are talking about the number of rows or records, then most likely your data source is not a big data source since big data is typically measured in gigabytes, terabytes, and petabytes. However, space doesn't always mean big, as these size measurements can vary greatly in terms of both volume and functionality. Additionally, data sources of several million records may qualify as big data, given their structure (or lack of structure).</p></div></div>

<div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Varieties"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch13lvl2sec111" class="calibre1"/>Varieties</h2></div></div></div><p class="calibre8">Data used <a id="id947" class="calibre1"/>in predictive models may be structured or unstructured (or both) and include transactions from databases, survey results, website logs, application messages, and so on (by using a data source consisting of a higher variety of data, you are usually able to cover a broader context for the analytics you derive from it). Variety, much like volume, is considered a normal qualifier for big data.</p></div></div>

<div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Sources and spans"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch13lvl2sec112" class="calibre1"/>Sources and spans</h2></div></div></div><p class="calibre8">If the <a id="id948" class="calibre1"/>data source for your predictive analytics project is the result of integrating several sources, you most likely hit on both criteria of volume and variety and your data qualifies as big data. If your project uses data that is affected by governmental mandates, consumer requests is a historical analysis, you are almost certainty using big data. Government regulations usually require that certain types of data need to be stored for several years. Products can be consumer driven over the lifetime of the product and with today's trends, historical analysis data is usually available for more than five years. Again, all examples of big data sources.</p></div></div>

<div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Structure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch13lvl2sec113" class="calibre1"/>Structure</h2></div></div></div><p class="calibre8">You will <a id="id949" class="calibre1"/>often find that data sources typically fall into one of the following three categories:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Sources with little or no structure in the data (such as simple text files).</li><li class="listitem" value="2">Sources containing both structured and unstructured data (like data that is sourced from document management systems or various websites, and so on).</li><li class="listitem" value="3">Sources containing highly structured data (like transactional data stored in a relational database example).</li></ol><div class="calibre13"/></div><p class="calibre8">how your data source is categorized will determine how you prepare and work with your data in each phase of your predictive analytics project.</p><p class="calibre8">Although <a id="id950" class="calibre1"/>data sources with structure can obviously still fall into the category of big data, it's data containing both structured and unstructured data (and of course totally unstructured data) that fit as big data and will require special handling and or pre-processing.</p></div></div>

<div class="book" title="Characteristics of big data" id="2RHM01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Statistical noise"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch13lvl2sec114" class="calibre1"/>Statistical noise</h2></div></div></div><p class="calibre8">Finally, we <a id="id951" class="calibre1"/>should take a note here that other factors (other than those discussed already in the chapter) can qualify your project data source as being unwieldy, overly complex, or a big data source.</p><p class="calibre8">These include (but are not limited to):</p><div class="book"><ul class="itemizedlist"><li class="listitem">Statistical noise (a term for recognized amounts of unexplained variations within the data)</li><li class="listitem">Data suffering from mismatched understandings (the differences in interpretations of the data by communities, cultures, practices, and so on)</li><li class="listitem">And others</li></ul></div><p class="calibre8">Once you have determined that the data source that you will be using in your predictive analytics project seems to qualify as big (again as we are using the term here) then you can proceed with the process of deciding how to manage and manipulate that data source, based upon the known challenges this type of data demands, so as to be most effective.</p><p class="calibre8">In the next section, we will review some of these common problems, before we go on to offer useable solutions.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Training models at scale" id="2SG6I1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13lvl1sec90" class="calibre1"/>Training models at scale</h1></div></div></div><p class="calibre8">In an <a id="id952" class="calibre1"/>earlier section of this chapter, we listed and studied what the industry experts agree on as the most common phases of any predictive analytics project.</p><p class="calibre8">To recall, they are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Defining the data source</li><li class="listitem">Profiling and preparation of the data source</li><li class="listitem">Determining the question(s) that you want to ask your data</li><li class="listitem">Choosing an algorithm to train on the data source</li><li class="listitem">Application of a predictive model</li></ul></div><p class="calibre8">In a predictive analytics project using big data, those same phases are present, but may be slightly <a id="id953" class="calibre1"/>varied and require some supplementary efforts.</p></div>

<div class="book" title="Training models at scale" id="2SG6I1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Pain by phase"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch13lvl2sec115" class="calibre1"/>Pain by phase</h2></div></div></div><p class="calibre8">In the <a id="id954" class="calibre1"/>initial phase of a project, once you've chosen a source for your data (determined the data source), the data must be attained. Some industry experts describe this as the acquisition and recording of data. In a predictive project that involves a more common data source, access to the data might be as straightforward as opening a file on your local disk; with a big data source, it's a bit more difficult. For example, suppose your project sources data from a combination of devices (multiple servers and many mobile devices, that is, the Internet of Things data). </p><p class="calibre8">This activity-generated data might include a combination of website tracking information, application logs, sensor data – among other machine-generated content – perfect for your analysis. You can see how the effort to access this information as a single data source for your project would take some effort (and expertise!).</p><p class="calibre8">In the profiling and preparation phase, data is extracted, cleaned, and annotated. Typically, any analytics project will require this pre-processing of the data: setting context, identifying operational definitions and statistical types, and so on. This step is critical as this is the phase where we establish an understanding of the data challenges so that later surprises can be minimized. This phase usually involves time spent querying and re-querying the data, creating visualizations to validate findings, and then performing updates to the data to address areas of concern. Big data inhibits these activities since it includes either more data to process, is perhaps inconsistent in format, and could be changing rapidly.</p><p class="calibre8">In the phase where question determination takes place, data integration, aggregation, and the representation of the data must be considered so that the proper questions to ask the data can be identified. This phase may be divided into three steps; preparation, integration, and determination (of questions). The prep step involves assembling the data, identification of unique keys, aggregation/duplication, scrubbing as required, format manipulation, and perhaps mapping of values. The integration step involves merging data, testing, and reconciliation. Finally, project questions are established. Once again, big data's volumes, varieties, and velocities can slow this phase down considerably.</p><p class="calibre8">Choosing an algorithm and application of a predictive model are the phases where there is analysis, modeling, and interpretation of the data. Considering the volumes, varieties, and velocities of a big data source, selecting the appropriate algorithm to be used to train data can be much more involved. An example would be the idea that predictive modeling works best given the lowest level of granularity possible and, in the previous phase, perhaps the sheer volume of a big data source required that extensive aggregation be done, thus potentially burying anomalies and variations that exist within the data.</p></div></div>

<div class="book" title="Training models at scale" id="2SG6I1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Specific challenges"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch13lvl2sec116" class="calibre1"/>Specific challenges</h2></div></div></div><p class="calibre8">Let's take <a id="id955" class="calibre1"/>a few moments to address some very specific challenges brought on by big data. Among these top topics are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Heterogeneity</li><li class="listitem">Scale</li><li class="listitem">Location</li><li class="listitem">Timeliness</li><li class="listitem">Privacy</li><li class="listitem">Collaborations</li><li class="listitem">Reproducibility</li></ul></div><div class="book" title="Heterogeneity"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec14" class="calibre1"/>Heterogeneity</h3></div></div></div><p class="calibre8">By variety, we usually need to consider the heterogeneity of data types, representation, and <a id="id956" class="calibre1"/>semantic interpretation. Efforts to correctly review and understand these variations in big data sources can be time consuming and complex. Interestingly, an element may be homogeneous (more uniform) on a larger scale, compared to being heterogeneous (less uniform) on a smaller scale. This means that your approach to addressing a big data source may cause very different results!</p></div><div class="book" title="Scale"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec15" class="calibre1"/>Scale</h3></div></div></div><p class="calibre8">We've already <a id="id957" class="calibre1"/>touched on the idea of scale – typically scale refers to the sheer size of the data source, but could also refer to its complexities.</p></div><div class="book" title="Location"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec16" class="calibre1"/>Location</h3></div></div></div><p class="calibre8">Typically, you'll <a id="id958" class="calibre1"/>see that when you decide to use a big data source, it is not located all in one place, but spread throughout electronic space. This means that any process (manual or automated) will have to consolidate the data – physically or virtually before it can be properly used in a project.</p></div><div class="book" title="Timeliness"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec17" class="calibre1"/>Timeliness</h3></div></div></div><p class="calibre8">The bigger <a id="id959" class="calibre1"/>the data, the more time it will take to analyze. However, it is not just this time that is meant when one speaks of velocity in the context of big data. Rather, there is the challenge of the acquisition rate of the data. In other words, with data piling up or being updated continuously within the data source, when (or how often) is the correct snapshot established? In addition, scanning the entire data source to find a suitable sample pertinent to a particular predictive analytics objective is obviously impractical.</p></div><div class="book" title="Privacy"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec18" class="calibre1"/>Privacy</h3></div></div></div><p class="calibre8">Data <a id="id960" class="calibre1"/>privacy should be a consideration when using any data source, and one that increases in complexity in the context of big data. The most well-known example of this is with electronic health records – which have strict laws governing them.</p><p class="calibre8">Suppose, for example having the requirement to pre-process a big data source that is over a terabyte in size to hide both a user's identify and location information?</p></div><div class="book" title="Collaborations"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec19" class="calibre1"/>Collaborations</h3></div></div></div><p class="calibre8">One might <a id="id961" class="calibre1"/>think that in this day and age, analytics and predictive models are entirely computational (especially when you hear the term machine learning), however, no matter how advanced a predicative algorithm or model proclaims to be, there remain many patterns in data that humans can simply detect, but computer algorithms no matter how complex in their logic, have a hard time finding.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note54" class="calibre1"/>Note</h3><p class="calibre8">There is a new following in analytics that may be considered a sub-field of <span class="strong"><em class="calibre9">visual</em></span> analytics that utilizes SME input, at least with respect to the modelling and analysis phase of a predictive project.</p></div><p class="calibre8">To include a subject matter expert in a predictive analytics project might not be a huge issue but, with a big data source, it often takes multiple experts from different domains to really understand what is going on with the data and to share their respective exploration of results and advice.</p><p class="calibre8">These multiple experts may be separated in space and time and be difficult to assemble in one location at one time. Again, causing additional time and effort to be spent.</p></div><div class="book" title="Reproducibility"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch13lvl3sec20" class="calibre1"/>Reproducibility</h3></div></div></div><p class="calibre8">Believe <a id="id962" class="calibre1"/>it or not, most predictive analytics projects are repeated for a variety of reasons. For example, if the results are in question for any reason or if the data is suspect, all of the phases of the project may be repeated. Reproduction of a big data analytics project is seldom reasonable. In most cases, all that can be done is that the bad data in a big data resource will be found and flagged as such.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="A path forward"><div class="book" id="2TEN42-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13lvl1sec91" class="calibre1"/>A path forward</h1></div></div></div><p class="calibre8">So, the <a id="id963" class="calibre1"/>inkling of having more than enough data for training a model seems very appealing.</p><p class="calibre8">Big data sources would appear to answer this desire, however in practice, a big data source is not often (if ever) analyzed in its entirety. You can pretty much count on performing a sweeping filtering process aimed to reduce the big data into small(er) data (more on this in the next section).</p><p class="calibre8">In the <a id="id964" class="calibre1"/>following section, we will review various approaches to addressing the various challenges of using big data as a source for your predictive analytics project.</p></div>

<div class="book" title="A path forward">
<div class="book" title="Opportunities"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch13lvl2sec117" class="calibre1"/>Opportunities</h2></div></div></div><p class="calibre8">In this <a id="id965" class="calibre1"/>section, we offer a few recommendations for handling big data sources in predictive analytic projects using R. Also, we'll offer some practical use case examples.</p></div></div>

<div class="book" title="A path forward">
<div class="book" title="Bigger data, bigger hardware"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch13lvl2sec118" class="calibre1"/>Bigger data, bigger hardware</h2></div></div></div><p class="calibre8">We are <a id="id966" class="calibre1"/>starting with the most obvious option first.</p><p class="calibre8">To be <a id="id967" class="calibre1"/>clear, R keeps all of its objects in memory, which is a limitation if the data source gets too large. One of the easiest ways to deal with big data in R is simply to increase the machine's memory.</p><p class="calibre8">At the time of writing, R can use 8 TB of RAM if it runs on a 64-bit machine (compared to only 2 GB addressable RAM on 32-bit machines). Most machines used for predictive analytics projects are (at least should be) 64-bit already, so you just need to add RAM.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note55" class="calibre1"/>Note</h3><p class="calibre8">There are both 32-bit and 64-bit versions of R. Do yourself a favor and use the 64-bit version!</p></div><p class="calibre8">If you know your data source well and have added appropriate amounts of memory to your machine, then you'll most likely be OK to work with a big data source efficiently, especially if you use one of the approaches outlined in the following sections of this chapter.</p></div></div>

<div class="book" title="A path forward">
<div class="book" title="Breaking up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch13lvl2sec119" class="calibre1"/>Breaking up</h2></div></div></div><p class="calibre8">One of <a id="id968" class="calibre1"/>the most straightforward and proven approaches to taming a big data source with R (or any language for that matter) is to create workable subsets of data prepared from the big data resource.</p><p class="calibre8">For example, suppose we have patient health records making up a current big data source. There are literally trillions of patient case records in the data with more added almost every minute. These cases record both the basics (sex, age, height, weight, and so on) as well as specifics around the patient's background (such as if the patient is a smoker, drinker, currently on medications, has ever been operated on, and so on). Luckily, our file does not contain any information that can be used to identify the patient (such as name or social security number) so we won't be in violation of any laws.</p><p class="calibre8">The data <a id="id969" class="calibre1"/>source is fed by hospitals and doctors' offices all over the country. Our predictive project is one that is looking to determine relationships between a patient's health and the state that they live in. Rather than attempting to train on all of the data (a mostly impractical effort), we can use some logic to prepare our series of smaller, more workable subsets. For example, we could simply separate out our overall data source into 50 smaller files – one for each state. This would help, but the smaller files may still be massive, so with a little profiling of the data, we may be able to identify other measurements that we can use to divide our data.</p><p class="calibre8">The process of data discovery and separation might look pretty close to the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Since we are dealing with a big data source and are not sure of the number of cases or records within the file, we can start by creating an R data object from our comma separated file and restricting the number of records to be read:<div class="informalexample"><pre class="programlisting">x&lt;-read.table(file="HCSurvey20170202.txt", sep=",", nrows=150)</pre></div></li><li class="listitem" value="2"><code class="email">x</code> now contains 150 records that we can review looking for interesting measures we may be able to use to logically split our data on. You can also utilize the summary function to evaluate variables within the data source. For example, we see that column 9 is the patients' home state, column 5 is the patients' current body weight, and column 79 indicates the patients' weight 1 year ago:<div class="mediaobject"><img src="../images/00214.jpeg" alt="Breaking up" class="calibre10"/></div><p class="calibre16"> </p></li><li class="listitem" value="3">Now, we can perhaps create a series of smaller subsets where there are 50 state files, but each containing only cases that have patients who have gained more than five pounds in the past year:<div class="mediaobject"><img src="../images/00215.jpeg" alt="Breaking up" class="calibre10"/></div><p class="calibre16"> </p></li></ol><div class="calibre13"/></div><p class="calibre8">We do <a id="id970" class="calibre1"/>end up with 50 files, but each file should be much smaller and easier to work with then a single, large big data source. This is also a simple example and in practice, you may (and probably will) end up rerunning the split code and stitching together multiple state files.</p><p class="calibre8">The preceding is one example of how big data research typically works—by constructing smaller datasets that can be efficiently analyzed!</p></div></div>

<div class="book" title="A path forward">
<div class="book" title="Sampling"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch13lvl2sec120" class="calibre1"/>Sampling</h2></div></div></div><p class="calibre8">Another <a id="id971" class="calibre1"/>method for dealing with the volume of a big data source is with population sampling.</p><p class="calibre8">Sampling is a selection of or a subset of cases from within a statistical population intended to estimate or represent characteristics of the whole population. The net-net is the size of the data to be trained on, is reduced.</p><p class="calibre8">There is some concern that sampling may decrease the performance (not as in processing time, but in the accuracy of results generated) of a model. This may be somewhat true as typically the more data the model is trained on, the better the result, but depending upon the objective, the decrease in performance can be negligible.</p><p class="calibre8">Overall, it is safe to say that if sampling can be avoided, it is recommendable to use another big data strategy. But if you find that sampling is necessary, it still can lead to satisfying models.</p><p class="calibre8">When you use sampling as a big data predictive strategy, you should try to keep the sample as big as you can, consider carefully the size of the sample in proportion to the full population and ensure as best you can that the sample is not biased.</p><p class="calibre8">One of the easiest methods for creating a sample is with the R function sample. Sample takes a sample of the specified size from the elements of <code class="email">x</code> using either with or without replacement.</p><p class="calibre8">The following lines of R code are a simple example of creating a random sample of 500 cases from our original data. Notice the row counts (indicated by using the R function <code class="email">nrow</code>):</p><div class="mediaobject"><img src="../images/00216.jpeg" alt="Sampling" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="A path forward">
<div class="book" title="Aggregation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch13lvl2sec121" class="calibre1"/>Aggregation</h2></div></div></div><p class="calibre8">Another <a id="id972" class="calibre1"/>method for reducing the size of a big data source (again depending on your projects' objectives) is by statistical aggregation of the data. In other words, you simply may not require the level of granularity in the data that is available.</p><p class="calibre8">In statistical data aggregation, the data can be combined from several measurements. This means that groups of observations are replaced with summary statistics based on those observations. Aggregation is used a lot in descriptive analytics, but can also be used to prepare data for a predictive project.</p><p class="calibre8">For larger and especially disparately located big data sources one might use a Hadoop and Hive (or similar technology) solution to aggregate the data. If the data is in a transactional database, you may even be able to use native SQL. In an all-natural R solution, you have more work to do.</p><p class="calibre8">R provides a convenient function named <code class="email">aggregate</code> that can be used for big data aggregation, once you have determined how you want to (or need to) use the data in your project.</p><p class="calibre8">For example, the following code shows applying the function to the original data (stored in the data object named <code class="email">x</code>) being aggregated on the 3 variables (patient <code class="email">sex</code>):</p><div class="informalexample"><pre class="programlisting">aggregate(x, by=x["sex"], FUN=mean, na.rm=TRUE)</pre></div><p class="calibre8">Going back to an earlier section, to our example of splitting data into 50 state files, we could potentially instead use the R code shown as follows to aggregate and generate summary statistics by state. Notice that the original case count was 5,994 and after aggregating the data, we have a case count of 50 (one summary record for each state):</p><div class="mediaobject"><img src="../images/00217.jpeg" alt="Aggregation" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="A path forward">
<div class="book" title="Dimensional reduction"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch13lvl2sec122" class="calibre1"/>Dimensional reduction</h2></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 8. Dimensionality Reduction" href="part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7">Chapter 8</a>, <span class="strong"><em class="calibre9">Dimensionality Reduction</em></span>, we introduced the process of dimensional reduction, which (as we pointed out then) allows the data scientist to minimize the data's dimensionality, but can also reduce the overall volume of a big data source, thereby reducing the <a id="id973" class="calibre1"/>amount of time and memory required for processing the data, allowing it to be more easily visualized, and eliminating features irrelevant to the model's purpose, reduce model noise, and so on.</p><p class="calibre8">Like breaking up the data into smaller more manageable files, using dimensional reduction will help, but takes a good understanding of the data as well as perhaps plenty of processing steps to eventually produce a workable data population.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Alternatives" id="2UD7M1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13lvl1sec92" class="calibre1"/>Alternatives</h1></div></div></div><p class="calibre8">Since <a id="id974" class="calibre1"/>R is an in-memory language, it sometimes has a reputation of not being able to handle big data. However, using some creativity and strategic thinking, you can use big data in your predictive analytics projects quite successfully.</p><p class="calibre8">In addition to the preceding approaches, there are currently a number of alternative approaches you may wish to research, such as:</p></div>

<div class="book" title="Alternatives" id="2UD7M1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Chunking"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch13lvl2sec123" class="calibre1"/>Chunking</h2></div></div></div><p class="calibre8">There are <a id="id975" class="calibre1"/>packages available that avoid storing data in memory. Instead, objects are stored on hard disk and analyzed in chunks. As a side effect, the chunking also leads naturally to parallelization, if the algorithms allow parallel analysis of the chunks in principle. You can search: Revolution R Enterprise for some background on the topic.</p></div></div>

<div class="book" title="Alternatives" id="2UD7M1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Alternative language integrations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch13lvl2sec124" class="calibre1"/>Alternative language integrations</h2></div></div></div><p class="calibre8">Integrating <a id="id976" class="calibre1"/>higher performing programming languages is becoming a popular alternative to dealing with big data sources in R. This concept takes portions of R code and moves them to another language that may be better suited to carry out the logic or work than R is. This blends the best of R while avoiding performance bottlenecks.</p><p class="calibre8">This outsourcing of code chunks from R to another language can easily be hidden in functions. In this case, proficiency in other programming languages is mandatory for the developers, but not for the users of these functions.</p></div></div>
<div class="book" title="Summary" id="2VBO81-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch13lvl1sec93" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this <a id="id977" class="calibre1"/>chapter, we broke out a typical predict analytics project into phases and explained that the first phase is where you define what data is to be used.</p><p class="calibre8">Typically, the more the data, there is better the performance (or results) of a predictive model, but at some point (as in the case of a big data source) there may be too much data, at least to effectively deal with.</p><p class="calibre8">After reviewing the reasons why big data is so challenging, we instructed on how to gauge your data source, to qualify it as a big source, and then offered various proven techniques for addressing the common challenges of using big data.</p></div></body></html>