- en: Chapter 14. Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 深度学习
- en: The purpose of this chapter is to tackle the very important topic of *deep learning*
    and the how and why of how it has been growing in importance to the statistical
    field in recent years.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是探讨非常重要的主题**深度学习**，以及近年来它如何以及为什么在统计学领域变得越来越重要。
- en: 'We will start by providing a bit of an explanation of what *machine* learning
    is then move on with some discussion around what *deep* learning is, how it compares
    to machine learning, and the reasoning behind how it has been continually growing
    in importance almost day by day. For clarification of the concepts, we will then
    present two hallmark sample use cases: *word embedding* with some talk about natural
    language processing or NLP application logic, and **recurrent neural networks**
    (**RNNs**) which is an interesting and more advanced and efficient type of artificial
    neural network.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先简要解释一下什么是**机器学习**，然后讨论一下**深度学习**是什么，它与机器学习的比较，以及它为什么几乎每天都在不断增长的重要性。为了阐明这些概念，我们将然后展示两个标志性的示例用例：*词嵌入*，其中涉及一些关于自然语言处理或NLP应用逻辑的讨论，以及**循环神经网络**（**RNNs**），这是一种有趣且更高级、更高效的类型的人工神经网络。
- en: Machine learning or deep learning
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习或深度学习
- en: With *machine learning*, algorithm options are selected and used to analyze
    data and data sources and, rather than make decisions on them, they learn from
    them so that they can use patterns or results found in the data to make decisions
    or predictions about a certain topic, or to solve a specific problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在**机器学习**中，选择并使用算法来分析数据和数据源，而不是对它们做出决策，而是从它们中学习，以便它们可以使用在数据中发现的模式或结果来对某个主题做出决策或预测，或者解决特定问题。
- en: What this translates to is that instead of you programming or writing out each
    rule and instruction that needs to be used for a specific task such as making
    a prediction, the computer is trained using large amounts of data and algorithms
    which give it the ability to actually learn how to perform a task, make a prediction,
    solve a problem, or meet an objective in mind.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，你不需要编程或编写出用于特定任务（如做出预测）的每个规则和指令，而是通过大量数据和算法来训练计算机，使其能够真正学习如何执行任务、做出预测、解决问题或达到目标。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Just how much data qualifies as enough data for successful machine learning?**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**究竟多少数据才能算作足够的数据以实现成功的机器学习？**'
- en: Usually *the bigger, the better*, but in practice, you must gather a *sufficient*
    amount of data, based upon your intended purpose or need. Given a shortage of
    quantity, the wise data scientist should always focus on the *quality* or suitability
    of the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常“越大越好”，但在实践中，你必须根据你的目的或需求收集足够的**数据**。如果数量不足，明智的数据科学家应该始终关注数据的**质量**或适用性。
- en: An example commonly used by experts within the field of statistics to illustrate
    how machine learning works is the scenario of an algorithm or model predicting
    a person's body weight based upon what their height happens to be. In this example,
    given a decent amount of experience (or actual data cases that provide a person's
    actual physical height and body weight), a model can be built to predict a person's
    body weight given their height measurements.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学领域的专家常用以下场景来举例说明机器学习是如何工作的：一个算法或模型根据一个人的身高预测其体重。在这个例子中，如果有一个相当丰富的经验（或实际的数据案例，提供了一个人的实际身高和体重），就可以建立一个模型，根据身高测量值预测一个人的体重。
- en: Obviously, the *more experience* (or more actual data consumed and analyzed
    by the model), the *better* the results (or predictions).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，**经验越多**（或模型消耗和分析的实际数据越多），结果（或预测）就越好。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is common for data scientists to refer to a model's experience as the amount
    of raw data it has been trained on over time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常将模型的经验称为它在一段时间内训练过的原始数据量。
- en: There are many kinds, or methods of machine learning, and we find that over
    time, the industry experts have categorized them by the *type* of learning the
    algorithm or model uses.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的种类或方法有很多，我们发现随着时间的推移，行业专家们根据算法或模型使用的**学习类型**对这些方法进行了分类。
- en: 'The typical or most common types of machine learning usually include the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见或最常见的机器学习类型通常包括以下几种：
- en: Supervised
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Semi-supervised
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Reinforcement
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Transduction and so on
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转导等
- en: Deep learning is different; even though machine learning is grouped by type,
    deep learning is *not* a type. *Deep learning* is considered a method or way of
    implementing machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不同；尽管机器学习按类型分组，但深度学习**不是**一种类型。**深度学习**被视为一种实现机器学习的方法或方式。
- en: The next section will take a closer look at that concept.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将更深入地探讨这个概念。
- en: What is deep learning?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习是什么？
- en: Deep learning (also known by some within the industry as **deep structured learning**
    or **hierarchical learning**, among other titles) is really part of a wider family,
    or branch, of machine learning methods, as mentioned earlier. These methods are
    based on learning what is known as *representations* (that is, where the model
    discovers from the data the representations, patterns, or rules needed to carry
    out a desired task or meet an objective), as opposed to *task specific algorithms*
    (that is, detailed rules written out or predefined, describing how to perform
    a specific task).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（在行业内也被称为**深度结构学习**或**分层学习**等名称）实际上是机器学习方法更广泛家族或分支的一部分，如前所述。这些方法基于学习所谓的**表示**（即，模型从数据中发现执行所需任务或满足目标所需的表示、模式或规则），而不是**特定任务的算法**（即，详细写出的或预先定义的规则，描述如何执行特定任务）。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Representations or feature representations are critical to all types of learning.
    Feature representations can be learned and predefined *manually* or defined *automatically*
    by the model while analyzing the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表示或特征表示对所有类型的机器学习都是至关重要的。特征表示可以通过学习或由模型在分析数据时**手动**或**自动**定义。
- en: An alternative to manual instruction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动指令的替代方案
- en: As an alternative to the process of manually creating rules, instructions, or
    equations deemed essential to solving a problem and then organizing data to be
    run through them, the process of deep learning simply sets up fundamental parameters
    about the problem to be solved and then trains the computer to learn on its own
    by recognizing patterns within that data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为手动创建规则、指令或方程式的替代方案，这些方程式被认为是解决问题的关键，然后组织数据通过它们运行，深度学习的过程只是设定关于要解决的问题的基本参数，然后训练计算机通过识别数据中的模式来自主学习。
- en: This is accomplished by using multiple layers of processing. For example, the
    first layer may establish the most basic feature or features by finding a simple
    or basic pattern. The next layer is then fed this identified information, which
    then works to break out the next level of information and feed that to another
    layer and so on, until the final layer can determine an outcome or make a prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用多层处理来实现的。例如，第一层可能通过找到简单或基本的模式来建立最基本的特征或特征。然后，下一层将接收这些已识别的信息，然后努力提取下一层次的信息，并将其传递给另一层，依此类推，直到最终层可以确定结果或做出预测。
- en: This process is typically illustrated using the tree-like flow of a decision
    tree or decision flow diagram. This graphical representation can visually show
    decisions and their possible consequences, including chance event outcomes, and
    so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程通常通过决策树或决策流程图的树状流程来展示。这种图形表示可以直观地显示决策及其可能的后果，包括随机事件的结果等。
- en: If we again exploit our previously mentioned physical height and body weight
    example, using machine learning, one would have to define features, instructions,
    or rules based upon whether an individual was male or female, their age and ethnicity,
    and perhaps their BMI or body mass index. In short, you would outline the physical
    attributes to be used to meet the objective (guess the correct body weight) and
    then let the system use the more important features to determine a subject's suspected
    body weight.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次利用之前提到的身高和体重示例，使用机器学习，就必须根据个体是男性还是女性、他们的年龄和种族，以及可能还有他们的BMI或体质指数来定义特征、指令或规则。简而言之，你需要概述用于满足目标（猜测正确的体重）的物理属性，然后让系统使用更重要的特征来确定一个主体的疑似体重。
- en: 'So, deep learning automatically discovers or finds out the features that are
    important to be used for making the prediction. This finding out process might
    be described as following the steps listed as follows (again, if we use the body
    height and weight use case example):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度学习会自动发现或找出用于预测的重要特征。这个发现过程可以描述为以下列出的步骤（再次强调，如果我们使用身高和体重用例示例）：
- en: First the process attempts to identify which physical attributes are most relevant
    to determining body weight
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，该过程试图确定哪些物理属性与确定体重最相关
- en: Next, it builds a hierarchy rather like that decision flowchart we mentioned
    earlier which it can use to determine a subject's body weight (for example, whether
    a subject is male or female or is within a certain height range, and so on)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，它构建一个类似于我们之前提到的决策流程图的层次结构，它可以利用这个层次结构来确定主体的体重（例如，主体是男性还是女性，或者是否在某个身高范围内，等等）
- en: After consecutive hierarchical identification (or classification) of these combinations,
    it then decides which of these features are responsible for predicting the answer
    (that is, the subjects body weight)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对这些组合进行连续的层次识别（或分类）之后，它随后决定哪些特征负责预测答案（即，主体的体重）
- en: To summarize, while classical machine learning requires the extraction and establishment
    of rules or features from data, followed by the preprocessing or organizing of
    the data (and these steps are typically 85 to 90 percent a human effort) before
    the model can be used to make predictions, deep learning uses deep learning algorithms
    to perform its own feature learning and then is able to make its predictions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然经典机器学习需要从数据中提取和建立规则或特征，然后对数据进行预处理或组织（这些步骤通常是85%到90%的人工努力），之后模型才能用于做出预测，而深度学习则使用深度学习算法进行自己的特征学习，然后能够做出预测。
- en: At the time of writing, deep learning is typically assumed to be one of four
    fundamental architectures.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，深度学习通常被认为是四种基本架构之一。
- en: 'These are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括：
- en: Unsupervised Pre-Trained
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督预训练
- en: Convolutional Neural
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Recurrent Neural
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: Recursive Neural
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'These deep learning architectures have been successfully applied to various
    fields and have produced results comparable to (and in some cases superior to)
    appropriately skilled human **subject matter experts** (**SMEs**):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些深度学习架构已经成功应用于各个领域，并产生了与（在某些情况下优于）适当技能的人类**主题专家**（**SMEs**）相当的结果：
- en: Computer vision
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Speech recognition
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Natural language processing
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Audio recognition
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频识别
- en: Social network filtering
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交网络过滤
- en: Machine translation
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Bioinformatics
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生物信息学
- en: Growing importance
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日益重要的
- en: Today, deep learning has been established as a key instrument for practical
    machine learning use cases. Since computers are ever more powerful, using deep
    learning techniques to learn from the ever growing data sources (even *big data*),
    we can expect to process and predict quicker and with higher rates of accuracy
    than ever before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，深度学习已被确立为实际机器学习用例的关键工具。由于计算机的日益强大，使用深度学习技术从不断增长的数据源（甚至*大数据*）中学习，我们预计可以比以往任何时候都更快、更准确地处理和预测。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: '*Big data* is a term being used for data that is so large or complex that traditional
    algorithms and system software is insufficient to deal with it.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据*是一个用于描述数据量如此之大或如此复杂，以至于传统的算法和系统软件不足以处理它的术语。'
- en: Furthermore, the concept of deep learning has been described many times in the
    media as more than a method or practice of machine learning (as we mentioned earlier
    in this chapter), but more of a ground-breaking attitude to learning, using cognitive
    skills such as the ability to analyze, produce, solve problems, and thinking meta-cognitively
    in order to construct long-term understanding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习的概念在媒体中被多次描述为不仅仅是一种机器学习的方法或实践（如我们在本章前面提到的），而是一种革命性的学习态度，它使用认知技能，如分析、产生、解决问题以及进行元认知思考的能力，以构建长期理解。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: Cognitive skills usually refers to the capacity to develop a meaning and/or
    certain knowledge from reviewing data (also called experience or information).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 认知技能通常指的是从审查数据（也称为经验或信息）中发展意义和/或特定知识的能力。
- en: The use of deep learning techniques promotes understanding and application for
    life at a much more advanced, more effective, and quicker proportion than other
    forms of learning, therefore it is an area with extremely high potential to impact
    the world as we know it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术的应用促进了我们对生活知识的理解和应用，其效果比其他学习形式更为先进、有效和迅速，因此它是一个具有极高潜力的领域，有可能深刻影响我们所知的世界。
- en: Deeper data?
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深的数据？
- en: Pretty much everyone, everywhere has heard of the term *big data*. Although
    there may still be some debate or disagreement as to what the term actually means,
    the bottom line is that there is a lot more data available today then there was
    yesterday (and there will be even more tomorrow!).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个人、每个地方都听说过“大数据”这个术语。尽管可能仍然有一些关于这个术语实际含义的争论或分歧，但底线是，今天可用的数据比昨天多得多（而且明天还会更多！）。
- en: What this means is that this data is available to build more neural networks
    with many deeper layers, providing even more accurate (or at least perhaps more
    interesting) outcomes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这些数据可用于构建具有许多更深层的神经网络，提供更准确（或者至少更有趣）的结果。
- en: Deep learning for IoT
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物联网深度学习
- en: Also, new and exciting, is the fascinating world of the **internet of things**
    (**IoT**). The acronym IoT describes the way devices, vehicles, buildings, and
    many, many other items speak or communicate with each other. Almost all devices
    these days and into the future have or will have the ability to be smart devices
    or become connected devices, capturing information about their usage and surrounding
    environments and conditions, and then connecting and sharing the information and
    events they collect.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有令人兴奋的新兴领域，那就是**物联网**（**IoT**）。IoT这个缩写描述了设备、车辆、建筑以及许多其他物品如何相互交流或通信。如今几乎所有设备以及未来都将具备成为智能设备或连接设备的能力，捕捉它们的使用和周围环境及条件的信息，然后连接并共享它们收集的信息和事件。
- en: Machine and deep learning models and algorithms will play a significant role
    in the IoT analytics. Data from IoT devices is sparse and/or has a temporal element
    in it, and deep learning algorithms can be trained with this information to yield
    significant insights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器和深度学习模型和算法将在物联网分析中发挥重要作用。物联网设备的数据稀疏且/或具有时间元素，深度学习算法可以用这些信息进行训练，以产生重大见解。
- en: The many, many recent advances in the area of distributed cloud computing and
    graphics processing units have made incredible computing power available for use,
    which in turn advances the ability for maximum positive effectiveness of deep
    learning applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式云计算和图形处理单元领域的许多、许多最近进展使得难以置信的计算能力可用于使用，这反过来又提高了深度学习应用的最大积极效果能力。
- en: Use cases
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例
- en: 'Many real-life use cases exist today for applying deep learning algorithms,
    including (just to name a few):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 今天已经存在许多实际应用案例，可以应用深度学习算法，包括（仅举几个例子）：
- en: Fraud detection
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: Image recognition
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别
- en: Voice recognition
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Natural language processing
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Now becoming more main stream, the growing field of *predictive analysis* and
    *predictive analytics* is using deep learning in the areas of finance, accounting,
    government, security, hardware manufacturing, search engines, e-commerce, and
    medicine.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在越来越主流，日益增长的*预测分析*和*预测分析学*领域正在使用深度学习在金融、会计、政府、安全、硬件制造、搜索引擎、电子商务和医学等领域。
- en: One newer, very exciting, and perhaps growing ever more important use case for
    deep learning is with motion detection for *situation evaluation*, security, and
    defense.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习来说，一个较新、非常激动人心且可能越来越重要的用例是与运动检测用于*情况评估*、安全和防御。
- en: Word embedding
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: '**Natural language processing** (**NLP**) is an area of computer science (or
    more specifically, computational linguistics) that focuses on the interactions
    between computers and the human language.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是计算机科学（或更具体地说，计算语言学）的一个领域，专注于计算机与人类语言之间的交互。'
- en: In a natural language application, there is an attempt to process an extreme
    amount of real-world text, formally called a natural language corpora data source.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言应用中，试图处理极端大量的真实世界文本，正式称为自然语言语料库数据源。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Corpora is equivalent to the word samples. In this context, a natural language
    corpora data source would be a database or file filled with actual words and phrases
    of text, in an expected language.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库相当于单词样本。在这个背景下，自然语言语料库数据源将是一个包含实际单词和短语文本的数据库或文件，这些文本是预期的语言。
- en: 'Speech recognition is one of the most well-known and perhaps most developed
    applications of NLP, even so, challenges are many and typically include:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别是自然语言处理（NLP）最知名且可能最发达的应用之一，即便如此，挑战仍然很多，通常包括：
- en: Natural language understanding
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: Natural language generation
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言生成
- en: Connecting language and machine perception
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接语言和机器感知
- en: Dialog systems
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话系统
- en: Some combination of all of these
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些的组合
- en: Word embedding is a very popular method of language modeling and feature learning
    techniques used in many natural language processing applications.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是语言建模和特征学习技术中非常流行的方法，这些方法被广泛应用于许多自然语言处理应用中。
- en: This is the practice of using words or phrases from a vocabulary and mapping
    them to vectors of real numbers. Simply speaking, word embedding is the process
    of turning text into numbers and this text-to-numeric transformation is required
    because most deep learning algorithms require their input to be vectors of continuous
    numeric values (they don't work on strings of plain text) and, well, computers
    just unsurprisingly process numbers better.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种使用词汇表中的词或短语并将它们映射到实数向量中的做法。简单来说，词嵌入是将文本转换为数字的过程，这种文本到数字的转换是必需的，因为大多数深度学习算法都需要它们的输入是连续数值的向量（它们不能处理纯文本字符串），而且，嗯，计算机处理数字的能力出奇地好。
- en: 'So, with the preceding definition in mind, word embedding is used to map words
    or phrases from a vocabulary to a corresponding vector of real numbers that also
    provides the following benefits:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据前面的定义，词嵌入被用来将词汇表中的词或短语映射到相应的实数向量，这个向量还提供了以下好处：
- en: '**Dimensionality Reduction**: Reducing phrases to numbers obviously is a more
    efficient representation'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：将短语简化为数字显然是一种更有效的表示'
- en: '**Contextual Similarity**: Numerics can be a more expressive representation'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文相似性**：数值可以是一种更具有表现力的表示'
- en: '"Contextual Word Similarity is nothing but identifying different types of similarities
    between words. It is one of the goals of NLP. Statistical approaches are used
    for computing the degree of similarity between words."'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “上下文词相似性不过是识别词之间不同类型的相似性。它是自然语言处理的一个目标。统计方法用于计算词之间相似度的程度。”
- en: ''
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Robin, December 10th, 2012
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – Robin，2012年12月10日
- en: Word prediction
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词预测
- en: For a statistical *language model* to be able to predict the meaning of some
    text, it needs to be conscious of the *contextual similarity of words*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使统计语言模型能够预测某些文本的意义，它需要意识到词的*上下文相似性*。
- en: For example, you would probably agree that you would expect to find words such
    as *martini* or *cosmopolitan* within sentences where they're *dry*, *shaken*,
    *stirred*, and *chilled*, but would not expect to find those same concepts in
    such close proximity to, say, the word automobile.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会同意，你期望在句子中找到像*martini*或*cosmopolitan*这样的词，这些词在句子中是*dry*、*shaken*、*stirred*和*chilled*的，但你不会期望在这些词附近找到像*automobile*这样的概念。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Another form of word prediction is *Autocomplete or word completion*. This is
    when an algorithm can predict the rest of a word a user is typing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种词预测的形式是*自动完成或词补全*。这是当算法可以预测用户输入的词的其余部分时发生的情况。
- en: Word vectors
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词向量
- en: The word vectors (actually they are *numeric vectors*) that are produced by
    applying the logic and reason of word embedding expose these similarities, so
    words that regularly occur nearby in text will also be in close proximity within
    a vector space.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用词嵌入的逻辑和推理产生的词向量（实际上它们是*数值向量*）揭示了这些相似性，因此，在文本中经常相邻出现的词，在向量空间中也会彼此靠近。
- en: It is very important to understand how these words or numeric vectors work,
    so let's go over a short (and hopefully simple), explanation of this notion.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些词或数值向量是如何工作的非常重要，所以让我们简要（并且希望简单）地解释一下这个概念。
- en: If a word vector is divided into several hundred elements, each word in a vocabulary
    is represented by a distribution of weights across those elements (in that vector).
    So instead of a one-to-one mapping between an element in the vector and a word,
    the representation of that word is spread across all of the elements in that vector,
    and each element in the vector *contributes* to the definition of many words.
    Such a vector comes to represent in some abstract way the meaning of a word.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词向量被分成几百个元素，词汇表中的每个词都通过这些元素（在该向量中）的权重分布来表示。因此，在向量中的一个元素和词之间不再是简单的映射，该词的表示分布在向量的所有元素上，向量的每个元素*都贡献*于许多词的定义。这样的向量以某种抽象的方式代表了词的意义。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'A really easy to understand tutorial along with some nice illustrations on
    word or numeric vectors can be found online at: [https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在线可以找到一篇易于理解的教程和一些关于词语或数值向量的精美插图：[https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)。
- en: So, again, let's answer the question of what is word embedding?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，再次回答一下什么是词嵌入的问题？
- en: '*"…Word Embedding is a means of creating a low-dimensional vector representation
    from corpus of text, which preserves the contextual similarity of words…"*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*"…词嵌入是一种从文本语料库中创建低维向量表示的方法，它保留了词语的上下文相似性…"*'
- en: Numerical representations of contextual similarities
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文相似性的数值表示
- en: An additional bonus of implementing *word* vectors is that they can be manipulated
    arithmetically (just like any other numeric vector can). Since words in a vocabulary
    are translated into *numerical* vectors, and there are semantic relationships
    in the position of those vectors, one can use or apply *simple arithmetic* on
    the vectors to find additional meanings and insights.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实现词向量的一个额外好处是它们可以进行算术操作（就像任何其他数值向量一样）。由于词汇表中的词语被转换成*数值*向量，并且这些向量的位置存在语义关系，因此可以在这些向量上使用或应用*简单的算术*来找到额外的含义和洞察。
- en: Many examples do exist to illustrate this concept, including the operation of
    moving across in embedding space from *Man* to *Queen* by subtracting *King* and
    adding *Woman*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多例子可以说明这个概念，包括通过在嵌入空间中从“Man”（男人）移动到“Queen”（女王）来减去“King”（国王）并加上“Woman”（女人）。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: The arithmetic manipulation performed on word or numeric vectors is known within
    the field as *vector math*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在该领域，对词语或数值向量进行的算术操作被称为*向量数学*。
- en: By exploiting this technique, groupings of words are not simply close variations
    or *synonyms*, but rather unique words that make up a contextual collection or
    just belong together.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这种技术，词语的分组不仅仅是接近的变体或*同义词*，而是构成上下文集合的独特词语或仅仅是属于一起的词语。
- en: Netflix learns
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Netflix学习
- en: One of my most favorite machine learning use case examples is Netflix (a website
    that specializes in and provides streaming media and video-on-demand online).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的机器学习用例之一是Netflix（一个专注于并提供流媒体和视频点播的网站）。
- en: A typical view of Netflix services (movies and videos available for streaming)
    provides over 40 rows of possible selections. Just like any other business, a
    consumer loses interest after about two minutes of window shopping for a video
    to watch so Netflix has very little time to catch the customer's attention.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix服务（可流媒体播放的电影和视频）的典型视图提供了超过40行的可能选择。就像任何其他业务一样，消费者在浏览约两分钟的视频选择后就会失去兴趣，因此Netflix几乎没有时间吸引客户的注意力。
- en: 'Rather than rely on customer ratings and surveys, Netflix leverages a very
    broad set of data assets: what each member watches, when they watch, the place
    on the Netflix screen the customer found the video, recommendations the customer
    didn''t pick, and the popularity of videos in the catalogue.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix并非依赖于客户评分和调查，而是利用一个非常广泛的数据资产集：每个会员观看的内容、观看时间、客户在Netflix屏幕上找到视频的位置、客户未选择的推荐以及目录中视频的流行度。
- en: '"All of this data is read by numerous algorithms powered by machine-learning
    techniques. Approaches use both supervised (classification, regression) and unsupervised
    (dimensionality reduction through clustering or compression) approaches…,"'
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"所有这些数据都是由众多算法读取的，这些算法由机器学习技术驱动。方法使用监督（分类、回归）和无监督（通过聚类或压缩进行维度降低）方法…"，'
- en: ''
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- C. Raphel.'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- C. Raphel.'
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: 'The report mentioned is available online here: [https://www.rtinsights.com/netflix-recommendations-machine-learning-algorithms](https://www.rtinsights.com/netflix-recommendations-machine-learning-algorithms).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提到的报告可在以下网址在线获取：[https://www.rtinsights.com/netflix-recommendations-machine-learning-algorithms](https://www.rtinsights.com/netflix-recommendations-machine-learning-algorithms)。
- en: A video-to-video similarity algorithm, or Sims, makes recommendations in the
    *"Because You Watched"* row
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 视频到视频相似性算法，或称Sims，在“因为你观看了”这一行提供推荐
- en: ''
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- *C. Raphel*.'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- *C. Raphel*.'
- en: One may discern that selections are made by genre alone, but the idea of contextual
    similarities surely plays a role in mining selections that fit the consumer or
    viewers mindset. Words that fit together can spawn ideas for films that might
    be enjoyed by the viewer. Manipulating word vectors can produce an almost endless
    list of ideas.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有人会认为选择仅基于类型，但上下文相似性的概念在挖掘符合消费者或观众心态的选择中肯定起到了作用。适合一起使用的词语可以激发出观众可能喜欢的电影想法。操纵词向量可以产生几乎无穷无尽的想法。
- en: 'As the following paragraph reports, results from the Netflix algorithms actually
    have a better success rate in making recommendations that what is intuitively
    believed:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如下一段所述，Netflix算法的结果实际上在做出推荐方面比直觉认为的成功率更高：
- en: '"…as an example, the authors describe recommendations for shows similar to
    "House of Cards." While one might think that political or business dramas such
    as "The West Wing" or "Mad Men" would increase customer engagement, it turns out
    that popular but outside-of-genre titles such as "Parks and Recreation" and "Orange
    Is the New Black" fared better. The authors call this a case of "intuition failure..."'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “…例如，作者描述了与《纸牌屋》类似的电视剧推荐。虽然有人可能会认为像《西翼》或《广告狂人》这样的政治或商业剧会增加客户参与度，但结果却表明，像《公园与游憩》和《橙子不是新的黑色》这样的流行但非类型作品表现更好。作者称这为‘直觉失败’的案例...”
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – C. Raphel
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – C. Raphel
- en: Implementations
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: So how do we implement word or numeric vectors in a typical word embedding application?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在典型的词嵌入应用中实现词或数字向量呢？
- en: One of the most popular algorithms available for producing word embedding models
    is **word2vec**, created by Google in 2013\. Word2vec, written in C++, but also
    has been implemented in Java/Scala and Python, accepts a text corpus (or speaking
    informally, expects a sequence of sentences as its input and each sentence a list
    of words) as input and produces word vectors as output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于生成词嵌入模型的最受欢迎的算法之一是**word2vec**，由谷歌在2013年创建。Word2vec是用C++编写的，但也实现了Java/Scala和Python，接受文本语料库（或者非正式地说，期望输入是一个句子序列，每个句子是一个单词列表）作为输入，并产生词向量作为输出。
- en: 'Another note about the input to word2vec, it only requires that your input
    data be provided as sequential sentences, you do not have to worry about storing
    it all in memory at one time to process it. This means that you can:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 关于word2vec输入的另一个备注，它只要求您提供的数据以顺序句子形式提供，您不必担心一次性将所有内容存储在内存中以便处理。这意味着您可以：
- en: Provide one sentence
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一句话
- en: Process it
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理它
- en: Load another sentence
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载另一句话
- en: Process it
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理它
- en: Repeat…
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复...
- en: This means large data, such as those that qualify as big data, sources (discussed
    in [Chapter 11](part0082_split_000.html#2E6E41-c6198d576bbb4f42b630392bd61137d7
    "Chapter 11. Topic Modeling"), *Topic Modeling* of this book), which may consist
    of data spread over several files in multiple locations, can be processed by one
    sentence per line (instead of loading everything into an in-memory list, input
    file by file, line by line). This kind of architecture also allows preprocessing
    such as converting to Unicode, lowercase, removing numbers, extracting named entities,
    and so on, to occur without word2vec even being aware of it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大量数据，例如那些符合大数据标准的数据源（在第11章中讨论），本书的*主题建模*，可能由分布在多个位置的多份文件中的数据组成，可以通过每行一句（而不是将所有内容一次性加载到内存列表中，逐个文件，逐行读取）进行处理。这种架构还允许进行预处理，例如转换为Unicode、转换为小写、去除数字、提取命名实体等，而无需word2vec意识到这些操作。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Word2vec isn't a good choice for data that is very small in size. For real results,
    as reported through trials, you should have a minimum of a million words. Small
    data files or sources are not enough for a concise word similarity or proper word
    vector creation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec对于非常小的数据来说不是一个好的选择。为了得到真实的结果，如通过试验报告，您应该至少有一百万个单词。小数据文件或来源不足以创建简洁的词相似度或适当的词向量。
- en: Word2vec is also set up to accept some parameters such as `min_count`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec还设置了一些参数，例如`min_count`。
- en: This parameter is very effective for setting the lower limit for words to appear
    in the data. For example, any words that appear only a few times in a million-word
    data source are probably typos and garbage and should be ignored in word vector
    creation. This parameter allows you to automatically drop uninteresting or unimportant
    words. The default is set to `5`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数对于设置数据中出现单词的下限非常有效。例如，在百万词数据源中只出现几次的任何单词可能都是打字错误或垃圾，应该在创建词向量时忽略。此参数允许您自动删除不感兴趣或不重要的单词。默认设置为`5`。
- en: Word2vec first constructs a vocabulary from the text data provided as input
    and then learns vector representation of words. The resulting word vector file
    can be used as featured in many natural language processing and machine learning
    applications.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec首先从提供的文本数据中构建词汇表，然后学习单词的向量表示。生成的词向量文件可以用作许多自然语言处理和机器学习应用中的特征。
- en: 'The following is a partial word vector image created by word2vec:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是word2vec创建的部分词向量图像：
- en: '![Implementations](img/00218.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![实现](img/00218.jpeg)'
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Even though word2vec is a powerful tool, even Google declares that it is not
    user-friendly and various open source packages have been developed over time to
    add a user friendly interface to the algorithm. You can go online and access word2vec
    at [https://code.google.com/p/word2vec](https://code.google.com/p/word2vec).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管word2vec是一个强大的工具，即使是谷歌也宣称它不够用户友好，并且随着时间的推移已经开发了各种开源软件包，为该算法添加了用户友好的界面。您可以在网上访问word2vec，网址为[https://code.google.com/p/word2vec](https://code.google.com/p/word2vec)。
- en: As with many implementations in statistics, there is some disagreement as to
    exactly what word2vec is or how the logic has ultimately been implemented. Is
    it an example of the classical machine learning model? An example of implemented
    deep learning? Or, can we say that it is some sort of hybrid model?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 就像统计学中的许多实现一样，关于word2vec究竟是什么或者其逻辑最终是如何实现的，存在一些分歧。它是经典机器学习模型的例子吗？是实施深度学习的例子吗？或者，我们可以说它是一种某种混合模型？
- en: 'A bit of online research reveals numerous opinions, for example, A.Thakker,
    June 18, 2017:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一点在线研究揭示了众多观点，例如，A.Thakker，2017年6月18日：
- en: '"…Word2Vec is considered (by some within the industry) as a starter of "Deep
    Learning in NLP". However, Word2Vec is not deep. But the output of Word2Vec is
    what Deep Learning models can easily understand. Word2vec is basically a computationally
    efficient predictive model for learning word embeddings from raw text. The purpose
    of Word2Vec is to group words that are semantically similar in vector space. It
    computes similarities mathematically. Given a huge amount of data…."'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “……Word2Vec被认为（在行业内的一些人看来）是“自然语言处理中的深度学习”的起点。然而，Word2Vec并不深。但Word2Vec的输出是深度学习模型可以轻松理解的。Word2Vec基本上是一个从原始文本中学习词嵌入的计算效率高的预测模型。Word2Vec的目的是在向量空间中将语义相似的单词分组在一起。它通过数学计算相似度。给定大量数据……”
- en: Let's go over the architectures of deep learning, starting in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下深度学习的架构，从下一节开始。
- en: Deep learning architectures
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习架构
- en: 'We indicated earlier in this chapter, under the Deep Learning section that
    there are currently (or at least at the time of writing) four basic deep learning
    architectures. We''ll fleetingly look at three (Unsupervised Pre-Trained, Convolutional
    Neural, and Recursive Neural) now and then do a deeper dive into one of the most
    stimulating and effective (at least for appropriate use cases) *Recurrent Neural
    Networks*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的“深度学习”部分之前已经指出，目前（至少在写作的时候）有四种基本的深度学习架构。现在我们将简要地看看三种（无监督预训练、卷积神经网络和递归神经网络），然后深入探讨其中最刺激和最有效的（至少对于适当的用例）*循环神经网络*：
- en: '**Unsupervised pre-trained neural networks**: Think of stacking the deck by
    making weighting adjustments before the model training actually begins.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无监督预训练神经网络**：想象在模型训练真正开始之前通过调整权重来“洗牌”。'
- en: '**Convolutional neural networks**: A feed-forward model, that uses a variation
    of multilayer perceptrons (or individual learning units) designed to require minimal
    preprocessing, used for visual imagery processing and natural language processing.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**：一种前馈模型，使用多层感知器（或单个学习单元）的变体，旨在需要最少的预处理，用于视觉图像处理和自然语言处理。'
- en: '**Recursive neural networks**: These are created by applying the same set of
    weights recursively over a structure, in an attempt to produce a *structured prediction*
    (that is, *the ability to predict* structured objects rather than discreet or
    real values).'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**递归神经网络**：这些网络通过在结构上递归地应用同一组权重来创建，试图产生一种**结构化预测**（即，预测结构化对象的能力，而不是离散或实值）。'
- en: Artificial neural networks
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: '**Artificial neural networks** (**ANNs**) systems are computing systems, algorithms,
    or models that are inspired by and based upon how biological neural networks in
    our human brains work.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（ANNs）系统是受我们人类大脑中生物神经网络工作方式启发的计算系统、算法或模型。'
- en: These systems learn to perform work and solve problems by considering patterns
    found in data (referred to as gaining experience), generally without having to
    program specific logic prompts.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统通过考虑数据中发现的模式（称为获得经验）来学习执行工作和解决问题，通常无需编程特定的逻辑提示。
- en: ANNs are a *big part* of deep learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs是深度学习的一个重要部分。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Most artificial neural networks bear only a slight resemblance to their more
    complex biological counterparts, but are very effective at intended tasks such
    as classification or segmentation. For more information, refer to: [https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人工神经网络与它们更复杂的生物对应物只有轻微的相似之处，但在分类或分割等预期任务上非常有效。更多信息，请参阅：[https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks)。
- en: In [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks*, we covered *Neural Networks*
    in some detail, specifically, ANNs. In the next section of this chapter we pick
    up that thread again and move onto the topic of **Recurrent neural networks**
    (**RNNs**).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7 "第5章。神经网络")中，我们详细介绍了**神经网络**，特别是人工神经网络（ANNs）。在本章的下一节中，我们将继续这一主题，并转向**循环神经网络**（RNNs）这一主题。
- en: Recurrent neural networks
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Generally speaking, it is accepted within the industry that there really are
    just two chief types of neural networks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，行业内普遍认为实际上只有两种主要的神经网络类型。
- en: 'These are:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是：
- en: Feed forward
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈
- en: Recurrent
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环
- en: The feed forward neural network was the first and simplest type that was developed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈**神经网络是首先开发的第一种也是最简单的一种类型。'
- en: In a *feed forward* network, activation is pushed through the network from the
    input layers to the output layers. In this network the information moves only
    from the input layer *straight through* any hidden layers to the output layer
    without cycles or looping.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在**前馈**网络中，激活从输入层推向输出层。在这个网络中，信息仅从输入层**直接通过**任何隐藏层到输出层，没有循环或循环。
- en: In other words, feed forward neural networks are a one-way street.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，前馈神经网络是一条单行道。
- en: Note
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Most all of the types of neural networks are organized in layers. Layers are
    made up of interconnected nodes which contain what is known as an activation function.
    *Patterns* are presented to the network by the input layer, which then communicates
    to one or more **hidden layers**. Hidden layers are where the real work is done
    using a system of weighted connections.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络类型都是按层组织的。层由相互连接的节点组成，这些节点包含所谓的激活函数。**模式**由输入层呈现给网络，然后与一个或多个**隐藏层**进行通信。隐藏层是真正工作的地方，使用加权连接系统。
- en: Let's continue on with our dialogue by stating that a **recurrent neural network**
    (or **RNN**) is an interesting and unique *class* of ANN.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的对话，声明一个**循环神经网络**（或RNN）是人工神经网络（ANN）中一个有趣且独特的**类别**。
- en: The objective of using RNN logic is to make use of *sequential or chronological*
    data. This is much different to the logic used by a traditional neural network,
    where it is assumed that all inputs and outputs are independent of each other,
    or have no relevance to each others. This kind of presumption (or limitation)
    works or is at least sufficient for some applications, but for many tasks this
    is not an acceptable premise. For example, if you are trying to predict the next
    word someone is typing in a search engine, you need to know which words were typed
    before it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN逻辑的目标是利用*顺序或时间序列*数据。这与传统神经网络所使用的逻辑大不相同，传统神经网络假设所有输入和输出都是相互独立的，或者彼此之间没有关联。这种假设（或限制）对于某些应用来说是有效的，或者至少是足够的，但对于许多任务来说，这不是一个可接受的假设。例如，如果你试图预测某人正在搜索引擎中输入的下一个单词，你需要知道之前输入了哪些单词。
- en: RNNs are called *recurrent* because they perform the *same task* for *every
    element in a sequence*, with the output being dependent on all of the previous
    computations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs被称为*循环*，因为它们对序列中的每个元素执行*相同的任务*，输出依赖于所有之前的计算。
- en: Another way to think about RNNs is that they can remember information about
    what has been calculated thus far within a sequence. This allows it to exhibit
    *dynamic temporal (or related) behaviors*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考RNNs的方式是，它们可以记住序列中到目前为止已计算的信息。这使得它能够表现出*动态时间（或相关）行为*。
- en: Remember, RNNs use a special layer that is called a state layer, which is updated
    not only with the external input information of the network, but also with activation
    information from the previous forward propagation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，RNNs使用一个称为状态层的特殊层，该层不仅更新网络的外部输入信息，还更新来自先前前向传播的激活信息。
- en: 'There is an interesting blog that provides valuable insight into how RNNs work.
    The following figure is based upon that information. The reader can review the
    information at: [https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks](https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个有趣的博客，提供了关于RNNs工作原理的宝贵见解。以下图表基于这些信息。读者可以在以下链接查看信息：[https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks](https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks)。
- en: '![Recurrent neural networks](img/00219.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/00219.jpeg)'
- en: To show how much this is a valuable feature, as an example, the word *aliens*
    might have a different meaning if it was part of the sequence *ancient aliens*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一点是多么有价值的一个特性，例如，单词*aliens*如果它是序列*ancient aliens*的一部分，可能会有不同的含义。
- en: Note
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In theory, RNNs can make use of information in arbitrarily long sequences, but
    in practice they are limited to looking back only a few steps.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNNs可以使用任意长序列中的信息，但在实践中，它们只能回溯查看几个步骤。
- en: As we stated earlier in this section, unlike an artificial neural network where
    connections between logic layers do not form a loop (technically referred to as
    a feed forward neural network), RNNs can use their internal memory to process
    *arbitrary sequences of inputs*. This makes them a great choice for applications
    such as handwriting recognition or speech recognition.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节前面所述，与逻辑层之间没有形成循环（技术上称为前馈神经网络）的人工神经网络不同，RNNs可以使用它们的内部记忆来处理*任意输入序列*。这使得它们非常适合于手写识别或语音识别等应用。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the topics of machine and deep learning and the
    difference between the two. We also mentioned how deep learning has the capacity
    to drive change in the world.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了机器学习和深度学习以及两者之间的区别。我们还提到了深度学习如何有能力推动世界的变化。
- en: We saw how deep learning reduces the effort required by humans and listed some
    of the current applications where these algorithms have been successfully applied.
    We then looked at using *word embedding* for a use case such as NLP applications,
    and explained how it works.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了深度学习如何减少人类所需的努力，并列出了一些这些算法已经成功应用的应用案例。然后我们探讨了在NLP应用等用例中使用*词嵌入*，并解释了它是如何工作的。
- en: Finally, we wrapped up with a discussion on neural networks, specifically RNNs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了神经网络，特别是RNNs。
- en: With this chapter, we bring our journey to its end, having provided in-depth
    information around performance metrics and learning curves, polynomial regression,
    Poisson, and negative binomial regression, back-propagation, radial basis function
    networks, and others. We also discussed the process of working with very large
    datasets.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们结束了这次旅程，提供了关于性能指标和学习曲线、多项式回归、泊松回归和负二项式回归、反向传播、径向基函数网络等方面的深入信息。我们还讨论了处理非常大的数据集的过程。
- en: Hopefully you have enjoyed exploring and testing these popular modeling techniques
    and mastered a range of predictive analytics styles.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您已经享受了探索和测试这些流行建模技术的过程，并掌握了多种预测分析风格。
