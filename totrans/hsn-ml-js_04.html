<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Grouping with Clustering Algorithms</h1>
                </header>
            
            <article>
                
<p>A common and introductory unsupervised learning problem is that of <em>clustering.</em> Often, you have large datasets that you wish to organize into smaller groups, or wish to break up into logically similar groups. For instance, you can try to divide census data of household incomes into three groups: low, high, and super rich. If you feed the household income data into a clustering algorithm, you would expect to see three data points as a result, with each corresponding to the average value of your three categories. Even this one-dimensional problem of clustering household incomes may be difficult to do by hand, because you might not know where one group should end and the other should begin. You could use governmental definitions of income brackets, but there's no guarantee that those brackets are geometrically balanced; they were invented by policymakers and may not accurately represent the data.</p>
<p>A <em>cluster</em> is a group of logically similar data points. They can be users with similar behavior, citizens with similar income ranges, pixels with similar colors, and so on. The k-means algorithm is numerical and geometric, so the clusters it identifies will all be numerically similar, with data points that are geometrically close to one another. Fortunately, most data can be represented numerically, so the k-means algorithm is useful for many different problem domains.</p>
<p>The k-means algorithm is a powerful, fast, and popular clustering algorithm for numerical data. The name k-means is comprised of two parts: <em><strong>k</strong></em>, which represents the number of clusters that we want the algorithm to find, and <strong><em>means,</em></strong> which is the method of determining where those cluster centers are (you could, for instance, also use k-medians or k-modes). Translated into plain English, we might ask the algorithm to find us three cluster centers that are the mean values of the points they represent. In that case, <em>k = 3</em> and we can tell our bosses that we did a k-means analysis with <em>k = 3</em> when filing our report.</p>
<p>The k-means algorithm is an iterative algorithm, which means it runs a loop and continually updates its model until the model reaches steady state, at which point it will return its results. Put into narrative form, the k-means algorithm works like this: plot the data that you wish to analyze, and pick a value for <em>k</em>. You must know the value of <em>k</em> beforehand, or at least have an idea of what it should be (though we'll also explore a way around this later in the chapter). Randomly create <em>k</em> points (if <em>k = 5</em>, create five points) and add them to your plot; these points are called the <strong>centroids</strong>, as they will ultimately represent the geometric centers of the clusters. For each data point in the plot, find the centroid closest to that point and connect or assign it to the point. Once all the assignments have been made, look at each centroid in turn and update its position to the mean position of all the points assigned to it. Repeat the assign-then-update procedure until the centroids stop moving; these final positions of the centroids are the output of the algorithm, and can be considered your cluster centers. If the narrative is hard to follow, don't worry, we'll dig into it more deeply as we build this algorithm from scratch.</p>
<p>In this chapter, we'll first discuss the concepts of average and distance and how they apply to the k-means algorithm. Then we'll describe the algorithm itself and build a JavaScript class from scratch to implement the k-means algorithm. We'll test our k-means solver with a couple of simple datasets, and then discuss what to do when you don't know the value of <em>k</em> beforehand. We'll build another tool that automates the discovery of the value <em>k</em>. We'll also discuss what the concept of <em>error</em> means for k-means applications, and how to design an error algorithm that helps us achieve our goals. The following are the topics that will be covered in this chapter:</p>
<ul>
<li>Average and distance</li>
<li>Writing the k-means algorithm</li>
<li>Example 1—k-means on simple 2D data</li>
<li>Example 2—3D data</li>
<li>K-means where <em>k</em> is unknown</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Average and distance</h1>
                </header>
            
            <article>
                
<p>The k-means algorithm relies on two concepts in order to operate: average and distance. In order to tell you where the center of a cluster is, the algorithm will calculate an average value for these points. In this case, we will use the arithmetic mean, or the sum of values divided by the number of values, to represent our average. In ES5/classic JavaScript (I'm also being purposefully explicit in this example, for any readers who are not familiar with calculating the mean), we might write a function like this:</p>
<pre><span>/**<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} numbers<br/></span><span> * </span><span>@return </span><span>{float}<br/></span><span> */<br/></span><span>function </span><span>mean</span>(numbers) {<br/>    <span>var </span><span>sum </span>= <span>0</span>, <span>length </span>= numbers.<span>length</span>;<br/><br/>    <span>if </span>(<span>length </span>=== <span>0</span>) {<br/>        <span>/**<br/></span><span>         * Mathematically, the mean of an empty set is undefined,<br/></span><span>         * so we could return early here. We could also allow the function<br/></span><span>         * to attempt dividing 0/0, would would return NaN in JavaScript but<br/></span><span>         * fail in some other languages (so probably a bad habit to encourage).<br/></span><span>         * Ultimately, I would like this function to not return mixed types,<br/></span><span>         * so instead let's throw an error.<br/></span><span>         */<br/></span><span>        </span><span>throw new </span>Error(<span>'Cannot calculate mean of empty set'</span>);<br/>    }<br/><br/>    <span>for </span>(<span>var </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>length</span>; <span>i</span>++) {<br/>        <span>sum </span>+= numbers[<span>i</span>];<br/>    }<br/><br/>    <span>return </span><span>sum </span>/ <span>length</span>;<br/>}</pre>
<p class="mce-root">In ES6, we can abuse our shorthand privileges and write the following:</p>
<pre><span>const </span><span>mean </span>= numbers =&gt; numbers.<span>reduce</span>((sum, val) =&gt; sum + val, <span>0</span>) / numbers.<span>length</span>;</pre>
<p class="mce-root">This is a handy ES6 one-liner to keep in your back pocket, however, it assumes all values are already numeric and defined, and it will return NaN if you give it an empty array. If the shorthand is confusing, we can break it up like so:</p>
<pre class="mce-root">const sum = (numbers) =&gt; numbers.reduce((sum, val) =&gt; sum + val, 0);<br/>const mean = (numbers) =&gt; sum(numbers) / numbers.length;</pre>
<p class="mce-root">Keep in mind we can use any type of average, including the median and mode. In fact, it's sometimes preferable to use k-medians over k-means. The median does a better job of muting outliers than the mean does. You should therefore always ask yourself which average you actually need. If you want a representation of total resources consumed, for instance, you might use the arithmetic mean. If you suspect outliers are caused by faulty measurements and should be ignored, k-medians could suit you better.<br/>
<br/>
We will also need a concept of distance in this algorithm. It can be any distance measure, however, for numeric data you will mostly use the typical Euclidean distance—the standard distance measure you'd learn in high school—which in ES5 JavaScript looks like this for two dimensions:</p>
<pre><span>/**<br/></span><span> * Calculates only the 2-dimensional distance between two points a and b.<br/></span><span> * Each point should be an array with length = 2, and both elements defined and numeric.<br/></span><span> * </span><span>@param </span><span>{Array.number} a<br/></span><span> * </span><span>@param </span><span>{Array.number} b<br/></span><span> * </span><span>@return </span><span>{float}<br/></span><span> */<br/></span><span>function </span><span>distance2d</span>(a, b) {<br/>    <span>// Difference between b[0] and a[0]<br/></span><span>    </span><span>var </span><span>diff_0 </span>= b[<span>0</span>] - a[<span>0</span>];<br/>    <span>// Difference between b[1] and a[1]<br/></span><span>    </span><span>var </span><span>diff_1 </span>= b[<span>1</span>] - a[<span>1</span>];<br/><br/>    <span>return </span><span>Math</span>.<span>sqrt</span>(<span>diff_0</span>*<span>diff_0 </span>+ <span>diff_1</span>*<span>diff_1</span>);<br/>}</pre>
<p class="mce-root">We must support many more than two dimensions, however, so we can generalize to the following:</p>
<pre><span>/**<br/></span><span> * Calculates the N-dimensional distance between two points a and b.<br/></span><span> * Each point should be an array with equal length, and all elements defined and numeric.<br/></span><span> * </span><span>@param </span><span>{Array.number} a<br/></span><span> * </span><span>@param </span><span>{Array.number} b<br/></span><span> * </span><span>@return </span><span>{float}<br/></span><span> */<br/></span><span>function </span><span>distance</span>(a, b) {<br/><br/>    <span>var </span><span>length </span>= a.length,<br/>        <span>sumOfSquares </span>= <span>0</span>;<br/><br/>    <span>if </span>(<span>length </span>!== b.length) {<br/>        <span>throw new </span>Error(<span>'Points a and b must be the same length'</span>);<br/>    }<br/><br/>    <span>for </span>(<span>var </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>length</span>; <span>i</span>++) {<br/>        <span>var </span><span>diff </span>= b[<span>i</span>] - a[<span>i</span>];<br/>        <span>sumOfSquares </span>+= <span>diff</span>*<span>diff</span>;<br/>    }<br/><br/>    <span>return </span><span>Math</span>.<span>sqrt</span>(<span>sumOfSquares</span>);<br/>}</pre>
<p class="mce-root">We can write an ES6 one-liner for this, but it won't be as readable as the lengthier, explicit example:</p>
<pre><span>const </span><span>distance </span>= (a, b) =&gt; <span>Math</span>.<span>sqrt</span>(<br/>    a.<span>map</span>((aPoint, i) =&gt; b[i] - aPoint)<br/>     .<span>reduce</span>((sumOfSquares, diff) =&gt; sumOfSquares + (diff*diff), <span>0</span>)<br/>);</pre>
<p class="mce-root">Armed with these tools, we can start writing the k-means algorithm itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the k-means algorithm</h1>
                </header>
            
            <article>
                
<p>The k-means algorithm is relatively simple to implement, so in this chapter we'll write it from scratch. The algorithm requires only two pieces of information: the <em>k</em> in k-means (the number of clusters we wish to identify), and the data points to evaluate. There are additional parameters the algorithm can use, for example, the maximum number of iterations to allow, but they are not required. The only required output of the algorithm is <em>k</em> centroids, or a list of points that represent the centers of the clusters of data. If <em>k = 3</em>, then the algorithm must return three centroids as its output. The algorithm may also return other metrics, such as the total error, the total number of iterations required to reach steady state, and so on, but again these are optional.</p>
<p>A high-level description of the k-means algorithm is as follows:</p>
<ol>
<li>Given the parameter <em>k</em> and the data to process, initialize <em>k</em> candidate centroids randomly</li>
<li>For each data point, determine which candidate centroid is closest to that point and <em>assign</em> the point to that centroid</li>
<li>For each centroid, update its position to be the mean position of all the points assigned to it</li>
<li>Repeat <em>Step 2</em> and <em>Step 3</em> until the centroids' positions reach steady state (that is, the centroids stop moving)</li>
</ol>
<p>At the end of this process, you may return the positions of the centroids as the algorithm's output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the environment</h1>
                </header>
            
            <article>
                
<p>Let's take a moment to set up our development environment for this algorithm. The environment will be as described in <a href="370a39e1-e618-475d-9722-9b4315a9a6ee.xhtml" target="_blank">Chapter 1</a>, <em>Exploring the Potential of JavaScript,</em> however, we'll run through the entire process here.</p>
<p>First, create a new folder for this project. I've named the folder <kbd>Ch4-kmeans</kbd>. Create a subfolder called <kbd>src</kbd> inside <kbd>Ch4-kmeans</kbd>.</p>
<p>Next, add a file called <kbd>package.json</kbd> to the <kbd>Ch4-kmeans</kbd> folder. Add the following content to the file:</p>
<pre>{<br/>  <span>"name"</span>: <span>"Ch4-kmeans"</span>,<br/>  <span>"version"</span>: <span>"1.0.0"</span>,<br/>  <span>"description"</span>: <span>"ML in JS Example for Chapter 4 - kmeans"</span>,<br/>  <span>"main"</span>: <span>"src/index.js"</span>,<br/>  <span>"author"</span>: <span>"Burak Kanber"</span>,<br/>  <span>"license"</span>: <span>"MIT"</span>,<br/>  <span>"scripts"</span>: {<br/>    <span>"build-web"</span>: <span>"browserify src/index.js -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"build-cli"</span>: <span>"browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"start"</span>: <span>"yarn build-cli &amp;&amp; node dist/index.js"<br/></span><span>  </span>},<br/>  <span>"dependencies"</span>: {<br/>    <span>"babel-core"</span>: <span>"^6.26.0"</span>,<br/>    <span>"babel-preset-env"</span>: <span>"^1.6.1"</span>,<br/>    <span>"babelify"</span>: <span>"^8.0.0"</span>,<br/>    <span>"browserify"</span>: <span>"^15.1.0"<br/></span><span>  </span>}<br/>}</pre>
<p>After creating the <kbd>package.json</kbd> file, switch to your terminal program and (from the <kbd>Ch4-kmeans</kbd> folder) run the <kbd>yarn install</kbd> command.</p>
<p>Next, create three new files inside the <kbd>Ch4-kmeans/src</kbd> folder: <kbd>index.js</kbd>, <kbd>data.js</kbd>, and <kbd>kmeans.js</kbd>. We will write the actual k-means algorithm inside <kbd>kmeans.js</kbd>, we will load some example data into <kbd>data.js</kbd>, and we'll use <kbd>index.js</kbd> as our bootstrapping point to set up and run a number of examples.</p>
<p>At this point, you may want to stop and test that everything is working. Add a simple <kbd>console.log("Hello");</kbd> to <kbd>index.js</kbd> and then run the command <kbd>yarn start</kbd> from the command line. You should see the file compile and run, printing <kbd>Hello</kbd> to the screen before exiting. If you get errors or do not see the <kbd>Hello</kbd>, you may want to take a step back and double-check your environment. If everything is working, you can delete the <kbd>console.log("Hello");</kbd> from <kbd>index.js</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing the algorithm</h1>
                </header>
            
            <article>
                
<p>In this section, we'll be working in the <kbd>kmeans.js</kbd> file. The first thing to do is to add our functions for mean and distance to the top of the file. Since these are generic functions that can be called <strong>statistically</strong>, we will not define them inside a class. Add the following to the top of the file:</p>
<pre><span>/**<br/></span><span> * Calculate the mean of an array of numbers.<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} numbers<br/></span><span> * </span><span>@return </span><span>{number}<br/></span><span> */<br/></span><span>const </span><span>mean </span>= numbers =&gt; numbers.<span>reduce</span>((sum, val) =&gt; sum + val, <span>0</span>) / numbers.<span>length</span>;<br/><br/><span>/**<br/></span><span> * Calculate the distance between two points.<br/></span><span> * Points must be given as arrays or objects with equivalent keys.<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} a<br/></span><span> * </span><span>@param </span><span>{Array.&lt;number&gt;} b<br/></span><span> * </span><span>@return </span><span>{number}<br/></span><span> */<br/></span><span>const </span><span>distance </span>= (a, b) =&gt; <span>Math</span>.<span>sqrt</span>(<br/>    a.<span>map</span>((aPoint, i) =&gt; b[i] - aPoint)<br/>     .<span>reduce</span>((sumOfSquares, diff) =&gt; sumOfSquares + (diff*diff), <span>0</span>)<br/>);</pre>
<p>Next, create and export a <kbd>KMeans</kbd> class. We will fill this in with many more methods throughout this chapter, but let's start with the following. Add this to the <kbd>kmeans.js</kbd> file beneath the code you just added:</p>
<pre><span>class </span>KMeans {<br/><br/>    <span>/**<br/></span><span>     * </span><span>@param </span><span>k<br/></span><span>     * </span><span>@param </span><span>data<br/></span><span>     */<br/></span><span>    </span><span>constructor</span>(k, data) {<br/>        <span>this</span>.<span>k </span>= k;<br/>        <span>this</span>.<span>data </span>= data;<br/>        <span>this</span>.<span>reset</span>();<br/>    }<br/><br/>    <span>/**<br/></span><span>     * Resets the solver state; use this if you wish to run the<br/></span><span>     * same solver instance again with the same data points<br/></span><span>     * but different initial conditions.<br/></span><span>     */<br/></span><span>    </span><span>reset</span>() {<br/>        <span>this</span>.<span>error </span>= <span>null</span>;<br/>        <span>this</span>.<span>iterations </span>= <span>0</span>;<br/>        <span>this</span>.<span>iterationLogs </span>= [];<br/>        <span>this</span>.<span>centroids </span>= <span>this</span>.<span>initRandomCentroids</span>();<br/>        <span>this</span>.<span>centroidAssignments </span>= [];<br/>    }<br/>    <br/>}<br/><br/><span>export default </span>KMeans;</pre>
<p>We've created a class called <kbd>KMeans</kbd> and are exporting it as the default export for this file. The preceding code also initializes some of the instance variables that the class will need, which we will describe shortly.</p>
<p>The constructor for the class takes two parameters, <kbd>k</kbd> and <kbd>data</kbd>, and stores both as instance variables. The <kbd>k</kbd> parameter represents the <kbd>k</kbd> in k-means, or the desired number of clusters as the algorithm's output. The <kbd>data</kbd> parameter is an array of data points that the algorithm will process.</p>
<p>At the end of the constructor, we call the <kbd>reset()</kbd> method, which is used to initialize (or reset) the solver's state. Specifically, the instance variables we initialize in the <kbd>reset</kbd> method are:</p>
<ul>
<li><kbd>iterations</kbd>, which is a simple counter of how many iterations the solver has run, starting from 0</li>
<li><kbd>error</kbd>, which records the <strong>root mean square error</strong> (<strong>RMSE</strong>) of the points' distance to their centroids for the current iteration</li>
<li><kbd>centroidAssignments</kbd>, which is an array of data point index numbers that map to a centroid index number</li>
<li><kbd>centroids</kbd>, which will store the solver's candidates for the <em>k</em> centroids at the current iteration</li>
</ul>
<p>Notice that in the <kbd>reset</kbd> method, we're making a call to <kbd>this.initRandomCentroids()</kbd>, which we have not yet defined. The k-means algorithm must start with a set of candidate centroids, so the purpose of that method is to generate the correct number of centroids randomly. Because the algorithm starts with a random state, it can be expected that multiple runs of the algorithm will return different results based on the initial conditions. This is actually a desired property of the k-means algorithm, because it is susceptible to finding local optima, and running the algorithm multiple times with different initial conditions may help you find the global optimum.</p>
<p>We have some prerequisites to satisfy before we can generate our random centroids. First, we must know the dimensionality of the data. Are we working with 2D data, 3D data, 10D data, or 1324D data? The random centroids we generate must have the same number of dimensions as the rest of the data points. This is an easy problem to solve; we assume that all the data points have the same number of dimensions, so we can just inspect the first data point we encounter. Add the following method to the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Determines the number of dimensions in the dataset.<br/></span><span> * </span><span>@return </span><span>{number}<br/></span><span> */<br/></span><span>getDimensionality</span>() {<br/>    <span>const </span><span>point </span>= <span>this</span>.<span>data</span>[<span>0</span>];<br/>    <span>return </span><span>point</span>.<span>length</span>;<br/>}</pre>
<p>The other consideration we must make when generating random initial centroids is that the centroids should be close to the data that we're working with. For instance, if all your data points are points between (0, 0) and (10, 10), you would not want to generate a random centroid such as (1200, 740). Similarly, if your data points are all negative, you would not want to generate positive centroids, and so on.</p>
<p>Why should we care where the random centroids start? In this algorithm, points will be assigned to the centroid closest to it and gradually <em>pull</em> the centroid towards the cluster center. If the centroids are all to the right of the data points, then the centroids themselves will follow similar paths towards the data and may get all clumped together in one single cluster, converging to a local optimum. By making sure that the centroids are randomly distributed within the range of the data, we have a better chance of avoiding this type of local optimum.</p>
<p>Our approach to generating our centroid starting positions will be to determine the range of each dimension of the data, and then choose random values for our centroid's position within those ranges. For instance, imagine three two-dimensional data points in an <em>x, </em><em>y</em> plane: (1, 3), (5, 8), and (3, 0). The range of the <em>x </em>dimension lies between 1 and 5, while the range of the <em>y</em> dimension lies between 0 and 8. Therefore, when creating a randomly initialized centroid, we will choose a random number between 1 and 5 for its <em>x</em> position, and a random number between 0 and 8 for its <em>y</em> position.</p>
<p>We can use JavaScript's <kbd>Math.min</kbd> and <kbd>Math.max</kbd> to determine the data ranges for each dimension. Add the following method to the <kbd>KMeans</kbd> class:</p>
<pre><br/><span>/**<br/></span><span> * For a given dimension in the dataset, determine the minimum<br/></span><span> * and maximum value. This is used during random initialization<br/></span><span> * to make sure the random centroids are in the same range as<br/></span><span> * the data.<br/></span><span> *<br/></span><span> * </span><span>@param </span><span>n<br/></span><span> * </span><span>@returns </span><span>{{min: *, max: *}}<br/></span><span> */<br/></span><span>getRangeForDimension</span>(n) {<br/>    <span>const </span><span>values </span>= <span>this</span>.<span>data</span>.<span>map</span>(point =&gt; point[n]);<br/>    <span>return </span>{<br/>        <span>min</span>: <span>Math</span>.<span>min</span>.<span>apply</span>(<span>null</span>, <span>values</span>),<br/>        <span>max</span>: <span>Math</span>.<span>max</span>.<span>apply</span>(<span>null</span>, <span>values</span>)<br/>    };<br/>}</pre>
<p class="mce-root">This method first collects all the values of the given dimension from the data points as an array and then returns an object containing that range's <kbd>min</kbd> and <kbd>max</kbd>. Returning to our preceding example of three data points ((1, 3), (5, 8), and (3, 0)), calling <kbd>getRangeForDimension(0)</kbd> would return <kbd>{min: 1, max: 5}</kbd>, and calling <kbd>getRangeForDimension(1)</kbd> would return <kbd>{min: 0, max: 8}</kbd>.</p>
<p>It will be useful for us to have an object of all dimensions and their ranges that we can cache while initializing centroids, so add the following method to the <kbd>KMeans</kbd> class as well:</p>
<pre><span>/**<br/></span><span> * Get ranges for all dimensions.<br/></span><span> * </span><span>@see </span><span>getRangeForDimension<br/></span><span> * </span><span>@returns </span><span>{Array} Array whose indices are the dimension number and whose members are the output of getRangeForDimension<br/></span><span> */<br/></span><span>getAllDimensionRanges</span>() {<br/>    <span>const </span><span>dimensionRanges </span>= [];<br/>    <span>const </span><span>dimensionality </span>= <span>this</span>.<span>getDimensionality</span>();<br/><br/>    <span>for </span>(<span>let </span><span>dimension </span>= <span>0</span>; <span>dimension </span>&lt; <span>dimensionality</span>; <span>dimension</span>++) {<br/>        <span>dimensionRanges</span>[<span>dimension</span>] = <span>this</span>.<span>getRangeForDimension</span>(<span>dimension</span>);<br/>    }<br/><br/>    <span>return </span><span>dimensionRanges</span>;<br/><br/>}</pre>
<p>This method simply looks at all dimensions and returns the <kbd>min</kbd> and <kbd>max</kbd> ranges for each, structured as objects in an array indexed by the dimension. This method is primarily a convenience, but we will use it shortly.</p>
<p>We can finally generate our randomly initialized centroids. We will need to create <em>k</em> centroids, and work dimension by dimension to choose a random point inside the range of each dimension. Add the following method to the <kbd>KMeans</kbd> class:</p>
<pre><br/><span>/**<br/></span><span> * Initializes random centroids, using the ranges of the data<br/></span><span> * to set minimum and maximum bounds for the centroids.<br/></span><span> * You may inspect the output of this method if you need to debug<br/></span><span> * random initialization, otherwise this is an internal method.<br/></span><span> * </span><span>@see </span><span>getAllDimensionRanges<br/></span><span> * </span><span>@see </span><span>getRangeForDimension<br/></span><span> * </span><span>@returns </span><span>{Array}<br/></span><span> */<br/></span><span>initRandomCentroids</span>() {<br/><br/>    <span>const </span><span>dimensionality </span>= <span>this</span>.<span>getDimensionality</span>();<br/>    <span>const </span><span>dimensionRanges </span>= <span>this</span>.<span>getAllDimensionRanges</span>();<br/>    <span>const </span><span>centroids </span>= [];<br/><br/>    <span>// We must create 'k' centroids.<br/></span><span>    </span><span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>k</span>; <span>i</span>++) {<br/><br/>        <span>// Since each dimension has its own range, create a placeholder at first<br/></span><span>        </span><span>let </span><span>point </span>= [];<br/><br/>        <span>/**<br/></span><span>         * For each dimension in the data find the min/max range of that dimension,<br/></span><span>         * and choose a random value that lies within that range. <br/></span><span>         */<br/></span><span>        </span><span>for </span>(<span>let </span><span>dimension </span>= <span>0</span>; <span>dimension </span>&lt; <span>dimensionality</span>; <span>dimension</span>++) {<br/>            <span>const </span>{<span>min</span>, <span>max</span>} = <span>dimensionRanges</span>[<span>dimension</span>];<br/>            <span>point</span>[<span>dimension</span>] = <span>min </span>+ (<span>Math</span>.<span>random</span>()*(<span>max-min)</span>);<br/>        }<br/><br/>        <span>centroids</span>.<span>push</span>(<span>point</span>);<br/><br/>    }<br/><br/>    <span>return </span><span>centroids</span>;<br/><br/>}</pre>
<p>The preceding algorithm contains two loops; the outer loop creates <kbd>k</kbd> candidate centroids. Because the number of dimensions in the dataset is arbitrary, and because each dimension itself has an arbitrary range, we must then work dimension by dimension for each centroid in order to generate a random position. If your data is three-dimensional, the inner loop will consider dimensions 0, 1, and 2 separately, determining the <kbd>min</kbd> and <kbd>max</kbd> values for each dimension, choosing a random value in that range, and assigning that value to that specific dimension of the centroid point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing random centroid generation</h1>
                </header>
            
            <article>
                
<p>We've already written a significant amount of code, so now would be a good time to stop and test our work. We should also start setting up our <kbd>data.js</kbd> file that we'll use to hold some example data.</p>
<p>Open up the <kbd>data.js</kbd> file and add the following:</p>
<pre><span>const </span><span>example_randomCentroids </span>= [<br/>    [<span>1</span>, <span>3</span>], [<span>5</span>, <span>8</span>], [<span>3</span>, <span>0</span>]<br/>];<br/><br/><span>export default </span>{<br/>    <span>example_randomCentroids<br/></span>};</pre>
<p>The values used are the same ones from our simple data point example written in the preceding block.</p>
<p>Now, switch to <kbd>index.js</kbd> and add the following code:</p>
<pre><span>import </span>KMeans <span>from </span><span>'./kmeans.js'</span>;<br/><span>import </span>example_data <span>from </span><span>'./data.js'</span>;<br/><br/><span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>ML in JS Chapter 4 k-means clustering examples."</span>);<br/><span>console</span>.<span>log</span>(<span>"===============================================</span><span>\n</span><span>"</span>);<span><br/></span><span><br/></span><span>console</span>.<span>log</span>(<span>"Testing centroid generation:"</span>);<br/><span>console</span>.<span>log</span>(<span>"===============================================</span><span>\n</span><span>"</span>);<br/><br/><span>const </span><span>ex_randomCentroids_solver </span>= <span>new </span>KMeans(<span>2</span>, example_data.<span>example_randomCentroids</span>);<br/><br/><span>console</span>.<span>log</span>(<span>"Randomly initialized centroids: "</span>);<br/><span>console</span>.<span>log</span>(<span>ex_randomCentroids_solver</span>.<span>centroids</span>);<br/><span>console</span>.<span>log</span>(<span>"</span><span>\n</span><span>-----------------------------------------------</span><span>\n\n</span><span>"</span>);</pre>
<p>First, we import the <kbd>KMeans</kbd> class and the <kbd>example_data</kbd> object from their respective files. We print some helpful output to the screen, and then initialize a <kbd>KMeans</kbd> solver instance for our simple data. We can check the randomly initialized centroids by inspecting the value of <kbd>ex_randomCentroids_solver.centroids</kbd>.</p>
<p>Once you've added this code, run <kbd>yarn start</kbd> from the command line, and you should see something similar to the following output. Note that because the centroid initialization is random, you will not see the same values that I see; however, what we're looking for is to make sure that the random centroids lie within the correct ranges. Specifically, we want our centroids to have <em>x</em> positions between 1 and 5, and <em>y</em> positions between 0 and 8. Run the code a number of times to make sure that the centroids have the correct position:</p>
<pre class="mce-root"><strong>$ yarn start</strong><br/><strong>yarn run v1.3.2</strong><br/><strong>$ yarn build-cli &amp;&amp; node dist/index.js</strong><br/><strong>$ browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]</strong><br/> <br/><strong> ML in JS Chapter 4 k-means clustering examples.</strong><br/><strong> ===============================================</strong><br/> <br/><strong> Testing centroid generation:</strong><br/><strong> ===============================================</strong><br/> <br/><strong> Randomly initialized centroids:</strong><br/><strong> [ [ 4.038663181817283, 7.765675509733137 ],</strong><br/><strong> [ 1.976405159755187, 0.026837564634993427 ] ]</strong></pre>
<p>If you see something similar to the preceding block, that means everything is working well so far and we're ready to continue implementing the algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assigning points to centroids</h1>
                </header>
            
            <article>
                
<p>The iteration loop that the k-means algorithm performs has two steps: assigning each point to the centroid closest to it, and then updating the location of the centroids to be the mean value of all the points assigned to that centroid. In this section, we'll implement the first part of the algorithm: assigning points to centroids.</p>
<p>At a high level, our task is to consider each point in the dataset and determine which centroid is closest to it. We also need to record the results of this assignment so that we can later update the centroid's location based on the points assigned to it.</p>
<p>Add the following method to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Given a point in the data to consider, determine the closest<br/></span><span> * centroid and assign the point to that centroid.<br/></span><span> * The return value of this method is a boolean which represents<br/></span><span> * whether the point's centroid assignment has changed;<br/></span><span> * this is used to determine the termination condition for the algorithm.<br/></span><span> * </span><span>@param </span><span>pointIndex<br/></span><span> * </span><span>@returns </span><span>{boolean} Did the point change its assignment?<br/></span><span> */<br/></span><span>assignPointToCentroid</span>(pointIndex) {<br/><br/>    <span>const </span><span>lastAssignedCentroid </span>= <span>this</span>.<span>centroidAssignments</span>[pointIndex];<br/>    <span>const </span><span>point </span>= <span>this</span>.<span>data</span>[pointIndex];<br/>    <span>let </span><span>minDistance </span>= <span>null</span>;<br/>    <span>let </span><span>assignedCentroid </span>= <span>null</span>;<br/><br/>    <span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>centroids</span>.<span>length</span>; <span>i</span>++) {<br/>        <span>const </span><span>centroid </span>= <span>this</span>.<span>centroids</span>[<span>i</span>];<br/>        <span>const </span><span>distanceToCentroid </span>= <span>distance</span>(<span>point</span>, <span>centroid</span>);<br/><br/>        <span>if </span>(<span>minDistance </span>=== <span>null </span>|| <span>distanceToCentroid </span>&lt; <span>minDistance</span>) {<br/>            <span>minDistance </span>= <span>distanceToCentroid</span>;<br/>            <span>assignedCentroid </span>= <span>i</span>;<br/>        }<br/><br/>    }<br/><br/>    <span>this</span>.<span>centroidAssignments</span>[pointIndex] = <span>assignedCentroid</span>;<br/><br/>    <span>return </span><span>lastAssignedCentroid </span>!== <span>assignedCentroid</span>;<br/><br/>}</pre>
<p>This method considers a single data point, given by its index, and considers each centroid in the system in turn. We also keep track of the last centroid this point has been assigned to in order to determine if the assignment has changed.</p>
<p>In the preceding code, we loop over all centroids and use our <kbd>distance</kbd> function to determine the distance between the point and the centroid. If the distance is less than the lowest distance we've seen so far, or if this is the first centroid we're considering for this point (<kbd>minDistance</kbd> will be null in that case), we record the distance and the index position of the centroid. After looping over all centroids, we will now know which centroid is closest to the point under consideration.</p>
<p>Finally, we record the assignment of the centroid to the point by setting it to the <kbd>this.centroidAssignments</kbd> array—in this array, the index is the index of the point, and the value is the index of the centroid. We return a Boolean from this method by comparing the last known centroid assignment to the new centroid assignment—it will return <kbd>true</kbd> if the assignment has changed, or <kbd>false</kbd> if not. We'll use this information to figure out when the algorithm has reached steady state.</p>
<p>The previous method considers only a single point, so we should also write a method to process the centroid assignments of all points. Additionally, the method we write should determine if <em>any</em> point has updated its centroid assignment. Add the following to the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * For all points in the data, call assignPointsToCentroids<br/></span><span> * and returns whether _any_ point's centroid assignment has<br/></span><span> * been updated.<br/></span><span> *<br/></span><span> * </span><span>@see </span><span>assignPointToCentroid<br/></span><span> * </span><span>@returns </span><span>{boolean} Was any point's centroid assignment updated?<br/></span><span> */<br/></span><span>assignPointsToCentroids</span>() {<br/>    <span>let </span><span>didAnyPointsGetReassigned </span>= <span>false</span>;<br/>    <span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>data</span>.<span>length</span>; <span>i</span>++) {<br/>        <span>const </span><span>wasReassigned </span>= <span>this</span>.<span>assignPointToCentroid</span>(<span>i</span>);<br/>        <span>if </span>(<span>wasReassigned</span>) <span>didAnyPointsGetReassigned </span>= <span>true</span>;<br/>    }<br/>    <span>return </span><span>didAnyPointsGetReassigned</span>;<br/>}</pre>
<p>This method defines a variable called <kbd>didAnyPointsGetReassigned</kbd>, initialized to <kbd>false</kbd>, and then loops over all points in the dataset to update their centroid assignments. If any point is assigned to a new centroid, the method will return <kbd>true</kbd>. If no assignments were changed, the method returns <kbd>false</kbd>. The return value from this method will become one of our termination conditions; if no points update after an iteration, we can consider the algorithm to have reached steady state and can terminate it.</p>
<p>Let's now address the second part of the k-means algorithm: updating centroid locations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating centroid locations</h1>
                </header>
            
            <article>
                
<p>In the previous section, we implemented the first part of the k-means algorithm: looking at all points in the dataset and assigning them to the centroid that's geographically closest. The next step in the algorithm is to look at all centroids and update their locations to the mean value of all the points assigned to them.</p>
<p>To make an analogy, you can imagine each point reaching out and grabbing the centroid closest to it. The points give the centroid a tug, trying to pull it closer to them. We've already implemented the <em>reach out and grab portion</em> of the algorithm, and now we'll implement the <em>pull the centroid closer</em> portion.</p>
<p>At a high level, our task is to loop over all the centroids, and for each, determine the mean position of all the points assigned to it. We'll then update the centroid's position to that mean value. Breaking this down further, we must first collect all the points assigned to a centroid, and then we have to calculate the average value of these points, always keeping in mind that points can have any number of dimensions.</p>
<p>Let's start with the easy task of collecting all the points assigned to a centroid. We already have a mapping of point indexes to centroid indexes in our <kbd>this.centroidAssignments</kbd> instance variable. Add the following code to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Given a centroid to consider, returns an array<br/></span><span> * of all points assigned to that centroid.<br/></span><span> *<br/></span><span> * </span><span>@param </span><span>centroidIndex<br/></span><span> * </span><span>@returns </span><span>{Array}<br/></span><span> */<br/></span><span>getPointsForCentroid</span>(centroidIndex) {<br/>    <span>const </span><span>points </span>= [];<br/>    <span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>data</span>.<span>length</span>; <span>i</span>++) {<br/>        <span>const </span><span>assignment </span>= <span>this</span>.<span>centroidAssignments</span>[<span>i</span>];<br/>        <span>if </span>(<span>assignment </span>=== centroidIndex) {<br/>            <span>points</span>.<span>push</span>(<span>this</span>.<span>data</span>[<span>i</span>]);<br/>        }<br/>    }<br/>    <span>return </span><span>points</span>;<br/>}</pre>
<p>The preceding method is quite standard: looping over all data points, we look up that point's centroid assignment, and if it is assigned to the centroid in question, we add the point to an output array.</p>
<p>We can now use this list of points to update the centroid's location. Our goal is to update the centroid's location to be the mean value of all the points we found previously. Because the data may be multi-dimensional, we must also consider each dimension independently.</p>
<p>Using our simple example of points (1, 3), (5, 8), and (3, 0), we would find a mean location of (3, 3.6667). To get this value, we first calculate the mean value of the <em>x</em> dimension ( (1 + 5 + 3) / 3 = 3 ), and then calculate the mean value of the <em>y</em> dimension ( (3 + 8 + 0) / 3 = 11/3 = 3.6666... ). If we're working in more than two dimensions, we simply repeat the procedure for each dimension.</p>
<p>We can write this algorithm in JavaScript. Add the following to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Given a centroid to consider, update its location to<br/></span><span> * the mean value of the positions of points assigned to it.<br/></span><span> * </span><span>@see </span><span>getPointsForCentroid<br/></span><span> * </span><span>@param </span><span>centroidIndex<br/></span><span> * </span><span>@returns </span><span>{Array}<br/></span><span> */<br/></span><span>updateCentroidLocation</span>(centroidIndex) {<br/>    <span>const </span><span>thisCentroidPoints </span>= <span>this</span>.<span>getPointsForCentroid</span>(centroidIndex);<br/>    <span>const </span><span>dimensionality </span>= <span>this</span>.<span>getDimensionality</span>();<br/>    <span>const </span><span>newCentroid </span>= [];<br/>    <span>for </span>(<span>let </span><span>dimension </span>= <span>0</span>; <span>dimension </span>&lt; <span>dimensionality</span>; <span>dimension</span>++) {<br/>        <span>newCentroid</span>[<span>dimension</span>] = <span>mean</span>(<span>thisCentroidPoints</span>.<span>map</span>(point =&gt; point[<span>dimension</span>]));<br/>    }<br/>    <span>this</span>.<span>centroids</span>[centroidIndex] = <span>newCentroid</span>;<br/>    <span>return </span><span>newCentroid</span>;<br/>}</pre>
<p>The preceding method considers only one centroid at a time, specified by its index. We use the <kbd>getPointsForCentroid</kbd> method that we just added to get an array of points assigned to this centroid. We initialize a variable called <kbd>newCentroid</kbd> as an empty array; this will ultimately replace the current centroid.</p>
<p>Considering one dimension at a time, we collect the positions of the points <em>in that dimension only,</em> and then calculate the mean. We use JavaScript's <kbd>Array.map</kbd> to extract the positions for the correct dimension only, and then we use our <kbd>mean</kbd> function to calculate the average position in that dimension.</p>
<p>If we work this example by hand with the data points (1, 3), (5, 8), and (3, 0), we start by examining dimension 0, or the <em>x</em> dimension. The result of <kbd>thisCentroidPoints.map(point =&gt; point[dimension])</kbd> is the array <kbd>[1, 5, 3]</kbd> for dimension 0, and for dimension 1, the result is <kbd>[3, 8, 0]</kbd>. Each of these arrays is passed to the <kbd>mean</kbd> function, and the mean value is used in <kbd>newCentroid</kbd> for that dimension.</p>
<p>At the end of this method, we update our array of <kbd>this.centroids</kbd> with the newly calculated centroid position.</p>
<p>We will also write a convenience method to loop over all centroids and update their positions. Add the following to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * For all centroids, call updateCentroidLocation<br/></span><span> */<br/></span><span>updateCentroidLocations</span>() {<br/>    <span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>centroids</span>.<span>length</span>; <span>i</span>++) {<br/>        <span>this</span>.<span>updateCentroidLocation</span>(<span>i</span>);<br/>    }<br/>}</pre>
<p>Before finishing the algorithm and tying together all the pieces, we have one final prerequisite to satisfy. We're going to introduce the concept of <em>error</em> into the algorithm. Calculating the error is not required for the k-means algorithm to function, but you'll see later that this can be advantageous in certain situations.</p>
<p>Because this is an unsupervised learning algorithm, our concept of error does not relate to semantic error. Instead, we will use an error metric that represents the average distance of all points from their assigned centroids. We'll use the RMSE for this, which penalizes bigger distances more harshly, so our error metric will be a good indication of how tight the clustering is.</p>
<p>To perform this error calculation, we loop over all points and determine the distance of that point from its centroid. We square each distance before adding it to a running total (the <em>squared</em> in root-mean-squared), then divide the running total by the number of points (the <em>mean</em> in root-mean-squared), and finally take the square root of the whole thing (the <em>root</em> in root-mean-squared).</p>
<p>Add the following code to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Calculates the total "error" for the current state<br/></span><span> * of centroid positions and assignments.<br/></span><span> * Here, error is defined as the root-mean-squared distance<br/></span><span> * of all points to their centroids.<br/></span><span> * </span><span>@returns </span><span>{Number}<br/></span><span> */<br/></span><span>calculateError</span>() {<br/><br/>    <span>let </span><span>sumDistanceSquared </span>= <span>0</span>;<br/>    <span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>this</span>.<span>data</span>.<span>length</span>; <span>i</span>++) {<br/>        <span>const </span><span>centroidIndex </span>= <span>this</span>.<span>centroidAssignments</span>[<span>i</span>];<br/>        <span>const </span><span>centroid </span>= <span>this</span>.<span>centroids</span>[<span>centroidIndex</span>];<br/>        <span>const </span><span>point </span>= <span>this</span>.<span>data</span>[<span>i</span>];<br/>        <span>const </span><span>thisDistance </span>= <span>distance</span>(<span>point</span>, <span>centroid</span>);<br/>        <span>sumDistanceSquared </span>+= <span>thisDistance</span>*<span>thisDistance</span>;<br/>    }<br/><br/>    <span>this</span>.<span>error </span>= <span>Math</span>.<span>sqrt</span>(<span>sumDistanceSquared </span>/ <span>this</span>.<span>data</span>.<span>length</span>);<br/>    <span>return this</span>.<span>error</span>;<br/>}</pre>
<p>We're now ready to tie everything together and implement the main loop of the algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The main loop</h1>
                </header>
            
            <article>
                
<p>All the supporting and foundational logic for the k-means algorithm is now implemented. The last thing to do is to tie it all together and implement the main loop of the algorithm. To run the algorithm, we should repeat the procedure of assigning points to centroids and then updating centroid locations until the centroids stop moving. We can also perform optional steps such as calculating the error and making sure that the algorithm does not exceed some maximum allowable number of iterations.</p>
<p>Add the following code to the body of the <kbd>KMeans</kbd> class:</p>
<pre><span>/**<br/></span><span> * Run the k-means algorithm until either the solver reaches steady-state,<br/></span><span> * or the maxIterations allowed has been exceeded.<br/></span><span> *<br/></span><span> * The return value from this method is an object with properties:<br/></span><span> * {<br/></span><span> *  centroids {Array.&lt;Object&gt;},<br/></span><span> *  iteration {number},<br/></span><span> *  error {number},<br/></span><span> *  didReachSteadyState {Boolean}<br/></span><span> * }<br/></span><span> *<br/></span><span> * You are most likely interested in the centroids property of the output.<br/></span><span> *<br/></span><span> * </span><span>@param </span><span>{Number} maxIterations Default 1000<br/></span><span> * </span><span>@returns </span><span>{Object}<br/></span><span> */<br/></span><span>solve</span>(maxIterations = <span>1000</span>) {<br/><br/>    <span>while </span>(<span>this</span>.<span>iterations </span>&lt; maxIterations) {<br/><br/>        <span>const </span><span>didAssignmentsChange </span>= <span>this</span>.<span>assignPointsToCentroids</span>();<br/>        <span>this</span>.<span>updateCentroidLocations</span>();<br/>        <span>this</span>.<span>calculateError</span>();<br/><br/>        <span>this</span>.<span>iterationLogs</span>[<span>this</span>.<span>iterations</span>] = {<br/>            <span>centroids</span>: [...<span>this</span>.<span>centroids]</span>,<br/>            <span>iteration</span>: <span>this</span>.<span>iterations</span>,<br/>            <span>error</span>: <span>this</span>.<span>error</span>,<br/>            <span>didReachSteadyState</span>: !<span>didAssignmentsChange<br/></span><span>        </span>};<br/><br/>        <span>if </span>(<span>didAssignmentsChange </span>=== <span>false</span>) {<br/>            <span>break</span>;<br/>        }<br/><br/>        <span>this</span>.<span>iterations</span>++;<br/><br/>    }<br/><br/>    <span>return this</span>.<span>iterationLogs</span>[<span>this</span>.<span>iterationLogs</span>.<span>length </span>- <span>1</span>];<br/><br/>}</pre>
<p>We've written a <kbd>solve</kbd> method that also accepts a limit on the maximum number of iterations allowed, which we've defaulted to <kbd>1000</kbd>. We run the algorithm in a <kbd>while</kbd> loop, and for each iteration in the loop, we call <kbd>assignPointsToCentroids</kbd> (recording its output value, <kbd>didAssignmentsChange</kbd>), call <kbd>updateCentroidLocations</kbd>, and call <kbd>calculateError</kbd>.</p>
<p>In order to help debugging and maintain a history of what the algorithm has accomplished, we maintain an array of <kbd>this.iterationLogs</kbd>, and for each iteration we'll record the centroid locations, the iteration number, the calculated error, and whether or not the algorithm has reached steady state (which is the opposite of <kbd>didAssignmentsChange</kbd>). We use ES6's array spread operator on <kbd>this.centroids</kbd> when recording logs to avoid passing this array as a reference, otherwise the iteration logs would show the last state of centroids instead of its progress over time.</p>
<p>If the point/centroid assignments don't change from one iteration to the next, we consider the algorithm to have reached steady state and can return results. We accomplish this by using the <kbd>break</kbd> keyword to break from the <kbd>while</kbd> loop early. If the algorithm never reaches steady state, the <kbd>while</kbd> loop will continue until it has performed the maximum number of iterations allowed, and return the latest available result. The output for the <kbd>solve</kbd> method is simply the most recent iteration log, which contains all the information a user of this class would need to know.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1 – k-means on simple 2D data</h1>
                </header>
            
            <article>
                
<p>We've got our implementation of the k-means algorithm written and coded, so now it's time to see how it works. In our first example, we'll run our algorithm against a simple dataset of two-dimensional data. The data itself will be contrived so that the algorithm can easily find three distinct clusters.</p>
<p>First, modify the <kbd>data.js</kbd> file to add the following data, anywhere preceding the <kbd>export default</kbd> line:</p>
<pre><span>const </span><span>example_2d3k </span>= [<br/>    [<span>1</span>, <span>2</span>], [<span>2</span>, <span>3</span>], [<span>2</span>, <span>5</span>], [<span>1</span>, <span>6</span>], [<span>4</span>, <span>6</span>],<br/>    [<span>3</span>, <span>5</span>], [<span>2</span>, <span>4</span>], [<span>4</span>, <span>3</span>], [<span>5</span>, <span>2</span>], [<span>6</span>, <span>9</span>],<br/>    [<span>4</span>, <span>4</span>], [<span>3</span>, <span>3</span>], [<span>8</span>, <span>6</span>], [<span>7</span>, <span>5</span>], [<span>9</span>, <span>6</span>],<br/>    [<span>9</span>, <span>7</span>], [<span>8</span>, <span>8</span>], [<span>7</span>, <span>9</span>], [<span>11</span>, <span>3</span>], [<span>11</span>, <span>2</span>],<br/>    [<span>9</span>, <span>9</span>], [<span>7</span>, <span>8</span>], [<span>6</span>, <span>8</span>], [<span>12</span>, <span>2</span>], [<span>14</span>, <span>3</span>],<br/>    [<span>15</span>, <span>1</span>], [<span>15</span>, <span>4</span>], [<span>14</span>, <span>2</span>], [<span>13</span>, <span>1</span>], [<span>16</span>, <span>4</span>]<br/>];</pre>
<p>Then, update the final export line to look like this:</p>
<pre><span>export default </span>{<br/>    <span>example_randomCentroids</span>,<br/>    <span>example_2d3k<br/></span>};</pre>
<p>If we were to graph the preceding data points, we would see the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/29251362-e458-4369-b23a-b53b12378019.png" style="width:38.33em;height:23.67em;"/></div>
<p>Visually, we can see that there are three neatly clustered groups of data points. When we run the algorithm, we will use <em>k = 3</em> and expect that the centroids neatly position themselves to the centers of those three clusters. Let's try it out.</p>
<p>Open up <kbd>index.js</kbd> and add the following. You can either replace the code you added earlier (preserving the <kbd>import</kbd> statements), or simply add this at the bottom:</p>
<pre><strong><span>console</span>.<span>log</span>(<span>"Solving example: 2d data with 3 clusters:"</span>);</strong><br/><strong><span>console</span>.<span>log</span>(<span>"===============================================</span><span>\n</span><span>"</span>);</strong><br/><br/><br/><strong><span>console</span>.<span>log</span>(<span>"Solution for 2d data with 3 clusters:"</span>);</strong><br/><strong><span>console</span>.<span>log</span>(<span>"-------------------------------------"</span>);</strong><br/><strong><span>const </span><span>ex_1_solver </span>= <span>new </span>KMeans(<span>3</span>, example_data.<span>example_2d3k</span>);</strong><br/><strong><span>const </span><span>ex_1_centroids </span>= <span>ex_1_solver</span>.<span>solve</span>();</strong><br/><strong><span>console</span>.<span>log</span>(<span>ex_1_centroids</span>);</strong><br/><strong><span>console</span>.<span>log</span>(<span>""</span>);</strong></pre>
<p>After outputting some headers, we create a new <kbd>KMeans</kbd> instance called <kbd>ex_1_solver</kbd> and initialize it with k = 3 and the <kbd>example_data.example_2d3k</kbd> that we just added. We call the <kbd>solve</kbd> method, with no parameters (that is, the max allowed iterations will be 1,000), and capture the output in the variable <kbd>ex_1_centroids</kbd>. Finally, we print the results to the screen and add a newline—we'll add a few more tests and examples following this point.</p>
<div class="packt_tip">Note that the order of your centroids may be different to mine, since random initial conditions will differ.</div>
<p><span>You can now run </span><kbd>yarn start</kbd><span> and should see output that looks similar to </span>this<span>. </span>Additionally, because of the random initialization, it's possible that some runs of the solver will get caught in local optima and you'll see different centroids. Run the program a few times in a row and see what happens. Here's what my output looks like:</p>
<pre><strong>Solving example: 2d data with 3 clusters:</strong><br/><strong>==========================================================</strong><br/> <br/><strong>Solution for 2d data with 3 clusters:</strong><br/><strong>---------------------------------------------------------</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 7.6, 7.5 ] ],</strong><br/><strong>   iteration: 1,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong></pre>
<p>The output of the program tells us that the algorithm reached steady state after only two iterations (iteration 1 is the 2nd iteration, because we start counting from zero), and that our centroids are located at (2.8, 3.9), (13.4, 2.4), and (7.6, 7.5).</p>
<p>Let's graph these centroids along with the original data and see what it looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b5d4cbd5-39f2-4a08-8b02-9aa82f98d24c.png" style="width:39.75em;height:24.50em;"/></div>
<p>As you can see, k-means has done its job splendidly, reporting centroids exactly where we would expect them to be.</p>
<p>Let's take a deeper look into what this algorithm is doing, by printing the <kbd>iterationLogs</kbd> after the solution. Add the following code to the bottom of <kbd>index.js</kbd>:</p>
<pre><span>console</span>.<span>log</span>(<span>"Iteration log for 2d data with 3 clusters:"</span>);<br/><span>console</span>.<span>log</span>(<span>"------------------------------------------"</span>);<br/><span>ex_1_solver</span>.<span>iterationLogs</span>.<span>forEach</span>(log =&gt; <span>console</span>.<span>log</span>(log));<br/><span>console</span>.<span>log</span>(<span>""</span>);</pre>
<p>Run <kbd>yarn start</kbd> again, and you should see output like the following. As always, based on initial conditions, your version may have required more or fewer iterations than mine so your output will differ, but you should see something like this:</p>
<pre><strong>Solving example: 2d data with 3 clusters:</strong><br/><strong>=====================================================================</strong><br/> <br/><strong> Solution for 2d data with 3 clusters:</strong><br/><strong> --------------------------------------------------------------------</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 7.6, 7.5 ] ],</strong><br/><strong>   iteration: 4,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong><br/> <br/><strong> Iteration log for 2d data with 3 clusters:</strong><br/><strong> ----------------------------------------------------------------------</strong><br/><strong> { centroids: [ [ 2.7, 3.7 ], [ 9, 4.125 ], [ 10.75, 5.833333333333333 ] ],</strong><br/><strong>   iteration: 0,</strong><br/><strong>   error: 3.6193538404281806,</strong><br/><strong>   didReachSteadyState: false }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 9.714285714285714, 3.857142857142857 ],</strong><br/><strong>      [ 10.75, 5.833333333333333 ] ],</strong><br/><strong>   iteration: 1,</strong><br/><strong>   error: 3.4964164297074007,</strong><br/><strong>   didReachSteadyState: false }</strong><br/><strong> { centroids: [ [ 3.0833333333333335, 4.25 ], [ 11.375, 2.75 ], [ 10, 6.7 ] ],</strong><br/><strong>   iteration: 2,</strong><br/><strong>   error: 3.19709069137691,</strong><br/><strong>   didReachSteadyState: false }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 7.6, 7.5 ] ],</strong><br/><strong>   iteration: 3,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: false }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 7.6, 7.5 ] ],</strong><br/><strong>   iteration: 4,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong></pre>
<p>As you can see, the algorithm took five iterations to reach steady state, as opposed to only two iterations like it did earlier. This is normal and expected due to the differing random initial conditions between the two runs. Looking through the logs, you can see that the error reported by the algorithm goes down with time. Also, notice that the first centroid, (2.8, 3.9), reaches its final destination after the first iteration, while the other centroids take more time to catch up. This is because the first centroid was randomly initialized to a location very close to its final destination, starting at (2.7, 3.7) and finishing at (2.8, 3.9).</p>
<p>It is possible, though somewhat rare, to catch the algorithm caught in a local optimum on this dataset. Let's add the following code to the bottom of <kbd>index.js</kbd> to run the solver multiple times and see if it'll find a local optimum instead of a global optimum:</p>
<pre><span>console</span>.<span>log</span>(<span>"Test 2d data with 3 clusters five times:"</span>);<br/><span>console</span>.<span>log</span>(<span>"----------------------------------------"</span>);<br/><span>for </span>(<span>let </span><span>i </span>= <span>0</span>; <span>i </span>&lt; <span>5</span>; <span>i</span>++) {<br/>    <span>ex_1_solver</span>.<span>reset</span>();<br/>    <span>const </span><span>solution </span>= <span>ex_1_solver</span>.<span>solve</span>();<br/>    <span>console</span>.<span>log</span>(<span>solution</span>);<br/>}<br/><span>console</span>.<span>log</span>(<span>""</span>);</pre>
<p>Run this with <kbd>yarn start</kbd> a few times until you see an unexpected result. In my case, I found the following solution (I'm omitting the other output of the preceding program):</p>
<pre><strong>Test 2d data with 3 clusters five times:</strong><br/><strong>--------------------------------------------------------------</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 7.6, 7.5 ] ],</strong><br/><strong>   iteration: 2,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 7.6, 7.5 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ] ],</strong><br/><strong>   iteration: 1,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 7.6, 7.5 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ],</strong><br/><strong>      [ 2.8181818181818183, 3.909090909090909 ] ],</strong><br/><strong>   iteration: 3,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 11.333333333333334, 2.3333333333333335 ],</strong><br/><strong>      [ 5.095238095238095, 5.619047619047619 ],</strong><br/><strong>      [ 14.5, 2.5 ] ],</strong><br/><strong>   iteration: 2,</strong><br/><strong>   error: 3.0171467652692345,</strong><br/><strong>   didReachSteadyState: true }</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 7.6, 7.5 ],</strong><br/><strong>      [ 2.8181818181818183, 3.909090909090909 ],</strong><br/><strong>      [ 13.444444444444445, 2.4444444444444446 ] ],</strong><br/><strong>   iteration: 2,</strong><br/><strong>   error: 1.878739816915397,</strong><br/><strong>   didReachSteadyState: true }</strong></pre>
<p>The fourth run of the solver found a different answer from the other runs: it discovered (11.3, 2.3), (5.1, 5.6), (14.5, 2.5) as a solution. Because the other solution is much more common than this one, we can assume the algorithm has been trapped in a local optimum. Let's chart these values against the rest of the data and see what it looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/98271672-cb1c-4ee0-8c0a-7d5bb65e18b4.png" style="width:38.25em;height:23.67em;"/></div>
<p>In the preceding chart, our data points are represented by circles, the centroids we'd expect are represented by triangles, and the odd result we got is represented by <strong>X</strong> marks. Looking at the chart, you can understand how the algorithm may have come to this conclusion. One centroid, the <strong>X</strong> mark at (5.1, 5.6), has captured two different clusters and is sitting between them. The other two centroids have divided the third cluster into two. This is a perfect example of a local optimum: it's a solution that makes sense, it logically clusters the data points, but it is not the best available solution (the global optimum) for the data. Most likely, the two centroids on the right were both randomly initialized inside that cluster and got trapped there.</p>
<p>This is always a potential outcome for the k-means algorithm, and indeed all ML algorithms. Based on initial conditions and the quirks of the dataset, the algorithm may occasionally (or even frequently) discover local optima. Fortunately, if you compare the errors of the two solutions from the preceding output, the global solution has an error of 1.9 and the locally optimum solution reports an error of 3.0. In this case, our error calculation has done its job well, and correctly represented the tightness of the clustering.</p>
<p>To combat this issue with the k-means algorithm, you should generally run it more than one time, and look for either consensus (for example, four of five runs all agree), or the minimum error, and use that as your result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2 – 3D data</h1>
                </header>
            
            <article>
                
<p>Because we've written the k-means algorithm to handle any arbitrary number of dimensions, we can also test it with 3D data (or 10D, or 100D or any number of dimensions that you require). While this algorithm will work for more than three dimensions, we have no way of visually plotting the higher dimensions and therefore can't visually check the results—so we'll test with 3D data and move on.</p>
<p>Open up <kbd>data.js</kbd> and add the following to the middle of the file—anywhere preceding the <kbd>export default</kbd> line is OK:</p>
<pre><span>const </span><span>example_3d3k </span>= [<br/>    [<span>1</span>, <span>1</span>, <span>1</span>],<br/>    [<span>1</span>, <span>2</span>, <span>1</span>],<br/>    [<span>2</span>, <span>1</span>, <span>2</span>],<br/>    [<span>2</span>, <span>2</span>, <span>3</span>],<br/>    [<span>2</span>, <span>4</span>, <span>3</span>],<br/>    [<span>5</span>, <span>4</span>, <span>5</span>],<br/>    [<span>5</span>, <span>3</span>, <span>4</span>],<br/>    [<span>6</span>, <span>2</span>, <span>6</span>],<br/>    [<span>5</span>, <span>3</span>, <span>6</span>],<br/>    [<span>6</span>, <span>4</span>, <span>7</span>],<br/>    [<span>9</span>, <span>1</span>, <span>4</span>],<br/>    [<span>10</span>, <span>2</span>, <span>5</span>],<br/>    [<span>9</span>, <span>2</span>, <span>5</span>],<br/>    [<span>9</span>, <span>2</span>, <span>4</span>],<br/>    [<span>10</span>, <span>3</span>, <span>3</span>]<br/>];</pre>
<p>And then modify the export line to look like this (add the <kbd>example_3d3k</kbd> variable to the export):</p>
<pre><span>export default </span>{<br/>    <span>example_randomCentroids</span>,<br/>    <span>example_2d3k</span>,<br/>    <span>example_3d3k<br/></span>};</pre>
<p>The preceding data, when plotted in 3D, looks like this:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-301 image-border" src="assets/22e12169-6eea-4d16-ac25-6a0ce167876d.png" style="width:34.25em;height:23.17em;"/></div>
<p>As you can see, there are three clear clusters, and we'd expect k-means to handle this easily. Now, switch to <kbd>index.js</kbd> and add the following. We are simply creating a new solver for this example, loading the 3D data, and printing the results:</p>
<pre><span>console</span>.<span>log</span>(<span>"Solving example: 3d data with 3 clusters:"</span>);<br/><span>console</span>.<span>log</span>(<span>"===============================================</span><span>\n</span><span>"</span>);<br/><span>console</span>.<span>log</span>(<span>"Solution for 3d data with 3 clusters:"</span>);<br/><span>console</span>.<span>log</span>(<span>"-------------------------------------"</span>);<br/><span>const </span><span>ex_2_solver </span>= <span>new </span>KMeans(<span>3</span>, example_data.<span>example_3d3k</span>);<br/><span>const </span><span>ex_2_centroids </span>= <span>ex_2_solver</span>.<span>solve</span>();<br/><span>console</span>.<span>log</span>(<span>ex_2_centroids</span>);<br/><span>console</span>.<span>log</span>(<span>""</span>);</pre>
<p>Run the program with <kbd>yarn start</kbd> and you should see something like the following. I've omitted the output from the earlier 2D example:</p>
<pre><strong>Solving example: 3d data with 3 clusters:</strong><br/><strong>====================================================================</strong><br/> <br/><strong>Solution for 3d data with 3 clusters:</strong><br/><strong>---------------------------------------------------------------------</strong><br/><strong> { centroids: [ [ 1.6, 2, 2 ], [ 5.4, 3.2, 5.6 ], [ 9.4, 2, 4.2 ] ],</strong><br/><strong>   iteration: 5,</strong><br/><strong>   error: 1.3266499161421599,</strong><br/><strong>   didReachSteadyState: true }</strong></pre>
<p>Thankfully, our solver has given us 3D data points, so we know that, at the very least, the algorithm can differentiate between 2D and 3D problems. We see that it still only took a handful of iterations, and that the error is a reasonable number (meaning it's defined, not negative, and not too large).</p>
<p>If we plot these centroids against our original data we will see the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-302 image-border" src="assets/a9fd84dc-522b-4517-8b61-816538a9edc9.png" style="width:34.08em;height:22.75em;"/></div>
<p>The circles represent the data points, as before, and now we can see our black diamond centroids have found their homes in the middle of their clusters. Our algorithm has proven that it can work for three-dimensional data, and it will work just as well for any number of dimensions you can give it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-means where k is unknown</h1>
                </header>
            
            <article>
                
<p>So far, we've been able to define, in advance, how many clusters the algorithm should find. In each example, we've started the project knowing that our data has three clusters, so we've manually programmed a value of 3 for <em>k</em>. This is still a very useful algorithm, but you may not always know how many clusters are represented in your data. To solve this problem we need to extend the k-means algorithm.</p>
<p>A major reason I included the optional error calculation in our k-means implementation was to help solve this problem. Using an error metric—in any ML algorithm—doesn't only allow us to search for a solution, it also allows us to search for the best <em>parameters</em> that yield the best solution.</p>
<p>In a way, we need to build a meta-ML algorithm, or an algorithm that modifies our algorithm and its parameters. Our approach will be straightforward but effective: we'll build a new class called <kbd>KMeansAutoSolver</kbd>, and instead of specifying a value of <em>k</em>, we'll specify a range of <em>k</em> values to test. The new solver will run our k-means code for each value of <em>k</em> in the range and determine which value of <em>k</em> yields the lowest error. Additionally, we'll also run multiple trials of each <em>k</em> value so that we avoid getting caught in local optima.</p>
<p>Add a file to the <kbd>src/</kbd> folder called <kbd>kmeans-autosolver.js</kbd>. Add the following code to the file:</p>
<pre><span>import </span>KMeans <span>from </span><span>'./kmeans.js'</span>;<br/><br/><span>class </span>KMeansAutoSolver {<br/><br/>    <span>constructor</span>(kMin = <span>1</span>, kMax = <span>5</span>, maxTrials = <span>5</span>, data) {<br/>        <span>this</span>.<span>kMin </span>= kMin;<br/>        <span>this</span>.<span>kMax </span>= kMax;<br/>        <span>this</span>.<span>maxTrials </span>= maxTrials;<br/>        <span>this</span>.<span>data </span>= data;<br/>        <span>this</span>.<span>reset</span>();<br/>    }<br/><br/>    <span>reset</span>() {<br/>        <span>this</span>.<span>best </span>= <span>null</span>;<br/>        <span>this</span>.<span>log </span>= [];<br/>    }<br/><br/>    <span>solve</span>(maxIterations = <span>1000</span>) {<br/><br/>        <span>for </span>(<span>let </span><span>k </span>= <span>this</span>.<span>kMin</span>; <span>k </span>&lt; <span>this</span>.<span>kMax</span>; <span>k</span>++) {<br/><br/>            <span>for </span>(<span>let </span><span>currentTrial </span>= <span>0</span>; <span>currentTrial </span>&lt; <span>this</span>.<span>maxTrials</span>; <span>currentTrial</span>++) {<br/><br/>                <span>const </span><span>solver </span>= <span>new </span>KMeans(<span>k</span>, <span>this</span>.<span>data</span>);<br/>                <span>// Add k and currentTrial number to the solution before logging<br/></span><span>                </span><span>const </span><span>solution </span>= Object.<span>assign</span>({}, <span>solver</span>.<span>solve</span>(maxIterations), {<span>k</span>, <span>currentTrial</span>});<br/>                <span>this</span>.<span>log</span>.<span>push</span>(<span>solution</span>);<br/><br/>                <span>if </span>(<span>this</span>.<span>best </span>=== <span>null </span>|| <span>solution</span>.<span>error </span>&lt; <span>this</span>.<span>best</span>.<span>error</span>) {<br/>                    <span>this</span>.<span>best </span>= <span>solution</span>;<br/>                }<br/><br/>            }<br/><br/>        }<br/><br/>        <span>return this</span>.<span>best</span>;<br/><br/>    }<br/>}<br/><br/><span>export default </span>KMeansAutoSolver;</pre>
<p>The <kbd>KMeansAutoSolver</kbd> class contains a constructor which accepts <kbd>kMin</kbd>, <kbd>kMax</kbd>, <kbd>maxTrials</kbd>, and <kbd>data</kbd>. The <kbd>data</kbd> parameter is the same data you would give to the <kbd>KMeans</kbd> class. Instead of providing the class with a value for <em>k,</em> you provide a range of <em>k </em>values to test, as specified by <kbd>kMin</kbd> and <kbd>kMax</kbd>. Additionally, we'll also program this solver to run the k-means algorithm a number of times for each value of <em>k,</em> in order to avoid finding local optima, as we demonstrated earlier.</p>
<p>The main part of the class is the <kbd>solve</kbd> method, which, like the <kbd>KMeans</kbd> class, also accepts a <kbd>maxIterations</kbd> argument. The <kbd>solve</kbd> method also returns the same thing the <kbd>KMeans</kbd> class returns, except that we've also added the value of <em>k</em> to the output and also the <kbd>currentTrial</kbd> number. Adding the value of <em>k</em> to the output is a bit redundant, as you could just count the number of centroids returned, but it's nice to see in the output.</p>
<p>The body of the <kbd>solve</kbd> method is straightforward. For each value of <em>k</em> in the range between <kbd>kMin</kbd> and <kbd>kMax</kbd><em>,</em> we run the <kbd>KMeans</kbd> solver <kbd>maxTrials</kbd> times<em>.</em> If the solution beats the current best solution in terms of error, we record this solution as the best. At the end of the method, we return the solution with the best (lowest) error.</p>
<p>Let's try it out. Open up <kbd>data.js</kbd> and add the following:</p>
<pre><span>const </span><span>example_2dnk </span>= [<br/> [<span>1</span>, <span>2</span>], [<span>1</span>, <span>1</span>], [<span>2</span>, <span>3</span>], [<span>2</span>, <span>4</span>], [<span>3</span>, <span>3</span>],<br/> [<span>4</span>, <span>4</span>], [<span>2</span>, <span>12</span>], [<span>2</span>, <span>14</span>], [<span>3</span>, <span>14</span>], [<span>4</span>, <span>13</span>],<br/> [<span>4</span>, <span>15</span>], [<span>3</span>, <span>17</span>], [<span>8</span>, <span>4</span>], [<span>7</span>, <span>6</span>], [<span>7</span>, <span>5</span>],<br/> [<span>8</span>, <span>7</span>], [<span>9</span>, <span>7</span>], [<span>9</span>, <span>8</span>], [<span>8</span>, <span>14</span>], [<span>9</span>, <span>15</span>],<br/> [<span>9</span>, <span>16</span>], [<span>10</span>, <span>15</span>], [<span>10</span>, <span>14</span>], [<span>11</span>, <span>14</span>], [<span>10</span>, <span>18</span>]<br/>];</pre>
<p>And update the export line as follows:</p>
<pre><span>export default </span>{<br/>    <span>example_randomCentroids</span>,<br/>    <span>example_2d3k</span>,<br/>    <span>example_3d3k</span>,<br/>    <span>example_2dnk<br/></span>};</pre>
<p>Graphing this data, we see four neat clusters:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-68 image-border" src="assets/77724ab4-a38e-4f3a-8895-592e857470bd.png" style="width:35.08em;height:21.58em;"/></div>
<p>However, for the purposes of this example, we do not know how many clusters to expect, only that it's likely between one and five.</p>
<p>Next, open up <kbd>index.js</kbd> and import the <kbd>KMeansAutoSolver</kbd> at the top of the file:</p>
<pre><span>import </span>KMeansAutoSolver <span>from </span><span>'./kmeans-autosolver'</span>;</pre>
<p>Then, at the bottom of the file, add the following:</p>
<pre><span>console</span>.<span>log</span>(<span>"Solving example: 2d data with unknown clusters:"</span>);<br/><span>console</span>.<span>log</span>(<span>"===============================================</span><span>\n</span><span>"</span>);<br/><span>console</span>.<span>log</span>(<span>"Solution for 2d data with unknown clusters:"</span>);<br/><span>console</span>.<span>log</span>(<span>"-------------------------------------"</span>);<br/><span>const </span><span>ex_3_solver </span>= <span>new </span>KMeansAutoSolver(<span>1</span>, <span>5</span>, <span>5</span>, example_data.<span>example_2dnk</span>);<br/><span>const </span><span>ex_3_solution </span>= <span>ex_3_solver</span>.<span>solve</span>();<br/><span>console</span>.<span>log</span>(<span>ex_3_solution</span>);</pre>
<p>Run <kbd>yarn start</kbd> and you should see output similar to the following (previous output omitted):</p>
<pre><strong>Solving example: 2d data with unknown clusters:</strong><br/><strong>================================================================</strong><br/> <br/><strong>Solution for 2d data with unknown clusters:</strong><br/><strong>----------------------------------------------------------------</strong><br/><strong> { centroids:</strong><br/><strong>    [ [ 2.1666666666666665, 2.8333333333333335 ],</strong><br/><strong>      [ 9.571428571428571, 15.142857142857142 ],</strong><br/><strong>      [ 8, 6.166666666666667 ],</strong><br/><strong>      [ 3, 14.166666666666666 ] ],</strong><br/><strong>   iteration: 2,</strong><br/><strong>   error: 1.6236349578000828,</strong><br/><strong>   didReachSteadyState: true,</strong><br/><strong>   k: 4,</strong><br/><strong>   currentTrial: 0 }</strong></pre>
<p>Right away, you can see that the solver found an answer with <kbd>k: 4</kbd>, which is what we expected, and that the algorithm reached steady state in only three iterations with a low error value—all good signs.</p>
<p>Plotting these centroids against our data, we see that the algorithm has determined both the correct value for <em>k</em> and the centroid positions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-70 image-border" src="assets/0be74388-4e0e-4154-b914-8649691d2d86.png" style="width:40.08em;height:24.58em;"/></div>
<p>Note that our k-means auto-solver is also susceptible to local optima, and will not always guess the correct value for <em>k</em>. The reason? Increasing the value of <em>k</em> means that we can distribute more centroids amongst the data and reduce the error value. If we have 25 data points and set the range of <em>k</em> to anywhere from 1 to 30, the solver may end up finding a solution where k = 25, and each centroid sits on top of each individual data point for a total error of 0! This can be considered <em>overfitting</em>, where the algorithm finds a correct answer but hasn't sufficiently generalized the problem to give us the results we want. Even when using the auto-solver, you must be careful of the range of <em>k</em> values you give it, and keep the range as small as possible.</p>
<p>For example, if we increase <kbd>kMax</kbd> from 5 to 10 in the preceding example, we find that it gives a result for <em>k = 7</em> as the best. As always, the local optimum makes sense, but it is not quite what we were looking for:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-71 image-border" src="assets/dd2f5145-2c4f-4057-8e60-0eb0a213423f.png" style="width:39.83em;height:24.50em;"/></div>
<p>Because the auto-solver uses the error value calculation as its only guidance, you may be able to tune the error calculation to also consider the value of <em>k</em> and penalize solutions with too many clusters<em>.</em> The previous error calculation was a purely geometric calculation, representing the average distance each point is from its centroid. We may want to upgrade our error calculation so that it also prefers solutions with fewer centroids. Let's see what that would look like. </p>
<p>Return to the <kbd>kmeans.js</kbd> file and modify the <kbd>calculateError</kbd> method. Find the following line:</p>
<pre><span>const </span><span>thisDistance </span>= <span>distance</span>(<span>point</span>, <span>centroid</span>);</pre>
<p>And modify it to add the value of <kbd>k</kbd> to the distance:</p>
<pre><span>const </span><span>thisDistance </span>= <span>distance</span>(<span>point</span>, <span>centroid</span>) + <span>this</span>.<span>k</span>;</pre>
<p>When running the <kbd>KMeans</kbd> class by itself, this modification will do no harm because the value of <kbd>k</kbd> will be constant for that solver. The only time this modification may be undesirable is if you're actually interpreting and using the error value as a representation of <em>distance specifically</em>, as opposed to just looking for lower error values. Meaning, if you <em>need</em> the error to be a representation of distance, then you should not make this change. In all other cases, however, it can be advantageous, as this modification will prefer solutions with fewer clusters.</p>
<p>Now, return to <kbd>index.js</kbd> and modify the <kbd>ex_3_solver</kbd> to search the range of <kbd>k</kbd> values from 1 through 30. Run the program again with <kbd>yarn start</kbd> and you will see that the auto-solver once again correctly returns results for <em>k = 4</em>! While the previous, locally optimal solution with <em>k = 7</em> has a low error rate, adding the value of <kbd>k</kbd> to the error penalized the <em>k = 7</em> solution enough that the solver now prefers the <em>k = 4</em> solution. Because of this modification to the error calculation, we can be a little less careful when choosing our <kbd>kMin</kbd> and <kbd>kMax</kbd>, which is very helpful when we have no idea what <em>k</em> will be.</p>
<p>While our error calculation is no longer a geometrical representation of the cluster tightness, you can see that being thoughtful about the error calculation can give you a lot of leeway when trying to optimize for certain system properties. In our case, we wanted to find not just the tightest geometrical clusters, but the tightest geometrical clusters <em>with the fewest amount of clusters possible</em>, so updating the error calculation to consider <em>k</em> was a very helpful step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the problem of clustering, or grouping data points into logically similar groups. Specifically, we introduced the k-means algorithm, which is the most popular numerical clustering algorithm in ML. We then implemented the k-means algorithm in the form of a <kbd>KMeans</kbd> JavaScript class and tested it with both two and three-dimensional data. We also discussed how to approach the clustering problem when the number of clusters you desire is unknown beforehand, and built a new JavaScript class called <kbd>KMeansAutoSolver</kbd> to solve this problem. Along the way, we also discussed the impact of error calculations, and made a modification to our error calculation that helps generalize our solution to avoid overfitting.</p>
<p>In the next chapter we'll take a look at classification algorithms. Classification algorithms are supervised learning algorithms that can be seen as a more sophisticated extension of clustering algorithms. Rather than simply grouping data points by their similarity or proximity, classification algorithms can be trained to learn specific labels that should be applied to data. </p>
<p> </p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>