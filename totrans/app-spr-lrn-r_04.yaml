- en: '*Chapter 4:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Formulate regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement various types of regression approaches and its use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze and choose the right regression approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect statistics and machine learning through the lens of regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep dive into model diagnostics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will focus on various type of regression and when to use
    which one along with demonstrations in R.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we understood linear regression models and the linear
    relationship between an input variable (independent variable) and a target variable
    (dependent variable or explanatory variable). If one variable is used as an independent
    variable, it is defined as **simple linear regression**. If more than one explanatory
    (independent) variable is used, it's called **multiple linear regression**.
  prefs: []
  type: TYPE_NORMAL
- en: Regression algorithms and problems are based on predicting a numeric target
    variable (often called **dependent**), given all the input variables (often called
    **independent** variables), for example, predicting a house price based on location,
    area, proximity to a shopping mall, and many other factors. Many of the concepts
    of regression are derived from statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The entire field of machine learning is now a right balance of mathematics,
    statistics, and computer science. In this chapter, we will use regression techniques
    to understand how to establish a relationship between input(s) and the target
    variable. We will also emphasize on model diagnostics as regression is full of
    assumptions, which needs to be checked before a model can be used in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: '*Essentially, all models are wrong, but some are useful. – George Box*'
  prefs: []
  type: TYPE_NORMAL
- en: We have briefly touched upon simple and multiple linear regression in *Chapter
    3*, *Introduction to Supervised Learning*. In this chapter, we will focus more
    on **model diagnostics** and other types of **regression algorithm**, and how
    it is different from linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s revisit the multiple linear regression from *Chapter 3*, *Introduction
    to Supervised Learning*. The following equation is the mathematical representation
    of a linear equation, or linear predictor function, with `p` explanatory variables
    and `n` observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where each ![A picture containing furniture, table
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_23.png) is a vector of column
    values (**explanatory variable**) and ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_24.png) is the **unknown
    parameters** or **coefficients**. ![A picture containing furniture, seat
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_25.png), makes this equation
    suitable for simple linear regression. There are many algorithms to fit this function
    onto the data. The most popular one is **Ordinary Least Square** (**OLS**).
  prefs: []
  type: TYPE_NORMAL
- en: Before understanding the details of OLS, first let's interpret the equation
    we got while trying to fit the Beijing PM2.5 data from the model building section
    of simple and multiple linear regression from *Chapter 3*, *Introduction to Supervised
    Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we substitute the values of regression coefficients, ![A drawing of a face
  prefs: []
  type: TYPE_NORMAL
- en: 'Description automatically generated](img/C12624_04_27.png) and ![](img/C12624_04_28.png)
    from the output of the `lm()` function, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation attempts to answer the question "Are the factors `DEWP`,
    `TEMP`, and `Iws` important for predicting the `pm2.5` level?"
  prefs: []
  type: TYPE_NORMAL
- en: The model estimates how, on average, the `DEWP`, `TEMP`, and `Iws` values affect
    the `pm2.5` level. For example, a unit increase in `DEWP` will increase the `pm2.5`
    value by `4.384196`. That is why we often call these coefficients **weights**.
    It is important to note that if the **R-squared value** is low, these estimated
    coefficients are not reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 50: Print the Coefficient and Residual Values Using the multiple_PM_25_linear_model
    Object'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will print the coefficient and residual values using the
    `multiple_PM25_linear_model` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the attribute coefficients using the `$` operator on the `multiple_PM25_linear_model`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the attribute residuals using the `$` operator on the `multiple_PM25_linear_model`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 7: Printing Various Attributes Using Model Object Without Using the
    Summary Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the *Multiple Linear Regression Model* section of *Chapter 3*, *Introduction
    to Supervised Learning*, we created a multiple linear regression model and stored
    it in the model object `multiple_PM25_linear_model` using the model object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This activity will help in understanding how to extract some important model
    attributes once the model is built. In few cases, we will use the `$` operator,
    and in other cases, we will perform some simple calculation. Print the following
    model attributes using the `multiple_PM25_linear_model` object:'
  prefs: []
  type: TYPE_NORMAL
- en: Residuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R-Ssquared value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F-statistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficient p-value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s print these values using the model object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, print the coefficient values. Make sure the output is like the output
    of the `summary` function using the `coefficients` option. The coefficients are
    fitted values from the model that uses the OLS algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the residual value (difference) of the predicted and actual values of
    PM2.5, which should be as small as possible. Residual reflects how far the fitted
    values using the coefficients are from the actual value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, find the fitted values, which should be closer to the actual PM2.5 values
    for best model. Using the coefficients, we can compute the fitted values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the R-Squared values. They should look the same as the ones you obtained
    in the output of the `summary` function next to the multiple R-squared text. R-square
    helps in evaluating the model performance. If the value is closer to 1, the better
    the model is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the F statistic values. Make sure the output looks the same as the one
    you obtained in the output of the `summary` function next to the text F statistics.
    This will tell you if your model fits better than just using the mean of the target
    variables. In many practical applications, F-Statistic is used along with p-values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, find the coefficient p-values and make sure the values look the same
    as the one you obtained in the output of the `summary` function under *coefficients*
    for each variable. It will be present under the column titled `Pr(>|t|):`. If
    the value is less than 0.05, the variable is statistically significant in predicting
    the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 449.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Ordinary Least Square** (**OLS**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 3*, *Introduction to Supervised Learning*, we saw sum of squared
    residuals ![](img/C12624_04_30.png) (also called the **error sum of square** or
    **residual sum of squares**), which is a measure of the overall model fit, is
    given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *T* represents the matrix transpose, and the rows of *X* represent the
    values of all the input variables related to a specific value of the target variable
    are ![](img/C12624_04_32.png). The value of ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_33.png) that minimizes ![](img/C12624_04_34.png)is
    called the **OLS estimator** for β. The OLS algorithm is designed to find the
    global minimum of ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_35.png) that will minimize
    ![](img/C12624_04_36.png).
  prefs: []
  type: TYPE_NORMAL
- en: From the previous chapter, you also learned that the R-squared value for `multiple_PM25_linear_model`
    on the Beijing PM2.5 dataset is quite low for this model to be useful in practical
    applications. One way of interpreting the poor results is to say the predictor
    variables `DEWP` and `TEMP` do not fully explain the variance in PM2.5, so they
    fall short of producing good results.
  prefs: []
  type: TYPE_NORMAL
- en: Before we could jump into the diagnostics of this model, let's see if we could
    explain some of the variances in PM2.5 using the variable `month` (of the readings).
    We will also use an interaction variable (more on this in the *Improving the Model*
    section) *DEWP*TEMP*month* in the `lm()` function that generates all possible
    combination of `DEWP`, `TEMP`, and `month`.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for using `month` is justified by *Figure 3.3* in *Chapter 3*, *Introduction
    to Supervised Learning*, where we saw the seasonal effect in the values of `TEMP`,
    `DEWP`, and `PRES` (showing a nice sinusoidal pattern). The output of the following
    exercise shows all the interaction terms that got created to explain the PM2.5
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The expression like `DEWP:TEMP` means multiplication and each value of `month`
    is a separate variable in `multiple_PM25_linear_model,` because we converted `month`
    into `factor` before running the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 51: Add the Interaction Term DEWP:TEMP:month in the lm() Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will add the interaction term to improve the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see how adding an additional interaction term helps in improving the
    model performance in terms of R-squared values. Perform the following steps to
    complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the Beijing PM2.5 dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, convert the `month` object into the `factor` variable as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the linear model with interaction terms of `DEWP`, `TEMP`, and `month`.
    Observe the term `DEWP*TEMP*month`, which will generate all the combinations of
    the variable `DEWP`, `TEMP`, and `month`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model to see the changes in coefficients and r-squared
    values because of the interaction term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice the two-fold jump in the R-squared value from 0.216 to 0.4217\. However,
    such a jump is at the cost of **model interpretability**. Though it is simple
    to explain the explanatory power of the model using individual variables, their
    multiplication creates an effect that is difficult to articulate.
  prefs: []
  type: TYPE_NORMAL
- en: In our example of Beijing PM2.5, it is more logical to think of the interaction
    `DEWP` and `TEMP` have with the `month` object of the `year` object, since both
    of these are environmental factors which vary with season.
  prefs: []
  type: TYPE_NORMAL
- en: However, we would also like to perform some diagnostics to fully understand
    how a linear regression model is studied end to end and not just look at the R-squared
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Model Diagnostics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often statistical models such as linear regression and logistic regressions
    come with many assumptions that need to be validated before accepting the final
    solution. A model violating the assumptions will result in erroneous prediction
    and results will be prone to misinterpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows a method for obtaining the diagnostic plots from the
    output of the `lm()` method. The plot has four different plots looking at the
    residuals. Let''s understand how to interpret each plot. All these plots are about
    how well the fit matches the regression assumptions. If there is a violation,
    it will be clearly shown in the plots of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Diagnostics plot for the linear model fit on the Beijing PM2.5
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Diagnostics plot for the linear model fit on the Beijing PM2.5
    dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next four sections, we will explore each of the plots with randomly generated
    data from a linear equation ![](img/C12624_04_37.png)and a quadratic equation
    ![](img/C12624_04_38.png), and later come back to explain how the four plots in
    *Figure 4.1* fare in comparison with the ideal scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the quadratic equation, ![](img/C12624_04_39.png) ![](img/C12624_04_40.png)
    is assumed to be normally distributed with mean 0 and variance 2.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will generate the plots using the linear and quadratic
    equations. Later, we will deep dive into understanding the various assumptions
    a linear model should follow using the model fit on the random data generated
    through the two equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 52: Generating and Fitting Models Using the Linear and Quadratic Equations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will understand the linear and polynomial function and
    what happens when we fit a linear model on both.
  prefs: []
  type: TYPE_NORMAL
- en: Generate random numbers using a linear and a polynomial equation, and fit a
    linear model on both. Observe the difference between the two plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate the required plots:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the linear function using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the quadratic function as shown in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, generate the uniform random numbers (`x`), as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the linear values (`y`) using (`x`), as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the quadratic values (`y`) using (`x`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit a linear model for `linear_values_y` using `uniform_random_x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the diagnostic plot fora linear relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2: Plots using the linear regression'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_04_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.2: Plots using the linear regression'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fit a linear model for `quadratic_values_y` using `uniform_random_x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a diagnostic for non-linear relationships:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3: Plots using the quadratic regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.3: Plots using the quadratic regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The difference between the plot in step 7 and 9 show the good and a poor fit
    of a linear relationship. A linear model can't fit a non-linear relationship between
    *y* and *x*. In the next sections, we will deep dive into understanding the four
    parts of the plots generated in step 7 and 9.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Introduction to Supervised Learning*, *Figure 3.5* we discussed
    the various assumptions to consider while building a linear regression model.
    Through the four plots mentioned earlier in the chapter, we will examine if any
    of the assumptions are violated or not.
  prefs: []
  type: TYPE_NORMAL
- en: Residual versus Fitted Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of plot is between the fitted values and the residual (difference
    between `lm()` method. If the predictor and target variables have a non-linear
    relationship, the plot will help us identify.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, the top plot shows the point scattered all around
    and the linear relationship between the predictor and target variable is clearly
    captured. In the bottom plot, the unexplained non-linear relationship is left
    out in the residuals, and hence the curve. The bottom plot clearly shows it is
    not the right fit for a linear regression model, a violation of the linear relationship
    between the predictor and target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: [Top] Residual versus fitted plot of the linear function. [Bottom]
    Residual versus fitted plot of the quadratic function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: [Top] Residual versus fitted plot of the linear function. [Bottom]
    Residual versus fitted plot of the quadratic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Normal Q-Q Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Q-Q plot**, also called **Quantile-Quantile plot**, supports to check if
    the data plausibly comes from approximately theoretical distribution; in this
    instance, **Normal Distribution**. A Q-Q plot is a scatterplot shaped by plotting
    two sets of quantiles (points below which a certain proportion of the data falls)
    in contrast to one another. If both groups of quantiles came from a similar distribution,
    we must see the points creating a coarsely straight line. Provided a vector of
    data, the normal Q-Q plot plots the data in sorted order versus quantiles from
    a standard normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The second assumption in linear regression was that all the predictor variables
    are normally distributed. If it is true, the residuals will also be normally distributed.
    Normal Q-Q is a plot between standardized residuals and theoretical quantiles.
    Visually, we can inspect whether the residuals follow the straight line, if it
    is normally distributed, or if there is any deviation that indicates violation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, the top part demonstrates the linear function, which
    shows an alignment with the straight diagonal line, with a few exceptions like
    observation number 39, 30, and 50\. On the other hand, the bottom part of the
    figure shows the quadratic function, which surprisingly shows a fair alignment
    with the straight line, not exactly like the linear, as some divergence is seen
    in the top-right side of the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: [Top] Normal Q-Q plot of the linear function. [Bottom] Normal
    Q-Q plot of the quadratic function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: [Top] Normal Q-Q plot of the linear function. [Bottom] Normal Q-Q
    plot of the quadratic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scale-Location Plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scale-Location plot** shows whether residuals are spread equally along the
    ranges of input variables (predictor). The assumption of equal variance (**homoscedasticity**)
    could also be checked with this plot. If we see a horizontal line with randomly
    spread points, it means that the model is good.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot is between fitted values and the square root of standardized residuals.
    In the following figure, the top plot shows the linear function, and the residuals
    are spread randomly along the horizontal lines, whereas in the bottom plot, there
    seems to be a pattern that is not random. Hence, the variance is not equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: [Top] Scale-Location plot of the linear function. [Bottom] Scale-Location
    plot of the quadratic function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: [Top] Scale-Location plot of the linear function. [Bottom] Scale-Location
    plot of the quadratic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Residual versus Leverage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If there are any influential points in the data, the **Residual versus Leverage**
    plot helps in identifying it. It's common to think that all outlier points are
    influential, that is, it decides how the regression line comes out. However, not
    all outliers are influential points. Even if a point is within a reasonable range
    of values (not an outlier), it could still be an influential point.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next plot, we will look out for far off values at the top-right corner
    or at the bottom-right corner. Those regions are the spaces where observation
    can be *influential* in contrast to a regression line. In *Figure 4.7*, the observations
    of the red dashed line with high `40` and `39` outside of the dashed line (high
    Cook''s distance). Note that these observations are consistently appearing in
    the other three plots as well, giving us a strong reason to eliminate these points,
    if we would like to see the linear relationship in the data. The plot on the top
    seems to have no red dashed line, ascertaining a good fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: [Top] Residual versus Leverage plot of the linear function. [Bottom]
    Residual versus Leverage plot of the quadratic function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: [Top] Residual versus Leverage plot of the linear function. [Bottom]
    Residual versus Leverage plot of the quadratic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, if we revisit *Figure 4.1*, the diagnostics plot we obtained from the Beijing
    PM2.5 dataset; it seems like the model fit is *not the best* to be used for practical
    purposes. All the four plots show slight violation of linearity, normality and
    homoscedasticity assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we have listed a few ways to improve the model, which may
    incrementally help increase the R-squared value and better fit the data. Also,
    similar to the visual inspection methods we just discussed, many statistical methods
    such as **Kolmogorov-Smirnov test** for testing normality, **Correlation** for
    testing multicollinearity, **Goldfeld–Quandt test** for testing homoscedasticity
    could be used.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have seen the problems in the data, but you may ask whether you can
    fix or improve it. Let's discuss some ways to do that. In this section, you will
    learn some of the ways, such as variable transformation, dealing with outlier
    points, adding interaction effect and deciding to go with a non-linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Transform the Predictor or Target Variable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common way to improve the model is to transform one or more variables
    (could also be the target variable) using a `log` function.
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation corrects the skewed distribution. It gives the ability to
    handle the skewness in the data and at the same time the original value could
    be easily computed once the model is built. The most popular log transformation
    is natural *log*. A more detailed explanation for log transformation could be
    found in the section *Log Transformation* of *Chapter 6*, *Feature Selection and
    Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to bring the normal distribution in the data by transforming.
    So, whichever function helps in attaining that is a good transformation. After
    log, square root is also widely used. Look at the distribution of the transformed
    variable to see if a symmetrical distribution (bell shaped) is obtained; if yes,
    then the transformation is going to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Choose a Non-Linear Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It might be possible to get struck in a scenario where a linear model is not
    a right fit because there is a non-linear relationship between the predictor and
    the target variable and only a non-linear function could fit such data. See the
    section *Polynomial Regression* later in this chapter for more details on such
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: Remove an Outlier or Influential Point
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the *Residual versus Leverage* section's diagnostic plot,
    we may find an outlier or influential point playing a spoil spot in getting us
    the best model. If you have identified it properly, try seeing by removing the
    observation and see if things improve.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the Interaction Effect
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We might at times see the value(s) of two or more predictor (independent) variables
    in the dataset influencing the dependent variable in a multiplicative way. A linear
    regression equation with an interaction term might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One can go for the higher order of such an interaction (for example, using three
    variables); however, those are difficult to interpret and usually avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Quantile Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the data presents outliers, high skewness, and conditions leading to heteroscedasticity,
    we employ quantile regression for modelling. Also, one key question quantile regression
    answers, which linear regression cannot, is "Does `DEWP`, `TEMP`, and `Iws` influence
    PM2.5 levels differently for high PM2.5 than for average PM2.5?"
  prefs: []
  type: TYPE_NORMAL
- en: Quantile regression is quite similar to linear regression; however, the quantile
    regression parameter estimates the change in a certain quantile of the `response`
    variable produced by a unit change in the input `predictor` variable. In order
    to fully understand this statement, let's fit our Beijing data using quantile
    regression (without using the interaction terms).
  prefs: []
  type: TYPE_NORMAL
- en: We need to install the `quantreg` package to fit the quantile regression into
    the data. The package offers the method, `rq()` to fit the data using the argument
    `tau`, which is the model parameter specifying the value of quantile to be used
    for fitting the model into the data. Observe that the other parts of the arguments
    to the `rq()` method looks similar to `lm()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 53: Fit a Quantile Regression on the Beijing PM2.5 Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will observe the difference in the quantile regression
    fit at various quantiles, particularly 25th, 50th, and 75th. The `rq()` function
    from `quantreg` will be used for building the model. In *Figure 4.8*, we will
    compare the coefficient values obtained through the `lm()` function versus the
    `rq()` function to compare the two types of regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the Beijing PM2.5 dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the next step is to install the required package. Use the following command
    to load the `quantreg` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the quantile regression tau values as 0.25, 0.5, and 0.75, which corresponds
    to the 25th, 50th, and 75th quantiles, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the quantile regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following table summarizes the coefficient values of the linear regression
    that we obtained using `lm()` in the *Regression* section of *Chapter 3*, *Introduction
    to Supervised Learning* and the values we obtained using `rq()` in three quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the linear regression model, the mean PM2.5 level in the atmosphere
    increases by `4.384196` with one unit increase in `DEWP`. The quantile regression
    results in the following table, and it indicates that `DEWP` has a larger negative
    impact on the higher quantiles (observe the 75th quantile) of PM2.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Coefficient estimates for the 25th, 50th, 75th quantile regression
    and the linear regression coefficient estimates for the Beijing''s PM2.5 estimation
    model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Coefficient estimates for the 25th, 50th, 75th quantile regression
    and the linear regression coefficient estimates for the Beijing''s PM2.5 estimation
    model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 54: Plotting Various Quantiles with More Granularity'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, instead of using the 25th, 50th and 75th quantiles, we will
    use the more granular values for tau in the `rq` function. The plot will help
    visualize the change in the coefficient values based on the quantile value. Use
    the `seq()` function from R that sets the quantile values starting from 0.05 to
    0.95 with an increment of 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `quantile_regression_PM25_granular` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, store the value from the previously created variable using the `summary`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s use the following command to plot the graph. Observe for the different
    values of tau, how the values of `Intercept`, `DEWP`, `TEMP`, and `Iws` change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9: Shows the various values of coefficients of DEWP, TEMP, and Iws
    for various values of quantiles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.9: Shows the various values of coefficients of DEWP, TEMP, and Iws
    for various values of quantiles'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we explored the granularity of the variable using more granular
    values for tau in the `rq` function. The previous figure shows the various values
    of the coefficients of `DEWP`, `TEMP`, and `Iws`. The X-axis in the plot shows
    the quantile. The single dotted line shows the estimation of the quantile regression,
    and the gray area is the confidence interval. The middle gray line is a representation
    of the OLS coefficient estimates, and the double dotted lines display the confidence
    intervals around the OLS coefficients. Observe that the red and the gray areas
    do not overlap, which justifies our use of quantile regression. If the two lines
    overlap, then there is no difference in the estimates using OLS and quantile regression.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are not claiming that quantile regression is giving better results than linear
    regression. The adjusted R-squared value is still low for this model and it works
    well in the real world. However, we claim that quantile regression can help in
    estimating the PM2.5 at different levels than just the average, which provides
    a robust interpretation for data with outliers, high skewness, and heteroscedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often in real-world data, the response variable and the predictor variable
    don''t have a linear relationship, and we may need a **nonlinear polynomial function**
    to fit the data. Various scatterplot-like residual versus each predictor and residual
    versus fitted values reveal the violation of linearity if any, which could potentially
    help in identifying the need for introducing the quadratic or cubic term in the
    equation. The following function is a generic polynomial equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *k* is the degree of the polynomial. For *k=2*, *f(X)* is called **quadratic**
    and *h=4* is called **cubic**. Note that polynomial regression is still considered
    linear regression since it is still linear in coefficient ![A drawing of a face
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_43.png).
  prefs: []
  type: TYPE_NORMAL
- en: Before revisiting the Beijing PM2.5 example, let's understand how polynomial
    regression works using simulated data from the quadratic equation we introduced
    in the *Linear Regression* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 55: Performing Uniform Distribution Using the runif() Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will generate 50 random numbers from a uniform distribution
    using the function `runif()` in R and store the results in `uniform_random_x`.
    We have defined a function to generate values using the previous quadratic equation.
    Note that we will separately add ![A close up of a logo
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_40b.png) to the values returned
    by the function; ![A close up of a logo
  prefs: []
  type: TYPE_NORMAL
- en: 'Description automatically generated](img/C12624_04_40a.png) is generated from
    the normal distribution using the `rnorm()` function in R. The final value will
    then be stored in `quadratic_values_y`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to perform the uniform distribution using the `runif()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the quadratic equation as illustrated in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, generate the uniform random number for `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the error term to the quadratic equation, which is normally distributed
    with mean `0` and variance `2` (*standard deviation(sd) = square root of variance*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To store the data in data frame, let''s use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, plot the relationship between `x` and `y` based on the quadratic equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Performing uniform distribution using the function runif()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.10: Performing uniform distribution using the function runif()'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The following figure clearly shows the relationship between `uniform_random_x`
    and `quadratic_values_y` is not linear as expected. Now, if we try to fit a linear
    model, we expect to see some trouble in the diagnostics plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Residuals versus fitted value plots in *Figure 4.12* display a curvature and
    they do not demonstrate uniform randomness as we have seen before. Also, **Normal
    Probability Plot** (**NPP**) seems to diverge from a straight line and curves
    down at the far away percentiles. These plots suggest that there is something
    incorrect with the model being used and indicate that a higher-order model may
    be needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11: The plot shows the non-linear relationship between uniformly
    generated random number(x) and the value of x in the quadratic equation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.11: The plot shows the non-linear relationship between uniformly generated
    random number(x) and the value of x in the quadratic equation'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, fit a linear regression model to the polynomial (quadratic) equation and
    display the diagnostic plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12: Diagnostic plot for the quadratic equation data fit using the
    lm() method'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.12: Diagnostic plot for the quadratic equation data fit using the
    lm() method'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, let's see how polynomial regression fares on the Beijing PM2.5 dataset.
    We have introduced an additional quadratic term `DEWP^2`, which is simply `DEWP`
    raised to the power `2`. Refer to the scatterplot illustrated in *Figure 3.5*
    of *Chapter 3*, *Introduction to Supervised Learning* to justify the addition
    of such a higher order term.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use polynomial regression on the Beijing PM2.5 dataset with the quadratic and
    cubic terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To print the model summary, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that in spite of an additional quadratic term, we are not attaining
    any better R-squared value than the linear model. At this juncture, we may conclude
    that the PM2.5 prediction needs a better independent variable, which could explain
    the variance in it to get the R-squared value to any higher level. The diagnostics
    plot seems to show similar interpretation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the diagnostics plot using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13: The diagnostics plot for polynomial regression model fit on
    the Beijing PM2.5 dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.13: The diagnostics plot for polynomial regression model fit on the
    Beijing PM2.5 dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in linear regression, **Ordinary Least Square** (**OLS**) estimates
    the value of ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_44.png) in such a way that
    the sum of squares of residual ![](img/C12624_04_45.png) is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Since ![A picture containing furniture
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_46.png) is an estimate we
    compute from a given sample and it's not a *true population parameter*, we need
    to be careful of certain characteristics of an estimate. The two such primary
    characteristics are the bias and the variance.
  prefs: []
  type: TYPE_NORMAL
- en: If ![A close up of a logo
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_48.png) is the fit at the
    ![](img/C12624_04_49.png) value of ![A close up of a logo
  prefs: []
  type: TYPE_NORMAL
- en: 'Description automatically generated](img/C12624_04_50.png), then the average
    (or expected) ![](img/C12624_04_47.png) on the test dataset could be decomposed
    into three quantities, the variance, the squared bias, and the variance of error
    terms as represented by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the best estimate, a suitable algorithm such as OLS should simultaneously
    achieve low bias and low variance. We commonly call this the **Bias-Variance**
    trade off. The popular bull''s eye picture shown in the following figure helps
    understand the various scenarios of the tradeoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14: The popular bull''s eye picture for explaining Bias and Variance
    scenarios'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.14: The popular bull''s eye picture for explaining Bias and Variance
    scenarios'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The bull''s eye represents the true population value that OLS is trying to
    estimate, and the shots at it are the values of our estimates resulting from four
    different estimators. These are broadly classified into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Low Bias and Low Variance (most favorable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low Bias and High Variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High Bias and Low Variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High Bias and High Variance (least favorable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OLS method treats all variables as equally likely, thus having a low bias (results
    in **under-fitting** during training) and high variance (results in **prediction
    error** in testing data) as shown in the *Figure 4.11*. Such behavior is not ideal
    for obtaining the optimal model complexity. The general solution to this issue
    is to reduce variance at the cost of introducing some bias. This approach is called
    regularization. So, ridge regression could be thought of as an extension of linear
    regression with an additional regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general form of multiple linear regression could be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where, **argmin** means the minimum value of βs that make the function attain
    the minimum. In the context, it finds the βs that minimize the RSS. The βs are
    subject to the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Regularization Term – L2 Norm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](img/C12624_04_54.jpg)![](img/C12624_04_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The penalty term in the ridge beta increases if RSS increases. The following
    figure shows a plot between **Model Complexity** (number of predictors) and **Error**.
    It shows that when the number of predictors increase (model complexity increases),
    the **Variance** goes up and the **Bias** goes down.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OLS estimate finds a place in the right side, away from the optimal trade-off
    point. This scenario necessitates the introduction of the regularization term
    and hence ridge regression becomes suitable choice of model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: Bias versus variance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: Bias versus variance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The OLS loss function for ridge regression could be represented by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Minimizing the ![A drawing of a face
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12624_04_57.png) function with the
    regularization term that gives the ridge regression estimates. The interesting
    property of this loss function is that as ![](img/C12624_04_58.png) becomes larger,
    the variance decreases and the bias increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 56: Ridge Regression on the Beijing PM2.5 dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This exercise fits the ridge regression on the Beijing PM2.5 dataset. We will
    use `glmnet` library's cross-validation function `cv.glmnet()` with the parameter
    `alpha = 0` and varying lambda values. The aim is to obtain an optimal value for
    lambda that will be returned in the `lambda.min` attribute of the function output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `glmnet` library and preprocess the PM25 DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s use the following code to set up the `seed` to get similar results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To find the optimal value of lambda after cross validation, execute the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Coefficient values from the model fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `predict` function again and pass the matrix X to the `newx` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see how ridge regression could be used to fit the Beijing PM2.5 dataset using
    the `glmnet` library.
  prefs: []
  type: TYPE_NORMAL
- en: LASSO Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Least Absolute Shrinkage and Selection Operator** (**LASSO**) follows a similar
    structure to that of ridge regression, except for the penalty term, which in LASSO
    regression is L1 (sum of absolute values of the coefficient estimates) in contrast
    to ridge regression where it''s L2 (sum of squared coefficients):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: LASSO regression turns some coefficients to zero, thus the effect of a particular
    variable is nullified. This makes it efficient in feature selection while fitting
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 57: LASSO Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will apply LASSO regression on the Beijing PM2.5 dataset.
    We will use the same `cv.glmnet()` function to find the optimal lambda value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set up `seed` to get similar results using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command to find the optimal value of lambda after cross
    validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the following command to find the coefficient values from the model
    fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to find the prediction from the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe the similarity in the predictions of ridge and LASSO regression. The
    Beijing PM2.5 dataset doesn't show any difference in these two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Elastic Net** combines the penalty terms of ridge and LASSO regression to
    avoid the overdependence on data for variable selection (coefficient values tending
    to zero by which highly correlated variables are kept in check). Elastic Net minimizes
    the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where the parameter `α` controls the right mix between ridge and LASSO.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, if a model has many predictor variables or correlated variables,
    introducing the regularization term helps in reducing the variance and increase
    bias optimally, thus bringing the right balance of model complexity and error.
    *Figure 4.16* provides a flow diagram to help one choose between multiple, ridge,
    LASSO, and elastic net regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16: Selection criteria to choose between multiple, eidge, LASSO,
    and elastic net regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.16: Selection criteria to choose between multiple, ridge, LASSO, and
    elastic net regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 58: Elastic Net Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform elastic net regression on the Beijing PM2.5
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first set up `seed` to get similar results using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command to find the optimal value of lambda after cross
    validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, execute the following command to find the coefficient values from the
    model fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to find the prediction from the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Elastic Net Regression gives more or less the same predictions as of ridge and
    LASSO regression. In the next section, we compare all three together.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between Coefficients and Residual Standard Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following tables show the comparison of `DEWP`, `TEMP`, and `Iws`), there
    isn''t much difference in the values, which suggests that ridge, LASSO, and elastic
    net regression with regularization terms are not any better than multiple linear
    regression approach. This also suggests that `DEWP`, `TEMP`, and `Iws` are independent
    variables with low or no correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17: Comparison of residual standard error between linear, ridge,
    LASSO, and elastic net regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.17: Comparison of residual standard error between linear, ridge, LASSO,
    and elastic net regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following figure shows a comparison of the coefficient values of intercept
    and DEWP, TEMP and Iws variables using Linear, Ridge, LASSO and Elastic net regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18: Comparison of coefficient values between Linear, Ridge, LASSO,
    and Elastic Net regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.18: Comparison of coefficient values between Linear, Ridge, LASSO,
    and Elastic Net regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 59: Computing the RSE of Linear, Ridge, LASSO, and Elastic Net Regressions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will compute the RSE of Linear, Ridge, LASSO, and Elastic
    Net regressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to fit a linear model using the `Iws`, `DEWP`, and `TEMP`
    variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command for finding the **Residual Standard Error**
    (**RSE**) of linear regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Similarly, we will find the RSE of the remaining regression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RSE of ridge regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'RSE of LASSO regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'RSE of Elastic Net regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows that the RSE for all three isn't significantly different.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In linear regression, we saw an equation of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In `Y` is a count or rate (`Y/t`) that has a **Poisson distribution** with expected
    (mean) count of ![](img/C12624_04_62.png) as ![](img/C12624_04_63.png), which
    is equal to variance.
  prefs: []
  type: TYPE_NORMAL
- en: In case of logistic regression, we would probe for values that can maximize
    log-likelihood to get the **maximum likelihood estimators** (**MLEs**) for coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: There are no closed-form solutions, hence the estimations of maximum likelihood
    would be obtained using iterative algorithms such as **Newton-Raphson** and **Iteratively
    re-weighted least squares** (**IRWLS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Poisson regression is suitable for the count-dependent variable, which must
    meet the following guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: It follows a Poisson distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counts are not negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values are whole numbers (no fractions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dataset used here to demonstrate Poisson regression comes from A. Colin
    Cameron and Per Johansson, "*Count Data Regression Using Series Expansion: With
    Applications*", Journal of Applied Econometrics, Vol. 12, No. 3, 1997, pp. 203-224.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following table succinctly describes the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19: Variables and its description from an Australian Health Survey
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.19: Variables and its description from an Australian Health Survey
    dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The blog [http://www.econ.uiuc.edu/~econ508/Stata/e-ta16_Stata.html](http://www.econ.uiuc.edu/~econ508/Stata/e-ta16_Stata.html)
    demonstrates the usage of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 60: Performing Poisson Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform Poisson regression on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Carry out Poisson regression load the library `foreign` to read `dta` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the `read.data` function from the `foreign` library to read the Australian
    health survey dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit a generalized linear model using the `glm()` function with Poisson regression
    as the value in the family parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `ggplot2` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine the actual values of `NONDOCCO` and Poisson regression-fitted values
    of `NONDOCCO`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Name the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the actual versus predicted values of the `NONDOCCO` target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: Comparing actual and predicted values of NONDOCCO'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.20: Comparing actual and predicted values of NONDOCCO'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the value of the residual deviance statistic of 5052.5 with 5178 degree
    of freedom, the p-value is zero and the *5052.5/5178 = 0.975* is less than 1,
    so the model does to a certain level. We can also check overdispersion (presence
    of greater variability in a dataset than would be expected based on a given statistical
    model). Overdispersion is computed by dividing `sample_variance` with `sample_mean`.
    Let's examine the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 61: Computing Overdispersion'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform computing overdispersion on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s find the sample mean using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command for finding the sample variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, overdispersion can be computed using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, even if we try adding predictor variables to model fit, overdispersion starts
    to go down. In our example, the dispersion is well within limits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s calculate the dispersion using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, in cases where dispersion is over the limit, a higher order Poisson
    regression is a suitable solution. Keeping the scope of this book in mind, we
    will not delve into such model in detail here. Interested readers could read more
    on Baseline Density Poisson (**Poisson Polynomial of order p** (**PPp**) models).
  prefs: []
  type: TYPE_NORMAL
- en: Cox Proportional-Hazards Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basis for the Cox regression models comes from the survival analysis, a
    set of statistical methods helpful in investigating the time it takes for an event
    to occur. Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Time until a lead is converted to sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time until a product failure from the start of usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time after the start of the insurance policy until death
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time after diagnosing until death
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time until a warranty is claimed for a product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time from customer registration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All these examples are some of the use cases of survival analysis. In most
    of the survival analysis, there are three wide-spread methods used for carrying
    out such time-to-event analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Kaplan-Meier survival curves for analysis of different groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logrank test for comparing two or more survival curves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cox proportional hazards regression to describe the effect of variables on survival
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping in mind the scope of this chapter and book, we will focus only on the
    Cox proportional hazards regression. The fundamental idea is that the first two
    methods only help in performing univariate analysis, in other words, you can understand
    the effect of only one factor on the time-to-event, whereas Cox regression helps
    in assessing the effect of multiple factors on the survival time. Also, Cox regression
    works equally good with both categorical and numeric factors, while the first
    two methods only work with categorical factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cox model is expressed by the hazard function denoted by `h(t)`, which
    represents in the medical research, where its predominately used, the risk of
    dying at time `t`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_04_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some observations from this equation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`t` denotes the survival time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h(t)` represents the hazard function determined by the `p` covariates ![](img/C12624_04_65.png).
    Covariates is the term used for describing predictor variables in survival analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coefficients ![](img/C12624_04_66.png) suggest the impact of covariates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term ![](img/C12624_04_67.png) is called the baseline hazard at time t.
    If all the coefficients are zero ![](img/C12624_04_68.png), ![](img/C12624_04_69.png)
    becomes the value of the hazard function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function looks somewhat relatable to logistic regression (uses an exponential
    term), which will be discussed in detail in *Chapter 5*, *Classification*. We
    have logically split all the supervised learning algorithms discussed in this
    book into `yes/no`, `1/0`) but disregards the timing of events.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have observed from the hazard function, survival models comprise
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: A continuous variable demonstrative of the time to event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A binary variable illustrative of the status whether event happened or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NCCTG Lung Cancer Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NCCTG Lung Cancer Data** from survival in patients with advanced lung cancer
    is from the *North Central Cancer Treatment Group*. The data is a collection of
    few metadata, such as which institution collected it, age of the patient, sex,
    and so on. The performance scores in this dataset rates how well the patient can
    perform the daily activities. The most important variable in any survival analysis
    dataset is the *knowledge* about the **time-to-event**, for example, time until
    death.'
  prefs: []
  type: TYPE_NORMAL
- en: Survival analysis is usually defined as a set of methods for examining data
    where the outcome variable is the time till the incidence of an event of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21: Variables and its descriptions of North Central Cancer Treatment
    Group'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_04_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.21: Variables and its descriptions of North Central Cancer Treatment
    Group'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next exercise, we will learn how to create the survival object using
    the method `Surv` from the `survival` package. Note that in the summary of the
    dataset after adding the survival object, two additional variables `SurvObject.time`
    and `SurvObject.status` are created, which stores the information about time-to-event
    (time until death), which then becomes the dependent variable for the **Cox Proportional-Hazards
    Regression Model**.
  prefs: []
  type: TYPE_NORMAL
- en: Observations are **censored** when there is a scarce number of indications around
    a patient's survival time. Popularly, the most prevalent form is right censoring.
    Let's assume that we are following a study for 20 weeks. A patient not going through
    the event of interest during the study can be called as right censored. The person's
    survival time is at least the duration of the study; in this case, 20 weeks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 62: Exploring the NCCTG Lung Cancer Data Using Cox-Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore the NCCTG Lung Cancer Data using Cox-Regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `survival` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the Lung Cancer Data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the dataset using the `head` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lung Cancer Data where `status == 2` represents death:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the Cox Proportional Hazards Regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This exercise demonstrates the Cox proportional hazards regression model using
    the survival library.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we discussed linear regression in more detail after a brief
    introduction in the previous chapter. Certainly, the discussion on linear regression
    led to a series of diagnostics that gave directions to discussing other type of
    regression algorithms. Quantile, polynomial, ridge, LASSO, and elastic net, all
    of these are derived from linear regression, with the differences coming from
    the fact that there are some limitations in linear regression that each of these
    algorithms helped overcome. Poisson and Cox proportional hazards regression model
    came out as a special case of regression algorithms that work with count and time-to-event
    dependent variables, respectively, unlike the others that work with any quantitative
    dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the second most commonly applied machine
    learning algorithm and solve problems associated with it. You will also learn
    more about classification in detail. *Chapter 5*, *Classification*, similar to
    this chapter, is designed to cover classification algorithms ranging from **Decision
    Trees** to **Deep Neural Network** in detail.
  prefs: []
  type: TYPE_NORMAL
