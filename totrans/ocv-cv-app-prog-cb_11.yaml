- en: Chapter 11. Processing Video Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading video sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the video frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing video sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking feature points in a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the foreground objects in a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video signals constitute a rich source of visual information. They are made
    of a sequence of images, called **frames**, that are taken at regular time intervals
    (specified as the **frame rate**, generally expressed in frames per second) and
    show a scene in motion. With the advent of powerful computers, it is now possible
    to perform advanced visual analysis on video sequences—sometimes at rates close
    to, or even faster than, the actual video frame rate. This chapter will show you
    how to read, process, and store video sequences.
  prefs: []
  type: TYPE_NORMAL
- en: We will see that once the individual frames of a video sequence have been extracted,
    the different image processing functions presented in this book can be applied
    to each of them. In addition, we will also look at a few algorithms that perform
    a temporal analysis of the video sequence, compare adjacent frames to track objects,
    or cumulate image statistics over time in order to extract foreground objects.
  prefs: []
  type: TYPE_NORMAL
- en: Reading video sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to process a video sequence, we need to be able to read each of its
    frames. OpenCV has put in place an easy-to-use framework that can help us perform
    frame extraction from video files or even from USB or IP cameras. This recipe
    shows you how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Basically, all you need to do in order to read the frames of a video sequence
    is create an instance of the `cv::VideoCapture` class. You then create a loop
    that will extract and read each video frame. Here is a basic `main` function that
    displays the frames of a video sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A window will appear on which the video will play as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To open a video, you simply need to specify the video filename. This can be
    done by providing the name of the file in the constructor of the `cv::VideoCapture`
    object. It is also possible to use the open method if the `cv::VideoCapture object`
    has already been created. Once the video is successfully opened (this can be verified
    through the `isOpened` method), it is possible to start the frame extraction.
    It is also possible to query the `cv::VideoCapture` object for information associated
    with the video file by using its get method with the appropriate flag. In the
    preceding example, we obtained the frame rate using the `CV_CAP_PROP_FPS` flag.
    Since it is a generic function, it always returns a double even if another type
    would be expected in some cases. For example, the total number of frames in the
    video file would be obtained (as an integer) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Have a look at the different flags that are available in the OpenCV documentation
    in order to find out what information can be obtained from the video.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a `set` method that allows you to input parameters into the `cv::VideoCapture`
    instance. For example, you can request to move to a specific frame using the `CV_CAP_PROP_POS_FRAMES`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can also specify the position in milliseconds using `CV_CAP_PROP_POS_MSEC`,
    or you can specify the relative position inside the video using `CV_CAP_PROP_POS_AVI_RATIO`
    (with 0.0 corresponding to the beginning of the video and 1.0 to the end). The
    method returns `true` if the requested parameter setting is successful. Note that
    the possibility to get or set a particular video parameter largely depends on
    the codec that is used to compress and store the video sequence. If you are unsuccessful
    with some parameters, that could be simply due to the specific codec you are using.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the captured video is successfully opened, the frames can be sequentially
    obtained by repetitively calling the `read` method as we did in the example of
    the previous section. One can equivalently call the overloaded reading operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to call the two basic methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Also note how, in our example, we introduced a delay in displaying each frame.
    This is done using the `cv::waitKey` function. Here, we set the delay at a value
    that corresponds to the input video frame rate (if `fps` is the number of frames
    per second, then `1000/fps` is the delay between two frames in milliseconds).
    You can obviously change this value to display the video at a slower or faster
    speed. However, if you are going to display the video frames, it is important
    that you insert such a delay if you want to make sure that the window has sufficient
    time to refresh (since it is a process of low priority, it will never refresh
    if the CPU is too busy). The `cv::waitKey` function also allows us to interrupt
    the reading process by pressing any key. In such a case, the function returns
    the ASCII code of the key that is pressed. Note that if the delay specified to
    the `cv::waitKey` function is `0`, then it will wait indefinitely for the user
    to press a key. This is very useful if someone wants to trace a process by examining
    the results frame by frame.
  prefs: []
  type: TYPE_NORMAL
- en: The final statement calls the `release` method, which will close the video file.
    However, this call is not required since `release` is also called by the `cv::VideoCapture`
    destructor.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that in order to open the specified video file, your
    computer must have the corresponding codec installed; otherwise, `cv::VideoCapture`
    will not be able to decode the input file. Normally, if you are able to open your
    video file with a video player on your machine (such as Windows Media Player),
    then OpenCV should also be able to read this file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also read the video stream capture of a camera that is connected to
    your computer (a USB camera, for example). In this case, you simply specify an
    ID number (an integer) instead of a filename to the `open` function. Specifying
    `0` for the ID will open the default installed camera. In this case, the role
    of the `cv::waitKey` function that stops the processing becomes essential, since
    the video stream from the camera will be infinitely read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is also possible to load a video from the Web. In this case, all
    you have to do is provide the correct address, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Writing video sequences* recipe in this chapter has more information on
    video codecs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [http://ffmpeg.org/](http://ffmpeg.org/) website presents a complete open
    source and cross-platform solution for audio/video reading, recording, converting,
    and streaming. The OpenCV classes that manipulate video files are built on top
    of this library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the video frames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, our objective is to apply some processing function to each of
    the frames of a video sequence. We will do this by encapsulating the OpenCV video
    capture framework into our own class. Among other things, this class will allow
    us to specify a function that will be called each time a new frame is extracted.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What we want is to be able to specify a processing function (a callback function)
    that will be called for each frame of a video sequence. This function can be defined
    as receiving a `cv::Mat` instance and outputting a processed frame. Therefore,
    in our framework, the processing function must have the following signature to
    be a valid callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example of such a processing function, consider the following simple
    function that computes the Canny edges of an input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `VideoProcessor` class encapsulates all aspects of a video-processing task.
    Using this class, the procedure will be to create a class instance, specify an
    input video file, attach the callback function to it, and then start the process.
    Programmatically, these steps are accomplished using our proposed class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If this code is run, then two windows will play the input video and the output
    result at the original frame rate (a consequence of the delay introduced by the
    `setDelay` method). For example, considering the input video for which a frame
    is shown in the previous recipe, the output window will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we did in other recipes, our objective was to create a class that encapsulates
    the common functionalities of a video-processing algorithm. As one might expect,
    the class includes several member variables that control the different aspects
    of the video frame processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The first member variable is the `cv::VideoCapture` object. The second attribute
    is the `process` function pointer that will point to the callback function. This
    function can be specified using the corresponding setter method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following method opens the video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It is generally interesting to display the frames as they are processed. Therefore,
    two methods are used to create the display windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The main method, called `run`, is the one that contains the frame extraction
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This method uses a `private` method that reads the frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `run` method proceeds by first calling the `read` method of the `cv::VideoCapture`
    OpenCV class. There is then a series of operations that are executed, but before
    each of them is invoked, a check is made to determine whether it has been requested.
    The input window is displayed only if an input window name has been specified
    (using the `displayInput` method); the callback function is called only if one
    has been specified (using `setFrameProcessor`). The output window is displayed
    only if an output window name has been defined (using `displayOutput`); a delay
    is introduced only if one has been specified (using `setDelay` method). Finally,
    the current frame number is checked if a stop frame has been defined (using `stopAtFrameNo`).
  prefs: []
  type: TYPE_NORMAL
- en: 'One might also wish to simply open and play the video file (without calling
    the callback function). Therefore, we have two methods that specify whether or
    not we want the callback function to be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the class also offers us the possibility to stop at a certain frame
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The class also contains a number of getter and setter methods that are basically
    just a wrapper over the general `set` and `get` methods of the `cv::VideoCapture`
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our `VideoProcessor` class is there to facilitate the deployment of a video-processing
    module. Few additional refinements can be made to it.
  prefs: []
  type: TYPE_NORMAL
- en: Processing a sequence of images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, the input sequence is made of a series of images that are individually
    stored in distinct files. Our class can be easily modified to accommodate such
    input. You just need to add a member variable that will hold a vector of image
    filenames and its corresponding iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A new `setInput` method is used to specify the filenames to be read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `isOpened` method becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The last method that needs to be modified is the private `readNextFrame` method
    that will read from the video or from the vector of filenames, depending on the
    input that has been specified. The test is that if the vector of image filenames
    is not empty, then that is because the input is an image sequence. The call to
    `setInput` with a video filename clears this vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Using a frame processor class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In an object-oriented context, it might make more sense to use a frame processing
    class instead of a frame processing function. Indeed, a class would give the programmer
    much more flexibility in the definition of a video-processing algorithm. We can,
    therefore, define an interface that any class that wishes to be used inside the
    `VideoProcessor` will need to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A setter method allows you to input a `FrameProcessor` instance to the `VideoProcessor`
    framework and assign it to the added member variable `frameProcessor` that is
    defined as a pointer to a `FrameProcessor` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When a frame processor class instance is specified, it invalidates any frame
    processing function that could have been set previously. The same obviously applies
    if a frame processing function is specified instead. The `while` loop of the `run`
    method is modified to take into account this modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Tracking feature points in a video* recipe in this chapter gives you an
    example of how to use the `FrameProcessor` class interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing video sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipes, we learned how to read a video file and extract its
    frames. This recipe will show you how to write frames and, therefore, create a
    video file. This will allow us to complete the typical video-processing chain:
    reading an input video stream, processing its frames, and then storing the results
    in a new video file.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Writing video files in OpenCV is done using the `cv::VideoWriter` class. An
    instance is constructed by specifying the filename, the frame rate at which the
    generated video should play, the size of each frame, and whether or not the video
    will be created in color:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In addition, you must specify the way you want the video data to be saved. This
    is the `codec` argument; this will be discussed at the end of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the video file is opened, frames can be added to it by repetitively calling
    the `write` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `cv::VideoWriter` class, our `VideoProcessor` class introduced in
    the previous recipe can easily be expanded in order to give it the ability to
    write video files. A simple program that will read a video, process it, and write
    the result to a video file would then be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Proceeding as we did in the preceding recipe, we also want to give the user
    the possibility to write the frames as individual images. In our framework, we
    adopt a naming convention that consists of a prefix name followed by a number
    made of a given number of digits. This number is automatically incremented as
    frames are saved. Then, to save the output result as a series of images, you would
    change the preceding statement with this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Using the specified number of digits, this call will create the `bikeOut000.jpg`,
    `bikeOut001.jpg`, and `bikeOut002.jpg` files, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now describe how to modify our `VideoProcessor` class in order to give
    it the ability to write video files. First, a `cv::VideoWriter` variable member
    must be added to our class (plus a few other attributes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'An extra method is used to specify (and open) the output video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A private method, called the `writeNextFrame` method, handles the frame writing
    procedure (in a video file or as a series of images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For the case where the output is made of individual image files, we need an
    additional setter method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, a new step is then added to the video capture loop of the `run` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a video is written to a file, it is saved using a codec. A **codec** is
    a software module that is capable of encoding and decoding video streams. The
    codec defines both the format of the file and the compression scheme that is used
    to store the information. Obviously, a video that has been encoded using a given
    codec must be decoded with the same codec. For this reason, four-character codes
    have been introduced to uniquely identified codecs. This way, when a software
    tool needs to write a video file, it determines the codec to be used by reading
    the specified four-character code.
  prefs: []
  type: TYPE_NORMAL
- en: The codec four-character code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the name suggests, the four-character code is made up of four ASCII characters
    that can also be converted into an integer by appending them together. Using the
    `CV_CAP_PROP_FOURCC` flag of the get method of an opened `cv::VideoCapture` instance,
    you can obtain this code of an opened video file. We can define a method in our
    `VideoProcessor` class to return the four-character code of an input video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get` method always returns a `double` value that is then casted into an
    integer. This integer represents the code from which the four characters can be
    extracted using a `union` data structure. If we open our test video sequence,
    then we have the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding statements, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'When a video file is written, the codec must be specified using its four-character
    code. This is the second parameter in the `open` method of the `cv::VideoWriter`
    class. You can use, for example, the same one as the input video (this is the
    default option in our `setOutput` method). You can also pass the value `-1` and
    the method will pop up a window that will ask you to select one codec from the
    list of available codecs, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The codec four-character code](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The list you will see on this window corresponds to the list of installed codecs
    on your machine. The code of the selected codec is then automatically sent to
    the `open` method.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [https://www.xvid.com/](https://www.xvid.com/) website offers you an open
    source video codec library based on the MPEG-4 standard for video compression.
    Xvid also has a competitor called DivX, which offers proprietary but free codec
    and software tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking feature points in a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is about reading, writing, and processing video sequences. The
    objective is to be able to analyze a complete video sequence. As an example, in
    this recipe, you will learn how to perform temporal analysis of the sequence in
    order to track feature points as they move from frame to frame.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start the tracking process, the first thing to do is to detect the feature
    points in an initial frame. You then try to track these points in the next frame.
    Obviously, since we are dealing with a video sequence, there is a good chance
    that the object on which the feature points are found has moved (this motion can
    also be due to camera movement). Therefore, you must search around a point's previous
    location in order to find its new location in the next frame. This is what accomplishes
    the `cv::calcOpticalFlowPyrLK` function. You input two consecutive frames and
    a vector of feature points in the first image; the function returns a vector of
    new point locations. To track points over a complete sequence, you repeat this
    process from frame to frame. Note that as you follow the points across the sequence,
    you will unavoidably lose track of some of them such that the number of tracked
    feature points will gradually reduce. Therefore, it could be a good idea to detect
    new features from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now take benefit of the framework we defined in the previous recipes
    and we will define a class that implements the `FrameProcessor` interface introduced
    in the *Processing the video frames* recipe of this chapter. The data attributes
    of this class include the variables that are required to perform both the detection
    of feature points and their tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the `process` method that will be called for each frame of
    the sequence. Basically, we need to proceed as follows. First, feature points
    are detected if necessary. Next, these points are tracked. You reject points that
    you cannot track or you no longer want to track. You are now ready to handle the
    successfully tracked points. Finally, the current frame and its points become
    the previous frame and points for the next iteration. Here is how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This method makes use of four utility methods. It should be easy for you to
    change any of these methods in order to define a new behavior for your own tracker.
    The first of these methods detects the feature points. Note that we already discussed
    the `cv::goodFeatureToTrack` function in the first recipe of [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The second method determines whether new feature points should be detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The third method rejects some of the tracked points based on a criteria defined
    by the application. Here, we decided to reject points that do not move (in addition
    to those that cannot be tracked by the `cv::calcOpticalFlowPyrLK` function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the fourth method handles the tracked feature points by drawing all
    of the tracked points with a line that joins them to their initial position (that
    is, the position where they were detected the first time) on the current frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple main function to track feature points in a video sequence would then
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting program will show you the evolution of the moving tracked features
    over time. Here are, for example, two such frames at two different instants. In
    this video, the camera is fixed. The young cyclist is, therefore, the only moving
    object. Here is the result that is obtained after a few frames have been processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A few seconds later, we obtain the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To track feature points from frame to frame, we must locate the new position
    of a feature point in the subsequent frame. If we assume that the intensity of
    the feature point does not change from one frame to the next one, we are looking
    for a displacement *(u,v)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *I* [*t*] and *I* [*t+1*] are the current frame and the one at the next
    instant, respectively. This constant intensity assumption generally holds for
    small displacement in images that are taken at two nearby instants. We can then
    use the Taylor expansion in order to approximate this equation by an equation
    that involves the image derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This latter equation leads us to another equation (as a consequence of the
    constant intensity assumption that cancels the two intensity terms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This well-known constraint is the fundamental **optical flow** constraint equation.
    This constraint is exploited by the so-called Lukas-Kanade feature-tracking algorithm
    that also makes an additional assumption that the displacement of all points in
    the neighborhood of the feature point is the same. We can, therefore, impose the
    optical flow constraint for all of these points with a unique *(u,v)* unknown
    displacement. This gives us more equations than the number of unknowns (2), and
    therefore, we can solve this system of equations in a mean-square sense. In practice,
    it is solved iteratively and the OpenCV implementation also offers us the possibility
    to perform this estimation at a different resolution in order to make the search
    more efficient and more tolerant to larger displacement. By default, the number
    of image levels is `3` and the window size is `15`. These parameters can obviously
    be changed. You can also specify the termination criteria, which define the conditions
    that stop the iterative search. The sixth parameter of `cv::calcOpticalFlowPyrLK`
    contains the residual mean-square error that can be used to assess the quality
    of the tracking. The fifth parameter contains binary flags that tell us whether
    tracking the corresponding point was considered successful or not.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding description represents the basic principles behind the Lukas-Kanade
    tracker. The current implementation contains other optimizations and improvements
    that make the algorithm more efficient in the computation of the displacement
    of a large number of feature points.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 8](part0058_split_000.html#page "Chapter 8. Detecting Interest Points"),
    *Detecting Interest Points*, has a discussion on feature point detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classic article by B. Lucas and T. Kanade, *An Iterative Image Registration
    Technique with an Application to Stereo Vision* in *Int. Joint Conference in Artificial
    Intelligence, pp. 674-679, 1981*, describes the original feature point tracking
    algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by J. Shi and C. Tomasi, *Good Features to Track in IEEE Conference
    on Computer Vision and Pattern Recognition*, pp. 593-600, 1994, describes an improved
    version of the original feature point tracking algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the foreground objects in a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a fixed camera observes a scene, the background remains mostly unchanged.
    In this case, the interesting elements are the moving objects that evolve inside
    this scene. In order to extract these foreground objects, we need to build a model
    of the background, and then compare this model with a current frame in order to
    detect any foreground objects. This is what we will do in this recipe. Foreground
    extraction is a fundamental step in intelligent surveillance applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had an image of the background of the scene (that is, a frame that contains
    no foreground objects) at our disposal, then it would be easy to extract the foreground
    of a current frame through a simple image difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Each pixel for which this difference is high enough would then be declared as
    a foreground pixel. However, most of the time, this background image is not readily
    available. Indeed, it could be difficult to guarantee that no foreground objects
    are present in a given image, and in busy scenes, such situations might rarely
    occur. Moreover, the background scene often evolves over time because, for instance,
    the lighting condition changes (for example, from sunrise to sunset) or because
    new objects can be added or removed from the background.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is necessary to dynamically build a model of the background scene.
    This can be done by observing the scene for a period of time. If we assume that
    most often, the background is visible at each pixel location, then it could be
    a good strategy to simply compute the average of all of the observations. However,
    this is not feasible for a number of reasons. First, this would require a large
    number of images to be stored before computing the background. Second, while we
    are accumulating images to compute our average image, no foreground extraction
    will be done. This solution also raises the problem of when and how many images
    should be accumulated to compute an acceptable background model. In addition,
    the images where a given pixel is observing a foreground object would have an
    impact on the computation of the average background.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better strategy is to dynamically build the background model by regularly
    updating it. This can be accomplished by computing what is called a **running
    average** (also called **moving average**). This is a way to compute the average
    value of a temporal signal that takes into account the latest received values.
    If pt is the pixel value at a given time *t* and *μ* [*t-1*] is the current average
    value, then this average is updated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting the foreground objects in a video](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The *α* parameter is called the **learning rate**, and it defines the influence
    of the current value over the currently estimated average. The larger this value
    is, the faster the running average will adapt to changes in the observed values.
    To build a background model, one just has to compute a running average for every
    pixel of the incoming frames. The decision to declare a foreground pixel is then
    simply based on the difference between the current image and the background model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s build a class that will learn about a background model using moving
    averages and that will extract foreground objects by subtraction. The required
    attributes are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The main process consists of comparing the current frame with the background
    model and then updating this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Using our video-processing framework, the foreground extraction program will
    be built as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the resulting binary foreground images that will be displayed is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computing the running average of an image is easily accomplished through the
    `cv::accumulateWeighted` function that applies the running average formula to
    each pixel of the image. Note that the resulting image must be a floating point
    image. This is why we had to convert the background model into a background image
    before comparing it with the current frame. A simple thresholded absolute difference
    (computed by `cv::absdiff` followed by `cv::threshold`) extracts the foreground
    image. Note that we then used the foreground image as a mask to cv`::accumulateWeighte`d
    in order to avoid the updating of pixels declared as foreground. This works because
    our foreground image is defined as being false (that is, `0`) at foreground pixels
    (which also explains why the foreground objects are displayed as black pixels
    in the resulting image).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it should be noted that, for simplicity, the background model that
    is built by our program is based on the gray-level version of the extracted frames.
    Maintaining a color background would require the computation of a running average
    in some color space. However, the main difficulty in the presented approach is
    to determine the appropriate value for the threshold that would give good results
    for a given video.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding simple method to extract foreground objects in a scene works well
    for simple scenes that show a relatively stable background. However, in many situations,
    the background scene might fluctuate in certain areas between different values,
    thus causing frequent false foreground detections. These might be due to, for
    example, a moving background object (for example, tree leaves) or a glaring effect
    (for example, on the surface of water). Casted shadows also pose a problem since
    they are often detected as part of a moving object. In order to cope with these
    problems, more sophisticated background modeling methods have been introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The Mixture of Gaussian method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of these algorithms is the **Mixture of Gaussian** method. It proceeds in
    a way that is similar to the method presented in this recipe but adds a number
    of improvements.
  prefs: []
  type: TYPE_NORMAL
- en: First, the method maintains more than one model per pixel (that is, more than
    one running average). This way, if a background pixel fluctuates between, let's
    say, two values, two running averages are then stored. A new pixel value will
    be declared as the foreground only if it does not belong to any of the most frequently
    observed models. The number of models used is a parameter of the method and a
    typical value is `5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, not only is the running average maintained for each model, but also
    for the running variance. This is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mixture of Gaussian method](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These computed averages and variances are used to build a Gaussian model from
    which the probability of a given pixel value to belong to the background can be
    estimated. This makes it easier to determine an appropriate threshold since it
    is now expressed as a probability rather than an absolute difference. Consequently,
    in areas where the background values have larger fluctuations, a greater difference
    will be required to declare a foreground object.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when a given Gaussian model is not hit sufficiently often, it is excluded
    as being part of the background model. Reciprocally, when a pixel value is found
    to be outside the currently maintained background models (that is, it is a foreground
    pixel), a new Gaussian model is created. If in the future this new model becomes
    a hit, then it becomes associated with the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'This more sophisticated algorithm is obviously more complex to implement than
    our simple background/foreground segmentor. Fortunately, an OpenCV implementation
    exists, called `cv::BackgroundSubtractorMOG`, and is defined as a subclass of
    the more general `cv::BackgroundSubtractor` class. When used with its default
    parameter, this class is very easy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As it can be seen, it is just a matter of creating the class instance and calling
    the method that simultaneously updates the background and returns the foreground
    image (the extra parameter being the learning rate). Also note that the background
    model is computed in color here. The method implemented in OpenCV also includes
    a mechanism to reject shadows by checking whether the observed pixel variation
    is simply caused by a local change in brightness (if so, then it is probably due
    to a shadow) or whether it also includes some change in chromaticity.
  prefs: []
  type: TYPE_NORMAL
- en: A second implementation is also available and is simply called `cv::BackgroundSubtractorMOG2`.
    One of the improvements is that the number of appropriate Gaussian models per
    pixel to be used is now determined dynamically. You can use this in place of the
    previous one in the preceding example. You should run these different methods
    on a number of videos in order to appreciate their respective performances. In
    general, you will observe that `cv::BackgroundSubtractorMOG2` is much faster.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article by C. Stauffer and W.E.L. Grimson, *Adaptive Background Mixture
    Models for Real-Time Tracking*, in *Conf. on Computer Vision and Pattern Recognition,
    1999*, gives you a more complete description of the Mixture of Gaussian algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
