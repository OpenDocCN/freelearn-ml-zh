["```py\n\nimport numpy as np \n\nseed = 128 \n\nnp.random.seed(seed)\n\n```", "```py\n\nfrom sklearn.datasets import make_classification \n\ndata, labels = make_classification(n_samples = 2500, \n\n    n_features = 2, n_informative = 2, n_redundant = 0, \n\n    weights = (0.2,0.8), class_sep = 0.5, random_state = seed)\n\n```", "```py\n\nimport matplotlib.pyplot as plt \n\nfor i in range(2): \n\n    plt.hist(data[:,i][labels == 1], bins=100, alpha=0.8, label = \"Negative\") \n\n    plt.hist(data[:,i][labels == 0], bins=100, alpha=0.8, label = \"Positive\") \n\n    plt.legend() \n\n    plt.show()\n\n```", "```py\n\nfrom sklearn.model_selection import train_test_split \n\n# Split into a training and a test dataset. \n\nx_tr, x_test, y_tr, y_test = train_test_split( \n\n    data, labels, shuffle = True, train_size = 0.8) \n\n# Split the test dataset to get a validation one. \n\nx_val, x_test, y_val, y_test = train_test_split( \n\n    x_test, y_test, shuffle = True, train_size = 0.5)\n\n```", "```py\n\nimport tensorflow as tf \n\ntf.random.set_seed(seed) \n\nmodel = tf.keras.Sequential([ \n\n    tf.keras.layers.Input(2), \n\n    tf.keras.layers.Dense(8, activation = \"elu\"), \n\n    tf.keras.layers.Dense(16, activation = \"elu\"), \n\n    tf.keras.layers.Dense(8, activation = \"elu\"), \n\n    tf.keras.layers.Dense(1, activation = \"sigmoid\"), \n\n])\n\n```", "```py\n\nopt = tf.keras.optimizers.Adam() \n\nlossf = tf.keras.losses.BinaryCrossentropy() \n\nmodel.compile(optimizer = opt, loss = lossf)\n\n```", "```py\n\nhistory = model.fit(x_tr, y_tr, \n\n    validation_data = (x_val, y_val), epochs = 8, \n\n    batch_size = None)\n\n```", "```py\n\nEpoch 1/8 \n63/63 [====================] - 1s 3ms/step - loss: 0.6748 \n- val_loss: 0.4859 \nEpoch 2/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.4144 \n- val_loss: 0.3095 \nEpoch 3/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.3173 \n- val_loss: 0.2502 \nEpoch 4/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.2908 \n- val_loss: 0.2315 \nEpoch 5/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.2830 \n- val_loss: 0.2262 \nEpoch 6/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.2793 \n- val_loss: 0.2221 \nEpoch 7/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.2765 \n- val_loss: 0.2187 \nEpoch 8/8 \n63/63 [====================] - 0s 1ms/step - loss: 0.2744 \n- val_loss: 0.2185\n\n```", "```py\n\nearly_stp = tf.keras.callbacks.EarlyStopping( \n\n    monitor = \"val_loss\", patience = 3, min_delta = 0.001)\n\n```", "```py\n\noutput = model.predict(x_test) \n\nresult = (output > 0.5).astype(float)\n\n```", "```py\n\nfrom sklearn.metrics import accuracy_score \n\nprint(accuracy_score(result, y_test))\n\n```", "```py\n\nfrom sklearn.metrics import confusion_matrix \n\nconfusion_matrix(y_true = y_test, y_pred = result)\n\n```", "```py\n\nfrom sklearn.metrics import classification_report \n\nprint(classification_report(y_true = y_test, y_pred = result))\n\n```", "```py\n\n              precision    recall  f1-score   support \n\n           0       0.77      0.55      0.64        44 \n           1       0.91      0.97      0.94       206 \n\n    accuracy                           0.89       250 \n   macro avg       0.84      0.76      0.79       250 \nweighted avg       0.89      0.89      0.88       250\n\n```", "```py\n\nfrom sklearn.metrics import roc_curve \n\nfpr, tpr, _ = roc_curve(y_test, output) \n\nplt.plot(fpr, tpr) \n\nplt.plot([0,1],[0,1],linestyle=\"--\",color=\"black\") \n\nplt.xlabel(\"FPR\"); plt.ylabel(\"TPR\") \n\nplt.show()\n\n```", "```py\n\nfrom sklearn.metrics import auc \n\nprint(auc(fpr,tpr))\n\n```"]