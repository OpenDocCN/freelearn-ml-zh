- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Deploying Machine Learning Models at Scale
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模部署机器学习模型
- en: In previous chapters, we learned about how to store data, carry out data processing,
    and perform model training for machine learning applications. After training a
    machine learning model and validating it using a test dataset, the next task is
    generally to perform inference on new and unseen data. It is important for any
    machine learning application that the trained model should generalize well for
    unseen data to avoid overfitting. In addition, for real-time applications, the
    model should be able to carry out inference with minimal latency while accessing
    all the relevant data (both new and stored) needed for the model to do inference.
    Also, the compute resources associated with the model should be able to scale
    up or down depending on the number of inference requests, in order to optimize
    cost while not sacrificing performance and inference requirements for real-time
    machine learning applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何存储数据、执行数据处理以及为机器学习应用进行模型训练。在训练机器学习模型并使用测试数据集进行验证后，接下来的任务通常是对新数据和未见数据进行推理。对于任何机器学习应用来说，训练好的模型应该能够很好地泛化未见数据，以避免过拟合。此外，对于实时应用，模型应该能够在访问所有相关数据（包括新数据和存储数据）的同时，以最小的延迟执行推理。同时，与模型相关的计算资源应该能够根据推理请求数量进行扩展或缩减，以便在优化成本的同时，不牺牲性能和实时机器学习应用的需求。
- en: For use cases that do not require real-time inference, the trained model should
    be able to carry out inference on very large datasets with thousands of variables
    in a reasonable amount of time as well. In addition, in several scenarios, we
    may not want to go through the effort of managing servers and software packages
    needed for inference, and instead, focus our effort on developing and improving
    our machine learning models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不需要实时推理的使用案例，训练好的模型应该能够在合理的时间内对包含数千个变量的非常大的数据集进行推理。此外，在几种情况下，我们可能不想费力去管理推理所需的服务器和软件包，而是将精力集中在开发和改进我们的机器学习模型上。
- en: Keeping all these aforementioned factors in mind, AWS provides multiple options
    for deploying machine learning models to carry out inference on new and unseen
    data. These options consist of real-time inference, batch inference, and asynchronous
    inference. In this chapter, we are going to discuss the managed deployment options
    of machine learning models using Amazon SageMaker, along with various features
    such as high availability of models, auto-scaling, and blue/green deployments.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述所有因素，AWS提供了多种选项来部署机器学习模型，以便对新数据和未见数据执行推理。这些选项包括实时推理、批量推理和异步推理。在本章中，我们将讨论使用Amazon
    SageMaker进行机器学习模型的托管部署选项，以及各种功能，如模型的高可用性、自动扩展和蓝/绿部署。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Managed deployment on AWS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的托管部署
- en: Choosing the right deployment option
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的部署选项
- en: Batch inference
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量推理
- en: Real-time inference
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时推理
- en: Asynchronous inference
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步推理
- en: High availability of model endpoints
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型端点的高可用性
- en: Blue/green deployments
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Managed deployment on AWS
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS上的托管部署
- en: Data scientists and machine learning practitioners working on developing machine
    learning models to solve business problems are often very focused on model development.
    Problem formulation and developing an elegant solution, choosing the right algorithm,
    and training the model so that it provides reliable and accurate results are the
    main components of the machine learning problem solving cycle that we want our
    data scientists and data engineers to focus on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和机器学习从业者通常非常专注于开发机器学习模型以解决业务问题。问题表述和开发优雅的解决方案、选择合适的算法以及训练模型以确保其提供可靠和准确的结果是我们希望我们的数据科学家和数据工程师关注的机器学习问题解决周期的主要组成部分。
- en: However, once we have a good model, we want to run real-time or batch inference
    on new data. Deploying the model and then managing it are tasks that often require
    dedicated engineers and computation resources. This is because, we first need
    to make sure that we have all the right packages and libraries for the model to
    work correctly. Then, we also need to decide on the type and amount of compute
    resources needed for the model to run. In real-time applications, we often end
    up designing for peak performance requirements, just like provisioning servers
    for IT projects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦我们有一个好的模型，我们希望在新的数据上运行实时或批量推理。部署模型并管理它通常是需要专门工程师和计算资源的任务。这是因为，我们首先需要确保我们有所有正确的包和库，以便模型能够正确工作。然后，我们还需要决定模型运行所需的计算资源类型和数量。在实时应用中，我们通常是为了满足峰值性能需求而进行设计，就像为
    IT 项目配置服务器一样。
- en: After the model is deployed and is running, we also need to make sure that everything
    stays working in the manner that we expect it to. Furthermore, in real world scenarios,
    data scientists often have to manually carry out analysis periodically to detect
    model or data drift. In the event that either of these drifts are detected, the
    data scientists go through the entire cycle of exploratory analysis, feature engineering,
    model development, model training, hyperparameter optimization, model evaluation,
    and model deployment again. All these tasks consume a lot of effort and resources
    and due to this reason, many organizations have moved to automating these processes
    using **machine learning operations** (**MLOps**) workflows and managed model
    deployment options that scale well with varying workloads.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署并运行后，我们还需要确保一切按预期工作。此外，在现实世界场景中，数据科学家通常需要定期手动进行分析，以检测模型或数据漂移。如果在这些漂移中检测到任何一个，数据科学家将再次经历整个探索性分析、特征工程、模型开发、模型训练、超参数优化、模型评估和模型部署的周期。所有这些任务都需要大量的努力和资源，因此，许多组织已经转向使用
    **机器学习操作**（**MLOps**）工作流程和管理模型部署选项来自动化这些流程，这些选项能够很好地适应不断变化的工作负载。
- en: Amazon SageMaker offers multiple fully managed model deployment options. In
    this section, we give an overview of these managed deployment options along with
    their benefits and then discuss a few of these deployment options in detail in
    the following sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供了多种完全管理的模型部署选项。在本节中，我们将概述这些管理部署选项及其优势，然后在接下来的几节中详细讨论其中的一些部署选项。
- en: Amazon SageMaker managed model deployment options
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon SageMaker 管理模型部署选项
- en: 'Amazon SageMaker offers the following managed deployment model options:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供以下管理部署模型选项：
- en: '**Batch Transform**: SageMaker Batch Transform is used to carry out inference
    on large datasets. There is no persistent endpoint in this case. This method is
    commonly used to carry out inference in a non-real-time machine learning use case
    requiring offline inference on larger datasets.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量转换**：SageMaker 批量转换用于对大型数据集进行推理。在这种情况下没有持久端点。此方法通常用于在需要离线对大型数据集进行推理的非实时机器学习用例中执行推理。'
- en: '**Real-time endpoint**: A SageMaker real-time endpoint is for use cases where
    a persistent machine learning model endpoint is needed, which carries out inference
    on a few data samples in real time.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时端点**：SageMaker 实时端点用于需要持久机器学习模型端点用例，该端点对少量数据样本进行实时推理。'
- en: '**Asynchronous inference**: Amazon SageMaker Asynchronous Inference deploys
    an asynchronous endpoint for carrying out inference on large payloads (up to 1
    GB) with large processing times and low latency.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步推理**：Amazon SageMaker 异步推理部署了一个异步端点，用于对大型负载（高达 1 GB）进行推理，具有长处理时间和低延迟。'
- en: '**Serverless Inference**: In all the previous methods, the user is required
    to select the compute instance types for inference. Amazon SageMaker Serverless
    Inference automatically chooses the server type and scales up and down based on
    the load on the endpoint. It is often useful for applications that have unpredictable
    traffic patterns.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器推理**：在所有之前的方法中，用户都需要选择用于推理的计算实例类型。Amazon SageMaker 无服务器推理会自动选择服务器类型，并根据端点的负载进行扩展和缩减。这对于具有不可预测流量模式的应用程序通常很有用。'
- en: Let’s explore the variety of available compute resources next.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来探索可用的计算资源种类。
- en: The variety of compute resources available
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的计算资源种类
- en: To carry out inference, there are a wide variety of computation instances available.
    At the time of writing, approximately 70+ instances are available for carrying
    out machine learning inference. These instances have varying levels of computation
    power and memory available to serve different use cases. There is also the option
    of using **graphical processing units** (**GPUs**) for inference. In addition,
    SageMaker also supports Inf1 instances for high-performance and low-cost inference.
    These options make SageMaker model deployment and inference highly versatile and
    suitable for a variety of machine learning use cases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行推理，有各种各样的计算实例可供选择。在撰写本文时，大约有70多个实例可用于执行机器学习推理。这些实例具有不同的计算能力和内存，可以服务于不同的用例。还有使用**图形处理单元**（**GPUs**）进行推理的选项。此外，SageMaker还支持Inf1实例，用于高性能和低成本推理。这些选项使SageMaker模型部署和推理非常灵活，适用于各种机器学习用例。
- en: Cost-effective model deployment
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经济高效的模型部署
- en: Amazon SageMaker has various options for optimizing model deployment cost. There
    are multi-model endpoints, where multiple models can share a container. This helps
    with reducing hosting costs since endpoint utilization is increased due to multiple
    models sharing the same endpoint. In addition, it also enables time sharing of
    memory resources across different models. SageMaker also has the option of building
    and deploying multi-container endpoints. Furthermore, we can attach scaling policies
    to our endpoints to allocate more compute resources when traffic increases, and
    shut down instances when traffic decreases in order to save on costs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker有多种优化模型部署成本的选择。有多个模型可以共享一个容器的多模型端点。这有助于降低托管成本，因为端点利用率由于多个模型共享相同的端点而提高。此外，它还允许不同模型之间共享内存资源。SageMaker还有构建和部署多容器端点的选项。此外，我们可以将扩展策略附加到我们的端点，以便在流量增加时分配更多计算资源，在流量减少时关闭实例以节省成本。
- en: Another cost-effective option for model deployment is SageMaker Serverless Inference.
    Serverless Inference utilizes AWS Lambda to scale up and down compute resources
    as traffic increases or decreases. It is especially useful for scenarios with
    unpredictable traffic patterns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种经济高效的模型部署选项是SageMaker无服务器推理。无服务器推理利用AWS Lambda根据流量增加或减少来扩展和缩减计算资源。它对于流量模式不可预测的场景特别有用。
- en: Blue/green deployments
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: 'SageMaker automatically uses blue/green deployment whenever we update a SageMaker
    model endpoint. In blue/green deployment, SageMaker uses a new fleet of instances
    to deploy the updated endpoints and then shifts the traffic to the updated endpoint
    from the old fleet to the new one. Amazon SageMaker offers the following traffic-shifting
    strategies for blue/green deployments:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们更新SageMaker模型端点时，SageMaker都会自动使用蓝绿部署。在蓝绿部署中，SageMaker使用一组新实例来部署更新的端点，然后将流量从旧实例群切换到新实例群。Amazon
    SageMaker为蓝绿部署提供以下流量切换策略：
- en: All at once traffic shifting
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时流量切换
- en: Canary traffic shifting
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀流量切换
- en: Linear traffic shifting
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性流量切换
- en: We will discuss these traffic patterns in more detail later in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更详细地讨论这些流量模式。
- en: Inference recommender
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理推荐器
- en: With Amazon SageMaker Inference Recommender, we can automatically get recommendations
    on the type of compute instance to use for deploying our model endpoint. It gives
    us instance recommendations by load testing various instances and outputs inference
    costs, along with throughput and latency, for the tested instance types. This
    helps us decide on the type of instance to use for deploying our model endpoint.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker推理推荐器，我们可以自动获取用于部署我们的模型端点的计算实例类型的推荐。它通过负载测试各种实例并提供测试实例类型的推理成本、吞吐量和延迟来给出实例推荐。这有助于我们决定用于部署模型端点的实例类型。
- en: MLOps integration
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps集成
- en: Using Amazon SageMaker, we can easily build machine learning workflows that
    integrate with **continuous integration and continuous delivery** (**CI/CD**)
    pipelines. These workflows can be used to automate the entire machine learning
    life cycle, including data labeling, data processing and feature engineering,
    model training and registry, post-processing, endpoint deployment, and model monitoring
    for data and model drift monitoring. For model deployment, these workflows can
    be used to automate the process of doing batch inference, as well as pushing model
    endpoints from development to staging to production environments.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker，我们可以轻松构建与**持续集成和持续交付**（**CI/CD**）管道集成的机器学习工作流程。这些工作流程可用于自动化整个机器学习生命周期，包括数据标注、数据处理和特征工程、模型训练和注册、后处理、端点部署以及数据漂移和模型漂移监控的模型监控。对于模型部署，这些工作流程可用于自动化批量推理的过程，以及将模型端点从开发环境推送到预生产和生产环境。
- en: Model registry
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型注册
- en: Amazon SageMaker provides the capability to register and catalog machine learning
    models with the SageMaker model registry. Using the model registry, we can include
    different versions of a trained model in a model package group. This way, whenever
    we train and register a model, it is added as a new version to the model package
    group. In addition, using the model registry, we can also associate metadata and
    training metrics to a machine learning model, approve or reject a model, and if
    approved, move the models to production. These features of the model registry
    facilitate the building of CI/CD pipelines needed for automating machine learning
    workflows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker提供了使用SageMaker模型注册功能注册和编目机器学习模型的能力。使用模型注册，我们可以将训练模型的多个版本包含在模型包组中。这样，每次我们训练和注册模型时，它都会作为新版本添加到模型包组中。此外，使用模型注册，我们还可以将元数据和训练指标关联到机器学习模型，批准或拒绝模型，如果批准，则将模型移至生产环境。模型注册的这些功能有助于构建自动化机器学习工作流程所需的CI/CD管道。
- en: Elastic inference
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性推理
- en: For machine learning use cases that require very high throughput and low latency,
    we often end up using GPU machines, thereby increasing the cost of inference significantly.
    Using Amazon SageMaker, we can add elastic inference to our endpoints. Elastic
    inference provides inference acceleration to our endpoint, by attaching just the
    right amount of GPU-powered inference acceleration to any SageMaker instance type.
    This helps significantly with latency and throughput, while also achieving it
    at a much lower cost compared to using GPU instances for inference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要非常高的吞吐量和低延迟的机器学习用例，我们通常最终会使用GPU机器，从而显著增加推理的成本。使用Amazon SageMaker，我们可以将弹性推理添加到我们的端点。弹性推理通过仅附加适量的GPU推理加速器到任何SageMaker实例类型，为我们端点提供推理加速。这有助于显著降低延迟和提高吞吐量，同时与使用GPU实例进行推理相比，成本要低得多。
- en: Deployment on edge devices
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在边缘设备上部署
- en: Many machine learning use cases require models to run on edge devices such as
    mobile devices, cameras, and specialized devices. These devices often have low
    compute resources, memory, and storage. Furthermore, deploying, managing, and
    monitoring machine learning models on a fleet of devices is a difficult task because
    of the variability in device hardware and operating systems. With Amazon SageMaker
    Edge Manager, machine learning models can be deployed, monitored, and managed
    on a fleet of devices with different hardware and software configurations. SageMaker
    Edge Manager uses SageMaker Neo to compile machine learning models and packages
    these compiled models to be deployed on edge devices. In addition, we can also
    sample the data used by the model on the edge devices and send them to the cloud
    to carry out analysis to determine quality issues such as data and model drift.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习用例需要模型在边缘设备上运行，例如移动设备、摄像头和专用设备。这些设备通常具有较低的计算资源、内存和存储。此外，由于设备硬件和操作系统的可变性，部署、管理和监控设备上的机器学习模型是一项困难的任务。使用Amazon
    SageMaker Edge Manager，可以在具有不同硬件和软件配置的设备群上部署、监控和管理机器学习模型。SageMaker Edge Manager使用SageMaker
    Neo编译机器学习模型，并将这些编译后的模型打包以部署到边缘设备。此外，我们还可以收集边缘设备上模型使用的数据样本，并将它们发送到云端进行数据分析，以确定数据和质量问题，如数据漂移和模型漂移。
- en: Now, let’s discuss the various model deployment options on AWS in the following
    section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节中讨论AWS上各种模型部署选项。
- en: Choosing the right deployment option
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的部署选项
- en: 'As mentioned in the previous section, AWS has multiple model deployment and
    inference options. It can get confusing and overwhelming sometimes to decide on
    the right option for model deployment. The decision to select the right model
    deployment option really depends on the use case parameters and requirements.
    A few important factors to consider while deciding on deployment options are listed
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AWS提供了多种模型部署和推理选项。有时决定正确的模型部署选项可能会让人困惑和不知所措。选择正确的模型部署选项的决定实际上取决于用例参数和需求。在决定部署选项时，以下是一些重要的考虑因素：
- en: Do we have an application that needs a real-time, persistent endpoint to carry
    out on-demand inference on new data in real time and very quickly with low latency
    and high availability?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有需要实时、持久端点以实时和非常快速地按需对新数据进行推理的应用程序？
- en: Can our application wait for a minute or two for the compute resources to come
    online before getting the inference results?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的应用程序是否可以等待一分钟或两分钟，直到计算资源上线后再获取推理结果？
- en: Do we have a use case where we do not need results in near real time? Can we
    do inference on a batch of data once a day/week or on an as-needed basis?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有不需要近乎实时结果的使用场景？我们能否每天/每周或按需对一批数据进行推理？
- en: Do we have an unpredictable and non-uniform traffic pattern requiring inference?
    Do we need to scale up and down our compute resources based on the traffic?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有不可预测且不均匀的流量模式需要推理？我们是否需要根据流量调整计算资源的大小？
- en: How big is the data (number of data points/rows) that we are trying to do inference
    on?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们试图进行推理的数据有多大（数据点/行数）？
- en: Do we need dedicated resources all the time to carry out inference or can we
    follow a serverless approach?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要一直使用专用资源来执行推理，或者我们可以采用无服务器的方法？
- en: Can we pack in multiple models in a single endpoint to save on cost?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否在一个端点中打包多个模型以节省成本？
- en: Do we need to have an inference pipeline consisting of multiple models, pre-processing
    steps, and post-processing steps?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要一个由多个模型、预处理步骤和后处理步骤组成的推理管道？
- en: In the following subsections, we will discuss when to pick the different types
    of model deployment and inference options provided by Amazon SageMaker, while
    addressing the previously mentioned questions. We will also provide examples of
    typical example use cases for each of the model deployment options.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将讨论何时选择亚马逊SageMaker提供的不同类型的模型部署和推理选项，同时解答之前提到的问题。我们还将提供每个模型部署选项的典型用例示例。
- en: Using batch inference
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用批量推理
- en: '**Amazon SageMaker Batch Transform** is used when there is no need for a persistent,
    real-time endpoint and inference can be done on large batches of data. The following
    examples illustrate the use of SageMaker Batch Transform:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊SageMaker批量转换**用于不需要持久实时端点且可以在大量数据上执行推理的情况。以下示例说明了SageMaker批量转换的使用：'
- en: '**Predictive maintenance**: In a manufacturing plant, sensor and machine data
    for various components could be collected the entire day. For such a use case,
    there is no need for real-time or asynchronous endpoints. At night, machine learning
    models can be used for predicting whether a component is about to fail, or whether
    a part of the machinery needs maintenance. These models would run on large batches
    of data and carry out inference using SageMaker Batch Transform. The results from
    these models could then be used to make and execute a maintenance schedule.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测性维护**：在制造工厂中，各种组件的传感器和机器数据可能全天收集。对于此类用例，不需要实时或异步端点。夜间，可以使用机器学习模型预测某个组件是否即将失效，或者机械的一部分是否需要维护。这些模型将在大量数据上运行，并使用SageMaker批量转换进行推理。这些模型的结果可以用来制定和执行维护计划。'
- en: '**Home prices prediction**: Real estate companies collect data for a few days
    (and sometimes weeks) before coming out with new home prices and market direction
    predictions. These models do not need real-time or asynchronous endpoints as they
    need to be run only after a few days or weeks and on large amounts of data. For
    such use cases, SageMaker Batch Transform is the ideal option for inference. SageMaker
    Batch Transform jobs could run on a fixed interval in a machine learning pipeline
    on new and historical data, carrying out inference to predict home price adjustments
    and market direction by localities. These results can, in turn, then be used to
    determine if the machine learning models need to be retrained.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**房价预测**：房地产公司在新房价和市场方向预测出台前收集几天（有时是几周）的数据。这些模型不需要实时或异步端点，因为它们只需要在几天或几周后以及在大量的数据上运行。对于此类用例，SageMaker
    批量转换是推理的理想选择。SageMaker 批量转换作业可以在机器学习管道中定期运行，处理新数据和历史数据，通过地区预测房价调整和市场方向。这些结果反过来又可以用来确定是否需要重新训练机器学习模型。'
- en: We will cover batch, real-time, and asynchronous inference options in Amazon
    SageMaker in detail in the later sections of this chapter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面的部分详细介绍 Amazon SageMaker 中的批量、实时和异步推理选项。
- en: Using real-time endpoints
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用实时端点
- en: 'Amazon SageMaker real-time endpoints should be the choice for model deployment
    when there is a need for a real-time persistent model endpoint, doing predictions
    with low latency as new data arrives. Real-time endpoints are fully managed by
    Amazon SageMaker and can be deployed as multi-model and multi-container endpoints.
    The following are some example use cases for real-time endpoints:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 的实时端点应在需要实时持久化模型端点、随着新数据的到来进行低延迟预测时作为模型部署的选择。实时端点由 Amazon SageMaker
    完全管理，并可部署为多模型和多容器端点。以下是一些实时端点的示例用例：
- en: '**Fraudulent transaction**: A customer uses a credit card to purchase an item
    online or physically in a retail store. This financial transaction can be carried
    out by the actual owner of the credit card or it can be a stolen credit card as
    well. The financial institution needs to make the decision in real time whether
    to approve the transaction or not. In such a scenario, a machine learning model
    can be deployed as a real-time endpoint. This model could use customer’s demographic
    data and history of purchases from historical data tables, while also using some
    data from the current transaction, such as IP address and web browser parameters
    (if it is an online transaction), or store location and image and/or video from
    a camera in real time (if it is a physical transaction), to classify whether the
    transaction is fraudulent or not. This decision can then be used by the financial
    institution to either approve or reject the transaction, or contact the customer
    for notification, manual authentication, and approval.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈交易**：客户使用信用卡在线购买商品或在实体零售店购买。这种金融交易可以是信用卡的实际持卡人进行的，也可以是盗窃的信用卡进行的。金融机构需要实时决定是否批准这笔交易。在这种情况下，可以将机器学习模型部署为实时端点。该模型可以使用客户的人口统计数据和历史购买记录（来自历史数据表），同时使用一些来自当前交易的数据，例如
    IP 地址和网页浏览器参数（如果是在线交易），或实时使用来自摄像头的商店位置和图像以及/或视频（如果是实体交易），以判断交易是否为欺诈。金融机构可以使用这个决定来批准或拒绝交易，或联系客户进行通知、手动验证和批准。'
- en: '**Real-time sentiment analysis**: A customer is having a chat or phone conversation
    with a customer care agent. A machine learning model is analyzing the chat or
    transcribed text from voice conversation in real time to decide on the sentiment
    that the customer is showing. Based on the sentiment, if the customer is unhappy,
    the agent can offer various promotions or escalate the case to a supervisor before
    things get out of hand. This machine learning model should be deployed using a
    real-time endpoint so that the sentiment can be correctly determined without any
    lag or delay.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时情感分析**：客户在与客户服务代表进行聊天或电话交谈。机器学习模型实时分析聊天或语音对话的转录文本，以确定客户所表现出的情感。根据情感，如果客户不满意，代表可以提供各种促销活动或升级案件至主管，以防止事情失控。这个机器学习模型应该使用实时端点部署，以便正确确定情感而没有任何延迟或延迟。'
- en: '**Quality assurance**: In a manufacturing plant, products are being assembled
    on an assembly line and there needs to be strict quality control to remove defective
    products as soon as possible. This is again an application where real-time inference
    from a machine learning model classifying the objects as defective or normal using
    live images or video feed will be useful.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量保证**：在制造工厂中，产品正在装配线上组装，需要进行严格的质量控制，以便尽快去除有缺陷的产品。这同样是一个应用，其中来自机器学习模型的实时推理，该模型使用实时图像或视频流将对象分类为有缺陷或正常，将非常有用。'
- en: Similar to real-time endpoints, we also have the option of using asynchronous
    endpoints, which we will learn about in the following section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与实时端点类似，我们还有使用异步端点的选项，我们将在下一节中了解。
- en: Using asynchronous inference
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用异步推理
- en: Amazon SageMaker Asynchronous Inference endpoints are very similar to real-time
    endpoints. Asynchronous endpoints can queue inference requests and are the deployment
    option of choice when near real-time latency is needed, while also processing
    large workloads. The following example illustrates a potential asynchronous endpoints
    use case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 异步推理端点与实时端点非常相似。异步端点可以排队推理请求，并且在需要接近实时延迟的同时处理大量工作负载时是首选的部署选项。以下示例说明了潜在的异步端点用例。
- en: '**Train track inspection**: Several trains are running on their everyday routes
    with attached cameras that take images and videos of train tracks and switches
    for defect detection. These trains do not have a high bandwidth available to transmit
    this data in real time for inference. When these trains dock at a station, a large
    number of images and videos could be sent to a SageMaker Asynchronous Inference
    endpoint for inference to find out whether everything is normal, or if there are
    any defects present anywhere on the track or switches. The compute instance associated
    with the asynchronous endpoint will start as soon as it receives data from any
    of the trains, carrying out inference on the data, then shutting down once all
    the data has been processed. This will help with the reduction in costs compared
    to a real-time endpoint.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**轨道检查**：几列火车在其日常路线上运行，并配备了摄像头，这些摄像头可以捕捉轨道和开关的图像和视频以进行缺陷检测。这些火车没有足够的带宽来实时传输这些数据以进行推理。当这些火车停靠在车站时，大量图像和视频可以发送到
    SageMaker 异步推理端点进行推理，以确定一切是否正常，或者轨道或开关上是否存在任何缺陷。与异步端点关联的计算实例将在接收到来自任何火车的数据后立即启动，对数据进行推理，然后一旦所有数据都已被处理，就关闭。这将有助于降低与实时端点相比的成本。'
- en: Batch inference
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量推理
- en: For carrying out batch inference on datasets, we can use SageMaker Batch Transform.
    It should be used for inference when there is no need for a real-time persistent
    deployed machine learning model. Batch Transform is also useful when the dataset
    for inference is large or if we need to carry out heavy preprocessing on the dataset.
    For example, removing bias or noise from the data, converting speech data to text,
    and filtering and normalization of images and video data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在数据集上执行批量推理，我们可以使用 SageMaker 批量转换。当不需要实时持久部署的机器学习模型时，应使用推理。批量转换在推理数据集很大或我们需要对数据集进行大量预处理时也非常有用。例如，从数据中去除偏差或噪声，将语音数据转换为文本，以及过滤和归一化图像和视频数据。
- en: 'We can pass input data to SageMaker Batch Transform in either one file or using
    multiple files. For tabular data in one file, each row in the file is interpreted
    as one data record. If we have selected more than one instance for carrying out
    the batch transform job, SageMaker distributes the input files to different instances
    for batch transform jobs. Individual data files can also be split into multiple
    mini-batches and batch transform on can be carried out these mini-batches in parallel
    on separate instances. *Figure 7**.1* shows a simplified typical architecture
    example of SageMaker Batch Transform:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将输入数据传递给 SageMaker 批量转换，要么是一个文件，要么是多个文件。对于单个文件中的表格数据，文件中的每一行都被解释为一个数据记录。如果我们选择了多个实例来执行批量转换作业，SageMaker
    将将输入文件分发到不同的实例以进行批量转换作业。单个数据文件也可以分成多个小批量，并且可以在不同的实例上并行对这些小批量进行批量转换。*图 7.1* 展示了
    SageMaker 批量转换的简化典型架构示例：
- en: '![Figure 7.1 – Example architecture for Amazon SageMaker Batch Transform](img/B18493_07_001.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – Amazon SageMaker 批量转换的示例架构](img/B18493_07_001.jpg)'
- en: Figure 7.1 – Example architecture for Amazon SageMaker Batch Transform
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Amazon SageMaker 批量转换的示例架构
- en: 'As shown in the figure, data is read from **Amazon S3**. Preprocessing and
    feature engineering is carried out on this data using **Amazon SageMaker Processing**
    in order to transform the data in the right format that the machine learning model
    expects. A trained machine learning model is then used by **Amazon SageMaker Batch
    Transform** to carry out batch inference. The results are then written back to
    **Amazon S3**. The machine learning model used by Batch Transform could either
    be trained using Amazon SageMaker or can be a model trained outside of Amazon
    SageMaker. The two main steps in Batch Transform are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，数据是从**Amazon S3**读取的。使用**Amazon SageMaker Processing**对数据进行预处理和特征工程，以便将数据转换为机器学习模型期望的正确格式。然后，使用**Amazon
    SageMaker Batch Transform**对训练好的机器学习模型进行批量推理。结果随后写回到**Amazon S3**。批量转换中使用的机器学习模型可以是使用Amazon
    SageMaker训练的，也可以是在Amazon SageMaker之外训练的模型。批量转换的两个主要步骤如下：
- en: Creating a transformer object.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个转换器对象。
- en: Creating a batch transform job for carrying out inference.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个批量转换作业以进行推理。
- en: These steps are described in the following subsections.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在以下小节中描述。
- en: Creating a transformer object
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个转换器对象
- en: 'We first need to create an object of the `Transformer` class in order to run
    a SageMaker batch transform job. While creating the `Transformer` object, we can
    specify the following parameters:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建`Transformer`类的对象，以便运行SageMaker批量转换作业。在创建`Transformer`对象时，我们可以指定以下参数：
- en: '`model_name`: This is the name of the machine learning model that we are going
    to use for inference. This can also be a built-in SageMaker model, for which we
    can directly use the `transformer` method of the built-in estimator.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`：这是我们打算用于推理的机器学习模型的名称。这也可以是一个内置的SageMaker模型，对于这种模型，我们可以直接使用内置估计器的`transformer`方法。'
- en: '`instance_count`: The number of EC2 instances that we are going to use to run
    our batch transform job.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_count`：我们将用于运行批量转换作业的EC2实例数量。'
- en: '`instance_type`: The type of EC2 instances that we can use. A large variety
    of instances are available to be used for batch transform jobs. The choice of
    instance depends on our use case’s compute and memory requirements, as well as
    the data type and size.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_type`：我们可以使用的EC2实例类型。有多种实例可供批量转换作业使用。实例的选择取决于我们用例的计算和内存需求，以及数据类型和大小。'
- en: In addition, we can also specify several other parameters such as batch strategy,
    and output path. The complete list of parameters can be found on the documentation
    page in the *References* section. In the following example, we used SageMaker’s
    built-in XGBoost container. For running batch transformers for your own containers/models
    or frameworks, such as PyTorch and TensorFlow, the container images may vary.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以指定其他几个参数，例如批量策略和输出路径。完整的参数列表可以在*参考文献*部分的文档页面上找到。在以下示例中，我们使用了SageMaker的内置XGBoost容器。对于运行自己的容器/模型或框架（如PyTorch和TensorFlow）的批量转换器，容器镜像可能会有所不同。
- en: '*Figures 7.2* – *7.8* show an example of SageMaker’s XGBoost model being fit
    on our training data and then a transformer object being created for this training
    model. We specified that the transform job should run on one instance of type
    `ml.m4.xlarge` and should expect `text/csv` data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.2* – *7.8*展示了SageMaker的XGBoost模型在训练数据上拟合的示例，然后为这个训练模型创建了一个转换器对象。我们指定转换作业应在`ml.m4.xlarge`类型的一个实例上运行，并期望`text/csv`数据：'
- en: As shown in *Figure 7**.2*, we can specify the various packages and SageMaker
    parameters needed to run the code in the example.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如*图7.2*所示，我们可以指定运行示例中代码所需的各个包和SageMaker参数。
- en: '![Figure 7.2 – Setting up the packages and bucket in SageMaker and downloading
    the dataset to be used for model training and inference](img/B18493_07_002.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 在SageMaker中设置包和存储桶，并下载用于模型训练和推理的数据集](img/B18493_07_002.jpg)'
- en: Figure 7.2 – Setting up the packages and bucket in SageMaker and downloading
    the dataset to be used for model training and inference
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 在SageMaker中设置包和存储桶，并下载用于模型训练和推理的数据集
- en: As shown in *Figure 7**.3*, we can read the data and carry out one hot encoding
    on the categorical variables.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如*图7.3*所示，我们可以读取数据并对分类变量进行独热编码。
- en: '![Figure 7.3 – Doing one hot encoding on categorical variables and displaying
    the results](img/B18493_07_003.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 对分类变量进行独热编码并显示结果](img/B18493_07_003.jpg)'
- en: Figure 7.3 – Doing one hot encoding on categorical variables and displaying
    the results
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.4* shows the data being split into training, validation, and testing
    partitions to be used during the model training process.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Splitting the data into train, validation, and test sets for
    model training and testing](img/B18493_07_004.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Splitting the data into train, validation, and test sets for model
    training and testing
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7**.5*, we can rearrange the columns in the data table in
    the order that the machine learning model (XGBoost) expects it to be. Furthermore,
    we can also upload the training and validation data files to an S3 bucket for
    the model training step.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Reorganizing the data and uploading to S3 bucket for training](img/B18493_07_005.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Reorganizing the data and uploading to S3 bucket for training
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.6* shows that we are using SageMaker’s XGBoost container for training
    our model. It also specifies the data channels for training and validating the
    model.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Specifying the container for model training along with'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: training and validation data path in S3](img/B18493_07_006.jpg)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Specifying the container for model training along with training
    and validation data path in S3
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7.7*, we need to define the estimator, the instance type,
    and instance count, and set various hyperparameters needed by the XGBoost model.
    We will also start the training job by calling the `fit` method.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Defining the SageMaker estimator for training and setting up
    the hyperparameters](img/B18493_07_007.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Defining the SageMaker estimator for training and setting up the
    hyperparameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s create a batch transform job.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Creating a batch transform job for carrying out inference
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After creating the transformer object, we need to create a batch transform job.
    *Figure 7**.8* shows an example of starting a batch transform job using the `transform`
    method call of the batch transformer object. In this transform call, we will specify
    the location of data in Amazon S3 on which we want to carry out batch inference.
    In addition, we will also specify the content type of the data (`text/csv`, in
    this case), and how the records are split in the data file (each line containing
    one record, in this case).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Creating a transformer object, preparing data for batch inference,
    and starting the batch transform job](img/B18493_07_008.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Creating a transformer object, preparing data for batch inference,
    and starting the batch transform job
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.9* shows an example of reading the results from S3 and then plotting
    the results (actual versus predictions):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Creating a helper function for reading the results of batch
    transform, and plotting the results (actual versus predictions)](img/B18493_07_009.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Creating a helper function for reading the results of batch transform,
    and plotting the results (actual versus predictions)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.10* shows the resulting plot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.10 – Plot of actual versus prediction \uFEFFRings\uFEFF values](img/B18493_07_010.jpg)"
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Plot of actual versus prediction Rings values
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The code and steps discussed in this section outline the process of training
    a machine learning model and then carrying out batch inference on the data using
    the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a batch transform job
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker Batch Transform also gives us the option of optimizing the transform
    job using a few hyperparameters, as described here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`max_concurrent_transforms`: The maximum number of HTTP requests that can be
    made to each batch transform container at any given time. To get the best performance,
    this value should be equal to the number of compute workers that we are using
    to run our batch transform job.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_payload`: This value specifies the maximum size (in MB) of the payload
    in a single HTTP request sent to the batch transform for inference.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strategy`: This specifies the strategy whether we want to have just one record
    or multiple records in a batch.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Batch Transform is a very useful option to carry out inference on
    large datasets for use cases that do not require real-time latency and high throughput.
    The execution of SageMaker Batch Transform can be carried out by using an MLOps
    workflow, which can be triggered whenever there is new data on which inference
    needs to be carried out. Automated reports can then be generated using the batch
    transform job results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about deploying a real-time endpoint for making predictions
    on data using Amazon SageMaker’s real-time inference option.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Real-time inference
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier in this chapter, the need for real-time inference arises
    when we need results with very low latency. Several day-to-day use cases are examples
    of using real-time inference from machine learning models, such as face detection,
    fraud detection, defect and anomaly detection, and sentiment analysis in live
    chats. Real-time inference in Amazon SageMaker can be carried out by deploying
    our model to the SageMaker hosting services as a real-time endpoint. *Figure 7**.11*
    shows a typical SageMaker machine learning workflow of using a real-time endpoint.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Example architecture of a SageMaker real-time endpoint](img/B18493_07_011.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Example architecture of a SageMaker real-time endpoint
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, we first read our data from an Amazon S3 bucket. Data preprocessing
    and feature engineering are carried out on this data using Amazon SageMaker Processing.
    A machine learning model is then trained on the processed data, followed by results
    evaluation and post-processing (if any). After that, the model is deployed as
    a real-time endpoint for carrying out inference on new data in real time with
    low latency. Also shown in the figure is SageMaker Model Monitor, which is attached
    to the endpoint in order to detect data and concept drift on new data that is
    sent to the endpoint for inference. SageMaker also provides the option of registering
    our machine learning model with the SageMaker Model Registry, for various purposes,
    such as cataloging, versioning, and automating deployment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Hosting a machine learning model as a real-time endpoint
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker provides us with many options to host a model or multiple models
    as real-time endpoints. We can either use SageMaker Python SDK, the AWS SDK for
    Python (Boto3), the SageMaker console, or the AWS **command-line interface** (**CLI**)
    to host our models. Furthermore, these endpoints can also host a single model,
    multiple models in one container in a single endpoint, and multiple models using
    different containers in a single endpoint. In addition, a single endpoint can
    also host models containing preprocessing logic as a serial inference pipeline.
    We will go through these multiple options in the following subsections.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Single model
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in the preceding section, we can host a model endpoint using multiple
    options. Here, we will show you how to host an endpoint containing a single model
    using the Amazon SageMaker SDK. There are two steps involved in creating a single
    model endpoint using the Amazon SageMaker SDK, as described here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '`Model` class to create a model object that can be deployed as a HTTPS endpoint.
    We can also use the model trained in SageMaker. For example, the XGBoost model
    trained in the example shown in *Figure 7**.7*.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`deploy()` method to create an HTTPS endpoint. The `deploy` method requires
    the instance type as well as an initial instance count to deploy the model with.
    This is shown in *Figure 7**.12*:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Calling the deploy method of the XGBoost estimator we trained
    earlier to deploy our model on a single instance of type ml.m4.xlarge](img/B18493_07_012.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Calling the deploy method of the XGBoost estimator we trained
    earlier to deploy our model on a single instance of type ml.m4.xlarge
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.13* show a custom inference function to serialize the test data,
    and send it to the real-time endpoint for inference:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 7.13 – Serializing the data to be sent to the real-time endpoint.
    Also, creating a helper function to carry out inference using the endpoint](img/B18493_07_013.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Serializing the data to be sent to the real-time endpoint. Also,
    creating a helper function to carry out inference using the endpoint
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.14* displays the inference results for a few records along with
    the actual values of our target variable—`Rings`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Showing a few prediction results versus actual values (Rings)](img/B18493_07_014.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Showing a few prediction results versus actual values (Rings)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The endpoint will continue incurring costs as long as it is not deleted. Therefore,
    we should use real-time endpoints only when we have a use case in which we need
    inference results in real time. For use cases where we can do inference in batches,
    we should use SageMaker Batch Transform.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Multiple models
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For hosting multiple models in a single endpoint, we can use SageMaker multi-model
    endpoints. These endpoints can be used to also host multiple variants of the same
    model. Multi-model endpoints are a very cost-effective method of saving our inference
    cost for real-time endpoints since the endpoint utilization is generally more
    when we use multi-model endpoints. We can use business logic to decide on the
    model to use for inference.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'With multi-model endpoints, memory resources across models are also shared.
    This is very useful when our models are comparable in size and latency. If there
    are models that have significantly different latency requirements or transactions
    per second, then single model endpoints for the models are recommended. We can
    create multi-model endpoints using either the SageMaker console or the AWS SDK
    for Python (Boto3). We can follow similar steps as those for the creation of a
    single-model endpoint to create a multi-model endpoint, with a few differences.
    The steps for Boto3 are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: First, we need a container supporting multi-model endpoints deployment.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we need to create a model that uses this container using Boto3 SageMaker
    client.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For multi-model endpoints, we also need to create an endpoint configuration,
    specifying instance types and initial counts.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we need to create the endpoint using the `create_endpoint()`API call
    of the Boto3 SageMaker client.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While invoking a multi-model endpoint, we also need to pass a target model parameter
    to specify the model that we want to use for inference with the data in the request.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Multiple containers
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also deploy models that use different containers (such as different frameworks)
    as multi-container endpoints. These containers can be run individually or can
    also be run in a sequence as an inference pipeline. Multi-container endpoints
    also help improve endpoint utilization efficiency, hence cutting down on the cost
    associated with real-time endpoints. Multi-container endpoints can be created
    using Boto3\. The process to create a multi-container endpoint is very similar
    to creating multi-model and single-model endpoints. First, we need to create a
    model with multiple containers as a parameter, followed by creating an endpoint
    configuration, and finally creating the endpoint.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Inference pipelines
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also host real-time endpoints consisting of two to five containers as
    an inference pipeline behind a single endpoint. Each of these containers can be
    a pretrained SageMaker built-in algorithm, our custom algorithm, preprocessing
    code, predictions, or postprocessing code. All the containers in the inference
    pipeline function in a sequential manner. The first container processes the initial
    HTTP request. The response from the first container is sent as a request to the
    second container, and so on. The response from the final container is sent by
    SageMaker to the client. An inference pipeline can be considered as a single model
    that can be hosted behind a single endpoint or can also be used to run batch transform
    jobs. Since all the containers in an inference pipeline are running on the same
    EC2 instance, there is very low latency in communication between the containers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring deployed models
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After deploying a model into production, data scientists and machine learning
    engineers have to continuously check on the model’s quality. This is because with
    time, the model’s quality may drift and it may start to predict incorrectly. This
    may occur because of several reasons, such as a change in one or more variables’
    distribution in the dataset, the introduction of bias in the dataset with time,
    or some other unknown process or parameter being changed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, data scientists often run their analysis every few weeks to check
    if there has been any change in the data or model quality, and if there is, they
    go through the entire process of feature engineering, model training, and deployment
    again. With SageMaker Model Monitor, these steps can be automated. We can set
    up alarms to detect if there is any drift in data quality, model quality, bias
    drift in the model and feature distribution drift, and then take corrective actions
    such as fixing quality issues and retraining models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.15* shows an example of using SageMaker Model Monitor with an endpoint
    (real-time or asynchronous):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – SageMaker Model Monitor workflow for a deployed endpoint (real-time
    or asynchronous)](img/B18493_07_015.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – SageMaker Model Monitor workflow for a deployed endpoint (real-time
    or asynchronous)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to enable Model Monitor on our endpoint, either at the time of
    creation or later. In addition, we also need to run a baseline processing job.
    This processing job analyzes the data and creates statistics and constraints for
    the baseline data (generally the same dataset with which the model has been trained
    or validated on). We also need to enable data capture on our SageMaker endpoint
    to be able to capture the new data along with inference results. Another processing
    job is then run on fixed intervals to create new statistics and constraints, compares
    them with the baseline statistics and constraints, and then configures alarms
    using Amazon CloudWatch metrics in case there is drift in any of the metrics we
    are analyzing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: In case of a violation in any of the metrics, alarms are generated and these
    can be sent to a data scientist or machine learning engineer for further analysis
    and model retraining if needed. Alternatively, we can also use these alarms to
    trigger a workflow (MLOps) to retrain our machine learning model. Model Monitor
    is a very valuable tool for data scientists. It can cut down on tedious manual
    processes for detecting bias and drift in data and model as time progresses after
    deploying a model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous inference
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker real-time endpoints are suitable for machine learning use cases that
    have very low latency inference requirements (up to 60 seconds), along with the
    data size for inference not being large (maximum 6 MB). On the other hand, batch
    transforms are suitable for offline inference on very large datasets. Asynchronous
    inference is another relatively new inference option in SageMaker that can process
    data up to 1 GB and can take up to 15 minutes in processing inference requests.
    Hence, they are useful for use cases that do not have very low latency inference
    requirements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous endpoints have several similarities to real-time endpoints. To
    create asynchronous endpoints, like with real-time endpoints, we need to carry
    out the following steps:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Create a model.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an endpoint configuration for the asynchronous endpoint. There are some
    additional parameters for asynchronous endpoints.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the asynchronous endpoint.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Asynchronous endpoints also have differences when compared to real-time endpoints,
    as outlined here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: One main difference from real-time endpoints is that we can scale endpoint instances
    down to zero when there are no inference requests. This can cut down on the costs
    associated with having an endpoint for inference.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another key difference compared to a real-time endpoint is that instead of passing
    the payload in line with the request for inference, we upload the data to an Amazon
    S3 location, and pass on the S3 URI along with the request. Internally, SageMaker
    keeps a queue of these requests and processes them in the order that they were
    received. Just like real-time endpoints, we can also do monitoring on the asynchronous
    endpoint in order to detect model and data drift, as well as new bias.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code to show a SageMaker asynchronous endpoint is shown in *Figure 7**.16*,
    using the same model that we created in the batch transform example.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Creating an asynchronous endpoint configuration, followed by
    the creation of the asynchronous endpoint](img/B18493_07_016.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Creating an asynchronous endpoint configuration, followed by the
    creation of the asynchronous endpoint
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.17* shows a sample of results for carrying out asynchronous inference
    on Abalone data used for the batch transform and real-time endpoints examples
    in previous sections.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Serializing the inference request and calling the asynchronous
    endpoint to carry out inference on the data.](img/B18493_07_017.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Serializing the inference request and calling the asynchronous
    endpoint to carry out inference on the data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.18* shows the actual and predicted results for a few data points.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.18 – Showing a few predicted results versus the actual values (\uFEFF\
    Rings\uFEFF) using the asynchronous endpoint](img/B18493_07_018.jpg)"
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Showing a few predicted results versus the actual values (Rings)
    using the asynchronous endpoint
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will look into the high availability and fault
    tolerance capabilities of SageMaker endpoints.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The high availability of model endpoints
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker provides fault tolerance and high availability of the deployed
    endpoints. In this section, we will discuss various features and options of AWS
    cloud infrastructure and Amazon SageMaker, that we can use to ensure that our
    endpoints are fault-tolerant, resilient, and highly available.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Deployment on multiple instances
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker gives us the option of deploying our endpoints on multiple instances.
    This protects from instance failures. If one instance goes down, then other instances
    can still serve the inference requests. In addition, if our endpoints are deployed
    on multiple instances and an availability zone outage occurs or an instance fails,
    SageMaker automatically tries to distribute our instances across different availability
    zones, thereby improving the resiliency of our endpoints. It is also a good practice
    to deploy our endpoints using small instance types spread across different availability
    zones.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints autoscaling
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oftentimes, we design our online applications for peak load and traffic. This
    is also true for machine learning-based applications, where we need hosted endpoints
    to carry out inference in real time or near real time. In such a scenario, we
    generally deploy models using the maximum number of instances in order to serve
    the peak workload and traffic.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t do this, then our application may start timing out when there are
    more inference requests than the instances can handle in a combined fashion. This
    approach results in either the wastage of compute resources or interruptions in
    the service of our machine learning applications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this kind of scenario, Amazon SageMaker lets us configure our endpoints
    with an autoscaling policy. Using the autoscaling policy, the number of instances
    on which our endpoint is deployed increases as traffic increases, and decreases
    as traffic decreases. This helps not only with the high availability of our inference
    endpoints but also helps in reducing the cost significantly. We can enable autoscaling
    for a model using either the SageMaker console, the AWS CLI, or the Application
    Auto Scaling API.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply autoscaling, we need an autoscaling policy. The autoscaling
    policy uses Amazon CloudWatch metrics and target values assigned by us to decide
    when to scale the instances up or down. We also need to define the minimum and
    maximum number of instances that the endpoint can be deployed on. Other components
    of the autoscaling policy include the required permissions to carry out autoscaling,
    a service-linked IAM role, and a cool-down period to wait after a scaling activity
    before starting the next scaling activity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*Figures 7.19* – *7.22* show autoscaling being configured on an asynchronous
    endpoint using the Amazon SageMaker console:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.19* shows that initially the endpoint is just deployed on a single
    instance:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.19 – The endpoint run time settings showing the endpoint being deployed
    on an instance and autoscaling not being used](img/B18493_07_019.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – The endpoint run time settings showing the endpoint being deployed
    on an instance and autoscaling not being used
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on the **Configure auto scaling** button, your screen should
    look like those shown in *Figure 7**.20* and *Figure 7**.21*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: As seen in *Figure 7**.20*, we need to update the minimum and maximum instance
    counts to `2` and `10`, respectively.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Configuring autoscaling on the endpoint to use 2 – 10 instances,
    depending on traffic (using SageMaker console).](img/B18493_07_020.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Configuring autoscaling on the endpoint to use 2 – 10 instances,
    depending on traffic (using SageMaker console).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: As seen in *Figure 7**.21*, we need to update the value for the `200`.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Setting the scale in and out time along with the target metric
    threshold to trigger autoscaling](img/B18493_07_021.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Setting the scale in and out time along with the target metric
    threshold to trigger autoscaling
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Once this target value is hit and we are out of the cool-down period, SageMaker
    will automatically start a new instance or shut down the instance regardless of
    whether we are above or below the target value.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.22* shows the results of the endpoint being updated with our autoscaling
    policy.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.22 – The endpoint running with the autoscaling policy applied](img/B18493_07_022.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – The endpoint running with the autoscaling policy applied
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The autoscaling policy can also be updated or deleted after being applied.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Endpoint modification without disruption
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also modify deployed endpoints without affecting the availability of
    the models deployed in production. In addition to applying an autoscaling policy
    as discussed in the previous section, we can also update compute instance configurations
    of the endpoints. We can also add new model variants and change the traffic patterns
    between different model variants. All these tasks can be achieved without negatively
    affecting the endpoints deployed in production.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed how we can ensure that our endpoints are highly available,
    let’s discuss Blue/green deployments next.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a production environment where our models are running to make inferences
    in real time or near real time, it is very important that when we need to update
    our endpoints that it can happen without any disruption or problems. Amazon SageMaker
    automatically uses blue/green deployment methodology whenever we update our endpoints.
    In this kind of scenario, a new fleet, called the green fleet, is provisioned
    with our updated endpoints. The workload is then shifted from the old fleet, called
    the blue fleet, to the green fleet. After an evaluation period to make sure that
    everything is running without any issues, the blue fleet is terminated. SageMaker
    also provides the following three different traffic-shifted modes for blue/green
    deployment, allowing us to have more control over the traffic-shifting patterns.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: All at once
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this traffic-shifting mode, all of the traffic is shifted at once from the
    blue fleet to the green fleet. The blue (old) fleet is kept in service for a period
    of time (called the baking period) to make sure everything is working fine, and
    performance and functionality are as expected. After the baking period, the blue
    fleet is terminated all at once. This type of blue/green deployment minimizes
    the overall update duration, while also minimizing the cost. One disadvantage
    of the all-at-once method is that regressive updates affect all of the traffic
    since the entire traffic is shifted to the green fleet at once.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Canary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In canary blue/green deployment, a small portion of the traffic is first shifted
    from the blue fleet to the green fleet. This portion of the green fleet that starts
    serving a portion of the traffic is called the canary, and it should be less than
    50% of the new fleet’s capacity. If everything works fine during the baking period
    and no CloudWatch alarms are triggered, the rest of the traffic is also shifted
    to the green fleet, after which the blue fleet is terminated. If any alarms go
    off during the baking period, SageMaker rolls back all the traffic to the blue
    fleet.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: An advantage of using canary blue/green deployment is that it confines the blast
    radius of regressive updates only to the canary fleet and not to the whole fleet,
    unlike all-at-once blue/green deployment. A disadvantage of using canary deployment
    is that both the blue and the green fleets are operational for the entire deployment
    period, thus adding to the cost.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In linear blue/green deployment, traffic is shifted from the blue fleet to the
    green fleet in a fixed number of pre-specified steps. In the beginning, the first
    portion of traffic is shifted to the green fleet, and the same portion of the
    blue fleet is deactivated. If no alarms go off during the baking period, SageMaker
    initiates the shifting of the second portion, and so on. If at any point an alarm
    goes off, SageMaker rolls back all the traffic to the blue fleet. Since traffic
    is shifted over to the green fleet in several steps, linear blue/green deployment
    reduces the risk of regressive updates significantly. The cost of the linear deployment
    method is proportional to the number of steps configured to shift the traffic
    from the blue fleet to the green fleet.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in this section, SageMaker has these blue/green deployment methods
    to make sure that there are safety guardrails when we are updating our endpoints.
    These blue/green deployment methods ensure that we are able to update our inference
    endpoints with no or minimal disruption to our deployed machine learning models
    in a production environment.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now summarize what we’ve covered in this chapter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the various managed deployment methods available
    when using Amazon SageMaker. We talked about the suitability of the different
    deployment/inference methods for different use case types. We showed examples
    of how we can do batch inference and deploy real-time and asynchronous endpoints.
    We also discussed how SageMaker can be configured to automatically scale both
    up and down, and how SageMaker ensures that in case of an outage, our endpoints
    are deployed to multiple availability zones. We also touched upon the various
    blue/green deployment methodologies available with Amazon SageMaker, in order
    to update our endpoints in production.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: In a lot of real-world scenarios, we do not have high-performance clusters of
    instances available for carrying out inference on new and unseen data in real
    time. For such applications, we need to use edge computing devices. These devices
    often have limitations on compute power, memory, connectivity, and bandwidth,
    and need the models to be optimized to be able to use on these edge devices.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend this discussion to learn about using machine
    learning models on edge devices.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following resources for more information:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosting multiple models on a single endpoint: [https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon EC2 Inf1 instances: [https://aws.amazon.com/ec2/instance-types/inf1/](https://aws.amazon.com/ec2/instance-types/inf1/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using your own inference code with SageMaker Batch Transform: [https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple models with different containers behind a single endpoint: [https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serverless inference on Amazon SageMaker: [https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blue/green deployments using Amazon SageMaker: [https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Canary traffic shifting: [https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker’s Transformer class: [https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer](https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon SageMaker real-time inference: [https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Best practices for hosting models using Amazon SageMaker: [https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
