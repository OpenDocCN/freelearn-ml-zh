<html><head></head><body><div><div><h1 class="header-title">Working with PyCUDA</h1>
                
            
            
                
<p>In the last chapter, we saw the procedure to install PyCUDA for Windows and Linux operating systems. In this chapter, we will start by developing the first PyCUDA program that displays a string on the console. It is very important to know and access the device properties of the GPU on which PyCUDA is running; the method for doing this will be discussed in detail in this chapter. We will also look at the execution of threads and blocks for a kernel in PyCUDA. The important programming concepts for any CUDA programming, such as allocating and deallocating the memory on the device, transferring data from host to device and vice versa, and the kernel call will be discussed in detail, using an example of the vector addition program. The method to measure the performance of  PyCUDA programs using CUDA events and to compare it with the CPU program will also be discussed. These programming concepts will be used to develop some complex PyCUDA programs, such as the squaring of elements in an array and matrix multiplication. The last part of the chapter describes some advanced methods to define kernel functions in PyCUDA.   </p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Writing the first "Hello, PyCUDA!" program in PyCUDA</li>
<li>Accessing device properties from a PyCUDA program</li>
<li>Thread and block execution in PyCUDA</li>
<li>Basic PyCUDA programming concepts using a vector addition program</li>
<li>Measuring the performance of PyCUDA programs using CUDA events</li>
<li>Some complex programs in PyCUDA</li>
<li>Advanced kernel functions in PyCUDA</li>
</ul>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter requires a good understanding of the Python programming language. It also requires any computer or laptop with the Nvidia GPU onboard. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. </p>
<p>Check out the following video to see the code in action:<br/>
<a href="http://bit.ly/2QPWojV">http://bit.ly/2QPWojV</a></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Writing the first program in PyCUDA </h1>
                
            
            
                
<p>This section describes the procedure for writing a simple "Hello, PyCUDA!" program using PyCUDA. It will demonstrate the workflow for writing any PyCUDA programs. As Python is an interpreted language, the code can also be run line by line from the Python terminal, or it can be saved with the <kbd>.py</kbd> extension and executed as a file.</p>
<p>The program for displaying a simple string from the kernel using PyCUDA is shown as follows:</p>
<pre>import pycuda.driver as drv<br/>import pycuda.autoinit<br/>from pycuda.compiler import SourceModule<br/><br/>mod = SourceModule("""<br/>  #include &lt;stdio.h&gt;<br/><br/>  __global__ void myfirst_kernel()<br/>  {<br/>    printf("Hello,PyCUDA!!!");<br/>  }<br/>""")<br/> <br/>function = mod.get_function("myfirst_kernel")<br/>function(block=(1,1,1))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The first step while developing PyCUDA code is to include all libraries needed for the code. The <kbd>import</kbd> directive is used to include a library, module, class, or function in a file. This is similar to including a directive in C or C++, and it can be done in three different ways, as shown in the following steps. The use of three imported modules is also shown as follows:</p>
<ol>
<li>Import <kbd>pycuda.driver</kbd> as <kbd>drv</kbd><br/>
This indicates that the driver submodule of the pymodule is imported and it is given a short notation <kbd>drv</kbd> , so wherever functions from the <kbd>pycuda.driver</kbd> module are to be used then they can be used as <kbd>drv.functionname</kbd>. This module contains memory management functions, device properties, data direction functions, and so on.</li>
<li>Import <kbd>pycuda.autoinit</kbd><br/>
This command indicates the <kbd>autoint</kbd> module from <kbd>pycuda</kbd> is imported. It is not given any shorthand notation. The <kbd>autoint</kbd> module is used for device initialization, context creation, and memory cleanup. This module is not mandatory, and all the above functions can also be done manually.</li>
<li>From <kbd>pycuda.compiler</kbd> import <kbd>SourceModule</kbd><br/>
This command indicates that only the <kbd>SourceModule</kbd> class from the <kbd>pycuda.compiler</kbd> module is imported. This is important when you only want to use one class of a large module. The <kbd>SourceModule</kbd> class is used to define C-like kernel functions in PyCUDA.</li>
</ol>
<p>The C or C++ kernel code is fed as a constructor to the <kbd>Sourcemodule</kbd> class and the mod object is created. The kernel code is very simple as it is just printing a  <kbd>Hello, PyCUDA!</kbd> string on the console. As the <kbd>printf</kbd> function is used inside kernel code, it is very important to include the <kbd>stdio.h</kbd> header file. The <kbd>myfirst_kernel</kbd> function is defined inside the kernel code using the <kbd>__global__</kbd> directive to indicate that the function will be executed on the GPU. The function does not take any arguments. It just prints a string on the console. This kernel function will be compiled by the <kbd>nvcc</kbd> compiler.</p>
<p>This function can be used inside Python code by creating a pointer to the function using the <kbd>get_function</kbd> method of the <kbd>mod</kbd> object. The name of the kernel function is given as arguments in quotes. The pointer variable can be given any name. This pointer variable is used to call the kernel in the last line of the code. The arguments to the kernel function can be specified here, but the <kbd>myfirst_kernel</kbd> function does not have any arguments, so no arguments are specified. The number of threads per block and blocks per grid to be launched for a kernel can also be provided as an argument by using optional block and grid arguments. The block argument is given a value of (1,1,1) which is a 1 x 3 Python tuple, which indicates a block size of 1 x 1 x 1. So one thread will be launched that will print the string on the console.</p>
<p class="mce-root"/>
<p>The output of the program is shown as follows:   </p>
<div><img class="alignnone size-full wp-image-667 image-border" src="img/c5ea51cf-3057-4c87-8f71-b868bd39c37e.png" style="" width="838" height="56"/></div>
<p>To summarize, this section demonstrated the procedure to develop a simple PyCUDA program step by step. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">A kernel call</h1>
                
            
            
                
<p>The device code that is written using ANSI C keywords along with CUDA extension keywords is called a <strong>kernel</strong>. It is launched from a Python code by a method called  <strong>Kernel Call</strong>. Basically, the meaning of a kernel call is that we are launching a device code from the host code. A kernel call typically generates a large number of blocks and threads to exploit data parallelism on a GPU.  Kernel code is very similar to that of normal C functions; it is just that this code is executed by several threads in parallel. It has a very simple syntax in Python, which can be shown as follows:</p>
<pre>kernel (parameters for kernel,block=(tx,ty,tz) , grid=(bx,by,bz))</pre>
<p>It starts with the pointer of the kernel function that we want to launch. You should make sure that this kernel pointer is created using the <kbd>get_function</kbd> method. Then, it can include parameters of the kernel function separated by a comma. The block parameter indicates the number of threads to be launched, and the grid parameter indicates the number of blocks in the grid. The block and grid parameters are specified using a 1 x 3 Python tuple, which indicates blocks and threads in three dimensions. The total number of threads started by a kernel launch will be the multiplication of these numbers. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Accessing GPU device properties from PyCUDA program</h1>
                
            
            
                
<p class="mce-root">PyCUDA provides a simple API to find information such as, which CUDA-enabled GPU devices (if any) are present and which capabilities each device supports. It is important to find out the properties of a GPU device that is being used before writing PyCUDA programs so that the optimal resources of the device can be used. </p>
<p class="mce-root"/>
<p class="mce-root">The program for displaying all properties of CUDA-enabled devices on a system by using PyCUDA is shown as follows:</p>
<pre>import pycuda.driver as drv<br/>import pycuda.autoinit<br/>drv.init()<br/>print("%d device(s) found." % drv.Device.count())<br/>for i in range(drv.Device.count()):<br/>  dev = drv.Device(i)<br/>  print("Device #%d: %s" % (i, dev.name()))<br/>  print(" Compute Capability: %d.%d" % dev.compute_capability())<br/>  print(" Total Memory: %s GB" % (dev.total_memory()//(1024*1024*1024)))<br/>    <br/>  attributes = [(str(prop), value) <br/>    for prop, value in list(dev.get_attributes().items())]<br/>    attributes.sort()<br/>    n=0<br/>    <br/>    for prop, value in attributes:<br/>      print(" %s: %s " % (prop, value),end=" ")<br/>      n = n+1<br/>      if(n%2 == 0):<br/>        print(" ")</pre>
<p class="mce-root">First, it is important to get a count of how many CUDA-enabled devices are present on the system, as a system may contain more than one GPU-enabled device. This count can be determined by the <kbd>drv.Device.count()</kbd> function of a driver class in PyCUDA. All the devices present on a system are iterated to determine the properties of each device. A pointer object to each device is created using the <kbd>drv.Device</kbd> function. This pointer is used to determine all properties of a particular device. </p>
<p class="mce-root">The <kbd>name</kbd> function will give the name of a particular device and <kbd>total_memory</kbd> will give the size of the GPU global memory available on the device. The other properties are stored as a Python dictionary that can be fetched by the <kbd>get_attributes().items()</kbd> function. This is converted to a list of tuples by using list comprehension in Python. All the rows of this list contain the 2 x 1 tuple, which has the name of the property and its value.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This list is iterated using the <kbd>for</kbd> loop to display all the properties on the console. This program is executed on the laptop with a GeForce 940 GPU and CUDA 9. The output of the program is as shown:</p>
<div><img class="alignnone size-full wp-image-668 image-border" src="img/f52a087b-5ec6-4407-9326-48effac47ab0.png" style="" width="1366" height="768"/></div>
<p>The properties were discussed in detail in earlier chapters of the book, so we won't discuss them again; however, to summarize, this section demonstrated the method to access GPU device properties from a PyCUDA program. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Thread and block execution in PyCUDA</h1>
                
            
            
                
<p>We saw in the <em>A k</em><em>ernel call</em> section that we can start multiple blocks and multiple threads in parallel. So, in which order do these blocks and threads start and finish their execution? It is important to know this if we want to use the output of one thread in other threads. To understand this, we have modified the kernel in the <kbd>hello,PyCUDA!</kbd> program, seen in the earlier section, by including a print statement in a kernel call, which prints the block number. The modified code is shown as follows:</p>
<pre><br/>import pycuda.driver as drv<br/>import pycuda.autoinit<br/>from pycuda.compiler import SourceModule<br/><br/>mod = SourceModule("""<br/>  #include &lt;stdio.h&gt;<br/>  __global__ void myfirst_kernel()<br/>  {<br/>    printf("I am in block no: %d \\n", blockIdx.x);<br/>  }<br/>""")<br/> <br/>function = mod.get_function("myfirst_kernel")<br/>function(grid=(4,1),block=(1,1,1))</pre>
<p>As can be seen from the code, we are launching a kernel with 10 blocks in parallel, with each block having a single thread. In the kernel code, we are printing the block ID of the kernel execution. We can think of that as 10 copies of the same <kbd>myfirstkernel</kbd> start execution in parallel. Each of these copies will have a unique block ID, which can be accessed by the <kbd>blockIdx.x</kbd> directive, and unique thread ID, which can be accessed by <kbd>threadIdx.x</kbd>. These IDs will tell us which block and thread are executing the kernel. When you run the program many times, you will find that each time, blocks execute in different orders. One sample output can be shown as follows:  </p>
<div><img class="alignnone size-full wp-image-669 image-border" src="img/2aa20d82-b407-4168-8d84-f97a577bd75a.png" style="" width="868" height="226"/></div>
<p>It can produce <em>n</em> factorial number of different outputs, where <em>n</em> indicates the number of blocks started in parallel. So, whenever you are writing the program in PyCUDA, you should be careful that blocks execute in random order.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Basic programming concepts in PyCUDA </h1>
                
            
            
                
<p>We will start developing some useful stuff using PyCUDA in this section. The section will also demonstrate some useful functions and directives of PyCUDA, using a simple example of adding two numbers.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Adding two numbers in PyCUDA </h1>
                
            
            
                
<p class="mce-root">Python provides a very fast library for numerical operations which is called <strong>numpy (Numeric Python)</strong>. It is developed in C or C++ and is very useful for array manipulations in Python. It is used frequently in PyCUDA programs as arguments to PyCUDA kernel functions are passed as numpy arrays. This section explains how to add two numbers using PyCUDA. The basic kernel code for adding two numbers is shown as follows:</p>
<pre>import pycuda.autoinit<br/>import pycuda.driver as drv<br/>import numpy<br/>from pycuda.compiler import SourceModule<br/>mod = SourceModule("""<br/>                   <br/>  __global__ void add_num(float *d_result, float *d_a, float *d_b)<br/>  {<br/>     const int i = threadIdx.x; <br/>     d_result[i] = d_a[i] + d_b[i];<br/>  }<br/>""")<br/><br/></pre>
<p>The <kbd>SourceModule</kbd> class and driver class are imported as explained earlier. The <kbd>numpy</kbd> library is also imported as it will be required for passing arguments to the kernel code. The  <kbd>add_num</kbd> kernel function is defined as a constructor to the <kbd>SourceModule</kbd> class. The function takes two device pointers as input and one device pointer that points to the answer of the addition as output. It is important to note that, though we are adding two numbers, the kernel function is defined so that it can work on two array additions as well. Two single numbers are nothing but two arrays with one element each. If there arenâ€™t any errors, this code will be compiled and loaded onto the device. The code to call this kernel code from Python is shown as follows: </p>
<pre><br/>add_num = mod.get_function("add_num")<br/><br/>h_a = numpy.random.randn(1).astype(numpy.float32)<br/>h_b = numpy.random.randn(1).astype(numpy.float32)<br/><br/>h_result = numpy.zeros_like(h_a)<br/>d_a = drv.mem_alloc(h_a.nbytes)<br/>d_b = drv.mem_alloc(h_b.nbytes)<br/>d_result = drv.mem_alloc(h_result.nbytes)<br/>drv.memcpy_htod(d_a,h_a)<br/>drv.memcpy_htod(d_b,h_b)<br/><br/>add_num(<br/>  d_result, d_a, d_b,<br/>  block=(1,1,1), grid=(1,1))<br/>drv.memcpy_dtoh(h_result,d_result)<br/>print("Addition on GPU:")<br/>print(h_a[0],"+", h_b[0] , "=" , h_result[0])</pre>
<p>The pointer reference to the kernel function is created using <kbd>get_function</kbd>. Two random numbers are created using the <kbd>numpy.random.randn(1)</kbd> function, which is used to create a random number in the normal distribution. These numbers are converted to single precision floating point numbers, using the <kbd>astype(numpy.float32)</kbd> method. The numpy array to store the result on the host is initialized to zero.</p>
<p>The memory on the device can be allocated using the <kbd>mem_alloc</kbd> function of a driver class in PyCUDA. The size of memory is passed as an argument to the function. The size for input is found using the <kbd>h_a.nbytes </kbd>function. PyCUDA provides a <kbd>memcpy</kbd> function in the driver class to copy data from host memory to the device memory and vice versa. </p>
<p>The <kbd>drv.memcpy_htod</kbd> function copies data from the host memory to the device memory. The pointer to the device memory is passed as the first argument and the host memory pointer is passed as the second argument. The <kbd>add_num</kbd> kernel is called by passing device pointers as arguments along with numbers that specify the number of blocks and threads to be launched. In the code given before, one block is launched with one thread. The result computed by the kernel is copied back to the host by using the <kbd>drv.memcpy_dtoh</kbd> function. The result is displayed on the console, which is shown as follows:</p>
<div><img class="alignnone size-full wp-image-670 image-border" src="img/9455fd86-f37e-406b-a16f-e7e86be45476.png" style="" width="581" height="75"/></div>
<p>To summarize, this section demonstrated the structure of a PyCUDA program. It started with a kernel definition code. Then inputs are defined in Python. The memory is allocated on the device and inputs are transferred to the device memory. This is followed by a kernel call, which will compute the result. This result is transferred to the host for further processing. PyCUDA provides even simpler APIs to do this operation, which is explained in the next section.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Simplifying the addition program using driver class</h1>
                
            
            
                
<p>PyCUDA provides an even simpler API for kernel calling that does not require memory allocation and memory copying. It is done implicitly by the API. This can be accomplished by using the <kbd>In</kbd> and <kbd>Out</kbd> functions of the driver class in PyCUDA. The modified array addition code is shown as follows:</p>
<pre>import pycuda.autoinit<br/>import pycuda.driver as drv<br/>import numpy<br/>N = 10<br/>from pycuda.compiler import SourceModule<br/>mod = SourceModule("""<br/>                   <br/>  __global__ void add_num(float *d_result, float *d_a, float *d_b)<br/> {<br/>    const int i = threadIdx.x; <br/>    d_result[i] = d_a[i] + d_b[i];<br/> }<br/>""")<br/>add_num = mod.get_function("add_num")<br/>h_a = numpy.random.randn(N).astype(numpy.float32)<br/>h_b = numpy.random.randn(N).astype(numpy.float32)<br/>h_result = numpy.zeros_like(h_a)<br/>add_num(<br/>  drv.Out(h_result), drv.In(h_a), drv.In(h_b),<br/>  block=(N,1,1), grid=(1,1))<br/>print("Addition on GPU:")<br/>for i in range(0,N):<br/>  print(h_a[i],"+", h_b[i] , "=" , h_result[i])</pre>
<p>Ten elements of an array are added instead of single elements in the preceding code. The kernel function is exactly the same as the code seen previously. Two arrays of ten random numbers are created on the host. Now instead of creating the memory of them and transferring that to the device, the kernel is called directly. The kernel call is modified by specifying the direction of data using <kbd>drv.Out</kbd> or <kbd>drv.In</kbd>.  It simplifies the PyCUDA code and reduces the size of the code.</p>
<p>The kernel is called with one block and <em>N</em> threads per block. These <em>N</em> threads add <em>N</em> elements of the array in parallel, which accelerates the addition operation. The result of the kernel is automatically downloaded to the host memory by using the <kbd>drv.out</kbd> directive so this result is directly printed on the console using the <kbd>for</kbd> loop. The result for an addition of ten elements using PyCUDA is shown as follows:</p>
<p class="mce-root"/>
<div><img class="alignnone size-full wp-image-671 image-border" src="img/82f501bd-24c0-4e17-84e7-bec4ddd25f74.png" style="" width="277" height="172"/></div>
<p>To summarize, this section described the important concepts and functions of PyCUDA by taking a simple array addition program. The performance improvement of using PyCUDA can be quantified using CUDA events, which are explained in the next section.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Measuring performance of PyCUDA programs using CUDA events</h1>
                
            
            
                
<p>So far, we have not determined the performance of the PyCUDA programs explicitly. In this section, we will see how to measure the performance of the programs using CUDA events. This is a very important concept in PyCUDA because it will allow you to choose the best performing algorithms for a particular application from many options. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">CUDA events</h1>
                
            
            
                
<p>We can use Python time measuring options for measuring the performance of CUDA programs, but it will not give accurate results. It will include the time overhead of thread latency in the OS and scheduling in the OS among many other factors. The time measured using the CPU will also depend on the availability of a high-precision CPU timer. Many times, the host is performing asynchronous computations while the GPU kernel is running, and hence CPU timers of Python may not give correct times for kernel executions. So, to measure the time for GPU kernel computations, PyCUDA provides an event API.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>A CUDA event is a GPU timestamp recorded at a specified point in a PyCUDA program. In this API, the GPU records the timestamp, which eliminates the issues that were present when using CPU timers for measuring performance. There are two steps to measure time using CUDA events: creating an event and recording an event. We can record two events, one at the start of our code and one at the end. Then we will try to calculate the difference in time between the two events that will give an overall performance for our code. </p>
<p>In PyCUDA code, the following lines can be included to measure performance using a CUDA event API:</p>
<pre>import pycuda.driver as drv<br/>start = drv.Event()<br/>end=drv.Event()<br/>#Start Time<br/>start.record()<br/>#The kernel code for which time is to be measured<br/>#End Time<br/>end.record()<br/>end.synchronize()<br/>#Measure time difference<br/>secs = start.time_till(end)*1e-3</pre>
<p>The <kbd>record</kbd> method is used to measure a current timestamp. The timestamp is measured before and after the kernel code to measure time for the kernel execution. The difference between timestamps can be measured using the <kbd>time_till</kbd> method, as shown in the preceding code. It will give time in milliseconds, which is converted to seconds. In the next section, we will try to measure the performance of code using  a CUDA event. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Measuring performance of PyCUDA using large array addition   </h1>
                
            
            
                
<p>This section will demonstrate the method to use CUDA events to measure the performance of PyCUDA programs. The comparison of the performance of PyCUDA code with simple Python code is also described. The arrays with a million elements are taken so that performance can be accurately compared. The kernel code for large array addition is shown as follows:</p>
<pre>import pycuda.autoinit<br/>import pycuda.driver as drv<br/>import numpy<br/>import time<br/>import math<br/><br/>from pycuda.compiler import SourceModule<br/>N = 1000000<br/>mod = SourceModule("""<br/>                   <br/>__global__ void add_num(float *d_result, float *d_a, float *d_b,int N)<br/>{<br/> int tid = threadIdx.x + blockIdx.x * blockDim.x; <br/>  while (tid &lt; N)<br/>  {<br/>    d_result[tid] = d_a[tid] + d_b[tid];<br/>    tid = tid + blockDim.x * gridDim.x;<br/>  }<br/>}<br/>""")</pre>
<p>As the number of elements is high, multiple blocks and threads are launched. So, both the thread ID and block ID are used to calculate the thread index. If the total number of threads launched is not equal to the number of elements, then multiple elements are added by the same thread. This is done by the <kbd>while</kbd> loop inside the kernel function. It will also ensure that the thread index does not go beyond the array elements. Apart from the input array and the output array, the size of an array is also taken as a parameter for the kernel function, as Python global variables are not accessible to kernel code in <kbd>SourceModule</kbd>. The Python code for adding large arrays is shown as follows:</p>
<pre>start = drv.Event()end=drv.Event()<br/>add_num = mod.get_function("add_num")<br/><br/>h_a = numpy.random.randn(N).astype(numpy.float32)<br/>h_b = numpy.random.randn(N).astype(numpy.float32)<br/><br/>h_result = numpy.zeros_like(h_a)<br/>h_result1 = numpy.zeros_like(h_a)<br/>n_blocks = math.ceil((N/1024))<br/>start.record()<br/>add_num(<br/>  drv.Out(h_result), drv.In(h_a), drv.In(h_b),numpy.uint32(N),<br/>  block=(1024,1,1), grid=(n_blocks,1))<br/>end.record()<br/>end.synchronize()<br/>secs = start.time_till(end)*1e-3<br/>print("Addition of %d element of GPU"%N)<br/>print("%fs" % (secs))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Two events, <kbd>start</kbd> and <kbd>stop</kbd> are created to measure timings for the GPU code.The <kbd>Event()</kbd> function from the driver class is used to define event objects. Then, the pointer reference to the kernel function is created using the <kbd>get_function</kbd>. Two arrays with a million elements each are initialized with random numbers using the <kbd>randn</kbd> function of the <kbd>numpy</kbd> library. It will generate floating point numbers so they are converted to the single precision number to speed up computation on the device. </p>
<p>Each block supports 1,024 threads as we saw in the device property section. So based on that, the total number of blocks is calculated by dividing <em>N</em> by 1,024. It can be a float value so it is converted to next highest integer value using the <kbd>ceil</kbd> function of the <kbd>numpy</kbd> library. Then the kernel is launched with the calculated value number of blocks and 1,024 threads per block. The size of the array is passed with the <kbd>numpy.uint32</kbd> datatype.  </p>
<p>The time is recorded before and after calling the kernel function using the record function, and the time difference is calculated to measure the timing of the kernel function. The calculated time is printed on the console. To compare this performance with CPU timings, the following code is added to the program:</p>
<pre>start = time.time()<br/>for i in range(0,N):<br/>    h_result1[i] = h_a[i] +h_b[i]<br/>end = time.time()<br/>print("Addition of %d element of CPU"%N)<br/>print(end-start,"s")</pre>
<p>The time library from Python is used to measure CPU timings. The <kbd>for</kbd> loop is used to iterate through every element in an array. (Note: you can also use <kbd>h_result1 = h_a + h_b</kbd> as both arrays are numpy arrays.) Time is measured before and after the <kbd>for</kbd> loop using the <kbd>time.time()</kbd> function and the difference between this time is printed on the console. The output from the program is shown as follows:</p>
<div><img class="alignnone size-full wp-image-672 image-border" src="img/dbdc8dab-65bd-46de-bd99-aabacaede9b0.png" style="" width="620" height="108"/></div>
<p>As can be seen from the output, GPU takes 9.4 ms to add a million elements, while the CPU takes 415.15 ms, so around a 50-times improvement can be accomplished by using a GPU.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>To summarize, this section demonstrated the use of events to measure timings for GPU code. The performance of the GPU is compared with CPU performance to quantify the performance improvement while using the GPU.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Complex programs in PyCUDA</h1>
                
            
            
                
<p>The PyCUDA syntax and terminology will be familiar by now. We will use this knowledge to develop advanced programs and learn some advanced concepts in PyCUDA. In this section, we will develop a program to square elements of an array using three different methods in PyCUDA. We will also learn the code for doing matrix multiplication in PyCUDA.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Element-wise squaring of a matrix in PyCUDA</h1>
                
            
            
                
<p>In this section, the program to perform element-wise squaring of numbers in a matrix is performed using three different methods. While doing this, the concepts of using multidimensional threads and blocks, the <kbd>inout</kbd> directive of the driver class, and the <kbd>gpuarray</kbd> class is explained in detail.  </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Simple kernel invocation with multidimensional threads</h1>
                
            
            
                
<p>A simple kernel function using PyCUDA to square every element of a matrix is implemented in this section. The kernel function of squaring every element of a 5 x 5 matrix is shown as follows:</p>
<pre>import pycuda.driver as drv<br/>import pycuda.autoinit <br/>from pycuda.compiler import SourceModule<br/>import numpy<br/>mod = SourceModule("""<br/>  __global__ void square(float *d_a)<br/>  {<br/>    int idx = threadIdx.x + threadIdx.y*5;<br/>    d_a[idx] = d_a[idx]*d_a[idx];<br/>  }<br/>""")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The kernel function square takes only one device pointer that points to the matrix as input and replaces every element with the square of it.  As multidimensional threads are launched, the thread index in both <em>x</em> and <em>y </em>directions are used to index the value from a matrix. You can assume that a 5 x 5 matrix is flattened to a 1 x 25 vector to understand the indexing mechanism. Please, note that the size of a matrix is hardcoded to <kbd>5</kbd> in this code, but it can also be user-defined like the size of an array in the last section. The Python code to use this kernel function is shown as follows: </p>
<pre>start = drv.Event()<br/>end=drv.Event()<br/>h_a = numpy.random.randint(1,5,(5, 5))<br/>h_a = h_a.astype(numpy.float32)<br/>h_b=h_a.copy()<br/><br/>start.record()<br/><br/>d_a = drv.mem_alloc(h_a.size * h_a.dtype.itemsize)<br/>drv.memcpy_htod(d_a, h_a)<br/><br/>square = mod.get_function("square")<br/>square(d_a, block=(5, 5, 1), grid=(1, 1), shared=0)<br/><br/>h_result = numpy.empty_like(h_a)<br/>drv.memcpy_dtoh(h_result, d_a)<br/>end.record()<br/>end.synchronize()<br/>secs = start.time_till(end)*1e-3<br/>print("Time of Squaring on GPU without inout")<br/>print("%fs" % (secs))<br/>print("original array:")<br/>print(h_a)<br/>print("Square with kernel:")<br/>print(h_result)</pre>
<p>Two events are created to measure timings of a kernel function. The matrix of 5 x 5 is initialized with random numbers on the host. It is done by using a <kbd>randint</kbd> function of the <kbd>numpy.random</kbd> module. It requires three arguments. The first two arguments define the range of numbers used to generate random numbers. The first argument is the minimum value and the second argument is the maximum value used for generating numbers. The third argument is the size, which is specified as a tuple (5,5). This generated matrix is again converted to a single precision number for faster processing. The memory for the matrix is allocated on the device and the generated matrix of random numbers is copied to it. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The pointer reference to the kernel function is created and the kernel is called by passing a device memory pointer as an argument. The kernel is called with multidimensional threads with the value of 5 in the <em>x</em> and <em>y </em>directions. So the total number of threads launched is 25, with each thread calculating a square of a single element of the matrix.  The result calculated by the kernel is copied back to the host and displayed on the console. The time needed for the kernel is displayed on the console along with the input and output matrix. The output is displayed on the console.</p>
<div><img class="alignnone size-full wp-image-673 image-border" src="img/f3635a78-4477-4e62-a0a8-a8497255a5c5.png" style="" width="330" height="214"/></div>
<p>It takes 149 ms to calculate the square of each element of a 5 x 5 matrix. The same calculation can be simplified by using the <kbd>inout</kbd> directive of the driver class. This is explained in the next section.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using inout with the kernel invocation</h1>
                
            
            
                
<p>As can be seen from the kernel function of the program in the last section, the same array is used as both input and output. The driver module in PyCUDA provides an <kbd>inout</kbd> directive for this kind of cases. It removes the need for the separate allocation of memory for the array, uploading it to the device and downloading the result back to the host. All operations are performed simultaneously during a kernel call. This makes the code simpler and easier to read. The Python code for using the <kbd>inout</kbd> directive of driver class is shown as follows:</p>
<pre>start.record()<br/>start.synchronize()<br/><br/>square(drv.InOut(h_a), block=(5, 5, 1))<br/><br/>end.record()<br/>end.synchronize()<br/><br/>print("Square with InOut:")<br/>print(h_a)<br/>secs = start.time_till(end)*1e-3<br/>print("Time of Squaring on GPU with inout")<br/>print("%fs" % (secs))</pre>
<p>The CUDA events are initialized to measure the performance of the code using the <kbd>inout</kbd> directive. The kernel call is the same as in the last section so it is not repeated here. As can be seen, while calling the square kernel, a single variable is passed as an argument with the <kbd>drv.inout</kbd> directive. So, all device-related operations are performed in this single step. The kernel is called with multidimensional threads as is the case in the last section. The computed result and the time taken is printed on the console as the following shows: </p>
<div><img class="alignnone size-full wp-image-674 image-border" src="img/6e6b4841-76fc-4b09-aa48-80f126cdd70b.png" style="" width="265" height="129"/></div>
<p>The time taken is comparatively less than the original kernel. So, by using the <kbd>inout</kbd> directive the of driver class, the PyCUDA code can be made efficient and easy to read. PyCUDA also provides a <kbd>gpuarray</kbd> class for array-related operations. It can be also used for squaring operations, which is explained in the next section.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using gpuarray class</h1>
                
            
            
                
<p>Python provides a <kbd>numpy</kbd> library for numeric computations in Python. PyCUDA provides a <kbd>gpuarray</kbd> class, similar to  <kbd>numpy</kbd>, that stores its data and performs its computations on the GPU device. The shape and datatype of the arrays work exactly as in <kbd>numpy</kbd>. The <kbd>gpuarray</kbd> class provides many arithmetic methods for computations. It removes the need to specify the kernel code in C or C++ using  <kbd>SourceModule</kbd>. So, the PyCUDA code will contain only a Python code. The code of squaring every element of the matrix using the <kbd>gpuarray</kbd> class is shown as follows: </p>
<pre>import pycuda.gpuarray as gpuarray<br/>import numpy<br/>import pycuda.driver as drv<br/><br/>start = drv.Event()<br/>end=drv.Event()<br/>start.record()<br/>start.synchronize()<br/><br/>h_b = numpy.random.randint(1,5,(5, 5))<br/>d_b = gpuarray.to_gpu(h_b.astype(numpy.float32))<br/>h_result = (d_b**2).get()<br/>end.record()<br/>end.synchronize()<br/><br/>print("original array:")<br/>print(h_b)<br/>print("doubled with gpuarray:")<br/>print(h_result)<br/>secs = start.time_till(end)*1e-3<br/>print("Time of Squaring on GPU with gpuarray")<br/>print("%fs" % (secs))</pre>
<p>The <kbd>gpuarray</kbd> class needs to be imported for using in the code. It is available in the <kbd>pycuda.gpuarray</kbd> module. The matrix is initialized with random integers from 1 to 5 for computation. This matrix is uploaded to the device memory by using the <kbd>to_gpu()</kbd> method of the <kbd>gpuarray</kbd> class. The matrix to be uploaded is provided as an argument to this method. The matrix is converted to a single precision number.  All the operations on this uploaded matrix will be performed on the device. The square operation is performed in a similar way as we do in Python code but, as the variable is stored on the device using <kbd>gpuarray</kbd>, this operation will also be performed on the device. The result is downloaded back to the host by using the <kbd>get</kbd> method. This result along with the time needed to perform element-wise squaring using <kbd>gpuarray</kbd> is displayed on the console as follows:</p>
<div><img class="alignnone size-full wp-image-675 image-border" src="img/5c1561d4-76d2-420f-9718-56e7c2a1578c.png" style="" width="289" height="219"/></div>
<p class="mce-root"/>
<p>It takes around 58 ms to compute the square. It completely removes the need to define kernel functions in C language, and its functionality is similar to the <kbd>numpy</kbd> library so Python programmers can easily work with it.</p>
<p>To summarize, in this section we have developed an element-wise squaring program using PyCUDA in three different fashions. We have also seen the concepts of multidimensional threads, the <kbd>inout</kbd> directive and the <kbd>gpuarray</kbd> class in PyCUDA. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Dot product using GPU array</h1>
                
            
            
                
<p>The dot between two vectors is an important mathematical operation used in various applications. The <kbd>gpuarray</kbd> class used in the last section can be used to calculate the dot product between two vectors. The performance of the <kbd>gpuarray</kbd> method to calculate the dot product is compared with the <kbd>numpy</kbd> operation. The code used to calculate the dot product using <kbd>numpy</kbd> is shown as follows:  </p>
<pre>import pycuda.gpuarray as gpuarray<br/>import pycuda.driver as drv<br/>import numpy<br/>import time<br/>import pycuda.autoinit<br/>n=100<br/>h_a=numpy.float32(numpy.random.randint(1,5,(1,n)))<br/>h_b=numpy.float32(numpy.random.randint(1,5,(1,n)))<br/><br/>       <br/>start=time.time()<br/>h_result=numpy.sum(h_a*h_b)<br/><br/>#print(numpy.dot(a,b))<br/>end=time.time()-start<br/>print("Answer of Dot Product using numpy")<br/>print(h_result)<br/>print("Time taken for Dot Product using numpy")<br/>print(end,"s")</pre>
<p>Two vectors with 100 elements each are initialized with random integers for calculating the dot product. The time module of Python is used to calculate the time needed for the computation of the dot product. The <kbd>*</kbd> operator is used to calculate the element-wise multiplication of two vectors and the result of this is summed up to calculate the overall dot product. Please, note that the <kbd>numpy.dot</kbd> method calculated is used in matrix multiplication, which can't be used for the dot product. The calculated dot product and time are displayed on the console. The code to perform the same operation on a GPU using <kbd>gpuarray</kbd> is shown as follows:</p>
<pre>d_a = gpuarray.to_gpu(h_a)<br/>d_b = gpuarray.to_gpu(h_b)<br/><br/>start1 = drv.Event()<br/>end1=drv.Event()<br/>start1.record()<br/><br/>d_result = gpuarray.dot(d_a,d_b)<br/>end1.record()<br/>end1.synchronize()<br/>secs = start1.time_till(end1)*1e-3<br/>print("Answer of Dot Product on GPU")<br/>print(d_result.get())<br/>print("Time taken for Dot Product on GPU")<br/>print("%fs" % (secs))<br/>if(h_result==d_result.get()):<br/>  print("The computed dor product is correct")</pre>
<p>The <kbd>to_gpu</kbd> method is used to upload two vectors on a GPU for calculating the dot product. The <kbd>gpuarray</kbd> class provides a dot method, which can be used to calculate the dot product directly. It needs two GPU arrays as an argument. The calculated result is downloaded back to the host by using the <kbd>get()</kbd> method. The calculated result and time measured using CUDA events are displayed on the console. The result of the program is shown as follows:</p>
<div><img class="alignnone size-full wp-image-676 image-border" src="img/a712df91-3835-4c33-a470-33af90a81546.png" style="" width="589" height="180"/></div>
<p>As can be seen from the output, that same result is obtained by calculating the dot product using <kbd>numpy</kbd> and <kbd>gpuarray</kbd>. The <kbd>numpy</kbd> library takes 37 ms to compute the dot product, while the GPU takes only 0.1 ms to do the same operation. This further exemplifies the benifit of using GPU and PyCUDA to do complex mathematical operations.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Matrix multiplication</h1>
                
            
            
                
<p>An important mathematical operation used frequently is matrix multiplication. This section will demonstrate how it can be performed on a GPU using PyCUDA. It is a very complicated mathematical operation when the sizes of the matrix are very large. It should be kept in mind that for matrix multiplication, the number of columns in the first matrix should be equal to the number of rows in the second matrix. Matrix multiplication is not a cumulative operation. To avoid complexity, in this example, we are taking a square matrix of the same size. If you are familiar with the mathematics of matrix multiplication, then you may recall that a row in the first matrix will be multiplied with all the columns in the second matrix. This is repeated for all rows in the first matrix. The example of a 3 x 3 matrix multiplication is shown as follows:</p>
<div><img class="alignnone size-full wp-image-677 image-border" src="img/acf80d05-4b69-4582-8bc5-e5e48c2c7b97.png" style="" width="591" height="68"/></div>
<p>Every element in the resultant matrix will be calculated by multiplying the corresponding row in the first matrix and column in the second matrix. This concept is used to develop a kernel function shown as follows:</p>
<pre><br/>import numpy as np<br/>from pycuda import driver<br/>from pycuda.compiler import SourceModule<br/>import pycuda.autoinit<br/>MATRIX_SIZE = 3 <br/><br/>matrix_mul_kernel = """<br/>__global__ void Matrix_Mul_Kernel(float *d_a, float *d_b, float *d_c)<br/>{<br/>  int tx = threadIdx.x;<br/>  int ty = threadIdx.y;<br/>  float value = 0;<br/>  <br/>  for (int i = 0; i &lt; %(MATRIX_SIZE)s; ++i) {<br/>    float d_a_element = d_a[ty * %(MATRIX_SIZE)s + i];<br/>    float d_b_element = d_b[i * %(MATRIX_SIZE)s + tx];<br/>    value += d_a_element * d_b_element;<br/> }<br/> <br/>   d_c[ty * %(MATRIX_SIZE)s + tx] = value;<br/> } """<br/>  <br/>matrix_mul = matrix_mul_kernel % {'MATRIX_SIZE': MATRIX_SIZE}<br/>  <br/>mod = SourceModule(matrix_mul)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The kernel function takes two input arrays and one output array as arguments. The size of a matrix is passed as a constant to the kernel function. This removes the need to pass the size of a vector as one of the parameters of the kernel function, as was explained earlier in the chapter. Both methods are equally correct, and it is up to the programmer which they find more convenient. Each thread computes one element of a resultant matrix. All elements of the row from the first matrix and columns from the second matrix are multiplied and summed up inside the <kbd>for</kbd> loop. The answer is copied to the location in the resultant matrix. The details of calculating the index inside the kernel function can be found in  earlier chapters of the book. The Python code to use this kernel function is shown as follows:  </p>
<pre> <br/>h_a = np.random.randint(1,5,(MATRIX_SIZE, MATRIX_SIZE)).astype(np.float32)<br/>h_b = np.random.randint(1,5,(MATRIX_SIZE, MATRIX_SIZE)).astype(np.float32)<br/><br/>d_a = gpuarray.to_gpu(h_a) <br/>d_b = gpuarray.to_gpu(h_b)<br/>d_c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32)<br/> <br/>matrixmul = mod.get_function("Matrix_Mul_Kernel")<br/>  <br/>matrixmul(d_a, d_b,d_c_gpu, <br/>  block = (MATRIX_SIZE, MATRIX_SIZE, 1),<br/>)<br/>print("*" * 100)<br/>print("Matrix A:")<br/>print(d_a.get())<br/><br/>print("*" * 100)<br/>print("Matrix B:")<br/>print(d_b.get())<br/><br/>print("*" * 100)<br/>print("Matrix C:")<br/>print(d_c_gpu.get())<br/><br/>  # compute on the CPU to verify GPU computation<br/>h_c_cpu = np.dot(h_a, h_b)<br/>if h_c_cpu == d_c_gpu.get() :<br/>    print("The computed matrix multiplication is correct")</pre>
<p class="mce-root"/>
<p>Two matrices with the size of 3 x 3 are initialized with random integers from <kbd>1</kbd> to <kbd>5</kbd>. These matrices are uploaded to the device memory using the <kbd>to_gpu</kbd> method of the <kbd>gpuarray</kbd> class. The empty GPU array is created to store the result on the device. These three variables are passed as arguments to the kernel function. The kernel function is called with the matrix size as dimensions in <em>x</em> and <em>y</em> directions. The result is downloaded back to the host using the <kbd>get()</kbd> method. The two input matrices and the result calculated by the GPU are printed on the console. The matrix multiplication is also calculated on a CPU using the dot method of the <kbd>numpy</kbd> library. The result is compared with the GPU result for verifying the result of the kernel computation. The result of the program is displayed as follows:</p>
<div><img class="alignnone size-full wp-image-678 image-border" src="img/ff3f4d41-f609-45da-8617-9491925e791b.png" style="" width="1141" height="303"/></div>
<p>To summarize, We have developed a simple kernel function to perform matrix multiplication using PyCUDA. This kernel function can be further optimized by using shared memory, as explained earlier in the book. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Advanced kernel functions in PyCUDA</h1>
                
            
            
                
<p>So far, we have seen the use of the <kbd>SourceModule</kbd> class for defining kernel functions in C or C++. We have also used the <kbd>gpuarray</kbd> class for doing device computations without defining kernel functions explicitly. This section describes the advanced kernel definition features available in PyCUDA. These features are used to develop kernel functions for various parallel communication patterns like the map, reduce, and scan operations.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Element-wise kernel in PyCUDA</h1>
                
            
            
                
<p>This feature allows the programmer to define a kernel function that works on every element of an array. It allows the programmer to execute the kernel on complex expressions that are made of one or more operands into a single computational step. The kernel function for the element-wise addition of a large array can be defined in the following way:</p>
<pre>import pycuda.gpuarray as gpuarray<br/>import pycuda.driver as drv<br/>from pycuda.elementwise import ElementwiseKernel<br/>from pycuda.curandom import rand as curand<br/>add = ElementwiseKernel(<br/>  "float *d_a, float *d_b, float *d_c",<br/>  "d_c[i] = d_a[i] + d_b[i]",<br/>  "add")<br/><br/></pre>
<p>The <kbd>PyCuda.elementwise.ElementwiseKernel</kbd> function is used to define the element-wise kernel function. It requires three arguments. The first argument is the list of parameters for the kernel function. The second argument defines the operation to be performed on each element, and the third argument specifies the name of the kernel function. The Python code to use this kernel function is shown as follows:</p>
<pre><br/>n = 1000000<br/>d_a = curand(n)<br/>d_b = curand(n)<br/>d_c = gpuarray.empty_like(d_a)<br/>start = drv.Event()<br/>end=drv.Event()<br/>start.record()<br/>add(d_a, d_b, d_c)<br/>end.record()<br/>end.synchronize()<br/>secs = start.time_till(end)*1e-3<br/>print("Addition of %d element of GPU"%shape)<br/>print("%fs" % (secs))<br/># check the result<br/>if d_c == (d_a + d_b):<br/>  print("The sum computed on GPU is correct")</pre>
<p>The two arrays are initialized with random numbers using the <kbd>curand</kbd> function of the <kbd>pycuda.curandom</kbd> class. It is again a useful functionality, as it removes the need to initialize on the host and then upload to the device memory. An empty GPU array is created to store the result. The <kbd>add</kbd> kernel is called by passing these three variables as an argument. The time needed for the addition of a million elements is calculated using CUDA events and is displayed on the console.</p>
<p class="mce-root"/>
<p>The output of the program is shown as follows:</p>
<div><img class="alignnone size-full wp-image-679 image-border" src="img/93b4cd61-1772-4525-8105-8c726b838bc4.png" style="" width="625" height="83"/></div>
<p>The element-wise kernel only needs 0.6 ms for the addition of a million elements in an array. This performance is better than the program seen earlier in this chapter. So element-wise, kernel definition is a very important concept to remember when element-wise operations are to be performed on a vector.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Reduction kernel </h1>
                
            
            
                
<p>The reduction operation can be defined as reducing a collection of elements to a single value by using some expressions. It can be very useful in various parallel computation applications. The example of calculating the dot product between vectors is taken to demonstrate the concept of reduction in PyCUDA. The program for calculating the dot product using the feature of a reduction kernel in PyCUDA is shown as follows:</p>
<pre>import pycuda.gpuarray as gpuarray<br/>import pycuda.driver as drv<br/>import numpy<br/>from pycuda.reduction import ReductionKernel<br/>import pycuda.autoinit<br/>n=5<br/>start = drv.Event()<br/>end=drv.Event()<br/>start.record()<br/>d_a = gpuarray.arange(n,dtype= numpy.uint32)<br/>d_b = gpuarray.arange(n,dtype= numpy.uint32)<br/>kernel = ReductionKernel(numpy.uint32,neutral="0",reduce_expr="a+b",map_expr="d_a[i]*d_b[i]",arguments="int *d_a,int *d_b")<br/>d_result = kernel(d_a,d_b).get()<br/>end.record()<br/>end.synchronize()<br/>secs = start.time_till(end)*1e-3<br/>print("Vector A")<br/>print(d_a)<br/>print("Vector B")<br/>print(d_b)<br/>print("The computed dot product using reduction:")<br/>print(d_result)<br/>print("Dot Product on GPU")<br/>print("%fs" % (secs))</pre>
<p>PyCUDA provides the <kbd>pycuda.reduction.ReductionKernel</kbd> class to define reduction kernels. It requires many arguments. The first argument is the data type for the output. The second argument is neutral, which is mostly defined as <kbd>0</kbd>. The third argument is the expression used to reduce the collection of elements. The addition operation is defined in the preceding code. The fourth argument is defined as the expression used for mapping operations between operands before reduction. The element-wise multiplication is defined in the code. The final argument defines the argument for the kernel function.</p>
<p>The reduction kernel for computing the dot product requires element-wise multiplication between two vectors and then the addition of all elements. Two vectors are defined usingthe <kbd>arange</kbd> function. It works in a similar way as the <kbd>range</kbd> function in Python but <kbd>arange</kbd> saves the array on the device. The kernel function is called by passing these two vectors as an argument, and the result is fetched to the host. The time needed for computing is calculated using CUDA events and displayed on the console along with the result of the dot product, shown as follows:    </p>
<div><img class="alignnone size-full wp-image-680 image-border" src="img/87bdb5b4-fcfa-4ac9-acbd-a4a99f4a819a.png" style="" width="590" height="160"/></div>
<p>The reduction kernel takes around 2.5 s to compute the dot product, which is a relatively long time compared to the explicit kernel seen in the last section. Still, it is quite useful in parallel computing applications where reduction operation is required.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Scan kernel </h1>
                
            
            
                
<p>The scan operation is again a very important parallel computing paradigm. The scan operator applies a specified function to the first item in the input. The result of that function is provided as the input along with the second item from the original input.  All the intermediate results form the output sequence. This concept can be used for various applications. The example of a cumulative addition is taken as an example to demonstrate the concept of the scan kernel in PyCUDA. The cumulative addition is nothing but applying addition to every element of a vector sequentially. The example is shown as follows:</p>
<pre>Input Vector<br/>[7 5 9 2 9]<br/>Scan Operation for cumulative sum<br/>[7,7+5,7+5+9,7+5+9+2,7+2+9+2+7]</pre>
<p>As can be seen, the output of the previous addition is added to the current element to calculate the output at the current position. This is called an <strong>inclusive scan</strong> operation. If the current element of the input is not involved, then it is known as an<strong> exclusive scan</strong>. The program to perform cumulative summation using an inclusive scan is shown as follows:</p>
<pre><br/>import pycuda.gpuarray as gpuarray<br/>import pycuda.driver as drv<br/>import numpy<br/>from pycuda.scan import InclusiveScanKernel<br/>import pycuda.autoinit<br/>n=10<br/>start = drv.Event()<br/>end=drv.Event()<br/>start.record()<br/>kernel = InclusiveScanKernel(numpy.uint32,"a+b")<br/>h_a = numpy.random.randint(1,10,n).astype(numpy.int32)<br/>d_a = gpuarray.to_gpu(h_a)<br/>kernel(d_a)<br/>end.record()<br/>end.synchronize()<br/>secs = start.time_till(end)*1e-3<br/>assert(d_a.get() == numpy.cumsum(h_a,axis=0)).all()<br/>print("The input data:")<br/>print(h_a)<br/>print("The computed cumulative sum using Scan:")<br/>print(d_a.get())<br/>print("Cumulative Sum on GPU")<br/>print("%fs" % (secs))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>PyCUDA provides the <kbd>pycuda.scan.InclusiveScanKernel</kbd> class to define an inclusive scan kernel. It requires the data type of the output and the operation to be used for scanning as arguments. The addition operation is specified for a cumulative summation. The array with random integers is applied as an input to this kernel function. The kernel output will have the same size as the input. The input and output vector along with the time needed for calculating the cumulative sum is displayed on the console, as shown in the following:</p>
<div><img class="alignnone size-full wp-image-681 image-border" src="img/02280329-9c5d-498d-bc3c-83fdfd6646dd.png" style="" width="604" height="127"/></div>
<p>It takes around 2 ms to run a scan operation on 10 elements of an array. To summarize, in this section we saw various special methods for defining kernels for mapping, reduction, and scanning operations.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter demonstrated the concepts of programming in PyCUDA. It started with the development of a simple <kbd>Hello, PyCUDA</kbd> program using PyCUDA. The concepts of kernel definition in C or C++ and calling it from Python code and the API for accessing GPU device properties from a PyCUDA program were discussed in detail. The execution mechanism for multiple threads and blocks in a PyCUDA program was explained with a simple program. The basic structure of a PyCUDA program was described with a simple example of an array addition. The simplification of PyCUDA code was described by using directives from a driver class. The use of CUDA events to measure the performance of the PyCUDA programs was explained in detail. The functionality of the <kbd>inout</kbd> directive of the driver class and the <kbd>gpuarray</kbd> class was explained using an element-wise squaring example. The <kbd>gpuarray</kbd> class was used to develop code for calculating the dot product using PyCUDA. The PyCUDA code for a complex mathematical operation of matrix multiplication was explained in detail. The last part of the chapter described various kernel-defining methods used for mapping, reduction, and scanning operations. </p>
<p>The next chapter will build on this knowledge and describe some advanced kernels available in PyCUDA along with the development of computer vision applications using PyCUDA.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<p> </p>
<ol>
<li>Which programming language is used to define the kernel function using the <kbd>SourceModule</kbd> class in PyCUDA? Which compiler will be used to compile this kernel function?</li>
<li>Write a kernel call function for the <kbd>myfirst_kernel</kbd> function used in this chapter, with the number of blocks equal to 1024 x 1024 and threads per block equal to 512 x 512.</li>
<li>State true or false: The block execution inside PyCUDA program is in sequential order.</li>
<li>What is the advantage of using the <kbd>In</kbd>, <kbd>Out</kbd> ,and <kbd>inout</kbd> driver class primitives in PyCUDA programs?</li>
<li>Write a PyCUDA program to add two to every element of a vector with an arbitrary size using the <kbd>gpuarray</kbd> class.</li>
<li>What is the advantage of using CUDA events to measure the time for a kernel execution?</li>
<li>State true or false: The <kbd>gpuarray</kbd> class is the GPU device version of the <kbd>numpy</kbd> library in Python.</li>
</ol>


            

            
        
    </div></div></body></html>