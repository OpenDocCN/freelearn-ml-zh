- en: Sentiment Analysis of Amazon Reviews with NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP进行亚马逊评论的情感分析
- en: Every day, we generate data from emails, online posts such as blogs, social
    media comments, and more. It is not surprising to say that unstructured text data
    is much larger in size than the tabular data that exists in the databases of any
    organization. It is important for organizations to acquire useful insights from
    the text data pertaining to the organization. Due to the different nature of the
    text data when compared to data in databases, the methods that need to be employed
    to understand the text data are different. In this chapter, we will learn a number
    of key techniques in **natural language processing** (**NLP**) that help us to
    work on text data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每天我们都会从电子邮件、博客、社交媒体评论等在线帖子中生成数据。说非结构化文本数据比任何组织数据库中存在的表格数据大得多并不令人惊讶。对于组织来说，从与组织相关的文本数据中获得有用的见解非常重要。由于与数据库中的数据相比，文本数据具有不同的性质，因此需要采用不同的方法来理解文本数据。在本章中，我们将学习许多自然语言处理（NLP）的关键技术，这些技术帮助我们处理文本数据。
- en: 'The common definition of NLP is as follows: an area of computer science and
    artificial intelligence that deals with the interactions between computers and
    human (natural) languages; in particular, how to program computers to fruitfully
    process large amounts of natural language data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的常见定义如下：计算机科学和人工智能的一个领域，处理计算机与人类（自然）语言之间的交互；特别是，如何编程计算机以有效地处理大量自然语言数据。
- en: In general terms, NLP deals with understanding human speech as it is spoken.
    It helps machines read and understand "text".
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般意义上讲，NLP处理的是理解人类语言的自然表达。它帮助机器阅读和理解“文本”。
- en: Human languages are highly complex and several ambiguities need to be resolved
    in order to correctly comprehend the spoken language or written text. In the area
    of NLP, several techniques are applied in order to deal with these ambiguities,
    including the **Part-of-Speech** (**POS**) tagger, term disambiguation, entity
    extraction, relations' extraction, key term recognition, and more.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语言非常复杂，需要解决多个歧义才能正确理解口语或书面文本。在自然语言处理领域，应用了多种技术来处理这些歧义，包括词性标注器、术语消歧、实体提取、关系提取、关键词识别等。
- en: For natural language systems to work successfully, a consistent knowledge base,
    such as a detailed thesaurus, a lexicon of words, a dataset for linguistic and
    grammatical rules, an ontology, and up-to-date entities, are prerequisites.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自然语言系统能够成功工作，一个一致的知识库，如详细的同义词词典、词汇表、语言和语法规则的数据集、本体和最新的实体，是先决条件。
- en: 'It may be noted that NLP is concerned with understanding the text from not
    just the syntactic perspective, but also from a semantic perspective. Similar
    to humans, the idea is for the machines to be able to perceive underlying messages
    behind the spoken words and not just the structure of words in sentences. There
    are numerous application areas of NLP, and the following are just a few of these:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指出，自然语言处理（NLP）不仅关注从句法角度理解文本，还关注从语义角度理解文本。类似于人类，目标是让机器能够感知说话背后的潜在信息，而不仅仅是句子中词语的结构。NLP有众多应用领域，以下只是其中的一小部分：
- en: Speech recognition systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别系统
- en: Question answering systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Machine translation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Text summarization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Virtual agents or chatbots
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟代理或聊天机器人
- en: Text classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Topic segmentation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题分段
- en: As the NLP subject area in itself is very vast, it is not practical to cover
    all the areas in just one chapter. Therefore, we will be focusing on "text classification"
    in this chapter. We do this by implementing a project that performs sentiment
    analysis in the reviews expressed by Amazon.com customers. Sentiment analysis
    is a type of text classification task where we classify each of the documents
    (reviews) into one of the possible categories. The possible categories could be
    positive, negative, or neutral, or it could be positive, negative, or a rating
    on a scale of 1 to 10.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自然语言处理本身是一个非常大的领域，不可能在一个章节中涵盖所有领域。因此，我们将专注于本章的“文本分类”。我们通过实施一个项目来实现，该项目对亚马逊.com客户表达的评论进行情感分析。情感分析是一种文本分类任务，我们将每个文档（评论）分类到可能的类别之一。可能的类别可以是正面、负面或中性，或者可以是正面、负面或1到10的评分。
- en: Text documents that need to be classified cannot be input directly to a machine
    learning algorithm. Each of the documents needs to be represented in a certain
    format that is acceptable for the ML algorithm as input to work on. In this chapter,
    we explore, implement, and understand the **Bag of Words** (**BoW**) word embedding
    approaches. These are approaches in which text can be represented.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 需要分类的文本文档不能直接输入到机器学习算法中。每个文档都需要以机器学习算法可以接受的格式表示。在本章中，我们将探讨、实现和理解**词袋模型（BoW**）词嵌入方法。这些方法中，文本可以表示。
- en: 'As we progress with the chapter, we will cover the following topics:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的进展，我们将涵盖以下主题：
- en: The sentiment analysis problem
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析问题
- en: Understanding the Amazon reviews dataset
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解亚马逊评论数据集
- en: Building a text sentiment classifier with the BoW approach
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BoW方法构建文本情感分类器
- en: Understanding word embedding approaches
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词嵌入方法
- en: Building a text sentiment classifier with pretrained Word2Vec word embedding
    based on Reuters news corpus
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于路透社新闻语料库的预训练Word2Vec词嵌入构建文本情感分类器
- en: Building a text sentiment classifier with GloVe word embedding
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GloVe词嵌入构建文本情感分类器
- en: Building a text sentiment classifier with fastText
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用fastText构建文本情感分类器
- en: The sentiment analysis problem
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析问题
- en: Sentiment analysis is one of the most general text classification applications.
    The purpose of it is to analyze messages such as user reviews, and feedback from
    employees, in order to identify whether the underlying sentiment is positive,
    negative, or neutral.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是最通用的文本分类应用之一。其目的是分析用户评论、员工反馈等消息，以确定潜在的情感是积极的、消极的还是中性的。
- en: Analyzing and reporting sentiment in texts allows businesses to quickly get
    a consolidated high-level insight without having to read each one of the comments
    received.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和报告文本中的情感允许企业快速获得综合的高层次洞察，而无需阅读收到的每一条评论。
- en: While it is possible to generate holistic sentiment based on the overall comments
    received, there is also an extended area called **aspect-based sentiment analysis**.
    It is focused on deriving sentiment based on each area of the service. For example,
    a customer that visited a restaurant when writing a review would generally cover
    areas such as ambience, food quality, service quality, and price. Though the feedback
    about each of the areas may not be quoted under a specific heading, the sentences
    in the review comments would naturally cover the customer's opinion of one or
    more of these areas. Aspect-based sentiment analysis attempts to identify the
    sentences in the reviews in each of the areas and then identify whether the sentiment
    is positive, negative, or neutral. Providing sentiment by each area helps businesses
    quickly identify their weak areas.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以根据收到的总体评论生成整体情感，但还有一个扩展领域，称为**基于方面的情感分析**。它侧重于根据服务的每个方面推导情感。例如，一位在撰写评论时访问过餐厅的客户通常会涵盖环境、食品质量、服务质量、价格等方面。尽管关于每个方面的反馈可能不会在特定的标题下引用，但评论中的句子自然会涵盖客户对其中一个或多个这些方面的看法。基于方面的情感分析试图识别每个方面的评论中的句子，然后确定情感是积极的、消极的还是中性的。按每个方面提供情感有助于企业快速识别其薄弱环节。
- en: In this chapter, we will discuss and implement methods that are aimed at identifying
    the overall sentiment from the review texts. The task can be achieved in several
    ways, ranging from a simple lexicon method to a complex word embedding method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论和实现旨在从评论文本中识别整体情感的方法。这个任务可以通过多种方式实现，从简单的词典方法到复杂的词嵌入方法。
- en: A **lexicon** method is not really a machine learning method. It is more a rule
    based method that is based on a predefined positive and negative words dictionary.
    The method involves looking up the number of positive words and negative words
    in each review. If the count of positive words in the review is more than the
    count of negative words, then the review is marked as positive, otherwise it is
    marked as negative. If there are an equal number of positive and negative words,
    then the review is marked as neutral. As implementing this method is straightforward,
    and as it comes with a requirement for a predefined dictionary, we will not cover
    the implementation of the lexicon method in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**词库**方法实际上并不是一种机器学习方法。它更是一种基于预定义的正负词字典的规则方法。该方法涉及查找每个评论中的正词和负词的数量。如果评论中正词的数量多于负词的数量，则该评论被标记为正面，否则被标记为负面。如果正词和负词的数量相等，则评论被标记为中性。由于实现此方法很简单，并且它需要一个预定义的字典，因此我们不会在本章中介绍词库方法的实现。'
- en: While it is possible to consider the sentiment analysis problem as an unsupervised
    clustering problem, in this chapter we consider it as a supervised classification
    problem. This is because, we have the Amazon reviews labeled dataset available.
    We can make use of these labels to build classification models, and therefore,
    the supervised algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将情感分析问题视为无监督聚类问题是可能的，但在本章中，我们将它视为监督分类问题。这是因为我们有亚马逊评论标记数据集可用。我们可以利用这些标签来构建分类模型，因此，使用监督算法。
- en: Getting started
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'The dataset is available for download and use at the following URL:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可在以下URL下载和使用：
- en: '[https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) .'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) .'
- en: Understanding the Amazon reviews dataset
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解亚马逊评论数据集
- en: 'We use the Amazon product reviews polarity dataset for the various projects
    in this chapter. It is an open dataset constructed and made available by Xiang
    Zhang. It is used as a text classification benchmark in the paper: *Character-level
    Convolutional Networks for Text Classification* and *Advances in Neural Information
    Processing Systems* 28,*Xiang Zhang, Junbo Zhao, Yann LeCun,** (NIPS 2015)*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的各个项目中使用亚马逊产品评论极性数据集。这是一个由张翔构建并公开的数据集。它被用作论文《Character-level Convolutional
    Networks for Text Classification》和《Advances in Neural Information Processing Systems》28中的文本分类基准，作者为张翔、赵军波、杨立昆**（NIPS
    2015）**。
- en: The Amazon reviews polarity dataset is constructed by taking review score 1
    and 2 as negative, 4 and 5 as positive. Samples of score 3 are ignored. In the
    dataset, class 1 is the negative and class 2 is the positive. The dataset has
    1,800,000 training samples and 200,000 testing samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊评论极性数据集是通过将评分1和2视为负面，4和5视为正面来构建的。评分3的样本被忽略。在数据集中，类别1是负面，类别2是正面。该数据集有1,800,000个训练样本和200,000个测试样本。
- en: The `train.csv` and `test.csv` files contains all the samples as comma-separated
    values. There are three columns in them, corresponding to class index (1 or 2),
    review title, and review text. The review title and text are escaped using double
    quotes ("), and any internal double quote is escaped by 2 double quotes ("").
    New lines are escaped by a backslash followed with an "n" character that is "\n".
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.csv`和`test.csv`文件包含所有样本，以逗号分隔值的形式存在。它们包含三列，分别对应类别索引（1或2）、评论标题和评论文本。评论标题和文本使用双引号（"）进行转义，任何内部的引号通过两个双引号（"")进行转义。换行符通过反斜杠后跟一个“n”字符进行转义，即"\n"。'
- en: 'To ensure that we are able to run our projects, even with minimal infrastructure,
    let''s restrict the number of records to be considered in our dataset to 1,000
    records only. Of course, the code that we use in the projects can be extended
    to any number of records, as long as the hardware infrastructure support is available.
    Let''s first read the data and visualize the records with the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们能够运行我们的项目，即使是在最基本的基础设施下，让我们将我们的数据集中要考虑的记录数限制为仅1,000条。当然，我们在项目中使用的代码可以扩展到任意数量的记录，只要硬件基础设施支持即可。让我们首先读取数据，并使用以下代码可视化记录：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will result in the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b6bce54f-dc46-455b-ad50-0c6c06ae41ca.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6bce54f-dc46-455b-ad50-0c6c06ae41ca.png)'
- en: 'Post reading the file, we can see that there is only one column in the dataset
    and this column had both the review text and the sentiment components in it. We
    will slightly modify the format of the dataset for the purpose of using it with
    sentiment analysis projects in this chapter involving the BoW, Word2vec, and GloVe
    approaches. Let''s modify the format of the dataset with the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完文件后，我们可以看到数据集中只有一列，而这列包含了评论文本和情感成分。为了在本章中使用涉及BoW、Word2vec和GloVe方法的情感分析项目，我们将稍微修改数据集的格式。让我们用以下代码修改数据集的格式：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will result in the following output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/79b22882-b09c-4e3e-9e20-92f7bbbed8f0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79b22882-b09c-4e3e-9e20-92f7bbbed8f0.png)'
- en: 'Now we have two columns in our dataset. However, there is unnecessary punctuation
    that exists in both the columns that may cause problems with processing the dataset
    further. Let''s attempt to remove the punctuation with the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们数据集中有两列。然而，这两列中都存在可能引起进一步处理数据集问题的多余标点符号。让我们尝试用以下代码删除标点符号：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will result in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/0b22278e-b011-4631-bb32-b1e1742aba4f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b22278e-b011-4631-bb32-b1e1742aba4f.png)'
- en: From the preceding output, we see that we have a clean dataset that is ready
    for use. Also, we have written the output to a file. When we build the sentiment
    analyzer, we can start directly reading the dataset from the `Sentiment Analysis
    Dataset.csv` file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们看到我们有一个干净的数据集，可以立即使用。此外，我们还已将输出写入文件。当我们构建情感分析器时，我们可以直接从`Sentiment
    Analysis Dataset.csv`文件中读取数据集。
- en: 'The fastText algorithm expects the dataset to be in a different format. The
    data input to fastText should comply the following format:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: fastText算法期望数据集以不同的格式。fastText的数据输入应遵守以下格式：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, `X` is the class name. Text is the actual review text that
    led to the rating specified under the class. Both the rating and text should be
    placed on one line with no quotes. The classes are `__label__1` and `__label__2`,
    and there should be only one class per row. Let''s accomplish the `fastText` library
    required format with the following code block:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`X`是类名。Text是导致该类下指定评分的实际评论文本。评分和文本应放在同一行上，无需引号。类是`__label__1`和`__label__2`，每行应只有一个类。让我们用以下代码块完成`fastText`库所需的格式：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will result in the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From the output of basic EDA code, we can see that the dataset is in the required
    format, therefore we can proceed to our next section of implementing the sentiment
    analysis engine using the BoW approach. Along side the implementation, we will
    delve into learning the concept behind the approach, and explore the sub-techniques
    that can be leveraged in the approach to obtain better results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本的EDA代码输出中，我们可以看到数据集已经处于所需格式，因此我们可以继续到下一个部分，使用BoW方法实现情感分析引擎。在实现的同时，我们将深入研究该方法背后的概念，并探索该方法中可以用来获得更好结果的一些子技术。
- en: Building a text sentiment classifier with the BoW approach
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BoW方法构建文本情感分类器
- en: 'The intent of the BoW approach is to convert the review text provided into
    a matrix form. It represents documents as a set of distinct words by ignoring
    the order and meaning of the words. Each row of the matrix represents each review
    (otherwise called a document in NLP), and the columns represent the universal
    set of words present in all the reviews. For each document, and across each word,
    the existence of the word, or the frequency of the word occurrence, in that specific
    document is recorded. Finally, the matrix created from word frequency vectors
    represents the documents set. This methodology is used to create input datasets
    that are required to train the models, and also to prepare the test dataset that
    need to be used by the trained models to perform text classification. Now that
    we understand the BoW motivation, let''s jump into implementing the steps to build
    a sentiment analysis classifier based on this approach, as shown in the following
    code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: BoW方法的目的是将提供的评论文本转换为矩阵形式。它通过忽略单词的顺序和意义，将文档表示为一组不同的单词。矩阵的每一行代表每个评论（在NLP中通常称为文档），列代表所有评论中存在的通用单词集。对于每个文档和每个单词，记录该单词在该特定文档中的存在或单词出现的频率。最后，从单词频率向量创建的矩阵表示文档集。这种方法用于创建训练模型所需的输入数据集，以及准备需要由训练模型使用的测试数据集以执行文本分类。现在我们了解了BoW的动机，让我们跳入实现基于此方法的情感分析分类器步骤，如下面的代码块所示：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will result in the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/068d0389-1c4c-418b-b6c3-ecb897f30581.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/068d0389-1c4c-418b-b6c3-ecb897f30581.png)'
- en: 'The first step in processing text data involves creating a *corpus*, which
    is a collection of text documents. The `VCorpus` function in the `tm` package
    enables conversion of the reviews comments column in the data frame into a volatile
    corpus. This can be achieved through the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据的第一个步骤是创建一个*语料库*，这是一个文本文档的集合。`tm`包中的`VCorpus`函数可以将数据框中的评论评论列转换为不稳定的语料库。这可以通过以下代码实现：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will result in the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From the volatile corpus, we create a **Document Term Matrix** (**DTM**). A
    DTM is a sparse matrix that is created using the `tm` library's `DocumentTermMatrix`
    function. The rows of the matrix indicate documents and the columns indicate features,
    that is, words. The matrix is sparse because all unique unigram sets of the dataset
    become columns in DTM and, as each review comment does not have all elements of
    the unigram set, most cells will have a 0, indicating the absence of the unigram.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从不稳定的语料库中，我们创建一个**文档-词矩阵**（**DTM**）。DTM是使用`tm`库的`DocumentTermMatrix`函数创建的稀疏矩阵。矩阵的行表示文档，列表示特征，即单词。该矩阵是稀疏的，因为数据集中的所有唯一单语元集都成为DTM的列，并且由于每个评论评论没有单语元集的所有元素，大多数单元格将有一个0，表示不存在单语元。
- en: 'While it is possible to extract n-grams (unigrams, bigrams, trigrams, and so
    on) as part of the BoW approach, the tokenize parameter can be set and passed
    as part of the control list in the `DocumentTermMatrix` function to accomplish
    n-grams in DTM. It must be noted that using n-grams as part of the DTM creates
    a very high number of columns in the DTM. This is one of the demerits of the BoW
    approach, and, in some cases, it could stall the execution of the project due
    to limited memory. As our specific case is also limited by hardware infrastructure,
    we restrict ourselves by including only the unigrams in DTM in this project. Apart
    from just generating unigrams, we also perform some additional processing on the
    reviews text document by passing parameters to the control list in the `tm` library''s
    `DocumentTermMatrix` function. The processing we do on the review text documents
    during the creation of the DTM is given here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在BoW（词袋）方法中可以提取n-gram（单语元、双语元、三元语元等）作为一部分，但可以将tokenize参数设置为控制列表的一部分，并在`DocumentTermMatrix`函数中传递以在DTM（文档-词矩阵）中实现n-gram。必须注意的是，将n-gram作为DTM的一部分会创建DTM中非常高的列数。这是BoW方法的一个缺点，在某些情况下，由于内存限制，它可能会阻碍项目的执行。鉴于我们的特定案例也受硬件基础设施的限制，我们在这个项目中仅包括DTM中的单语元。除了仅生成单语元之外，我们还通过对`tm`库的`DocumentTermMatrix`函数中的控制列表传递参数，对评论文本文档进行一些额外的处理。在创建DTM期间对评论文本文档进行的处理如下：
- en: Change the case of the text to lowercase.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本的字母大小写改为小写。
- en: Remove any numbers.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除任何数字。
- en: Remove stop words using the English language stop word list from the Snowball
    stemmer project.  Stop words are common words, such as a, an, in, and the, that
    do not add value in deciding sentiment based on review comments.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Snowball词干提取项目的英语语言停用词表来移除停用词。停用词是一些常见的单词，如a、an、in和the，它们在根据评论内容判断情感时并不增加价值。
- en: Remove punctuation.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除标点符号。
- en: Perform stemming, which aims at resolving a word into the base form of the word,
    that is, strip the plural *s* from nouns, the *ing* from verbs, or other affixes.
    A stem is a natural group of words with equal or very similar meaning. After the
    stemming process, every word is represented by its stem. The `SnowballC` library
    provides the capability to obtain the root for each of the words in the review
    comments.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行词干提取，其目的是将单词还原为单词的基本形式，即从名词中去除复数*s*，从动词中去除*ing*，或去除其他前缀。词干是一组具有相等或非常相似意义的自然词组。在词干提取过程之后，每个单词都由其词干表示。《SnowballC》库提供了获取评论评论中每个单词根的能力。
- en: 'Let''s now create a DTM from the volatile corpus and do the text preprocessing
    with the following code block:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下代码块从易变语料库创建DTM并进行文本预处理：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see from the output that there are 1,000 documents that were processed and
    form rows in the matrix. There are 5,794 columns representing unique unigrams
    from the reviews following the additional text processing. We also see that the
    DTM is 99% sparse and consists of non-zero entries only in 34,494 cells. The non-zero
    cells represent the frequency of occurrence of the word on the column in the document
    represent on the row of the DTM. The weighting is done through the default 'term
    frequency' weighting, as we did not specify any weighting parameter in the control
    list supplied to the `DocumentTermMatrix` function. Other forms of weighting,
    such as **term frequency-inverse document frequency** (**TFIDF**), are also possible
    just by passing the appropriate weight parameter in the control list to the `DocumentTermMatrix`
    function. For now, we will stick to weighting based on term frequency, which is
    the default. We also see from the `inspect` function that several sample documents
    were output along with the term frequencies in these documents.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，有1,000个文档被处理并形成矩阵的行。有5,794列代表经过额外文本处理后的独特单语素。我们还看到DTM有99%的稀疏性，并且仅在34,494个单元格中有非零条目。非零单元格代表单词在DTM行对应的文档中的出现频率。权重是通过默认的“词频”权重进行的，因为我们没有在提供给`DocumentTermMatrix`函数的控制列表中指定任何权重参数。通过在控制列表中传递适当的权重参数到`DocumentTermMatrix`函数，也可以实现其他形式的权重，例如**词频-逆文档频率**（**TFIDF**）。现在，我们将坚持基于词频的权重，这是默认的。我们还从`inspect`函数中看到，一些样本文档以及这些文档中的词频被输出。
- en: 'The DTM tends to get very big, even for normal sized datasets. Removing sparse
    terms, that is, terms occurring only in very few documents, is the technique that
    can be tried to reduce the size of the matrix without losing significant relations
    inherent to the matrix. Let''s remove sparse columns from the matrix. We will
    attempt to remove those terms that have at least a 99% of sparse elements with
    the following line of code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DTM（文档-词矩阵）往往变得非常大，即使是正常大小的数据集也是如此。移除稀疏项，即仅出现在极少数文档中的项，是一种可以尝试的技术，可以在不丢失矩阵固有的显著关系的情况下减小矩阵的大小。让我们从矩阵中移除稀疏列。我们将尝试使用以下代码行移除至少有99%稀疏元素的术语：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will result in the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now see from the output of the `inspect` function that the sparsity of the
    matrix is reduced to 97%, and the number of unigrams (columns of the matrix) is
    reduced to `686`. We are now ready with the DTM that can be used for training
    with any machine learning classification algorithm. In the next few lines of code,
    let''s attempt to divide our DTM into training and test dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从`inspect`函数的输出中看到，矩阵的稀疏性降低到97%，单语素（矩阵的列）的数量减少到`686`。我们现在已经准备好可以使用任何机器学习分类算法进行训练的DTM。在接下来的几行代码中，让我们尝试将我们的DTM划分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will be using a machine learning algorithm called **Naive Bayes** to create
    a model. Naive Bayes is generally trained on data with nominal features. We can
    observe that the cells in our DTM are numeric and therefore need to be converted
    to nominal prior to feeding the dataset as input for creating the model with Naive
    Bayes. As each cell indicates the word frequency in the review, and as the number
    of times a word used in the review does not impact sentiment, let''s write a function
    to convert the cell values with a non-zero value to `Y`, and in case of a zero,
    let''s convert it to `N`, with the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种名为**朴素贝叶斯**的机器学习算法来创建模型。朴素贝叶斯通常在具有名义特征的 数据上训练。我们可以观察到我们的DTM（词袋模型）中的单元格是数值型的，因此需要将其转换为名义型，以便将数据集作为输入提供给朴素贝叶斯模型进行创建。由于每个单元格表示评论中的单词频率，并且单词在评论中使用的次数不会影响情感，让我们编写一个函数将具有非零值的单元格值转换为`Y`，在值为零的情况下，将其转换为`N`，以下是一段代码：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let''s apply the function on all rows of the training dataset, and test
    dataset we have previously created in this project with the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用函数到训练数据集和测试数据集的所有行上，这些数据集是我们在这个项目中之前创建的，以下是一段代码：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will result in the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/c894b708-f3b8-4a46-9951-4f55652ce045.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c894b708-f3b8-4a46-9951-4f55652ce045.png)'
- en: 'We can see from the output that all the cells in the training and test DTMs
    are now converted to nominal values. Thus, let''s proceed to build a text sentiment
    analysis classifier using the Naive Bayes algorithm from the `e1071` library,
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，训练和测试DTM中的所有单元格现在都已转换为名义值。因此，让我们继续使用`e1071`库中的朴素贝叶斯算法构建文本情感分析分类器，如下所示：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will result in the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding summary output shows that the `nb_senti_classifier` object is
    successfully created from the training DTM. Let''s now use the model object to
    predict sentiment on the test data DTM. In the following code block, we are instructing
    that the predictions should be classes and not prediction probabilities:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的摘要输出显示，`nb_senti_classifier`对象已成功从训练DTM创建。现在让我们使用模型对象在测试数据DTM上预测情感。在以下代码块中，我们指示预测应该是类别而不是预测概率：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will result in the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the following code, let us now compute the accuracy of the model using
    the `mmetric` function in the `rminer` library:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们现在使用`rminer`库中的`mmetric`函数计算模型的准确率：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We achieved a 79% accuracy just with a very quick and basic BoW model. The model
    can be further improved by means of techniques such as parameter tuning, lemmatization,
    new features creation, and so on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用一个非常快速和基本的BoW模型就实现了79%的准确率。可以通过参数调整、词干提取、创建新特征等技术进一步提高模型。
- en: Pros and cons of the BoW approach
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BoW（词袋）方法的优缺点
- en: Now that we have an understanding of both the theory and implementation of the
    BoW approach, let's examine the pros and cons of the approach. When it comes to
    pros, the BoW approach is very simple to understand and implement and therefore
    offers a lot of flexibility for customization on any text dataset. It may be observed
    that the BoW approach does not retain the order of words specifically when only
    unigrams are considered. This problem is generally overcome by retaining n-grams
    in the DTM. However, it comes at the cost as larger infrastructure is needed to
    process the text and build a classifier. Another severe drawback of the approach
    is that it does not respect the semantics of the word. For example, the words
    "car" and "automobile" are often used in the same context. A model built based
    on BoW treats the sentences "buy used cars" and "purchase old automobiles" as
    very different sentences. While these sentences are the same, BoW models do not
    classify these sentences as the same, as the words in these sentences are not
    matching. It is possible to consider the semantics of words in sentences using
    an approach called word embedding. This is something we will explore in our next
    section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了BoW（词袋）方法的原理和实现，让我们来分析一下这种方法的优势和劣势。在优势方面，BoW方法非常简单易懂且易于实现，因此为任何文本数据集的定制化提供了很大的灵活性。可以观察到，当只考虑单词时，BoW方法并不保留单词的顺序。这个问题通常通过在DTM（词袋矩阵）中保留n-gram来解决。然而，这需要更大的基础设施来处理文本和构建分类器。该方法的另一个严重缺点是它不尊重单词的语义。例如，“汽车”和“汽车”这两个词在相同的上下文中经常被使用。基于BoW构建的模型将句子“购买二手汽车”和“购买旧汽车”视为非常不同的句子。尽管这些句子是相同的，但BoW模型不会将这些句子分类为相同，因为这些句子中的单词并不匹配。使用称为词嵌入的方法可以考虑到句子中单词的语义。这是我们将在下一节中探讨的内容。
- en: Understanding word embedding
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: The BoW models that we discussed in our earlier section suffer from a problem
    that they do not capture information about a word’s meaning or context. This means
    that potential relationships, such as contextual closeness, are not captured across
    collections of words. For example, the approach cannot capture simple relationships,
    such as determining that the words "cars" and "buses" both refer to vehicles that
    are often discussed in the context of transportation. This problem that we experience
    with the BoW approach will be overcome by word embedding, which is an improved
    approach to mapping semantically similar words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中讨论的BoW模型存在一个问题，即它们没有捕捉到关于单词意义或上下文的信息。这意味着潜在的关联，如上下文邻近性，在单词集合中没有被捕捉到。例如，这种方法无法捕捉简单的关联，例如确定“汽车”和“公交车”这两个词都指的是经常在交通上下文中讨论的车辆。通过词嵌入，我们可以克服BoW方法中遇到的问题，词嵌入是一种改进的映射语义相似单词的方法。
- en: Word vectors represent words as multidimensional continuous floating point numbers,
    where semantically similar words are mapped to proximate points in geometric space.
    For example, the words *fruit* and *leaves* would have a similar word vector, *tree*.
    This is due to the similarity of their meanings, whereas the word *television* would
    be quite distant in the geometrical space. In other words, words that are used
    in a similar context will be mapped to a proximate vector space.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量将单词表示为多维连续的浮点数，其中语义相似的单词在几何空间中被映射到邻近的点。例如，“水果”和“叶子”这两个词会有相似的词向量，“树”。这是由于它们意义的相似性，而“电视”这个词在几何空间中则相对较远。换句话说，在相似上下文中使用的单词将被映射到邻近的向量空间。
- en: The word vectors can be of *n* dimensions, and *n* can take any number as input
    from the user creating it (for example 10, 70, 500). The dimensions are latent
    in the sense that it may not be apparent to humans what each of these dimensions
    means in reality. There are methods such as **Continuous Bag of Words** (**CBOW**)
    and **Skip-Gram** that enable conceiving the word vectors from the text provided
    as training input to word embedding algorithms. Also, the individual numbers in
    the word vector represent the word's distributed weight across dimensions. In
    a general sense, each dimension represents a latent meaning, and the word's numerical
    weight on that dimension captures the closeness of its association with and to
    that meaning. Thus, the semantics of the word are embedded across the dimensions
    of the vector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量可以是*n*维的，*n*可以是用户创建时输入的任何数字（例如10、70、500）。这些维度是隐含的，因为对人类来说，这些维度在现实中代表什么可能并不明显。有如**连续词袋**（**CBOW**）和**跳字模型**（**Skip-Gram**）等方法，可以从提供的文本作为训练输入到词嵌入算法中构思词向量。此外，词向量中的单个数字代表单词在各个维度上的分布权重。在一般意义上，每个维度代表一个潜在的意义，而单词在该维度上的数值权重捕捉了它与该意义的关联程度。因此，单词的语义嵌入在向量的维度中。
- en: 'Though the word vectors are multidimensional and cannot be visualized directly,
    it is possible to visualize the vectors learned, by projecting them down to two
    dimensions using techniques such as the t-SNE dimensionality reduction technique.
    The following diagram displays learned word vectors in two dimensional spaces
    for country capitals, verb tenses, and gender relationships:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词向量是多维的，无法直接可视化，但可以通过使用如t-SNE降维技术等将它们投影到二维空间，从而可视化学习到的向量。以下图表显示了国家首都、动词时态和性别关系在二维空间中的学习词向量：
- en: '![](img/09d8ca48-57d7-4e72-a2f1-bf3c03b1a68a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/09d8ca48-57d7-4e72-a2f1-bf3c03b1a68a.png)'
- en: Visualization of word embeddings in a two dimensional space
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中可视化词嵌入
- en: When we observe the word embedding visualization, we can perceive that the vectors
    captured some general, and in fact quite useful, semantic information about words
    and their relationships to one another. With this, each word in the text now can
    be represented as a row in the matrix similar to that of the BoW approach, but,
    unlike the BoW approach, it captures the relationships between the words.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察词嵌入的可视化时，我们可以感知到向量捕捉了关于单词及其相互之间的一些通用、实际上非常有用的语义信息。有了这个，文本中的每个单词现在都可以表示为矩阵中的一行，类似于BoW方法，但与BoW方法不同，它捕捉了单词之间的关系。
- en: 'The advantage of representing words as vectors is that they lend themselves
    to mathematical operators. For example, we can add and subtract vectors. The canonical
    example here is showing that by using word vectors we can determine the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词表示为向量的优点是它们适用于数学运算。例如，我们可以对向量进行加法和减法。这里的典型例子是展示通过使用词向量，我们可以确定以下内容：
- en: '*king - man + woman = queen*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*国王 - 男人 + 女人 = 女王*'
- en: In the given example, we subtracted the gender (man) from the word vector for
    king and added another gender (woman), and we obtained a new word vector from
    the operation (*king - man + woman*) that maps most closely to the word vector
    for queen.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的例子中，我们从国王的词向量中减去了性别（男人），并添加了另一个性别（女人），从而通过操作（*国王 - 男人 + 女人*）获得了一个新的词向量，该向量与女王的词向量映射最为接近。
- en: 'A few more amazing examples of mathematical operations that can be achieved
    on word vectors are shown as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了可以在词向量上实现的更多数学运算的惊人例子：
- en: 'Given two words, we can establish the degree of similarity between them:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定两个单词，我们可以确定它们之间的相似度：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And the output is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finding the odd one out from the set of words given as input:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从给定的单词集中找出不同类的一个：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The odd one is given as the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类的单词如下所示：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Derive analogies, for example:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导类比，例如：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, what it all means for us is that machines are able to identify semantically
    similar words given in a sentence. The following diagram is a gag related to word
    embedding that made me laugh, but the gag does convey the power of word embedding
    application, which otherwise would not be possible with the BoW kind of text representations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这一切对我们意味着，机器能够识别句子中给出的语义相似的单词。以下图表是关于词嵌入的一个让我笑的笑话，但这个笑话确实传达了词嵌入应用的力量，否则使用BoW类型的文本表示是不可能实现的：
- en: '![](img/e972fc21-e370-464a-9b08-22fef9b041bb.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e972fc21-e370-464a-9b08-22fef9b041bb.png)'
- en: A gag demonstrating the power of word embeddings application
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个展示词嵌入应用能力的笑话
- en: There are several techniques that can be used to learn word embedding from text
    data. Word2vec, GloVe, and fastText are some of the popular techniques. Each of
    these techniques allows us to either train our own word embedding from the text
    data we have, or use the readily available pretrained vectors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以从文本数据中学习词嵌入。Word2vec、GloVe和fastText是一些流行的技术。这些技术中的每一种都允许我们从我们拥有的文本数据中训练自己的词嵌入，或者使用现成的预训练向量。
- en: This approach of learning our own word embedding requires a lot of training
    data and can be slow, but this option will learn an embedding both targeted to
    the specific text data and the NLP task at hand.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习我们自己的词嵌入的方法需要大量的训练数据，可能会很慢，但这个选项将学习一个既针对特定文本数据又针对当前NLP任务的嵌入。
- en: Pretrained word embedding vectors are vectors that are trained on large amounts
    of text data (usually billions of words) available on sources such as Wikipedia.
    These are generally high-quality word embedding vectors made available by companies
    such as Google or Facebook. We can download these pretrained vector files and
    consume them to obtain word vectors for the words in the text that we would like
    to classify or cluster.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练词嵌入向量是在大量文本数据（通常为数十亿单词）上训练的向量，这些数据通常来自维基百科等来源。这些通常是谷歌或Facebook等公司提供的高质量词嵌入向量。我们可以下载这些预训练向量文件，并使用它们来获取我们想要分类或聚类的文本中单词的词向量。
- en: Building a text sentiment classifier with pretrained word2vec word embedding
    based on Reuters news corpus
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于路透社新闻语料库，使用预训练的word2vec词嵌入构建文本情感分类器
- en: Word2vec was developed by Tomas Mikolov, et al. at Google in 2013 as a response
    to making the neural-network-based training of the embedding more efficient, and
    since then it has become the de facto standard for developing pretrained word
    embedding.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec是由Tomas Mikolov等人于2013年在谷歌开发的，作为使基于神经网络的嵌入训练更高效的回应，从那时起，它已经成为开发预训练词嵌入的事实标准。
- en: 'Word2vec introduced the following two different learning models to learn the
    word embedding:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec引入了以下两种不同的学习模型来学习词嵌入：
- en: '**CBOW**: Learns the embedding by predicting the current word based on its
    context.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW**: 通过预测当前词的上下文来学习嵌入。'
- en: '**Continuous Skip-Gram**: The continuous Skip-Gram model learns by predicting
    the surrounding words given a current word.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续Skip-Gram**: 连续Skip-Gram模型通过预测给定当前词的周围词来学习。'
- en: Both CBOW and Skip-Gram methods of learning are focused on learning the words
    given their local usage context, where the context of the word itself is defined
    by a window of neighboring words. This window is a configurable parameter of the
    model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW和Skip-Gram学习方法都专注于根据局部使用上下文学习单词，其中单词的上下文由一个邻近单词的窗口定义。这个窗口是模型的可配置参数。
- en: The `softmaxreg` library in R offers pretrained `word2vec` word embedding that
    can be used for building our sentiment analysis engine for the Amazon reviews
    data. The pretrained vector is built using the `word2vec` model, and it is based
    on the `Reuter_50_50` dataset, UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: R中的`softmaxreg`库提供了预训练的`word2vec`词嵌入，可用于构建针对亚马逊评论数据的情感分析引擎。预训练的向量是使用`word2vec`模型构建的，并且基于`Reuter_50_50`数据集，UCI机器学习仓库([https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50))。
- en: 'Without any delay, let''s get into the code and also review the approach followed
    in this code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不加任何延迟，让我们进入代码并回顾一下这个代码中采用的方法：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s examine the `word2vec` pretrained emdeddings. It is just another data
    frame, and therefore can be reviewed through the regular `dim` and `View` commands
    as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查`word2vec`预训练嵌入。它只是一个数据框，因此可以通过常规的`dim`和`View`命令进行审查，如下所示：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will result in the following output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](img/7a78e718-5842-47c2-ab0d-09a046f7ebe5.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a78e718-5842-47c2-ab0d-09a046f7ebe5.png)'
- en: 'Here, let''s use the following `dim` command:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，让我们使用以下 `dim` 命令：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will result in the following output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From the preceding output, we can observe that there are `12853` words that
    have got word vectors in the pretrained vector. Each of the words is defined using
    20 dimensions, and these dimensions define the context of the words. In the next
    step, we can look up the word vector for each of the words in the review comments.
    As there are only 12,853 words in the pretrained word embedding, there is a possibility
    that we encounter a word that does not exist in the pretrained embedding. In such
    a case, the unidentified word is represented with a 20 dimension vector that is
    filled with zeros.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以观察到有`12853`个单词在预训练向量中获得了单词向量。每个单词都是使用20个维度定义的，这些维度定义了单词的上下文。在下一步中，我们可以查找评论中每个单词的单词向量。由于预训练的单词嵌入中只有12,853个单词，我们可能会遇到一个不在预训练嵌入中的单词。在这种情况下，未识别的单词用一个填充有零的20维向量表示。
- en: We also need to understand that the word vectors are available only at a word
    level, and therefore in order to decode the entire review, we take the mean of
    all the word vectors of the words that made up the review. Let's review the concept
    of getting the word vector for a sentence from individual word vectors with an
    example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要理解，单词向量仅在单词级别上可用，因此为了解码整个评论，我们取构成评论的所有单词的单词向量的平均值。让我们通过一个例子来回顾从单个单词向量获取句子单词向量的概念。
- en: Assume the sentence we want to get the word vector for is, *it is very bright
    and sunny this morning*. Individual words that comprise the sentence are *it*,
    *is*, *very*, *bright*, *and*, *sunny*, *this*, and *morning*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要获取单词向量的句子是，“今天早上非常明亮且阳光明媚”。构成句子的单个单词是“it”、“is”、“very”、“bright”、“and”、“sunny”、“this”和“morning”。
- en: 'Now, we can look up each of these words in the pretrained vector and get the
    corresponding word vectors as shown in the following table:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在预训练向量中查找这些单词，并获取以下表格中所示的相应单词向量：
- en: '| **Word** | **dim1** | **dim2** | **dim3** | **.....** | **....** | **dim19**
    | **dim20** |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **词** | **dim1** | **dim2** | **dim3** | **.....** | **....** | **dim19**
    | **dim20** |  |'
- en: '| `it` | -2.25 | 0.75 | 1.75 | -1.25 | -0.25 | -3.25 | -2.25 |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `it` | -2.25 | 0.75 | 1.75 | -1.25 | -0.25 | -3.25 | -2.25 |  |'
- en: '| `is` | 0.75 | 1.75 | 1.75 | -2.25 | -2.25 | 0.75 | -0.25 |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `is` | 0.75 | 1.75 | 1.75 | -2.25 | -2.25 | 0.75 | -0.25 |  |'
- en: '| `very` | -2.25 | 2.75 | 1.75 | -0.25 | 0.75 | 0.75 | -2.25 |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `very` | -2.25 | 2.75 | 1.75 | -0.25 | 0.75 | 0.75 | -2.25 |  |'
- en: '| `bright` | -3.25 | -3.25 | -2.25 | -1.25 | 0.75 | 1.75 | -0.25 |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `bright` | -3.25 | -3.25 | -2.25 | -1.25 | 0.75 | 1.75 | -0.25 |  |'
- en: '| `and` | -0.25 | -1.25 | -2.25 | 2.75 | -3.25 | -0.25 | 1.75 |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `and` | -0.25 | -1.25 | -2.25 | 2.75 | -3.25 | -0.25 | 1.75 |  |'
- en: '| `sunny` | 0 | 0 | 0 | 0 | 0 | 0 | 0 |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `sunny` | 0 | 0 | 0 | 0 | 0 | 0 | 0 |  |'
- en: '| `this` | -2.25 | -3.25 | 2.75 | 0.75 | -0.25 | -0.25 | -0.25 |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `this` | -2.25 | -3.25 | 2.75 | 0.75 | -0.25 | -0.25 | -0.25 |  |'
- en: '| `morning` | -0.25 | -3.25 | -2.25 | 1.75 | 0.75 | 2.75 | 2.75 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `morning` | -0.25 | -3.25 | -2.25 | 1.75 | 0.75 | 2.75 | 2.75 |  |'
- en: 'Now, we have word vectors that comprise the sentence. Please note that these
    are not actual word vector values but just are made up to demonstrate the approach.
    Also, observe that the word `sunny` is represented with zeros across the dimensions
    to symbolize that the word is not found in the pretrained word embedding. In order
    to get the word vector for the sentence, we just compute the mean of each dimension.
    The resulting vector is a 1 x 20 vector representing the sentence, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了构成句子的单词向量。请注意，这些并不是实际的单词向量值，只是为了演示方法而编造的。此外，观察单词`sunny`在所有维度上用零表示，以表示该单词未在预训练的单词嵌入中找到。为了获取句子的单词向量，我们只需计算每个维度的平均值。得到的向量是一个1
    x 20的向量，代表句子，如下所示：
- en: '| Sentence | -1.21875 | -0.71875 | 0.15625 | 0.03125 | -0.46875 | 0.28125 |
    -0.09375 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | -1.21875 | -0.71875 | 0.15625 | 0.03125 | -0.46875 | 0.28125 | -0.09375
    |'
- en: 'The `softmaxreg` library offers the `wordEmbed` function where we could pass
    a sentence and ask it to compute the `mean` word vector for the sentence. The
    following code is a custom function that was created to apply the `wordEmbed`
    function on each of the Amazon reviews we have in hand. At the end of applying
    this function to the reviews dataset, we expect to have a *n* x 20 matrix that
    is the word vector representation of our reviews. The *n* in the *n* x 20 represents
    the number of rows and 20 is the number of dimensions through which each review
    is represented, as seen in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmaxreg`库提供了`wordEmbed`函数，我们可以传递一个句子并要求它计算句子的`mean`词向量。以下是一个自定义函数，用于将`wordEmbed`函数应用于我们手头的每个亚马逊评论。在将此函数应用于评论数据集之后，我们期望得到一个*n*
    x 20的矩阵，这是我们评论的词向量表示。*n*在*n* x 20中代表行数，20是通过每个评论表示的维度数，如下所示：'
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will result in the following output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](img/af33d08a-c790-4859-ae29-5e9f48ab5d29.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af33d08a-c790-4859-ae29-5e9f48ab5d29.png)'
- en: 'Then we review `temp` using the `dim` command, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`dim`命令来审查`temp`，如下所示：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will result in the following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can see from the output that we have word vectors created for each of the
    reviews in our corpus. This data frame can now be used to build classification
    models using an ML algorithm. The following code to achieve classification is
    no different from the one we did for the BoW approach:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，我们为语料库中的每个评论创建了词向量。现在可以使用这个数据框通过机器学习算法构建分类模型。以下用于分类的代码与用于BoW方法的代码没有不同：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will result in the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding output shows that the Random Forest model object is successfully
    created. Of course, the model can be improved further; however we are not going
    to be doing that here as the focus is to demonstrate making use of word embeddings,
    rather than getting the best performing classifier.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示，随机森林模型对象已成功创建。当然，模型可以进一步改进；然而，我们在这里不会这么做，因为重点是展示如何利用词嵌入，而不是获得最佳性能的分类器。
- en: 'Next, with the following code we make use of the Random Forest model to make
    predictions on the test data and then report out the performance:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码利用随机森林模型对测试数据进行预测，并报告性能：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will result in the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We see that we get a 62% accuracy from using the pretrained `word2vec` embeddings
    made out of the Reuters news group's dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，使用从路透社新闻组数据集制作的预训练`word2vec`嵌入，我们得到了62%的准确率。
- en: Building a text sentiment classifier with GloVe word embedding
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GloVe词嵌入构建文本情感分类器
- en: Stanford University's Pennington, et al. developed an extension of the `word2vec`
    method that is called **Global Vectors for Word Representation** (**GloVe**) for
    efficiently learning word vectors.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学的Pennington等人开发了一种`word2vec`方法的扩展，称为**全局词表示**（**GloVe**），用于高效地学习词向量。
- en: GloVe combines the global statistics of matrix factorization techniques, such
    as LSA, with the local context-based learning in `word2vec`. Also, unlike `word2vec`,
    rather than using a window to define local context, GloVe constructs an explicit
    word context or word co-occurrence matrix using statistics across the whole text
    corpus. As an effect, the learning model yields generally better word embeddings.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe结合了矩阵分解技术的全局统计，如LSA，以及`word2vec`中的基于局部上下文的学习。与`word2vec`不同，GloVe不是使用窗口来定义局部上下文，而是通过整个文本语料库的统计来构建一个显式的词上下文或词共现矩阵。因此，学习模型产生了通常更好的词嵌入。
- en: The `text2vec` library in R has a GloVe implementation that we could use to
    train to obtain word embeddings from our own training corpus. Alternatively, pretrained
    GloVe word embeddings can be downloaded and reused, similar to the way we did
    in the earlier `word2vec` pretrained embedding project covered in the previous
    section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: R中的`text2vec`库有一个GloVe实现，我们可以用它来训练，从自己的训练语料库中获得词嵌入。或者，可以下载预训练的GloVe词嵌入并重复使用，就像我们在上一节中提到的早期`word2vec`预训练嵌入项目中所做的那样。
- en: 'The following code block demonstrates the way in which GloVe word embeddings
    can be created and used for sentiment analysis, or, for that matter, any text
    classification task. We are not going to discuss explicitly the steps involved,
    since the code is already heavily commented with detailed explanations of each
    of the steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了如何创建和使用GloVe词嵌入进行情感分析，或者，实际上，任何文本分类任务。我们不会明确讨论涉及的步骤，因为代码已经对每个步骤进行了详细的注释：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will result in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following uses the `glove` model to obtain the combined word vector:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下使用`glove`模型来获取组合词向量：
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will result in the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](img/644b36df-c202-4d9a-9433-2579473dd24a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/644b36df-c202-4d9a-9433-2579473dd24a.png)'
- en: 'We make use of the `softmaxreg` library to obtain the mean word vector for
    each review. This is similar to what we did in `word2vec` pretrained embedding
    in the previous section. Observe that we are passing our own trained word embedding
    `word_vectors` to the `wordEmbed()` function, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`softmaxreg`库来获取每个评论的平均词向量。这与我们在上一节中使用的`word2vec`预训练嵌入类似。注意，我们将我们自己的训练词嵌入`word_vectors`传递给`wordEmbed()`函数，如下所示：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will result in the following output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](img/c48d11a0-df92-4dfa-8ee4-910afd3931bf.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c48d11a0-df92-4dfa-8ee4-910afd3931bf.png)'
- en: 'We will now split the dataset into train and test portions, and use the `randomforest`
    library to build a model to train, as shown in the following lines of code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集分成训练和测试部分，并使用`randomforest`库构建一个模型进行训练，如下面的代码行所示：
- en: '[PRE44]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will result in the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we use the Random Forest model created to predict labels, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用创建的随机森林模型来预测标签，如下所示：
- en: '[PRE46]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will result in the following output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE47]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: With this method, we obtain an accuracy of 66%. This is despite the fact that
    the word embeddings are obtained from words in just 1,000 text samples. The model
    may be further improved by using a pretrained embedding. The overall framework
    to use the pretrained embedding remains the same as what we did in `word2vec`
    project in the previous section.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们获得了66%的准确率。尽管词嵌入是从仅1,000个文本样本中的词获得的，但模型可能通过使用预训练嵌入进一步改进。使用预训练嵌入的整体框架与我们在上一节中`word2vec`项目中使用的相同。
- en: Building a text sentiment classifier with fastText
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用fastText构建文本情感分类器
- en: '`fastText` is a library and is an extension of `word2vec` for word representation.
    It was created by the Facebook Research Team in 2016\. While Word2vec and GloVe
    approaches treat words as the smallest unit to train on, fastText breaks words
    into several n-grams, that is, subwords. For example, the trigrams for the word
    apple are app, ppl, and ple. The word embedding for the word apple is sum of all
    the word n-grams. Due to the nature of the algorithm''s embedding generation,
    fastText is more resource-intensive and takes additional time to train. Some of
    the advantages of `fastText` are as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`fastText`是一个库，是`word2vec`的词表示扩展。它由Facebook研究团队在2016年创建。虽然Word2vec和GloVe方法将词作为训练的最小单元，但fastText将词分解成多个n-gram，即子词。例如，单词apple的三元组是app、ppl和ple。单词apple的词嵌入是所有词n-gram的总和。由于算法嵌入生成的性质，fastText更占用资源，并且需要额外的时间来训练。`fastText`的一些优点如下：'
- en: It generates better word embeddings for rare words (including misspelled words).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为罕见词（包括拼写错误的词）生成更好的词嵌入。
- en: For out of vocabulary words, fastText can construct the vector for a word from
    its character n-grams, even if a word doesn't appear in training corpus. This
    is not a possibility for both Word2vec and GloVe.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于不在词汇表中的词，fastText可以从其字符n-gram中构建一个词的向量，即使这个词没有出现在训练语料库中。Word2vec和GloVe都不具备这种可能性。
- en: The `fastTextR` library provides an interface to the fastText. Let's make use
    of the `fastTextR` library for our project to build a sentiment analysis engine
    on Amazon reviews. While it is possible to download pretrained fastText word embedding
    and make use of it for our project, let's make an attempt to train a word embedding
    based on the reviews dataset we have in hand. It should be noted that the approach
    in terms of making use of fastText pretrained word embedding is similar to the
    approach we followed in the `word2vec` based project that we dealt with earlier.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the project covered in the previous section, comments are included
    inline in the code. The comments explain each of the lines indicating the approach
    taken to build the Amazon reviews sentiment analyzer in this project. Let''s look
    into the following code now:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This will result in the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26daddc5-2f6b-4dbf-b95a-0b13ec3705a7.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s divide the reviews into training and test datasets, and view them
    using the following lines of code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This will give the following output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e27d1e8e-4ef3-48da-853a-807bf9c58c63.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to view the test dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will give the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eed25069-abad-4348-bac7-0b4b5b15f278.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'We will now create a `.txt` file for the train and test dataset using the following
    code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now we will view the no labels test dataset for confirmation using the following
    command:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will result in the following output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70533eb-7e36-4f83-a262-00279dcf2f66.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now write the no labels test dataset to a file so we can use it for
    testing, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will result in the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa10b4e0-e95f-4b37-b32f-191a4f03d45f.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Now we will obtain the word vectors from a previously trained model and view
    the word vectors for each word in our training dataset, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will result in the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a2104e3-a082-4b1b-b99b-d0b9c33536ed.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'We will predict the labels for the reviews in the no labels test dataset and
    write it to a file for future reference. Then we will get the predictions into
    a data frame to compute the performance and see the estimate of the accuracy using
    the following lines of code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will result in the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We have a 58% accuracy with the `fastText` method on our reviews data. As a
    next step, we could check whether the accuracy may be further improved by making
    use of fastText pretrained word embedding. As we already know, implementing a
    project by making use of pretrained embedding is not very different from the implementation
    that we followed in the `word2vec` project described in the earlier section of
    this chapter. The difference is just that the training step to obtain word embedding
    needs to be discarded and the model variable in the code covered in this project
    code should be initiated with the pretrained word embeddings.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned various NLP techniques, namely BoW, Word2vec, GloVe,
    and fastText. We built projects involving these techniques to perform sentiment
    analysis on an Amazon reviews dataset. The projects that were built involved two
    approaches, making use of pretrained word embeddings and building the word embeddings
    from our own dataset. We tried both these approaches to represent text in a format
    that can be consumed by ML algorithms that resulted in models with the ability
    to perform sentiment analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了各种自然语言处理技术，包括BoW、Word2vec、GloVe和fastText。我们构建了涉及这些技术的项目，以对亚马逊评论数据集进行情感分析。所构建的项目涉及两种方法，一种是利用预训练的词嵌入，另一种是从我们自己的数据集中构建词嵌入。我们尝试了这两种方法来表示文本，以便可以被机器学习算法消费，从而产生了能够执行情感分析的能力的模型。
- en: In the next chapter, we will learn about customer segmentation by making use
    of a wholesale dataset. We will look at customer segmentation as an unsupervised
    problem and build projects with various techniques that can identify inherent
    groups within the e-commerce company's customer base. Come, let's explore the
    world of building an e-commerce customer segmentation engine with ML!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过利用批发数据集来学习客户细分。我们将把客户细分视为一个无监督问题，并使用各种技术构建项目，以识别电子商务公司客户群内的固有群体。来吧，让我们一起探索使用机器学习构建电子商务客户细分引擎的世界！
