- en: Sentiment Analysis of Amazon Reviews with NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every day, we generate data from emails, online posts such as blogs, social
    media comments, and more. It is not surprising to say that unstructured text data
    is much larger in size than the tabular data that exists in the databases of any
    organization. It is important for organizations to acquire useful insights from
    the text data pertaining to the organization. Due to the different nature of the
    text data when compared to data in databases, the methods that need to be employed
    to understand the text data are different. In this chapter, we will learn a number
    of key techniques in **natural language processing** (**NLP**) that help us to
    work on text data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'The common definition of NLP is as follows: an area of computer science and
    artificial intelligence that deals with the interactions between computers and
    human (natural) languages; in particular, how to program computers to fruitfully
    process large amounts of natural language data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, NLP deals with understanding human speech as it is spoken.
    It helps machines read and understand "text".
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Human languages are highly complex and several ambiguities need to be resolved
    in order to correctly comprehend the spoken language or written text. In the area
    of NLP, several techniques are applied in order to deal with these ambiguities,
    including the **Part-of-Speech** (**POS**) tagger, term disambiguation, entity
    extraction, relations' extraction, key term recognition, and more.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: For natural language systems to work successfully, a consistent knowledge base,
    such as a detailed thesaurus, a lexicon of words, a dataset for linguistic and
    grammatical rules, an ontology, and up-to-date entities, are prerequisites.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be noted that NLP is concerned with understanding the text from not
    just the syntactic perspective, but also from a semantic perspective. Similar
    to humans, the idea is for the machines to be able to perceive underlying messages
    behind the spoken words and not just the structure of words in sentences. There
    are numerous application areas of NLP, and the following are just a few of these:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual agents or chatbots
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic segmentation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the NLP subject area in itself is very vast, it is not practical to cover
    all the areas in just one chapter. Therefore, we will be focusing on "text classification"
    in this chapter. We do this by implementing a project that performs sentiment
    analysis in the reviews expressed by Amazon.com customers. Sentiment analysis
    is a type of text classification task where we classify each of the documents
    (reviews) into one of the possible categories. The possible categories could be
    positive, negative, or neutral, or it could be positive, negative, or a rating
    on a scale of 1 to 10.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Text documents that need to be classified cannot be input directly to a machine
    learning algorithm. Each of the documents needs to be represented in a certain
    format that is acceptable for the ML algorithm as input to work on. In this chapter,
    we explore, implement, and understand the **Bag of Words** (**BoW**) word embedding
    approaches. These are approaches in which text can be represented.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'As we progress with the chapter, we will cover the following topics:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The sentiment analysis problem
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Amazon reviews dataset
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with the BoW approach
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding word embedding approaches
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with pretrained Word2Vec word embedding
    based on Reuters news corpus
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with GloVe word embedding
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with fastText
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sentiment analysis problem
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is one of the most general text classification applications.
    The purpose of it is to analyze messages such as user reviews, and feedback from
    employees, in order to identify whether the underlying sentiment is positive,
    negative, or neutral.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and reporting sentiment in texts allows businesses to quickly get
    a consolidated high-level insight without having to read each one of the comments
    received.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to generate holistic sentiment based on the overall comments
    received, there is also an extended area called **aspect-based sentiment analysis**.
    It is focused on deriving sentiment based on each area of the service. For example,
    a customer that visited a restaurant when writing a review would generally cover
    areas such as ambience, food quality, service quality, and price. Though the feedback
    about each of the areas may not be quoted under a specific heading, the sentences
    in the review comments would naturally cover the customer's opinion of one or
    more of these areas. Aspect-based sentiment analysis attempts to identify the
    sentences in the reviews in each of the areas and then identify whether the sentiment
    is positive, negative, or neutral. Providing sentiment by each area helps businesses
    quickly identify their weak areas.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss and implement methods that are aimed at identifying
    the overall sentiment from the review texts. The task can be achieved in several
    ways, ranging from a simple lexicon method to a complex word embedding method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: A **lexicon** method is not really a machine learning method. It is more a rule
    based method that is based on a predefined positive and negative words dictionary.
    The method involves looking up the number of positive words and negative words
    in each review. If the count of positive words in the review is more than the
    count of negative words, then the review is marked as positive, otherwise it is
    marked as negative. If there are an equal number of positive and negative words,
    then the review is marked as neutral. As implementing this method is straightforward,
    and as it comes with a requirement for a predefined dictionary, we will not cover
    the implementation of the lexicon method in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to consider the sentiment analysis problem as an unsupervised
    clustering problem, in this chapter we consider it as a supervised classification
    problem. This is because, we have the Amazon reviews labeled dataset available.
    We can make use of these labels to build classification models, and therefore,
    the supervised algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is available for download and use at the following URL:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) .'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Amazon reviews dataset
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the Amazon product reviews polarity dataset for the various projects
    in this chapter. It is an open dataset constructed and made available by Xiang
    Zhang. It is used as a text classification benchmark in the paper: *Character-level
    Convolutional Networks for Text Classification* and *Advances in Neural Information
    Processing Systems* 28,*Xiang Zhang, Junbo Zhao, Yann LeCun,** (NIPS 2015)*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon reviews polarity dataset is constructed by taking review score 1
    and 2 as negative, 4 and 5 as positive. Samples of score 3 are ignored. In the
    dataset, class 1 is the negative and class 2 is the positive. The dataset has
    1,800,000 training samples and 200,000 testing samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: The `train.csv` and `test.csv` files contains all the samples as comma-separated
    values. There are three columns in them, corresponding to class index (1 or 2),
    review title, and review text. The review title and text are escaped using double
    quotes ("), and any internal double quote is escaped by 2 double quotes ("").
    New lines are escaped by a backslash followed with an "n" character that is "\n".
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that we are able to run our projects, even with minimal infrastructure,
    let''s restrict the number of records to be considered in our dataset to 1,000
    records only. Of course, the code that we use in the projects can be extended
    to any number of records, as long as the hardware infrastructure support is available.
    Let''s first read the data and visualize the records with the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will result in the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6bce54f-dc46-455b-ad50-0c6c06ae41ca.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Post reading the file, we can see that there is only one column in the dataset
    and this column had both the review text and the sentiment components in it. We
    will slightly modify the format of the dataset for the purpose of using it with
    sentiment analysis projects in this chapter involving the BoW, Word2vec, and GloVe
    approaches. Let''s modify the format of the dataset with the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will result in the following output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b22882-b09c-4e3e-9e20-92f7bbbed8f0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Now we have two columns in our dataset. However, there is unnecessary punctuation
    that exists in both the columns that may cause problems with processing the dataset
    further. Let''s attempt to remove the punctuation with the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will result in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b22278e-b011-4631-bb32-b1e1742aba4f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we see that we have a clean dataset that is ready
    for use. Also, we have written the output to a file. When we build the sentiment
    analyzer, we can start directly reading the dataset from the `Sentiment Analysis
    Dataset.csv` file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'The fastText algorithm expects the dataset to be in a different format. The
    data input to fastText should comply the following format:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, `X` is the class name. Text is the actual review text that
    led to the rating specified under the class. Both the rating and text should be
    placed on one line with no quotes. The classes are `__label__1` and `__label__2`,
    and there should be only one class per row. Let''s accomplish the `fastText` library
    required format with the following code block:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will result in the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From the output of basic EDA code, we can see that the dataset is in the required
    format, therefore we can proceed to our next section of implementing the sentiment
    analysis engine using the BoW approach. Along side the implementation, we will
    delve into learning the concept behind the approach, and explore the sub-techniques
    that can be leveraged in the approach to obtain better results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with the BoW approach
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The intent of the BoW approach is to convert the review text provided into
    a matrix form. It represents documents as a set of distinct words by ignoring
    the order and meaning of the words. Each row of the matrix represents each review
    (otherwise called a document in NLP), and the columns represent the universal
    set of words present in all the reviews. For each document, and across each word,
    the existence of the word, or the frequency of the word occurrence, in that specific
    document is recorded. Finally, the matrix created from word frequency vectors
    represents the documents set. This methodology is used to create input datasets
    that are required to train the models, and also to prepare the test dataset that
    need to be used by the trained models to perform text classification. Now that
    we understand the BoW motivation, let''s jump into implementing the steps to build
    a sentiment analysis classifier based on this approach, as shown in the following
    code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will result in the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/068d0389-1c4c-418b-b6c3-ecb897f30581.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'The first step in processing text data involves creating a *corpus*, which
    is a collection of text documents. The `VCorpus` function in the `tm` package
    enables conversion of the reviews comments column in the data frame into a volatile
    corpus. This can be achieved through the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will result in the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From the volatile corpus, we create a **Document Term Matrix** (**DTM**). A
    DTM is a sparse matrix that is created using the `tm` library's `DocumentTermMatrix`
    function. The rows of the matrix indicate documents and the columns indicate features,
    that is, words. The matrix is sparse because all unique unigram sets of the dataset
    become columns in DTM and, as each review comment does not have all elements of
    the unigram set, most cells will have a 0, indicating the absence of the unigram.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is possible to extract n-grams (unigrams, bigrams, trigrams, and so
    on) as part of the BoW approach, the tokenize parameter can be set and passed
    as part of the control list in the `DocumentTermMatrix` function to accomplish
    n-grams in DTM. It must be noted that using n-grams as part of the DTM creates
    a very high number of columns in the DTM. This is one of the demerits of the BoW
    approach, and, in some cases, it could stall the execution of the project due
    to limited memory. As our specific case is also limited by hardware infrastructure,
    we restrict ourselves by including only the unigrams in DTM in this project. Apart
    from just generating unigrams, we also perform some additional processing on the
    reviews text document by passing parameters to the control list in the `tm` library''s
    `DocumentTermMatrix` function. The processing we do on the review text documents
    during the creation of the DTM is given here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Change the case of the text to lowercase.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove any numbers.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stop words using the English language stop word list from the Snowball
    stemmer project.  Stop words are common words, such as a, an, in, and the, that
    do not add value in deciding sentiment based on review comments.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove punctuation.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform stemming, which aims at resolving a word into the base form of the word,
    that is, strip the plural *s* from nouns, the *ing* from verbs, or other affixes.
    A stem is a natural group of words with equal or very similar meaning. After the
    stemming process, every word is represented by its stem. The `SnowballC` library
    provides the capability to obtain the root for each of the words in the review
    comments.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now create a DTM from the volatile corpus and do the text preprocessing
    with the following code block:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see from the output that there are 1,000 documents that were processed and
    form rows in the matrix. There are 5,794 columns representing unique unigrams
    from the reviews following the additional text processing. We also see that the
    DTM is 99% sparse and consists of non-zero entries only in 34,494 cells. The non-zero
    cells represent the frequency of occurrence of the word on the column in the document
    represent on the row of the DTM. The weighting is done through the default 'term
    frequency' weighting, as we did not specify any weighting parameter in the control
    list supplied to the `DocumentTermMatrix` function. Other forms of weighting,
    such as **term frequency-inverse document frequency** (**TFIDF**), are also possible
    just by passing the appropriate weight parameter in the control list to the `DocumentTermMatrix`
    function. For now, we will stick to weighting based on term frequency, which is
    the default. We also see from the `inspect` function that several sample documents
    were output along with the term frequencies in these documents.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'The DTM tends to get very big, even for normal sized datasets. Removing sparse
    terms, that is, terms occurring only in very few documents, is the technique that
    can be tried to reduce the size of the matrix without losing significant relations
    inherent to the matrix. Let''s remove sparse columns from the matrix. We will
    attempt to remove those terms that have at least a 99% of sparse elements with
    the following line of code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will result in the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now see from the output of the `inspect` function that the sparsity of the
    matrix is reduced to 97%, and the number of unigrams (columns of the matrix) is
    reduced to `686`. We are now ready with the DTM that can be used for training
    with any machine learning classification algorithm. In the next few lines of code,
    let''s attempt to divide our DTM into training and test dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will be using a machine learning algorithm called **Naive Bayes** to create
    a model. Naive Bayes is generally trained on data with nominal features. We can
    observe that the cells in our DTM are numeric and therefore need to be converted
    to nominal prior to feeding the dataset as input for creating the model with Naive
    Bayes. As each cell indicates the word frequency in the review, and as the number
    of times a word used in the review does not impact sentiment, let''s write a function
    to convert the cell values with a non-zero value to `Y`, and in case of a zero,
    let''s convert it to `N`, with the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let''s apply the function on all rows of the training dataset, and test
    dataset we have previously created in this project with the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will result in the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c894b708-f3b8-4a46-9951-4f55652ce045.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'We can see from the output that all the cells in the training and test DTMs
    are now converted to nominal values. Thus, let''s proceed to build a text sentiment
    analysis classifier using the Naive Bayes algorithm from the `e1071` library,
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will result in the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding summary output shows that the `nb_senti_classifier` object is
    successfully created from the training DTM. Let''s now use the model object to
    predict sentiment on the test data DTM. In the following code block, we are instructing
    that the predictions should be classes and not prediction probabilities:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will result in the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the following code, let us now compute the accuracy of the model using
    the `mmetric` function in the `rminer` library:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We achieved a 79% accuracy just with a very quick and basic BoW model. The model
    can be further improved by means of techniques such as parameter tuning, lemmatization,
    new features creation, and so on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of the BoW approach
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of both the theory and implementation of the
    BoW approach, let's examine the pros and cons of the approach. When it comes to
    pros, the BoW approach is very simple to understand and implement and therefore
    offers a lot of flexibility for customization on any text dataset. It may be observed
    that the BoW approach does not retain the order of words specifically when only
    unigrams are considered. This problem is generally overcome by retaining n-grams
    in the DTM. However, it comes at the cost as larger infrastructure is needed to
    process the text and build a classifier. Another severe drawback of the approach
    is that it does not respect the semantics of the word. For example, the words
    "car" and "automobile" are often used in the same context. A model built based
    on BoW treats the sentences "buy used cars" and "purchase old automobiles" as
    very different sentences. While these sentences are the same, BoW models do not
    classify these sentences as the same, as the words in these sentences are not
    matching. It is possible to consider the semantics of words in sentences using
    an approach called word embedding. This is something we will explore in our next
    section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了BoW（词袋）方法的原理和实现，让我们来分析一下这种方法的优势和劣势。在优势方面，BoW方法非常简单易懂且易于实现，因此为任何文本数据集的定制化提供了很大的灵活性。可以观察到，当只考虑单词时，BoW方法并不保留单词的顺序。这个问题通常通过在DTM（词袋矩阵）中保留n-gram来解决。然而，这需要更大的基础设施来处理文本和构建分类器。该方法的另一个严重缺点是它不尊重单词的语义。例如，“汽车”和“汽车”这两个词在相同的上下文中经常被使用。基于BoW构建的模型将句子“购买二手汽车”和“购买旧汽车”视为非常不同的句子。尽管这些句子是相同的，但BoW模型不会将这些句子分类为相同，因为这些句子中的单词并不匹配。使用称为词嵌入的方法可以考虑到句子中单词的语义。这是我们将在下一节中探讨的内容。
- en: Understanding word embedding
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: The BoW models that we discussed in our earlier section suffer from a problem
    that they do not capture information about a word’s meaning or context. This means
    that potential relationships, such as contextual closeness, are not captured across
    collections of words. For example, the approach cannot capture simple relationships,
    such as determining that the words "cars" and "buses" both refer to vehicles that
    are often discussed in the context of transportation. This problem that we experience
    with the BoW approach will be overcome by word embedding, which is an improved
    approach to mapping semantically similar words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中讨论的BoW模型存在一个问题，即它们没有捕捉到关于单词意义或上下文的信息。这意味着潜在的关联，如上下文邻近性，在单词集合中没有被捕捉到。例如，这种方法无法捕捉简单的关联，例如确定“汽车”和“公交车”这两个词都指的是经常在交通上下文中讨论的车辆。通过词嵌入，我们可以克服BoW方法中遇到的问题，词嵌入是一种改进的映射语义相似单词的方法。
- en: Word vectors represent words as multidimensional continuous floating point numbers,
    where semantically similar words are mapped to proximate points in geometric space.
    For example, the words *fruit* and *leaves* would have a similar word vector, *tree*.
    This is due to the similarity of their meanings, whereas the word *television* would
    be quite distant in the geometrical space. In other words, words that are used
    in a similar context will be mapped to a proximate vector space.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量将单词表示为多维连续的浮点数，其中语义相似的单词在几何空间中被映射到邻近的点。例如，“水果”和“叶子”这两个词会有相似的词向量，“树”。这是由于它们意义的相似性，而“电视”这个词在几何空间中则相对较远。换句话说，在相似上下文中使用的单词将被映射到邻近的向量空间。
- en: The word vectors can be of *n* dimensions, and *n* can take any number as input
    from the user creating it (for example 10, 70, 500). The dimensions are latent
    in the sense that it may not be apparent to humans what each of these dimensions
    means in reality. There are methods such as **Continuous Bag of Words** (**CBOW**)
    and **Skip-Gram** that enable conceiving the word vectors from the text provided
    as training input to word embedding algorithms. Also, the individual numbers in
    the word vector represent the word's distributed weight across dimensions. In
    a general sense, each dimension represents a latent meaning, and the word's numerical
    weight on that dimension captures the closeness of its association with and to
    that meaning. Thus, the semantics of the word are embedded across the dimensions
    of the vector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the word vectors are multidimensional and cannot be visualized directly,
    it is possible to visualize the vectors learned, by projecting them down to two
    dimensions using techniques such as the t-SNE dimensionality reduction technique.
    The following diagram displays learned word vectors in two dimensional spaces
    for country capitals, verb tenses, and gender relationships:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09d8ca48-57d7-4e72-a2f1-bf3c03b1a68a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Visualization of word embeddings in a two dimensional space
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: When we observe the word embedding visualization, we can perceive that the vectors
    captured some general, and in fact quite useful, semantic information about words
    and their relationships to one another. With this, each word in the text now can
    be represented as a row in the matrix similar to that of the BoW approach, but,
    unlike the BoW approach, it captures the relationships between the words.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of representing words as vectors is that they lend themselves
    to mathematical operators. For example, we can add and subtract vectors. The canonical
    example here is showing that by using word vectors we can determine the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '*king - man + woman = queen*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: In the given example, we subtracted the gender (man) from the word vector for
    king and added another gender (woman), and we obtained a new word vector from
    the operation (*king - man + woman*) that maps most closely to the word vector
    for queen.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'A few more amazing examples of mathematical operations that can be achieved
    on word vectors are shown as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two words, we can establish the degree of similarity between them:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And the output is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finding the odd one out from the set of words given as input:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The odd one is given as the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Derive analogies, for example:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, what it all means for us is that machines are able to identify semantically
    similar words given in a sentence. The following diagram is a gag related to word
    embedding that made me laugh, but the gag does convey the power of word embedding
    application, which otherwise would not be possible with the BoW kind of text representations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e972fc21-e370-464a-9b08-22fef9b041bb.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: A gag demonstrating the power of word embeddings application
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques that can be used to learn word embedding from text
    data. Word2vec, GloVe, and fastText are some of the popular techniques. Each of
    these techniques allows us to either train our own word embedding from the text
    data we have, or use the readily available pretrained vectors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: This approach of learning our own word embedding requires a lot of training
    data and can be slow, but this option will learn an embedding both targeted to
    the specific text data and the NLP task at hand.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained word embedding vectors are vectors that are trained on large amounts
    of text data (usually billions of words) available on sources such as Wikipedia.
    These are generally high-quality word embedding vectors made available by companies
    such as Google or Facebook. We can download these pretrained vector files and
    consume them to obtain word vectors for the words in the text that we would like
    to classify or cluster.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with pretrained word2vec word embedding
    based on Reuters news corpus
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec was developed by Tomas Mikolov, et al. at Google in 2013 as a response
    to making the neural-network-based training of the embedding more efficient, and
    since then it has become the de facto standard for developing pretrained word
    embedding.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2vec introduced the following two different learning models to learn the
    word embedding:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**CBOW**: Learns the embedding by predicting the current word based on its
    context.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous Skip-Gram**: The continuous Skip-Gram model learns by predicting
    the surrounding words given a current word.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both CBOW and Skip-Gram methods of learning are focused on learning the words
    given their local usage context, where the context of the word itself is defined
    by a window of neighboring words. This window is a configurable parameter of the
    model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The `softmaxreg` library in R offers pretrained `word2vec` word embedding that
    can be used for building our sentiment analysis engine for the Amazon reviews
    data. The pretrained vector is built using the `word2vec` model, and it is based
    on the `Reuter_50_50` dataset, UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any delay, let''s get into the code and also review the approach followed
    in this code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s examine the `word2vec` pretrained emdeddings. It is just another data
    frame, and therefore can be reviewed through the regular `dim` and `View` commands
    as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will result in the following output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a78e718-5842-47c2-ab0d-09a046f7ebe5.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Here, let''s use the following `dim` command:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will result in the following output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From the preceding output, we can observe that there are `12853` words that
    have got word vectors in the pretrained vector. Each of the words is defined using
    20 dimensions, and these dimensions define the context of the words. In the next
    step, we can look up the word vector for each of the words in the review comments.
    As there are only 12,853 words in the pretrained word embedding, there is a possibility
    that we encounter a word that does not exist in the pretrained embedding. In such
    a case, the unidentified word is represented with a 20 dimension vector that is
    filled with zeros.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: We also need to understand that the word vectors are available only at a word
    level, and therefore in order to decode the entire review, we take the mean of
    all the word vectors of the words that made up the review. Let's review the concept
    of getting the word vector for a sentence from individual word vectors with an
    example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Assume the sentence we want to get the word vector for is, *it is very bright
    and sunny this morning*. Individual words that comprise the sentence are *it*,
    *is*, *very*, *bright*, *and*, *sunny*, *this*, and *morning*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can look up each of these words in the pretrained vector and get the
    corresponding word vectors as shown in the following table:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **dim1** | **dim2** | **dim3** | **.....** | **....** | **dim19**
    | **dim20** |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| `it` | -2.25 | 0.75 | 1.75 | -1.25 | -0.25 | -3.25 | -2.25 |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| `is` | 0.75 | 1.75 | 1.75 | -2.25 | -2.25 | 0.75 | -0.25 |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| `very` | -2.25 | 2.75 | 1.75 | -0.25 | 0.75 | 0.75 | -2.25 |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| `bright` | -3.25 | -3.25 | -2.25 | -1.25 | 0.75 | 1.75 | -0.25 |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| `and` | -0.25 | -1.25 | -2.25 | 2.75 | -3.25 | -0.25 | 1.75 |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| `sunny` | 0 | 0 | 0 | 0 | 0 | 0 | 0 |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| `this` | -2.25 | -3.25 | 2.75 | 0.75 | -0.25 | -0.25 | -0.25 |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| `morning` | -0.25 | -3.25 | -2.25 | 1.75 | 0.75 | 2.75 | 2.75 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: 'Now, we have word vectors that comprise the sentence. Please note that these
    are not actual word vector values but just are made up to demonstrate the approach.
    Also, observe that the word `sunny` is represented with zeros across the dimensions
    to symbolize that the word is not found in the pretrained word embedding. In order
    to get the word vector for the sentence, we just compute the mean of each dimension.
    The resulting vector is a 1 x 20 vector representing the sentence, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '| Sentence | -1.21875 | -0.71875 | 0.15625 | 0.03125 | -0.46875 | 0.28125 |
    -0.09375 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: 'The `softmaxreg` library offers the `wordEmbed` function where we could pass
    a sentence and ask it to compute the `mean` word vector for the sentence. The
    following code is a custom function that was created to apply the `wordEmbed`
    function on each of the Amazon reviews we have in hand. At the end of applying
    this function to the reviews dataset, we expect to have a *n* x 20 matrix that
    is the word vector representation of our reviews. The *n* in the *n* x 20 represents
    the number of rows and 20 is the number of dimensions through which each review
    is represented, as seen in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will result in the following output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af33d08a-c790-4859-ae29-5e9f48ab5d29.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Then we review `temp` using the `dim` command, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will result in the following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can see from the output that we have word vectors created for each of the
    reviews in our corpus. This data frame can now be used to build classification
    models using an ML algorithm. The following code to achieve classification is
    no different from the one we did for the BoW approach:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will result in the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding output shows that the Random Forest model object is successfully
    created. Of course, the model can be improved further; however we are not going
    to be doing that here as the focus is to demonstrate making use of word embeddings,
    rather than getting the best performing classifier.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, with the following code we make use of the Random Forest model to make
    predictions on the test data and then report out the performance:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will result in the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We see that we get a 62% accuracy from using the pretrained `word2vec` embeddings
    made out of the Reuters news group's dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with GloVe word embedding
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stanford University's Pennington, et al. developed an extension of the `word2vec`
    method that is called **Global Vectors for Word Representation** (**GloVe**) for
    efficiently learning word vectors.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: GloVe combines the global statistics of matrix factorization techniques, such
    as LSA, with the local context-based learning in `word2vec`. Also, unlike `word2vec`,
    rather than using a window to define local context, GloVe constructs an explicit
    word context or word co-occurrence matrix using statistics across the whole text
    corpus. As an effect, the learning model yields generally better word embeddings.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The `text2vec` library in R has a GloVe implementation that we could use to
    train to obtain word embeddings from our own training corpus. Alternatively, pretrained
    GloVe word embeddings can be downloaded and reused, similar to the way we did
    in the earlier `word2vec` pretrained embedding project covered in the previous
    section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block demonstrates the way in which GloVe word embeddings
    can be created and used for sentiment analysis, or, for that matter, any text
    classification task. We are not going to discuss explicitly the steps involved,
    since the code is already heavily commented with detailed explanations of each
    of the steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will result in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following uses the `glove` model to obtain the combined word vector:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will result in the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/644b36df-c202-4d9a-9433-2579473dd24a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'We make use of the `softmaxreg` library to obtain the mean word vector for
    each review. This is similar to what we did in `word2vec` pretrained embedding
    in the previous section. Observe that we are passing our own trained word embedding
    `word_vectors` to the `wordEmbed()` function, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will result in the following output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c48d11a0-df92-4dfa-8ee4-910afd3931bf.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'We will now split the dataset into train and test portions, and use the `randomforest`
    library to build a model to train, as shown in the following lines of code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will result in the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we use the Random Forest model created to predict labels, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will result in the following output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: With this method, we obtain an accuracy of 66%. This is despite the fact that
    the word embeddings are obtained from words in just 1,000 text samples. The model
    may be further improved by using a pretrained embedding. The overall framework
    to use the pretrained embedding remains the same as what we did in `word2vec`
    project in the previous section.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with fastText
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`fastText` is a library and is an extension of `word2vec` for word representation.
    It was created by the Facebook Research Team in 2016\. While Word2vec and GloVe
    approaches treat words as the smallest unit to train on, fastText breaks words
    into several n-grams, that is, subwords. For example, the trigrams for the word
    apple are app, ppl, and ple. The word embedding for the word apple is sum of all
    the word n-grams. Due to the nature of the algorithm''s embedding generation,
    fastText is more resource-intensive and takes additional time to train. Some of
    the advantages of `fastText` are as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: It generates better word embeddings for rare words (including misspelled words).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For out of vocabulary words, fastText can construct the vector for a word from
    its character n-grams, even if a word doesn't appear in training corpus. This
    is not a possibility for both Word2vec and GloVe.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fastTextR` library provides an interface to the fastText. Let's make use
    of the `fastTextR` library for our project to build a sentiment analysis engine
    on Amazon reviews. While it is possible to download pretrained fastText word embedding
    and make use of it for our project, let's make an attempt to train a word embedding
    based on the reviews dataset we have in hand. It should be noted that the approach
    in terms of making use of fastText pretrained word embedding is similar to the
    approach we followed in the `word2vec` based project that we dealt with earlier.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the project covered in the previous section, comments are included
    inline in the code. The comments explain each of the lines indicating the approach
    taken to build the Amazon reviews sentiment analyzer in this project. Let''s look
    into the following code now:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This will result in the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26daddc5-2f6b-4dbf-b95a-0b13ec3705a7.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s divide the reviews into training and test datasets, and view them
    using the following lines of code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This will give the following output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e27d1e8e-4ef3-48da-853a-807bf9c58c63.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to view the test dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will give the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eed25069-abad-4348-bac7-0b4b5b15f278.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'We will now create a `.txt` file for the train and test dataset using the following
    code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now we will view the no labels test dataset for confirmation using the following
    command:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will result in the following output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70533eb-7e36-4f83-a262-00279dcf2f66.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now write the no labels test dataset to a file so we can use it for
    testing, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will result in the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa10b4e0-e95f-4b37-b32f-191a4f03d45f.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Now we will obtain the word vectors from a previously trained model and view
    the word vectors for each word in our training dataset, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will result in the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a2104e3-a082-4b1b-b99b-d0b9c33536ed.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'We will predict the labels for the reviews in the no labels test dataset and
    write it to a file for future reference. Then we will get the predictions into
    a data frame to compute the performance and see the estimate of the accuracy using
    the following lines of code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will result in the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We have a 58% accuracy with the `fastText` method on our reviews data. As a
    next step, we could check whether the accuracy may be further improved by making
    use of fastText pretrained word embedding. As we already know, implementing a
    project by making use of pretrained embedding is not very different from the implementation
    that we followed in the `word2vec` project described in the earlier section of
    this chapter. The difference is just that the training step to obtain word embedding
    needs to be discarded and the model variable in the code covered in this project
    code should be initiated with the pretrained word embeddings.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned various NLP techniques, namely BoW, Word2vec, GloVe,
    and fastText. We built projects involving these techniques to perform sentiment
    analysis on an Amazon reviews dataset. The projects that were built involved two
    approaches, making use of pretrained word embeddings and building the word embeddings
    from our own dataset. We tried both these approaches to represent text in a format
    that can be consumed by ML algorithms that resulted in models with the ability
    to perform sentiment analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about customer segmentation by making use
    of a wholesale dataset. We will look at customer segmentation as an unsupervised
    problem and build projects with various techniques that can identify inherent
    groups within the e-commerce company's customer base. Come, let's explore the
    world of building an e-commerce customer segmentation engine with ML!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
