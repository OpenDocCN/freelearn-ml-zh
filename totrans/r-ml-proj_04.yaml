- en: Sentiment Analysis of Amazon Reviews with NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every day, we generate data from emails, online posts such as blogs, social
    media comments, and more. It is not surprising to say that unstructured text data
    is much larger in size than the tabular data that exists in the databases of any
    organization. It is important for organizations to acquire useful insights from
    the text data pertaining to the organization. Due to the different nature of the
    text data when compared to data in databases, the methods that need to be employed
    to understand the text data are different. In this chapter, we will learn a number
    of key techniques in **natural language processing** (**NLP**) that help us to
    work on text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The common definition of NLP is as follows: an area of computer science and
    artificial intelligence that deals with the interactions between computers and
    human (natural) languages; in particular, how to program computers to fruitfully
    process large amounts of natural language data.'
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, NLP deals with understanding human speech as it is spoken.
    It helps machines read and understand "text".
  prefs: []
  type: TYPE_NORMAL
- en: Human languages are highly complex and several ambiguities need to be resolved
    in order to correctly comprehend the spoken language or written text. In the area
    of NLP, several techniques are applied in order to deal with these ambiguities,
    including the **Part-of-Speech** (**POS**) tagger, term disambiguation, entity
    extraction, relations' extraction, key term recognition, and more.
  prefs: []
  type: TYPE_NORMAL
- en: For natural language systems to work successfully, a consistent knowledge base,
    such as a detailed thesaurus, a lexicon of words, a dataset for linguistic and
    grammatical rules, an ontology, and up-to-date entities, are prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be noted that NLP is concerned with understanding the text from not
    just the syntactic perspective, but also from a semantic perspective. Similar
    to humans, the idea is for the machines to be able to perceive underlying messages
    behind the spoken words and not just the structure of words in sentences. There
    are numerous application areas of NLP, and the following are just a few of these:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual agents or chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the NLP subject area in itself is very vast, it is not practical to cover
    all the areas in just one chapter. Therefore, we will be focusing on "text classification"
    in this chapter. We do this by implementing a project that performs sentiment
    analysis in the reviews expressed by Amazon.com customers. Sentiment analysis
    is a type of text classification task where we classify each of the documents
    (reviews) into one of the possible categories. The possible categories could be
    positive, negative, or neutral, or it could be positive, negative, or a rating
    on a scale of 1 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: Text documents that need to be classified cannot be input directly to a machine
    learning algorithm. Each of the documents needs to be represented in a certain
    format that is acceptable for the ML algorithm as input to work on. In this chapter,
    we explore, implement, and understand the **Bag of Words** (**BoW**) word embedding
    approaches. These are approaches in which text can be represented.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we progress with the chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The sentiment analysis problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Amazon reviews dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with the BoW approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding word embedding approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with pretrained Word2Vec word embedding
    based on Reuters news corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with GloVe word embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with fastText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sentiment analysis problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is one of the most general text classification applications.
    The purpose of it is to analyze messages such as user reviews, and feedback from
    employees, in order to identify whether the underlying sentiment is positive,
    negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and reporting sentiment in texts allows businesses to quickly get
    a consolidated high-level insight without having to read each one of the comments
    received.
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to generate holistic sentiment based on the overall comments
    received, there is also an extended area called **aspect-based sentiment analysis**.
    It is focused on deriving sentiment based on each area of the service. For example,
    a customer that visited a restaurant when writing a review would generally cover
    areas such as ambience, food quality, service quality, and price. Though the feedback
    about each of the areas may not be quoted under a specific heading, the sentences
    in the review comments would naturally cover the customer's opinion of one or
    more of these areas. Aspect-based sentiment analysis attempts to identify the
    sentences in the reviews in each of the areas and then identify whether the sentiment
    is positive, negative, or neutral. Providing sentiment by each area helps businesses
    quickly identify their weak areas.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss and implement methods that are aimed at identifying
    the overall sentiment from the review texts. The task can be achieved in several
    ways, ranging from a simple lexicon method to a complex word embedding method.
  prefs: []
  type: TYPE_NORMAL
- en: A **lexicon** method is not really a machine learning method. It is more a rule
    based method that is based on a predefined positive and negative words dictionary.
    The method involves looking up the number of positive words and negative words
    in each review. If the count of positive words in the review is more than the
    count of negative words, then the review is marked as positive, otherwise it is
    marked as negative. If there are an equal number of positive and negative words,
    then the review is marked as neutral. As implementing this method is straightforward,
    and as it comes with a requirement for a predefined dictionary, we will not cover
    the implementation of the lexicon method in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to consider the sentiment analysis problem as an unsupervised
    clustering problem, in this chapter we consider it as a supervised classification
    problem. This is because, we have the Amazon reviews labeled dataset available.
    We can make use of these labels to build classification models, and therefore,
    the supervised algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is available for download and use at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) .'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Amazon reviews dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the Amazon product reviews polarity dataset for the various projects
    in this chapter. It is an open dataset constructed and made available by Xiang
    Zhang. It is used as a text classification benchmark in the paper: *Character-level
    Convolutional Networks for Text Classification* and *Advances in Neural Information
    Processing Systems* 28,*Xiang Zhang, Junbo Zhao, Yann LeCun,** (NIPS 2015)*.'
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon reviews polarity dataset is constructed by taking review score 1
    and 2 as negative, 4 and 5 as positive. Samples of score 3 are ignored. In the
    dataset, class 1 is the negative and class 2 is the positive. The dataset has
    1,800,000 training samples and 200,000 testing samples.
  prefs: []
  type: TYPE_NORMAL
- en: The `train.csv` and `test.csv` files contains all the samples as comma-separated
    values. There are three columns in them, corresponding to class index (1 or 2),
    review title, and review text. The review title and text are escaped using double
    quotes ("), and any internal double quote is escaped by 2 double quotes ("").
    New lines are escaped by a backslash followed with an "n" character that is "\n".
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that we are able to run our projects, even with minimal infrastructure,
    let''s restrict the number of records to be considered in our dataset to 1,000
    records only. Of course, the code that we use in the projects can be extended
    to any number of records, as long as the hardware infrastructure support is available.
    Let''s first read the data and visualize the records with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6bce54f-dc46-455b-ad50-0c6c06ae41ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Post reading the file, we can see that there is only one column in the dataset
    and this column had both the review text and the sentiment components in it. We
    will slightly modify the format of the dataset for the purpose of using it with
    sentiment analysis projects in this chapter involving the BoW, Word2vec, and GloVe
    approaches. Let''s modify the format of the dataset with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b22882-b09c-4e3e-9e20-92f7bbbed8f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have two columns in our dataset. However, there is unnecessary punctuation
    that exists in both the columns that may cause problems with processing the dataset
    further. Let''s attempt to remove the punctuation with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b22278e-b011-4631-bb32-b1e1742aba4f.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we see that we have a clean dataset that is ready
    for use. Also, we have written the output to a file. When we build the sentiment
    analyzer, we can start directly reading the dataset from the `Sentiment Analysis
    Dataset.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fastText algorithm expects the dataset to be in a different format. The
    data input to fastText should comply the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `X` is the class name. Text is the actual review text that
    led to the rating specified under the class. Both the rating and text should be
    placed on one line with no quotes. The classes are `__label__1` and `__label__2`,
    and there should be only one class per row. Let''s accomplish the `fastText` library
    required format with the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From the output of basic EDA code, we can see that the dataset is in the required
    format, therefore we can proceed to our next section of implementing the sentiment
    analysis engine using the BoW approach. Along side the implementation, we will
    delve into learning the concept behind the approach, and explore the sub-techniques
    that can be leveraged in the approach to obtain better results.
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with the BoW approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The intent of the BoW approach is to convert the review text provided into
    a matrix form. It represents documents as a set of distinct words by ignoring
    the order and meaning of the words. Each row of the matrix represents each review
    (otherwise called a document in NLP), and the columns represent the universal
    set of words present in all the reviews. For each document, and across each word,
    the existence of the word, or the frequency of the word occurrence, in that specific
    document is recorded. Finally, the matrix created from word frequency vectors
    represents the documents set. This methodology is used to create input datasets
    that are required to train the models, and also to prepare the test dataset that
    need to be used by the trained models to perform text classification. Now that
    we understand the BoW motivation, let''s jump into implementing the steps to build
    a sentiment analysis classifier based on this approach, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/068d0389-1c4c-418b-b6c3-ecb897f30581.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first step in processing text data involves creating a *corpus*, which
    is a collection of text documents. The `VCorpus` function in the `tm` package
    enables conversion of the reviews comments column in the data frame into a volatile
    corpus. This can be achieved through the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: From the volatile corpus, we create a **Document Term Matrix** (**DTM**). A
    DTM is a sparse matrix that is created using the `tm` library's `DocumentTermMatrix`
    function. The rows of the matrix indicate documents and the columns indicate features,
    that is, words. The matrix is sparse because all unique unigram sets of the dataset
    become columns in DTM and, as each review comment does not have all elements of
    the unigram set, most cells will have a 0, indicating the absence of the unigram.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is possible to extract n-grams (unigrams, bigrams, trigrams, and so
    on) as part of the BoW approach, the tokenize parameter can be set and passed
    as part of the control list in the `DocumentTermMatrix` function to accomplish
    n-grams in DTM. It must be noted that using n-grams as part of the DTM creates
    a very high number of columns in the DTM. This is one of the demerits of the BoW
    approach, and, in some cases, it could stall the execution of the project due
    to limited memory. As our specific case is also limited by hardware infrastructure,
    we restrict ourselves by including only the unigrams in DTM in this project. Apart
    from just generating unigrams, we also perform some additional processing on the
    reviews text document by passing parameters to the control list in the `tm` library''s
    `DocumentTermMatrix` function. The processing we do on the review text documents
    during the creation of the DTM is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the case of the text to lowercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove any numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stop words using the English language stop word list from the Snowball
    stemmer project.  Stop words are common words, such as a, an, in, and the, that
    do not add value in deciding sentiment based on review comments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove punctuation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform stemming, which aims at resolving a word into the base form of the word,
    that is, strip the plural *s* from nouns, the *ing* from verbs, or other affixes.
    A stem is a natural group of words with equal or very similar meaning. After the
    stemming process, every word is represented by its stem. The `SnowballC` library
    provides the capability to obtain the root for each of the words in the review
    comments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now create a DTM from the volatile corpus and do the text preprocessing
    with the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We see from the output that there are 1,000 documents that were processed and
    form rows in the matrix. There are 5,794 columns representing unique unigrams
    from the reviews following the additional text processing. We also see that the
    DTM is 99% sparse and consists of non-zero entries only in 34,494 cells. The non-zero
    cells represent the frequency of occurrence of the word on the column in the document
    represent on the row of the DTM. The weighting is done through the default 'term
    frequency' weighting, as we did not specify any weighting parameter in the control
    list supplied to the `DocumentTermMatrix` function. Other forms of weighting,
    such as **term frequency-inverse document frequency** (**TFIDF**), are also possible
    just by passing the appropriate weight parameter in the control list to the `DocumentTermMatrix`
    function. For now, we will stick to weighting based on term frequency, which is
    the default. We also see from the `inspect` function that several sample documents
    were output along with the term frequencies in these documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DTM tends to get very big, even for normal sized datasets. Removing sparse
    terms, that is, terms occurring only in very few documents, is the technique that
    can be tried to reduce the size of the matrix without losing significant relations
    inherent to the matrix. Let''s remove sparse columns from the matrix. We will
    attempt to remove those terms that have at least a 99% of sparse elements with
    the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now see from the output of the `inspect` function that the sparsity of the
    matrix is reduced to 97%, and the number of unigrams (columns of the matrix) is
    reduced to `686`. We are now ready with the DTM that can be used for training
    with any machine learning classification algorithm. In the next few lines of code,
    let''s attempt to divide our DTM into training and test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using a machine learning algorithm called **Naive Bayes** to create
    a model. Naive Bayes is generally trained on data with nominal features. We can
    observe that the cells in our DTM are numeric and therefore need to be converted
    to nominal prior to feeding the dataset as input for creating the model with Naive
    Bayes. As each cell indicates the word frequency in the review, and as the number
    of times a word used in the review does not impact sentiment, let''s write a function
    to convert the cell values with a non-zero value to `Y`, and in case of a zero,
    let''s convert it to `N`, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s apply the function on all rows of the training dataset, and test
    dataset we have previously created in this project with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c894b708-f3b8-4a46-9951-4f55652ce045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see from the output that all the cells in the training and test DTMs
    are now converted to nominal values. Thus, let''s proceed to build a text sentiment
    analysis classifier using the Naive Bayes algorithm from the `e1071` library,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding summary output shows that the `nb_senti_classifier` object is
    successfully created from the training DTM. Let''s now use the model object to
    predict sentiment on the test data DTM. In the following code block, we are instructing
    that the predictions should be classes and not prediction probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following code, let us now compute the accuracy of the model using
    the `mmetric` function in the `rminer` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We achieved a 79% accuracy just with a very quick and basic BoW model. The model
    can be further improved by means of techniques such as parameter tuning, lemmatization,
    new features creation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of the BoW approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of both the theory and implementation of the
    BoW approach, let's examine the pros and cons of the approach. When it comes to
    pros, the BoW approach is very simple to understand and implement and therefore
    offers a lot of flexibility for customization on any text dataset. It may be observed
    that the BoW approach does not retain the order of words specifically when only
    unigrams are considered. This problem is generally overcome by retaining n-grams
    in the DTM. However, it comes at the cost as larger infrastructure is needed to
    process the text and build a classifier. Another severe drawback of the approach
    is that it does not respect the semantics of the word. For example, the words
    "car" and "automobile" are often used in the same context. A model built based
    on BoW treats the sentences "buy used cars" and "purchase old automobiles" as
    very different sentences. While these sentences are the same, BoW models do not
    classify these sentences as the same, as the words in these sentences are not
    matching. It is possible to consider the semantics of words in sentences using
    an approach called word embedding. This is something we will explore in our next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The BoW models that we discussed in our earlier section suffer from a problem
    that they do not capture information about a word’s meaning or context. This means
    that potential relationships, such as contextual closeness, are not captured across
    collections of words. For example, the approach cannot capture simple relationships,
    such as determining that the words "cars" and "buses" both refer to vehicles that
    are often discussed in the context of transportation. This problem that we experience
    with the BoW approach will be overcome by word embedding, which is an improved
    approach to mapping semantically similar words.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors represent words as multidimensional continuous floating point numbers,
    where semantically similar words are mapped to proximate points in geometric space.
    For example, the words *fruit* and *leaves* would have a similar word vector, *tree*.
    This is due to the similarity of their meanings, whereas the word *television* would
    be quite distant in the geometrical space. In other words, words that are used
    in a similar context will be mapped to a proximate vector space.
  prefs: []
  type: TYPE_NORMAL
- en: The word vectors can be of *n* dimensions, and *n* can take any number as input
    from the user creating it (for example 10, 70, 500). The dimensions are latent
    in the sense that it may not be apparent to humans what each of these dimensions
    means in reality. There are methods such as **Continuous Bag of Words** (**CBOW**)
    and **Skip-Gram** that enable conceiving the word vectors from the text provided
    as training input to word embedding algorithms. Also, the individual numbers in
    the word vector represent the word's distributed weight across dimensions. In
    a general sense, each dimension represents a latent meaning, and the word's numerical
    weight on that dimension captures the closeness of its association with and to
    that meaning. Thus, the semantics of the word are embedded across the dimensions
    of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the word vectors are multidimensional and cannot be visualized directly,
    it is possible to visualize the vectors learned, by projecting them down to two
    dimensions using techniques such as the t-SNE dimensionality reduction technique.
    The following diagram displays learned word vectors in two dimensional spaces
    for country capitals, verb tenses, and gender relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09d8ca48-57d7-4e72-a2f1-bf3c03b1a68a.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of word embeddings in a two dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: When we observe the word embedding visualization, we can perceive that the vectors
    captured some general, and in fact quite useful, semantic information about words
    and their relationships to one another. With this, each word in the text now can
    be represented as a row in the matrix similar to that of the BoW approach, but,
    unlike the BoW approach, it captures the relationships between the words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of representing words as vectors is that they lend themselves
    to mathematical operators. For example, we can add and subtract vectors. The canonical
    example here is showing that by using word vectors we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king - man + woman = queen*'
  prefs: []
  type: TYPE_NORMAL
- en: In the given example, we subtracted the gender (man) from the word vector for
    king and added another gender (woman), and we obtained a new word vector from
    the operation (*king - man + woman*) that maps most closely to the word vector
    for queen.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few more amazing examples of mathematical operations that can be achieved
    on word vectors are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two words, we can establish the degree of similarity between them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding the odd one out from the set of words given as input:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The odd one is given as the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Derive analogies, for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, what it all means for us is that machines are able to identify semantically
    similar words given in a sentence. The following diagram is a gag related to word
    embedding that made me laugh, but the gag does convey the power of word embedding
    application, which otherwise would not be possible with the BoW kind of text representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e972fc21-e370-464a-9b08-22fef9b041bb.png)'
  prefs: []
  type: TYPE_IMG
- en: A gag demonstrating the power of word embeddings application
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques that can be used to learn word embedding from text
    data. Word2vec, GloVe, and fastText are some of the popular techniques. Each of
    these techniques allows us to either train our own word embedding from the text
    data we have, or use the readily available pretrained vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This approach of learning our own word embedding requires a lot of training
    data and can be slow, but this option will learn an embedding both targeted to
    the specific text data and the NLP task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained word embedding vectors are vectors that are trained on large amounts
    of text data (usually billions of words) available on sources such as Wikipedia.
    These are generally high-quality word embedding vectors made available by companies
    such as Google or Facebook. We can download these pretrained vector files and
    consume them to obtain word vectors for the words in the text that we would like
    to classify or cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with pretrained word2vec word embedding
    based on Reuters news corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec was developed by Tomas Mikolov, et al. at Google in 2013 as a response
    to making the neural-network-based training of the embedding more efficient, and
    since then it has become the de facto standard for developing pretrained word
    embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2vec introduced the following two different learning models to learn the
    word embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CBOW**: Learns the embedding by predicting the current word based on its
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous Skip-Gram**: The continuous Skip-Gram model learns by predicting
    the surrounding words given a current word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both CBOW and Skip-Gram methods of learning are focused on learning the words
    given their local usage context, where the context of the word itself is defined
    by a window of neighboring words. This window is a configurable parameter of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The `softmaxreg` library in R offers pretrained `word2vec` word embedding that
    can be used for building our sentiment analysis engine for the Amazon reviews
    data. The pretrained vector is built using the `word2vec` model, and it is based
    on the `Reuter_50_50` dataset, UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any delay, let''s get into the code and also review the approach followed
    in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the `word2vec` pretrained emdeddings. It is just another data
    frame, and therefore can be reviewed through the regular `dim` and `View` commands
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a78e718-5842-47c2-ab0d-09a046f7ebe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, let''s use the following `dim` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can observe that there are `12853` words that
    have got word vectors in the pretrained vector. Each of the words is defined using
    20 dimensions, and these dimensions define the context of the words. In the next
    step, we can look up the word vector for each of the words in the review comments.
    As there are only 12,853 words in the pretrained word embedding, there is a possibility
    that we encounter a word that does not exist in the pretrained embedding. In such
    a case, the unidentified word is represented with a 20 dimension vector that is
    filled with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to understand that the word vectors are available only at a word
    level, and therefore in order to decode the entire review, we take the mean of
    all the word vectors of the words that made up the review. Let's review the concept
    of getting the word vector for a sentence from individual word vectors with an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Assume the sentence we want to get the word vector for is, *it is very bright
    and sunny this morning*. Individual words that comprise the sentence are *it*,
    *is*, *very*, *bright*, *and*, *sunny*, *this*, and *morning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can look up each of these words in the pretrained vector and get the
    corresponding word vectors as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **dim1** | **dim2** | **dim3** | **.....** | **....** | **dim19**
    | **dim20** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `it` | -2.25 | 0.75 | 1.75 | -1.25 | -0.25 | -3.25 | -2.25 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `is` | 0.75 | 1.75 | 1.75 | -2.25 | -2.25 | 0.75 | -0.25 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `very` | -2.25 | 2.75 | 1.75 | -0.25 | 0.75 | 0.75 | -2.25 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `bright` | -3.25 | -3.25 | -2.25 | -1.25 | 0.75 | 1.75 | -0.25 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `and` | -0.25 | -1.25 | -2.25 | 2.75 | -3.25 | -0.25 | 1.75 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `sunny` | 0 | 0 | 0 | 0 | 0 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `this` | -2.25 | -3.25 | 2.75 | 0.75 | -0.25 | -0.25 | -0.25 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `morning` | -0.25 | -3.25 | -2.25 | 1.75 | 0.75 | 2.75 | 2.75 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we have word vectors that comprise the sentence. Please note that these
    are not actual word vector values but just are made up to demonstrate the approach.
    Also, observe that the word `sunny` is represented with zeros across the dimensions
    to symbolize that the word is not found in the pretrained word embedding. In order
    to get the word vector for the sentence, we just compute the mean of each dimension.
    The resulting vector is a 1 x 20 vector representing the sentence, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sentence | -1.21875 | -0.71875 | 0.15625 | 0.03125 | -0.46875 | 0.28125 |
    -0.09375 |'
  prefs: []
  type: TYPE_TB
- en: 'The `softmaxreg` library offers the `wordEmbed` function where we could pass
    a sentence and ask it to compute the `mean` word vector for the sentence. The
    following code is a custom function that was created to apply the `wordEmbed`
    function on each of the Amazon reviews we have in hand. At the end of applying
    this function to the reviews dataset, we expect to have a *n* x 20 matrix that
    is the word vector representation of our reviews. The *n* in the *n* x 20 represents
    the number of rows and 20 is the number of dimensions through which each review
    is represented, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af33d08a-c790-4859-ae29-5e9f48ab5d29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we review `temp` using the `dim` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see from the output that we have word vectors created for each of the
    reviews in our corpus. This data frame can now be used to build classification
    models using an ML algorithm. The following code to achieve classification is
    no different from the one we did for the BoW approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that the Random Forest model object is successfully
    created. Of course, the model can be improved further; however we are not going
    to be doing that here as the focus is to demonstrate making use of word embeddings,
    rather than getting the best performing classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, with the following code we make use of the Random Forest model to make
    predictions on the test data and then report out the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We see that we get a 62% accuracy from using the pretrained `word2vec` embeddings
    made out of the Reuters news group's dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with GloVe word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stanford University's Pennington, et al. developed an extension of the `word2vec`
    method that is called **Global Vectors for Word Representation** (**GloVe**) for
    efficiently learning word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: GloVe combines the global statistics of matrix factorization techniques, such
    as LSA, with the local context-based learning in `word2vec`. Also, unlike `word2vec`,
    rather than using a window to define local context, GloVe constructs an explicit
    word context or word co-occurrence matrix using statistics across the whole text
    corpus. As an effect, the learning model yields generally better word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The `text2vec` library in R has a GloVe implementation that we could use to
    train to obtain word embeddings from our own training corpus. Alternatively, pretrained
    GloVe word embeddings can be downloaded and reused, similar to the way we did
    in the earlier `word2vec` pretrained embedding project covered in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block demonstrates the way in which GloVe word embeddings
    can be created and used for sentiment analysis, or, for that matter, any text
    classification task. We are not going to discuss explicitly the steps involved,
    since the code is already heavily commented with detailed explanations of each
    of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following uses the `glove` model to obtain the combined word vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/644b36df-c202-4d9a-9433-2579473dd24a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We make use of the `softmaxreg` library to obtain the mean word vector for
    each review. This is similar to what we did in `word2vec` pretrained embedding
    in the previous section. Observe that we are passing our own trained word embedding
    `word_vectors` to the `wordEmbed()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c48d11a0-df92-4dfa-8ee4-910afd3931bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now split the dataset into train and test portions, and use the `randomforest`
    library to build a model to train, as shown in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the Random Forest model created to predict labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: With this method, we obtain an accuracy of 66%. This is despite the fact that
    the word embeddings are obtained from words in just 1,000 text samples. The model
    may be further improved by using a pretrained embedding. The overall framework
    to use the pretrained embedding remains the same as what we did in `word2vec`
    project in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a text sentiment classifier with fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`fastText` is a library and is an extension of `word2vec` for word representation.
    It was created by the Facebook Research Team in 2016\. While Word2vec and GloVe
    approaches treat words as the smallest unit to train on, fastText breaks words
    into several n-grams, that is, subwords. For example, the trigrams for the word
    apple are app, ppl, and ple. The word embedding for the word apple is sum of all
    the word n-grams. Due to the nature of the algorithm''s embedding generation,
    fastText is more resource-intensive and takes additional time to train. Some of
    the advantages of `fastText` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It generates better word embeddings for rare words (including misspelled words).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For out of vocabulary words, fastText can construct the vector for a word from
    its character n-grams, even if a word doesn't appear in training corpus. This
    is not a possibility for both Word2vec and GloVe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fastTextR` library provides an interface to the fastText. Let's make use
    of the `fastTextR` library for our project to build a sentiment analysis engine
    on Amazon reviews. While it is possible to download pretrained fastText word embedding
    and make use of it for our project, let's make an attempt to train a word embedding
    based on the reviews dataset we have in hand. It should be noted that the approach
    in terms of making use of fastText pretrained word embedding is similar to the
    approach we followed in the `word2vec` based project that we dealt with earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the project covered in the previous section, comments are included
    inline in the code. The comments explain each of the lines indicating the approach
    taken to build the Amazon reviews sentiment analyzer in this project. Let''s look
    into the following code now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26daddc5-2f6b-4dbf-b95a-0b13ec3705a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s divide the reviews into training and test datasets, and view them
    using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e27d1e8e-4ef3-48da-853a-807bf9c58c63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the following code to view the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eed25069-abad-4348-bac7-0b4b5b15f278.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now create a `.txt` file for the train and test dataset using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will view the no labels test dataset for confirmation using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70533eb-7e36-4f83-a262-00279dcf2f66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now write the no labels test dataset to a file so we can use it for
    testing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa10b4e0-e95f-4b37-b32f-191a4f03d45f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will obtain the word vectors from a previously trained model and view
    the word vectors for each word in our training dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a2104e3-a082-4b1b-b99b-d0b9c33536ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will predict the labels for the reviews in the no labels test dataset and
    write it to a file for future reference. Then we will get the predictions into
    a data frame to compute the performance and see the estimate of the accuracy using
    the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We have a 58% accuracy with the `fastText` method on our reviews data. As a
    next step, we could check whether the accuracy may be further improved by making
    use of fastText pretrained word embedding. As we already know, implementing a
    project by making use of pretrained embedding is not very different from the implementation
    that we followed in the `word2vec` project described in the earlier section of
    this chapter. The difference is just that the training step to obtain word embedding
    needs to be discarded and the model variable in the code covered in this project
    code should be initiated with the pretrained word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned various NLP techniques, namely BoW, Word2vec, GloVe,
    and fastText. We built projects involving these techniques to perform sentiment
    analysis on an Amazon reviews dataset. The projects that were built involved two
    approaches, making use of pretrained word embeddings and building the word embeddings
    from our own dataset. We tried both these approaches to represent text in a format
    that can be consumed by ML algorithms that resulted in models with the ability
    to perform sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about customer segmentation by making use
    of a wholesale dataset. We will look at customer segmentation as an unsupervised
    problem and build projects with various techniques that can identify inherent
    groups within the e-commerce company's customer base. Come, let's explore the
    world of building an e-commerce customer segmentation engine with ML!
  prefs: []
  type: TYPE_NORMAL
