- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Fake News with Graph Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked at tabular data, which was comprised of
    individual data points with their own features. While modeling and running our
    experiments, we did not consider any features of the relationship among the data
    points. Much real-world data, particularly that in the domain of cybersecurity,
    can naturally occur as graphs and be represented as a set of nodes, some of which
    are connected using edges. Examples include social networks, where users, photos,
    and posts can be connected using edges. Another example is the internet, which
    is a large graph of computers connected to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional machine learning algorithms cannot directly learn from graphs. Algorithms
    such as regression, neural networks, and trees, and optimization techniques such
    as gradient descent are designed to operate on Euclidean (flat) data structures.
    This has led to the development of **Graph Neural Networks** (**GNNs**), an upcoming
    area of research in the field of machine learning. This has found tremendous applications
    in cybersecurity, particularly in areas such as botnets, fake news detection,
    and fraud analytics. This chapter will focus on detecting fake news using GNNs.
    We will first cover the basics of graph theory, followed by how graph machine
    learning can be used as a tool to frame a security problem. Although we will learn
    how to use GNN models to detect fake news, the techniques we will introduce are
    generic and can be applied to multiple problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning on graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fake news detection with GNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have an understanding of how certain data
    can be modeled as graphs, and how to apply graph machine learning for effective
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208).
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let us understand what graphs are and the key terms related to graphs.
  prefs: []
  type: TYPE_NORMAL
- en: What is a graph?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A graph is a data structure that is represented as a set of nodes connected
    by a set of edges. Mathematically, we specify a graph *G* as (*V, E*), where *V*
    represents the nodes or vertices and *E* represents the edges between them, as
    shown in *Figure 8**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A simple graph](img/B19327_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A simple graph
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous graph, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: V = {1, 2, 3, 4, 5, 6}
  prefs: []
  type: TYPE_NORMAL
- en: E = {(1,3), (2,3), (2,5), (3,6), (4,6), (5.6)}
  prefs: []
  type: TYPE_NORMAL
- en: Note that the order in which the nodes and edges are mentioned does not matter.
    The graph shown in *Figure 8**.1* is an undirected graph, which means that the
    direction of the edges does not matter. There can also be directed graphs in which
    the definition of the edge has some meaning, which gives importance to the direction
    of the edge. For example, a graph depicting the water flow of from various cities
    would have directed edges, as water flowing from city A to city B does not imply
    that water flows from B to A.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a directed graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – An example of a directed graph](img/B19327_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – An example of a directed graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we would define the graph as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: V = {1, 2, 3, 4, 5, 6, 7, 8}
  prefs: []
  type: TYPE_NORMAL
- en: E = {(1,4), (4,1), (1,3), (2,3), (3,5), (5,7), (6,2), (7,3), (8,3)}
  prefs: []
  type: TYPE_NORMAL
- en: Note that here, as it is a directed graph, the order in which nodes in each
    edge tuple are specified matters. We have added both `(1,4)` and `(4,1)`.
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, the edges are unmarked (that is, there is no weight associated
    with an edge). Such a graph is called an unweighted graph. A graph in which there
    are weights associated with edges is called a weighted graph. Edge weights can
    represent properties of relationships between nodes, or the intensity of relationships.
    For example, in a highway transport network, where nodes represent cities, the
    edge weights can depict the distance on the highway or the time taken to traverse
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: Representing graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed, a graph can be represented by a set of vertices and edges between
    vertices. On a computer, or inside a program, a graph can be represented in one
    of two ways – an adjacency matrix or an adjacency list.
  prefs: []
  type: TYPE_NORMAL
- en: An adjacency matrix is an *N* x *N* matrix, where *N* is the total number of
    nodes in the graph. Entries in the matrix denote edge relations between the nodes.
    If *A* is the adjacency matrix, then *A*ij = 1 if there is an edge between the
    nodes *i* and *j*. If the graph is undirected, then we have *A*ij = *A*ji; if
    the graph is directed, then *A*ij = 1 means that there is an edge from node *i*
    to node *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following directed graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A sample-directed graph](img/B19327_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – A sample-directed graph
  prefs: []
  type: TYPE_NORMAL
- en: 'For this directed graph, the adjacency matrix would be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0 | 0 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – An adjacency matrix corresponding to the graph in Figure 8.3
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if the graph is weighted, then the entries in the adjacency matrix
    can be modified to represent the weight instead.
  prefs: []
  type: TYPE_NORMAL
- en: While adjacency matrices are interpretable and straightforward to understand,
    they are computationally expensive. As the number of nodes in the graph increases,
    the size of the adjacency matrix also increases. In technical terms, the space
    complexity is *O(N*2*)*, where *N* is the number of nodes. Larger adjacency matrices
    are difficult to store and also require a significant processing overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to represent a graph is an adjacency list. In this form
    of representation, the graph is stored as a dictionary or hash table. The keys
    represent the nodes, and the value is a list of nodes that the key node has an
    edge to. The list of nodes is represented in memory as a linked list. For the
    graph in *Figure 8**.3*, the adjacency list representation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – An adjacency list representation of the graph shown in Figure
    8.3](img/B19327_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – An adjacency list representation of the graph shown in Figure 8.3
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can clearly see the nodes that a given node has an edge to. Even when
    the number of nodes is large and the edges are sparse, the adjacency list turns
    out to be an efficient representation.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs in the real world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much real-world data in nearly all domains of science can be naturally represented
    as networks using graphs. In the chemical and medical sciences, drugs can be represented
    as a network of molecules, which are themselves networks of atoms. In information
    systems and the internet, a graph can be constructed that represents machines
    and the connections among them. In the field of engineering, a graph can be constructed
    to represent cities, countries, and the transport networks within them.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs provide us with an aggregate and feature-rich view of data. Whereas,
    traditionally, every data point would be examined independently, graphs now allow
    us to examine the relationships between various data points. Relationships and
    neighborhoods contain valuable information that can be leveraged in recommendations,
    clustering, fraud detection, and other analytic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphs are rapidly emerging as important analytic tools in the cybersecurity
    domain. Similar to the other fields, several security applications can be represented
    as graphs, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A social media network can be represented as a graph with users as nodes. Edges
    between nodes identify friendships and family relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network traffic can be modeled as a graph, with nodes as IP addresses and edges
    as communication messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domains, URLs, or websites can be represented as a graph, where edges indicate
    the presence of links between two websites or domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malware files can be represented as a graph, where nodes are functions and edges
    represent calls between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes represent entities of interest, and edges represent the relationships
    between them. Not all nodes have to be of the same category – for example, a social
    network graph can have both posts and users as nodes. Similarly, there can also
    be heterogeneity in edges. One kind of edge can connect users who are friends,
    while another one can connect users who have at least 10 mutual friends. A third
    one can connect users who have commented on the same post.
  prefs: []
  type: TYPE_NORMAL
- en: Every node in a graph can have features associated with it. These represent
    characteristics of the entity that the node represents. Features associated with
    a user can be their age, the age of the account, the number of friends, the number
    of posts shared, and so on. Similarly, edges can also have features. For example,
    if an edge represents two users being friends, the edge features can be the number
    of mutual friends, the age of the friendship, the common pages followed, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sample social network graph shown in *Figure 8**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – A social network graph](img/B19327_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – A social network graph
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, the nodes represent users. Every user has features associated
    with them – the number of friends, followers, and posts per week, as well as a
    list of pages liked. These become node attributes. There are two kinds of edges,
    marked by different colors. A green edge is constructed between two users if they
    have communicated over messages. A gray edge is added if the two have more than
    five mutual friends. Note that edges can also have features. In this case, the
    green edge (which denotes the fact that two users talked over messages) has properties
    related to that relationship, such as message timestamps, the text, the average
    duration between messages, and the number of images exchanged. These become the
    edge attributes.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on the fundamentals of graphs and how real-world
    data can naturally be represented in graph form. In the next section, we will
    look at how machine learning can be applied to graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning on graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning techniques (such as classification or clustering) can nowadays
    be applied to nodes, edges, or entire graphs. The concepts remain the same, but
    we apply the algorithms to graph entities, and therefore, some of the tasks can
    be framed as a node, link, or subgraph classification. For example, in a network
    of users on social media, identifying abusive or bot users would be a node classification
    task. Identifying malicious messages or transactions would be an edge classification
    problem. Detecting groups of hate speech disseminators would be a graph classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In graph machine learning, the challenge lies in extracting features from a
    graph. A possible approach would be using the adjacency matrix and node features
    as an attribute vector and feeding it to a traditional machine learning algorithm.
    However, the model produced will not be permutation-invariant, as there is no
    inherent order within the nodes in a graph; models based on a graph should be
    permutation-invariant, as the order of nodes should not really matter. There have
    been several approaches proposed to handle these challenges, which we will take
    a brief look at.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional graph learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a naïve approach to machine learning on graphs, we will parse the graph and
    extract features for the entity we’re interested in. If the task is at the node
    level, we extract a set of features for each node. If it is at the edge level,
    we extract a set of features for each edge. However, prior research has shown
    that this traditional approach is most suited for node-level tasks, such as node
    classification or clustering. These features are typically based on standard graph-based
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a social media network where nodes are users, and edges
    between users indicate some sort of relationship (e.g., the users are friends,
    they share a certain number of mutual connections, they have a common or similar
    activity, or they have interacted via comments or messages).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can extract common graph metrics for every user, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The node in-degree (the number of incoming edges to a node)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The node out-degree (the number of outgoing edges from a node)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of outgoing edge weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of incoming edge weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance to the nearest neighbor (as defined by the edge weight)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The in-degree of the nearest neighbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance to the root (some predefined neighbor)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the node is part of a cyclic subgraph (that is, forming a loop)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we can use node-specific features that we would have used in traditional
    machine learning, such as the number of friends, the number of pages liked, the
    number of logins, the number of posts, the key topics the user writes about, and
    the average likes per post. Together, these feature sets will form our feature
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Once the feature vector has been created, any standard machine learning model
    (logistic regression, random forest, SVM, or a deep neural network) can be trained
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: While this approach is straightforward, it has a major disadvantage – it examines
    each node individually and does not consider the interrelationships within nodes.
    Additionally, features have to be handcrafted. Not all graph metrics will make
    sense in all use cases, so you may have to come up with more creative measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Graph embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section described traditional feature extraction on graphs and
    the disadvantages that come with it. In this section, we will look at a slightly
    advanced technique – node embeddings, which are analogous to word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Node embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at word embeddings using the Word2Vec algorithm.
    An embedding is a mathematical representation of a word, such that words with
    a similar meaning or closer in semantics are closer to each other in the embedding
    space. For example, the words *king* and *queen* will have embeddings that are
    very similar, as will the words *banana* and *apple*. Node embeddings operate
    under a similar concept; embeddings are generated for each node such that similar
    nodes have similar embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, see the following diagram, taken from the Stanford Course on
    Graph Neural Networks (CS224W):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A graph and its corresponding node embeddings](img/B19327_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – A graph and its corresponding node embeddings
  prefs: []
  type: TYPE_NORMAL
- en: On the left, you can see a graph where nodes are colored, and on the right,
    you can see the same nodes plotted in an embedding space (a high-dimensional embedding
    was generated, followed by principal component analysis to reduce it to two dimensions).
    We can clearly see that nodes of a similar color cluster together in the embedding
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Node embeddings are generated based on random walks from a node. A random walk
    is basically the sequence of nodes traversed, starting at a source node. Given
    a source node, we select a neighbor at random and move to it. At this neighbor,
    we again select another neighbor (of this new node) and move to it. We do this
    for a fixed number of steps. The sequence of nodes visited forms a random walk.
  prefs: []
  type: TYPE_NORMAL
- en: We use random walks to generate node embeddings. The underlying rationale is
    that if a random walk starting at *u* visits *v* with a high probability, then
    the nodes *u* and *v* are likely to be similar. Nodes visited in the same random
    walk will be close to each other in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate our embeddings, we estimate the probability of visiting node *v*
    on a random walk that started at node *u*. We learn the mapping from the node
    to the embedding space. We want to learn feature representations that are predictive
    of the nodes in the random walk. We first initialize at random an embedding for
    each node. We then run fixed-length, short random walks and calculate a loss for
    each walk. The loss function generally used is the log-likelihood. If *u* is the
    node of interest, the loss function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L =  ∑ u ∈V ∑ v ∈ N R(u)− log( P | z u)
  prefs: []
  type: TYPE_NORMAL
- en: Here, *z* denotes the embedding representation. *N*R(u) represents the neighborhood
    of node *u*. We estimate the probability *P* of two nodes co-occurring on the
    same random walk as the softmax of the dot product of their embeddings. The neighborhood
    set can be formed by any criterion (such as neighbors at one hop, two hops, or
    satisfying some criterion for similarity).
  prefs: []
  type: TYPE_NORMAL
- en: We optimize for the loss function using gradient descent, just like we would
    optimize for parameters in any other algorithm, such as a neural network or a
    linear regression. The embedding *z* is first initialized to some random value.
    For each node, a loss is calculated as well as the partial derivative of the loss
    with respect to *z*. Finally, *z* is updated by making adjustments in the right
    direction as suggested by the gradient. After this has been completed for multiple
    iterations, *z* will have the embeddings that result in the smallest possible
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: The approach described so far was based on random walks. However, a popular
    algorithm, Node2Vec, operates on the principle of biased walks. Instead of picking
    the next node to jump to randomly, it operates based on two parameters – *p*,
    which denotes the probability of returning to the previous node jumped from, and
    *q*, which denotes the probability of moving outward to another node. When the
    value of *p* is low, the random walk functions as a **breadth-first search** (**BFS**)
    (a graph traversal algorithm that explores all the vertices at the same distance
    from the starting vertex, before moving to the vertices at the next level). On
    the other hand, when *q* is low, it functions as a **depth-first search** (**DFS**)
    (a graph traversal algorithm that explores as far as possible along each branch
    before backtracking).
  prefs: []
  type: TYPE_NORMAL
- en: From node embeddings to graph embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The random walk and Node2Vec methods describe how to obtain an embedding representation
    for a single node. However, oftentimes, we want to solve tasks at the graph level,
    such as classifying subgraphs or entire graphs. There are two approaches that
    can compute embeddings for graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is calculating embeddings for each node separately and then
    aggregating them to obtain a graph-level representation. The aggregation can be
    as simple as a sum or average. The sum or average can be weighted by node importance,
    label, or some other feature. While this approach is fairly straightforward, it
    obfuscates the embeddings of each individual node; it does not take the structure
    and connections between nodes into account.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular approach is to introduce a dummy node into the graph or subgraph.
    This node is thought of as having edges to all of the other nodes in the graph.
    We then use the learned models to calculate the embedding for this dummy node.
    As this node is connected to all the other nodes, it captures the structural relationships
    between them and can represent the graph as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Existing methods for machine learning on graphs face the following two issues:'
  prefs: []
  type: TYPE_NORMAL
- en: If we use traditional methods of extracting features based on graph analytics,
    we lose out on incorporating the node-level features and their relationships.
    Aggregating metrics at a graph or subgraph level causes information loss, due
    to noisy data in the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we use node embeddings, we use only the interconnections between nodes and
    not the node features. Valuable signals that may have been meaningful for a classification
    model present in node features will be lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This led to the development of GNNs. Using GNN models, it is possible to apply
    deep learning algorithms (such as convolution, backpropagation, autoencoders,
    and attention-based transformers) directly to graphs. They accept entire graphs
    as input (instead of vectors) and learn embedding representations for each node.
    Every node has an internal state (initially set to a null vector, or based on
    the features of a node). A node aggregates information from its neighbors, followed
    by neural message passing, which propagates this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural message passing is the fundamental principle on which GNNs operate.
    Consider the following figure, illustrating the operation of message passing and
    a GNN in a node classification context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Message passing in GNNs](img/B19327_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Message passing in GNNs
  prefs: []
  type: TYPE_NORMAL
- en: Consider the graph with nodes and edges, shown in **(i)**. Let **D** be the
    initial node under consideration. This is an arbitrary choice, and any node can
    be chosen to start with. Each node has an internal state representation. At first,
    the internal state is set to the feature vector for the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each neighbor of **D** now passes messages to **D**, as shown in **(ii)**.
    The messages are simply the internal states, or some function of the internal
    state. After **D** receives the messages, it does two things, as shown in **(iii)**:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregates the messages to come up with a unified representation. Aggregation
    can be a sum, mean, or min/max pooling of the states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates its own internal state by applying an aggregation function *f* over
    the original state and the aggregated state produced in the previous step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, apply a function *g* such as a sigmoid or softmax to obtain a prediction
    for node **D**. Comparing this with the ground truth label for the node, we can
    calculate a loss, as shown in **(iv)**, which can be backpropagated.
  prefs: []
  type: TYPE_NORMAL
- en: All of these steps (**(i)**–**(iv)**) occur for each node at the same instant.
    The message passing happens first, and the updates of the state occur together.
    This process can be repeated for multiple iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Note an interesting consequence of message passing. At *t = 0* (the very first
    step), a node has information only about itself (based on its own features). At
    *t = 1* (after the first step of message passing has completed), a node has received
    feature information from its neighbors, so it has knowledge about its first neighbors.
    At *t = 2*, the node will receive messages from its neighbors, but these messages
    will already encode information from *their* neighbors; thus, a node will have
    knowledge of its two-hop neighbors as well. As the steps repeat, information spreads
    farther and farther.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have described graphs sufficiently, and grasped the concepts behind
    GNNs, it is time to put them to use.
  prefs: []
  type: TYPE_NORMAL
- en: Fake news detection with GNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how fake news can be detected using a GNN.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling a GNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While some problems can naturally be thought of as graphs, as data scientists,
    you need to conceptualize and build a graph. Data may still be available to you
    in tabular form, but it will be up to you to build a meaningful graph from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving any task with a GNN involves the following high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the entities that will be your nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining a rule or metric to connect nodes via edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining a set of features for nodes and edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determining the kind of graph task the given problem can translate into (node
    classification, edge classification, or subgraph classification).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In social media-related domains, such as friend recommendation, post virality,
    and fake news detection, we have multiple choices for nodes, their features, and
    the methodology for edges between them, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes can be users, their posts, or comments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features can be user-historical behavior or text-related features from content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edges can be based on whether they have mutual friends or follow the same page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a growing body of research that explores these methodologies and applications
    of GNNs for internet security.
  prefs: []
  type: TYPE_NORMAL
- en: The UPFD framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our experiment, we will be following the **User Preference-aware Fake News
    Detection** (**UPFD**) framework ([2104.12259] *User Preference-aware Fake News
    Detection* – [arxiv.org](http://arxiv.org)). In this UPFD framework, every news
    article is transformed into a graph. The task is to detect whether a news article
    is fake news or not. As a news article is a graph, this is essentially a graph
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For every news article, we obtain an information diffusion graph between users.
    In short, the process works as follows. For every news article, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the set of users who are engaged in propagating the news article (via
    likes or retweets). These become the nodes of the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Crawl the recent 200 tweets of each user (this is a design choice; you may decide
    to crawl more or less depending on your use case!).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using pre-trained BERT embeddings and word vectors, encode the news article
    into a feature vector. This feature vector per user represents the node features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the order of retweeting the news article, build an information diffusion
    path that indicates how the news article spread from one user to another. These
    form the edges of the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, using the GNN, train a classification model to classify each graph
    (that is, each article) as fake news or not, based on the graph structure as well
    as user representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset used for UPFD has been made available publicly. This dataset contains
    graph representations for news articles, after all the pre-processing, feature
    extraction, and information diffusion has been done. Using this dataset directly
    saves us the trouble of implementing data preparation pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first install the required libraries. As discussed in previous chapters,
    PyTorch is a deep learning framework developed by Facebook that can help us flexibly
    and easily implement most machine learning models, including neural networks.
    PyTorch Geometric is the graph counterpart of PyTorch that can be used to implement
    GNNs. It is built upon PyTorch and contains several methods for deep learning
    on graphs. The following commands will install PyTorch and PyTorch Geometric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that the installation was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will show you the version of PyTorch you are using.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now download the fake news dataset from the UPFD paper. This has been
    integrated with the PyTorch Geometric datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can examine the size of our training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'What does the training data look like? Let us look at the first element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This says that the first element is an object of the `Data` class. The `x` attribute
    represents the features of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize one of the graphs using the `networkx` library. First, install
    the library using the package manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can visualize the graphs using this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – A sample graph from the UPFD dataset](img/B19327_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – A sample graph from the UPFD dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'On some systems, or if you are using Google Colab, the `networkx` dependencies
    do not load properly, and you might get an error when visualizing the graph that
    says that the function is not defined. If that happens, you need to manually copy
    the code snippet of that function. For convenience, the function definition is
    given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Copying this into your script should resolve the error.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with what the data looks like internally, we can begin
    our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GNN-based fake news detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we read the data into the `DataLoader` object so that it can easily
    be consumed by our model. The `batch_size` parameter depicts the number of examples
    that will be processed in the same batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define the actual GNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let us deconstruct this code a little bit. At a high level, we write a class
    to define our GNN. The class has two functions – a constructor that creates objects
    and sets initial parameters (`__init__`), and a method that defines the structure
    of the network by laying out, step by step, the transformations that happen in
    the forward pass of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The constructor function takes in three parameters. These are hyperparameters
    and are chosen for optimal performance. The first parameter is the number of input
    features. The second parameter defines the size of the hidden layer. And the final
    parameter defines the number of neurons in the output layer (typically, either
    1, in the case of binary classification, or equal to the number of classes in
    multi-class classification).
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor defines the layers of the neural network we will need to process
    our data. First, we define three convolutional layers. Then, we define our readout
    layers. We will need three separate readout layers:'
  prefs: []
  type: TYPE_NORMAL
- en: One for the news features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One for the graph features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One to obtain softmax probability distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward()` function defines the steps taken in the forward pass of the
    neural network. In other words, it describes how the input features are transformed
    into the output label.
  prefs: []
  type: TYPE_NORMAL
- en: First, we see that the features are processed through the first convolution
    layer. The output is passed to a second convolution, and the output of this is
    passed to a third. Then, a pooling layer aggregates the output of this final convolution.
    The first readout layer will process the pooled output. This completes the processing
    of graph features.
  prefs: []
  type: TYPE_NORMAL
- en: We will follow the same philosophy outlined in the UPFD paper. We will extract
    news features as well. Going by the design of the dataset, the first node is the
    root node in the graph. We extract features from this node and pass them through
    the readout layer we defined for the news features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we concatenate the two – the readout output of the graph features and
    the readout output of the news features. This concatenated vector is passed through
    a final readout layer. We transform the output of this using a sigmoid function
    (as it is a binary classification) to obtain the output class label probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the class and the structure of the GNN, we can actually
    train the model! We begin by defining some important terms we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: First, we check whether a GPU is available via CUDA. If it is, we set `'cuda'`
    as the device, as opposed to the usual `'cpu'`. Using CUDA or any GPU significantly
    speeds up the model training, as matrix multiplication, gradient calculation,
    and backpropagation can be decoupled and parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define three key objects we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model**: We defined the GNN class that describes the model structure and
    operations in the forward pass. We use this class to create a GNN object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: We will calculate the output in the forward pass, compute a
    loss, and backpropagate with the gradients. The optimizer will help us perform
    gradient descent to minimize loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss**: The core of a neural network (or any machine learning model) is to
    minimize a loss so that the predicted values are as close to the actual values
    as possible. Here, for the loss function, we will use the binary cross-entropy
    loss, as it is a binary classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we write the actual training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us deconstruct what we have written in the training function. Recall that
    the basic steps in a neural network are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the features input to the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the features through the various layers till you reach the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the output is derived, compute the loss by comparing it with the ground
    truth label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the loss and adjust each of the parameters in the layers using
    gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1–4* for each example to complete one epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1–5* for multiple epochs to minimize the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The previous training code merely replicates these steps. We read the input
    features and copy them over to the GPU if needed. When we run them through the
    model, we basically do the forward pass. Based on the output, we calculate the
    loss and then initiate the backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: We will also define a function that calculates metrics for us. Recall that we
    have previously used the confusion matrix to compute certain metrics, such as
    accuracy, precision, recall, and the F-1 score. In this example, we will use the
    built-in modules provided by scikit-learn. These functions will take in the list
    of predicted and actual values, and compare them to calculate a given metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our metric function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have already written a function that will train the model. We will now write
    a similar function but just for inference. We first set the model in the evaluation
    mode using the `eval()` function. In the evaluation mode, there is no gradient
    calculation (as there is no training involved, we do not need the gradients for
    backpropagation; this saves computation time as well as memory).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the training function, the evaluation function reads a batch of data
    and moves it to the GPU if needed and available. It runs the data through the
    model and calculates the predicted output class. The predicted labels from the
    model output and actual labels from the ground truth are appended to two separate
    lists. Based on the model output and the ground truth, we calculate the loss.
    We repeat this for each batch and sum up the loss values.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end, when all the data has been processed, we have the average loss
    (the total loss divided by the number of data points). We use the actual and predicted
    lists to calculate the metrics using the metric function defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it is time to put all these functions into action! So far, we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A class that defines our GNN structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that will run a training iteration over the GNN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that will run an evaluation/inference iteration on the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that will compute common metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now use all of these to run our GNN experiment. We will first run the
    training function, and obtain a loss. We will run the test function and obtain
    the test loss. We repeat this for multiple epochs. As the epochs progress, we
    can observe how the loss changes in both the training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The training loop of the GNN](img/B19327_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The training loop of the GNN
  prefs: []
  type: TYPE_NORMAL
- en: 'We can manually observe the loss trends. However, it would be easier if we
    could visualize it as a time series over epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Loss trends across epochs](img/B19327_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Loss trends across epochs
  prefs: []
  type: TYPE_NORMAL
- en: We see that as the epochs progressed, the loss steadily decreased for the training
    data. The test loss has also decreased, but there are some spikes, probably caused
    by overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Playing around with the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in the previous chapters, this model also makes several design choices that
    affect its performance. We will leave it up to you to try out some changes and
    observe model performance. Some things to be considered are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameters**: The learning rate in the training function and the number
    of epochs are simply design choices. What happens if they are changed? How do
    the training and test loss trends change? How do the accuracy and F-1 score change?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph layers**: The previous example uses the **GATConv** layers; however,
    we can just as easily use other types of layers available in the PyTorch Geometric
    library. Read up on these layers – which other layers are suitable? How does using
    a different layer affect the performance?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architecture**: We used three convolutional layers and three graph layers
    in our model. However, we can go as high as we want. What happens if we use five
    layers? Ten layers? Twenty layers? What if we increase only the convolution layers
    and not the graph layers, or vice versa?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes our discussion of how to model data as graphs, and how GNNs can
    be used to detect fake news.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of real-world data can be naturally represented as graphs. Graphs are
    especially important in a social network context where multiple entities (users,
    posts, or media) are linked together, forming natural graphs. In recent times,
    the spread of misinformation and fake news is a problem of growing concern. This
    chapter focused on detecting fake news using GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: We began by first learning some basic concepts about graphs and techniques to
    learn on graphs. This included using static features extracted from graph analytics
    (such as degrees and path lengths), node and graph embeddings, and finally, neural
    message passing, using GNNs. We looked at the UPFD framework and how a graph can
    be built for a news article, complete with node features that incorporate historical
    user behavior. Finally, we trained a GNN model to build a graph classifier that
    detects whether a news article is fake or not.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of cybersecurity, graphs are especially important. This is because
    attacks are often coordinated from different entities, or directed toward a set
    of similar targets. Designing a graph and implementing a GNN-based classifier
    is a valuable skill for data scientists in the security domain.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at adversarial machine learning and how it
    can be used to fool or attack machine learning models.
  prefs: []
  type: TYPE_NORMAL
