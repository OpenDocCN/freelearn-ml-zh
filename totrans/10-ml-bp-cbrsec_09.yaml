- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Detecting Fake News with Graph Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图神经网络检测假新闻
- en: In the previous chapters, we looked at tabular data, which was comprised of
    individual data points with their own features. While modeling and running our
    experiments, we did not consider any features of the relationship among the data
    points. Much real-world data, particularly that in the domain of cybersecurity,
    can naturally occur as graphs and be represented as a set of nodes, some of which
    are connected using edges. Examples include social networks, where users, photos,
    and posts can be connected using edges. Another example is the internet, which
    is a large graph of computers connected to each other.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们研究了表格数据，它由具有自己特征的独立数据点组成。在建模和运行我们的实验时，我们没有考虑数据点之间关系的任何特征。在现实世界的许多数据中，尤其是在网络安全领域的数据，可以自然地以图的形式出现，并以一组节点表示，其中一些节点通过边连接。例如，社交网络，用户、照片和帖子可以通过边连接。另一个例子是互联网，它是由相互连接的计算机组成的大型图。
- en: Traditional machine learning algorithms cannot directly learn from graphs. Algorithms
    such as regression, neural networks, and trees, and optimization techniques such
    as gradient descent are designed to operate on Euclidean (flat) data structures.
    This has led to the development of **Graph Neural Networks** (**GNNs**), an upcoming
    area of research in the field of machine learning. This has found tremendous applications
    in cybersecurity, particularly in areas such as botnets, fake news detection,
    and fraud analytics. This chapter will focus on detecting fake news using GNNs.
    We will first cover the basics of graph theory, followed by how graph machine
    learning can be used as a tool to frame a security problem. Although we will learn
    how to use GNN models to detect fake news, the techniques we will introduce are
    generic and can be applied to multiple problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习算法不能直接从图中学习。例如，回归、神经网络和树以及梯度下降等优化技术都是为操作欧几里得（平面）数据结构而设计的。这导致了**图神经网络**（**GNNs**）这一机器学习领域新兴研究领域的开发。这在网络安全领域得到了巨大的应用，尤其是在僵尸网络、假新闻检测和欺诈分析等领域。本章将专注于使用
    GNNs 检测假新闻。我们将首先介绍图论的基础知识，然后介绍如何将图机器学习用作工具来构建安全问题。虽然我们将学习如何使用 GNN 模型来检测假新闻，但我们介绍的技术是通用的，可以应用于多个问题。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: An introduction to graphs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的简介
- en: Machine learning on graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图上的机器学习
- en: Fake news detection with GNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GNNs 检测假新闻
- en: By the end of this chapter, you will have an understanding of how certain data
    can be modeled as graphs, and how to apply graph machine learning for effective
    classification.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将了解某些数据如何被建模为图，以及如何应用图机器学习进行有效的分类。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到本章的代码文件，网址为 [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208)。
- en: An introduction to graphs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的简介
- en: First, let us understand what graphs are and the key terms related to graphs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解什么是图以及与图相关的关键术语。
- en: What is a graph?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是图？
- en: 'A graph is a data structure that is represented as a set of nodes connected
    by a set of edges. Mathematically, we specify a graph *G* as (*V, E*), where *V*
    represents the nodes or vertices and *E* represents the edges between them, as
    shown in *Figure 8**.1*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图是一种数据结构，它由一组节点通过一组边连接而成。在数学上，我们指定一个图 *G* 为 (*V, E*)，其中 *V* 代表节点或顶点，而 *E* 代表它们之间的边，如图
    *8.1* 所示。1*：
- en: '![Figure 8.1 – A simple graph](img/B19327_08_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 一个简单的图](img/B19327_08_01.jpg)'
- en: Figure 8.1 – A simple graph
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 一个简单的图
- en: 'In the previous graph, we have the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们有以下内容：
- en: V = {1, 2, 3, 4, 5, 6}
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: V = {1, 2, 3, 4, 5, 6}
- en: E = {(1,3), (2,3), (2,5), (3,6), (4,6), (5.6)}
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: E = {(1,3), (2,3), (2,5), (3,6), (4,6), (5,6)}
- en: Note that the order in which the nodes and edges are mentioned does not matter.
    The graph shown in *Figure 8**.1* is an undirected graph, which means that the
    direction of the edges does not matter. There can also be directed graphs in which
    the definition of the edge has some meaning, which gives importance to the direction
    of the edge. For example, a graph depicting the water flow of from various cities
    would have directed edges, as water flowing from city A to city B does not imply
    that water flows from B to A.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，提及节点和边的顺序并不重要。图8.1中所示的图是无向图，这意味着边的方向并不重要。也可以有定义边具有某些含义的有向图，这赋予了边的方向重要性。例如，表示从各个城市流向的水流的图会有有向边，因为从城市A流向城市B的水流并不意味着水从B流向A。
- en: 'An example of a directed graph is shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个有向图的示例：
- en: '![Figure 8.2 – An example of a directed graph](img/B19327_08_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 有向图的示例](img/B19327_08_02.jpg)'
- en: Figure 8.2 – An example of a directed graph
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 有向图的示例
- en: 'Here, we would define the graph as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将定义图如下：
- en: V = {1, 2, 3, 4, 5, 6, 7, 8}
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: V = {1, 2, 3, 4, 5, 6, 7, 8}
- en: E = {(1,4), (4,1), (1,3), (2,3), (3,5), (5,7), (6,2), (7,3), (8,3)}
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: E = {(1,4), (4,1), (1,3), (2,3), (3,5), (5,7), (6,2), (7,3), (8,3)}
- en: Note that here, as it is a directed graph, the order in which nodes in each
    edge tuple are specified matters. We have added both `(1,4)` and `(4,1)`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，由于这是一个有向图，每个边元中节点指定的顺序很重要。我们添加了`(1,4)`和`(4,1)`。
- en: In this graph, the edges are unmarked (that is, there is no weight associated
    with an edge). Such a graph is called an unweighted graph. A graph in which there
    are weights associated with edges is called a weighted graph. Edge weights can
    represent properties of relationships between nodes, or the intensity of relationships.
    For example, in a highway transport network, where nodes represent cities, the
    edge weights can depict the distance on the highway or the time taken to traverse
    on average.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，边没有标记（也就是说，边没有关联的权重）。这样的图称为无权图。如果边有关联的权重，则称为加权图。边权重可以表示节点之间关系的属性，或关系的强度。例如，在高速公路运输网络中，其中节点代表城市，边权重可以表示高速公路上的距离或平均穿越所需的时间。
- en: Representing graphs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图的表示
- en: As discussed, a graph can be represented by a set of vertices and edges between
    vertices. On a computer, or inside a program, a graph can be represented in one
    of two ways – an adjacency matrix or an adjacency list.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，图可以通过顶点集和顶点之间的边来表示。在计算机或程序内部，图可以通过两种方式之一来表示 – 邻接矩阵或邻接表。
- en: An adjacency matrix is an *N* x *N* matrix, where *N* is the total number of
    nodes in the graph. Entries in the matrix denote edge relations between the nodes.
    If *A* is the adjacency matrix, then *A*ij = 1 if there is an edge between the
    nodes *i* and *j*. If the graph is undirected, then we have *A*ij = *A*ji; if
    the graph is directed, then *A*ij = 1 means that there is an edge from node *i*
    to node *j*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵是一个 *N* x *N* 矩阵，其中 *N* 是图中节点总数。矩阵中的条目表示节点之间的边关系。如果 *A* 是邻接矩阵，那么 *A*ij =
    1 如果节点 *i* 和 *j* 之间存在边。如果图是无向的，那么我们就有 *A*ij = *A*ji；如果图是有向的，那么 *A*ij = 1 表示从节点
    *i* 到节点 *j* 有边。
- en: 'Consider the following directed graph:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下有向图：
- en: '![Figure 8.3 – A sample-directed graph](img/B19327_08_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 示例有向图](img/B19327_08_03.jpg)'
- en: Figure 8.3 – A sample-directed graph
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 一个示例有向图
- en: 'For this directed graph, the adjacency matrix would be defined as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个有向图，邻接矩阵的定义如下：
- en: '| 0 | 0 | 1 | 1 | 0 | 0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 1 | 0 | 0 |'
- en: '| 0 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 0 | 0 | 0 |'
- en: '| 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| 0 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 1 |'
- en: '| 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 0 |'
- en: Table 8.1 – An adjacency matrix corresponding to the graph in Figure 8.3
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – 对应图8.3的邻接矩阵
- en: Alternatively, if the graph is weighted, then the entries in the adjacency matrix
    can be modified to represent the weight instead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果图是加权的，那么邻接矩阵中的条目可以修改以表示权重。
- en: While adjacency matrices are interpretable and straightforward to understand,
    they are computationally expensive. As the number of nodes in the graph increases,
    the size of the adjacency matrix also increases. In technical terms, the space
    complexity is *O(N*2*)*, where *N* is the number of nodes. Larger adjacency matrices
    are difficult to store and also require a significant processing overhead.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然邻接矩阵可解释且易于理解，但它们在计算上非常昂贵。随着图中节点数量的增加，邻接矩阵的大小也会增加。从技术角度讲，空间复杂度是 *O(N^2)*，其中
    *N* 是节点的数量。较大的邻接矩阵难以存储，并且需要大量的处理开销。
- en: 'An alternative way to represent a graph is an adjacency list. In this form
    of representation, the graph is stored as a dictionary or hash table. The keys
    represent the nodes, and the value is a list of nodes that the key node has an
    edge to. The list of nodes is represented in memory as a linked list. For the
    graph in *Figure 8**.3*, the adjacency list representation is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表示图的另一种方法是邻接表。在这种表示形式中，图被存储为字典或哈希表。键代表节点，值是键节点相连的节点列表。节点列表在内存中以链表的形式表示。对于 *图
    8.3* 中的图，邻接表表示如下：
- en: '![Figure 8.4 – An adjacency list representation of the graph shown in Figure
    8.3](img/B19327_08_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 图 8.3 中所示图的邻接表表示](img/B19327_08_04.jpg)'
- en: Figure 8.4 – An adjacency list representation of the graph shown in Figure 8.3
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 图 8.3 中所示图的邻接表表示
- en: Here, we can clearly see the nodes that a given node has an edge to. Even when
    the number of nodes is large and the edges are sparse, the adjacency list turns
    out to be an efficient representation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以清楚地看到给定节点与之相连的节点。即使节点数量很大且边稀疏，邻接表仍然是一种有效的表示。
- en: Graphs in the real world
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界中的图
- en: Much real-world data in nearly all domains of science can be naturally represented
    as networks using graphs. In the chemical and medical sciences, drugs can be represented
    as a network of molecules, which are themselves networks of atoms. In information
    systems and the internet, a graph can be constructed that represents machines
    and the connections among them. In the field of engineering, a graph can be constructed
    to represent cities, countries, and the transport networks within them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有科学领域的真实世界数据都可以自然地使用图表示网络。在化学和医学科学中，药物可以表示为分子网络，而这些分子本身又是原子网络。在信息系统和互联网中，可以构建一个图来表示机器及其之间的连接。在工程领域，可以构建一个图来表示城市、国家和它们内部的交通网络。
- en: Graphs provide us with an aggregate and feature-rich view of data. Whereas,
    traditionally, every data point would be examined independently, graphs now allow
    us to examine the relationships between various data points. Relationships and
    neighborhoods contain valuable information that can be leveraged in recommendations,
    clustering, fraud detection, and other analytic tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图为我们提供了数据的汇总和丰富的视图。而传统上，每个数据点都会独立检查，现在图允许我们检查各个数据点之间的关系。关系和邻域包含有价值的信息，这些信息可以在推荐、聚类、欺诈检测和其他分析任务中发挥作用。
- en: 'Graphs are rapidly emerging as important analytic tools in the cybersecurity
    domain. Similar to the other fields, several security applications can be represented
    as graphs, such as the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图迅速成为网络安全领域的重要分析工具。与其他领域类似，几个安全应用可以表示为图，如下所示：
- en: A social media network can be represented as a graph with users as nodes. Edges
    between nodes identify friendships and family relationships.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体网络可以表示为图，其中用户作为节点。节点之间的边标识友谊和家庭关系。
- en: Network traffic can be modeled as a graph, with nodes as IP addresses and edges
    as communication messages.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络流量可以建模为图，节点为 IP 地址，边为通信消息。
- en: Domains, URLs, or websites can be represented as a graph, where edges indicate
    the presence of links between two websites or domains.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域、URL 或网站可以表示为图，其中边表示两个网站或领域之间的链接存在。
- en: Malware files can be represented as a graph, where nodes are functions and edges
    represent calls between them.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件文件可以表示为图，其中节点是函数，边表示它们之间的调用。
- en: Nodes represent entities of interest, and edges represent the relationships
    between them. Not all nodes have to be of the same category – for example, a social
    network graph can have both posts and users as nodes. Similarly, there can also
    be heterogeneity in edges. One kind of edge can connect users who are friends,
    while another one can connect users who have at least 10 mutual friends. A third
    one can connect users who have commented on the same post.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 节点代表感兴趣的实体，边代表它们之间的关系。并非所有节点都必须属于同一类别 – 例如，社交网络图可以同时包含帖子和用户作为节点。同样，边也可以存在异质性。一种类型的边可以连接朋友用户，而另一种类型的边可以连接至少有10个共同朋友的用户。第三种类型的边可以连接在相同帖子上发表评论的用户。
- en: Every node in a graph can have features associated with it. These represent
    characteristics of the entity that the node represents. Features associated with
    a user can be their age, the age of the account, the number of friends, the number
    of posts shared, and so on. Similarly, edges can also have features. For example,
    if an edge represents two users being friends, the edge features can be the number
    of mutual friends, the age of the friendship, the common pages followed, and so
    on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的每个节点都可以有与之关联的特征。这些特征代表了节点所代表的实体的特性。与用户关联的特征可以是他们的年龄、账户年龄、朋友数量、分享的帖子数量等。同样，边也可以有特征。例如，如果边代表两个用户是朋友，那么边的特征可以是共同朋友数量、友谊的年龄、共同关注的页面等。
- en: 'Consider the sample social network graph shown in *Figure 8**.5*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到图 *8.5* 中所示的示例社交网络图：
- en: '![Figure 8.5 – A social network graph](img/B19327_08_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 社交网络图](img/B19327_08_05.jpg)'
- en: Figure 8.5 – A social network graph
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 社交网络图
- en: In this graph, the nodes represent users. Every user has features associated
    with them – the number of friends, followers, and posts per week, as well as a
    list of pages liked. These become node attributes. There are two kinds of edges,
    marked by different colors. A green edge is constructed between two users if they
    have communicated over messages. A gray edge is added if the two have more than
    five mutual friends. Note that edges can also have features. In this case, the
    green edge (which denotes the fact that two users talked over messages) has properties
    related to that relationship, such as message timestamps, the text, the average
    duration between messages, and the number of images exchanged. These become the
    edge attributes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，节点代表用户。每个用户都有与之关联的特征 – 朋友数量、每周帖子数量以及喜欢的页面列表。这些成为节点属性。有两种类型的边，用不同的颜色标记。如果两个用户通过消息交流过，则会在他们之间构建一条绿色边。如果两个用户有超过五个共同朋友，则添加一条灰色边。请注意，边也可以有特征。在这种情况下，表示两个用户通过消息交流的绿色边具有与该关系相关的属性，例如消息时间戳、文本、消息之间的平均持续时间以及交换的图片数量。这些成为边属性。
- en: This concludes our discussion on the fundamentals of graphs and how real-world
    data can naturally be represented in graph form. In the next section, we will
    look at how machine learning can be applied to graphs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对图的基本原理以及现实世界数据如何自然地以图形式表示的讨论。在下一节中，我们将探讨机器学习如何应用于图。
- en: Machine learning on graphs
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图上的机器学习
- en: Machine learning techniques (such as classification or clustering) can nowadays
    be applied to nodes, edges, or entire graphs. The concepts remain the same, but
    we apply the algorithms to graph entities, and therefore, some of the tasks can
    be framed as a node, link, or subgraph classification. For example, in a network
    of users on social media, identifying abusive or bot users would be a node classification
    task. Identifying malicious messages or transactions would be an edge classification
    problem. Detecting groups of hate speech disseminators would be a graph classification
    problem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现今，机器学习技术（如分类或聚类）可以应用于节点、边或整个图。概念保持不变，但我们将这些算法应用于图实体，因此，一些任务可以表述为节点、链接或子图分类。例如，在社交媒体用户网络中，识别滥用或机器人用户将是一个节点分类任务。识别恶意消息或交易将是一个边分类问题。检测仇恨言论传播者群体将是一个图分类问题。
- en: In graph machine learning, the challenge lies in extracting features from a
    graph. A possible approach would be using the adjacency matrix and node features
    as an attribute vector and feeding it to a traditional machine learning algorithm.
    However, the model produced will not be permutation-invariant, as there is no
    inherent order within the nodes in a graph; models based on a graph should be
    permutation-invariant, as the order of nodes should not really matter. There have
    been several approaches proposed to handle these challenges, which we will take
    a brief look at.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在图机器学习中，挑战在于从图中提取特征。一种可能的方法是使用邻接矩阵和节点特征作为属性向量，并将其输入到传统的机器学习算法中。然而，生成的模型将不具有排列不变性，因为图中节点的内部没有固有的顺序；基于图的模型应该是排列不变的，因为节点的顺序实际上并不重要。已经提出了几种方法来处理这些挑战，我们将简要地看一下。
- en: Traditional graph learning
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统图学习
- en: In a naïve approach to machine learning on graphs, we will parse the graph and
    extract features for the entity we’re interested in. If the task is at the node
    level, we extract a set of features for each node. If it is at the edge level,
    we extract a set of features for each edge. However, prior research has shown
    that this traditional approach is most suited for node-level tasks, such as node
    classification or clustering. These features are typically based on standard graph-based
    metrics.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在图上的机器学习简单方法中，我们将解析图并提取我们感兴趣的实体的特征。如果任务是节点级别的，我们将为每个节点提取一组特征。如果是边级别的，我们将为每条边提取一组特征。然而，先前的研究表明，这种方法最适合节点级别的任务，如节点分类或聚类。这些特征通常基于标准的基于图的度量。
- en: For example, consider a social media network where nodes are users, and edges
    between users indicate some sort of relationship (e.g., the users are friends,
    they share a certain number of mutual connections, they have a common or similar
    activity, or they have interacted via comments or messages).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个社交媒体网络，其中节点是用户，用户之间的边表示某种关系（例如，用户是朋友，他们共享一定数量的共同连接，他们有共同或相似的活动，或者他们通过评论或消息互动）。
- en: 'First, you can extract common graph metrics for every user, such as the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以为每个用户提取常见的图度量，例如以下内容：
- en: The node in-degree (the number of incoming edges to a node)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的入度（指向节点的入边数量）
- en: The node out-degree (the number of outgoing edges from a node)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的出度（从节点出发的出边数量）
- en: The sum of outgoing edge weights
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出边权重的总和
- en: The sum of incoming edge weights
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入边权重的总和
- en: The distance to the nearest neighbor (as defined by the edge weight)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到最近邻居的距离（由边权重定义）
- en: The in-degree of the nearest neighbor
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的邻居的入度
- en: The distance to the root (some predefined neighbor)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到根的距离（某些预定义的邻居）
- en: Whether the node is part of a cyclic subgraph (that is, forming a loop)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点是否是循环子图的一部分（即形成循环）
- en: Additionally, we can use node-specific features that we would have used in traditional
    machine learning, such as the number of friends, the number of pages liked, the
    number of logins, the number of posts, the key topics the user writes about, and
    the average likes per post. Together, these feature sets will form our feature
    vector.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用在传统机器学习中会使用的节点特定特征，例如朋友数量、点赞页面数量、登录次数、帖子数量、用户撰写的关键主题以及每篇帖子的平均点赞数。这些特征集将形成我们的特征向量。
- en: Once the feature vector has been created, any standard machine learning model
    (logistic regression, random forest, SVM, or a deep neural network) can be trained
    for classification.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了特征向量，就可以为分类训练任何标准的机器学习模型（逻辑回归、随机森林、SVM或深度神经网络）。
- en: While this approach is straightforward, it has a major disadvantage – it examines
    each node individually and does not consider the interrelationships within nodes.
    Additionally, features have to be handcrafted. Not all graph metrics will make
    sense in all use cases, so you may have to come up with more creative measurements.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法很简单，但它有一个主要的缺点——它单独检查每个节点，并且不考虑节点内部的相互关系。此外，特征必须手工制作。并非所有图度量在所有用例中都适用，因此你可能需要想出更多创造性的度量方法。
- en: Graph embeddings
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图嵌入
- en: The previous section described traditional feature extraction on graphs and
    the disadvantages that come with it. In this section, we will look at a slightly
    advanced technique – node embeddings, which are analogous to word embeddings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节描述了图上的传统特征提取及其带来的缺点。在本节中，我们将探讨一种稍微高级的技术——节点嵌入，它与词嵌入类似。
- en: Node embeddings
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点嵌入
- en: In the previous chapter, we looked at word embeddings using the Word2Vec algorithm.
    An embedding is a mathematical representation of a word, such that words with
    a similar meaning or closer in semantics are closer to each other in the embedding
    space. For example, the words *king* and *queen* will have embeddings that are
    very similar, as will the words *banana* and *apple*. Node embeddings operate
    under a similar concept; embeddings are generated for each node such that similar
    nodes have similar embeddings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了Word2Vec算法来查看词嵌入。嵌入是单词的数学表示，使得具有相似意义或语义上更接近的单词在嵌入空间中彼此更接近。例如，单词 *king*
    和 *queen* 将会有非常相似的嵌入，同样，单词 *banana* 和 *apple* 也会如此。节点嵌入在类似的概念下运行；为每个节点生成嵌入，使得相似的节点具有相似的嵌入。
- en: 'As an example, see the following diagram, taken from the Stanford Course on
    Graph Neural Networks (CS224W):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，请参见以下来自斯坦福大学图神经网络课程（CS224W）的图表：
- en: '![Figure 8.6 – A graph and its corresponding node embeddings](img/B19327_08_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 一个图及其对应的节点嵌入](img/B19327_08_06.jpg)'
- en: Figure 8.6 – A graph and its corresponding node embeddings
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 一个图及其对应的节点嵌入
- en: On the left, you can see a graph where nodes are colored, and on the right,
    you can see the same nodes plotted in an embedding space (a high-dimensional embedding
    was generated, followed by principal component analysis to reduce it to two dimensions).
    We can clearly see that nodes of a similar color cluster together in the embedding
    space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，你可以看到一个节点着色的图，在右侧，你可以看到在嵌入空间中（生成一个高维嵌入后，通过主成分分析将其降低到二维）绘制的相同节点。我们可以清楚地看到，具有相似颜色的节点在嵌入空间中聚集在一起。
- en: Node embeddings are generated based on random walks from a node. A random walk
    is basically the sequence of nodes traversed, starting at a source node. Given
    a source node, we select a neighbor at random and move to it. At this neighbor,
    we again select another neighbor (of this new node) and move to it. We do this
    for a fixed number of steps. The sequence of nodes visited forms a random walk.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 节点嵌入是基于从节点开始的随机游走来生成的。随机游走基本上是遍历节点的序列，从源节点开始。给定一个源节点，我们随机选择一个邻居并移动到它。在这个邻居处，我们再次选择另一个邻居（这个新节点的邻居）并移动到它。我们这样做固定数量的步骤。访问的节点序列形成一个随机游走。
- en: We use random walks to generate node embeddings. The underlying rationale is
    that if a random walk starting at *u* visits *v* with a high probability, then
    the nodes *u* and *v* are likely to be similar. Nodes visited in the same random
    walk will be close to each other in the embedding space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用随机游走来生成节点嵌入。其基本原理是，如果一个从 *u* 开始的随机游走以高概率访问 *v*，那么节点 *u* 和 *v* 很可能是相似的。在相同随机游走中访问的节点将在嵌入空间中彼此靠近。
- en: 'To generate our embeddings, we estimate the probability of visiting node *v*
    on a random walk that started at node *u*. We learn the mapping from the node
    to the embedding space. We want to learn feature representations that are predictive
    of the nodes in the random walk. We first initialize at random an embedding for
    each node. We then run fixed-length, short random walks and calculate a loss for
    each walk. The loss function generally used is the log-likelihood. If *u* is the
    node of interest, the loss function is defined as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成我们的嵌入，我们估计在从节点 *u* 开始的随机游走中访问节点 *v* 的概率。我们学习从节点到嵌入空间的映射。我们希望学习能够预测随机游走中节点的特征表示。我们首先为每个节点随机初始化一个嵌入。然后我们运行固定长度的短随机游走，并为每个游走计算一个损失。通常使用的损失函数是对数似然。如果
    *u* 是感兴趣的节点，损失函数定义如下：
- en: L =  ∑ u ∈V ∑ v ∈ N R(u)− log( P | z u)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: L =  ∑ u ∈V ∑ v ∈ N R(u)− log( P | z u)
- en: Here, *z* denotes the embedding representation. *N*R(u) represents the neighborhood
    of node *u*. We estimate the probability *P* of two nodes co-occurring on the
    same random walk as the softmax of the dot product of their embeddings. The neighborhood
    set can be formed by any criterion (such as neighbors at one hop, two hops, or
    satisfying some criterion for similarity).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*z* 表示嵌入表示。*N*R(u)* 表示节点 *u* 的邻域。我们估计两个节点在相同随机游走中共同出现的概率 *P* 为它们嵌入的点积的softmax。邻域集可以通过任何标准（如一跳、两跳的邻居，或满足某些相似性标准）形成。
- en: We optimize for the loss function using gradient descent, just like we would
    optimize for parameters in any other algorithm, such as a neural network or a
    linear regression. The embedding *z* is first initialized to some random value.
    For each node, a loss is calculated as well as the partial derivative of the loss
    with respect to *z*. Finally, *z* is updated by making adjustments in the right
    direction as suggested by the gradient. After this has been completed for multiple
    iterations, *z* will have the embeddings that result in the smallest possible
    loss.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用梯度下降来优化损失函数，就像我们优化任何其他算法（如神经网络或线性回归）的参数一样。嵌入*z*首先初始化为某个随机值。对于每个节点，都计算损失以及损失相对于*z*的偏导数。最后，根据梯度建议的正确方向进行调整，更新*z*。经过多次迭代后，*z*将具有导致损失最小的嵌入。
- en: The approach described so far was based on random walks. However, a popular
    algorithm, Node2Vec, operates on the principle of biased walks. Instead of picking
    the next node to jump to randomly, it operates based on two parameters – *p*,
    which denotes the probability of returning to the previous node jumped from, and
    *q*, which denotes the probability of moving outward to another node. When the
    value of *p* is low, the random walk functions as a **breadth-first search** (**BFS**)
    (a graph traversal algorithm that explores all the vertices at the same distance
    from the starting vertex, before moving to the vertices at the next level). On
    the other hand, when *q* is low, it functions as a **depth-first search** (**DFS**)
    (a graph traversal algorithm that explores as far as possible along each branch
    before backtracking).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止所描述的方法是基于随机游走的。然而，一个流行的算法Node2Vec是基于有偏游走的原则。它不是随机选择下一个要跳转的节点，而是基于两个参数——*p*，表示返回之前跳转的节点的概率，以及*q*，表示移动到另一个节点的概率。当*p*的值较低时，随机游走充当**广度优先搜索**（**BFS**）（一种图遍历算法，在移动到下一级节点之前，先探索与起始节点相同距离的所有顶点）。另一方面，当*q*的值较低时，它充当**深度优先搜索**（**DFS**）（一种图遍历算法，在回溯之前尽可能沿着每个分支探索）。
- en: From node embeddings to graph embeddings
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从节点嵌入到图嵌入
- en: The random walk and Node2Vec methods describe how to obtain an embedding representation
    for a single node. However, oftentimes, we want to solve tasks at the graph level,
    such as classifying subgraphs or entire graphs. There are two approaches that
    can compute embeddings for graphs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走和Node2Vec方法描述了如何为单个节点获得嵌入表示。然而，很多时候，我们希望解决图级别的问题，例如分类子图或整个图。有两种方法可以计算图的嵌入。
- en: The first approach is calculating embeddings for each node separately and then
    aggregating them to obtain a graph-level representation. The aggregation can be
    as simple as a sum or average. The sum or average can be weighted by node importance,
    label, or some other feature. While this approach is fairly straightforward, it
    obfuscates the embeddings of each individual node; it does not take the structure
    and connections between nodes into account.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是对每个节点分别计算嵌入，然后将它们聚合以获得图级别的表示。聚合可以是简单的求和或平均。求和或平均可以按节点重要性、标签或其他特征进行加权。虽然这种方法相当直接，但它模糊了每个单独节点的嵌入；它没有考虑到节点之间的结构和连接。
- en: Another popular approach is to introduce a dummy node into the graph or subgraph.
    This node is thought of as having edges to all of the other nodes in the graph.
    We then use the learned models to calculate the embedding for this dummy node.
    As this node is connected to all the other nodes, it captures the structural relationships
    between them and can represent the graph as a whole.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的方法是在图或子图中引入一个虚拟节点。这个节点被认为是与图中所有其他节点都有边相连。然后我们使用学习到的模型来计算这个虚拟节点的嵌入。由于这个节点与所有其他节点相连，它捕捉了它们之间的结构关系，并可以代表整个图。
- en: GNNs
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GNNs
- en: 'Existing methods for machine learning on graphs face the following two issues:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图上机器学习现有方法面临以下两个问题：
- en: If we use traditional methods of extracting features based on graph analytics,
    we lose out on incorporating the node-level features and their relationships.
    Aggregating metrics at a graph or subgraph level causes information loss, due
    to noisy data in the model.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用基于图分析的常规方法提取特征，我们将失去结合节点级特征及其关系的机会。在图或子图级别聚合度量会导致信息损失，因为模型中的数据噪声。
- en: If we use node embeddings, we use only the interconnections between nodes and
    not the node features. Valuable signals that may have been meaningful for a classification
    model present in node features will be lost.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用节点嵌入，我们只使用节点之间的互连，而不是节点特征。可能对分类模型有意义的节点特征中的有价值信号将会丢失。
- en: This led to the development of GNNs. Using GNN models, it is possible to apply
    deep learning algorithms (such as convolution, backpropagation, autoencoders,
    and attention-based transformers) directly to graphs. They accept entire graphs
    as input (instead of vectors) and learn embedding representations for each node.
    Every node has an internal state (initially set to a null vector, or based on
    the features of a node). A node aggregates information from its neighbors, followed
    by neural message passing, which propagates this information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了图神经网络（GNNs）的发展。使用GNN模型，可以将深度学习算法（如卷积、反向传播、自编码器和基于注意力的转换器）直接应用于图。它们接受整个图作为输入（而不是向量）并为每个节点学习嵌入表示。每个节点都有一个内部状态（最初设置为空向量，或基于节点的特征）。节点从其邻居聚合信息，然后通过神经消息传递来传播这些信息。
- en: 'Neural message passing is the fundamental principle on which GNNs operate.
    Consider the following figure, illustrating the operation of message passing and
    a GNN in a node classification context:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 神经消息传递是GNN操作的基本原则。考虑以下图，说明了在节点分类上下文中消息传递和GNN的操作：
- en: '![Figure 8.7 – Message passing in GNNs](img/B19327_08_07.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – GNN中的消息传递](img/B19327_08_07.jpg)'
- en: Figure 8.7 – Message passing in GNNs
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – GNN中的消息传递
- en: Consider the graph with nodes and edges, shown in **(i)**. Let **D** be the
    initial node under consideration. This is an arbitrary choice, and any node can
    be chosen to start with. Each node has an internal state representation. At first,
    the internal state is set to the feature vector for the node.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 **（i）** 中显示的具有节点和边的图。设 **D** 为考虑的初始节点。这是一个任意选择，任何节点都可以作为起点。每个节点都有一个内部状态表示。最初，内部状态被设置为节点的特征向量。
- en: 'Each neighbor of **D** now passes messages to **D**, as shown in **(ii)**.
    The messages are simply the internal states, or some function of the internal
    state. After **D** receives the messages, it does two things, as shown in **(iii)**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 **D** 的每个邻居都向 **D** 传递消息，如图 **（ii）** 所示。消息仅仅是内部状态，或者内部状态的某个函数。在 **D** 收到消息后，它执行两个操作，如图
    **（iii）** 所示：
- en: Aggregates the messages to come up with a unified representation. Aggregation
    can be a sum, mean, or min/max pooling of the states.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合即将到来的消息，以得出一个统一表示。聚合可以是状态的求和、平均值或min/max池化。
- en: Updates its own internal state by applying an aggregation function *f* over
    the original state and the aggregated state produced in the previous step.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在原始状态和前一步产生的聚合状态上应用聚合函数 *f* 来更新其自身的内部状态。
- en: Finally, apply a function *g* such as a sigmoid or softmax to obtain a prediction
    for node **D**. Comparing this with the ground truth label for the node, we can
    calculate a loss, as shown in **(iv)**, which can be backpropagated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，应用一个函数 *g*，例如sigmoid或softmax，以获得节点 **D** 的预测。将此与节点的真实标签进行比较，我们可以计算一个损失，如**（iv）**中所示，这可以反向传播。
- en: All of these steps (**(i)**–**(iv)**) occur for each node at the same instant.
    The message passing happens first, and the updates of the state occur together.
    This process can be repeated for multiple iterations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些步骤（**（i）**–**（iv）**）在每个节点上同时发生。消息传递首先发生，然后状态更新同时发生。这个过程可以重复多次迭代。
- en: Note an interesting consequence of message passing. At *t = 0* (the very first
    step), a node has information only about itself (based on its own features). At
    *t = 1* (after the first step of message passing has completed), a node has received
    feature information from its neighbors, so it has knowledge about its first neighbors.
    At *t = 2*, the node will receive messages from its neighbors, but these messages
    will already encode information from *their* neighbors; thus, a node will have
    knowledge of its two-hop neighbors as well. As the steps repeat, information spreads
    farther and farther.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意消息传递的一个有趣后果。在 *t = 0*（第一步的非常开始），节点只有关于自己的信息（基于其自身的特征）。在 *t = 1*（消息传递的第一步完成后），节点已经从其邻居那里收到了特征信息，因此它对其第一邻居有了知识。在
    *t = 2*，节点将收到其邻居的消息，但这些消息已经包含了 *他们* 的邻居的信息；因此，节点将对其两跳邻居也有知识。随着步骤的重复，信息传播得越来越远。
- en: Now that we have described graphs sufficiently, and grasped the concepts behind
    GNNs, it is time to put them to use.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经充分描述了图，并掌握了GNN背后的概念，是时候将它们付诸实践了。
- en: Fake news detection with GNN
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假新闻检测与GNN
- en: In this section, we will learn how fake news can be detected using a GNN.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用GNN检测假新闻。
- en: Modeling a GNN
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模GNN
- en: While some problems can naturally be thought of as graphs, as data scientists,
    you need to conceptualize and build a graph. Data may still be available to you
    in tabular form, but it will be up to you to build a meaningful graph from it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些问题可以自然地被视为图，但作为数据科学家，你需要概念化和构建一个图。数据可能仍然以表格形式提供给你，但构建一个有意义的图将取决于你。
- en: 'Solving any task with a GNN involves the following high-level steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GNN解决任何任务涉及以下高级步骤：
- en: Identifying the entities that will be your nodes.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定将成为你的节点的实体。
- en: Defining a rule or metric to connect nodes via edges.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个规则或指标来通过边连接节点。
- en: Defining a set of features for nodes and edges.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义节点和边的特征集。
- en: Determining the kind of graph task the given problem can translate into (node
    classification, edge classification, or subgraph classification).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定给定问题可以转换成的图任务类型（节点分类、边分类或子图分类）。
- en: 'In social media-related domains, such as friend recommendation, post virality,
    and fake news detection, we have multiple choices for nodes, their features, and
    the methodology for edges between them, such as the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交媒体相关领域，如朋友推荐、帖子病毒性和假新闻检测中，我们有多个选择节点、它们的特征以及它们之间边的构建方法，如下所示：
- en: Nodes can be users, their posts, or comments
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点可以是用户、他们的帖子或评论
- en: Features can be user-historical behavior or text-related features from content
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征可以是用户历史行为或内容中的文本相关特征
- en: Edges can be based on whether they have mutual friends or follow the same page
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边可以基于它们是否有共同的朋友或关注相同的页面
- en: There is a growing body of research that explores these methodologies and applications
    of GNNs for internet security.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有越来越多的研究探索了GNN在网络安全中的这些方法和应用。
- en: The UPFD framework
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UPFD框架
- en: For our experiment, we will be following the **User Preference-aware Fake News
    Detection** (**UPFD**) framework ([2104.12259] *User Preference-aware Fake News
    Detection* – [arxiv.org](http://arxiv.org)). In this UPFD framework, every news
    article is transformed into a graph. The task is to detect whether a news article
    is fake news or not. As a news article is a graph, this is essentially a graph
    classification problem.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将遵循**用户偏好感知假新闻检测**（UPFD）框架（[2104.12259] *用户偏好感知假新闻检测* – [arxiv.org](http://arxiv.org)）。在这个UPFD框架中，每篇新闻文章都被转换成一个图。任务是检测新闻文章是否为假新闻。由于新闻文章是一个图，这本质上是一个图分类问题。
- en: 'For every news article, we obtain an information diffusion graph between users.
    In short, the process works as follows. For every news article, we do the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每篇新闻文章，我们获得用户之间的信息扩散图。简而言之，这个过程如下。对于每篇新闻文章，我们做以下事情：
- en: Identify the set of users who are engaged in propagating the news article (via
    likes or retweets). These become the nodes of the graph.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别参与传播新闻文章的用户集合（通过点赞或转发）。这些成为图的节点。
- en: Crawl the recent 200 tweets of each user (this is a design choice; you may decide
    to crawl more or less depending on your use case!).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爬取每个用户的最近200条推文（这是一个设计选择；根据你的用例，你可能决定爬取更多或更少的推文！）。
- en: Using pre-trained BERT embeddings and word vectors, encode the news article
    into a feature vector. This feature vector per user represents the node features.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的BERT嵌入和词向量，将新闻文章编码成一个特征向量。这个特征向量代表每个用户的节点特征。
- en: Based on the order of retweeting the news article, build an information diffusion
    path that indicates how the news article spread from one user to another. These
    form the edges of the graph.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据新闻文章的转发顺序，构建一个信息扩散路径，表明新闻文章是如何从一个用户传播到另一个用户的。这些构成了图的边。
- en: Finally, using the GNN, train a classification model to classify each graph
    (that is, each article) as fake news or not, based on the graph structure as well
    as user representations.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用GNN训练一个分类模型，根据图结构和用户表示来将每个图（即每篇文章）分类为假新闻或非假新闻。
- en: The dataset used for UPFD has been made available publicly. This dataset contains
    graph representations for news articles, after all the pre-processing, feature
    extraction, and information diffusion has been done. Using this dataset directly
    saves us the trouble of implementing data preparation pipelines.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: UPFD 所使用的数据集已经公开提供。这个数据集包含经过预处理、特征提取和信息扩散的新闻文章的图表示，使用这个数据集可以直接节省我们实现数据准备管道的麻烦。
- en: Dataset and setup
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集和设置
- en: 'We will first install the required libraries. As discussed in previous chapters,
    PyTorch is a deep learning framework developed by Facebook that can help us flexibly
    and easily implement most machine learning models, including neural networks.
    PyTorch Geometric is the graph counterpart of PyTorch that can be used to implement
    GNNs. It is built upon PyTorch and contains several methods for deep learning
    on graphs. The following commands will install PyTorch and PyTorch Geometric:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将安装所需的库。正如前几章所讨论的，PyTorch 是由 Facebook 开发的一个深度学习框架，可以帮助我们灵活且容易地实现大多数机器学习模型，包括神经网络。PyTorch
    Geometric 是 PyTorch 的图对应框架，可以用来实现 GNN。它是基于 PyTorch 构建的，并包含用于图上深度学习的几种方法。以下命令将安装
    PyTorch 和 PyTorch Geometric：
- en: '[PRE0]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can check that the installation was successful:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查安装是否成功：
- en: '[PRE1]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will show you the version of PyTorch you are using.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示你正在使用的 PyTorch 版本。
- en: 'We will now download the fake news dataset from the UPFD paper. This has been
    integrated with the PyTorch Geometric datasets:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将从 UPFD 论文中下载假新闻数据集。这个数据集已经与 PyTorch Geometric 数据集集成：
- en: '[PRE2]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can examine the size of our training and test sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查我们的训练集和测试集的大小：
- en: '[PRE3]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This should give you the following output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给出以下输出：
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What does the training data look like? Let us look at the first element:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据看起来是什么样子？让我们看看第一个元素：
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This says that the first element is an object of the `Data` class. The `x` attribute
    represents the features of the nodes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示第一个元素是 `Data` 类的对象。`x` 属性表示节点的特征。
- en: 'We can visualize one of the graphs using the `networkx` library. First, install
    the library using the package manager:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `networkx` 库来可视化其中一个图。首先，使用包管理器安装库：
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can visualize the graphs using this library:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个库来可视化图：
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It will produce the following result:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生以下结果：
- en: '![Figure 8.8 – A sample graph from the UPFD dataset](img/B19327_08_08.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – UPFD 数据集的一个示例图](img/B19327_08_08.jpg)'
- en: Figure 8.8 – A sample graph from the UPFD dataset
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – UPFD 数据集的一个示例图
- en: 'On some systems, or if you are using Google Colab, the `networkx` dependencies
    do not load properly, and you might get an error when visualizing the graph that
    says that the function is not defined. If that happens, you need to manually copy
    the code snippet of that function. For convenience, the function definition is
    given here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些系统上，或者如果你使用 Google Colab，`networkx` 依赖项可能无法正确加载，你可能会在可视化图时遇到错误，显示该函数未定义。如果发生这种情况，你需要手动复制该函数的代码片段。为了方便，函数定义如下：
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Copying this into your script should resolve the error.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将此复制到你的脚本中应该可以解决错误。
- en: Now that we are familiar with what the data looks like internally, we can begin
    our experiment.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了数据的内部结构，我们可以开始我们的实验。
- en: Implementing GNN-based fake news detection
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基于 GNN 的假新闻检测
- en: 'First, we read the data into the `DataLoader` object so that it can easily
    be consumed by our model. The `batch_size` parameter depicts the number of examples
    that will be processed in the same batch:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据读入 `DataLoader` 对象中，以便它可以轻松地被我们的模型消费。`batch_size` 参数表示将在同一批次中处理的示例数量：
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we will define the actual GNN:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义实际的 GNN：
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let us deconstruct this code a little bit. At a high level, we write a class
    to define our GNN. The class has two functions – a constructor that creates objects
    and sets initial parameters (`__init__`), and a method that defines the structure
    of the network by laying out, step by step, the transformations that happen in
    the forward pass of the neural network.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微分解一下这段代码。从高层次来看，我们编写一个类来定义我们的 GNN。该类有两个函数——一个构造函数（`__init__`），用于创建对象并设置初始参数，以及一个方法，通过逐步展开神经网络前向传递中发生的转换来定义网络的结构。
- en: The constructor function takes in three parameters. These are hyperparameters
    and are chosen for optimal performance. The first parameter is the number of input
    features. The second parameter defines the size of the hidden layer. And the final
    parameter defines the number of neurons in the output layer (typically, either
    1, in the case of binary classification, or equal to the number of classes in
    multi-class classification).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受三个参数。这些是超参数，用于获得最佳性能。第一个参数是输入特征的数目。第二个参数定义了隐藏层的大小。最后一个参数定义了输出层中的神经元数目（通常，在二分类的情况下为1，在多分类的情况下等于类别数目）。
- en: 'The constructor defines the layers of the neural network we will need to process
    our data. First, we define three convolutional layers. Then, we define our readout
    layers. We will need three separate readout layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数定义了我们将需要用于处理数据的神经网络层。首先，我们定义了三个卷积层。然后，我们定义了我们的输出层。我们需要三个独立的输出层：
- en: One for the news features
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于新闻特征
- en: One for the graph features
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于图特征
- en: One to obtain softmax probability distributions
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于获得softmax概率分布
- en: The `forward()` function defines the steps taken in the forward pass of the
    neural network. In other words, it describes how the input features are transformed
    into the output label.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()`函数定义了神经网络正向传播中采取的步骤。换句话说，它描述了输入特征如何转换为输出标签。'
- en: First, we see that the features are processed through the first convolution
    layer. The output is passed to a second convolution, and the output of this is
    passed to a third. Then, a pooling layer aggregates the output of this final convolution.
    The first readout layer will process the pooled output. This completes the processing
    of graph features.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到特征通过第一个卷积层进行处理。输出传递到第二个卷积，然后这个输出的结果传递到第三个。然后，一个池化层聚合了最后一个卷积的输出。第一个输出层将处理池化输出。这完成了图特征的加工。
- en: We will follow the same philosophy outlined in the UPFD paper. We will extract
    news features as well. Going by the design of the dataset, the first node is the
    root node in the graph. We extract features from this node and pass them through
    the readout layer we defined for the news features.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循UPFD论文中概述的相同哲学。我们还将提取新闻特征。根据数据集的设计，图中的第一个节点是根节点。我们从该节点提取特征，并通过我们为新闻特征定义的输出层传递它们。
- en: Finally, we concatenate the two – the readout output of the graph features and
    the readout output of the news features. This concatenated vector is passed through
    a final readout layer. We transform the output of this using a sigmoid function
    (as it is a binary classification) to obtain the output class label probability.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这两个输出连接起来——图特征的输出层和新闻特征的输出层。这个连接向量通过一个最终的输出层。我们使用sigmoid函数（因为它是一个二分类）转换这个输出，以获得输出类标签的概率。
- en: 'Now that we have defined the class and the structure of the GNN, we can actually
    train the model! We begin by defining some important terms we need:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了GNN的类和结构，我们实际上可以训练模型了！我们首先定义一些我们需要的重要术语：
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First, we check whether a GPU is available via CUDA. If it is, we set `'cuda'`
    as the device, as opposed to the usual `'cpu'`. Using CUDA or any GPU significantly
    speeds up the model training, as matrix multiplication, gradient calculation,
    and backpropagation can be decoupled and parallelized.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过CUDA检查GPU是否可用。如果可用，我们将设备设置为`'cuda'`，而不是通常的`'cpu'`。使用CUDA或任何GPU可以显著加快模型训练速度，因为矩阵乘法、梯度计算和反向传播可以解耦并并行化。
- en: 'Then, we define three key objects we need:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了三个我们需要的关键对象：
- en: '**Model**: We defined the GNN class that describes the model structure and
    operations in the forward pass. We use this class to create a GNN object.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：我们定义了描述模型结构和正向传播操作的GNN类。我们使用这个类来创建一个GNN对象。'
- en: '**Optimizer**: We will calculate the output in the forward pass, compute a
    loss, and backpropagate with the gradients. The optimizer will help us perform
    gradient descent to minimize loss.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：在正向传播中，我们将计算输出，计算损失，并使用梯度进行反向传播。优化器将帮助我们执行梯度下降以最小化损失。'
- en: '**Loss**: The core of a neural network (or any machine learning model) is to
    minimize a loss so that the predicted values are as close to the actual values
    as possible. Here, for the loss function, we will use the binary cross-entropy
    loss, as it is a binary classification problem.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**：神经网络（或任何机器学习模型）的核心是使损失最小化，以便预测值尽可能接近实际值。在这里，对于损失函数，我们将使用二元交叉熵损失，因为这是一个二元分类问题。'
- en: 'Now, we write the actual training function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们编写实际的训练函数：
- en: '[PRE12]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let us deconstruct what we have written in the training function. Recall that
    the basic steps in a neural network are as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解我们在训练函数中编写的代码。回想一下，神经网络的基本步骤如下：
- en: Pass the features input to the neural network.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征输入传递给神经网络。
- en: Process the features through the various layers till you reach the output.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征通过各个层处理，直到达到输出。
- en: Once the output is derived, compute the loss by comparing it with the ground
    truth label.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦得到输出，通过将其与真实标签进行比较来计算损失。
- en: Backpropagate the loss and adjust each of the parameters in the layers using
    gradient descent.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梯度下降反向传播损失并调整各层的参数。
- en: Repeat *steps 1–4* for each example to complete one epoch.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个示例重复 *步骤 1–4* 以完成一个 epoch。
- en: Repeat *steps 1–5* for multiple epochs to minimize the loss.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 1–5* 多次以最小化损失。
- en: The previous training code merely replicates these steps. We read the input
    features and copy them over to the GPU if needed. When we run them through the
    model, we basically do the forward pass. Based on the output, we calculate the
    loss and then initiate the backward propagation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的训练代码仅仅重复了这些步骤。我们读取输入特征，并在需要时将它们复制到 GPU 上。当我们通过模型运行它们时，我们基本上进行正向传播。根据输出，我们计算损失，然后启动反向传播。
- en: We will also define a function that calculates metrics for us. Recall that we
    have previously used the confusion matrix to compute certain metrics, such as
    accuracy, precision, recall, and the F-1 score. In this example, we will use the
    built-in modules provided by scikit-learn. These functions will take in the list
    of predicted and actual values, and compare them to calculate a given metric.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义一个函数来计算我们的指标。回想一下，我们之前已经使用混淆矩阵来计算某些指标，例如准确率、精确率、召回率和 F-1 分数。在这个例子中，我们将使用
    scikit-learn 提供的内置模块。这些函数将接受预测值和实际值的列表，并将它们进行比较以计算给定的指标。
- en: 'Our metric function is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的指标函数如下：
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have already written a function that will train the model. We will now write
    a similar function but just for inference. We first set the model in the evaluation
    mode using the `eval()` function. In the evaluation mode, there is no gradient
    calculation (as there is no training involved, we do not need the gradients for
    backpropagation; this saves computation time as well as memory).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经编写了一个训练模型的函数。现在我们将编写一个类似的函数，但仅用于推理。我们首先使用 `eval()` 函数将模型设置为评估模式。在评估模式下，没有梯度计算（因为没有涉及训练，我们不需要梯度进行反向传播；这也节省了计算时间和内存）。
- en: Similar to the training function, the evaluation function reads a batch of data
    and moves it to the GPU if needed and available. It runs the data through the
    model and calculates the predicted output class. The predicted labels from the
    model output and actual labels from the ground truth are appended to two separate
    lists. Based on the model output and the ground truth, we calculate the loss.
    We repeat this for each batch and sum up the loss values.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练函数类似，评估函数读取一批数据，并在需要和可用时将其移动到 GPU 上。它将数据通过模型运行，并计算预测输出类别。模型输出的预测标签和来自真实标签的实际标签被附加到两个单独的列表中。根据模型输出和真实标签，我们计算损失。我们对每个批次重复此操作并汇总损失值。
- en: 'At the end, when all the data has been processed, we have the average loss
    (the total loss divided by the number of data points). We use the actual and predicted
    lists to calculate the metrics using the metric function defined earlier:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当所有数据都处理完毕后，我们得到平均损失（总损失除以数据点数）。我们使用实际和预测列表，使用之前定义的指标函数来计算指标：
- en: '[PRE14]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, it is time to put all these functions into action! So far, we have
    the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候将这些函数投入使用了！到目前为止，我们有以下内容：
- en: A class that defines our GNN structure
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个定义我们 GNN 结构的类
- en: A function that will run a training iteration over the GNN model
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将在 GNN 模型上运行训练迭代的函数
- en: A function that will run an evaluation/inference iteration on the model
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将在模型上运行评估/推理迭代的函数
- en: A function that will compute common metrics
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将计算常用指标的函数
- en: 'We will now use all of these to run our GNN experiment. We will first run the
    training function, and obtain a loss. We will run the test function and obtain
    the test loss. We repeat this for multiple epochs. As the epochs progress, we
    can observe how the loss changes in both the training and test data:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用所有这些来运行我们的GNN实验。我们首先运行训练函数，并得到一个损失。然后运行测试函数，得到测试损失。我们重复这个过程多个epoch。随着epoch的进行，我们可以观察到训练和测试数据中的损失如何变化：
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This should produce something like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生类似以下的结果：
- en: '![Figure 8.9 – The training loop of the GNN](img/B19327_08_09.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – GNN的训练循环](img/B19327_08_09.jpg)'
- en: Figure 8.9 – The training loop of the GNN
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – GNN的训练循环
- en: 'We can manually observe the loss trends. However, it would be easier if we
    could visualize it as a time series over epochs:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动观察损失趋势。然而，如果我们能将其可视化为epoch的时间序列，那就更容易了：
- en: '[PRE16]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will get the following result:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下结果：
- en: '![Figure 8.10 – Loss trends across epochs](img/B19327_08_10.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – epoch跨度的损失趋势](img/B19327_08_10.jpg)'
- en: Figure 8.10 – Loss trends across epochs
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – epoch跨度的损失趋势
- en: We see that as the epochs progressed, the loss steadily decreased for the training
    data. The test loss has also decreased, but there are some spikes, probably caused
    by overfitting.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，随着epoch的进行，训练数据的损失稳步下降。测试损失也有所下降，但有一些峰值，可能是过拟合造成的。
- en: Playing around with the model
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型实验
- en: 'As in the previous chapters, this model also makes several design choices that
    affect its performance. We will leave it up to you to try out some changes and
    observe model performance. Some things to be considered are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，这个模型也做出了几个影响其性能的设计选择。我们将把这个选择权交给你，尝试一些变化并观察模型性能。以下是一些需要考虑的事项：
- en: '**Hyperparameters**: The learning rate in the training function and the number
    of epochs are simply design choices. What happens if they are changed? How do
    the training and test loss trends change? How do the accuracy and F-1 score change?'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：训练函数中的学习率和epoch的数量只是设计选择。如果它们被改变会发生什么？训练和测试损失趋势如何变化？准确率和F-1分数如何变化？'
- en: '**Graph layers**: The previous example uses the **GATConv** layers; however,
    we can just as easily use other types of layers available in the PyTorch Geometric
    library. Read up on these layers – which other layers are suitable? How does using
    a different layer affect the performance?'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图层**：前面的例子使用了**GATConv**层；然而，我们同样可以轻松地使用PyTorch Geometric库中可用的其他类型的层。了解这些层——哪些其他层是合适的？使用不同的层如何影响性能？'
- en: '**Architecture**: We used three convolutional layers and three graph layers
    in our model. However, we can go as high as we want. What happens if we use five
    layers? Ten layers? Twenty layers? What if we increase only the convolution layers
    and not the graph layers, or vice versa?'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：我们在模型中使用了三个卷积层和三个图层。然而，我们可以选择更高的层数。如果我们使用五个层？十个层？二十个层呢？如果我们只增加卷积层而不增加图层，或者相反呢？'
- en: This completes our discussion of how to model data as graphs, and how GNNs can
    be used to detect fake news.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对如何将数据建模为图以及如何使用GNN检测假新闻的讨论。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A lot of real-world data can be naturally represented as graphs. Graphs are
    especially important in a social network context where multiple entities (users,
    posts, or media) are linked together, forming natural graphs. In recent times,
    the spread of misinformation and fake news is a problem of growing concern. This
    chapter focused on detecting fake news using GNNs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的数据可以自然地表示为图。在多个实体（用户、帖子或媒体）相互连接形成自然图的社会网络环境中，图尤为重要。近年来，错误信息和假新闻的传播成为一个日益关注的问题。本章重点介绍了使用GNN检测假新闻。
- en: We began by first learning some basic concepts about graphs and techniques to
    learn on graphs. This included using static features extracted from graph analytics
    (such as degrees and path lengths), node and graph embeddings, and finally, neural
    message passing, using GNNs. We looked at the UPFD framework and how a graph can
    be built for a news article, complete with node features that incorporate historical
    user behavior. Finally, we trained a GNN model to build a graph classifier that
    detects whether a news article is fake or not.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先学习了一些关于图和图学习技术的基本概念。这包括使用从图分析中提取的静态特征（如度和路径长度）、节点和图嵌入，最后使用GNN进行神经消息传递。我们研究了UPFD框架以及如何为新闻文章构建一个图，包括结合历史用户行为的节点特征。最后，我们训练了一个GNN模型来构建一个图分类器，用于检测新闻文章是否为假。
- en: In the field of cybersecurity, graphs are especially important. This is because
    attacks are often coordinated from different entities, or directed toward a set
    of similar targets. Designing a graph and implementing a GNN-based classifier
    is a valuable skill for data scientists in the security domain.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，图特别重要。这是因为攻击通常是由不同的实体协调进行的，或者针对一组相似的目标。为图设计并实现基于GNN的分类器是安全领域数据科学家的一项宝贵技能。
- en: In the next chapter, we will look at adversarial machine learning and how it
    can be used to fool or attack machine learning models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨对抗性机器学习以及它如何被用来欺骗或攻击机器学习模型。
