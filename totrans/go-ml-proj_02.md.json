["```py\n// ingest is a function that ingests the file and outputs the header, data, and index.\nfunc ingest(f io.Reader) (header []string, data [][]string, indices []map[string][]int, err error) {\n  r := csv.NewReader(f)\n\n  // handle header\n  if header, err = r.Read(); err != nil {\n    return\n  }\n\n  indices = make([]map[string][]int, len(header))\n  var rowCount, colCount int = 0, len(header)\n  for rec, err := r.Read(); err == nil; rec, err = r.Read() {\n    if len(rec) != colCount {\n      return nil, nil, nil, errors.Errorf(\"Expected Columns: %d. Got %d columns in row %d\", colCount, len(rec), rowCount)\n    }\n    data = append(data, rec)\n    for j, val := range rec {\n      if indices[j] == nil {\n        indices[j] = make(map[string][]int)\n      }\n      indices[j][val] = append(indices[j][val], rowCount)\n    }\n    rowCount++\n  }\n  return\n}\n```", "```py\n// cardinality counts the number of unique values in a column. \n// This assumes that the index i of indices represents a column.\nfunc cardinality(indices []map[string][]int) []int {\n  retVal := make([]int, len(indices))\n  for i, m := range indices {\n    retVal[i] = len(m)\n  }\n  return retVal\n}\n```", "```py\nfunc main() {\n  f, err := os.Open(\"train.csv\")\n  mHandleErr(err)\n  hdr, data, indices, err := ingest(f)\n  mHandleErr(err)\n  c := cardinality(indices)\n\n  fmt.Printf(\"Original Data: \\nRows: %d, Cols: %d\\n========\\n\", len(data), len(hdr))\n  c := cardinality(indices)\n  for i, h := range hdr {\n    fmt.Printf(\"%v: %v\\n\", h, c[i])\n  }\n  fmt.Println(\"\")\n\n}\n```", "```py\n// mHandleErr is the error handler for the main function. \n// If an error happens within the main function, it is not \n// unexpected for a fatal error to be logged and for the program to immediately quit.\nfunc mHandleErr(err error){\n  if err != nil {\n    log.Fatal(err)\n  }\n}\n```", "```py\n$ go run *.go\nRows: 1460\n========\nId: 1460\nMSSubClass: 15\nMSZoning: 5\nLotFrontage: 111\nLotArea: 1073\nSaleCondition: 6\nSalePrice: 663\n```", "```py\n// hints is a slice of bools indicating whether it's a categorical variable\nfunc clean(hdr []string, data [][]string, indices []map[string][]int, hints []bool, ignored []string) (int, int, []float64, []float64, []string, []bool) {\n  modes := mode(indices)\n  var Xs, Ys []float64\n  var newHints []bool\n  var newHdr []string\n  var cols int\n\n  for i, row := range data {\n\n    for j, col := range row {\n      if hdr[j] == \"Id\" { // skip id\n        continue\n      }\n      if hdr[j] == \"SalePrice\" { // we'll put SalePrice into Ys\n        cxx, _ := convert(col, false, nil, hdr[j])\n        Ys = append(Ys, cxx...)\n        continue\n      }\n\n      if inList(hdr[j], ignored) {\n        continue\n      }\n\n      if hints[j] {\n        col = imputeCategorical(col, j, hdr, modes)\n      }\n      cxx, newHdrs := convert(col, hints[j], indices[j], hdr[j])\n      Xs = append(Xs, cxx...)\n\n      if i == 0 {\n        h := make([]bool, len(cxx))\n        for k := range h {\n          h[k] = hints[j]\n        }\n        newHints = append(newHints, h...)\n        newHdr = append(newHdr, newHdrs...)\n      }\n    }\n    // add bias\n\n    if i == 0 {\n      cols = len(Xs)\n    }\n  }\n  rows := len(data)\n  if len(Ys) == 0 { // it's possible that there are no Ys (i.e. the test.csv file)\n    Ys = make([]float64, len(data))\n  }\n  return rows, cols, Xs, Ys, newHdr, newHints\n}\n```", "```py\n// imputeCategorical replaces \"NA\" with the mode of categorical values\nfunc imputeCategorical(a string, col int, hdr []string, modes []string) string {\n  if a != \"NA\" || a != \"\" {\n    return a\n  }\n  switch hdr[col] {\n  case \"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \"Electrical\", \"KitchenQual\", \"SaleType\", \"Exterior1st\", \"Exterior2nd\":\n    return modes[col]\n  }\n  return a\n}\n```", "```py\n// mode finds the most common value for each variable\nfunc mode(index []map[string][]int) []string {\n  retVal := make([]string, len(index))\n  for i, m := range index {\n    var max int\n    for k, v := range m {\n      if len(v) > max {\n        max = len(v)\n        retVal[i] = k\n      }\n    }\n  }\n  return retVal\n}\n```", "```py\n// convert converts a string into a slice of floats\nfunc convert(a string, isCat bool, index map[string][]int, varName string) ([]float64, []string) {\n  if isCat {\n    return convertCategorical(a, index, varName)\n  }\n  // here we deliberately ignore errors, because the zero value of float64 is well, zero.\n  f, _ := strconv.ParseFloat(a, 64)\n  return []float64{f}, []string{varName}\n}\n\n// convertCategorical is a basic function that encodes a categorical variable as a slice of floats.\n// There are no smarts involved at the moment.\n// The encoder takes the first value of the map as the default value, encoding it as a []float{0,0,0,...}\nfunc convertCategorical(a string, index map[string][]int, varName string) ([]float64, []string) {\n  retVal := make([]float64, len(index)-1)\n\n  // important: Go actually randomizes access to maps, so we actually need to sort the keys\n  // optimization point: this function can be made stateful.\n  tmp := make([]string, 0, len(index))\n  for k := range index {\n    tmp = append(tmp, k)\n  }\n\n  // numerical \"categories\" should be sorted numerically\n  tmp = tryNumCat(a, index, tmp)\n\n  // find NAs and swap with 0\n  var naIndex int\n  for i, v := range tmp {\n    if v == \"NA\" {\n      naIndex = i\n      break\n    }\n  }\n  tmp[0], tmp[naIndex] = tmp[naIndex], tmp[0]\n\n  // build the encoding\n  for i, v := range tmp[1:] {\n    if v == a {\n      retVal[i] = 1\n      break\n    }\n  }\n  for i, v := range tmp {\n    tmp[i] = fmt.Sprintf(\"%v_%v\", varName, v)\n  }\n\n  return retVal, tmp[1:]\n}\n```", "```py\nfunc main() {\n f, err := os.Open(\"train.csv\")\n mHandleErr(err)\n hdr, data, indices, err := ingest(f)\n mHandleErr(err)\n fmt.Printf(\"Original Data: nRows: %d, Cols: %dn========n\", len(data), len(hdr))\n c := cardinality(indices)\n for i, h := range hdr {\n  fmt.Printf(\"%v: %vn\", h, c[i])\n }\n fmt.Println(\"\")\n fmt.Printf(\"Building into matricesn=============n\")\n rows, cols, XsBack, YsBack, newHdr, _ := clean(hdr, data, indices, datahints, nil)\n Xs := tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(XsBack))\n Ys := tensor.New(tensor.WithShape(rows, 1), tensor.WithBacking(YsBack\n fmt.Printf(\"Xs:\\n%+1.1snYs:\\n%1.1sn\", Xs, Ys)\n fmt.Println(\"\")\n}\n```", "```py\nOriginal Data:\nRows: 1460, Cols: 81\n========\nId: 1460\nMSSubClass: 15\nMSZoning: 5\nLotFrontage: 111\nLotArea: 1073\nStreet: 2\n ⋮\nBuilding into matrices\n=============\nXs:\n⎡ 0 0 ⋯ 1 0⎤\n⎢ 0 0 ⋯ 1 0⎥\n ⋮\n⎢ 0 0 ⋯ 1 0⎥\n⎣ 0 0 ⋯ 1 0⎦\nYs:\nC[2e+05 2e+05 ⋯ 1e+05 1e+05]\n```", "```py\nfunc CEF(Ys []float64, col int, index []map[string][]int) map[string]float64 {\n  retVal := make(map[string]float64)\n  for k, v := range index[col] {\n    var mean float64\n    for _, i := range v {\n      mean += Ys[i]\n    }\n    mean /= float64(len(v))\n    retVal[k]=mean\n  }\n  return retVal\n}\n```", "```py\n// plotCEF plots the CEF. This is a simple plot with only the CEF. \n// More advanced plots can be also drawn to expose more nuance in understanding the data.\nfunc plotCEF(m map[string]float64) (*plot.Plot, error) {\n  ordered := make([]string, 0, len(m))\n  for k := range m {\n    ordered = append(ordered, k)\n  }\n  sort.Strings(ordered)\n\n  p, err := plot.New()\n  if err != nil {\n    return nil, err\n  }\n\n  points := make(plotter.XYs, len(ordered))\n  for i, val := range ordered {\n    // if val can be converted into a float, we'll use it\n    // otherwise, we'll stick with using the index\n    points[i].X = float64(i)\n    if x, err := strconv.ParseFloat(val, 64); err == nil {\n      points[i].X = x\n    }\n\n    points[i].Y = m[val]\n  }\n  if err := plotutil.AddLinePoints(p, \"CEF\", points); err != nil {\n    return nil, err\n  }\n  return p, nil\n}\n```", "```py\nofInterest := 19 // variable of interest is in column 19\ncef := CEF(YsBack, ofInterest, indices)\nplt, err := plotCEF(cef)\nmHandleErr(err)\nplt.Title.Text = fmt.Sprintf(\"CEF for %v\", hdr[ofInterest])\nplt.X.Label.Text = hdr[ofInterest]\nplt.Y.Label.Text = \"Conditionally Expected House Price\"\nmHandleErr(plt.Save(25*vg.Centimeter, 25*vg.Centimeter, \"CEF.png\"))\n```", "```py\nfunc hist(a []float64) (*plot.Plot, error){\n  h, err := plotter.NewHist(plotter.Values(a), 10)\n  if err != nil {\n    return nil, err\n  }\n  p, err := plot.New()\n  if err != nil {\n    return nil, err\n  }\n\n  h.Normalize(1)\n  p.Add(h)\n  return p, nil\n}\n```", "```py\nhist, err := plotHist(YsBack)\nmHandleErr(err)\nhist.Title.Text = \"Histogram of House Prices\"\nmHandleErr(hist.Save(25*vg.Centimeter, 25*vg.Centimeter, \"hist.png\"))\n```", "```py\nfor i := range YsBack {\n YsBack[i] = math.Log1p(YsBack[i])\n }\n hist2, err := plotHist(YsBack)\n mHandleErr(err)\n hist2.Title.Text = \"Histogram of House Prices (Processed)\"\n mHandleErr(hist2.Save(25*vg.Centimeter, 25*vg.Centimeter, \"hist2.png\"))\n```", "```py\n  it, err := native.MatrixF64(Xs)\n  mHandleErr(err)\n  for i, isCat := range datahints {\n    if isCat {\n      continue\n    }\n    skewness := skew(it, i)\n    if skewness > 0.75 {\n      log1pCol(it, i)\n    }\n  }\n```", "```py\n// skew returns the skewness of a column/variable\nfunc skew(it [][]float64, col int) float64 {\n  a := make([]float64, 0, len(it[0]))\n  for _, row := range it {\n    for _, col := range row {\n      a = append(a, col)\n    }\n  }\n  return stat.Skew(a, nil)\n}\n\n// log1pCol applies the log1p transformation on a column\nfunc log1pCol(it [][]float64, col int) {\n  for i := range it {\n    it[i][col] = math.Log1p(it[i][col])\n  }\n}\n```", "```py\n  m64, err := tensor.ToMat64(Xs, tensor.UseUnsafe())\n  mHandleErr(err)\n  corr := stat.CorrelationMatrix(nil, m64, nil)\n  hm, err := plotHeatMap(corr, newHdr)\n  mHandleErr(err)\n  hm.Save(60*vg.Centimeter, 60*vg.Centimeter, \"heatmap.png\")\n```", "```py\ntype heatmap struct {\n  x mat.Matrix\n}\n\nfunc (m heatmap) Dims() (c, r int) { r, c = m.x.Dims(); return c, r }\nfunc (m heatmap) Z(c, r int) float64 { return m.x.At(r, c) }\nfunc (m heatmap) X(c int) float64 { return float64(c) }\nfunc (m heatmap) Y(r int) float64 { return float64(r) }\n\ntype ticks []string\n\nfunc (t ticks) Ticks(min, max float64) []plot.Tick {\n  var retVal []plot.Tick\n  for i := math.Trunc(min); i <= max; i++ {\n    retVal = append(retVal, plot.Tick{Value: i, Label: t[int(i)]})\n  }\n  return retVal\n}\n\nfunc plotHeatMap(corr mat.Matrix, labels []string) (p *plot.Plot, err error) {\n  pal := palette.Heat(48, 1)\n  m := heatmap{corr}\n  hm := plotter.NewHeatMap(m, pal)\n  if p, err = plot.New(); err != nil {\n    return\n  }\n  hm.NaN = color.RGBA{0, 0, 0, 0} // black\n\n  // add and adjust the prettiness of the chart\n  p.Add(hm)\n  p.X.Tick.Label.Rotation = 1.5\n  p.Y.Tick.Label.Font.Size = 6\n  p.X.Tick.Label.Font.Size = 6\n  p.X.Tick.Label.XAlign = draw.XRight\n  p.X.Tick.Marker = ticks(labels)\n  p.Y.Tick.Marker = ticks(labels)\n\n  // add legend\n  l, err := plot.NewLegend()\n  if err != nil {\n    return p, err\n  }\n\n  thumbs := plotter.PaletteThumbnailers(pal)\n  for i := len(thumbs) - 1; i >= 0; i-- {\n    t := thumbs[i]\n    if i != 0 && i != len(thumbs)-1 {\n      l.Add(\"\", t)\n      continue\n    }\n    var val float64\n    switch i {\n    case 0:\n      val = hm.Min\n    case len(thumbs) - 1:\n      val = hm.Max\n    }\n    l.Add(fmt.Sprintf(\"%.2g\", val), t)\n  }\n\n  // this is a hack. I place the legends between the axis and the actual heatmap\n  // because if the legend is on the right, we'd need to create a custom canvas to take\n  // into account the additional width of the legend.\n  //\n  // So instead, we shrink the legend width to fit snugly within the margins of the plot and the axes.\n  l.Left = true\n  l.XOffs = -5\n  l.ThumbnailWidth = 5\n  l.Font.Size = 5\n\n  p.Legend = l\n  return\n}\n```", "```py\n  // heatmaps are nice to look at, but are quite ridiculous.\n  var tba []struct {\n    h1, h2 string\n    corr float64\n  }\n  for i, h1 := range newHdr {\n    for j, h2 := range newHdr {\n      if c := corr.At(i, j); math.Abs(c) >= 0.5 && h1 != h2 {\n        tba = append(tba, struct {\n          h1, h2 string\n          corr float64\n        }{h1: h1, h2: h2, corr: c})\n      }\n    }\n  }\n  fmt.Println(\"High Correlations:\")\n  for _, a := range tba {\n    fmt.Printf(\"\\t%v-%v: %v\\n\", a.h1, a.h2, a.corr)\n  }\n```", "```py\nfunc scale(a [][]float64, j int) {\n  l, m, h := iqr(a, 0.25, 0.75, j)\n  s := h - l\n  if s == 0 {\n    s = 1\n  }\n\n  for _, row := range a {\n    row[j] = (row[j] - m) / s\n  }\n}\n\nfunc scaleStd(a [][]float64, j int) {\n  var mean, variance, n float64\n  for _, row := range a {\n    mean += row[j]\n    n++\n  }\n  mean /= n\n  for _, row := range a {\n    variance += (row[j] - mean) * (row[j] - mean)\n  }\n  variance /= (n-1)\n\n  for _, row := range a {\n    row[j] = (row[j] - mean) / variance\n  }\n}\n```", "```py\nfunc transform(it [][]float64, hdr []string, hints []bool) []int {\n  var transformed []int\n  for i, isCat := range hints {\n    if isCat {\n      continue\n    }\n    skewness := skew(it, i)\n    if skewness > 0.75 {\n      transformed = append(transformed, i)\n      log1pCol(it, i)\n    }\n  }\n  for i, h := range hints {\n    if !h {\n      scale(it, i)\n    }\n  }\n  return transformed\n}\n```", "```py\nfunc main() {\n  // exploratory() // commented out because we're done with exploratory work.\n\n  f, err := os.Open(\"train.csv\")\n  mHandleErr(err)\n  defer f.Close()\n  hdr, data, indices, err := ingest(f)\n  rows, cols, XsBack, YsBack, newHdr, newHints := clean(hdr, data, indices, datahints, ignored)\n  Xs := tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(XsBack))\n  it, err := native.MatrixF64(Xs)\n  mHandleErr(err)\n\n  // transform the Ys\n  for i := range YsBack {\n    YsBack[i] = math.Log1p(YsBack[i])\n  }\n  // transform the Xs\n  transform(it, newHdr, newHints)\n\n  // partition the data\n  shuffle(it, YsBack)\n  testingRows := int(float64(rows) * 0.2)\n  trainingRows := rows - testingRows\n  testingSet := it[trainingRows:]\n  testingYs := YsBack[trainingRows:]\n  it = it[:trainingRows]\n  YsBack = YsBack[:trainingRows]\n  log.Printf(\"len(it): %d || %d\", len(it), len(YsBack))\n...\n```", "```py\nfunc runRegression(Xs [][]float64, Ys []float64, hdr []string) (r *regression.Regression, stdErr []float64) {\n  r = new(regression.Regression)\n  dp := make(regression.DataPoints, 0, len(Xs))\n  for i, h := range hdr {\n    r.SetVar(i, h)\n  }\n  for i, row := range Xs {\n    if i < 3 {\n      log.Printf(\"Y %v Row %v\", Ys[i], row)\n    }\n    dp = append(dp, regression.DataPoint(Ys[i], row))\n  }\n  r.Train(dp...)\n  r.Run()\n\n  // calculate StdErr\n  var sseY float64\n  sseX := make([]float64, len(hdr)+1)\n  meanX := make([]float64, len(hdr)+1)\n  for i, row := range Xs {\n    pred, _ := r.Predict(row)\n    sseY += (Ys[i] - pred) * (Ys[i] - pred)\n    for j, c := range row {\n      meanX[j+1] += c\n    }\n  }\n  sseY /= float64(len(Xs) - len(hdr) - 1) // n - df ; df = len(hdr) + 1\n  vecf64.ScaleInv(meanX, float64(len(Xs)))\n  sseX[0] = 1\n  for _, row := range Xs {\n    for j, c := range row {\n      sseX[j+1] += (c - meanX[j+1]) * (c - meanX[j+1])\n    }\n  }\n  sseY = math.Sqrt(sseY)\n  vecf64.Sqrt(sseX)\n  vecf64.ScaleInvR(sseX, sseY)\n\n  return r, sseX\n}\n```", "```py\n  // do the regessions\n  r, stdErr := runRegression(it, YsBack, newHdr)\n  tdist := distuv.StudentsT{Mu: 0, Sigma: 1, Nu: float64(len(it) - len(newHdr) - 1), Src: rand.New(rand.NewSource(uint64(time.Now().UnixNano())))}\n  fmt.Printf(\"R^2: %1.3f\\n\", r.R2)\n  fmt.Printf(\"\\tVariable \\tCoefficient \\tStdErr \\tt-stat\\tp-value\\n\")\n  fmt.Printf(\"\\tIntercept: \\t%1.5f \\t%1.5f \\t%1.5f \\t%1.5f\\n\", r.Coeff(0), stdErr[0], r.Coeff(0)/stdErr[0], tdist.Prob(r.Coeff(0)/stdErr[0]))\n  for i, h := range newHdr {\n    b := r.Coeff(i + 1)\n    e := stdErr[i+1]\n    t := b / e\n    p := tdist.Prob(t)\n    fmt.Printf(\"\\t%v: \\t%1.5f \\t%1.5f \\t%1.5f \\t%1.5f\\n\", h, b, e, t, p)\n  }\n...\n```", "```py\n  // VERY simple cross validation\n  var MSE float64\n  for i, row := range testingSet {\n    pred, err := r.Predict(row)\n    mHandleErr(err)\n    correct := testingYs[i]\n    eStar := correct - pred\n    e2 := eStar * eStar\n    MSE += e2\n  }\n  MSE /= float64(len(testingSet))\n  fmt.Printf(\"RMSE: %v\\n\", math.Sqrt(MSE))\n```", "```py\nR^2: 0.871\n Variable Coefficient StdErr t-stat p-value\n Intercept: 12.38352 0.14768 83.85454 0.00000\n MSSubClass_30: -0.06466 0.02135 -3.02913 0.00412\n MSSubClass_40: -0.03771 0.08537 -0.44172 0.36175\n MSSubClass_45: -0.12998 0.04942 -2.63027 0.01264\n MSSubClass_50: -0.01901 0.01486 -1.27946 0.17590\n MSSubClass_60: -0.06634 0.01061 -6.25069 0.00000\n MSSubClass_70: 0.04089 0.02269 1.80156 0.07878\n MSSubClass_75: 0.04604 0.03838 1.19960 0.19420\n MSSubClass_80: -0.01971 0.02177 -0.90562 0.26462\n MSSubClass_85: -0.02167 0.03838 -0.56458 0.34005\n MSSubClass_90: -0.05748 0.02222 -2.58741 0.01413\n MSSubClass_120: -0.06537 0.01763 -3.70858 0.00043\n MSSubClass_160: -0.15650 0.02135 -7.33109 0.00000\n MSSubClass_180: -0.01552 0.05599 -0.27726 0.38380\n MSSubClass_190: -0.04344 0.02986 -1.45500 0.13840\n LotFrontage: -0.00015 0.00265 -0.05811 0.39818\n LotArea: 0.00799 0.00090 8.83264 0.00000\n Neighborhood_Blueste: 0.02080 0.10451 0.19903 0.39102\n Neighborhood_BrDale: -0.06919 0.04285 -1.61467 0.10835\n Neighborhood_BrkSide: -0.06680 0.02177 -3.06894 0.00365\n Neighborhood_ClearCr: -0.04217 0.03110 -1.35601 0.15904\n Neighborhood_CollgCr: -0.06036 0.01403 -4.30270 0.00004\n Neighborhood_Crawfor: 0.08813 0.02500 3.52515 0.00082\n Neighborhood_Edwards: -0.18718 0.01820 -10.28179 0.00000\n Neighborhood_Gilbert: -0.09673 0.01858 -5.20545 0.00000\n Neighborhood_IDOTRR: -0.18867 0.02825 -6.67878 0.00000\n Neighborhood_MeadowV: -0.24387 0.03971 -6.14163 0.00000\n Neighborhood_Mitchel: -0.15112 0.02348 -6.43650 0.00000\n Neighborhood_NAmes: -0.11880 0.01211 -9.81203 0.00000\n Neighborhood_NPkVill: -0.05093 0.05599 -0.90968 0.26364\n Neighborhood_NWAmes: -0.12200 0.01913 -6.37776 0.00000\n Neighborhood_NoRidge: 0.13126 0.02688 4.88253 0.00000\n Neighborhood_NridgHt: 0.16263 0.01899 8.56507 0.00000\n Neighborhood_OldTown: -0.15781 0.01588 -9.93456 0.00000\n Neighborhood_SWISU: -0.12722 0.03252 -3.91199 0.00020\n Neighborhood_Sawyer: -0.17758 0.02040 -8.70518 0.00000\n Neighborhood_SawyerW: -0.11027 0.02115 -5.21481 0.00000\n Neighborhood_Somerst: 0.05793 0.01845 3.13903 0.00294\n Neighborhood_StoneBr: 0.21206 0.03252 6.52102 0.00000\n Neighborhood_Timber: -0.00449 0.02825 -0.15891 0.39384\n Neighborhood_Veenker: 0.04530 0.04474 1.01249 0.23884\n HouseStyle_1.5Unf: 0.16961 0.04474 3.79130 0.00031\n HouseStyle_1Story: -0.03547 0.00864 -4.10428 0.00009\n HouseStyle_2.5Fin: 0.16478 0.05599 2.94334 0.00531\n HouseStyle_2.5Unf: 0.04816 0.04690 1.02676 0.23539\n HouseStyle_2Story: 0.03271 0.00937 3.49038 0.00093\n HouseStyle_SFoyer: 0.02498 0.02777 0.89968 0.26604\n HouseStyle_SLvl: -0.02233 0.02076 -1.07547 0.22364\n YearBuilt: 0.01403 0.00151 9.28853 0.00000\n YearRemodAdd: 5.06512 0.41586 12.17991 0.00000\n MasVnrArea: 0.00215 0.00164 1.30935 0.16923\n Foundation_CBlock: -0.01183 0.00873 -1.35570 0.15910\n Foundation_PConc: 0.01978 0.00869 2.27607 0.03003\n Foundation_Slab: 0.01795 0.03416 0.52548 0.34738\n Foundation_Stone: 0.03423 0.08537 0.40094 0.36802\n Foundation_Wood: -0.08163 0.08537 -0.95620 0.25245\n BsmtFinSF1: 0.01223 0.00145 8.44620 0.00000\n BsmtFinSF2: -0.00148 0.00236 -0.62695 0.32764\n BsmtUnfSF: -0.00737 0.00229 -3.21186 0.00234\n TotalBsmtSF: 0.02759 0.00375 7.36536 0.00000\n Heating_GasA: 0.02397 0.02825 0.84858 0.27820\n Heating_GasW: 0.06687 0.03838 1.74239 0.08747\n Heating_Grav: -0.15081 0.06044 -2.49506 0.01785\n Heating_OthW: -0.00467 0.10451 -0.04465 0.39845\n Heating_Wall: 0.06265 0.07397 0.84695 0.27858\n CentralAir_Y: 0.10319 0.01752 5.89008 0.00000\n 1stFlrSF: 0.01854 0.00071 26.15440 0.00000\n 2ndFlrSF: 0.01769 0.00131 13.46733 0.00000\n FullBath: 0.10586 0.01360 7.78368 0.00000\n HalfBath: 0.09048 0.01271 7.11693 0.00000\n Fireplaces: 0.07432 0.01096 6.77947 0.00000\n GarageType_Attchd: -0.37539 0.00884 -42.44613 0.00000\n GarageType_Basment: -0.47446 0.03718 -12.76278 0.00000\n GarageType_BuiltIn: -0.33740 0.01899 -17.76959 0.00000\n GarageType_CarPort: -0.60816 0.06044 -10.06143 0.00000\n GarageType_Detchd: -0.39468 0.00983 -40.16266 0.00000\n GarageType_2Types: -0.54960 0.06619 -8.30394 0.00000\n GarageArea: 0.07987 0.00301 26.56053 0.00000\n PavedDrive_P: 0.01773 0.03046 0.58214 0.33664\n PavedDrive_Y: 0.02663 0.01637 1.62690 0.10623\n WoodDeckSF: 0.00448 0.00166 2.69397 0.01068\n OpenPorchSF: 0.00640 0.00201 3.18224 0.00257\n PoolArea: -0.00075 0.00882 -0.08469 0.39742\n MoSold: 0.00839 0.01020 0.82262 0.28430\n YrSold: -4.27193 6.55001 -0.65220 0.32239\nRMSE: 0.1428929042451045\n```"]