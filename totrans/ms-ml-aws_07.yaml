- en: Customer Segmentation Using Clustering Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类算法进行客户细分
- en: This chapter will introduce the main clustering algorithms by exploring how
    to apply them to customer segmentation based on their behavioral patterns. In
    particular, we will demonstrate how Apache Spark and Amazon SageMaker can seamlessly
    interoperate to perform clustering. Throughout this chapter, we will be using
    the **Kaggle Dataset E-Commerce** data from **Fabien Daniel**,which can be downloaded
    from [https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过探讨如何根据他们的行为模式将聚类算法应用于客户细分来介绍主要的聚类算法。特别是，我们将演示Apache Spark和Amazon SageMaker如何无缝交互以执行聚类。在本章中，我们将使用**Kaggle数据集E-Commerce**，这是由**Fabien
    Daniel**提供的，可以从[https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)下载。
- en: 'Let''s take a look at the topics we will be  covering:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们将要讨论的主题：
- en: Understanding how clustering algorithms work
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类算法的工作原理
- en: Clustering with **Apache Spark** on **Elastic MapReduce** (**EMR**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**弹性映射减少**（**EMR**）上使用**Apache Spark**进行聚类
- en: Clustering using **SageMaker** through Spark integration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Spark集成使用**SageMaker**进行聚类
- en: Understanding How Clustering Algorithms Work
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类算法的工作原理
- en: '**Cluster analysis**, or clustering, is a process of grouping a set of observations
    based on their similarities. The idea is that the observations in a cluster are
    more similar to one another than the observations from other clusters. Hence,
    the outcome of this algorithm is a set of clusters that can identify the patterns
    in the dataset and arrange the data into different clusters.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类分析**，或聚类，是一个基于观察值的相似性将一组观察值分组的过程。其思想是，一个簇中的观察值彼此之间比来自其他簇的观察值更相似。因此，该算法的输出是一组簇，可以识别数据集中的模式并将数据安排到不同的簇中。'
- en: Clustering algorithms are referred to as **unsupervised learning algorithms**.
    Unsupervised learning does not depend on predicting ground truth and is designed
    to discover the natural patterns in the data. Since there is no ground truth provided,
    it is difficult to compare different unsupervised learning models. Unsupervised
    learning is generally used for exploratory analysis and dimensionality reduction.
    Clustering is an example of exploratory analysis. In this task, you are looking
    for patterns and structure in the dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法被称为**无监督学习算法**。无监督学习不依赖于预测真实值，旨在发现数据中的自然模式。由于没有提供真实值，比较不同的无监督学习模型比较困难。无监督学习通常用于探索性分析和降维。聚类是探索性分析的一个例子。在这个任务中，你正在寻找数据集中的模式和结构。
- en: 'This is different than the algorithms we have been studying so far in the book.
    **Naive Bayes**, **linear regression**, and **decision trees** algorithms are
    all examples of supervised learning. There is an assumption that each dataset
    has a set of observations and an event class associated with those observations.
    Hence, the data is already grouped based on the actual outcome event for each
    observation. However, not every dataset has labeled outcomes associated with each
    event. For example, consider a dataset where you have information regarding each
    transaction on an e-commerce website:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在书中迄今为止所研究的算法不同。**朴素贝叶斯**、**线性回归**和**决策树**算法都是监督学习的例子。有一个假设，即每个数据集都有一组观察结果和与这些观察结果相关的事件类别。因此，数据已经根据每个观察结果的实际结果事件进行分组。然而，并非每个数据集都与每个事件相关联有标记的结果。例如，考虑一个包含有关电子商务网站上每个交易的信息的数据集：
- en: '| **SKU** | **Item name** | **Customer ID** | **Country** |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| **SKU** | **商品名称** | **客户ID** | **国家** |'
- en: '| 12423 | iPhone | 10 | USA |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 12423 | iPhone | 10 | 美国 |'
- en: '| 12423 | iPhone | 11 | USA |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 12423 | iPhone | 11 | 美国 |'
- en: '| 12423 | iPhone | 12 | USA |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 12423 | iPhone | 12 | 美国 |'
- en: '| 11011 | Samsung S9 | 13 | UK |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 11011 | 三星S9 | 13 | 英国 |'
- en: '| 11011 | Samsung S9 | 10 | USA |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 11011 | 三星S9 | 10 | 美国 |'
- en: '| 11011 | Samsung S9 | 14 | UK |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 11011 | 三星S9 | 14 | 英国 |'
- en: 'This dataset is a list of transactions but does not have any class variable
    that informs us regarding what kind of users buy specific products. Hence, if
    the task is to identify patterns from this dataset, we cannot use any algorithms
    that can predict a specific event. That is where clustering algorithms come into
    the picture. We want to explore whether we can find trends in the transactions
    on the website based on the dataset. Let''s look at a simple example. Consider
    that the dataset only had one feature: **Item name**. We will discover that the
    data can be arranged in three clusters, namely, iPhone, Samsung S9, and Pixel
    2\. Similarly, if we consider that the only feature to cluster on is **Country**,
    the data can be clustered into two clusters: USA and UK. Once you generate the
    clusters, you can analyze the statistics in each cluster to understand the type
    of audience buying certain things.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集是一系列交易记录，但没有任何类别变量告诉我们哪些用户购买了特定的产品。因此，如果任务是识别数据集中的模式，我们不能使用任何可以预测特定事件的算法。这就是聚类算法发挥作用的地方。我们希望探索是否可以根据数据集找到网站交易的趋势。让我们看看一个简单的例子。假设数据集只有一个特征：**商品名称**。我们将发现数据可以排列成三个簇，即iPhone、Samsung
    S9和Pixel 2。同样，如果我们考虑的唯一聚类特征是**国家**，数据可以聚类成两个簇：USA和UK。一旦生成了簇，你就可以分析每个簇中的统计数据，以了解购买特定商品的观众类型。
- en: 'However, in most of your experiences, you will have to cluster the data based
    on more than one feature. There are many clustering algorithms that you can deploy
    in clustering the data into clusters. The following diagram shows an example of
    how clusters would look for the dataset:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数经验中，你将不得不根据多个特征对数据进行聚类。有许多聚类算法可以用来将数据聚类成簇。以下图表显示了数据集簇的示例：
- en: '![](img/6cece5e6-2a84-46dc-ad32-95548553028c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6cece5e6-2a84-46dc-ad32-95548553028c.png)'
- en: Clustering helps us get an outcome where we can group the data into two clusters
    and understand the patterns in each cluster. We may be able to cluster the customers
    into users who buy a certain kind of phone. By analyzing the clusters, we can
    learn the patterns of which users buy an iPhone or Samsung S9 phone.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类帮助我们得到一个结果，我们可以将数据分成两个簇，并理解每个簇中的模式。我们可能能够将客户聚类为购买某种手机的用户。通过分析簇，我们可以了解购买iPhone或Samsung
    S9手机的用户模式。
- en: 'In this chapter, we will study two common clustering approaches:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究两种常见的聚类方法：
- en: k-means clustering
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: Hierarchical clustering
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: k-means clustering
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: The **k-means clustering** algorithm aims to cluster a dataset into k clusters
    by selecting k centroids in the dataset. Each record is evaluated based on its
    distance to the centroid and assigned a cluster. Centroids are observations that
    are at the center of each cluster. To define k-means clustering formally, we are
    optimizing the **Within-Cluster Sum of the Square** *(***WCSS***)* distance of
    observations. Hence, the most optimal clustering would ensure that each cluster
    has all the observations close to its centroid, and as far away from the other
    centroids as possible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-means 聚类**算法旨在通过在数据集中选择k个质心来将数据集聚类成k个簇。每个记录根据其到质心的距离进行评估，并分配到一个簇中。质心是位于每个簇中心的观测值。为了正式定义k-means聚类，我们正在优化观测值的**簇内平方和**（***WCSS**）距离。因此，最理想的聚类将确保每个簇中的观测值都靠近其质心，并且尽可能远离其他质心。'
- en: 'There are two important parameters in k-means clustering. Firstly, we need
    to discover the centroids in our dataset. One of the popular methodologies for
    selecting centroids is called **Random partitioning**. This methodology uses a
    technique called **Expectation Maximization** (**EM**) to achieve high-quality
    clusters. In the first step, we randomly assign a cluster to each observation.
    Once each observation is assigned to a cluster, we calculate the quality of the
    cluster using the WCSS methodology:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类中有两个重要的参数。首先，我们需要在我们的数据集中发现质心。选择质心的流行方法之一称为**随机划分**。这种方法使用称为**期望最大化**（**EM**）的技术来获得高质量的簇。在第一步中，我们随机将一个簇分配给每个观测值。一旦每个观测值被分配到一个簇，我们就使用WCSS方法计算簇的质量：
- en: '![](img/ecf2864c-bc47-445c-9291-635fe6cba82a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecf2864c-bc47-445c-9291-635fe6cba82a.png)'
- en: '*J* represents the WCSS score for the clusters that are generated where we
    have *M* observations and have generated *K* clusters. ![](img/08a4dad4-65e2-404e-b49d-ab177ab1d566.png) is
    1 if the observation *i* belongs to cluster *k*, and 0 if the observation *i*
    does not belong to cluster *k*. ![](img/97a87758-272e-4eda-802e-7872dbd08ff4.png) is
    the observation, while ![](img/9b7475ed-080c-42b9-b83d-4782273d2d79.png) is the
    centroid of cluster *k*. The difference between ![](img/6f0c9f95-2408-4a18-8f67-99485a2df395.png) and ![](img/a6cc06fe-37a0-4392-ad0a-dac23cacbee4.png) represents
    the distance between the observation and the centroid. Our aim is to minimize
    the value of *J*.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*J*代表生成具有*M*个观察值和*K*个聚类的聚类的WCSS得分。如果观察值*i*属于聚类*k*，则![](img/08a4dad4-65e2-404e-b49d-ab177ab1d566.png)为1，如果不属于，则为0。![](img/97a87758-272e-4eda-802e-7872dbd08ff4.png)是观察值，而![](img/9b7475ed-080c-42b9-b83d-4782273d2d79.png)是聚类*k*的质心。![](img/6f0c9f95-2408-4a18-8f67-99485a2df395.png)和![](img/a6cc06fe-37a0-4392-ad0a-dac23cacbee4.png)之间的差异表示观察值和质心之间的距离。我们的目标是使*J*的值最小化。'
- en: 'In the next step, we calculate new centroids again based on the current clusters
    in the first step. This is the maximization step in the EM algorithm, where we
    try to step toward more optimal cluster assignments for records. The new centroid
    values are calculated using the following formula:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们根据第一步中的当前聚类再次计算新的质心。这是EM算法中的最大化步骤，我们试图为记录尝试更优的聚类分配。新的质心值使用以下公式计算：
- en: '![](img/f889985e-9683-4c62-aa3d-83ff060f2eec.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f889985e-9683-4c62-aa3d-83ff060f2eec.png)'
- en: This represents the fact that we recalculate the centroids based on the mean
    of the clusters created in the previous steps. Based on the new centroids, we
    assign each observation in the dataset to a centroid based on their distance to
    the centroids. Distance is the measure of similarity between two observations. We
    will discuss the concept of how to calculate distance later in this section. We
    recalculate the WCSS score for the new clusters and repeat the minimization step
    again. We repeat these steps until the assignments of the observations in the
    cluster do not change.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们根据前几步中创建的聚类均值重新计算质心。根据新的质心，我们根据每个观察值与质心的距离将数据集中的每个观察值分配给一个质心。距离是两个观察值之间相似度的度量。我们将在本节后面讨论如何计算距离的概念。我们重新计算新聚类的WCSS得分，并再次重复最小化步骤。我们重复这些步骤，直到聚类中观察值的分配不再改变。
- en: Although the random partition algorithm allows the k-means algorithm to discover
    centroids with a low WCSS score, they do not guarantee a global optimum solution.
    This is because the EM algorithm may greedily find a local optimum solution and
    stop exploring, for a more optimum solution. Also, selecting different random
    centroids in the first step may lead to different optimal solutions at the end
    of this algorithm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机划分算法允许k-means算法以较低的WCSS得分发现质心，但它们并不能保证全局最优解。这是因为EM算法可能会贪婪地找到一个局部最优解并停止探索，以寻找更优的解。此外，在第一步中选择不同的随机质心可能会导致算法结束时得到不同的最优解。
- en: To address this issue, there are other algorithms such as the **Forgy algorithm**,
    where we choose random observations from the dataset as centroids in the first
    step. This leads to more spread out centroids in the first step, compared to the
    random partition algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，有其他算法，例如**Forgy算法**，我们在第一步中选择数据集中的随机观察值作为质心。与随机划分算法相比，这导致第一步中的质心分布更分散。
- en: As we discussed before, we have to calculate the distance between the observations
    and the centroid of the cluster. There are various methodologies to calculate
    this distance. The two popular methodologies are the **Euclidean distance** and
    the **Manhattan distance**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们必须计算观察值和聚类质心之间的距离。有各种方法来计算这个距离。两种流行的方 法是**欧几里得距离**和**曼哈顿距离**。
- en: Euclidean distance
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The Euclidean distance between two points is the length of the line connecting
    them. For the n-dimensional points *P* and *Q*, where both the vectors have *n*
    values, the Euclidean distance is calculated using this formula:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两个点之间的欧几里得距离是连接它们的线段的长度。对于具有*n*个值的n维点*P*和*Q*，欧几里得距离使用以下公式计算：
- en: '![](img/bd0b051c-0341-4fc1-a596-833eb3bf5e88.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd0b051c-0341-4fc1-a596-833eb3bf5e88.png)'
- en: If the values of the data points are categorical values, then ![](img/02482353-46c5-4319-a68f-32cd65412314.png) is
    1 if both the observations have the same values for a feature and 0 if the observations
    have different values. For continuous variables, we can calculate the normalized
    distance between the values of the attributions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据点的值是分类值，那么 ![图片](img/02482353-46c5-4319-a68f-32cd65412314.png) 如果两个观测值对于某个特征具有相同的值则为
    1，如果观测值具有不同的值则为 0。对于连续变量，我们可以计算属性值之间的归一化距离。
- en: Manhattan distance
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: 'The **Manhattan distance** is the sum of absolute differences between two data
    points. For the n-dimensional points *P* and *Q*, where both the vectors have
    *n* values, we calculate the Manhattan distance using the following formula:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离是两个数据点之间绝对差值的总和。对于具有 *n* 个值的 n 维点 *P* 和 *Q*，我们使用以下公式计算曼哈顿距离：
- en: '![](img/d7af374d-c24a-4cc7-9975-521e9b808d0a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7af374d-c24a-4cc7-9975-521e9b808d0a.png)'
- en: The Manhattan distance reduces the effects of outliers in the data, and hence,
    should be used when we have noisy data with a lot of outliers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离减少了数据中异常值的影响，因此，当我们有噪声数据且异常值很多时，应该使用曼哈顿距离。
- en: Hierarchical clustering
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: '**Hierarchical clustering** aims to build a hierarchical structure of clusters
    from the observations. There are two strategies for generating the hierarchy:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次聚类**旨在从观测值构建聚类的层次结构。有两种生成层次结构的方法：'
- en: '**Agglomerative clustering**: In this approach, we use a bottom-up methodology,
    where each observation starts as its own cluster and clusters are merged at each
    stage of generating a hierarchy.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合聚类**：在这种方法中，我们使用自下而上的方法，其中每个观测值最初是其自己的聚类，并在生成层次结构的每个阶段合并聚类。'
- en: '**Divisive clustering**: In this approach, we use a top-down methodology, where
    we divide the observations into smaller clusters as we move down the stages of
    the hierarchy.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**划分聚类**：在这种方法中，我们使用自上而下的方法，随着我们向下移动层次结构的阶段，我们将观测值划分为更小的聚类。'
- en: Agglomerative clustering
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合聚类
- en: In **agglomerative clustering**, we start with each observation as its own cluster
    and combine these clusters based on certain criteria so that we end up with one
    cluster that contains all the observations. Similar to k-means clustering, we
    use distance metrics such as the Manhattan distance and the Euclidean distance
    in order to calculate the distance between two observations. We also use **linkage
    criteria**, which can represent the distance between two clusters. In this section,
    we study three linkage criteria, namely, **complete-linkage clustering**, **single-linkage
    clustering**, and **average-linkage clustering**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **聚合聚类** 中，我们以每个观测值作为其自己的聚类开始，并根据某些标准将这些聚类合并，以便我们最终得到一个包含所有观测值的聚类。类似于 k-means
    聚类，我们使用距离度量，如曼哈顿距离和欧几里得距离来计算两个观测值之间的距离。我们还使用 **连接标准**，它可以表示两个聚类之间的距离。在本节中，我们研究了三种连接标准，即 **完全连接聚类**、**单连接聚类**和**平均连接聚类**。
- en: 'Complete-linkage clustering calculates the distance between two clusters as
    the maximum distance between observations from two clusters and is represented
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接聚类计算两个聚类之间的距离为两个聚类中观测值的最大距离，表示如下：
- en: '![](img/f6e4cb11-684a-4bb2-a529-8b0a410a4bf8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6e4cb11-684a-4bb2-a529-8b0a410a4bf8.png)'
- en: 'Single-linkage clustering calculates the distance between two clusters as the
    minimum distance between observations from two clusters and is represented as
    follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 单连接聚类计算两个聚类之间的距离为两个聚类中观测值的最近距离，表示如下：
- en: '![](img/579ab169-d590-4bb8-8ca2-3b17d2c5911c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/579ab169-d590-4bb8-8ca2-3b17d2c5911c.png)'
- en: 'Average-linkage clustering calculates the distance between each observation
    from cluster A with cluster B and normalizes it based on the observations in cluster
    A and B. This is represented as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 平均连接聚类计算来自聚类 A 的每个观测值与聚类 B 之间的距离，并根据聚类 A 和 B 中的观测值进行归一化。这表示如下：
- en: '![](img/c3561735-c411-4586-93f0-5a95df3b2c4c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c3561735-c411-4586-93f0-5a95df3b2c4c.png)'
- en: Thus, in the first step of agglomerative clustering, we use distance methodology
    to calculate the distance between each observation and merge observations with
    the smallest distance. For the second step, we calculate the distances between
    each cluster using linkage criteria based on the methodologies just presented.
    We run the necessary iterations until we only have one cluster left with all observations
    in it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在层次聚类法的第一步中，我们使用距离方法计算每个观测值之间的距离，并将距离最小的观测值合并。对于第二步，我们使用刚刚介绍的方法的链接标准来计算每个簇之间的距离。我们运行必要的迭代，直到只剩下一个包含所有观测值的簇。
- en: 'The following diagram shows how agglomerative clustering would work for observations
    with only one continuous variable:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了对于只有一个连续变量的观测值，聚合聚类将如何工作：
- en: '![](img/0ea5a2bb-81df-45e0-a62b-76365fedbabe.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ea5a2bb-81df-45e0-a62b-76365fedbabe.png)'
- en: In this example, we have five observations with one continuous feature. In the
    first iteration, we look at the Euclidean distance between each observation and
    can deduce that records 1 and 2 are closest to each other. Hence, in the first
    iteration, we merge the observations 1 and 2\. In the second iteration, we discover
    that the observations 10 and 15 are the closest records and create a new cluster
    from it. In the third iteration, we observe that the distance between the (1,2) cluster
    and the (10,15) cluster is smaller than any of those clusters and observation
    90**.** Hence, we create a cluster of (1,2,10,15) in the third iteration. Finally,
    in the last iteration, we add element 90 to the cluster and terminate the process
    of agglomerative clustering.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有五个观测值和一个连续特征。在第一次迭代中，我们查看每个观测值之间的欧几里得距离，可以推断出记录1和2彼此最接近。因此，在第一次迭代中，我们将观测值1和2合并。在第二次迭代中，我们发现观测值10和15是最接近的记录，并从中创建一个新的簇。在第三次迭代中，我们观察到(1,2)簇和(10,15)簇之间的距离小于这些簇和观测值90**。**
    因此，我们在第三次迭代中创建了一个包含(1,2,10,15)的簇。最后，在最后一次迭代中，我们将元素90添加到簇中，并终止聚合聚类的过程。
- en: Divisive clustering
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层聚类
- en: '**Divisive clustering** is a top-bottom approach where we first start with
    a large cluster with all observations and, at iteration, we split the clusters
    into smaller clusters. The process is similar to using distances and linkage criteria
    such as agglomerative clustering. The aim is to find an observation or cluster
    in the larger cluster that has the furthest distance from the rest of the cluster.
    In each iteration, we look at a cluster and recursively split the larger clusters by
    finding clusters that have the farthest distance from one another. Finally, the
    process is stopped when each observation is its own cluster. Divisive clustering
    uses an exhaustive search to find the perfect split in each cluster, which may
    be computationally very expensive.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**分层聚类**是一种自上而下的方法，我们首先从一个包含所有观测值的大簇开始，并在迭代过程中将簇分割成更小的簇。这个过程类似于使用距离和链接标准，如聚合聚类。目标是找到一个在较大簇中与簇内其他部分距离最远的观测值或簇。在每次迭代中，我们查看一个簇，并通过找到彼此之间距离最远的簇来递归地分割较大的簇。最后，当每个观测值都是其自己的簇时，停止这个过程。分层聚类使用穷举搜索在每个簇中找到完美的分割，这可能在计算上非常昂贵。'
- en: Hierarchical clustering approaches are computationally more expensive than a
    k-means approach. Hence, even on medium-sized datasets, hierarchical cluster approaches
    may struggle to generate results compared to a k-means approach. However, since
    we do not need to start with a random partition at the start of hierarchical clustering,
    they remove the risks in the k-means approach where a bad random partition may
    hurt the clustering process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-means方法相比，层次聚类方法在计算上更昂贵。因此，即使在中等规模的数据集上，层次聚类方法可能比k-means方法更难以生成结果。然而，由于我们不需要在层次聚类的开始时从一个随机的分区开始，它们消除了k-means方法中的风险，即一个糟糕的随机分区可能会损害聚类过程。
- en: Clustering with Apache Spark on EMR
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EMR上使用Apache Spark进行聚类
- en: 'In this section, we step through the creation of a clustering model capable
    of grouping consumer patterns in three distinct clusters. The first step will
    be to launch an EMR notebook along with a small cluster (a single `m5.xlarge`
    node works fine as the dataset we selected is not very large). Simply follow these
    steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步创建一个聚类模型，该模型能够将消费者模式分为三个不同的簇。第一步将是启动一个EMR笔记本和一个小簇（由于我们选择的数据集不是很大，一个`m5.xlarge`节点就足够了）。只需遵循以下步骤：
- en: 'The first step is to load the dataframe and inspect the dataset:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是加载数据框并检查数据集：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the first few lines of our `df` dataframe:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了我们的`df`数据框的前几行：
- en: '![](img/0ac39800-71d9-4400-a9e8-e0fe9d0518be.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ac39800-71d9-4400-a9e8-e0fe9d0518be.png)'
- en: 'As you see, the dataset involves transactions of products bought by different
    customers at different times and in different locations. We attempt to cluster
    these customer transactions using k-means by looking at three factors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，数据集涉及不同客户在不同时间、不同地点购买的产品交易。我们尝试通过查看三个因素来使用k-means对这些客户交易进行聚类：
- en: The product (represented by the `StockCode` column)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品（由`StockCode`列表示）
- en: The country where the product was bought
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买产品的国家
- en: The total amount spent by the customer across all products
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户在所有产品上的总消费金额
- en: Note that this last factor is not directly available in the dataset, but it
    seems like an intuitively valuable feature (whether the client is a big spender
    or not). Oftentimes, during our feature preparation, we need to find aggregate
    values and plug them into our dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后一个因素在数据集中不可直接获得，但它似乎是一个直观上有价值的特征（客户是否是大额消费者）。在特征准备过程中，我们经常需要找到汇总值并将它们插入到我们的数据集中。
- en: 'On this occasion, we first find the total amount spent by each customer by
    multiplying the `Quantity` and `UnitPrice` columns on a new column:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这次事件中，我们首先通过将`Quantity`和`UnitPrice`列相乘来找到每个客户的总消费金额：
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following screenshot shows the first few lines of our modified `df` dataframe:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了我们的修改后的`df`数据框的前几行：
- en: '![](img/ce250198-23ec-4819-b866-11e0ccfa48c2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce250198-23ec-4819-b866-11e0ccfa48c2.png)'
- en: 'Then, we proceed to aggregate the `TotalBought` column by a customer:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们继续按客户聚合`TotalBought`列：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot shows the first few lines of the `customer_df` dataframe:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`customer_df`数据框的前几行：
- en: '![](img/29eae442-050e-48aa-9bb0-59c99d1f00e6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/29eae442-050e-48aa-9bb0-59c99d1f00e6.png)'
- en: 'We can then join back this new column back to our original dataset based on
    the customer:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以根据客户将这个新列重新连接到我们的原始数据集：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot shows the first few lines of the `joined_df` dataframe:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`joined_df`数据框的前几行：
- en: '![](img/cbc94201-5161-4703-8c1a-cc5aa0687792.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbc94201-5161-4703-8c1a-cc5aa0687792.png)'
- en: Note that two of the features that we are interested in using for clustering
    (**Country** and **StockCode**) are categorical. Hence, we need to find a way
    to encode those two numbers, similar to what we did in the previous chapter. String
    indexing these features would not be suitable in this case, as k-means works by
    computing distances between data points. Distances between artificial indices
    assigned to string values do not convey a lot of information. Instead, we apply
    one hot encoding to these features so that the vector distances represent something
    meaningful (note that two data points coinciding on most vector components have
    a cosine or Euclidean distance closer to 0).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们感兴趣的用于聚类的两个特征（**国家**和**StockCode**）是分类的。因此，我们需要找到一种方法来编码这两个数字，类似于我们在上一章中所做的。在这种情况下，对这些特征进行字符串索引是不合适的，因为k-means通过计算数据点之间的距离来工作。分配给字符串值的人工索引之间的距离不包含很多信息。相反，我们对这些特征应用独热编码，以便向量距离表示有意义的含义（注意，两个在大多数向量组件上巧合的数据点具有接近0的余弦或欧几里得距离）。
- en: Our pipeline will consist of two one hot encoding steps (for **Country** and
    **Product**), and a column that represents whether a customer is a big, normal,
    or small spender. To determine this, we discretize the `SumTotalBought` column
    into three values using a `QuantileDiscretizer`, which will result in three buckets
    depending on the quantile each customer falls into. We use the vector assembler
    to compile a vector of features. Given that the k-means algorithm works by computing
    distances, we normalize the feature vector so that the third feature (spender
    bucker) does not have a higher influence on the distance, as it has larger absolute
    values in the vector component. Finally, our pipeline will run the k-means estimator.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流程将包括两个独热编码步骤（用于**国家**和**产品**），以及一个表示客户是大额、正常还是小额消费者的列。为了确定这一点，我们使用`QuantileDiscretizer`将`SumTotalBought`列离散化为三个值，这将根据每个客户所在的分位数产生三个桶。我们使用向量组装器来编译特征向量。鉴于k-means算法通过计算距离来工作，我们规范化特征向量，以便第三个特征（支出桶）不会对距离有更高的影响，因为它在向量组件中有更大的绝对值。最后，我们的流程将运行k-means估计器。
- en: 'In the following code block, we define the stages of our pipeline and fit a
    model:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们定义了流程的阶段并拟合了一个模型：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we have a model, we apply that model to our dataset to obtain the clusters
    each transaction falls into:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了模型，我们就将该模型应用于我们的数据集以获得每个交易所属的簇：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following screenshot shows the first lines of the `df_with_clusters` dataframe:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了`df_with_clusters`数据框的前几行：
- en: '![](img/32b5eb56-16ff-4907-b74a-8ff01df740c5.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32b5eb56-16ff-4907-b74a-8ff01df740c5.png)'
- en: 'Note the new **prediction** column, which holds the value of cluster each row
    belongs to. We evaluate how well the clusters were formed by using the silhouette
    metric, which measures how similar data points are within their cluster compared
    to other clusters:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意新的**预测**列，它包含每行所属簇的值。我们使用轮廓指标评估簇的形成情况，该指标衡量数据点在其簇内与其他簇的相似性：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, we got a value of 0.35, which is average as a clustering score
    (ideally it's near 1.0, but at least it's positive). One main reason for not having
    a larger value is because we did not reduce the dimensionality of our vectors.
    Typically, before clustering, we apply some transformation for reducing the cardinality
    of the feature vector, such as **principal component analysis** (**PCA**). We
    didn't include such a step in this example for simplicity.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们得到了0.35的值，这是一个平均的聚类分数（理想情况下它接近1.0，但至少是正的）。没有更大值的主要原因是我们没有减少向量的维度。通常，在聚类之前，我们应用一些变换来减少特征向量的基数，例如**主成分分析**（**PCA**）。为了简化，我们没有在这个例子中包含这一步。
- en: 'We can now examine each cluster to have a sense of how the data was clustered.
    The first thing to look at is the size of each cluster. As we can see in the following,
    the clusters vary in size, where one cluster captures more than half of the data
    points:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以检查每个簇，以了解数据是如何聚类的。首先要注意的是每个簇的大小。正如我们下面可以看到的，簇的大小各不相同，其中一个簇捕获了超过一半的数据点：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following diagram shows the relative sizes of the different clusters:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了不同簇的相对大小：
- en: '![](img/9599bbf9-7d2f-4c94-89c5-5c9ca363c0bd.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9599bbf9-7d2f-4c94-89c5-5c9ca363c0bd.png)'
- en: 'If we look at the countries contained on each cluster, we can see that two
    clusters just contain data points from the UK, and the third cluster only contains
    points from the rest of the countries. We first inspect the counts for cluster
    0:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们查看每个簇包含的国家，我们可以看到有两个簇仅包含来自英国的数据点，第三个簇只包含来自其他国家的数据点。我们首先检查簇0的计数：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, the count for cluster 1 is displayed:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，显示簇1的计数：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, the count for cluster 2 is shown:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，显示簇2的计数：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'An interesting observation is that the different clusters seem to have very
    different spending profiles:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个有趣的观察是，不同的簇似乎有非常不同的消费模式：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding `plot()` command produces the following diagram:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`plot()`命令生成了以下图：
- en: '![](img/9a7332a4-f25c-4dcf-889d-a2ee487523c1.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a7332a4-f25c-4dcf-889d-a2ee487523c1.png)'
- en: 'To have a sense of how each cluster classifies the products, we look at the
    product description field of the different clusters. A nice visual representation
    is to use a word cloud with the words that appear on the product descriptions
    of each cluster. Using the Python `wordcloud` library, we can create a function
    that strips out the words of the products and constructs a wordcloud:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了了解每个簇如何分类产品，我们查看不同簇的产品描述字段。一个很好的视觉表示是使用词云，其中包含每个簇产品描述中出现的单词。使用Python的`wordcloud`库，我们可以创建一个函数，该函数去除产品的单词并构建一个词云：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We call this function on each cluster and obtain the following:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在每个簇上调用此函数并得到以下结果：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting word cloud for cluster 0 is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 簇0的结果词云如下：
- en: '![](img/c7bf2242-eafa-4777-9c1d-76175feba753.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7bf2242-eafa-4777-9c1d-76175feba753.png)'
- en: 'Take a look at the following code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看下面的代码：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting word cloud for cluster 1 is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 簇1的结果词云如下：
- en: '![](img/80f586a8-d947-4a86-93aa-c6b130d6e63d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80f586a8-d947-4a86-93aa-c6b130d6e63d.png)'
- en: 'Take a look at the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的代码：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting word cloud for cluster 2 is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 簇2的结果词云如下：
- en: '![](img/80b490b1-b5bb-47f2-ad51-3207ef270632.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80b490b1-b5bb-47f2-ad51-3207ef270632.png)'
- en: We can see in the word clouds that, despite some very common words, the relative
    importance of a few words such as C*hristmas* or *retrospot* comes out with a
    higher weight on one of the clusters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在词云中看到，尽管有一些非常常见的单词，但像*C*hristmas*或*retrospot*这样的几个单词的相对重要性在簇中的一个权重更高。
- en: Clustering with Spark and SageMaker on EMR
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EMR上使用Spark和SageMaker进行聚类
- en: In this section, we will show how **Spark** and **SageMaker** can work together
    seamlessly through code integration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示 **Spark** 和 **SageMaker** 如何通过代码集成无缝协作。
- en: In the previous chapter, regarding decision trees, we performed the data preparation
    in Apache Spark through **EMR** and uploaded the prepared data in S3 to then open
    a SageMaker notebook instance using the `SageMaker` Python library to perform
    the training. There is an alternative way of doing the same thing, which, on many
    occasions, is more convenient, using the `sagemaker_pyspark` library. This library
    allows us to perform the training stage through SageMaker services just by adding
    a special stage to our pipeline. To do this, we will define a pipeline identical
    to the one we wrote in the previous section, with the difference being the last
    stage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章关于决策树的内容中，我们通过 Apache Spark 进行数据准备，通过 **EMR** 上传准备好的数据到 S3，然后使用 `SageMaker`
    Python 库打开 SageMaker 笔记本实例以执行训练。有一种做同样事情的不同方法，在很多情况下更为方便，那就是使用 `sagemaker_pyspark`
    库。这个库允许我们通过在我们的管道中添加一个特殊阶段，通过 SageMaker 服务执行训练阶段。为此，我们将定义一个与上一节中写的管道相同的管道，区别在于最后一个阶段。
- en: 'Instead of including Apache Spark''s implementation of `KMeans`, we will use  `KMeansSageMakerEstimator`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `KMeansSageMakerEstimator` 而不是包含 Apache Spark 的 `KMeans` 实现：
- en: 'Firstly, we will import all the necessary dependencies:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入所有必要的依赖项：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we start by defining the IAM role to use and the full pipeline:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们首先定义要使用的 IAM 角色和完整管道：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`KMeansSageMakerEstimator` implements the estimator interface from Apache Spark,
    so it is included as any other estimator or transformer on our pipelines. Through
    the `KMeansSageMakerEstimator` constructor, we define the amount and type of machines
    to use as well as the IAM role. We explain the purpose of the role in the next
    subsection, *Understanding the purpose of the IAM Role*. Additionally, we set
    the number of clusters we want to create (value of *k*) as well as the length
    of the vectors we''ll be using for training (which we find by examining the output
    rows from the last section).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeansSageMakerEstimator` 实现了 Apache Spark 的估算器接口，因此它被包含在我们的管道上的任何其他估算器或转换器中。通过
    `KMeansSageMakerEstimator` 构造函数，我们定义要使用的机器的数量和类型以及 IAM 角色。我们将在下一小节中解释该角色的目的，*理解
    IAM 角色的目的*。此外，我们设置我们想要创建的集群数量（*k* 的值）以及我们将用于训练的向量的长度（我们通过检查上一节输出的行来找到它）。'
- en: Let's look at what happens under the hood when we call `fit()` on the pipeline. The
    first part of the pipeline works exactly the same as before, whereby the different
    stages launch Spark jobs that run a series of transformations to the dataset by
    appending columns (for example, the encoded vectors or discretized features).
    The last stage, being a SageMaker estimator, works in a slightly different way.
    Instead of using the EMR cluster resources to compute and train the clustering
    model, it saves the data in S3 and makes an API call to the SageMaker KMeans service
    pointing to that S3 temporary location. The SageMaker service, in turn, spins
    up EC2 servers to perform the training and creates both a SageMaker model and
    endpoint. Once the training is complete, the `KMeansSageMakerEstimator` stores
    a reference to the newly created endpoint that is used each time the model's `transfom()`
    method is called.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们对管道调用 `fit()` 时幕后发生了什么。管道的前一部分工作方式与之前完全相同，即不同的阶段启动 Spark 作业，通过添加列（例如，编码向量或离散特征）对数据集执行一系列转换。最后一个阶段，作为一个
    SageMaker 估算器，工作方式略有不同。它不是使用 EMR 集群资源来计算和训练聚类模型，而是将数据保存到 S3，并通过指向该 S3 临时位置的 API
    调用 SageMaker KMeans 服务。SageMaker 服务随后启动 EC2 服务器以执行训练，并创建一个 SageMaker 模型和端点。一旦训练完成，`KMeansSageMakerEstimator`
    存储对新创建的端点的引用，每次调用模型的 `transform()` 方法时都会使用该端点。
- en: You can find the models and endopoints created by  `KMeansSageMakerEstimator` by
    inspecting the SageMaker AWS console at [https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查 SageMaker AWS 控制台在 [https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/)
    来找到由 `KMeansSageMakerEstimator` 创建的模型和端点。
- en: 'Now, follow these steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，按照以下步骤操作：
- en: 'Let''s examine what happens when we call the `transform()` method of the pipeline:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看当我们调用管道的 `transform()` 方法时会发生什么：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The first series of transformations (the data preparation stages) will run on
    the EMR cluster through Spark jobs. As the final transformation is a SageMaker
    model, it relies on the SageMaker endpoint to obtain the predictions (in our case,
    the cluster assignment).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组转换（数据准备阶段）将通过Spark作业在EMR集群上运行。作为最终转换的SageMaker模型依赖于SageMaker端点来获取预测（在我们的案例中，是集群分配）。
- en: You should remember to delete the endpoint (for example using the console) once
    it's no longer required.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 应该记住，一旦不再需要，就要删除端点（例如使用控制台）。
- en: 'Then, run the following code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，运行以下代码：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Take a look at the following screenshot:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的截图：
- en: '![](img/98afac8f-020c-4fe6-89d6-cd9834d34d98.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98afac8f-020c-4fe6-89d6-cd9834d34d98.png)'
- en: '*(*The image has been truncated to show just the last few columns.*)*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*(*图片已被截断，仅显示最后几列.*)*'
- en: Note how the two columns were added by the  `distance_to_cluster` and `closest_cluster`
    pipelines.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`distance_to_cluster`和`closest_cluster`管道是如何添加这两个列的。
- en: 'By instructing the cluster evaluator to use this column, we can evaluate the
    clustering ability:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指示集群评估器使用此列，我们可以评估聚类能力：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The silhouette value we get is almost the same as the one using Spark's algorithm.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的轮廓值几乎与使用Spark算法得到的一样。
- en: Understanding the purpose of the IAM role
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解IAM角色的目的
- en: 'SageMaker is a managed service on AWS that manages the hardware responsible
    for training and inference. In order for SageMaker to perform such tasks on your
    behalf, you need to allow it through IAM configuration. For example, if you''re
    running on EMR, the EC2 instances (that is, the computers) in the cluster are
    running with a specific role. This role can be found by going to the cluster page
    on the EMR console: [https://console.aws.amazon.com/elasticmapreduce/.](https://console.aws.amazon.com/elasticmapreduce/)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker是AWS上的一种托管服务，负责管理训练和推理所需的硬件。为了使SageMaker能够代表您执行此类任务，您需要通过IAM配置允许它。例如，如果您在EMR上运行，集群中的EC2实例（即计算机）正在运行具有特定角色的配置。您可以通过访问EMR控制台上的集群页面来找到此角色：[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)
- en: 'The following screenshot shows the cluster details, including the security
    and access information:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了集群的详细信息，包括安全和访问信息：
- en: '![](img/784faccc-3f9a-42a9-a96b-af2e924b2207.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/784faccc-3f9a-42a9-a96b-af2e924b2207.png)'
- en: The role under EC2 instance profile in the previous screenshot is the one we
    are using, that is, `EMR_EC2_DefaultRole`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的截图中的EC2实例配置文件下的角色就是我们正在使用的，即`EMR_EC2_DefaultRole`。
- en: 'We then go to the IAM console at [https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home)
    to edit the permissions of that role to grant access to SageMaker resources, as
    well as allow the role to be assumed:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们转到IAM控制台[https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home)来编辑该角色的权限，以授予对SageMaker资源的访问权限，并允许该角色被假定：
- en: '![](img/08002147-dc8f-4698-bb68-17ffa7ea963f.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08002147-dc8f-4698-bb68-17ffa7ea963f.png)'
- en: 'In the Trust relationships section, we click on the Edit trust relationship
    button to open the dialog that will allow us to add the settings:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在信任关系部分，我们点击编辑信任关系按钮以打开对话框，允许我们添加设置：
- en: '![](img/54eee129-2ba9-4315-9eb4-e3067ebcbf48.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54eee129-2ba9-4315-9eb4-e3067ebcbf48.png)'
- en: 'You can edit and allow the role to be assumed as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以如下编辑并允许角色被假定：
- en: '![](img/0fa43a28-a41b-4805-8db5-4e66534142aa.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0fa43a28-a41b-4805-8db5-4e66534142aa.png)'
- en: The previous changes are required to allow our EMR cluster to talk to SageMaker
    and enable the kind of integration described in this section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的更改是必要的，以便我们的EMR集群能与SageMaker通信，并启用本节中描述的集成类型。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied the difference between supervised and unsupervised
    learning and looked at situations when unsupervised learning is applied. We studied
    the exploratory analysis application of unsupervised learning, where clustering
    approaches are used. We studied the k-means clustering and hierarchical clustering
    approaches in detail and looked at examples of how they are applied.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了监督学习和无监督学习之间的区别，并探讨了无监督学习应用的情况。我们研究了无监督学习的探索性分析应用，其中使用了聚类方法。我们详细研究了k-means聚类和层次聚类方法，并探讨了它们的应用示例。
- en: We also looked at how clustering approaches can be implemented on Apache Spark
    on AWS clusters. In our experience, clustering tasks are generally done on larger
    datasets, and, hence, taking the setup of the cluster into account for such tasks
    is important. We discussed these nuances in this chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了如何在AWS集群上实现Apache Spark的聚类方法。根据我们的经验，聚类任务通常在较大的数据集上完成，因此，对于此类任务考虑集群的设置是很重要的。我们在本章中讨论了这些细微差别。
- en: As a data scientist, there have been many situations where we analyze data with
    the sole purpose of extracting value from the data. You should consider clustering
    approaches in these cases as it will help you to understand the inherent structure
    in your data. Once you discover the patterns in your data, you can identify events
    and categories by which your data is arranged. Once you have established your
    clusters, you can also evaluate any new observation based on which cluster it
    may belong to and predict that the observation will exhibit similar behavior to
    other observations in the cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们遇到过很多只为了从数据中提取价值而分析数据的情况。在这些情况下，你应该考虑聚类方法，因为它将帮助你理解数据中的固有结构。一旦你发现了数据中的模式，你就可以通过数据排列的事件和类别来识别事件和类别。一旦你建立了你的聚类，你还可以根据它可能属于哪个聚类来评估任何新的观察，并预测该观察将表现出与聚类中其他观察相似的行为。
- en: 'In the next chapter, we will cover a very interesting problem: how to make
    recommendations through machine learning by finding products that similar users
    find relevant.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一个非常有趣的问题：如何通过找到相似用户认为相关的产品来通过机器学习进行推荐。
- en: Exercises
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the situations where you would apply the k-means algorithm compared
    to hierarchical clustering?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与层次聚类相比，你会在什么情况下应用k-means算法？
- en: What is the difference between a regular Spark estimator and an estimator that
    calls SageMaker?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正常的Spark估计器与调用SageMaker的估计器之间有什么区别？
- en: For a dataset that takes too long to train, why would it not be a good idea
    to launch such a job using a SageMaker estimator?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练时间太长的数据集，为什么使用SageMaker估计器启动此类作业不是一个好主意？
- en: Research and establish other alternative metrics for cluster evaluation.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究并建立其他用于聚类评估的替代指标。
- en: Why is string indexing not a good idea when encoding features for k-means?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在为k-means编码特征时，字符串索引不是一个好主意？
