- en: Customer Segmentation Using Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce the main clustering algorithms by exploring how
    to apply them to customer segmentation based on their behavioral patterns. In
    particular, we will demonstrate how Apache Spark and Amazon SageMaker can seamlessly
    interoperate to perform clustering. Throughout this chapter, we will be using
    the **Kaggle Dataset E-Commerce** data from **Fabien Daniel**,which can be downloaded
    from [https://www.kaggle.com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the topics we will be  covering:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how clustering algorithms work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering with **Apache Spark** on **Elastic MapReduce** (**EMR**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering using **SageMaker** through Spark integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding How Clustering Algorithms Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cluster analysis**, or clustering, is a process of grouping a set of observations
    based on their similarities. The idea is that the observations in a cluster are
    more similar to one another than the observations from other clusters. Hence,
    the outcome of this algorithm is a set of clusters that can identify the patterns
    in the dataset and arrange the data into different clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms are referred to as **unsupervised learning algorithms**.
    Unsupervised learning does not depend on predicting ground truth and is designed
    to discover the natural patterns in the data. Since there is no ground truth provided,
    it is difficult to compare different unsupervised learning models. Unsupervised
    learning is generally used for exploratory analysis and dimensionality reduction.
    Clustering is an example of exploratory analysis. In this task, you are looking
    for patterns and structure in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is different than the algorithms we have been studying so far in the book.
    **Naive Bayes**, **linear regression**, and **decision trees** algorithms are
    all examples of supervised learning. There is an assumption that each dataset
    has a set of observations and an event class associated with those observations.
    Hence, the data is already grouped based on the actual outcome event for each
    observation. However, not every dataset has labeled outcomes associated with each
    event. For example, consider a dataset where you have information regarding each
    transaction on an e-commerce website:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **SKU** | **Item name** | **Customer ID** | **Country** |'
  prefs: []
  type: TYPE_TB
- en: '| 12423 | iPhone | 10 | USA |'
  prefs: []
  type: TYPE_TB
- en: '| 12423 | iPhone | 11 | USA |'
  prefs: []
  type: TYPE_TB
- en: '| 12423 | iPhone | 12 | USA |'
  prefs: []
  type: TYPE_TB
- en: '| 11011 | Samsung S9 | 13 | UK |'
  prefs: []
  type: TYPE_TB
- en: '| 11011 | Samsung S9 | 10 | USA |'
  prefs: []
  type: TYPE_TB
- en: '| 11011 | Samsung S9 | 14 | UK |'
  prefs: []
  type: TYPE_TB
- en: 'This dataset is a list of transactions but does not have any class variable
    that informs us regarding what kind of users buy specific products. Hence, if
    the task is to identify patterns from this dataset, we cannot use any algorithms
    that can predict a specific event. That is where clustering algorithms come into
    the picture. We want to explore whether we can find trends in the transactions
    on the website based on the dataset. Let''s look at a simple example. Consider
    that the dataset only had one feature: **Item name**. We will discover that the
    data can be arranged in three clusters, namely, iPhone, Samsung S9, and Pixel
    2\. Similarly, if we consider that the only feature to cluster on is **Country**,
    the data can be clustered into two clusters: USA and UK. Once you generate the
    clusters, you can analyze the statistics in each cluster to understand the type
    of audience buying certain things.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in most of your experiences, you will have to cluster the data based
    on more than one feature. There are many clustering algorithms that you can deploy
    in clustering the data into clusters. The following diagram shows an example of
    how clusters would look for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cece5e6-2a84-46dc-ad32-95548553028c.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering helps us get an outcome where we can group the data into two clusters
    and understand the patterns in each cluster. We may be able to cluster the customers
    into users who buy a certain kind of phone. By analyzing the clusters, we can
    learn the patterns of which users buy an iPhone or Samsung S9 phone.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will study two common clustering approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **k-means clustering** algorithm aims to cluster a dataset into k clusters
    by selecting k centroids in the dataset. Each record is evaluated based on its
    distance to the centroid and assigned a cluster. Centroids are observations that
    are at the center of each cluster. To define k-means clustering formally, we are
    optimizing the **Within-Cluster Sum of the Square** *(***WCSS***)* distance of
    observations. Hence, the most optimal clustering would ensure that each cluster
    has all the observations close to its centroid, and as far away from the other
    centroids as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important parameters in k-means clustering. Firstly, we need
    to discover the centroids in our dataset. One of the popular methodologies for
    selecting centroids is called **Random partitioning**. This methodology uses a
    technique called **Expectation Maximization** (**EM**) to achieve high-quality
    clusters. In the first step, we randomly assign a cluster to each observation.
    Once each observation is assigned to a cluster, we calculate the quality of the
    cluster using the WCSS methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecf2864c-bc47-445c-9291-635fe6cba82a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*J* represents the WCSS score for the clusters that are generated where we
    have *M* observations and have generated *K* clusters. ![](img/08a4dad4-65e2-404e-b49d-ab177ab1d566.png) is
    1 if the observation *i* belongs to cluster *k*, and 0 if the observation *i*
    does not belong to cluster *k*. ![](img/97a87758-272e-4eda-802e-7872dbd08ff4.png) is
    the observation, while ![](img/9b7475ed-080c-42b9-b83d-4782273d2d79.png) is the
    centroid of cluster *k*. The difference between ![](img/6f0c9f95-2408-4a18-8f67-99485a2df395.png) and ![](img/a6cc06fe-37a0-4392-ad0a-dac23cacbee4.png) represents
    the distance between the observation and the centroid. Our aim is to minimize
    the value of *J*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we calculate new centroids again based on the current clusters
    in the first step. This is the maximization step in the EM algorithm, where we
    try to step toward more optimal cluster assignments for records. The new centroid
    values are calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f889985e-9683-4c62-aa3d-83ff060f2eec.png)'
  prefs: []
  type: TYPE_IMG
- en: This represents the fact that we recalculate the centroids based on the mean
    of the clusters created in the previous steps. Based on the new centroids, we
    assign each observation in the dataset to a centroid based on their distance to
    the centroids. Distance is the measure of similarity between two observations. We
    will discuss the concept of how to calculate distance later in this section. We
    recalculate the WCSS score for the new clusters and repeat the minimization step
    again. We repeat these steps until the assignments of the observations in the
    cluster do not change.
  prefs: []
  type: TYPE_NORMAL
- en: Although the random partition algorithm allows the k-means algorithm to discover
    centroids with a low WCSS score, they do not guarantee a global optimum solution.
    This is because the EM algorithm may greedily find a local optimum solution and
    stop exploring, for a more optimum solution. Also, selecting different random
    centroids in the first step may lead to different optimal solutions at the end
    of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, there are other algorithms such as the **Forgy algorithm**,
    where we choose random observations from the dataset as centroids in the first
    step. This leads to more spread out centroids in the first step, compared to the
    random partition algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed before, we have to calculate the distance between the observations
    and the centroid of the cluster. There are various methodologies to calculate
    this distance. The two popular methodologies are the **Euclidean distance** and
    the **Manhattan distance**.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Euclidean distance between two points is the length of the line connecting
    them. For the n-dimensional points *P* and *Q*, where both the vectors have *n*
    values, the Euclidean distance is calculated using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd0b051c-0341-4fc1-a596-833eb3bf5e88.png)'
  prefs: []
  type: TYPE_IMG
- en: If the values of the data points are categorical values, then ![](img/02482353-46c5-4319-a68f-32cd65412314.png) is
    1 if both the observations have the same values for a feature and 0 if the observations
    have different values. For continuous variables, we can calculate the normalized
    distance between the values of the attributions.
  prefs: []
  type: TYPE_NORMAL
- en: Manhattan distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Manhattan distance** is the sum of absolute differences between two data
    points. For the n-dimensional points *P* and *Q*, where both the vectors have
    *n* values, we calculate the Manhattan distance using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7af374d-c24a-4cc7-9975-521e9b808d0a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Manhattan distance reduces the effects of outliers in the data, and hence,
    should be used when we have noisy data with a lot of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** aims to build a hierarchical structure of clusters
    from the observations. There are two strategies for generating the hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative clustering**: In this approach, we use a bottom-up methodology,
    where each observation starts as its own cluster and clusters are merged at each
    stage of generating a hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive clustering**: In this approach, we use a top-down methodology, where
    we divide the observations into smaller clusters as we move down the stages of
    the hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agglomerative clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **agglomerative clustering**, we start with each observation as its own cluster
    and combine these clusters based on certain criteria so that we end up with one
    cluster that contains all the observations. Similar to k-means clustering, we
    use distance metrics such as the Manhattan distance and the Euclidean distance
    in order to calculate the distance between two observations. We also use **linkage
    criteria**, which can represent the distance between two clusters. In this section,
    we study three linkage criteria, namely, **complete-linkage clustering**, **single-linkage
    clustering**, and **average-linkage clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complete-linkage clustering calculates the distance between two clusters as
    the maximum distance between observations from two clusters and is represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6e4cb11-684a-4bb2-a529-8b0a410a4bf8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Single-linkage clustering calculates the distance between two clusters as the
    minimum distance between observations from two clusters and is represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/579ab169-d590-4bb8-8ca2-3b17d2c5911c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Average-linkage clustering calculates the distance between each observation
    from cluster A with cluster B and normalizes it based on the observations in cluster
    A and B. This is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3561735-c411-4586-93f0-5a95df3b2c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in the first step of agglomerative clustering, we use distance methodology
    to calculate the distance between each observation and merge observations with
    the smallest distance. For the second step, we calculate the distances between
    each cluster using linkage criteria based on the methodologies just presented.
    We run the necessary iterations until we only have one cluster left with all observations
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how agglomerative clustering would work for observations
    with only one continuous variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ea5a2bb-81df-45e0-a62b-76365fedbabe.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we have five observations with one continuous feature. In the
    first iteration, we look at the Euclidean distance between each observation and
    can deduce that records 1 and 2 are closest to each other. Hence, in the first
    iteration, we merge the observations 1 and 2\. In the second iteration, we discover
    that the observations 10 and 15 are the closest records and create a new cluster
    from it. In the third iteration, we observe that the distance between the (1,2) cluster
    and the (10,15) cluster is smaller than any of those clusters and observation
    90**.** Hence, we create a cluster of (1,2,10,15) in the third iteration. Finally,
    in the last iteration, we add element 90 to the cluster and terminate the process
    of agglomerative clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Divisive clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Divisive clustering** is a top-bottom approach where we first start with
    a large cluster with all observations and, at iteration, we split the clusters
    into smaller clusters. The process is similar to using distances and linkage criteria
    such as agglomerative clustering. The aim is to find an observation or cluster
    in the larger cluster that has the furthest distance from the rest of the cluster.
    In each iteration, we look at a cluster and recursively split the larger clusters by
    finding clusters that have the farthest distance from one another. Finally, the
    process is stopped when each observation is its own cluster. Divisive clustering
    uses an exhaustive search to find the perfect split in each cluster, which may
    be computationally very expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering approaches are computationally more expensive than a
    k-means approach. Hence, even on medium-sized datasets, hierarchical cluster approaches
    may struggle to generate results compared to a k-means approach. However, since
    we do not need to start with a random partition at the start of hierarchical clustering,
    they remove the risks in the k-means approach where a bad random partition may
    hurt the clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with Apache Spark on EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we step through the creation of a clustering model capable
    of grouping consumer patterns in three distinct clusters. The first step will
    be to launch an EMR notebook along with a small cluster (a single `m5.xlarge`
    node works fine as the dataset we selected is not very large). Simply follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the dataframe and inspect the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of our `df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ac39800-71d9-4400-a9e8-e0fe9d0518be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you see, the dataset involves transactions of products bought by different
    customers at different times and in different locations. We attempt to cluster
    these customer transactions using k-means by looking at three factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The product (represented by the `StockCode` column)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The country where the product was bought
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total amount spent by the customer across all products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this last factor is not directly available in the dataset, but it
    seems like an intuitively valuable feature (whether the client is a big spender
    or not). Oftentimes, during our feature preparation, we need to find aggregate
    values and plug them into our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'On this occasion, we first find the total amount spent by each customer by
    multiplying the `Quantity` and `UnitPrice` columns on a new column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of our modified `df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce250198-23ec-4819-b866-11e0ccfa48c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we proceed to aggregate the `TotalBought` column by a customer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of the `customer_df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29eae442-050e-48aa-9bb0-59c99d1f00e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then join back this new column back to our original dataset based on
    the customer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of the `joined_df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbc94201-5161-4703-8c1a-cc5aa0687792.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that two of the features that we are interested in using for clustering
    (**Country** and **StockCode**) are categorical. Hence, we need to find a way
    to encode those two numbers, similar to what we did in the previous chapter. String
    indexing these features would not be suitable in this case, as k-means works by
    computing distances between data points. Distances between artificial indices
    assigned to string values do not convey a lot of information. Instead, we apply
    one hot encoding to these features so that the vector distances represent something
    meaningful (note that two data points coinciding on most vector components have
    a cosine or Euclidean distance closer to 0).
  prefs: []
  type: TYPE_NORMAL
- en: Our pipeline will consist of two one hot encoding steps (for **Country** and
    **Product**), and a column that represents whether a customer is a big, normal,
    or small spender. To determine this, we discretize the `SumTotalBought` column
    into three values using a `QuantileDiscretizer`, which will result in three buckets
    depending on the quantile each customer falls into. We use the vector assembler
    to compile a vector of features. Given that the k-means algorithm works by computing
    distances, we normalize the feature vector so that the third feature (spender
    bucker) does not have a higher influence on the distance, as it has larger absolute
    values in the vector component. Finally, our pipeline will run the k-means estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we define the stages of our pipeline and fit a
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have a model, we apply that model to our dataset to obtain the clusters
    each transaction falls into:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first lines of the `df_with_clusters` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32b5eb56-16ff-4907-b74a-8ff01df740c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note the new **prediction** column, which holds the value of cluster each row
    belongs to. We evaluate how well the clusters were formed by using the silhouette
    metric, which measures how similar data points are within their cluster compared
    to other clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we got a value of 0.35, which is average as a clustering score
    (ideally it's near 1.0, but at least it's positive). One main reason for not having
    a larger value is because we did not reduce the dimensionality of our vectors.
    Typically, before clustering, we apply some transformation for reducing the cardinality
    of the feature vector, such as **principal component analysis** (**PCA**). We
    didn't include such a step in this example for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now examine each cluster to have a sense of how the data was clustered.
    The first thing to look at is the size of each cluster. As we can see in the following,
    the clusters vary in size, where one cluster captures more than half of the data
    points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the relative sizes of the different clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9599bbf9-7d2f-4c94-89c5-5c9ca363c0bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the countries contained on each cluster, we can see that two
    clusters just contain data points from the UK, and the third cluster only contains
    points from the rest of the countries. We first inspect the counts for cluster
    0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the count for cluster 1 is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the count for cluster 2 is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting observation is that the different clusters seem to have very
    different spending profiles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `plot()` command produces the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a7332a4-f25c-4dcf-889d-a2ee487523c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To have a sense of how each cluster classifies the products, we look at the
    product description field of the different clusters. A nice visual representation
    is to use a word cloud with the words that appear on the product descriptions
    of each cluster. Using the Python `wordcloud` library, we can create a function
    that strips out the words of the products and constructs a wordcloud:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We call this function on each cluster and obtain the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting word cloud for cluster 0 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7bf2242-eafa-4777-9c1d-76175feba753.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting word cloud for cluster 1 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80f586a8-d947-4a86-93aa-c6b130d6e63d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting word cloud for cluster 2 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80b490b1-b5bb-47f2-ad51-3207ef270632.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see in the word clouds that, despite some very common words, the relative
    importance of a few words such as C*hristmas* or *retrospot* comes out with a
    higher weight on one of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with Spark and SageMaker on EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show how **Spark** and **SageMaker** can work together
    seamlessly through code integration.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, regarding decision trees, we performed the data preparation
    in Apache Spark through **EMR** and uploaded the prepared data in S3 to then open
    a SageMaker notebook instance using the `SageMaker` Python library to perform
    the training. There is an alternative way of doing the same thing, which, on many
    occasions, is more convenient, using the `sagemaker_pyspark` library. This library
    allows us to perform the training stage through SageMaker services just by adding
    a special stage to our pipeline. To do this, we will define a pipeline identical
    to the one we wrote in the previous section, with the difference being the last
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of including Apache Spark''s implementation of `KMeans`, we will use  `KMeansSageMakerEstimator`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we will import all the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we start by defining the IAM role to use and the full pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`KMeansSageMakerEstimator` implements the estimator interface from Apache Spark,
    so it is included as any other estimator or transformer on our pipelines. Through
    the `KMeansSageMakerEstimator` constructor, we define the amount and type of machines
    to use as well as the IAM role. We explain the purpose of the role in the next
    subsection, *Understanding the purpose of the IAM Role*. Additionally, we set
    the number of clusters we want to create (value of *k*) as well as the length
    of the vectors we''ll be using for training (which we find by examining the output
    rows from the last section).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at what happens under the hood when we call `fit()` on the pipeline. The
    first part of the pipeline works exactly the same as before, whereby the different
    stages launch Spark jobs that run a series of transformations to the dataset by
    appending columns (for example, the encoded vectors or discretized features).
    The last stage, being a SageMaker estimator, works in a slightly different way.
    Instead of using the EMR cluster resources to compute and train the clustering
    model, it saves the data in S3 and makes an API call to the SageMaker KMeans service
    pointing to that S3 temporary location. The SageMaker service, in turn, spins
    up EC2 servers to perform the training and creates both a SageMaker model and
    endpoint. Once the training is complete, the `KMeansSageMakerEstimator` stores
    a reference to the newly created endpoint that is used each time the model's `transfom()`
    method is called.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the models and endopoints created by  `KMeansSageMakerEstimator` by
    inspecting the SageMaker AWS console at [https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine what happens when we call the `transform()` method of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first series of transformations (the data preparation stages) will run on
    the EMR cluster through Spark jobs. As the final transformation is a SageMaker
    model, it relies on the SageMaker endpoint to obtain the predictions (in our case,
    the cluster assignment).
  prefs: []
  type: TYPE_NORMAL
- en: You should remember to delete the endpoint (for example using the console) once
    it's no longer required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98afac8f-020c-4fe6-89d6-cd9834d34d98.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(*The image has been truncated to show just the last few columns.*)*'
  prefs: []
  type: TYPE_NORMAL
- en: Note how the two columns were added by the  `distance_to_cluster` and `closest_cluster`
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'By instructing the cluster evaluator to use this column, we can evaluate the
    clustering ability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The silhouette value we get is almost the same as the one using Spark's algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the purpose of the IAM role
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker is a managed service on AWS that manages the hardware responsible
    for training and inference. In order for SageMaker to perform such tasks on your
    behalf, you need to allow it through IAM configuration. For example, if you''re
    running on EMR, the EC2 instances (that is, the computers) in the cluster are
    running with a specific role. This role can be found by going to the cluster page
    on the EMR console: [https://console.aws.amazon.com/elasticmapreduce/.](https://console.aws.amazon.com/elasticmapreduce/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the cluster details, including the security
    and access information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/784faccc-3f9a-42a9-a96b-af2e924b2207.png)'
  prefs: []
  type: TYPE_IMG
- en: The role under EC2 instance profile in the previous screenshot is the one we
    are using, that is, `EMR_EC2_DefaultRole`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then go to the IAM console at [https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home)
    to edit the permissions of that role to grant access to SageMaker resources, as
    well as allow the role to be assumed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08002147-dc8f-4698-bb68-17ffa7ea963f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Trust relationships section, we click on the Edit trust relationship
    button to open the dialog that will allow us to add the settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54eee129-2ba9-4315-9eb4-e3067ebcbf48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can edit and allow the role to be assumed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fa43a28-a41b-4805-8db5-4e66534142aa.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous changes are required to allow our EMR cluster to talk to SageMaker
    and enable the kind of integration described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied the difference between supervised and unsupervised
    learning and looked at situations when unsupervised learning is applied. We studied
    the exploratory analysis application of unsupervised learning, where clustering
    approaches are used. We studied the k-means clustering and hierarchical clustering
    approaches in detail and looked at examples of how they are applied.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how clustering approaches can be implemented on Apache Spark
    on AWS clusters. In our experience, clustering tasks are generally done on larger
    datasets, and, hence, taking the setup of the cluster into account for such tasks
    is important. We discussed these nuances in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, there have been many situations where we analyze data with
    the sole purpose of extracting value from the data. You should consider clustering
    approaches in these cases as it will help you to understand the inherent structure
    in your data. Once you discover the patterns in your data, you can identify events
    and categories by which your data is arranged. Once you have established your
    clusters, you can also evaluate any new observation based on which cluster it
    may belong to and predict that the observation will exhibit similar behavior to
    other observations in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover a very interesting problem: how to make
    recommendations through machine learning by finding products that similar users
    find relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the situations where you would apply the k-means algorithm compared
    to hierarchical clustering?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a regular Spark estimator and an estimator that
    calls SageMaker?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a dataset that takes too long to train, why would it not be a good idea
    to launch such a job using a SageMaker estimator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Research and establish other alternative metrics for cluster evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is string indexing not a good idea when encoding features for k-means?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
