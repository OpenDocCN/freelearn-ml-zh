<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assisted Drawing with CNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>So far, we have seen how we can leverage Core ML and, in general, <strong>machine learning</strong> (<strong>ML</strong>) to better understand the physical world we live in (perceptual tasks). From the perspective of designing user interfaces, this allows us to reduce the friction between the user and the system. For example, if you are able to identify the user from a picture of their face, you can remove the steps required for authentication, as demonstrated with Apple's Face ID feature which is available on iPhone X. With Core ML, we have the potential to have devices better serve us rather than us serving them. This adheres to a rule stated by developer Eric Raymond that <em>a</em></span> <span><em>computer should never ask the user for any information that it can auto detect, copy, or deduce</em></span><span>.</span><br/></p>
<p>We can take this idea even further; given sufficient amounts of data, we can anticipate what the user is trying to do and assist them in achieving their tasks. This is the premise of this chapter. Largely inspired and influenced by Google's AutoDraw AI experiment, we will implement an application that will attempt to guess what the user is trying to draw and provide pre-drawn drawings that the user can subtitute with (image search).</p>
<p>In this<span> chapter, we'll explore this idea by looking at how we can try to predict what the user is trying to draw, and find suggestions for them to substitute it with. We will be exploring two techniques. The first is using a <strong>convolutional neural network</strong> (<strong>CNN</strong>), which we are becoming familiar with, to make the prediction, and then look at how we can apply a context-based similarity sorting strategy to better align the suggestions with what the user is trying to sketch. In the next chapter, we will continue our exploration by looking at how we can use a <strong>recurrent neural network</strong> (<strong>RNN</strong>) for the same task.</span></p>
<p>By the end of this chapter, you will have: </p>
<ul>
<li>Applied CNNs to the task of sketch recognition </li>
<li>Gained further experience preparing input for a model </li>
<li>Learned how feature maps can be extracted from <span>CNNs and used to measure how similar two images are</span></li>
</ul>
<p><span>There is a lot to cover, so let's get started by building a simple drawing application. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Towards intelligent interfaces </h1>
                </header>
            
            <article>
                
<p>Before jumping into how, let's quickly discuss the why in order to motivate us as well as encourage creative exploration of this concept. As alluded to in the introduction, the first motivator is to reduce friction. Consider the soft keyboard (keyboard with no physical buttons) on your phone; due to the constraints of the medium, such as lack of space and feedback, inputting text without predictive text would be cumbersome to the point of rendering it unusable. Similarly, despite the convenience of drawing with our fingers, our fingers are not that accurate, which makes things difficult to draw.</p>
<p>The other reason why this concept (augmentation) is advantageous is its ability to democratize the technical skill of drawing. It's common for people to not even attempt to draw because they have convinced themselves that it is beyond their abilities, or possibly that we can enhance one's ability to draw. This was the motivation behind the research project <em>ShadowDraw</em> presented at SIGGRAPH in 2011 by Yong Jae Lee, Larry Zitnick, and Michael Cohen. Their project had shown that guiding the user with a shadow image underlying the user's stroke significantly improved the quality of the output.</p>
<p>Finally, the last reason I want to highlight as to why this concept is interesting is providing a way for users to work at a higher level of abstraction. For example, imagine you were tasked with sketching out the storyboard for a new animation. As you sketch out your scene, the system would substitute your sketches with their associated characters and props as they were being worked on, allowing you to design at a higher level of fidelity without sacrificing speed.</p>
<p>Hopefully, by now, I have <span>convinced you of the potential opportunity of integrating artificial intelligence into the user interface. Let's shift our focus to the how, which we will begin to do in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing</h1>
                </header>
            
            <article>
                
<p>In this section, we will start off by inspecting an existing starter application and implement the drawing functionality. Then, in the next section, we will look at how we can augment the user by predicting what they are trying to draw and providing substitutes they can swap with.  </p>
<p>If you haven't done so, pull down the latest code from the accompanying repository at <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the <kbd>Chapter7/Start/QuickDraw/</kbd> <span>directory </span>and open the project <kbd>QuickDraw.xcodeproj</kbd>. Once loaded, you will see the starter project for this chapter, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ee8848b5-3cae-4787-ad88-f0bd5555414d.png"/></div>
<p>In the previous screenshot, you can see the application in its entirety; the interface consists of a single view, which contains a simple toolbar down on the left, allowing the user to toggle between sketch and move. There is a button for clearing everything. The area to the right of the toolbar is the canvas, which will be responsible for rendering what the user has drawn and any substituted images. Finally, the top area consists of a label and collection view. The collection view will make the suggested images that the user can substitute with available, while the label is simply something that is made visible to make the user aware of the purpose of the images presented to them via the collection view. </p>
<p>Our first task, as previously mentioned, will be to implement the functionality of drawing. Some of the plumbing has already been done but a majority is left out, giving us the opportunity to walk through the code to better understand the architecture of the application and how ML has been integrated. Before jumping into the code, let's briefly discuss the purpose of each relevant source file within the project. Like a contents page of a book, this will give you a better sense of how things are stitched together and help you become more familiar with the project:</p>
<ul>
<li><kbd>SketchView</kbd>: This is a custom <kbd>UIControl</kbd> that will be responsible for capturing the user's touches and converting them to drawings. It will also be responsible for rendering these drawings and substituted drawings, that is, sketches that have been replaced. As seen earlier, this control has already been added to the view. In this section, we will be implementing the functionality of touch events. </li>
<li><span><kbd>SketchViewController</kbd>: The controller behind the main view, and it is responsible for listening for when the user finishes editing (lifts their finder) and passing the current sketch to the <kbd>QueryFacade</kbd> for processing. This controller is also responsible for handling mode switches (sketching, moving, or clearing everything) and dragging the sketches around the screen when in the move mode. </span></li>
<li><kbd>BingService</kbd>: We will be using Microsoft's Bing Image Search API to find our suggested images. Bing provides a simple RESTful service to allow for image searches, along with relevant parameters for fine-tuning of your search. Note: we won't be editing this.</li>
<li><kbd>SketchPreviewCell</kbd>: A simple extension of the <kbd>UICollectionViewCell</kbd> class that makes the <kbd>UIImageView</kbd> nested within the cell <span>available</span>. <span>Note: we won't be editing this. </span></li>
<li><kbd>CIImage</kbd>: This should look familiar to you—something we implemented back in <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml" target="_blank">Chapter 3</a>, <em>Recognizing Objects in the World</em>. We'll use it extensively in this chapter for resizing and getting access to raw data of the images (including the sketch).</li>
<li><kbd>Sketch</kbd>: This is our model of a sketch; we will implement two versions of this. One is for rendering sketches from the user, created by strokes, and the other is for encapsulating a <kbd>UIImage</kbd>, which has substituted a sketch (of strokes). </li>
<li><kbd>Stroke</kbd>: A data object that describes part of a sketch, essentially encoding the path the user draws so that we can render it. </li>
<li><kbd>QueryFacade</kbd>: This is the class that will do all the heavy lifting. Once the user has finished editing, the view controller will export the sketch and pass it to <kbd>QueryFacade</kbd>, which will be responsible for three things: guessing what the user is trying to draw, fetching and downloading relevant suggestions, and sorting them before passing back to the view controller to present to the user via the collection view. An illustration of this process can be seen here: </li>
</ul>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9185a2cc-6f90-4294-8ae9-771fc9f21e0f.png"/></div>
<p>Hopefully, you now have a better sense of how everything fits together; let's bring it to life, starting at the bottom and working our way up. Click on the <kbd>Stroke.swift</kbd> file to focus the file in the main area; once open, you will be greeted by an unassuming amount of code, as shown in this snippet:</p>
<pre>import UIKit<br/>    class Stroke{    <br/>}</pre>
<p>Just to recap, the purpose of a <kbd>Stroke</kbd> is to encapsulate a single path the user has drawn such as to be able to recreate it when rendering it back onto the screen. A path is nothing more than a list of points that are captured as the user moves their finger along the screen. Along with the path, we will also store a color and width of the stroke; these determine the visual aesthetics of the stroke. Add the following properties to the <kbd>Stroke</kbd> class along with the class's constructor:</p>
<pre>var points : [CGPoint] = [CGPoint]()<br/>var color : UIColor!<br/>var width : CGFloat!<br/><br/>init(startingPoint:CGPoint,<br/>     color:UIColor=UIColor.black,<br/>     width:CGFloat=10.0) {<br/>    self.points.append(startingPoint)<br/>    self.color = color<br/>    self.width = width<br/>}</pre>
<p>Next, we will add some computed properties to our <kbd>Stroke</kbd> class that we will make use of when rendering and exporting the sketch. Starting with the property to assist with rendering, we'll be using the Core Graphics framework to render the path of each stroke associated with a sketch. Rendering is done using a Core Graphics context (<kbd>CGContext</kbd>), which conveniently exposes methods for rendering a path using the methods <kbd>addPath</kbd> and <kbd>drawPath</kbd>, as we'll see soon. The <kbd>addPath</kbd> method expects a type of <kbd>CGPath</kbd>, which is nothing more than a series of drawing instructions describing how to draw the path, something we can easily derive from our stroke's points. Let's do that now; add the <kbd>path</kbd> <span>property</span><span> </span><span>to the</span> <kbd>Stroke</kbd> <span>class:</span></p>
<pre>var path : CGPath{<br/>    get{<br/>        let path = CGMutablePath.init()<br/>        if points.count &gt; 0{<br/>            for (idx, point) in self.points.enumerated(){<br/>                if idx == 0{<br/>                    path.move(to: point)<br/>                } else{<br/>                    path.addLine(to: point)<br/>                }<br/>            }<br/>        }        <br/>        return path<br/>    }<br/>}</pre>
<p>As mentioned previously, a <kbd>CGPath</kbd> is made up of a series of drawing instructions. In the preceding snippet, we are creating a path using the points associated with <kbd>Stroke</kbd>. All except the first connects each point by a line while the first simply moves it into position. </p>
<div class="packt_infobox">The Core Graphics framework is a lightweight and low-level 2D drawing engine. It includes drawing functionality, such as path-based drawing, transformations, color management, off-screen rendering, patterns and shadings, image creation, and image masking.</div>
<p>Our next two properties are used to obtain the bounding box of the sketch, that is, the bounds that would encompass the minimum and maximum <kbd>x</kbd> and <kbd>y</kbd> positions of all strokes. Implementing these within the stroke itself will make our task easier later on. Add the properties <kbd>minPoint</kbd> and <kbd>maxPoint</kbd> to your <kbd>Stroke</kbd> class, as shown in the following code block:</p>
<pre>var minPoint : CGPoint{<br/>    get{<br/>        guard points.count &gt; 0 else{<br/>            return CGPoint(x: 0, y: 0)<br/>        }<br/>        <br/>        let minX : CGFloat = points.map { (cp) -&gt; CGFloat in<br/>            return cp.x<br/>            }.min() ?? 0<br/>        <br/>        let minY : CGFloat = points.map { (cp) -&gt; CGFloat in<br/>            return cp.y<br/>            }.min() ?? 0<br/>        <br/>        return CGPoint(x: minX, y: minY)<br/>    }<br/>}<br/><br/>var maxPoint : CGPoint{<br/>    get{<br/>        guard points.count &gt; 0 else{<br/>            return CGPoint(x: 0, y: 0)<br/>        }<br/>        <br/>        let maxX : CGFloat = points.map { (cp) -&gt; CGFloat in<br/>            return cp.x<br/>            }.max() ?? 0<br/>        <br/>        let maxY : CGFloat = points.map { (cp) -&gt; CGFloat in<br/>            return cp.y<br/>            }.max() ?? 0<br/>        <br/>        return CGPoint(x: maxX, y: maxY)<br/>    }<br/>}</pre>
<p>For each property, we simply map each axis (<em>x</em> and <em>y</em>) into its own array and then find either the minimum or maximum with respect to their method. This now completes our <kbd>Stroke</kbd> class. Let's move our way up the layers and implement the functionality of the <kbd>Sketch</kbd> class. Select <kbd>Sketch.swift</kbd> from the left-hand-side panel to open in the editing window. Before making amendments, let's inspect what is already there and what's left to do:</p>
<pre>protocol Sketch : class{<br/>    var boundingBox : CGRect{ get }<br/>    var center : CGPoint{ get set }<br/>    func draw(context:CGContext)<br/>    func exportSketch(size:CGSize?) -&gt; CIImage?<br/>}</pre>
<p>Currently, no concrete class exists and this will be our task for this part of this section. Before we start coding, let's review the responsibility of the <kbd>Sketch</kbd>. As implied earlier, our <kbd>Sketch</kbd> will be responsible for rendering either the collection of strokes associated with the user's drawing or an image that the user has selected to substitute their own drawing with. For this reason, we will be using two implementations of the <kbd>Stroke</kbd> class, one specifically for dealing with strokes and the other for images; we'll start with the one responsible for managing and rendering strokes.</p>
<p>Each implementation is expected to expose a <kbd>draw</kbd> and <kbd>exportSketch</kbd> method and the properties <kbd>boundingBox</kbd> and <kbd>center</kbd>. Let's now briefly describe each of these methods, starting with the most obvious: <kbd>draw</kbd>. We are expecting <kbd>Sketch</kbd> to be responsible for rendering itself, either drawing each stroke or rendering the assigned image depending on the type of sketch. The <kbd>exportSketch</kbd> method will be used to obtain a rasterized version of the sketch and is dependent on the <kbd>boundingBox</kbd> property, using it to determine what area of the canvas contains information (that is, drawings). Then, it proceeds to rasterize the sketch to a <kbd>CIImage</kbd>, which can <span>then </span>be used to feed the model. The last property, <kbd>center</kbd>, returns and sets the center and is used when the user drags it around the screen while in move mode, as described earlier.</p>
<p>Let's now proceed to implement a concrete version of a <kbd>Sketch</kbd> for dealing with strokes. Add the following code in the <kbd>Sketch</kbd> class, still within the <kbd>Sketch.swift</kbd> file:</p>
<pre>class StrokeSketch : Sketch{<br/>    var label : String?  <br/>    var strokes = [Stroke]()<br/>    var currentStroke : Stroke?{<br/>        get{<br/>            return strokes.count &gt; 0 ? strokes.last : nil <br/>        }<br/>    }   <br/>    func addStroke(stroke:Stroke){<br/>        self.strokes.append(stroke)<br/>    }<br/>}  </pre>
<p>Here, we have defined a new class, <kbd>StrokeSketch</kbd>, adhering to the <kbd>Sketch</kbd> protocol. We have defined two properties: a list for holding all the strokes and a string we can use to annotate the sketch. We have also exposed two helper methods. One is for returning the current stroke, which will be used while the user is drawing, and another is a convenient method for adding a new stroke.</p>
<p>Let's now implement the functionality that will be responsible for rendering the sketch; add the following code to the <kbd>StrokeSketch</kbd> class:</p>
<pre> func draw(context:CGContext){<br/>    self.drawStrokes(context:context)<br/>}<br/><br/>func drawStrokes(context:CGContext){<br/>    for stroke in self.strokes{<br/>        self.drawStroke(context: context, stroke: stroke)<br/>    }<br/>}<br/><br/>private func drawStroke(context:CGContext, stroke:Stroke){<br/>    context.setStrokeColor(stroke.color.cgColor)<br/>    context.setLineWidth(stroke.width)<br/>    context.addPath(stroke.path)<br/>    context.drawPath(using: .stroke)<br/>} </pre>
<p>We will implement the protocol's <kbd>draw</kbd> method but delegate the task of drawing to the methods <kbd>drawStrokes</kbd> and <kbd>drawStroke</kbd>. The <kbd>drawStrokes</kbd> <span>method </span>simply iterates over all strokes currently held by our sketch class and passes them to the <kbd>drawStoke</kbd> method, passing a reference of the Core Graphics context and current <kbd>Stroke</kbd>. Within the <kbd>drawStroke</kbd> method, we first update the context's stroke color and line width, and then we proceed to add and draw the associated path. With this now implemented, we have enough functionality for the user to draw. But for completeness, let's implement the functionality for obtaining the bounding box, obtaining and updating the sketches, center, and rasterizing the sketch to a <kbd>CIImage</kbd>. We start with the <kbd>boundingBox</kbd> property and associated methods. Add the following code to the <kbd>StrokeSketch</kbd> class:</p>
<pre>var minPoint : CGPoint{<br/>    get{<br/>        guard strokes.count &gt; 0 else{<br/>            return CGPoint(x: 0, y: 0)<br/>        }<br/>        <br/>        let minPoints = strokes.map { (stroke) -&gt; CGPoint in<br/>            return stroke.minPoint<br/>        }<br/>        <br/>        let minX : CGFloat = minPoints.map { (cp) -&gt; CGFloat in<br/>            return cp.x<br/>            }.min() ?? 0<br/>        <br/>        let minY : CGFloat = minPoints.map { (cp) -&gt; CGFloat in<br/>            return cp.y<br/>            }.min() ?? 0<br/>        <br/>        return CGPoint(x: minX, y: minY)<br/>    }<br/>}<br/><br/>var maxPoint : CGPoint{<br/>    get{<br/>        guard strokes.count &gt; 0 else{<br/>            return CGPoint(x: 0, y: 0)<br/>        }<br/>        <br/>        let maxPoints = strokes.map { (stroke) -&gt; CGPoint in<br/>            return stroke.maxPoint<br/>        }<br/>        <br/>        let maxX : CGFloat = maxPoints.map { (cp) -&gt; CGFloat in<br/>            return cp.x<br/>            }.max() ?? 0<br/>        <br/>        let maxY : CGFloat = maxPoints.map { (cp) -&gt; CGFloat in<br/>            return cp.y<br/>            }.max() ?? 0<br/>        <br/>        return CGPoint(x: maxX, y: maxY)<br/>    }<br/>}<br/><br/>var boundingBox : CGRect{<br/>    get{<br/>        let minPoint = self.minPoint<br/>        let maxPoint = self.maxPoint<br/>        <br/>        let size = CGSize(width: maxPoint.x - minPoint.x, height: maxPoint.y - minPoint.y)<br/>        <br/>        let paddingSize = CGSize(width: 5,<br/>                                 height: 5)<br/>        <br/>        return CGRect(x: minPoint.x - paddingSize.width,<br/>                      y: minPoint.y - paddingSize.height,<br/>                      width: size.width + (paddingSize.width * 2),<br/>                      height: size.height + (paddingSize.height * 2))<br/>    }<br/>}</pre>
<p>We first implement the properties <kbd>minPoint</kbd> and <kbd>maxPoint</kbd>; they <span>resemble</span><span> </span><span>our</span> <kbd>minPoint</kbd> <span>and</span> <kbd>maxPoint</kbd> <span>in our</span> <kbd>Stroke</kbd> <span>class. But instead of operating on a collection of points, they operate on a collection of strokes and utilize their counterparts (the</span> <kbd>minPoint</kbd> <span>and</span> <kbd>maxPoint</kbd> <span>properties of the</span> <kbd>Stroke</kbd> <span>class). Next, we implement the</span> <kbd>boundingBox</kbd> <span>property, which creates a</span> <kbd>CGRect</kbd><span> that encapsulates these minimum and maximum points with the addition of some padding to avoid cropping the stroke itself.</span></p>
<p>Now, we will implement the <kbd>center</kbd> property declared within the <kbd>Stroke</kbd> protocol. The protocol of this is expecting both <kbd>get</kbd> and <kbd>set</kbd> blocks to be implemented. The getter will simply return the center of the bounding box, which we have just implemented, while the setter will iterate over all strokes and translate each point using the difference of the previous center and new center value. Let's implement this now. Add the following code to your <kbd>StrokeSketch</kbd> class; here, the <kbd>boundingBox</kbd> property is a good place:</p>
<pre>var center : CGPoint{<br/>    get{<br/>        let bbox = self.boundingBox<br/>        return CGPoint(x:bbox.origin.x + bbox.size.width/2,<br/>                       y:bbox.origin.y + bbox.size.height/2)<br/>    }<br/>    set{<br/>        let previousCenter = self.center<br/>        let newCenter = newValue<br/>        let translation = CGPoint(x:newCenter.x - previousCenter.x,<br/>                                  y:newCenter.y - previousCenter.y)<br/>        for stroke in self.strokes{<br/>            for i in 0..&lt;stroke.points.count{<br/>                stroke.points[i] = CGPoint(<br/>                    x:stroke.points[i].x + translation.x,<br/>                    y:stroke.points[i].y + translation.y)<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Here, we first obtain the current center and then calculate the difference between this and the new center assigned to the property. After that, we iterate over all strokes and their corresponding points, adding this offset.</p>
<p>The final method we need to implement to adhere to the Sketch protocol is <kbd>exportSketch</kbd>. The purpose of this method is to rasterize the sketch into an image (<kbd>CIImage</kbd>) along with scaling it in accordance to the <kbd>size</kbd> argument, if available; otherwise it defaults to the actual size of the sketch itself. The method itself is fairly long but does nothing overly complicated. We have already implemented the functionality to render the sketch (via the <kbd>draw</kbd> method). But rather than rendering to a Core Graphic context that has been passed in by the view, we want to create a new context, adjust the scale with respect to the size argument and the actual sketch size, and finally create a <kbd>CIImage</kbd> instance from it.</p>
<p>To make it more readable, let's break the method down into these parts, starting with calculating the scale. Then we'll look at creating and rendering to a <kbd>context</kbd>, and finally wrap it in a <kbd>CIImage</kbd>; add the following code to your <kbd>StrokeSketch</kbd> class:</p>
<pre>func exportSketch(size:CGSize?=nil) -&gt; CIImage?{<br/>    let boundingBox = self.boundingBox<br/>    let targetSize = size ?? CGSize(<br/>        width: max(boundingBox.width, boundingBox.height),<br/>        height: max(boundingBox.width, boundingBox.height))<br/>    <br/>    var scale : CGFloat = 1.0<br/>    <br/>    if boundingBox.width &gt; boundingBox.height{<br/>        scale = targetSize.width / (boundingBox.width)<br/>    } else{<br/>        scale = targetSize.height / (boundingBox.height)<br/>    }<br/>    <br/>    guard boundingBox.width &gt; 0, boundingBox.height &gt; 0 else{<br/>        return nil<br/>    }     <br/>}</pre>
<p>In this code block, we declare our method and implement the functionality that determines the export size and scale. If no size is passed in, we simply fall back to the size of the sketch's bounding box property. Finally, we ensure that we have something to export.</p>
<p>Now our task is to create the <kbd>context</kbd> and render out our sketch with respect to the derived scale; append the following code to the <kbd>exportSketch</kbd> method: </p>
<pre>UIGraphicsBeginImageContextWithOptions(targetSize, true, 1.0)<br/><br/>guard let context = UIGraphicsGetCurrentContext() else{<br/>    return nil<br/>}<br/><br/>UIGraphicsPushContext(context)<br/><br/>UIColor.white.setFill()<br/>context.fill(CGRect(x: 0, y: 0,<br/>                    width: targetSize.width, height: targetSize.height))<br/><br/>context.scaleBy(x: scale, y: scale)<br/><br/>let scaledSize = CGSize(width: boundingBox.width * scale, height: boundingBox.height * scale)<br/><br/>context.translateBy(x: -boundingBox.origin.x + (targetSize.width - scaledSize.width)/2,<br/>                    y: -boundingBox.origin.y + (targetSize.height - scaledSize.height)/2)<br/><br/>self.drawStrokes(context: context)<br/><br/>UIGraphicsPopContext()</pre>
<p>We use <kbd>UIGraphicsBeginImageContextWithOptions</kbd> from Core Graphics to create a new <kbd>context</kbd> and obtain reference to this <kbd>context</kbd> using the <kbd>UIGraphicsGetCurrentContext</kbd> method. <kbd>UIGraphicsBeginImageContextWithOptions</kbd> creates a temporary rendering context, where the first argument is the target size for this context, the second determines whether we are using an opaque or transparent background, and the final argument determines the display scale factor. We then fill the <kbd>context</kbd> with white and update the context's <kbd>CGAffineTransform</kbd> property using the <kbd>scaleBy</kbd> method. Subsequent draw methods, such as moving and drawing, will be transformed by this, which nicely takes care of scaling for us. We then pass in this <kbd>context</kbd> to our sketch's <kbd>draw</kbd> method, which takes care of rendering the sketch to the context. Our final task is obtaining the image from the <kbd>context</kbd> and wrapping it in an instance of <kbd>CIImage</kbd>. Let's do that now; append the following code to your <kbd>exportSketch</kbd> method:</p>
<pre>guard let image = UIGraphicsGetImageFromCurrentImageContext() else{<br/>    UIGraphicsEndImageContext()<br/>    return nil<br/>}<br/>UIGraphicsEndImageContext()<br/><br/>return image.ciImage != nil ? image.ciImage : CIImage(cgImage: image.cgImage!)</pre>
<p>Thanks to the Core Graphics method <kbd>UIGraphicsGetImageFromCurrentImageContext</kbd>, the task is painless. <kbd>UIGraphicsGetImageFromCurrentImageContext</kbd> returns an instance of <kbd>CGImage</kbd> with a rasterized version of the context. To create an instance of <kbd>CIImage</kbd>, we simply pass in our image to the constructor and return it. </p>
<p>We have finished the <kbd>Sketch</kbd> class<span>—</span>for now—and slowly we're making our way up the layers. Next, we will flesh our the <kbd>SketchView</kbd> class, which will be responsible for facilitating the creation and drawing of the sketches. Select the <kbd>SketchView.swift</kbd> file from the left<span>-hand </span>panel to bring it up in the editing window, and let's quickly review the existing code. <kbd>SketchView</kbd> has been broken down into chunks using extensions; to make the code more legible, we will present each of the chunks along with its core functionality:</p>
<pre>class SketchView: UIControl {<br/>    var clearColor : UIColor = UIColor.white<br/>    var strokeColor : UIColor = UIColor.black<br/>    var strokeWidth : CGFloat = 1.0<br/>    var sketches = [Sketch]()<br/>    var currentSketch : Sketch?{<br/>        get{<br/>            return self.sketches.count &gt; 0 ? self.sketches.last : nil<br/>        }<br/>        set{<br/>            if let newValue = newValue{<br/>                if self.sketches.count &gt; 0{<br/>                    self.sketches[self.sketches.count-1] = newValue<br/>                } else{<br/>                    self.sketches.append(newValue)<br/>                }<br/>            } else if self.sketches.count &gt; 0{<br/>                self.sketches.removeLast()<br/>            }<br/>            self.setNeedsDisplay()<br/>        }<br/>    }<br/>    <br/>    override init(frame: CGRect) {<br/>        super.init(frame: frame)<br/>    }<br/>    <br/>    required init?(coder aDecoder: NSCoder) {<br/>        super.init(coder: aDecoder)<br/>    }<br/>    <br/>    func removeAllSketches(){<br/>        self.sketches.removeAll()<br/>        self.setNeedsDisplay()<br/>    }<br/>}  </pre>
<p>The majority of the previous code should be self-explanatory, but I do want to quickly draw your attention to the <kbd>currentSketch</kbd> property; we will use this getter to provide a convenient way for us to get access to the last sketch, which we will consider the currently active sketch. The setter is a little more ambiguous; it provides us with an easy way of replacing the currently active (last) sketch, which we will use when we come to handling the replacement of a user's sketch with an image suggested to them. The next chunk implements the drawing functionality, which should look familiar to you; here, we simply clear the <kbd>context</kbd> and iterate over all sketches, delegating the drawing to them:</p>
<pre>extension SketchView{ <br/>    override func draw(_ rect: CGRect) {<br/>        guard let context = UIGraphicsGetCurrentContext() else{ return }<br/>        self.clearColor.setFill()<br/>        UIRectFill(self.bounds)<br/>        // them draw themselves<br/>        for sketch in self.sketches{<br/>            sketch.draw(context: context)<br/>        }<br/>    }<br/>}  </pre>
<p>Our final chunk will be responsible for implementing the drawing functionality; currently, we have just stubbed out the methods to intercept the touch events. Fleshing these methods out will be our next task:</p>
<pre>extension SketchView{<br/>    override func beginTracking(_ touch: UITouch, <br/>                                with event: UIEvent?) -&gt; Bool{<br/>        return true<br/>    }<br/>    override func continueTracking(_ touch: UITouch?, <br/>                                   with event: UIEvent?) -&gt; Bool {<br/>        return true<br/>    }<br/>    override func endTracking(_ touch: UITouch?, <br/>                              with event: UIEvent?) {        <br/>    }<br/>    override func cancelTracking(with event: UIEvent?) { <br/>    }<br/>}</pre>
<p>Before we proceed with writing the code, let's briefly review what we are trying to achieve here. As mentioned previously, <kbd>SketchView</kbd> will be responsible for the functionality, allowing the user to sketch using their finger. We have spent the past few pages building the data objects (<kbd>Stroke</kbd> and <kbd>Sketch</kbd>) to support this functionality and it is here that we will make use of them.</p>
<p>A touch begins when the user first touches the view (<kbd>beginTracking</kbd>). When we detect this, we want to first check whether we have a currently active and appropriate sketch; if not, then we will create one and set it as the current sketch. Next, we will create a stroke that will be used to track the user's finger as they drag it around the screen. It is <span>considered </span>complete once the user has either lifted their finger or their finger is dragged outside the bounds of the view. We will <span>then </span>request the view to redraw itself and finally notify any listening parties by broadcasting the event <kbd>UIControlEvents.editingDidBegin</kbd> action. Let's put this into code; append the following code to <kbd>beginTracking</kbd> within the <kbd>SketchView</kbd> class:</p>
<pre>let point = touch.location(in: self)<br/>if sketches.count == 0 || !(sketches.last is StrokeSketch){<br/>    sketches.append(StrokeSketch())<br/>}<br/>guard let sketch = self.sketches.last as? StrokeSketch else {<br/>    return false<br/>}<br/>sketch.addStroke(stroke:Stroke(startingPoint: point,<br/>                               color:self.strokeColor,<br/>                               width:self.strokeWidth))<br/>self.setNeedsDisplay()<br/>self.sendActions(for: UIControlEvents.editingDidBegin)<br/>return true</pre>
<div class="packt_infobox">As described in the iOS documentation, here, we are adhering to the target-action mechanism, common in controls, by which we broadcast interesting events to simplify how other classes can integrate with this control.</div>
<p>Next, we will implement the body of the <kbd>continueTracking</kbd> method; here, we simply append a new point to the current sketches current stroke. As we did before, we request that the view to redraw itself and broadcast the <kbd>UIControlEvents.editingChanged</kbd> action. Append the following code to the body of the <kbd>continueTracking</kbd> method:</p>
<pre>guard let sketch = self.sketches.last as? StrokeSketch, let touch = touch else{<br/>    return false<br/>}<br/>let point = touch.location(in: self)<br/>sketch.currentStroke?.points.append(point)<br/>self.setNeedsDisplay()<br/>self.sendActions(for: UIControlEvents.editingChanged)<br/>return true</pre>
<p>The previous code resembles much of what we need when the user lifts their finger, with the exception of returning true (which tells the platform that this view wishes to continue consuming events) and replacing the <kbd>UIControlEvents.editingChanged</kbd> event with <kbd>UIControlEvents.editingDidEnd</kbd>. Add the following code to the body of your <kbd>endTracking</kbd> method:</p>
<pre>guard let sketch = self.sketches.last as? StrokeSketch, let touch = touch else{<br/>    return<br/>}<br/>let point = touch.location(in: self)<br/>sketch.currentStroke?.points.append(point)<br/>self.setNeedsDisplay()<br/>self.sendActions(for: UIControlEvents.editingDidEnd)</pre>
<p>The final piece of code we need to add to the <kbd>SketchView</kbd> class is for dealing with when the current finger tracking is canceled (triggered when the finger moves off the current view or out of the device's tracking range, that is, off the screen). Here, we are simply treating it as if the tracking has finished, with the exception of not adding the last point. Append the following code to the body of your <kbd>cancelTracking</kbd> method:</p>
<pre>guard let _ = self.sketches.last as? StrokeSketch else{<br/>    return<br/>}<br/>self.setNeedsDisplay()<br/>self.sendActions(for: UIControlEvents.editingDidEnd)</pre>
<p>With our <kbd>SketchView</kbd> finished, our application now supports the functionality of sketching. Now would be a good time to build and run the application on either the simulator or device and check that everything is working correctly. If it is, then you should be able to draw onto the screen, as shown in the following image:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/51a522b4-2f37-42e4-9721-a5c2528ab43c.png"/></div>
<p>The functionality of moving and clearing the canvas has already been implemented; tap on the Move button to drag your sketch around, and tap on the Trash button to clear the canvas. Our next task will be to import a trained Core ML model and implement the functionality of classifying and suggesting images to the user. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recognizing the user's sketch</h1>
                </header>
            
            <article>
                
<p>In this section, we will first review the dataset and model we will use to guess what the user is drawing. We will then proceed to integrate it into the workflow of the user who is sketching, and implement the functionality to support swapping out the user's sketch with a selected image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviewing the training data and model</h1>
                </header>
            
            <article>
                
<p>For this chapter, a CNN was trained on the dataset that was used and made available from the research paper <em>How Do Humans Sketch Objects?</em> by Mathias Eitz, James Hays, and Marc Alexa. The paper, presented at SIGGRAPH in 2012, compares the performance of humans classifying sketches to that of a machine. The dataset consists of 20,000 sketches evenly distributed across 250 object categories, ranging from airplanes to zebras; a few examples are shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/4070f2c9-8baa-4b34-8861-9dd706d5bdf7.png"/></div>
<p>From a perceptual study, they found that humans correctly identified the object category (such as snowman, grapes, and many more) of a sketch 73% of the time. The competitor, their ML model, got it right 56% of the time. Not bad! You can find out more about the research and download the accompanying dataset here at the official web page: <a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/">http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/</a>.</p>
<p>In this project, we will be using a slightly smaller set, with 205 out of the 250 categories; the exact categories can be found in the <span>CSV</span> file <kbd>/Chapter7/Training/sketch_classes.csv</kbd>, along with the Jupyter Notebooks used to prepare the data and train the model. The original sketches are available in <span>SVG and PNG </span>formats. Because we're using a <span>CNN,</span> rasterized images (PNG) were used but rescaled from 1111 x 1111 to 256 x 256; this is the expected input of our model. The data was then split into a training and a validation set, using 80% (64 samples from each category) for training and 20% <span>(17 samples from each category) </span>for validation.</p>
<p>The architecture of the network was not too dissimilar to what has been used in previous chapters, with the exception of a larger kernel window used in the first layer to extract the spare features of the sketch, as presented here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/51fd8cbe-382a-45a9-8556-fc90861c62c2.png"/></div>
<p>Recall that stacking convolution layers on top of each other allows the model to build up a shared set of high-level patterns that can then be used to perform classification, as opposed to using the raw pixels. The last <span>convolution layer</span> is flattened and then fed into a fully connected layer, where the prediction is finally made. You can think of these fully connected nodes as switches that turn on when certain (high-level) patterns are present in the input, as illustrated in the following diagram. We will return to this concept later on in this chapter when we implement sorting:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/0dfa0a19-ff7b-422b-b116-4a951b3fde8b.png" style="width:25.92em;height:20.50em;"/> </div>
<p>After 68 iterations (epochs), the model was able to achieve an accuracy of approximately 65% on the validation data. Not exceptional, but if we consider the top two or three predictions, then this accuracy increases to nearly 90%. The following diagram shows the plots comparing training and validation accuracy, and loss during training:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/79e0e42a-6233-4050-b0c7-48826ad7c30d.png"/></div>
<p>With our model trained, our next step is to export it using the Core ML Tools made available by Apple (as discussed in previous chapters) and imported into our project. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying sketches </h1>
                </header>
            
            <article>
                
<p>In this section, we will walk though importing the Core ML model into our project and hooking it up, including using the model to perform inference on the user's sketch and also searching and suggesting substitute images for the user to swap their sketch with. Let's get started with importing the Core ML model into our project. </p>
<p>Locate the model in the project repositories folder <kbd>/CoreMLModels/Chapter7/cnnsketchclassifier.mlmodel</kbd>; with the model selected, drag it into your Xcode project, leaving the defaults for the <span class="packt_screen">Import</span> options. Once imported, select the model to inspect the details, which should look similar to the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9be8dc44-63da-4ee5-9107-3beb970920b6.png"/></div>
<p>As with all our models, we verify that the model is included in the target by verifying that the appropriate <span class="packt_screen">Target Membership</span> is checked, and then we turn our attention to the inputs and outputs, which should be familiar by now. We can see that our model is expecting a single-channel (grayscale) 256 x 256 image and it returns the dominate class via the <span class="packt_screen">classLabel</span> property of the output, along with a dictionary of probabilities of all classes via the <span class="packt_screen">classLabelProbs</span> property.</p>
<p>With our model now imported, let's discuss the details of how we will be integrating it into our project. Recall that our <kbd>SketchView</kbd> emits the events <kbd>UIControlEvents.editingDidStart</kbd>, <kbd>UIControlEvents.editingChanged</kbd>, and <kbd>UIControlEvents.editingDidEnd</kbd> as the user draws. If you inspect the <kbd>SketchViewController</kbd>, you will see that we have already registered to listen for the <span><kbd>UIControlEvents.editingDidEnd</kbd> event, as shown in the following code snippet:<br/></span></p>
<pre>override func viewDidLoad() {<br/>        super.viewDidLoad()<br/>        <strong>...</strong><br/>        <strong>...</strong><br/>        <strong>self.sketchView.addTarget(self, action:</strong><br/><strong>            #selector(SketchViewController.onSketchViewEditingDidEnd),</strong><br/><strong>                                  for: .editingDidEnd)</strong><br/>        queryFacade.delegate = self <br/>}</pre>
<p>Each time the user ends a stroke, we will start the process of trying to guess what the user is sketching and search for suitable substitutes. This functionality is triggered via the <kbd>.editingDidEnd</kbd> action method <kbd>onSketchViewEditingDidEnd</kbd>, but will be delegated to the class <kbd>QueryFacade</kbd>, which will be responsible for implementing this functionality. This is where we will spend the majority of our time in this <span>section </span>and the next section. It's also probably worth highlighting the statement <kbd>queryFacade.delegate = self</kbd> in the previous code snippet. <kbd>QueryFacade</kbd> will be performing most of its work off the main thread and will notify this delegate of the status and results once finished, which we will get to in a short while.</p>
<p>Let's start by implementing the functionality of the <kbd>onSketchViewEditingDidEnd</kbd> method, before turning our attention to the <kbd>QueryFacade</kbd> class. Within the <kbd>SketchViewController</kbd> class, navigate to the <kbd>onSketchViewEditingDidEnd</kbd> <span>method </span>and append the following code:</p>
<pre>guard self.sketchView.currentSketch != nil,<br/>    let sketch = self.sketchView.currentSketch as? StrokeSketch else{<br/>    return<br/>} <br/><br/>queryFacade.asyncQuery(sketch: sketch)</pre>
<p>Here, we are getting the current sketch, and returning it if no sketch is available or if it's not a <kbd>StrokeSketch</kbd>; we hand it over to our <kbd>queryFacade</kbd> (an instance of the <kbd>QueryFacade</kbd> class). Let's now turn our attention to the <kbd>QueryFacade</kbd> class; select the <kbd>QueryFacade.swift</kbd> file from the left<span>-hand </span>panel within Xcode to bring it up in the editor area. A lot of plumbing has already been implemented to allow us to focus our attention on the core functionality of predicting, searching, and sorting. Let's quickly discuss some of the details, starting with the properties:</p>
<pre>let context = CIContext()<br/>let queryQueue = DispatchQueue(label: "query_queue")<br/>var targetSize = CGSize(width: 256, height: 256)<br/>weak var delegate : QueryDelegate?<br/>var currentSketch : Sketch?{<br/>    didSet{<br/>        self.newQueryWaiting = true<br/>        self.queryCanceled = false<br/>    }<br/>}<br/><br/>fileprivate var queryCanceled : Bool = false<br/>fileprivate var newQueryWaiting : Bool = false<br/>fileprivate var processingQuery : Bool = false<br/>var isProcessingQuery : Bool{<br/>    get{<br/>        return self.processingQuery<br/>    }<br/>}<br/><br/>var isInterrupted : Bool{<br/>    get{<br/>        return self.queryCanceled || self.newQueryWaiting<br/>    }<br/>} </pre>
<p><kbd>QueryFacade</kbd> is only concerned with the most current sketch. Therefore, each time a new sketch is assigned<span> using the </span><span><kbd>currentSketch</kbd></span> property, <kbd>queryCanceled</kbd> is set to <kbd>true</kbd>. During each task (such as performing prediction, search, and downloading), we check the <kbd>isInterrupted</kbd> property, and if <kbd>true</kbd>, we will exit early and proceed to process the latest sketch.</p>
<p>When you pass the sketch to the <kbd>asyncQuery</kbd> method, the sketch is assigned to the <kbd>currentSketch</kbd> property and then proceeds to call <kbd>queryCurrentSketch</kbd> to do the bulk of the work, unless there is one currently being processed:</p>
<pre>func asyncQuery(sketch:Sketch){<br/>    self.currentSketch = sketch<br/>    <br/>    if !self.processingQuery{<br/>        self.queryCurrentSketch()<br/>    }<br/>}<br/><br/>fileprivate func processNextQuery(){<br/>    self.queryCanceled = false<br/>    <br/>    if self.newQueryWaiting &amp;&amp; !self.processingQuery{<br/>        self.queryCurrentSketch()<br/>    }<br/>}<br/><br/>fileprivate func queryCurrentSketch(){<br/>    guard let sketch = self.currentSketch else{<br/>        self.processingQuery = false<br/>        self.newQueryWaiting = false<br/>        <br/>        return<br/>    }<br/>    <br/>    self.processingQuery = true<br/>    self.newQueryWaiting = false<br/>    <br/>    queryQueue.async {<br/>        <br/>        DispatchQueue.main.async{<br/>            self.processingQuery = false<br/>            self.delegate?.onQueryCompleted(<br/>                status:self.isInterrupted ? -1 : -1,<br/>                result:nil)<br/>            self.processNextQuery()<br/>        }<br/>    }<br/>}</pre>
<p>Eventually, we end up in the <kbd>queryCurrentSketch</kbd> method, where we will now turn our attention and implement the required functionality. But before doing so, let's quickly discuss what we'll be doing. </p>
<p>Recall that our goal is to assist the user in quickly sketching out a scene; we plan on achieving this by anticipating what the user is trying to draw and suggesting images, which the user can swap with their sketch. Prediction is a major component of this system and is made using the trained model we have just imported, but recall that we achieved approximately 65% accuracy on the validation dataset. This leaves a lot of room for errors, potentially inhibiting the user rather than augmenting them. To mitigate this and provide more utility, we will take the top 3-4 predictions and pull down the relevant images rather than relying on a single classification.</p>
<p>We pass these predicted classes to Microsoft's Bing Image Search API to find relevant images and then proceed to download each of them (admittedly not the most optimized approach, but sufficient for realizing this prototype). Once we have downloaded the images, we will perform some further processing by sorting the images based on how similar each image is to what the user has sketched; we will return to this in the next section, but for now we will concentrate on the steps preceding this. Let's move on to guessing what the user is trying to do. </p>
<p>As we have done previously, let's work bottom-up by implementing all the supporting methods before we tie everything together within the <kbd>queryCurrentSketch</kbd> method. Let's start by declaring an instance of our model; add the following variable within the <kbd>QueryFacade</kbd> class near the top:</p>
<pre>let sketchClassifier = cnnsketchclassifier()</pre>
<p>Now, with our model instantiated and ready, we will navigate to the <kbd>classifySketch</kbd> method of the <kbd>QueryFacade</kbd> class; it is here that we will make use of our imported model to perform inference, but let's first review what already exists:</p>
<pre>func classifySketch(sketch:Sketch) -&gt; [(key:String,value:Double)]?{<br/>    if let img = sketch.exportSketch(size: nil)?<br/>        .resize(size: self.targetSize).rescalePixels(){<br/>        return self.classifySketch(image: img)<br/>    }    <br/>    return nil<br/>}<br/>func classifySketch(image:CIImage) -&gt; [(key:String,value:Double)]?{    <br/>    return nil<br/>}</pre>
<p>Here, we see that the <kbd>classifySketch</kbd> is overloaded, with one method accepting a <kbd>Sketch</kbd> and the other a <kbd>CIImage</kbd>. The former, when called, will obtain the rasterize version of the sketch using the <kbd>exportSketch</kbd> method. If successful, it will resize the rasterized image using the <kbd>targetSize</kbd> property. Then, it will rescale the pixels before passing the prepared <kbd><span>CIImage</span></kbd> along to the alternative <kbd>classifySketch</kbd> method.</p>
<div class="packt_infobox">Pixel values are in the range of 0-255 (per channel; in this case, it's just a single channel). Typically, you try to avoid having large numbers in your network. The reason is that they make it more difficult for your model to learn (converge)—somewhat analogous to trying to drive a car whose steering wheel can only be turned hard left or hard right. These extremes would cause a lot of over-steering and make navigating anywhere extremely difficult.</div>
<p class="mce-root">The second <kbd>classifySketch</kbd> method will be responsible for performing the actual inference; we have already seen how we can do this in <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml" target="_blank">Chapter 3</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Recognizing Objects in the world</span></span></em>. Add the following code within the <kbd>classifySketch(image:CIImage)</kbd> method:</p>
<pre>if let pixelBuffer = image.toPixelBuffer(context: self.context, gray: true){<br/>    let prediction = try? self.sketchClassifier.prediction(image: pixelBuffer)<br/>    <br/>    if let classPredictions = prediction?.classLabelProbs{<br/>        let sortedClassPredictions = classPredictions.sorted(by: { (kvp1, kvp2) -&gt; Bool in<br/>            kvp1.value &gt; kvp2.value<br/>        })<br/>        <br/>        return sortedClassPredictions<br/>    }<br/>}<br/><br/>return nil</pre>
<p class="mce-root">Here, we use the images, <span><kbd>toPixelBuffer</kbd> method,</span> an extension we added to the <kbd>CIImage</kbd> class back in <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml">Chapter 3</a>, <span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Recognizing Objects in the World</em></span></span>, to obtain a grayscale <kbd><span>CVPixelBuffer</span></kbd> representation of itself. Now, with reference to its buffer, we pass it onto the <kbd>prediction</kbd> method of our model instance, <kbd>sketchClassifier</kbd>, to obtain the probabilities for each label. We finally sort these probabilities from the most likely to the least likely before returning the sorted results to the caller. </p>
<p class="mce-root">Now, with some inkling as to what the user is trying to sketch, we will proceed to search and download the ones we are most confident about. The task of searching and downloading will be the responsibility of the <kbd>downloadImages</kbd> method within the <kbd>QueryFacade</kbd> class. This method will make use of an existing <kbd>BingService</kbd> that exposes methods for searching and downloading images. Let's hook this up now; jump into the <span><kbd>downloadImages</kbd> method and append the following highlighted code to its body:</span></p>
<pre>func downloadImages(searchTerms:[String],<br/>                    searchTermsCount:Int=4,<br/>                    searchResultsCount:Int=2) -&gt; [CIImage]?{<br/><strong>    var bingResults = [BingServiceResult]()</strong><br/><br/><strong>    for i in 0..&lt;min(searchTermsCount, searchTerms.count){</strong><br/><strong>        let results = BingService.sharedInstance.syncSearch(</strong><br/><strong>            searchTerm: searchTerms[i], count:searchResultsCount)</strong><br/><strong>        </strong><br/><strong>        for bingResult in results{</strong><br/><strong>            bingResults.append(bingResult)</strong><br/><strong>        }</strong><br/><strong>        </strong><br/><strong>        if self.isInterrupted{</strong><br/><strong>            return nil</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/>}</pre>
<p>The <kbd>downloadImages</kbd> method takes the arguments <kbd>searchTerms</kbd>, <kbd>searchTermsCount</kbd>, and <kbd>searchResultsCount</kbd>. The <kbd>searchTerms</kbd> is a sorted list of labels returned by our <kbd>classifySketch</kbd> method, from which the <kbd>searchTermsCount</kbd> determines how many of these search terms we use (defaulting to 4). Finally, <kbd>searchResultsCount</kbd> limits the results returned for each search term.</p>
<p>The preceding code performs a sequential<span> </span>search using the search terms passed into the method. And as mentioned previously, here we are using Microsoft's Bing Image Search API, which requires registration, something we will return to shortly. After each search, we check the property <kbd>isInterrupted</kbd> to see whether we need to exit early; otherwise, we continue on to the next search.</p>
<p>The result returned by the search includes a URL referencing an image; we will use this next to download the image with each of the results, before returning an array of <kbd>CIImage</kbd> to the caller. Let's add this now. Append the following code to the <kbd>downloadImages</kbd> method:</p>
<pre>var images = [CIImage]()<br/><br/>for bingResult in bingResults{<br/>    if let image = BingService.sharedInstance.syncDownloadImage(<br/>        bingResult: bingResult){<br/>        images.append(image)<br/>    }<br/>    <br/>    if self.isInterrupted{<br/>        return nil<br/>    }<br/>}<br/><br/>return images</pre>
<p>As before, the process is synchronous and after each download, we check the <kbd>isInterrupted</kbd> property to see if we need to exit early, otherwise returning the list of downloaded images to the caller.</p>
<p>So far, we have implemented the functionality to support prediction, searching, and downloading; our next task is to hook all of this up. Head back to the <kbd>queryCurrentSketch</kbd> method and add the following code within the <kbd>queryQueue.async</kbd> block. Ensure that you replace the <kbd>DispatchQueue.main.async</kbd> block:</p>
<pre>queryQueue.async {<br/>    <br/><strong>    guard let predictions = self.classifySketch(</strong><br/><strong>        sketch: sketch) else{</strong><br/><strong>            DispatchQueue.main.async{</strong><br/><strong>                self.processingQuery = false</strong><br/><strong>                self.delegate?.onQueryCompleted(</strong><br/><strong>                    status:-1, result:nil)</strong><br/><strong>                self.processNextQuery()</strong><br/><strong>            }</strong><br/><strong>            return</strong><br/><strong>    }</strong><br/>    <br/><strong>    let searchTerms = predictions.map({ (key, value) -&gt; String in</strong><br/><strong>        return key</strong><br/><strong>    })</strong><br/>    <br/><strong>    guard let images = self.downloadImages(</strong><br/><strong>        searchTerms: searchTerms,</strong><br/><strong>        searchTermsCount: 4) else{</strong><br/><strong>            DispatchQueue.main.async{</strong><br/><strong>                self.processingQuery = false</strong><br/><strong>                self.delegate?.onQueryCompleted(</strong><br/><strong>                    status:-1, result:nil)</strong><br/><strong>                self.processNextQuery()</strong><br/><strong>            }</strong><br/><strong>            return</strong><br/><strong>    }</strong><br/>    <br/><strong>    guard let sortedImage = self.sortByVisualSimilarity(</strong><br/><strong>        images: images,</strong><br/><strong>        sketch: sketch) else{</strong><br/><strong>            DispatchQueue.main.async{</strong><br/><strong>                self.processingQuery = false</strong><br/><strong>                self.delegate?.onQueryCompleted(</strong><br/><strong>                    status:-1, result:nil)</strong><br/><strong>                self.processNextQuery()</strong><br/><strong>            }</strong><br/><strong>            return</strong><br/><strong>    }</strong><br/>    <br/>    <strong>DispatchQueue.main.async{</strong><br/><strong>        self.processingQuery = false</strong><br/><strong>        self.delegate?.onQueryCompleted(</strong><br/><strong>            status:self.isInterrupted ? -1 : 1,</strong><br/><strong>            result:QueryResult(</strong><br/><strong>                predictions: predictions,</strong><br/><strong>                images: sortedImage))</strong><br/><strong>        self.processNextQuery()</strong><br/><strong>    }</strong><br/>}</pre>
<p>It's a large block of code but nothing complicated; let's quickly walk our way through it. We start by calling the <kbd>classifySketch</kbd> method we just implemented. As you may recall, this method returns a sorted list of label and probability peers unless interrupted, in which case <kbd>nil</kbd> will be returned. We should handle this by notifying the delegate before exiting the method early (a check we apply to all of our tasks). </p>
<p>Once we've obtained the list of sorted labels, we pass them to the <kbd>downloadImages</kbd> method to receive the associated images, which we then pass to the <kbd>sortByVisualSimilarity</kbd> method. This method currently returns <span>just </span>the list of images, but it's something we will get back to in the next section. Finally, the method passes the status and sorted images wrapped in a <kbd>QueryResult</kbd> instance to the delegate via the main thread, before checking whether it needs to process a new sketch (by calling the <kbd>processNextQuery</kbd> method).</p>
<p>At this stage, we have implemented all the functionality required to download our substitute images based on our guess as to what the user is currently sketching. N<span>ow, </span>we just need to jump into the <kbd>SketchViewController</kbd> class to hook this up, but before doing so, we need to obtain a subscription key to use Bing's Image Search.</p>
<p>Within your browser, head to <a href="https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/">https://azure.microsoft.com/en-gb/services/cognitive-services/bing-image-search-api/</a> and click on the <span class="packt_screen">Try Bing Image Search API</span>, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a7e42186-fa61-40fd-a2c0-8054baed9ee7.png" style="width:41.75em;height:25.83em;"/></div>
<p>After clicking on <span class="packt_screen">Try Bing Image Search API</span>, you will be presented with a series of dialogs; read, and once (if) agreed, sign in or register. Continue following the screens until you reach a page informing you that the Bing Search API has been successfully added to your subscription, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ef546e09-8bd5-4d38-b920-8c5387185db8.png" style="width:30.92em;height:3.92em;"/></div>
<p>On this page, scroll down until you come across the entry <span class="packt_screen">Bing Search APIs v7</span>. If you inspect this block, you should see a list of <span class="packt_screen">Endpoints</span> and <span class="packt_screen">Keys</span>. Copy and paste one of these keys within the <kbd>BingService.swift</kbd> file, replacing the value of the constant <kbd>subscriptionKey</kbd>; the following screenshot shows the web page containing the service key:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/13940879-843b-4e07-b6fa-e1c5a76483be.png"/></div>
<p>Return to the <kbd>SketchViewController</kbd> by selecting the <kbd>SketchViewController.swift</kbd> file from the left<span>-hand </span>panel, and locate the method <kbd>onQueryCompleted</kbd>:</p>
<pre>func onQueryCompleted(status: Int, result:QueryResult?){<br/>}  </pre>
<p>Recall that this is a method signature defined in the <kbd>QueryDelegate</kbd> protocol, which the <kbd>QueryFacade</kbd> uses to notify the delegate if the query fails or completes. It is here that we will present the matching images we have found through the process we just implemented. We do this by first checking the status. If deemed successful (greater than zero), we remove every item that is referenced in the <kbd>queryImages</kbd> array, which is the data source for our <kbd>UICollectionView</kbd> used to present the suggested images to the user. Once emptied, we iterate through all the images referenced within the <kbd>QueryResult</kbd> instance, adding them to the <kbd>queryImages</kbd> array before requesting the <kbd>UICollectionView</kbd> to reload the data. Add the following code to the body of the <kbd>onQueryCompleted</kbd> method:</p>
<pre>guard status &gt; 0 else{<br/>    return<br/>}<br/><br/>queryImages.removeAll()<br/><br/>if let result = result{<br/>    for cimage in result.images{<br/>        if let cgImage = self.ciContext.createCGImage(cimage, from:cimage.extent){<br/>            queryImages.append(UIImage(cgImage:cgImage))<br/>        }<br/>    }<br/>}<br/><br/>toolBarLabel.isHidden = queryImages.count == 0<br/>collectionView.reloadData() </pre>
<p>There we have it; everything is in place to handle guessing of what the user draws and present possible suggestions. Now is a good time to build and run the application on either the simulator or the device to check whether everything is working correctly. If so, then you should see something similar to the following:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/dc92b5f4-4c43-459a-9714-ad6669acfa16.png"/></div>
<p>There is one more thing left to do before finishing off this section. Remembering that our goal is to assist the user to quickly sketch out a scene or something similar, our hypothesis is that guessing what the user is drawing and suggesting ready-drawn images will help them achieve their task. So far, we have performed prediction and provided suggestions to the user, but currently the user is unable to replace their sketch with any of the presented suggestions. Let's address this now. </p>
<p>Our <kbd>SketchView</kbd> currently only renders <kbd>StrokeSketch</kbd> (which encapsulates the metadata of the user's drawing). Because our suggestions are rasterized images, our choice is to either extend this class (to render strokes and rasterized images) or create a new concrete implementation of the <kbd>Sketch</kbd> protocol. In this example, we will opt for the latter and implement a new type of <kbd>Sketch</kbd> capable of rendering a rasterized image. Select the <kbd>Sketch.swift</kbd> file to bring it to focus in the editor area of Xcode, scroll to the bottom, and add the following code:</p>
<pre>class ImageSketch : Sketch{<br/>   var image : UIImage!<br/>   var size : CGSize!<br/>   var origin : CGPoint!<br/>   var label : String!<br/>   <br/>    init(image:UIImage, origin:CGPoint, size:CGSize, label: String) {<br/>        self.image = image<br/>        self.size = size<br/>        self.label = label<br/>        self.origin = origin<br/>    }    <br/>}</pre>
<p>We have defined a simple class that is referencing an image, origin, size, and label. The origin determines the top-left position where the image should be rendered, while the size determines its, well, size! To satisfy the <kbd>Sketch</kbd> protocol, we must implement the properties <kbd>center</kbd> and <kbd>boundingBox</kbd> along with the methods <kbd>draw</kbd> and <kbd>exportSketch</kbd>. Let's implement each of these in turn, starting with <kbd>boundingBox</kbd>. </p>
<p>The <kbd>boundingBox</kbd> <span>property </span>is a computed property derived from the properties <kbd>origin</kbd> and <kbd>size</kbd>. Add the following code to your <kbd>ImageSketch</kbd> class:</p>
<pre>var boundingBox : CGRect{<br/>    get{<br/>        return CGRect(origin: self.origin, size: self.size)<br/>    }<br/>} </pre>
<p>Similarly, <kbd>center</kbd> will be another computed property derived from the origin and size properties, simply translating the <kbd>origin</kbd> with respect to the <kbd>size</kbd>. Add the following code to your <kbd>ImageSketch</kbd> class:</p>
<pre>var center : CGPoint{<br/>    get{<br/>        let bbox = self.boundingBox<br/>        return CGPoint(x:bbox.origin.x + bbox.size.width/2,<br/>                       y:bbox.origin.y + bbox.size.height/2)<br/>    } set{<br/>        self.origin = CGPoint(x:newValue.x - self.size.width/2,<br/>                              y:newValue.y - self.size.height/2)<br/>    }<br/>}</pre>
<p>The <kbd>draw</kbd> method will simply use the passed-in <kbd>context</kbd> to render the assigned <kbd>image</kbd> within the <kbd>boundingBox</kbd>; append the following code to your <kbd>ImageSketch</kbd> class:</p>
<pre>func draw(context:CGContext){<br/>    self.image.draw(in: self.boundingBox)<br/>}  </pre>
<p>Our last method, <kbd>exportSketch</kbd>, is also fairly straightforward. Here, we create an instance of <kbd>CIImage</kbd>, passing in the <kbd>image</kbd> (of type <kbd>UIImage</kbd>). Then, we resize it using the extension method we implemented back in <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml" target="_blank">Chapter 3</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Recognizing Objects in the World</span></span></em>. Add the following code to finish off the <kbd>ImageSketch</kbd> class:</p>
<pre>func exportSketch(size:CGSize?) -&gt; CIImage?{<br/>    guard let ciImage = CIImage(image: self.image) else{<br/>        return nil<br/>    }<br/>    <br/>    if self.image.size.width == self.size.width &amp;&amp; self.image.size.height == self.size.height{<br/>        return ciImage<br/>    } else{<br/>        return ciImage.resize(size: self.size)<br/>    }<br/>} </pre>
<p>We now have an implementation of <kbd>Sketch</kbd> that can handle rendering of rasterized images (like those returned from our search). Our final task is to swap the user's sketch with an item the user selects from the <kbd>UICollectionView</kbd>. Return to <kbd>SketchViewController</kbd> class by selecting the <kbd>SketchViewController.swift</kbd> from the left<span>-hand-side</span> panel in Xcode to bring it up in the editor area. Once loaded, navigate to the method <kbd>collectionView(_ collectionView:, didSelectItemAt:)</kbd>; this should look familiar to most of you. It is the delegate method for handling cells selected from a <kbd>UICollectionView</kbd> and it's where we will handle swapping of the user's current sketch with the selected item.</p>
<p>Let's start by obtaining the current sketch and associated image that was selected. Add the following code to the body of the <kbd>collectionView(_collectionView:,didSelectItemAt:)</kbd> method:</p>
<pre>guard let sketch = self.sketchView.currentSketch else{<br/>    return<br/>}<br/>self.queryFacade.cancel()<br/>let image = self.queryImages[indexPath.row] </pre>
<p>Now, with reference to the current sketch and image, we want to try and keep the size relatively the same as the user's sketch. We will do this by simply obtaining the sketch's bounding box and scaling the dimensions to respect the aspect ratio of the selected image. Add the following code, which handles this:</p>
<pre>    var origin = CGPoint(x:0, y:0)<br/>    var size = CGSize(width:0, height:0)<br/>    <br/>    if bbox.size.width &gt; bbox.size.height{<br/>        let ratio = image.size.height / image.size.width<br/>        size.width = bbox.size.width<br/>        size.height = bbox.size.width * ratio<br/>    } else{<br/>        let ratio = image.size.width / image.size.height<br/>        size.width = bbox.size.height * ratio<br/>        size.height = bbox.size.height<br/>    } </pre>
<p>Next, we obtain the origin (top left of the image) by obtaining the center of the sketch and offsetting it relative to its width and height. Do this by appending the following code:</p>
<pre>origin.x = sketch.center.x - size.width / 2<br/>origin.y = sketch.center.y - size.height / 2</pre>
<p>We can now use the image, size, and origin to create an <kbd>ImageSketch</kbd>, and replace it with the current sketch simply by assigning it to the <kbd>currentSketch</kbd> property of the <kbd>SketchView</kbd> instance. Add the following code to do just that:</p>
<pre>self.sketchView.currentSketch = ImageSketch(image:image,<br/>                                            origin:origin,<br/>                                            size:size,<br/>                                            label:"")</pre>
<p>Finally, some housekeeping; we'll clear the <kbd>UICollectionView</kbd> by removing all images from the <kbd>queryImages</kbd> array (its data source) and request it to reload itself. Add the following block to complete the <kbd>collectionView(_ collectionView:,didSelectItemAt:)</kbd> method:</p>
<pre>self.queryImages.removeAll()<br/>self.toolBarLabel.isHidden = queryImages.count == 0<br/>self.collectionView.reloadData()</pre>
<p>Everything is now hooked up; we have implemented all of the functionality that guesses what the user is drawing, presents suggestions, and allows the user to swap their rough sketch with an alternative. Now is a good time to build and run to ensure that everything is working as planned. If so then, you should be able to swap out your sketch with one of the suggestions presented at the top, as shown in the following screenshot: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/46e73980-ff72-45ff-a499-9b7c0a43bc23.png"/></div>
<p>One last section before wrapping this chapter up. In this section, we will look at a technique to fine-tune our search results to better match what the user has drawn. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sorting by visual similarity</h1>
                </header>
            
            <article>
                
<p>So far, we have achieved what we set out to do, that is, inferring what the user is trying to draw and providing them with suggestions that they can swap their sketch with. But our solution currently falls short of understanding the user. Sure, it may predict correctly and provide the correct category of what the user is drawing, but it dismisses any style or details of the user's drawing. For example, if the user is drawing, and only wanting, a cats head, our model may predict correctly that the user is drawing a cat but ignore the fact that their drawing lacks a body. It is likely to suggest images of full-bodied cats. </p>
<p>In this section, we will look at a technique to be more sensitive with respect to the user's input, and provide a very rudimentary solution but one that can be built upon. This approach will attempt to sort images by how similar they are with the user's sketch. Before jumping into the code, let's take a quick detour to discuss similarity metrics, by looking at how we can measure the similarity between something in a different domain, such as sentences. The following are three sentences we will base our discussion on:</p>
<ul>
<li><strong>"the quick brown fox jumped over the lazy dog"</strong></li>
<li><strong>"the quick brown fox runs around the lazy farm dog" </strong></li>
<li><strong>"machine learning creates new opportunities for interacting with computers" </strong></li>
</ul>
<p>This exercise will be familiar to those withttps://packt-type-cloud.s3.amazonaws.com/uploads/sites/1956/2018/06/B09544_08_14.pngal representation. Here, we will create a vocabulary with all words that exist in our corpus (the three sentences, in this instance) and then create vectors for each sentence by incrementing the sentences words with their <span>corresponding index in the vocabulary, as shown in the following screenshot:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/67898693-ff13-4279-996c-b20e8f0c25fa.png"/></div>
<p>With our sentences now encoded as vectors, we can measure the similarity between each sentence by performing distance operations such as <strong>Euclidean Distance</strong> and <strong>Cosine Distance</strong>. The equations for each of these are as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ddb57d2f-f085-4302-aa51-7df5ee99d794.png"/></div>
<p><span>Let's now calculate the distances between each of the sentences and compare the results. See the following screenshot for the results:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/1430e9fd-3ceb-460b-9e98-1765d1101a2d.png"/></div>
<p>As you would expect, the sentences <strong>"the quick brown fox jumped over the lazy dog"</strong> and <strong>"the quick brown fox ran around the lazy farm dog"</strong> have a smaller distance between them compared to that for the sentence <strong>"machine learning creates new opportunities for interacting with computers"</strong>. If you were to build a recommendation engine, albeit a naive one, you would likely rank the sentences with more words in common higher than the ones with less words in common. The same is true for images, but unlike sentences, where we are using words as features, we use the features derived from layers of the network. </p>
<p>Recall that our network for classifying sketches consists of a stack of convolution layers, with each layer building higher level patterns based on the patterns from the layers below it. Intuitively, we can think of these higher level patterns as our words (features) and the fully connected network as the sentences representing what words are present for a given image. To make this clearer, a simple illustration is shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9e694871-cf9c-422e-8aa3-5bb4c45f2186.png" style="width:39.58em;height:29.83em;"/></div>
<p>Examining the figure, we can see the set of feature maps on the left, which can be thought of as convolutional kernels used to extract horizontal, vertical, left, and right diagonal edges from the images.</p>
<p>In the middle are the samples <span>from </span>which we will be extracting these features. Finally, on the far right, we have the extracted features (histogram) of each of the samples. We use these extracted features as our feature vectors and can use them to calculate the distance between them, as we saw in the previous figure. </p>
<p><span>So, if we are able to extract this type of feature vector from an image, then we will also be able to sort them relative to the user's sketch (using its extracted feature vectors). But how do we get this feature vector? Recall that we already have a network that has learned high-level feature maps. If we are able to obtain a vector indicating which of these features are most active for a given image, then we can use this vector as our feature vector and use it to calculate the distance between other images, such as the user's sketch and downloaded images. This is exactly what we will do; instead of feeding the network through a softmax activation layer (to perform prediction on the classes), we will remove this layer from our network, leaving the last fully connected layer as the new output layer. This essentially provides us with a feature vector that we can then use to compare with other images. The following figure shows how the updated network looks diagrammatically:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/834497fa-4925-47df-bb6c-78269ebd85cd.png"/></div>
<p>If you compare this with the network presented in the previous section, you will notice that the only change is the absence of the fully connected layer. The output of this network is now a feature vector of size 512. Let's make this concept more concrete by playing with it.</p>
<p><span>I assume you have already pulled down the accompanying code from the repository <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a></span><span>. Navigate to the </span><kbd>Chapter7/Start/QuickDraw/</kbd><span> directory and open the playground <kbd>FeatureExtraction.playground</kbd></span><span>. This playground includes the generated code and compiled model described earlier, along with some views and helper methods that we will make use of; all should be fairly self-explanatory. Let's begin by </span>importing some dependencies and declaring some variables by adding the following code to the top of the playground:</p>
<pre>import Accelerate<br/>import CoreML<br/><br/>let histogramViewFrame = CGRect(<br/>    x: 0, y: 0,<br/>    width: 600, height: 300)<br/><br/>let heatmapViewFrame = CGRect(<br/>    x: 0, y: 0,<br/>    width: 600, height: 600)<br/><br/>let sketchFeatureExtractor = cnnsketchfeatureextractor()<br/>let targetSize = CGSize(width: 256, height: 256)<br/>let context = CIContext()</pre>
<p>Here, we declare two rectangles; they will determine the frame of the views we will create later and, most importantly, instantiate our model, which we will use to extract features from each image. Talking about this, if you expand the <kbd>Resources</kbd> folder on the left<span>-hand </span>panel, then again in the <kbd>Images</kbd> folder, you'll see the images we will be using, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/dd2707ae-53ab-4857-8a6f-16e9ce4bf209.png"/></div>
<p>As we discussed, we want to be able to sort the images so that the suggested images closely match what the user is drawing. Continuing on from our example from the user drawing just a cat's head, we want a way to sort out the images so that those with just a cat's head show up before those with a cat and its body. Let's continue on with our experiment; add the following methods, which we will use to extract the features from a given image:</p>
<pre>func extractFeaturesFromImage(image:UIImage) -&gt; MLMultiArray?{<br/>    guard let image = CIImage(<br/>        image: image) else{<br/>        return nil<br/>    }<br/>    return extractFeaturesFromImage(image: image)<br/>}<br/><br/>func extractFeaturesFromImage(image:CIImage) -&gt; MLMultiArray?{<br/>    guard let imagePixelBuffer = image.resize(<br/>        size: targetSize)<br/>        .rescalePixels()?<br/>        .toPixelBuffer(context: context,<br/>                       gray: true) else {<br/>        return nil<br/>    }<br/><br/>    guard let features = try? sketchFeatureExtractor.prediction(<br/>        image: imagePixelBuffer) else{<br/>        return nil<br/>    }<br/><br/>    return features.classActivations<br/>}</pre>
<p>Most of the code should look familiar to you; we have an overloaded method for handling <kbd>UIImage</kbd>, which simply creates a <kbd>CIImage</kbd> instance of it before passing it to the other method. This will handle preparing the image and finally feed it into the model. Once inference has been performed, we return the model's property <kbd>classActiviations</kbd> as discussed previously. This is the output from the last fully connected layer, which we'll use as our feature vector for comparison. </p>
<p>Next, we will load all of our images and extract the features from each of them. Add the following code to your playground:</p>
<pre>var images = [UIImage]()<br/>var imageFeatures = [MLMultiArray]()<br/>for i in 1...6{<br/>    guard let image = UIImage(named:"images/cat_\(i).png"),<br/>        let features = extractFeaturesFromImage(image:image) else{<br/>            fatalError("Failed to extract features")<br/>    }<br/>    <br/>    images.append(image)<br/>    imageFeatures.append(features)<br/>}</pre>
<p>With our images and features now available, let's inspect a few of the images and their feature maps. We can do this by creating an instance of <kbd>HistogramView</kbd> and passing in the features. Here is the code to do just that:</p>
<pre>let img1 = images[0]<br/>let hist1 = HistogramView(frame:histogramViewFrame, data:imageFeatures[0])<br/><br/>let img2 = images[1]<br/>let hist2 = HistogramView(frame:histogramViewFrame, data:imageFeatures[1])<br/><br/>// cat front view<br/>let img3 = images[2]<br/>let hist3 = HistogramView(frame:histogramViewFrame, data:imageFeatures[2])<br/><br/>let img4 = images[3]<br/>let hist4 = HistogramView(frame:histogramViewFrame, data:imageFeatures[3])<br/><br/>// cats head<br/>let img5 = images[4]<br/>let hist5 = HistogramView(frame:histogramViewFrame, data:imageFeatures[4])<br/><br/>let img6 = images[5]<br/>let hist6 = HistogramView(frame:histogramViewFrame, data:imageFeatures[5]) </pre>
<p>You can manually inspect each of them by clicking on the eye icon within the preview view associated with the state, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/454353fc-845b-47b1-a7e4-d2d52c49cb66.png"/></div>
<p>Inspecting each of them individually doesn't provide much insight. So in this figure, I have presented three images that we can inspect:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/22bb497d-0132-458b-bee8-8c3d0c2cefc2.png"/></div>
<p>Without too much focus, you get a sense that the cat heads' feature vectors are more closely aligned than the feature vector of the side on view of the cat, especially on the right-hand of the plot.</p>
<p>Let's further explore this by calculating the cosine distance between each of the images and plotting them on a heat map. Start by adding the following code; it will be used to calculate the cosine distance:</p>
<pre>func dot(vecA: MLMultiArray, vecB: MLMultiArray) -&gt; Double {<br/>    guard vecA.shape.count == 1 &amp;&amp; vecB.shape.count == 1 else{<br/>        fatalError("Expecting vectors (tensor with 1 rank)")<br/>    }<br/>    <br/>    guard vecA.count == vecB.count else {<br/>        fatalError("Excepting count of both vectors to be equal")<br/>    }<br/>    <br/>    let count = vecA.count<br/>    let vecAPtr = UnsafeMutablePointer&lt;Double&gt;(OpaquePointer(vecA.dataPointer))<br/>    let vecBPptr = UnsafeMutablePointer&lt;Double&gt;(OpaquePointer(vecB.dataPointer))<br/>    var output: Double = 0.0<br/>    <br/>    vDSP_dotprD(vecAPtr, 1, vecBPptr, 1, &amp;output, vDSP_Length(count))<br/>    <br/>    var x: Double = 0<br/>    <br/>    for i in 0..&lt;vecA.count{<br/>        x += vecA[i].doubleValue * vecB[i].doubleValue<br/>    }<br/>    <br/>    return x<br/>}<br/><br/>func magnitude(vec: MLMultiArray) -&gt; Double {<br/>    guard vec.shape.count == 1 else{<br/>        fatalError("Expecting a vector (tensor with 1 rank)")<br/>    }<br/>    <br/>    let count = vec.count<br/>    let vecPtr = UnsafeMutablePointer&lt;Double&gt;(OpaquePointer(vec.dataPointer))<br/>    var output: Double = 0.0<br/>    vDSP_svsD(vecPtr, 1, &amp;output, vDSP_Length(count))<br/>    <br/>    return sqrt(output)<br/>} </pre>
<p>The details of the equation were presented before and this is just a translation of these into Swift; what is important is the use of the <strong>vector Digital Signal Processing</strong><span> </span><span>(</span><strong><span>vDSP </span></strong>) functions available within iOS's Accelerate framework. As described in the documentation, the vDSP API provides mathematical functions for applications such as speech, sound, audio, video processing, diagnostic medical imaging, radar signal processing, seismic analysis, and scientific data processing. Because it's built on top of Accelerate, it inherits the performance gains achieved through <strong>single instruction, multiple data</strong> (<strong>SIMD</strong>) running the same instruction concurrently across a vector of data—something very important when dealing with large vectors such as those from neural networks. Admittedly, at first it seems unintuitive, but the documentation provides most of what you'll need to make good use of it; let's inspect the <kbd>magnitude</kbd> method to get a feel for it.</p>
<p>We use the <kbd>vDSP_svsD</kbd> <span>function </span>to calculate the magnitude of our feature vectors; the function is expecting these arguments (in order): a pointer to the data (<kbd>UnsafePointer&lt;Double&gt;</kbd>), strides (<kbd>vDSP_Stride</kbd>), a pointer to the output variable (<kbd>UnsafeMutablePointer&lt;Double&gt;</kbd>), and finally the length (<kbd>vDSP_Length</kbd>). Most of the work is in preparing these arguments, as shown in this code snippet:</p>
<pre>let vecPtr = UnsafeMutablePointer&lt;Double&gt;(OpaquePointer(vec.dataPointer))<br/>var output: Double = 0.0<br/>vDSP_svsD(vecPtr, 1, &amp;output, vDSP_Length(vec.count))</pre>
<p>After this function returns, we will have the calculated the magnitude of a given vector stored in the <kbd>output</kbd> variable. Let's now make use of this and calculate the distance between each of the images. Add the following code to your playground:</p>
<pre>var similarities = Array(repeating: Array(repeating: 0.0, count: images.count), count: images.count)<br/><br/>for i in 0..&lt;imageFeatures.count{<br/>    for j in 0..&lt;imageFeatures.count{<br/>        let sim = cosineSimilarity(<br/>            vecA: imageFeatures[i],<br/>            vecB: imageFeatures[j])<br/>        similarities[i][j] = sim<br/>    }<br/>}</pre>
<p>Here, we are iterating through each of the images twice to create a matrix (multi-dimensional array, in this case) to store the distances (similarities) between each of the images. We will now feed this, along with the associated images, to an instance of <kbd>HeatmapView</kbd>, which will visualize the distances between each of the images. Add the following code and then expand the view by clicking on the eye icon within the results panel to see the result:</p>
<pre>let heatmap = HeatmapView(<br/>    frame:heatmapViewFrame,<br/>    images:images,<br/>    data:similarities)</pre>
<p>As mentioned previously, by previewing the view, you should see something similar to the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/14695986-814c-4877-bbda-7c1ef8f298e0.png" style="width:35.50em;height:35.83em;"/></div>
<p>This visualization shows the distance between each of the images; the darker the cell, the closer they are. For example, if you look at cell 1 x 1, cell 2 x 2, and so on, you will see that each of these cells are darker (a distance of 0 because they are the same image). You'll also notice another pattern form: clusters of four cells diagonally down the plot. This, consequently, was our goal—to see whether we could sort sketches by their similarities, such as cats drawn front on, cat heads, and cats drawn side on. </p>
<p>Armed with our new knowledge, let's return to the iPhone project <kbd>QuickDraw.xcodeproj</kbd>, where we will copy this code across and implement sorting. </p>
<p>With the <kbd>QuickDraw</kbd> project now open, locate the feature extractor model from <span>the project repositories folder</span> <kbd>/CoreMLModels/Chapter7/cnnsketchfeatureextractor.mlmodel</kbd><span>. With the model selected, drag it onto your Xcode project, leaving the defaults for the import options. </span></p>
<p>With the model now imported, select the file <kbd>QueryFacade.swift</kbd> from the left<span>-hand p</span>anel (within Xcode) to bring it up in the editor area. With the class open, add an instance variable to the top of the <kbd>QueryFacade</kbd> class, as shown here:</p>
<pre>let sketchFeatureExtractor = cnnsketchfeatureextractor()</pre>
<p>Next, copy across the methods <kbd>extractFeaturesFromImage</kbd>, <kbd>cosineSimilarity</kbd>, <kbd>dot</kbd>, and <kbd>magnitude</kbd> from your playground to the <kbd>QueryFacade</kbd> class, as shown here:</p>
<pre>func extractFeaturesFromImage(image:CIImage) -&gt; MLMultiArray?{<br/>    // obtain the CVPixelBuffer from the image<br/>    guard let imagePixelBuffer = image<br/>        .resize(size: self.targetSize)<br/>        .rescalePixels()?<br/>        .toPixelBuffer(context: self.context, gray: true) else {<br/>        return nil<br/>    }<br/>    <br/>    guard let features = try? self.sketchFeatureExtractor<br/>        .prediction(image: imagePixelBuffer) else{<br/>        return nil<br/>    }<br/>    <br/>    return features.classActivations<br/>}<br/><br/>func cosineSimilarity(vecA: MLMultiArray,<br/>                                  vecB: MLMultiArray) -&gt; Double {<br/>    return 1.0 - self.dot(vecA:vecA, vecB:vecB) /<br/>        (self.magnitude(vec: vecA) * self.magnitude(vec: vecB))<br/>}<br/><br/>func dot(vecA: MLMultiArray, vecB: MLMultiArray) -&gt; Double {<br/>    guard vecA.shape.count == 1 &amp;&amp; vecB.shape.count == 1 else{<br/>        fatalError("Expecting vectors (tensor with 1 rank)")<br/>    }<br/>    <br/>    guard vecA.count == vecB.count else {<br/>        fatalError("Excepting count of both vectors to be equal")<br/>    }<br/>    <br/>    let count = vecA.count<br/>    let vecAPtr = UnsafeMutablePointer&lt;Double&gt;(<br/>        OpaquePointer(vecA.dataPointer)<br/>    )<br/>    let vecBPptr = UnsafeMutablePointer&lt;Double&gt;(<br/>        OpaquePointer(vecB.dataPointer)<br/>    )<br/>    var output: Double = 0.0<br/>    <br/>    vDSP_dotprD(vecAPtr, 1,<br/>                vecBPptr, 1,<br/>                &amp;output,<br/>                vDSP_Length(count))<br/>    <br/>    var x: Double = 0<br/>    <br/>    for i in 0..&lt;vecA.count{<br/>        x += vecA[i].doubleValue * vecB[i].doubleValue<br/>    }<br/>    <br/>    return x<br/>}<br/><br/>func magnitude(vec: MLMultiArray) -&gt; Double {<br/>    guard vec.shape.count == 1 else{<br/>        fatalError("Expecting a vector (tensor with 1 rank)")<br/>    }<br/>    <br/>    let count = vec.count<br/>    let vecPtr = UnsafeMutablePointer&lt;Double&gt;(<br/>        OpaquePointer(vec.dataPointer)<br/>    )<br/>    var output: Double = 0.0<br/>    vDSP_svsD(vecPtr, 1, &amp;output, vDSP_Length(count))<br/>    <br/>    return sqrt(output)<br/>}</pre>
<p>With our methods now it place, it's time to make use of them. Locate the method <kbd>sortByVisualSimilarity(images:[CIImage], sketchImage:CIImage)</kbd>; this method is already called within the <kbd>queryCurrentSketch</kbd> method, but currently it just returns the list that was passed in. It's within this method that we want to add some order by sorting the list so that the images most similar to the user's sketch are first. Let's build this up in chunks, starting with extracting the image features of the user's sketch. Add the following code to the body of the <kbd>sortByVisualSimilarity</kbd> method, replacing its current contents:</p>
<pre>guard let sketchFeatures = self.extractFeaturesFromImage(<br/>    image: sketchImage) else{<br/>    return nil<br/>}</pre>
<p>Next, we want the features of all the other images, which we do simply by iterating over the list and storing them in an array. Add the following code to do just that:</p>
<pre>var similatiryScores = Array&lt;Double&gt;(<br/>    repeating:1.0,<br/>    count:images.count)<br/><br/>for i in 0..&lt;images.count{<br/>    var similarityScore : Double = 1.0<br/>    <br/>    if let imageFeatures = self.extractFeaturesFromImage(<br/>        image: images[i]){<br/>        similarityScore = self.cosineSimilarity(<br/>            vecA: sketchFeatures,<br/>            vecB: imageFeatures)<br/>    }<br/>    <br/>    similatiryScores[i] = similarityScore<br/>    <br/>    if self.isInterrupted{<br/>        return nil<br/>    }<br/>}</pre>
<p>As we did previously, after each image, we check whether the process has been interrupted by checking the property <kbd>isI<span>nterrupted</span></kbd>, before moving on to the next image. Our final task is to sort and return this images; add the following code to the body of the method <kbd>sortByVisualSimilarity</kbd>:</p>
<pre>return images.enumerated().sorted { (elemA, elemB) -&gt; Bool in<br/>    return similatiryScores[elemA.offset] &lt; similatiryScores[elemB.offset]<br/>    }.map { (item) -&gt; CIImage in<br/>        return item.element<br/>}</pre>
<p>With that implemented, now is a good time to build and run your project to see that is everything is working, and compare the results with the previous build. </p>
<p>And this concludes the chapter; we will briefly wrap up in the summary before moving on to the next chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>You are still here. I'm impressed, and congratulations! It was a long <span>but fruitful</span> chapter. We saw another example of how we can apply CNNs, and in doing so, we further developed our understanding of how they work, how to tune them, and ways in which we can modify them. We saw how we could use the learned features not just for classification but ranking, a technique used in many domains such as fashion discovery and recommendation engines. We also spent a significant amount of time building a drawing application, which we will continue to use in the next chapter. There, we will again explore how to perform sketch classification using a RNN trained on Google's <kbd>QuickDraw</kbd> dataset. Lots of fun ahead, so let's get started.</p>


            </article>

            
        </section>
    </body></html>