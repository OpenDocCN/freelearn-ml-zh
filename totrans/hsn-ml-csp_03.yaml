- en: Bayes Intuition – Solving the Hit and Run Mystery and Performing Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/8e56062f-7bbb-4d3b-9bef-be0912cb3e70.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the start of this chapter was an in-your-face algorithm? I wanted
    to make sure that the first thing you see is this formula. This underscores just
    how important it will become in your machine learning career. Write it down, put
    it on a sticky note on your monitor, or commit it to memory!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the famous Bayes' theorem to solve a very famous problem in computer science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show you how you can use Bayes' theorem and Naive Bayes to plot data, discover
    outliers from truth tables, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overviewing Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be honest, there are as many interpretations of Bayes' theorem as there are
    books about it. The one shown previously is the main one that we will be discussing.
    I would also encourage you to refer to [https://brilliant.org/wiki/bayes-theorem/](https://brilliant.org/wiki/bayes-theorem/)
    for further reading.
  prefs: []
  type: TYPE_NORMAL
- en: To make this more concrete and formal, let's start off with a bit of intuition
    and formality; it will help us set the stage for what is to come.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use Bayes'' theorem, we are measuring the degree of belief of something,
    the likelihood that an event will occur. Let''s just keep it that simple for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad43f1e3-6227-4a34-a79b-26c76628b873.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding formula means the probability of *A* given *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability is usually quantified as a number between 0 and 1, inclusive of
    both; 0 would indicate impossibility and 1 would indicate absolute certainty.
    The higher the probability, the more the certainty. The odds of a dice rolling
    a 6 and the odds of a coin flip coming up heads are two examples of probability
    that you are no doubt very familiar with. There''s also another example you are
    familiar with and encounter daily: spam.'
  prefs: []
  type: TYPE_NORMAL
- en: All of us usually have our email open right beside us, all day long (some of
    us all night long too!). And with the messages that we are expecting also come
    the ones that we are not and do not care to receive. We all hate dealing with
    spam, that nasty email that has nothing to do with anything but Viagra; yet we
    somehow always seem to get it. What is the probability that any one of those emails
    I get each day is spam? What is the probability that I care about its content?
    How would we even know?
  prefs: []
  type: TYPE_NORMAL
- en: So let's talk a little bit about how a spam filter works because, you see, it's
    perhaps the best example of probability we can use! To be precise and more formal,
    we are dealing with **conditional probability**, which is the probability of event
    *A* given the occurrence of event *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way most spam filters work, at least at the very basic level, is by defining
    a list of words that are used to indicate emails that we do not want or did not
    ask to receive. If the email contains those words, it''s considered spam and we
    deal with it accordingly. So, using Bayes'' theorem, we look for the probability
    that an email is spam given a list of words, which would look like this in a formulaic
    view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7eb54af-eab3-4059-8daa-8ddaf1db4f25.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The probability that an email is spam, given a set of words**: User Qniemiec
    in Wikipedia has an incredible visual diagram that explains in full force every
    combination of a probabilistic view, which is represented by the superposition
    of two event trees. If you are a visual person like I am, here is a complete visualization
    of Bayes'' theorem represented by the superposition of two event tree diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f346d0c-ec5b-4ccd-afc3-5ef501fa9ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's move on to a very famous problem. It is called by many names, but
    the basic problem is what is known as the **taxicab problem**. Here's our scenario,
    which we will attempt to solve using probability and Bayes' theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Uber driver was involved in a hit-and-run accident. The famous yellow taxi
    cabs and Uber drivers are the two companies that operate in the city and can be
    seen everywhere. We are given the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: 85% of the cabs in the city are yellow and 15% are Uber.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A witness identified the car involved in the hit and run and stated that it
    had an Uber sticker on it. That being said, we know how reliable witness testimony
    is, so the court decided to test the user and determine their reliability. Using
    the same set of circumstances that existed on the night of the accident, the court
    concluded that the witness correctly identified each one of the two vehicles 80%
    of the time, but failed 20% of the time. This is going to be important, so stay
    with me on this!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Our dilemma**: What is the probability that the car involved in the accident
    was an Uber driver versus a yellow cab?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, here''s how we get to the answer we need:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of Uber drivers identified correctly is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*15 * 0.8 = 12*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The witness is incorrect 20% of the time, so the total number of vehicles incorrectly
    identified is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*85 * 0.2 = 17*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the total vehicles identified by the witness is *12 + 17 = 29*.
    The probability that they identified the Uber driver correctly is hence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*12/29 = @41.3%*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see whether we can develop a simple program that can help us arrive
    at that number to prove our solution works and is viable. To accomplish this,
    we are going to dive into our first open source toolkit: **Encog**. Encog is designed
    to handle problems exactly like this.'
  prefs: []
  type: TYPE_NORMAL
- en: The Encog framework is a full-fledged machine learning framework and was developed
    by Mr. Jeff Heaton. Mr. Heaton has also published several books on the Encog framework,
    as well as other subjects, and I encourage you to seek them out if you plan to
    use this framework extensively. I persoally own every one of them and I consider
    them seminal works.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the code it's going to take to solve our problem. As you'll notice,
    math, statistics, probability... it's all abstracted from you. Encog can allow
    you to focus on the business problem you are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: The complete execution block looks like the following code. We'll begin to dissect
    it in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: OK, let's break this down into more digestible pieces. The first thing we are
    going to do is create a Bayesian network. This object will be at the center of
    solving our mystery. The `BayesianNetwork` object is a wrapper around a probability
    and classification engine.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian network is comprised of one or more `BayesianEvents`. An event
    will be one of three distinct types—`Evidence`, `Outcome`, or `Hidden`—and will
    usually correspond to a number in the training data. An `Event` is always discrete,
    but continuous values (if present and desired) can be mapped to a range of discrete
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the initial network object, we create an event for the Uber
    driver as well as for the witness who claimed they saw the driver involved in
    the hit and run. We will create a dependency between the Uber driver and the witness,
    and then finalize the structure of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to build the actual truth tables. A truth table is a listing
    of all possible values a function can have. There are one or more rows of increasing
    complexity, and the last row is the final function value. If you remember logic
    theory, there are basically three operations that you can have: `NOT`, `AND`,
    and `OR`. 0 usually represents `false`, and 1 usually represents `true`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look just a little bit deeper, we will see that we end up with the following
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If A = 0, -A = 1**If A = 1, -A = 0**A+B = 1, except when A and B = 0**A+B
    = 0 if A and B = 0**A*B = 0, except when A and B = 1**A*B = 1 if A and B = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, back to our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the truth table, we will need to know the probability and the result
    value. In the case of our problem, the probability that an Uber driver was involved
    in the accident is 85%. As for the witness, there is an 80% chance they are telling
    the truth and a 20% chance that they are mistaken. We will use the `AddLine` function
    of the truth table to add this information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's talk a bit more about truth tables. Here is an extended truth table showing
    all the possible truth functions of two variables, *P* and *Q*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac69c6d3-1f8e-4ceb-ab07-3fcf0976fafe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we were to program our truth table more extensively, here is an example
    of how we could do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that our network and truth tables are built, it's time to define some events.
    As we mentioned earlier, events are any one of `Evidence`, `Hidden`, or `Outcome`.
    The `Hidden` event, which is neither `Evidence` nor `Outcome`, is still involved
    in the Bayesian graph itself. We will not be using `Hidden` but I wanted you to
    know that it does exist.
  prefs: []
  type: TYPE_NORMAL
- en: To solve our mystery, we must accumulate evidence. In our case, the evidence
    that we have is that the witness reported seeing an Uber driver involved in the
    hit and run. We will define an event type of `Evidence` and assign it to what
    the witness reported. The result, or outcome, is that it was an Uber driver, so
    we will assign an event type of outcome to that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we must account for the fact that, at least some of the time, the
    witness''s report of seeing an Uber driver involved was incorrect. So we must
    create event values for both probabilities—that the witness did not see an Uber
    driver, and that an Uber driver was not involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the query we are going to execute is an `EnumerationQuery`. This
    object allows probabilistic queries against a Bayesian network. This is done by
    calculating every combination of hidden nodes and using total probability to find
    the result. The performance can be weak if our Bayesian network is large, but
    fortunately for us, it is not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we execute our query against our Bayesian network definition and print
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/326f782d-f4b1-4e77-9e13-4b3026ca3c2a.png)'
  prefs: []
  type: TYPE_IMG
- en: The result, as we had hoped for, was 41%.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an exercise for you, see whether you can now use Encog to solve another
    very famous example. In this example, we wake up in the morning and find out that
    the grass was wet. Did it rain, was the sprinkler on, or both? Here''s what our
    truth tables look like on pen and paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99ee0c33-fdf3-44c3-83a3-8ed9aaa5cc95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability that it rained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/458aa8e9-2dee-4938-95bf-6feb70fd95d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The complete truth table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3dafc76f-282d-468d-89c2-31f566fcad11.png)'
  prefs: []
  type: TYPE_IMG
- en: Overviewing Naive Bayes and plotting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we discussed Bayes' theorem, we would be doing a great disservice to
    you if we did not discuss Naive Bayes. It's everywhere, and for good reasons.
    It almost always works well (hence the name, Naive), and you will most certainly
    be exposed to it during your machine learning career. It is a simplistic technique
    based upon a premise that the value of any one feature is completely independent
    from the value of any other. For example, an orange is round, the color is orange,
    the skin is not smooth, and it's 10-20 cm in diameter. A Naive Bayes classifier
    would then consider each feature described previously to contribute independently
    that this is an orange versus an apple, lemon, and so on, even if there is some
    data relationship amongst its features.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, Naïve Bayes is surprisingly efficient in resolving complex situations.
    Although there are scenarios where it can certainly be outperformed, it can be
    a great first-try algorithm to apply to your problem. We only need a very small
    amount of training data compared to many other models.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our next application, we will be using the fantastic Accord.NET machine learning
    framework to provide you with a tool with which you can enter data, watch it being
    plotted, and learn about false positives and negatives. We will be able to enter
    data for objects that exist in our data space and categorize them as either being
    green or blue. We will be able to change that data and see how it is classified
    and, more importantly, visually represented. Our objective is to learn which set
    new cases fall into as they arrive; they are either green or blue. In addition,
    we want to track false positives and false negatives. Naive Bayes will do this
    for us based upon the data that exists within our data space. Remember, after
    we train our Naive Bayes classifier, the end goal is that it can recognize new
    objects from data it has previously never seen. If it cannot, then we need to
    circle back to the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly discussed truth tables, and now it's time to go back and put a bit
    more formality behind that definition. More concretely, let's talk in terms of
    a **confusion matrix**. In machine learning, a confusion matrix (error matrix
    or matching matrix) is a table layout that lets you visualize the performance
    of an algorithm. Each row represents predicted class instances, while each column
    represents actual class instances. It's called a confusion matrix because the
    visualization makes it easy to see whether you are confusing one with the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'An abstract view of a truth table would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **X present** | **X absent** |  |'
  prefs: []
  type: TYPE_TB
- en: '| Test positive | True positive | False positive | Total positive |'
  prefs: []
  type: TYPE_TB
- en: '| Test negative | False negative | True negative | Total negative |'
  prefs: []
  type: TYPE_TB
- en: '|  | Total with X | Total without X | Grand total |'
  prefs: []
  type: TYPE_TB
- en: 'A more visual view of the same truth table would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb4b97cd-5cfd-4804-aabf-2e0c0949798d.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual view of the truth table
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, a more formal view of a true confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0a00c1f-f694-4660-9dc7-cf0648e81e89.png)'
  prefs: []
  type: TYPE_IMG
- en: In the field of machine learning, the truth table/confusion matrix allows you
    to visually assess the performance of your algorithm. As you will see in our following
    application, every time you add or change data, you will be able to see whether
    any of these false or negative conditions occur.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the test data we will start out with is split evenly between green
    and blue objects, so there's no reasonable probability that any new case is more
    likely to be one versus the other. This reasonable probability, sometimes called
    a **belief**, is more formally known as the **prior probability** (there's that
    word again!). Prior probabilities are based upon prior experience with what we've
    seen with the data and, in many cases, this information is used to predict outcomes
    prior to them happening. Given a prior probability or belief, we will formulate
    a conclusion which then becomes our **posterior belief**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we are looking at:'
  prefs: []
  type: TYPE_NORMAL
- en: The prior probability of green objects being the total number of green objects/the
    total number of objects in our data space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prior probability of blue objects being the total number of blue objects/the
    total number of objects in our data space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look a little bit further into what's happening.
  prefs: []
  type: TYPE_NORMAL
- en: You can see what our data looks like in the following screenshot. The *X* and
    *Y* columns indicate coordinates in our data space along an *x* and *y* axis,
    and the *G* column is a label as to whether or not the object is green. Remember,
    supervised learning should give us the objective we are trying to arrive at, and
    Naive Bayes should make it easy to see whether that's true.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7914ccf9-d845-4e8b-b919-e90892751181.png)'
  prefs: []
  type: TYPE_IMG
- en: If we take the preceding data and create a scatter plot of it, it will look
    like the following screenshot. As you can see, all the points in our data space
    are plotted, and the ones with our *G* column having a value of 0 are plotted
    as blue, while those having a value of 1 are plotted as green.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each data point is plotted against its *X*/*Y* location in our data space,
    represented by the *x*/*y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/828b307b-375c-40a0-8620-4babe7b20456.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But what happens when we add new objects to our data space that the Naive Bayes
    classifier cannot correctly classify? We end up with what is known as false negatives
    and false positives, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6247b05-3d46-4daa-9079-23b34a5f0561.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have only two categories of data (green and blue), we need to determine
    how these new data objects will be correctly classified. As you can see, we have
    14 new data points, and the color coding shows where they align to the *x* and
    *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s view our application in its full form. The following is a screenshot
    of our main screen. Under the Data Samples tab on the left-hand side of the screen,
    we can see that we have our data space loaded. On the right-hand side of the screen,
    we can see that we have a scatter plot visual diagram that helps us visualize
    that data space. As you can see, all the data points have been plotted and color-coded
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56cebee2-9632-455e-b8ad-6740cffc1b11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take a look at how the probabilities are classified and plotted, you
    can see that the data presents itself almost in two enclosed but overlapping clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/076e34d3-9c73-453a-b6be-a52ce3a3e5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: When a data point in our space overlaps a different data point of a different
    color, that's where we need Naive Bayes to do its job for us.
  prefs: []
  type: TYPE_NORMAL
- en: If we switch to our Model Testing tab, we can see the new data points we added.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6247b05-3d46-4daa-9079-23b34a5f0561.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's modify some of the data points that we have added in order to show
    how any one data point can become a false negative or a false positive. Note that
    we start this exercise with seven false negatives and seven false positives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/041a0a99-8d46-4295-bc5e-f948f664b1fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The data modifications we made previously result in the following plot. As
    you can see, we have additional false positives now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79345884-955a-4004-a9f3-7970f160c9a8.png)'
  prefs: []
  type: TYPE_IMG
- en: I will leave it up to you to experiment with the data and continue your Naive
    Bayes learning!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about probability theory, Bayes theorem, Naive Bayes,
    and how to apply it to real-world problems. We also learned how to develop a tool
    that will help us test out our classifier and see whether our data holds any false
    negatives or positives.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will dive deeper into the world of machine learning
    and talk about reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creative Commons Attribution—ShareAlike License
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heaton, J. (2015). *Encog: Library of Interchangeable Machine Learning Models
    for Java and C#*, *Journal of Machine Learning Research*, 16, 1243-1247'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copyright 2008-2014, Heaton Research, Inc
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copyright (c) 2009-2017, Accord.NET authors authors@accord-framework.net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Case Study: The base rate fallacy reconsidered* (Koehler (1996))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
