["```py\n> load(\"../Data/GC2.RData\")\n> table(GC2$good_bad)\n bad good \n 300  700 \n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(GC2),replace = \n+ TRUE,prob = c(0.7,0.3))\n> head(Train_Test)\n[1] \"Test\"  \"Test\"  \"Test\"  \"Test\"  \"Train\" \"Train\"\n> GC2_Train <- GC2[Train_Test==\"Train\",]\n> GC2_TestX <- within(GC2[Train_Test==\"Test\",],rm(good_bad))\n> GC2_TestY <- GC2[Train_Test==\"Test\",\"good_bad\"]\n> GC2_TestY_numeric <- as.numeric(GC2_TestY)\n> GC2_Formula <- as.formula(\"good_bad~.\")\n> p <- ncol(GC2_TestX)\n> ntr <- nrow(GC2_Train) \n> nte <- nrow(GC2_TestX) \n> # Logistic Regression\n> LR_fit <- glm(GC2_Formula,data=GC2_Train,family = binomial())\n> LR_Predict_Train <- predict(LR_fit,newdata=GC2_Train,\n+ type=\"response\")\n> LR_Predict_Train <- as.factor(ifelse(LR_Predict_Train>0.5,\n+ \"good\",\"bad\"))\n> LR_Accuracy_Train <- sum(LR_Predict_Train==GC2_Train$good_bad)/\n+ ntr\n> LR_Accuracy_Train\n[1] 0.78\n> LR_Predict_Test <- predict(LR_fit,newdata=GC2_TestX,\n+ type=\"response\")\n> LR_Predict_Test_Bin <- ifelse(LR_Predict_Test>0.5,2,1)\n> LR_Accuracy_Test <- sum(LR_Predict_Test_Bin==\n+ GC2_TestY_numeric)/nte\n> LR_Accuracy_Test\n[1] 0.757\n> # Naive Bayes\n> NB_fit <- naiveBayes(GC2_Formula,data=GC2_Train)\n> NB_Predict_Train <- predict(NB_fit,newdata=GC2_Train)\n> NB_Accuracy_Train <- sum(NB_Predict_Train==\n+ GC2_Train$good_bad)/ntr\n> NB_Accuracy_Train\n[1] 0.767\n> NB_Predict_Test <- predict(NB_fit,newdata=GC2_TestX)\n> NB_Accuracy_Test <- sum(NB_Predict_Test==GC2_TestY)/nte\n> NB_Accuracy_Test\n[1] 0.808\n> # Decision Tree\n> CT_fit <- rpart(GC2_Formula,data=GC2_Train)\n> CT_Predict_Train <- predict(CT_fit,newdata=GC2_Train,\n+ type=\"class\")\n> CT_Accuracy_Train <- sum(CT_Predict_Train==\n+ GC2_Train$good_bad)/ntr\n> CT_Accuracy_Train\n[1] 0.83\n> CT_Predict_Test <- predict(CT_fit,newdata=GC2_TestX,\n+ type=\"class\")\n> CT_Accuracy_Test <- sum(CT_Predict_Test==GC2_TestY)/nte\n> CT_Accuracy_Test\n[1] 0.706\n> # Support Vector Machine\n> SVM_fit <- svm(GC2_Formula,data=GC2_Train)\n> SVM_Predict_Train <- predict(SVM_fit,newdata=GC2_Train,\n+ type=\"class\")\n> SVM_Accuracy_Train <- sum(SVM_Predict_Train==\n+ GC2_Train$good_bad)/ntr\n> SVM_Accuracy_Train\n[1] 0.77\n> SVM_Predict_Test <- predict(SVM_fit,newdata=GC2_TestX,\n+ type=\"class\")\n> SVM_Accuracy_Test <- sum(SVM_Predict_Test==GC2_TestY)/nte\n> SVM_Accuracy_Test\n[1] 0.754\n```", "```py\n> DN <- read.csv(\"../Data/Diverse_Numeric.csv\")\n> windows(height=100,width=100)\n> plot(NULL,xlim=c(5,70),ylim=c(0,7),yaxt='n',xlab=\"X-values\",ylab=\"\")\n> points(DN[1,2:6],rep(1,5),pch=c(19,1,1,1,0),cex=2)\n> points(DN[2,2:6],rep(2,5),pch=c(19,1,1,1,0),cex=2)\n> points(DN[3,2:6],rep(3,5),pch=c(19,1,1,1,0),cex=2)\n> points(DN[4,2:6],rep(4,5),pch=c(19,1,1,1,0),cex=2)\n> points(DN[5,2:6],rep(5,5),pch=c(19,1,1,1,0),cex=2)\n> points(DN[6,2:6],rep(6,5),pch=c(19,1,1,1,0),cex=2)\n> legend(x=45,y=7,c(\"Actual\",\"Model\",\"Ensemble\"),pch=c(19,1,0))\n> axis(2,at=1:6,labels=paste(\"Case\",1:6),las=1)\n```", "```py\n> # Drop the Geese Pair\n>GP<- function(Pred_Matrix) {\n+   L<- ncol(Pred_Matrix) # Number of classifiers\n+   N<- nrow(Pred_Matrix)\n+   GP_Matrix <- matrix(TRUE,nrow=L,ncol=L)\n+   for(i in 1:(L-1)){\n+     for(j in (i+1):L){\n+       GP_Matrix[i,j] <- ifelse(sum(Pred_Matrix[,i]==Pred_Matrix[,j])==N,\n+                                TRUE,FALSE)\n+       GP_Matrix[j,i] <- GP_Matrix[i,j]\n+     }\n+   }\n+   return(GP_Matrix)\n+ }\n```", "```py\n> data(CART_Dummy)\n> CART_Dummy$Y <- as.factor(CART_Dummy$Y)\n> attach(CART_Dummy)\n> windows(height=100,width=200)\n> par(mfrow=c(1,2))\n> plot(c(0,12),c(0,10),type=\"n\",xlab=\"X1\",ylab=\"X2\")\n> points(X1[Y==0],X2[Y==0],pch=15,col=\"red\")\n> points(X1[Y==1],X2[Y==1],pch=19,col=\"green\")\n> title(main=\"A Difficult Classification Problem\")\n> plot(c(0,12),c(0,10),type=\"n\",xlab=\"X1\",ylab=\"X2\")\n> points(X1[Y==0],X2[Y==0],pch=15,col=\"red\")\n> points(X1[Y==1],X2[Y==1],pch=19,col=\"green\")\n> segments(x0=c(0,0,6,6),y0=c(3.75,6.25,2.25,5),\n+          x1=c(6,6,12,12),y1=c(3.75,6.25,2.25,5),lwd=2)\n> abline(v=6,lwd=2)\n> title(main=\"Looks a Solvable Problem Under Partitions\")\n```", "```py\n> CD <- CART_Dummy \n> CD$Y <- as.factor(CD$Y)\n> set.seed(1234567)\n> CD_RF <- randomForest(Y~.,data=CD,ntree=500)\n> CD_RF_Predict <- predict(CD_RF,newdata=CD,\n+                           type=\"class\",predict.all=TRUE)\n> CD_RF_Predict_Matrix <- CD_RF_Predict$individual\n> CD_GP <- GP(CD_RF_Predict_Matrix)\n> CD_GP[1:8,1:8]\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[6,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[7,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n> rowSums(CD_ST)\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n[186] 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1\n\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```", "```py\n> which(rowSums(CD_GP)>1)\n [1]  21  42 176 188 206 221 256 278 290 363 365 385 424 442\n> which(CD_GP[21,]==TRUE)\n[1]  21 188\n> which(CD_GP[42,]==TRUE)\n[1]  42 290\n> which(CD_GP[176,]==TRUE)\n[1] 176 363\n> which(CD_GP[206,]==TRUE)\n[1] 206 256\n> which(CD_GP[221,]==TRUE)\n[1] 221 278\n> which(CD_GP[365,]==TRUE)\n[1] 365 424\n> which(CD_GP[385,]==TRUE)\n[1] 385 442\n```", "```py\n> set.seed(12345)\n> GC2_RF3 <- randomForest(GC2_Formula,data=GC2_Train,mtry=10,\n+                         parms = list(split=\"information\",\n+                                      loss=matrix(c(0,1,1000,0),byrow = TRUE,nrow=2)),\n+                         ntree=1000)\n> GC2_RF_Train_Predict <- predict(GC2_RF3,newdata=GC2_Train,\n+                                 type=\"class\",predict.all=TRUE)\n> GC2_RF_Train_Predict_Matrix <- GC2_RF_Train_Predict$individual\n> GC2_GP <- GP(GC2_RF_Train_Predict_Matrix)\n> rowSums(GC2_GP)\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [37] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n[973] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n> which(rowSums(GC2_GP)>1)\ninteger(0)\n```", "```py\n> # Oracle Output\n> Oracle <- function(PM,Actual){\n+   # PM = Prediction Matrix, Actual = the true Y's\n+   OM <- matrix(0,nrow=nrow(PM),ncol=ncol(PM))\n+   for(i in 1:ncol(OM)) {\n+     OM[,i] <- as.numeric(PM[,i]==Actual)\n+   }\n+   return(OM)\n+ }\n> GC_Oracle <- Oracle(PM=GC2_RF_Train_Predict$individual,\n+                     Actual=GC2_Train$good_bad)\n> colSums(GC_Oracle)/nrow(GC_Oracle)\n   [1] 0.872 0.884 0.859 0.869 0.866 0.878 0.888 0.872 0.869 0.875 0.885 0.869\n  [13] 0.881 0.866 0.879 0.856 0.870 0.869 0.857 0.870 0.878 0.868 0.886 0.892\n  [25] 0.881 0.863 0.866 0.856 0.886 0.876 0.873 0.879 0.875 0.885 0.872 0.872\n\n[973] 0.860 0.873 0.869 0.888 0.863 0.879 0.882 0.865 0.891 0.863 0.878 0.879\n [985] 0.878 0.869 0.856 0.872 0.889 0.881 0.868 0.881 0.884 0.854 0.882 0.882\n [997] 0.862 0.884 0.873 0.885\n```", "```py\n> # Disagreement Measure\n> DM <- function(prediction1,prediction2){\n+   tp <- table(prediction1,prediction2)\n+   Diss <- (tp[1,2]+tp[2,1])/length(prediction1)\n+   return(Diss)\n+ }\n```", "```py\n> DM(LR_Predict_Train,NB_Predict_Train)\n[1] 0.121\n> DM(LR_Predict_Train,CT_Predict_Train)\n[1] 0.154\n> DM(LR_Predict_Train,SVM_Predict_Train)\n[1] 0.153\n> DM(NB_Predict_Train,CT_Predict_Train)\n[1] 0.179\n> DM(NB_Predict_Train,SVM_Predict_Train)\n[1] 0.154\n> DM(CT_Predict_Train,SVM_Predict_Train)\n[1] 0.167\n```", "```py\n> # Q-statistic \n> Yule <- function(prediction1,prediction2){\n+   tp <- table(prediction1,prediction2)\n+   Yu <- (tp[1,1]*tp[2,2]-tp[1,2]*tp[2,1])/(tp[1,1]*tp[2,2]+tp[1,2]*tp[2,1])\n+   return(Yu)\n+ }\n> Yule(LR_Predict_Train,NB_Predict_Train)\n[1] 0.949\n> Yule(LR_Predict_Train,CT_Predict_Train)\n[1] 0.906\n> Yule(LR_Predict_Train,SVM_Predict_Train)\n[1] 0.98\n> Yule(NB_Predict_Train,CT_Predict_Train)\n[1] 0.865\n> Yule(NB_Predict_Train,SVM_Predict_Train)\n[1] 0.985\n> Yule(CT_Predict_Train,SVM_Predict_Train)\n[1] 0.912\n```", "```py\n> 1-DM(LR_Predict_Train,NB_Predict_Train)\n[1] 0.879\n> 1-DM(LR_Predict_Train,CT_Predict_Train)\n[1] 0.846\n> 1-DM(LR_Predict_Train,SVM_Predict_Train)\n[1] 0.847\n> 1-DM(NB_Predict_Train,CT_Predict_Train)\n[1] 0.821\n> 1-DM(NB_Predict_Train,SVM_Predict_Train)\n[1] 0.846\n> 1-DM(CT_Predict_Train,SVM_Predict_Train)\n[1] 0.833\n```", "```py\n> # Correlation coefficient \n> # Sneath and Sokal, 1973\n> SS_Cor <- function(prediction1, prediction2){\n+   tp <- table(prediction1,prediction2)\n+   a <- tp[1,1]; b <- tp[2,1]; c <- tp[1,2]; d <- tp[2,2]\n+   SS <- (a*d-b*c)/sqrt(exp(log(a+b)+log(a+c)+log(c+d)+log(b+d)))\n+   return(SS)\n+ }\n```", "```py\n> SS_Cor(LR_Predict_Train,NB_Predict_Train)\n[1] 0.69\n> SS_Cor(LR_Predict_Train,CT_Predict_Train)\n[1] 0.593\n> SS_Cor(LR_Predict_Train,SVM_Predict_Train)\n[1] 0.584\n> SS_Cor(NB_Predict_Train,CT_Predict_Train)\n[1] 0.531\n> SS_Cor(NB_Predict_Train,SVM_Predict_Train)\n[1] 0.587\n> SS_Cor(CT_Predict_Train,SVM_Predict_Train)\n[1] 0.493\n```", "```py\n> # Kappa-statistic \n> # Cohen's Statistic\n> Kappa <- function(prediction1, prediction2){\n+   tp <- table(prediction1,prediction2)\n+   a <- tp[1,1]; b <- tp[2,1]; c <- tp[1,2]; d <- tp[2,2]\n+   n <- length(prediction1)\n+   theta1 <- (a+d)/n\n+   theta2 <- (((a+b)*(a+c))+((c+d)*(b+d)))/n^2\n+   kappa <- (theta1-theta2)/(1-theta2)\n+   return(kappa)\n+ }\n```", "```py\n> Kappa(LR_Predict_Train,NB_Predict_Train)\n[1] 0.69\n> Kappa(LR_Predict_Train,CT_Predict_Train)\n[1] 0.592\n> Kappa(LR_Predict_Train,SVM_Predict_Train)\n[1] 0.524\n> Kappa(NB_Predict_Train,CT_Predict_Train)\n[1] 0.53\n> Kappa(NB_Predict_Train,SVM_Predict_Train)\n[1] 0.525\n> Kappa(CT_Predict_Train,SVM_Predict_Train)\n[1] 0.453\n```", "```py\n> # Double-fault Measure\n> Double_Fault <- function(prediction1,prediction2,actual){\n+   DF <- sum((prediction1!=actual)*(prediction2!=actual))/\n+         length(actual)\n+   return(DF)\n+ }\n> Double_Fault(LR_Predict_Train,NB_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.166\n> Double_Fault(LR_Predict_Train,CT_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.118\n> Double_Fault(LR_Predict_Train,SVM_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.148\n> Double_Fault(NB_Predict_Train,CT_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.709\n> Double_Fault(NB_Predict_Train,SVM_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.154\n> Double_Fault(CT_Predict_Train,SVM_Predict_Train,\n+ GC2_Train$good_bad)\n[1] 0.116\n```", "```py\n> # Entropy Measure\n> # Page 250 of Kuncheva (2014)\n> Entropy_Measure <- function(OM){\n+   # OM = Oracle Matrix\n+   N <- nrow(OM); L <- ncol(OM)\n+   E <- 0\n+   for(i in 1:N){\n+     E <- E+min(sum(OM[i,]),L-sum(OM[i,]))\n+   }\n+   E <- 2*E/(N*(L-1))\n+   return(E)\n+ }\n> Entropy_Measure(GC_Oracle)\n[1] 0.255\n```", "```py\n> # Kohavi-Wolpert variance \n> # Using the predicted probability\n> KW <- function(Prob){\n+   N <- nrow(Prob)\n+   kw <- mean(1-Prob[,1]^2-Prob[,2]^2)/2\n+   return(kw)\n+ }\n> GC2_RF_Train_Predict_Prob <- predict(GC2_RF3,newdata=GC2_Train,\n+                                 type=\"prob\",predict.all=TRUE)\n> GC2_RF_Train_Prob <- GC2_RF_Train_Predict_Prob$aggregate\n> KW(GC2_RF_Train_Prob)\n[1] 0.104\n```", "```py\n> # Using the Oracle matrix\n> KW_OM<- function(OM){\n+   # OM is the oracle matrix\n+   N <- nrow(OM); L <- ncol(OM)\n+   kw <- 0\n+   for(i in 1:N){\n+     lz <- sum(OM[i,])\n+     kw <- kw + lz*(L-lz)\n+   }\n+   kw <- kw/(N*L^2)\n+   return(kw)\n+ }\n> KW_OM(GC_Oracle)\n[1] 0.104\n```", "```py\n> # Disagreement Measure OVerall on Oracle Matrix\n> DMO <- function(OM){\n+   # OM is the oracle matrix\n+   N <- nrow(OM); L <- ncol(OM)\n+   dmo <- 0\n+   for(i in 1:L){\n+     for(j in c(c(1:L)[c(1:L)!=i])){\n+       dmo <- dmo + sum((OM[,i]-OM[,j])^2)/N\n+     }\n+   }\n+   dmo <- dmo/(L*(L-1))\n+   return(dmo)\n+ }\n> DM_GC <- DMO(OM=GC_Oracle)\n> DM_GC\n[1] 0.208\n> KW(GC_Oracle)\n[1] 0.104\n> DM_GC*999/2000\n[1] 0.104\n```", "```py\n> Avg_Ensemble_Acc <- function(Oracle){\n+   return(mean(colSums(GC_Oracle)/nrow(GC_Oracle)))\n+ }\n> Avg_Ensemble_Acc(GC_Oracle)\n[1] 0.872\n> Kappa <- function(Oracle){\n+   pbar <- Avg_Ensemble_Acc(Oracle)\n+   AvgL <- 0\n+   N <- nrow(Oracle); L <- ncol(Oracle)\n+   for(i in 1:N){\n+     lz <- sum(Oracle[i,])\n+     AvgL <- AvgL + lz*(L-lz)\n+   }\n+   Avgl <- AvgL/L\n+   kappa <- 1-Avgl/(N*(L-1)*pbar*(1-pbar))\n+   return(kappa)\n+ }\n> Kappa(GC_Oracle)\n[1] 0.0657\n> 1-DM_GC/(2*Avg_Ensemble_Acc(GC_Oracle)*(1-\n+ Avg_Ensemble_Acc(GC_Oracle)))\n[1] 0.0657\n```"]