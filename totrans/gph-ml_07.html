<html><head></head><body>
		<div id="_idContainer465">
			<h1 id="_idParaDest-76"><a id="_idTextAnchor079"/>Chapter 5: Problems with Machine Learning on Graphs</h1>
			<p>Graph <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) approaches can be useful for a wide range of tasks, with applications ranging from drug design to recommender systems in social networks. Furthermore, given the fact that such methods are <em class="italic">general by design</em> (meaning that they are not tailored to a specific problem), the same algorithm can be used to solve different problems.</p>
			<p>There are common problems that can be solved using graph-based learning techniques. In this chapter, we will mention some of the most well studied of these by providing details about how a specific algorithm, among the ones we have already learned about in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning,</em> and<em class="italic"> </em><a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>, can be used to solve a task. After reading this chapter, you will be aware of the formal definition of many common problems you may encounter when dealing with graphs. In addition, you will learn useful ML pipelines that you can reuse on future real-world problems you will deal with.</p>
			<p>More precisely, the following topics will be covered in this chapter:</p>
			<ul>
				<li>Predicting missing links in a graph </li>
				<li>Detecting meaningful structures such as communities </li>
				<li>Detecting graph similarities and graph matching</li>
			</ul>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor080"/>Technical requirements</h1>
			<p>We will be using <em class="italic">Jupyter</em> Notebooks with Python 3.8 for all of our exercises. In the following code block, you can see a list of the Python libraries that will be installed for this chapter using <strong class="source-inline">pip</strong> (for example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line):</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">karateclub==1.0.19</p>
			<p class="source-code">scikit-learn==0.24.0</p>
			<p class="source-code">pandas==1.1.3</p>
			<p class="source-code">node2vec==0.3.3</p>
			<p class="source-code">numpy==1.19.2</p>
			<p class="source-code">tensorflow==2.4.1</p>
			<p class="source-code">stellargraph==1.2.1</p>
			<p class="source-code">communities==2.2.0</p>
			<p class="source-code">git+https://github.com/palash1992/GEM.git </p>
			<p>All code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter05">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter05</a>.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor081"/>Predicting missing links in a graph</h1>
			<p><strong class="bold">Link prediction</strong>, also known as <strong class="bold">graph completion</strong>, is a <a id="_idIndexMarker539"/>common problem when dealing with graphs. More precisely, from a partially observed graph—a graph where for a certain pair of nodes it is not <a id="_idIndexMarker540"/>possible to exactly know if there is (or there is not) an edge between them—we want to predict whether or not edges exist for the <a id="_idIndexMarker541"/>unknown status node pairs, as seen in <em class="italic">Figure 5.1</em>. Formally, let <img src="image/B16069_05_001.png" alt=""/> be a graph where <img src="image/B16069_05_002.png" alt=""/> is its set of nodes and <img src="image/B16069_05_003.png" alt=""/> is its set of edges. The set of edges <img src="image/B16069_05_004.png" alt=""/> are known as <em class="italic">observed links</em>, while the set of edges <img src="image/B16069_05_005.png" alt=""/> are known as <em class="italic">unknown links</em>. The goal of the link prediction problem is to exploit the information of <img src="image/B16069_05_006.png" alt=""/> and <img src="image/B16069_05_007.png" alt=""/> to estimate <img src="image/B16069_05_008.png" alt=""/>. This problem is also common when dealing with temporal graph data. In this setting, let <img src="image/B16069_05_009.png" alt=""/> be a graph observed at a given timepoint <img src="image/B16069_05_010.png" alt=""/>, where we want to predict the edges of this graph at a given timepoint <img src="image/B16069_05_011.png" alt=""/>. The partially observed graph can be seen here:</p>
			<div>
				<div id="_idContainer388" class="IMG---Figure">
					<img src="image/B16069_05_01.jpg" alt="Figure 5.1 – Partially observed graph with observed link  (solid lines) and unknown link  (dashed lines)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Partially observed graph with observed link <img src="image/B16069_05_012.png" alt=""/> (solid lines) and unknown link <img src="image/B16069_05_013.png" alt=""/> (dashed lines)</p>
			<p>The link prediction problem is widely used in different domains, such as a recommender system in order to propose friendships in social networks or items to purchase on e-commerce websites. It is also used in criminal network investigations in order to <a id="_idIndexMarker542"/>find hidden connections between criminal clusters, as well as in bioinformatics for the analysis of <a id="_idIndexMarker543"/>protein-protein interactions. In the next sections, we will discuss two families of approaches to solve the link prediction problem—namely, <strong class="bold">similarity-based</strong> and <strong class="bold">embedding-based</strong> methods.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor082"/>Similarity-based methods</h2>
			<p>In this subsection, we show <a id="_idIndexMarker544"/>several simple algorithms to solve the label prediction problem. The main shared idea behind all these algorithms is to estimate a similarity function between each couple of <a id="_idIndexMarker545"/>nodes in a graph. If, according to the function, the nodes <em class="italic">look similar</em>, they will have a high probability of being connected by an edge. We will divide these algorithms into two sub-families: <strong class="bold">index-based</strong> and <strong class="bold">community-based</strong> methods. The former contains all the methods through a simple calculation of an index based on the neighbors of a given couple of nodes. The latter contains more sophisticated algorithms, whereby the index is computed using information about the community to which a given couple of nodes belong. In order to give a practical example of these algorithms, we will use the standard implementation available in the <strong class="source-inline">networkx</strong> library in the <strong class="source-inline">networkx.algorithms.link_prediction</strong> package. </p>
			<h3>Index-based methods</h3>
			<p>In this section, we will <a id="_idIndexMarker546"/>show some algorithms available in <strong class="source-inline">networkx</strong> to compute the probability of an edge between two disconnected nodes. These <a id="_idIndexMarker547"/>algorithms are based on the calculation of a simple index through information obtained by analyzing the neighbors of the two disconnected nodes.</p>
			<h4>Resource allocatio<a id="_idTextAnchor083"/><a id="_idTextAnchor084"/>n index</h4>
			<p>The resource allocation index <a id="_idIndexMarker548"/>method estimates the probability that <a id="_idIndexMarker549"/>two nodes <img src="image/B16069_05_014.png" alt=""/> and <img src="image/B16069_05_015.png" alt=""/> are connected by estimating the <em class="italic">resource allocation index</em> for all node pairs according to the following formula:</p>
			<div>
				<div id="_idContainer393" class="IMG---Figure">
					<img src="image/B16069_05_016.jpg" alt=""/>
				</div>
			</div>
			<p>In the given formula, the <img src="image/B16069_05_017.png" alt=""/> function computes the neighbors of the <img src="image/B16069_05_018.png" alt=""/> nod<a id="_idTextAnchor085"/><a id="_idTextAnchor086"/>es and, as visible in the formula, <img src="image/B16069_05_019.png" alt=""/> is a node who is a neighbor of both <img src="image/B16069_05_020.png" alt=""/> and <img src="image/B16069_05_021.png" alt=""/>. This index can be computed in <strong class="source-inline">networkx</strong> using the following code:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">edges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]</p>
			<p class="source-code"> G = nx.from_edgelist(edges)</p>
			<p class="source-code"> preds = nx.resource_allocation_index(G,[(1,2),(2,5),(3,4)])</p>
			<p>The first parameter for the <strong class="source-inline">resource_allocation_index</strong> function is an input graph, while the second parameter is a list of possible edges. We want to compute the probability of a connection. As a result, we get the following output:</p>
			<p class="source-code">[(1, 2, 0.5), (2, 5, 0.5), (3, 4, 0.5)]</p>
			<p>The output is a list <a id="_idIndexMarker550"/>containing couples of nodes such as <strong class="source-inline">(1,2)</strong>, <strong class="source-inline">(2,5)</strong>, and <strong class="source-inline">(3,4)</strong>, which form the resource allocation index. According <a id="_idIndexMarker551"/>to this output, the probability of having an edge between those couples of nodes is <strong class="source-inline">0.5</strong>.</p>
			<h4>Jaccard coefficient</h4>
			<p>The algorithm <a id="_idIndexMarker552"/>computes the probability of a connection <a id="_idIndexMarker553"/>between two nodes <img src="image/B16069_05_0201.png" alt=""/> and <img src="image/B16069_05_021.png" alt=""/>, according to the <em class="italic">Jaccard coefficient</em>, computed as follows:</p>
			<div>
				<div id="_idContainer401" class="IMG---Figure">
					<img src="image/B16069_05_022.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_05_023.png" alt=""/> is used to compute the neighbors of the <img src="image/B16069_05_024.png" alt=""/> node. The function can be used in <strong class="source-inline">networkx</strong> using the following code:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">edges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]</p>
			<p class="source-code"> G = nx.from_edgelist(edges)</p>
			<p class="source-code"> preds = nx.resource_allocation_index(G,[(1,2),(2,5),(3,4)])</p>
			<p>The <strong class="source-inline">resource_allocation_index</strong> function has the same parameters as the previous function. The result of the code is shown here:</p>
			<p class="source-code">[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]</p>
			<p>According to this <a id="_idIndexMarker554"/>output, the probability of having an edge between nodes <strong class="source-inline">(1,2)</strong> is <strong class="source-inline">0.5</strong>, while between nodes <strong class="source-inline">(2,5)</strong> this is <strong class="source-inline">0.25</strong>, and between nodes <strong class="source-inline">(3,4)</strong> this is <strong class="source-inline">0.333</strong>.</p>
			<p>In <strong class="source-inline">networkx</strong>, other <a id="_idIndexMarker555"/>methods to compute the probability of a connection between two nodes based on their similarity score are <strong class="source-inline">nx.adamic_adar_index</strong> and <strong class="source-inline">nx.preferential_attachment</strong>, based on <em class="italic">Adamic/Adar index</em> and <em class="italic">preferential attachment index</em> calculations respectively. Those functions have the same parameters as the others, and accept a graph and a list of a couple of nodes where we want to compute the score. In the next section, we will show another family of algorithms based on community detection.</p>
			<h3>Community-based methods</h3>
			<p>As with index-based methods, the <a id="_idIndexMarker556"/>algorithms belonging to this family also compute an index representing the probability of the <a id="_idIndexMarker557"/>disconnected nodes being connected. The main difference between index-based and community-based methods is related to the logic behind them. Indeed, community-based methods, before generating the index, need to compute information about the community belonging to those nodes. In this subsection, we will show—also providing several examples—some common community-based methods.</p>
			<h4>Community common neighbor</h4>
			<p>In order to estimate the probability of two nodes being connected, this algorithm computes the number of common <a id="_idIndexMarker558"/>neighbors and adds to this value the number of common neighbors belonging to the same community. Formally, for two nodes <img src="image/B16069_05_025.png" alt=""/> and <img src="image/B16069_05_026.png" alt=""/>, the community common neighbor value is computed as follows:</p>
			<div>
				<div id="_idContainer406" class="IMG---Figure">
					<img src="image/B16069_05_027.jpg" alt=""/>
				</div>
			</div>
			<p>In this formula, <img src="image/B16069_05_028.png" alt=""/> is used to compute the neighbors of node <img src="image/B16069_05_029.png" alt=""/>, while <img src="image/B16069_05_030.png" alt=""/> if <img src="image/B16069_05_031.png" alt=""/> belongs to the same community of <img src="image/B16069_05_032.png" alt=""/> and <img src="image/B16069_05_033.png" alt=""/>; otherwise, this is 0. The function can be computed in <strong class="source-inline">networkx</strong> using the following code:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">edges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]</p>
			<p class="source-code"> G = nx.from_edgelist(edges)</p>
			<p class="source-code"> </p>
			<p class="source-code">G.nodes[1]["community"] = 0</p>
			<p class="source-code">G.nodes[2]["community"] = 0</p>
			<p class="source-code">G.nodes[3]["community"] = 0</p>
			<p class="source-code">G.nodes[4]["community"] = 1</p>
			<p class="source-code">G.nodes[5]["community"] = 1</p>
			<p class="source-code">G.nodes[6]["community"] = 1</p>
			<p class="source-code">G.nodes[7]["community"] = 1</p>
			<p class="source-code">preds = nx.cn_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])</p>
			<p>From the preceding code snippet, it is possible to see how we need to assign the <strong class="source-inline">community</strong> property to each node of the graph. This property is used to identify nodes belonging to the same community when <a id="_idIndexMarker559"/>computing the function <img src="image/B16069_05_034.png" alt=""/> defined in the previous equation. The community value, as we will see in the next section, can also be automatically computed using specific algorithms. As we already saw, the <strong class="source-inline">cn_soundarajan_hopcroft</strong> function takes the input graph and a couple of nodes for which we want to compute the score. As a result, we get the following output:</p>
			<p class="source-code">[(1, 2, 2), (2, 5, 1), (3, 4, 1)]</p>
			<p>The main difference from the previous function is in the index value. Indeed, we can easily see that the output is not in the range <strong class="source-inline">(0,1)</strong>.</p>
			<h4>Community resource allocation</h4>
			<p>As with the previous method, the community resource allocation algorithm merges information obtained from the neighbors of the <a id="_idIndexMarker560"/>nodes with the community, as shown in the following formula: </p>
			<div>
				<div id="_idContainer414" class="IMG---Figure">
					<img src="image/B16069_05_035.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_05_036.png" alt=""/> is used to compute the neighbors of node <img src="image/B16069_05_037.png" alt=""/>, while <img src="image/B16069_05_038.png" alt=""/> if <img src="image/B16069_05_039.png" alt=""/> belongs to the same community of <img src="image/B16069_05_040.png" alt=""/> and <img src="image/B16069_05_041.png" alt=""/>; otherwise, this is 0. The function can be computed in <strong class="source-inline">networkx</strong> using the following code:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">edges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]</p>
			<p class="source-code"> G = nx.from_edgelist(edges)</p>
			<p class="source-code"> </p>
			<p class="source-code">G.nodes[1]["community"] = 0</p>
			<p class="source-code">G.nodes[2]["community"] = 0</p>
			<p class="source-code">G.nodes[3]["community"] = 0</p>
			<p class="source-code">G.nodes[4]["community"] = 1</p>
			<p class="source-code">G.nodes[5]["community"] = 1</p>
			<p class="source-code">G.nodes[6]["community"] = 1</p>
			<p class="source-code">G.nodes[7]["community"] = 1</p>
			<p class="source-code">preds = nx. ra_index_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])</p>
			<p>From the preceding code snippet, it is possible to see how we need to assign the <strong class="source-inline">community</strong> property to each node of the graph. This <a id="_idIndexMarker561"/>property is used to identify nodes belonging to the same community when computing the function <img src="image/B16069_05_042.png" alt=""/> defined in the previous equation. The community value, as we will see in the next section, can also be automatically computed using specific algorithms. As we already saw, the <strong class="source-inline">ra_index_soundarajan_hopcroft</strong> function takes the input graph and a couple of nodes for which we want to compute the score. As a result, we get the following output:</p>
			<p class="source-code">[(1, 2, 0.5), (2, 5, 0), (3, 4, 0)]</p>
			<p>From the preceding output, it is possible to see the influence of the community in the computation of the index. Since nodes <strong class="source-inline">1</strong> and <strong class="source-inline">2</strong> belong to the same community, they have a higher value in the index. On the contrary, edges <strong class="source-inline">(2,5)</strong> and <strong class="source-inline">(3,4)</strong> have a value of 0 since they belong to a different community from each other. </p>
			<p>In <strong class="source-inline">networkx</strong>, two other methods to <a id="_idIndexMarker562"/>compute the probability of a connection between two nodes based on their similarity score merged with community information are <strong class="source-inline">nx.a</strong> <strong class="source-inline">within_inter_cluster</strong> and <strong class="source-inline">nx.common_neighbor_centrality</strong>.</p>
			<p>In the next section, we will describe a more complex technique based on ML plus edge embedding to perform prediction of unknown edges.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor087"/>Embedding-based methods</h2>
			<p>In this section, we describe a <a id="_idIndexMarker563"/>more advanced way to perform link prediction. The idea behind this approach is to solve the link <a id="_idIndexMarker564"/>prediction problem as a supervised classification task. More precisely, for a given graph, each couple of nodes is represented with a feature vector (<img src="image/B16069_05_043.png" alt=""/>), and a class label (<img src="image/B16069_05_044.png" alt=""/>) is assigned to each of those node couples. Formally, let <img src="image/B16069_05_045.png" alt=""/> be a graph, and for each couple of nodes <img src="image/B16069_05_046.png" alt=""/>, we build the following formula:</p>
			<div>
				<div id="_idContainer426" class="IMG---Figure">
					<img src="image/B16069_05_047.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_05_048.png" alt=""/> is the <em class="italic">feature vector</em> representing the couple of nodes <img src="image/B16069_05_049.png" alt=""/>, and <img src="image/B16069_05_050.png" alt=""/> is their <em class="italic">label</em>. The value for <img src="image/B16069_05_051.png" alt=""/> is defined as follows: <img src="image/B16069_05_052.png" alt=""/> if, in the graph <strong class="source-inline">G</strong>, the edge connecting node <img src="image/B16069_05_053.png" alt=""/> exists; otherwise, <img src="image/B16069_05_054.png" alt=""/>. Using the feature vector and the labels, we can then train an ML algorithm in order to predict if a given couple of nodes constitute a plausible edge for the given graph.</p>
			<p>If it is easy to build the label vector for each couple of nodes, it is not so straightforward to build the feature space. In order to generate the feature vector for each couple of nodes, we will use some embedding techniques, such as <strong class="source-inline">node2vec</strong> and <strong class="source-inline">edge2vec</strong>, already discussed in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>. Using those embedding algorithms, the generation of the feature space will be greatly simplified. Indeed, the whole process can be summarized in two main steps, outlined as follows:</p>
			<ol>
				<li>For each node of the graph <strong class="source-inline">G</strong>, its embedding vector is computed using a <strong class="source-inline">node2vec</strong> algorithm.</li>
				<li>For all the possible couple of nodes in the graph, the embedding is computed using an <strong class="source-inline">edge2vec</strong> algorithm.</li>
			</ol>
			<p>We can apply now a generic ML algorithm to the generated feature vector in order to solve the classification problem.</p>
			<p>In order to give you a practical explanation of this procedure, we will provide an example in the following code snippet. More precisely, we will describe the whole pipeline (from graph to link prediction) using the <strong class="source-inline">networkx</strong>, <strong class="source-inline">stellargraph</strong>, and <strong class="source-inline">node2vec</strong> libraries. We <a id="_idIndexMarker565"/>will split the whole process into different steps in order to simplify our understanding of the <a id="_idIndexMarker566"/>different parts. The link prediction problem was applied to the citation network dataset described in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs in Python</em>, available at the following link: <a href="https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz">https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz</a>.</p>
			<p>As a first step, we will build a <strong class="source-inline">networkx</strong> graph using the citation dataset, as follows:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">edgelist = pd.read_csv("cora.cites", sep='\t', header=None, names=["target", "source"])</p>
			<p class="source-code">G = nx.from_pandas_edgelist(edgelist)</p>
			<p>Since the dataset is represented as an edge list (see <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs in Python</em>), we used the <strong class="source-inline">from_pandas_edgelist</strong> function to build the graph.</p>
			<p>As a second step, we need to create, from the graph <strong class="source-inline">G</strong>, training and test sets. More precisely, our training and test sets should contain not only a subset of real edges of the graph <strong class="source-inline">G</strong> but also couples of nodes that do not represent a real edge in <strong class="source-inline">G</strong>. The couples representing real edges will be <em class="italic">positive instances</em> (class label 1), while the couples that do not represent real edges will be <em class="italic">negative instances</em> (class label 0). This process can be easily performed as follows:</p>
			<p class="source-code">from stellargraph.data import EdgeSplitter</p>
			<p class="source-code">edgeSplitter = EdgeSplitter(G)</p>
			<p class="source-code"> graph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method="global")</p>
			<p>We used the <strong class="source-inline">EdgeSplitter</strong> class available in <strong class="source-inline">stellargraph</strong>. The main constructor parameter of the <strong class="source-inline">EdgeSplitter</strong> class is the graph (<strong class="source-inline">G</strong>) we want to use to perform our split. The real splitting is performed using the <strong class="source-inline">train_test_split</strong> function that will generate the following outputs:</p>
			<ul>
				<li><strong class="source-inline">graph_test</strong> is a subset of the original graph <img src="image/B16069_05_055.png" alt=""/> containing all the nodes but just a selected subset of edges.</li>
				<li><strong class="source-inline">samples_test</strong> is a vector containing in each position a couple of nodes. This vector will contain couples of nodes representing real edges (positive instance) but also couples of nodes that do not represent real edges (negative instance).</li>
				<li><strong class="source-inline">labels_test</strong> is a vector having the same length as <strong class="source-inline">samples_test</strong>. It contains only 0 or 1. The value of 0 is present in the position representing a negative instance in the <strong class="source-inline">samples_test</strong> vector, while the value of 1 is present in the position representing a positive instance in <strong class="source-inline">samples_test</strong>.</li>
			</ul>
			<p>By following the same <a id="_idIndexMarker567"/>procedure used to <a id="_idIndexMarker568"/>generate the test set, it is possible to generate the training set, as illustrated in the following code snippet:</p>
			<p class="source-code">edgeSplitter = EdgeSplitter(graph_test, G)</p>
			<p class="source-code"> graph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method="global")</p>
			<p>The main difference in this part of code is related to the initialization of <strong class="source-inline">EdgeSplitter</strong>. In this case, we also provide <strong class="source-inline">graph_test</strong> in order to not repeat positive and negative instances generated for the test set.</p>
			<p>At this point, we have our training and testing datasets with negative and positive instances. For each of those instances, we now need to generate their feature vector. In this example, we used the <strong class="source-inline">node2vec</strong> library to generate the node embedding. In general, every node embedding algorithm can be used to perform this task. For the training set, we can thus generate the feature vector with the following code:</p>
			<p class="source-code">from node2vec import Node2Vec</p>
			<p class="source-code">from node2vec.edges import HadamardEmbedder</p>
			<p class="source-code">node2vec = Node2Vec(graph_train)</p>
			<p class="source-code"> model = node2vec.fit()</p>
			<p class="source-code">edges_embs = HadamardEmbedder(keyed_vectors=model.wv)</p>
			<p class="source-code"> train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]</p>
			<p>From the <a id="_idIndexMarker569"/>previous code snippet, it is <a id="_idIndexMarker570"/>possible to see the following:</p>
			<ul>
				<li>We generate the embedding for each node in the training graph using the <strong class="source-inline">node2vec</strong> library.</li>
				<li>We use the <strong class="source-inline">HadamardEmbedder</strong> class to generate the embedding of each couple of nodes contained in the training set. Those values will be used as feature vectors to perform the training of our model. </li>
			</ul>
			<p>In this example, we used the <strong class="source-inline">HadamardEmbedder</strong> algorithm, but in general, other embedding algorithms can be used, such as the ones described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>.</p>
			<p>The previous step needs to also be performed for the test set, with the following code:</p>
			<p class="source-code">edges_embs = HadamardEmbedder(keyed_vectors=model.wv)</p>
			<p class="source-code"> test_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]</p>
			<p>The only difference here is given by the <strong class="source-inline">samples_test</strong> array used to compute the edge embeddings. Indeed, in this case, we use the data generated for the test set. Moreover, it should be noted that the <strong class="source-inline">node2vec</strong> algorithm was not recomputed for the test set. Indeed, given the stochastic nature of <strong class="source-inline">node2vec</strong>, it is not possible to ensure that the two learned embeddings are "comparable" and therefore <strong class="source-inline">node2vec</strong> embeddings will change between runs.</p>
			<p>Everything is set now. We can finally train—using the <strong class="source-inline">train_embeddings</strong> feature space and the <strong class="source-inline">train_labels</strong> label assignment—an ML algorithm to solve the label prediction problem, as follows:</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="source-code">rf = RandomForestClassifier(n_estimators=1000)</p>
			<p class="source-code"> rf.fit(train_embeddings, labels_train);</p>
			<p>In this example, we <a id="_idIndexMarker571"/>used a simple <strong class="source-inline">RandomForestClassifier</strong> class, but every ML algorithm can be used to solve this task. We <a id="_idIndexMarker572"/>can then apply the trained model on the <strong class="source-inline">test_embeddings</strong> feature space in order to quantify the quality of the classification, as shown in the following code block:</p>
			<p class="source-code">from sklearn import metrics</p>
			<p class="source-code">y_pred = rf.predict(test_embeddings)</p>
			<p class="source-code"> print('Precision:', metrics.precision_score(labels_test, y_pred))</p>
			<p class="source-code"> print('Recall:', metrics.recall_score(labels_test, y_pred))</p>
			<p class="source-code"> print('F1-Score:', metrics.f1_score(labels_test, y_pred))</p>
			<p>As a result, we get the following output:</p>
			<p class="source-code">Precision: 0.8557114228456913</p>
			<p class="source-code">Recall: 0.8102466793168881</p>
			<p class="source-code">F1-Score: 0.8323586744639375</p>
			<p>As we already mentioned, the methods we just described are just a general schema; each piece of the pipeline—such as the train/test split, the node/edge embedding, and the ML algorithm—can be changed according to the specific problem we are facing.</p>
			<p>This method is particularly useful when dealing with link prediction in temporal graphs. In this case, information relating to an edge obtained at timepoint <img src="image/B16069_05_056.png" alt=""/> used to train a model can be applied in order to predict edges at timepoint <img src="image/B16069_05_057.png" alt=""/>. </p>
			<p>In this section, we <a id="_idIndexMarker573"/>introduced the label prediction problem. We enriched our explanation by providing a description, with several examples, of different techniques used to find a solution to the link prediction problem. We showed that different ways to tackle the problem are available, from <a id="_idIndexMarker574"/>simple index-based techniques to more complex embedding-based techniques. However, the scientific literature is full of algorithms to solve the link prediction task, and there are different algorithms to solve this problem. In the paper <em class="italic">Review on Learning and Extracting Graph Features for Link Prediction</em> (<a href="https://arxiv.org/pdf/1901.03425.pdf">https://arxiv.org/pdf/1901.03425.pdf</a>), a good overview of different techniques used to solve the link prediction problem is available. In the next section, we will investigate the community detection problem.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor088"/>Detecting meaningful structures such as communities </h1>
			<p>One common problem <a id="_idIndexMarker575"/>data scientists face when dealing with networks is how to identify clusters and communities within a graph. This often arises when graphs are derived from social networks and communities are known to exist. However, the underlying algorithms and methods can also be used in other contexts, representing another option to perform clustering and segmentation. For example, these methods can effectively be used in text mining to identify emerging topics and to cluster documents that refer to single events/topics. A community detection task consists of partitioning a graph such that nodes belonging to the same community are tightly <a id="_idIndexMarker576"/>connected with each other and are weakly connected with nodes from other communities. There exist several strategies to identify communities. In general, we can define them as belonging to one of two categories, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Non-overlapping</strong> community detection algorithms <a id="_idIndexMarker577"/>that provide a one-to-one association between nodes and communities, thus with no overlapping nodes between communities</li>
				<li><strong class="bold">Overlapping</strong> community detection <a id="_idIndexMarker578"/>algorithms that allow a node to be included in more than one community—for instance, reflecting the natural tendencies of social networks to develop overlapping communities (for example, friends from school, neighbors, playmates, people being in the same football team, and so on), or in biology, where a single protein can be involved in more than one process and bioreaction </li>
			</ul>
			<p>In the following section, we will review some of the most used techniques in the context of community detection.  </p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor089"/>Embedding-based community detection </h2>
			<p>One first class of methods that <a id="_idIndexMarker579"/>allow us to partition nodes into communities can be simply obtained by applying standard shallow clustering techniques on the node embeddings, computed using the <a id="_idIndexMarker580"/>methods described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>. The embedding methods in fact allow us to project nodes into a vector space where a distance measure that represents a similarity between nodes can be defined. As we have shown in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, embedding algorithms are very effective in separating nodes with similar neighborhood and/or connectivity properties. Then, standard clustering techniques can be used, such as distance-based clustering (K-means), connectivity clustering (hierarchical clustering), distribution clustering (Gaussian mixture), and <a id="_idIndexMarker581"/>density-based clustering (<strong class="bold">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="bold">DBSCAN</strong>)). Depending on the algorithm, these techniques may both provide a single-association community detection or a soft cluster assignment. We will showcase how they would work on a simple barbell graph. We start by creating a simple barbell graph using the <strong class="source-inline">networkx</strong> utility function, as follows:</p>
			<p class="source-code">import networkx as nx </p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4) </p>
			<p>We can then first get the <a id="_idIndexMarker582"/>reduced dense node representation using one of the embedding algorithms we have seen previously (for instance, <strong class="source-inline">HOPE</strong>), shown as follows:</p>
			<p class="source-code">from gem.embedding.hope import HOPE </p>
			<p class="source-code">gf = HOPE(d=4, beta=0.01) </p>
			<p class="source-code">gf.learn_embedding(G) </p>
			<p class="source-code"> embeddings = gf.get_embedding() </p>
			<p>We can finally run a <a id="_idIndexMarker583"/>clustering algorithm on the resulting vector representation provided by the node embeddings, like this:</p>
			<p class="source-code">from sklearn.mixture import GaussianMixture</p>
			<p class="source-code">gm = GaussianMixture(n_components=3, random_state=0)</p>
			<p class="source-code"> labels = gm.fit_predict(embeddings)</p>
			<p>We can plot the network with the computed communities highlighted in different colors, like this:</p>
			<p class="source-code">colors = ["blue", "green", "red"]</p>
			<p class="source-code">nx.draw_spring(G, node_color=[colors[label] for label in labels])</p>
			<p>By doing so, you should obtain the output shown in the following screenshot: </p>
			<div>
				<div id="_idContainer437" class="IMG---Figure">
					<img src="image/B16069_05_02.jpg" alt="Figure 5.2 – Barbell graph where the community detection algorithm has been applied using embedding-based methods "/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Barbell graph where the community detection algorithm has been applied using embedding-based methods </p>
			<p>The two clusters, as well as the <a id="_idIndexMarker584"/>connecting nodes, have <a id="_idIndexMarker585"/>been correctly grouped into three different communities, reflecting the internal structure of the graph. </p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor090"/>Spectral methods and matrix factorization</h2>
			<p>Another way to achieve a <a id="_idIndexMarker586"/>graph partition is to process the adjacency matrix or the Laplacian matrix that represents the connectivity <a id="_idIndexMarker587"/>properties of the graph. For instance, spectral clustering can be obtained by applying standard clustering algorithms on the eigenvectors of the Laplacian matrix. In some sense, spectral <a id="_idIndexMarker588"/>clustering can also be seen as a special case of an embedding-based community detection algorithm where the embedding technique is so-called spectral embedding, obtained <a id="_idIndexMarker589"/>by considering the first k-eigenvectors of the Laplacian matrix. By considering different definitions of the Laplacian as well as different similarity matrices, variations to this method can be obtained. A convenient implementation of this method can be found within the <strong class="source-inline">communities</strong> Python library and can be used on the adjacency matrix representation easily obtained from a <strong class="source-inline">networkx</strong> graph, as illustrated in the following code snippet: </p>
			<p class="source-code">from communities.algorithms import spectral_clustering</p>
			<p class="source-code">adj=np.array(nx.adjacency_matrix(G).todense())</p>
			<p class="source-code">communities = spectral_clustering(adj, k=2)</p>
			<p>Moreover, the adjacency matrix (or the Laplacian) can also be decomposed using matrix factorization <a id="_idIndexMarker590"/>techniques other than the <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>) technique—such as <strong class="bold">non-negative matrix factorization</strong> (<strong class="bold">NMF</strong>)—that allow <a id="_idIndexMarker591"/>similar descriptions, as illustrated in the following code snippet:</p>
			<p class="source-code">from sklearn.decomposition import NMF</p>
			<p class="source-code">nmf = NMF(n_components=2)</p>
			<p class="source-code"> score = nmf.fit_transform(adj)</p>
			<p class="source-code">communities = [set(np.where(score [:,ith]&gt;0)[0])</p>
			<p class="source-code">               for ith in range(2)]</p>
			<p>The threshold for <a id="_idIndexMarker592"/>belonging to the community was set in this <a id="_idIndexMarker593"/>example to 0, although other values can also be used to retain only the community cores. Note that these methods are <a id="_idIndexMarker594"/>overlapping community detection algorithms, and nodes <a id="_idIndexMarker595"/>might belong to more than one community. </p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor091"/>Probability models</h2>
			<p>Community detection <a id="_idIndexMarker596"/>methods can also be derived from fitting the parameters of generative probabilistic graph models. Examples of generative models <a id="_idIndexMarker597"/>were already described in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs in Python</em>. However, they did not assume the presence of any underlying community, unlike <a id="_idIndexMarker598"/>the so-called <strong class="bold">stochastic block model</strong> (<strong class="bold">SBM</strong>). In fact, this model is based on the assumption that nodes can be partitioned into <em class="italic">K</em> disjoint communities and each community has a defined probability of being connected to another. For a network of <em class="italic">n</em> nodes and <em class="italic">K</em> communities, the generative model is thus parametrized by the following:</p>
			<ul>
				<li><strong class="bold">Membership matrix</strong>: <em class="italic">M</em>, which is a <em class="italic">n x K</em> matrix and represents <a id="_idIndexMarker599"/>the probability a given node belongs to a certain class <em class="italic">k</em> </li>
				<li><strong class="bold">Probability matrix</strong>: <em class="italic">B</em>, which is <em class="italic">K x K</em> matrix and represents the <a id="_idIndexMarker600"/>edge probability between a node belonging to community <em class="italic">i</em> and one node belonging to community <em class="italic">j</em></li>
			</ul>
			<p>The adjacency matrix is then generated by the following formula:</p>
			<div>
				<div id="_idContainer438" class="IMG---Figure">
					<img src="image/B16069_05_058.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_05_059.png" alt=""/> and <img src="image/B16069_05_060.png" alt=""/> represent the community, and they can be obtained by sampling from a multinomial distribution of probabilities <img src="image/B16069_05_061.png" alt=""/> and <img src="image/B16069_05_062.png" alt=""/>.</p>
			<p>In the SBM, we can basically invert the formulation and reduce the community detection problem to posterior estimation of the membership matrix <em class="italic">M</em> from the matrix <em class="italic">A</em>, via maximum likelihood estimation. A version of this approach has recently been used together with <a id="_idIndexMarker601"/>randomized spectral clustering in order to perform community detection in very large graphs. Note that the SBM model in the <a id="_idIndexMarker602"/>limit of the constant probability matrix (that is, <img src="image/B16069_05_063.png" alt=""/>) corresponds to the Erdős-Rényi model. These models have the advantage of also describing a relation between communities, identifying community-community relationships.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor092"/>Cost function minimization</h2>
			<p>Another possible way to <a id="_idIndexMarker603"/>detect communities within a graph is to optimize a given cost function that represents a graph structure and penalizes edges <a id="_idIndexMarker604"/>across communities versus edges within communities. This basically consists of building a measure for the quality of a community (as we will see shortly, its modularity) and then optimizing the node association to communities in order to maximize the overall quality of the partitioning. </p>
			<p>In the context of a binary associative community structure, the community association can be described by a dichotomic variable <img src="image/B16069_05_064.png" alt=""/> with values -1 or 1, depending on whether the node belongs to one of the two communities. In this setting, we can define the following quantity that can indeed be used to effectively represent the cost associated with having a link between two nodes of different communities: </p>
			<div>
				<div id="_idContainer445" class="IMG---Figure">
					<img src="image/B16069_05_065.jpg" alt=""/>
				</div>
			</div>
			<p>Indeed, when two connected nodes, <img src="image/B16069_05_066.png" alt=""/> belong to a different community <img src="image/B16069_05_067.png" alt=""/>, the contribution provided by the edge is positive. On the other hand, the contribution is 0, both when two nodes are not connected (<img src="image/B16069_05_068.png" alt=""/>) and when two connected nodes belong to the same community (<img src="image/B16069_05_069.png" alt=""/>). Therefore, the problem is to find the best community assignment (<img src="image/B16069_05_070.png" alt=""/> and <img src="image/B16069_05_071.png" alt=""/>) in order to minimize the preceding function. This method, however, applies only to binary community detection and is therefore rather limited in its application. </p>
			<p>Another very popular <a id="_idIndexMarker605"/>algorithm belonging to this class is the Louvain method, which takes its name from the university where it was invented. This <a id="_idIndexMarker606"/>algorithm aims to maximize the modularity, defined as follows:</p>
			<div>
				<div id="_idContainer452" class="IMG---Figure">
					<img src="image/B16069_05_072.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_05_073.png" alt=""/> represents the number of edges, <img src="image/B16069_05_074.png" alt=""/> and <img src="image/B16069_05_075.png" alt=""/> represent the degree of the i-th and j-th node respectively, and <img src="image/B16069_05_076.png" alt=""/> is the Kronecker delta function, which is 1 when <img src="image/B16069_05_077.png" alt=""/> and <img src="image/B16069_05_078.png" alt=""/> have the same value and 0 otherwise. The modularity basically represents a measure of how much better the community identification performs as compared to randomly rewiring the nodes and thus creating a random network that has the same number of edges and degree distribution. </p>
			<p>To maximize this modularity efficiently, the Louvain methods iteratively compute the following steps:</p>
			<ol>
				<li value="1"><strong class="bold">Modularity optimization</strong>: Nodes are <a id="_idIndexMarker607"/>swept iteratively, and for each node we compute the change of modularity <em class="italic">Q</em> there would be if the node were to be assigned to each community of its neighbors. Once all the <img src="image/B16069_05_079.png" alt=""/> values are computed, the node is assigned to the community that provides the largest increase. If there is no increase obtained by placing the node in any other community than the one it is in, the node remains in its original community. This optimization process continues until no changes are induced. </li>
				<li><strong class="bold">Node aggregation</strong>: In the <a id="_idIndexMarker608"/>second step, we build a new network by grouping all the nodes in the same community and connecting the communities using edges that result from the sum of all edges across the two communities. Edges within communities are accounted for as well by means of self-loops that have weights resulting from the sum of all edge weights belonging to the community. </li>
			</ol>
			<p>A Louvain implementation can already be found in the <strong class="source-inline">communities</strong> library, as can be seen in the following code snippet: </p>
			<p class="source-code">from communities.algorithms import louvain_method</p>
			<p class="source-code">communities = louvain_method(adj) </p>
			<p>Another method to <a id="_idIndexMarker609"/>maximize the modularity is the Girvan-Newman algorithm, which is based on iteratively removing edges that have the <a id="_idIndexMarker610"/>highest betweenness centrality (and thus connect two separate clusters of nodes) to create connected component communities. Here is the code related to this:</p>
			<p class="source-code">from communities.algorithms import girvan_newman</p>
			<p class="source-code">communities = girvan_newman(adj, n=2)</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The latter algorithm needs to compute the betweenness centrality of all edges to remove the edges. Such computations may be very expensive in large graphs. The Girvan-Newman algorithm in fact scales as <img src="image/B16069_05_080.png" alt=""/>, where <img src="image/B16069_05_081.png" alt=""/> is the number of edges and <img src="image/B16069_05_082.png" alt=""/> is the number of nodes, and should not be used when dealing with large datasets. </p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor093"/>Detecting graph similarities and graph matching</h1>
			<p>Learning a <a id="_idIndexMarker611"/>quantitative measure of the <em class="italic">similarity</em> among graphs is considered a key problem. Indeed, it is a critical step for network analysis and can also <a id="_idIndexMarker612"/>facilitate many ML problems, such as classification, clustering, and ranking. Many clustering algorithms, for example, use the concept of similarity for determining if an object should or should not be a member of a group. </p>
			<p>In the graph domain, finding an effective similarity measure constitutes a crucial problem for many applications. Consider, for instance, the <em class="italic">role</em> of a node inside a graph. This node might be very important for spreading information across a network or guaranteeing network robustness: for example, it could be the center of a star graph or it could be a member of a clique. In this scenario, it would be very useful to have a powerful method for comparing nodes according to their roles. For example, you might be interested in searching for individuals showing similar roles or presenting similar unusual and anomalous behaviors. You might also use it for searching similar subgraphs or to determine network compatibility for <em class="italic">knowledge transfer</em>. For example, if you find a method for increasing the robustness of a network and you know that such a network is very similar to another one, you may apply the same solution that worked well for the first network directly to the second one:</p>
			<div>
				<div id="_idContainer463" class="IMG---Figure">
					<img src="image/B16069_05_03.jpg" alt="Figure 5.3 – Example of differences between two graphs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Example of differences between two graphs</p>
			<p>Several metrics can be used for measuring the similarity (distance) between two objects. Some examples include the <em class="italic">Euclidean distance</em>, <em class="italic">Manhattan distance</em>, <em class="italic">cosine similarity</em>, and so on. However, these metrics might fail to capture the specific characteristics of the data being studied, especially on non-Euclidean structures such as graphs. Take a look at <em class="italic">Figure 5.3</em>: how "distant" are <strong class="bold">G1</strong> and <strong class="bold">G2</strong>? They look pretty similar. But what if the missing connection in the red community of <strong class="bold">G2</strong> causes a severe loss of information? Do they still look similar?</p>
			<p>Several algorithmic <a id="_idIndexMarker613"/>approaches and heuristics have been proposed, based on mathematical concepts such as <em class="italic">graph isomorphisms</em>, <em class="italic">edit distance</em>, and <em class="italic">common subgraphs</em> (we suggest reading <a href="https://link.springer.com/article/10.1007/s10044-012-0284-8">https://link.springer.com/article/10.1007/s10044-012-0284-8</a> for a detailed review). Many of these <a id="_idIndexMarker614"/>approaches are currently used in practical applications, even if they often require exponentially high computational time to provide a <a id="_idIndexMarker615"/>solution to <strong class="bold">NP-complete</strong> problems in general (where <strong class="bold">NP</strong> stands for <strong class="bold">nondeterministic polynomial time</strong>). Therefore, it is essential to find or learn a metric for measuring the similarity of data points involved in the specific task. Here is where ML comes to our aid.</p>
			<p>Many algorithms among the ones we have already seen in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning,</em> and <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em> might be useful for learning an effective similarity metric. According to the way they are used, a precise taxonomy can be defined. Here, we provide a simple overview of graph similarity techniques. A more comprehensive list can be found in the paper <em class="italic">Deep Graph Similarity Learning: A Survey</em> (<a href="https://arxiv.org/pdf/1912.11615.pdf">https://arxiv.org/pdf/1912.11615.pdf</a>). They can be essentially divided into three main categories, even if sophisticated combinations can also be developed. <strong class="bold">Graph embedding-based methods</strong> use embedding techniques to obtain an embedded representation of the graphs and exploit such a representation to learn the <a id="_idIndexMarker616"/>similarity function; <strong class="bold">graph kernel-based methods</strong> define the similarity between <a id="_idIndexMarker617"/>graphs by measuring the similarity of their constituting substructures; <strong class="bold">graph neural network-based methods</strong> use <strong class="bold">graph neural networks</strong> (<strong class="bold">GNNs</strong>) to jointly learn an embedded representation and a similarity function. Let's see all of them in more detail.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor094"/>Graph embedding-based methods</h2>
			<p>Such techniques seek to apply <a id="_idIndexMarker618"/>graph embedding techniques to obtain node-level or graph-level representations and further use the representations for similarity learning. For example, <em class="italic">DeepWalk</em> and <em class="italic">Node2Vec</em> can be used to extract meaningful embedding that can then be used to define a similarity function or to predict similarity scores. For example, in Tixier et al. (2015), <strong class="source-inline">node2vec</strong> was used for encoding node embeddings. Then, <strong class="bold">two-dimensional</strong> (<strong class="bold">2D</strong>) histograms obtained from those node embeddings were passed to a <a id="_idIndexMarker619"/>classical 2D <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) architecture designed for images. Such a simple yet powerful approach enabled good results to be derived from many benchmark datasets.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor095"/>Graph kernel-based methods</h2>
			<p><strong class="bold">Graph kernel-based methods</strong> have generated a <a id="_idIndexMarker620"/>lot of interest in terms of capturing the similarity between graphs. These approaches compute the similarity between two graphs as a function of the similarities between some of their substructures. Different graph kernels exist based on the substructures they use, which include random walks, shortest paths, and subgraphs. As an example, a method <a id="_idIndexMarker621"/>called <strong class="bold">Deep Graph Kernels</strong> (<strong class="bold">DGK</strong>) (Yanardag et al., 2015) decomposes graphs into substructures that are <a id="_idIndexMarker622"/>viewed as "words". Then, <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) approaches <a id="_idIndexMarker623"/>such as <strong class="bold">continuous bag of words</strong> (<strong class="bold">CBOW</strong>) and <strong class="bold">skip-gram</strong> are <a id="_idIndexMarker624"/>used to learn latent representations of the substructures. This way, the kernel between two graphs is defined based on the similarity of the substructure space.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor096"/>GNN-based methods</h2>
			<p>With the <a id="_idIndexMarker625"/>emergence of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) techniques, GNNs have become a powerful new tool for learning representations on graphs. Such powerful models can be easily adapted to various tasks, including <a id="_idIndexMarker626"/>graph similarity learning. Furthermore, they present a key advantage with respect to other traditional graph embedding approaches. Indeed, while the latter generally learn the representation in an isolated stage, in this kind of approach, the representation learning and the target learning task are conducted jointly. Therefore, the GNN deep models can better leverage the graph features for the specific learning task. We have already seen an example of similarity learning using GNNs in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, where a two-branch network was trained to estimate the proximity distance between two graphs.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor097"/>Applications</h2>
			<p>Similarity learning on graphs has <a id="_idIndexMarker627"/>already achieved promising results in many domains. Important applications may be found in chemistry and bioinformatics—for example, for finding the chemical compounds that are most similar to a query compound, as illustrated on the left-hand side of the following diagram. In neuroscience, similarity learning methods have started to be applied to measure the similarity of brain networks among multiple subjects, allowing the novel clinical investigation of brain diseases:</p>
			<div>
				<div id="_idContainer464" class="IMG---Figure">
					<img src="image/B16069_05_04.jpg" alt="Figure 5.4 – Example of how graphs can be useful for representing various objects: (a) differences between two chemical compounds; (b) differences between two human poses"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Example of how graphs can be useful for representing various objects: (a) differences between two chemical compounds; (b) differences between two human poses</p>
			<p>Graph similarity learning has also been explored in computer security, where novel approaches have been proposed for the detection of vulnerabilities in software systems as well as hardware security <a id="_idIndexMarker628"/>problems. Recently, a trend for applying such solutions to solve computer vision problems has been observed. Once the challenging problem of converting images into graph data has been solved, interesting solutions can indeed be proposed for human action recognition in video sequences and object matching in scenes, among other areas (as shown on the right-hand side of <em class="italic">Figure 5.4</em>).</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor098"/>Summary </h1>
			<p>In this chapter, we have learned how graph-based ML techniques can be used to solve many different problems.  </p>
			<p>In particular, we have seen that the same algorithm (or a slightly modified version of it) can be adapted to solve apparently very different tasks such as link prediction, community detection, and graph similarity learning. We have also seen that each problem has its own peculiarities, which have been exploited by researchers in order to design more sophisticated solutions.</p>
			<p>In the next chapter, we will explore real-life problems that have been solved using ML.</p>
		</div>
	</body></html>