["```py\nawk -F, '/setosa/ {print \"0 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;}; /versicolor/ {print \"1 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;}; /virginica/ {print \"1 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;};' iris.csv > iris-libsvm.txt\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\nscala> val splits = data.randomSplit(Array(0.6, 0.4), seed = 123L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:26, MapPartitionsRDD[8] at randomSplit at <console>:26)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:26\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:26\nscala> val numIterations = 100\nnumIterations: Int = 100\nscala> val model = SVMWithSGD.train(training, numIterations)\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = 0.0\nscala> model.clearThreshold()\nres0: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = None\nscala> val scoreAndLabels = test.map { point =>\n |   val score = model.predict(point.features)\n |   (score, point.label)\n | }\nscoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at <console>:36\nscala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@692e4a35\nscala> val auROC = metrics.areaUnderROC()\nauROC: Double = 1.0\n\nscala> println(\"Area under ROC = \" + auROC)\nArea under ROC = 1.0\nscala> model.save(sc, \"model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\nscala> model.intercept\nres5: Double = 0.0\n\nscala> model.weights\nres6: org.apache.spark.mllib.linalg.Vector = [-0.2469448809675877,-1.0692729424287566,1.7500423423258127,0.8105712661836376]\n\n```", "```py\n$ parquet-tools dump model/data/part-r-00000-7a86b825-569d-4c80-8796-8ee6972fd3b1.gz.parquet\n…\nDOUBLE weights.values.array \n----------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:3 V:-0.2469448809675877\nvalue 2: R:1 D:3 V:-1.0692729424287566\nvalue 3: R:1 D:3 V:1.7500423423258127\nvalue 4: R:1 D:3 V:0.8105712661836376\n\nDOUBLE intercept \n----------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 1 *** \nvalue 1: R:0 D:1 V:0.0\n…\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext\nscala> import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\nimport org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\nscala> import org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\nscala> val splits = data.randomSplit(Array(0.6, 0.4))\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:29, MapPartitionsRDD[8] at randomSplit at <console>:29)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:29\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:29\nscala> val model = new LogisticRegressionWithLBFGS().setNumClasses(3).run(training)\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 8, numClasses = 3, threshold = 0.5\nscala> val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n |   val prediction = model.predict(features)\n |   (prediction, label)\n | }\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[67] at map at <console>:37\nscala> val metrics = new MulticlassMetrics(predictionAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6d5254f3\nscala> val precision = metrics.precision\nprecision: Double = 0.9516129032258065\nscala> println(\"Precision = \" + precision)\nPrecision = 0.9516129032258065\nscala> model.intercept\nres5: Double = 0.0\nscala> model.weights\nres7: org.apache.spark.mllib.linalg.Vector = [10.644978886788556,-26.850171485157578,3.852594349297618,8.74629386938248,4.288703063075211,-31.029289381858273,9.790312529377474,22.058196856491996]\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.DecisionTree\nscala> import org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> import org.apache.spark.mllib.tree.configuration.Strategy\nimport org.apache.spark.mllib.tree.configuration.Strategy\nscala> import org.apache.spark.mllib.tree.configuration.Algo.Classification\nimport org.apache.spark.mllib.tree.configuration.Algo.Classification\nscala> import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}\nimport org.apache.spark.mllib.tree.impurity.{Entropy, Gini}\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\n\nscala> val splits = data.randomSplit(Array(0.7, 0.3), 11L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:30, MapPartitionsRDD[8] at randomSplit at <console>:30)\nscala> val (trainingData, testData) = (splits(0), splits(1))\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:30\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:30\nscala> val strategy = new Strategy(Classification, Gini, 10, 3, 10)\nstrategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@4110e631\nscala> val dt = new DecisionTree(strategy)\ndt: org.apache.spark.mllib.tree.DecisionTree = org.apache.spark.mllib.tree.DecisionTree@33d89052\nscala> val model = dt.run(trainingData)\nmodel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 6 with 21 nodes\nscala> val labelAndPreds = testData.map { point =>\n |   val prediction = model.predict(point.features)\n |   (point.label, prediction)\n | }\nlabelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[32] at map at <console>:36\nscala> val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\ntestErr: Double = 0.02631578947368421\nscala> println(\"Test Error = \" + testErr)\nTest Error = 0.02631578947368421\n\nscala> println(\"Learned classification tree model:\\n\" + model.toDebugString)\nLearned classification tree model:\nDecisionTreeModel classifier of depth 6 with 21 nodes\n If (feature 3 <= 0.4)\n Predict: 0.0\n Else (feature 3 > 0.4)\n If (feature 3 <= 1.7)\n If (feature 2 <= 4.9)\n If (feature 0 <= 5.3)\n If (feature 1 <= 2.8)\n If (feature 2 <= 3.9)\n Predict: 1.0\n Else (feature 2 > 3.9)\n Predict: 2.0\n Else (feature 1 > 2.8)\n Predict: 0.0\n Else (feature 0 > 5.3)\n Predict: 1.0\n Else (feature 2 > 4.9)\n If (feature 0 <= 6.0)\n If (feature 1 <= 2.4)\n Predict: 2.0\n Else (feature 1 > 2.4)\n Predict: 1.0\n Else (feature 0 > 6.0)\n Predict: 2.0\n Else (feature 3 > 1.7)\n If (feature 2 <= 4.9)\n If (feature 1 <= 3.0)\n Predict: 2.0\n Else (feature 1 > 3.0)\n Predict: 1.0\n Else (feature 2 > 4.9)\n Predict: 2.0\nscala> model.save(sc, \"dt-model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\n$ bin/spark-shell\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> val iris = sc.textFile(\"iris.txt\")\niris: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at <console>:23\n\nscala> val vectors = data.map(s => Vectors.dense(s.split('\\t').map(_.toDouble))).cache()\nvectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[5] at map at <console>:25\n\nscala> val numClusters = 3\nnumClusters: Int = 3\nscala> val numIterations = 20\nnumIterations: Int = 20\nscala> val clusters = KMeans.train(vectors, numClusters, numIterations)\nclusters: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@5dc9cb99\nscala> val centers = clusters.clusterCenters\ncenters: Array[org.apache.spark.mllib.linalg.Vector] = Array([5.005999999999999,3.4180000000000006,1.4640000000000002,0.2439999999999999], [6.8538461538461535,3.076923076923076,5.715384615384614,2.0538461538461537], [5.883606557377049,2.740983606557377,4.388524590163936,1.4344262295081966])\nscala> val SSE = clusters.computeCost(vectors)\nWSSSE: Double = 78.94506582597859\nscala> vectors.collect.map(x => clusters.predict(x))\nres18: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2)\nscala> println(\"Sum of Squared Errors = \" + SSE)\nSum of Squared Errors = 78.94506582597859\nscala> clusters.save(sc, \"model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\nscala> 1.to(10).foreach(i => println(\"i: \" + i + \" SSE: \" + KMeans.train(vectors, i, numIterations).computeCost(vectors)))\ni: 1 WSSSE: 680.8244\ni: 2 WSSSE: 152.3687064773393\ni: 3 WSSSE: 78.94506582597859\ni: 4 WSSSE: 57.47327326549501\ni: 5 WSSSE: 46.53558205128235\ni: 6 WSSSE: 38.9647878510374\ni: 7 WSSSE: 34.311167589868646\ni: 8 WSSSE: 32.607859500805034\ni: 9 WSSSE: 28.231729411088438\ni: 10 WSSSE: 29.435054384424078\n\n```", "```py\nscala> for (i <- 1.to(10)) println(i + \" -> \" + ((KMeans.train(vectors, i, numIterations).computeCost(vectors)) + 680 * scala.math.log(i) / scala.math.log(150)))\n1 -> 680.8244\n2 -> 246.436635016484\n3 -> 228.03498068120865\n4 -> 245.48126639400738\n5 -> 264.9805962616268\n6 -> 285.48857890531764\n7 -> 301.56808340425164\n8 -> 315.321639004243\n9 -> 326.47262191671723\n10 -> 344.87130979355675\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\nscala> import org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.feature.PCA\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val pca = new PCA(2).fit(data.map(_.features))\npca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@4eee0b1a\n\nscala> val reduced = data.map(p => p.copy(features = pca.transform(p.features)))\nreduced: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[311] at map at <console>:39\nscala> reduced.collect().take(10)\nres4: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[-2.827135972679021,-5.641331045573367]), (0.0,[-2.7959524821488393,-5.145166883252959]), (0.0,[-2.621523558165053,-5.177378121203953]), (0.0,[-2.764905900474235,-5.0035994150569865]), (0.0,[-2.7827501159516546,-5.6486482943774305]), (0.0,[-3.231445736773371,-6.062506444034109]), (0.0,[-2.6904524156023393,-5.232619219784292]), (0.0,[-2.8848611044591506,-5.485129079769268]), (0.0,[-2.6233845324473357,-4.743925704477387]), (0.0,[-2.8374984110638493,-5.208032027056245]))\n\nscala> import scala.language.postfixOps\nimport scala.language.postfixOps\n\nscala> pca pc\nres24: org.apache.spark.mllib.linalg.DenseMatrix = \n-0.36158967738145065  -0.6565398832858496 \n0.08226888989221656   -0.7297123713264776 \n-0.856572105290527    0.17576740342866465 \n-0.35884392624821626  0.07470647013502865\n\nscala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nscala> val splits = reduced.randomSplit(Array(0.6, 0.4), seed = 1L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[312] at randomSplit at <console>:44, MapPartitionsRDD[313] at randomSplit at <console>:44)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[312] at randomSplit at <console>:44\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[313] at randomSplit at <console>:44\nscala> val numIterations = 100\nnumIterations: Int = 100\nscala> val model = SVMWithSGD.train(training, numIterations)\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = 0.0\nscala> model.clearThreshold()\nres30: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = None\nscala> val scoreAndLabels = test.map { point =>\n |   val score = model.predict(point.features)\n |   (score, point.label)\n | }\nscoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[517] at map at <console>:54\nscala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27f49b8c\n\nscala> val auROC = metrics.areaUnderROC()\nauROC: Double = 1.0\nscala> println(\"Area under ROC = \" + auROC)\nArea under ROC = 1.0\n\n```"]