<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Bayesian Neural Networks" id="aid-1Q5IA1"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Bayesian Neural Networks</h1></div></div></div><p>As the name suggests, artificial neural networks are statistical models built taking inspirations from the architecture and cognitive capabilities of biological brains. Neural network models typically have a layered architecture consisting of a large number of neurons in each layer, and neurons between different layers are connected. The first layer is called input layer, the last layer is called output layer, and the rest of the layers in the middle are called hidden layers. Each neuron has a state that is determined by a nonlinear function of the state of all neurons connected to it. Each connection has a weight that is determined from the training data containing a set of input and output pairs. This kind of layered architecture of neurons and their connections is present in the <a id="id362" class="indexterm"/>
<span class="strong"><strong>neocortex</strong></span> region of human brain and is considered to be responsible for higher functions such as sensory perception and language understanding.</p><p>The first computational model for neural network was proposed by Warren McCulloch and Walter Pitts in 1943. Around the same time, psychologist Donald Hebb created a hypothesis of learning based on the mechanism of excitation and adaptation of neurons that is known as <span class="strong"><strong>Hebb's rule</strong></span>. The<a id="id363" class="indexterm"/> hypothesis can be summarized by saying <span class="emphasis"><em>Neurons that fire together, wire together</em></span>. Although there were several researchers who tried to implement computational models of neural networks, it was Frank Rosenblatt in 1958 who first created an algorithm for pattern recognition using a two-layer neural network called <a id="id364" class="indexterm"/>
<span class="strong"><strong>Perceptron</strong></span>.</p><p>The research and applications of neural networks had both stagnant and great periods of progress during 1970-2010. Some of the landmarks in the history of neural networks are the invention of the <span class="strong"><strong>backpropagation</strong></span> algorithm<a id="id365" class="indexterm"/> by Paul Werbos in 1975, a fast learning algorithm for learning multilayer neural networks (also called <a id="id366" class="indexterm"/>
<span class="strong"><strong>deep learning networks</strong></span>) by Geoffrey Hinton in 2006, and the use of GPGPUs to achieve greater computational power required for processing neural networks in the latter half of the last decade. </p><p>Today, neural network models and their applications have again taken a central stage in artificial intelligence with applications in computer vision, speech recognition, and natural language understanding. This is the reason this book has devoted one chapter specifically to this subject. The importance of Bayesian inference in neural network models will become clear when we go into detail in later sections.</p><div class="section" title="Two-layer neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec55"/>Two-layer neural networks</h1></div></div></div><p>Let us look<a id="id367" class="indexterm"/> at the formal definition of a two-layer neural network. We follow the notations and description used by David MacKay (reference 1, 2, and 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). The input to the NN is given by <span class="inlinemediaobject"><img src="../Images/image00529.jpeg" alt="Two-layer neural networks"/></span>. The input values are first multiplied by a set of weights to produce a weighted linear combination and then transformed using a nonlinear function to produce values of the state of neurons in the hidden layer:</p><div class="mediaobject"><img src="../Images/image00530.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>A similar operation is done at the second layer to produce final output values <span class="inlinemediaobject"><img src="../Images/image00531.jpeg" alt="Two-layer neural networks"/></span>:</p><div class="mediaobject"><img src="../Images/image00532.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>The function <span class="inlinemediaobject"><img src="../Images/image00533.jpeg" alt="Two-layer neural networks"/></span> is usually taken as either a <a id="id368" class="indexterm"/>
<span class="strong"><strong>sigmoid</strong></span> function <span class="inlinemediaobject"><img src="../Images/image00534.jpeg" alt="Two-layer neural networks"/></span> or <span class="inlinemediaobject"><img src="../Images/image00535.jpeg" alt="Two-layer neural networks"/></span>. Another common function used for multiclass classification is <span class="strong"><strong>softmax</strong></span><a id="id369" class="indexterm"/> defined as follows:</p><div class="mediaobject"><img src="../Images/image00536.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>This is a normalized exponential function.</p><p>All these <a id="id370" class="indexterm"/>are highly nonlinear functions exhibiting the property that the output value has a sharp increase as a function of the input. This nonlinear property gives neural networks more computational flexibility than standard linear or generalized linear models. Here, <span class="inlinemediaobject"><img src="../Images/image00537.jpeg" alt="Two-layer neural networks"/></span> is called a bias parameter. The weights <span class="inlinemediaobject"><img src="../Images/image00538.jpeg" alt="Two-layer neural networks"/></span> together with biases <span class="inlinemediaobject"><img src="../Images/image00539.jpeg" alt="Two-layer neural networks"/></span> form the weight vector <span class="strong"><strong>w</strong></span> .</p><p>The schematic structure of the two-layer neural network is shown here:</p><div class="mediaobject"><img src="../Images/image00540.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>The learning <a id="id371" class="indexterm"/>in neural networks corresponds to finding the value of weight vector such as <span class="strong"><strong>w</strong></span>, such that for a given dataset consisting of ground truth values input and target (output), <span class="inlinemediaobject"><img src="../Images/image00541.jpeg" alt="Two-layer neural networks"/></span>, the error of prediction of target values by the network is minimum. For regression problems, this is achieved by minimizing the error function:</p><div class="mediaobject"><img src="../Images/image00542.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>For the classification task, in neural network training, instead of squared error one uses a cross entropy defined as follows:</p><div class="mediaobject"><img src="../Images/image00543.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>To avoid overfitting, a regularization term is usually also included in the objective function. The<a id="id372" class="indexterm"/> form of the regularization function is usually <span class="inlinemediaobject"><img src="../Images/image00544.jpeg" alt="Two-layer neural networks"/></span>, which gives penalty to large values of <span class="strong"><strong>w</strong></span>, reducing the chances of overfitting. The resulting objective function is as follows:</p><div class="mediaobject"><img src="../Images/image00545.jpeg" alt="Two-layer neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00546.jpeg" alt="Two-layer neural networks"/></span> and <span class="inlinemediaobject"><img src="../Images/image00547.jpeg" alt="Two-layer neural networks"/></span> are free parameters for which the optimum values can be found from cross-validation experiments.</p><p>To minimize <span class="emphasis"><em>M(</em></span><span class="strong"><strong>w</strong></span><span class="emphasis"><em>)</em></span> with respect to <span class="strong"><strong>w</strong></span>, one uses the backpropagation algorithm as described in the classic paper by Rumelhart, Hinton, and Williams (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). In the backpropagation for each input/output pair, the value of the predicted output is computed using a forward pass from the input layer. The error, or the difference between the predicted output and actual output, is propagated back and at each node, the weights are readjusted so that the error is a minimum.</p></div></div>
<div class="section" title="Bayesian treatment of neural networks" id="aid-1R42S1"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Bayesian treatment of neural networks</h1></div></div></div><p>To set the <a id="id373" class="indexterm"/>neural network learning in a Bayesian context, consider the error function <span class="inlinemediaobject"><img src="../Images/image00548.jpeg" alt="Bayesian treatment of neural networks"/></span> for the regression case. It can be treated as a Gaussian noise term for observing the given dataset conditioned on the weights <span class="strong"><strong>w</strong></span>. This is precisely the likelihood function that can be written as follows:</p><div class="mediaobject"><img src="../Images/image00549.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00547.jpeg" alt="Bayesian treatment of neural networks"/></span> is the variance of the noise term given by <span class="inlinemediaobject"><img src="../Images/image00550.jpeg" alt="Bayesian treatment of neural networks"/></span> and <span class="inlinemediaobject"><img src="../Images/image00551.jpeg" alt="Bayesian treatment of neural networks"/></span>represents a probabilistic model. The regularization term can be considered as the log of the prior probability distribution over the parameters:</p><div class="mediaobject"><img src="../Images/image00552.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00553.jpeg" alt="Bayesian treatment of neural networks"/></span> is the variance of the prior distribution of weights. It can be easily shown using Bayes' theorem that the objective function <span class="emphasis"><em>M(</em></span><span class="strong"><strong>w</strong></span><span class="emphasis"><em>)</em></span> then corresponds to the posterior distribution of parameters <span class="strong"><strong>w</strong></span>:</p><div class="mediaobject"><img src="../Images/image00554.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>In the neural network case, we are interested in the local maxima of <span class="inlinemediaobject"><img src="../Images/image00555.jpeg" alt="Bayesian treatment of neural networks"/></span>. The posterior is then approximated as a Gaussian around each maxima <span class="inlinemediaobject"><img src="../Images/image00556.jpeg" alt="Bayesian treatment of neural networks"/></span>, as follows:</p><div class="mediaobject"><img src="../Images/image00557.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>A</em></span> is <a id="id374" class="indexterm"/>a matrix of the second derivative of <span class="emphasis"><em>M(</em></span><span class="strong"><strong>w</strong></span><span class="emphasis"><em>)</em></span> with respect to <span class="strong"><strong>w</strong></span> and represents an inverse of the covariance matrix. It is also known by the name <span class="strong"><strong>Hessian</strong></span> matrix.</p><p>The value of hyper parameters <span class="inlinemediaobject"><img src="../Images/image00546.jpeg" alt="Bayesian treatment of neural networks"/></span> and <span class="inlinemediaobject"><img src="../Images/image00547.jpeg" alt="Bayesian treatment of neural networks"/></span> is found using the <span class="strong"><strong>evidence framework</strong></span>. In this, the probability <span class="inlinemediaobject"><img src="../Images/image00558.jpeg" alt="Bayesian treatment of neural networks"/></span> is used as a evidence to find the best values of <span class="inlinemediaobject"><img src="../Images/image00546.jpeg" alt="Bayesian treatment of neural networks"/></span> and <span class="inlinemediaobject"><img src="../Images/image00547.jpeg" alt="Bayesian treatment of neural networks"/></span> from data <span class="emphasis"><em>D</em></span>. This is done through the following Bayesian rule:</p><div class="mediaobject"><img src="../Images/image00559.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>By using the evidence framework and Gaussian approximation of posterior (references 2 and 5 in the <span class="emphasis"><em>References</em></span> section of this chapter), one can show that the best value of <span class="inlinemediaobject"><img src="../Images/image00560.jpeg" alt="Bayesian treatment of neural networks"/></span> satisfies the following:</p><div class="mediaobject"><img src="../Images/image00561.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>Also, the best value of <span class="inlinemediaobject"><img src="../Images/image00562.jpeg" alt="Bayesian treatment of neural networks"/></span> satisfies the following:</p><div class="mediaobject"><img src="../Images/image00563.jpeg" alt="Bayesian treatment of neural networks"/></div><p style="clear:both; height: 1em;"> </p><p>In these <a id="id375" class="indexterm"/>equations, <span class="inlinemediaobject"><img src="../Images/image00564.jpeg" alt="Bayesian treatment of neural networks"/></span> is the number of well-determined parameters given by <span class="inlinemediaobject"><img src="../Images/image00565.jpeg" alt="Bayesian treatment of neural networks"/></span> where <span class="emphasis"><em>k</em></span> is the length of <span class="strong"><strong>w</strong></span>.</p></div>
<div class="section" title="The brnn R package" id="aid-1S2JE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>The brnn R package</h1></div></div></div><p>The <a id="id376" class="indexterm"/>
<span class="strong"><strong>brnn</strong></span> package was developed by Paulino Perez Rodriguez and Daniel Gianola, and it implements the two-layer Bayesian regularized neural network described in the previous section. The main function in the package is <code class="literal">brnn( )</code> that can be called using the following command:</p><div class="informalexample"><pre class="programlisting">&gt;brnn(x,y,neurons,normalize,epochs,…,Monte_Carlo,…)</pre></div><p>Here, <span class="emphasis"><em>x</em></span> is an <span class="emphasis"><em>n x p</em></span> matrix where <span class="emphasis"><em>n</em></span> is the number of data points and <span class="emphasis"><em>p</em></span> is the number of variables; <span class="emphasis"><em>y</em></span> is an <span class="emphasis"><em>n</em></span> dimensional vector containing target values. The number of neurons in the hidden layer of the network can be specified by the variable <code class="literal">neurons</code>. If the indicator function <code class="literal">normalize</code> is <code class="literal">TRUE</code>, it will normalize the input and output, which is the default option. The maximum number of iterations during model training is specified using <code class="literal">epochs</code>. If the indicator binary variable <code class="literal">Monte_Carlo</code> is true, then an MCMC method is used to estimate the trace of the inverse of the Hessian matrix A.</p><p>Let us try an example with the Auto MPG dataset that we used in <a class="link" title="Chapter 5. Bayesian Regression Models" href="part0041.xhtml#aid-173721">Chapter 5</a>, <span class="emphasis"><em>Bayesian Regression Models</em></span>. The<a id="id377" class="indexterm"/> following R code will import data, create training and test sets, train a neural network model using training data, and make predictions for the test set:</p><div class="informalexample"><pre class="programlisting">&gt;install.packages("brnn")  #one time installation
&gt;library(brnn)
&gt;mpgdataall &lt;- read.csv("C:/…/auto-mpg.csv")#give the correct full path
&gt;mpgdata &lt;- mpgdataall[,c(1,3,5,6)]
&gt;#Fitting Bayesian NN Model
&gt;ytrain &lt;- mpgdata[1:100,1]
&gt;xtrain &lt;- as.matrix(mpgdata[1:100,2:4])
&gt;mpg_brnn &lt;- brnn(xtrain,ytrain,neurons=2,normalize = TRUE,epochs = 1000,Monte_Carlo = TRUE)
&gt;summary(mpg_brnn)
A Bayesian regularized neural network
3 - 2 - 1 with 10 weights,biases and connection strengths
Inputs and output were normalized
Training finished because Changes in F= beta*SCE + alpha*Ew in last 3 iterations less than 0.001

&gt;#Prediction using trained model
&gt;ytest &lt;- mpgdata[101:150,1]
&gt;xtest &lt;- as.matrix(mpgdata[101:150,2:4])
&gt;ypred_brnn &lt;- predict.brnn(mpg_brnn,xtest)
&gt;plot(ytest,ypred_brnn)
&gt;err &lt;-ytest-ypred
&gt;summary(err)</pre></div></div>
<div class="section" title="Deep belief networks and deep learning"><div class="titlepage" id="aid-1T1402"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Deep belief networks and deep learning</h1></div></div></div><p>Some of the <a id="id378" class="indexterm"/>pioneering advancements in neural networks research in the last decade have opened up a new frontier in machine learning that is generally called by the name <span class="strong"><strong>deep learning</strong></span><a id="id379" class="indexterm"/> (references 5 and 7 in the <span class="emphasis"><em>References</em></span> section of this chapter). The general definition of deep learning is, <span class="emphasis"><em>a class of machine learning techniques, where many layers of information processing stages in hierarchical supervised architectures are exploited for unsupervised feature learning and for pattern analysis/classification. The essence of deep learning is to compute hierarchical features or representations of the observational data, where the higher-level features or factors are defined from lower-level ones</em></span> (reference 8 in the <span class="emphasis"><em>References</em></span> section of this chapter). Although there are many similar definitions and architectures for deep learning, two common elements in all of them are: <span class="emphasis"><em>multiple layers of nonlinear information processing</em></span> and <span class="emphasis"><em>supervised or unsupervised learning of feature representations at each layer from the features learned at the previous layer</em></span>. The initial works on deep learning were based on multilayer neural network models. Recently, many other forms of models have also been used, such as deep kernel machines and deep Q-networks.</p><p>Even in previous decades, researchers <a id="id380" class="indexterm"/>have experimented with multilayer neural networks. However, two reasons limited any progress with learning using such architectures. The first reason is that the learning of the network parameters is a non-convex optimization problem. Starting from random initial conditions, one gets stuck at local minima during minimization of error. The second reason is that the associated computational requirements were huge. A breakthrough for the first problem came when Geoffrey Hinton developed a fast algorithm for learning a special class of neural networks called <a id="id381" class="indexterm"/>
<span class="strong"><strong>deep belief nets</strong></span> (<span class="strong"><strong>DBN</strong></span>). We will describe DBNs in more detail in later sections. The high computational power requirements were met with the advancement in computing using <a id="id382" class="indexterm"/>
<span class="strong"><strong>general purpose graphical processing units</strong></span> (<span class="strong"><strong>GPGPUs</strong></span>). What made deep learning so popular for practical applications is the significant improvement in accuracy achieved in automatic speech recognition and computer vision. For example, the <span class="strong"><strong>word error rate</strong></span><a id="id383" class="indexterm"/> in automatic speech recognition of a switchboard conversational speech had reached a saturation of around 40% after years of research. </p><p>However, using deep learning, the word error rate reduced dramatically to close to 10% in a matter of a few years. Another well-known example is how <span class="strong"><strong>deep convolution neural network</strong></span> achieved the least error rate of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge compared to state-of-the-art methods that gave 26.2% as the least error rate (reference 7 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>In this chapter, we will describe one class of deep learning models called deep belief networks. Interested readers may wish to read the book by Li Deng and Dong Yu (reference 9 in the <span class="emphasis"><em>References</em></span> section of this chapter) for a detailed understanding of various methods and applications of deep learning. We will follow their notations in the rest of the chapter. We will also illustrate the use of DBN with the R package <span class="strong"><strong>darch</strong></span>.</p><div class="section" title="Restricted Boltzmann machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec45"/>Restricted Boltzmann machines</h2></div></div></div><p>A <a id="id384" class="indexterm"/>
<span class="strong"><strong>restricted Boltzmann machine</strong></span> (<span class="strong"><strong>RBM</strong></span>) is a <a id="id385" class="indexterm"/>two-layer network (bi-partite graph), in which one layer is a visible layer (<span class="emphasis"><em>v</em></span>) and the second layer is a hidden layer (<span class="emphasis"><em>h</em></span>). All nodes in the visible layer and all nodes in the hidden layer are connected by undirected edges, and there no connections between nodes in the same layer:</p><div class="mediaobject"><img src="../Images/image00566.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>An RBM is<a id="id386" class="indexterm"/> characterized by the joint distribution of states of all visible units <span class="inlinemediaobject"><img src="../Images/image00567.jpeg" alt="Restricted Boltzmann machines"/></span> and states of all hidden units <span class="inlinemediaobject"><img src="../Images/image00568.jpeg" alt="Restricted Boltzmann machines"/></span> given by:</p><div class="mediaobject"><img src="../Images/image00569.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00570.jpeg" alt="Restricted Boltzmann machines"/></span> is called the <a id="id387" class="indexterm"/>
<span class="strong"><strong>energy function</strong></span> and <span class="inlinemediaobject"><img src="../Images/image00571.jpeg" alt="Restricted Boltzmann machines"/></span> is the normalization constant known by the name <span class="strong"><strong>partition function</strong></span><a id="id388" class="indexterm"/> from Statistical Physics nomenclature.</p><p>There are mainly two types of RBMs. In the first one, both <span class="emphasis"><em>v</em></span> and <span class="emphasis"><em>h</em></span> are Bernoulli random variables. In the second type, <span class="emphasis"><em>h</em></span> is a Bernoulli random variable whereas <span class="emphasis"><em>v</em></span> is a Gaussian random variable. For Bernoulli RBM, the energy function is given by:</p><div class="mediaobject"><img src="../Images/image00572.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00573.jpeg" alt="Restricted Boltzmann machines"/></span> represents<a id="id389" class="indexterm"/> the weight of the edge between nodes <span class="inlinemediaobject"><img src="../Images/image00574.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="inlinemediaobject"><img src="../Images/image00575.jpeg" alt="Restricted Boltzmann machines"/></span>; <span class="inlinemediaobject"><img src="../Images/image00576.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="inlinemediaobject"><img src="../Images/image00577.jpeg" alt="Restricted Boltzmann machines"/></span> are bias parameters for the visible and hidden layers respectively. For this energy function, the exact expressions for the conditional probability can be derived as follows:</p><div class="mediaobject"><img src="../Images/image00578.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00579.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00580.jpeg" alt="Restricted Boltzmann machines"/></span> is the logistic function <span class="inlinemediaobject"><img src="../Images/image00581.jpeg" alt="Restricted Boltzmann machines"/></span>.</p><p>If the input variables are continuous, one can use the Gaussian RBM; the energy function of it is given by:</p><div class="mediaobject"><img src="../Images/image00582.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>Also, in this <a id="id390" class="indexterm"/>case, the conditional probabilities of <span class="inlinemediaobject"><img src="../Images/image00574.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="inlinemediaobject"><img src="../Images/image00575.jpeg" alt="Restricted Boltzmann machines"/></span> will become as follows:</p><div class="mediaobject"><img src="../Images/image00578.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00583.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>This is a normal distribution with mean <span class="inlinemediaobject"><img src="../Images/image00584.jpeg" alt="Restricted Boltzmann machines"/></span> and variance 1.</p><p>Now that we have described the basic architecture of an RBM, how is it that it is trained? If we try to use the standard approach of taking the gradient of log-likelihood, we get the following update rule:</p><div class="mediaobject"><img src="../Images/image00585.jpeg" alt="Restricted Boltzmann machines"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00586.jpeg" alt="Restricted Boltzmann machines"/></span> is the expectation of <span class="inlinemediaobject"><img src="../Images/image00587.jpeg" alt="Restricted Boltzmann machines"/></span> computed using the dataset and <span class="inlinemediaobject"><img src="../Images/image00588.jpeg" alt="Restricted Boltzmann machines"/></span> is the same expectation computed using the model. However, one cannot use this exact expression for updating weights because <span class="inlinemediaobject"><img src="../Images/image00588.jpeg" alt="Restricted Boltzmann machines"/></span> is difficult to compute.</p><p>The first <a id="id391" class="indexterm"/>breakthrough came to solve this problem and, hence, to train deep neural networks, when Hinton and team proposed an algorithm called <a id="id392" class="indexterm"/>
<span class="strong"><strong>Contrastive Divergence</strong></span> (<span class="strong"><strong>CD</strong></span>) (reference 7 in the <span class="emphasis"><em>References</em></span> section of this chapter). The essence of the algorithm is described in the next paragraph.</p><p>The idea is to approximate <span class="inlinemediaobject"><img src="../Images/image00588.jpeg" alt="Restricted Boltzmann machines"/></span> by using values of <span class="inlinemediaobject"><img src="../Images/image00574.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="inlinemediaobject"><img src="../Images/image00575.jpeg" alt="Restricted Boltzmann machines"/></span> generated using Gibbs sampling from the conditional distributions mentioned previously. One scheme of doing this is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize <span class="inlinemediaobject"><img src="../Images/image00589.jpeg" alt="Restricted Boltzmann machines"/></span> from the dataset.
</li><li class="listitem">
Find <span class="inlinemediaobject"><img src="../Images/image00590.jpeg" alt="Restricted Boltzmann machines"/></span> by sampling from the conditional distribution <span class="inlinemediaobject"><img src="../Images/image00591.jpeg" alt="Restricted Boltzmann machines"/></span>.
</li><li class="listitem">
Find <span class="inlinemediaobject"><img src="../Images/image00592.jpeg" alt="Restricted Boltzmann machines"/></span> by sampling from the conditional distribution <span class="inlinemediaobject"><img src="../Images/image00593.jpeg" alt="Restricted Boltzmann machines"/></span>.
</li><li class="listitem">
Find <span class="inlinemediaobject"><img src="../Images/image00594.jpeg" alt="Restricted Boltzmann machines"/></span> by sampling from the conditional distribution <span class="inlinemediaobject"><img src="../Images/image00595.jpeg" alt="Restricted Boltzmann machines"/></span>.
</li></ol><div style="height:10px; width: 1px"/></div><p>Once <a id="id393" class="indexterm"/>we find the values of <span class="inlinemediaobject"><img src="../Images/image00592.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="inlinemediaobject"><img src="../Images/image00594.jpeg" alt="Restricted Boltzmann machines"/></span>, use <span class="inlinemediaobject"><img src="../Images/image00596.jpeg" alt="Restricted Boltzmann machines"/></span>, which is the product of <span class="emphasis"><em>i</em></span><sup>th</sup> component of <span class="inlinemediaobject"><img src="../Images/image00592.jpeg" alt="Restricted Boltzmann machines"/></span> and <span class="emphasis"><em>j</em></span><sup>th</sup> component of <span class="inlinemediaobject"><img src="../Images/image00594.jpeg" alt="Restricted Boltzmann machines"/></span>, as an approximation for <span class="inlinemediaobject"><img src="../Images/image00588.jpeg" alt="Restricted Boltzmann machines"/></span>. This is called <a id="id394" class="indexterm"/>
<span class="strong"><strong>CD-1 algorithm</strong></span>. One can generalize this to use the values from the <span class="emphasis"><em>k</em></span><sup>th</sup> step of Gibbs sampling and it is known as <a id="id395" class="indexterm"/>
<span class="strong"><strong>CD-k algorithm</strong></span>. One can easily see the connection between RBMs and Bayesian inference. Since the CD algorithm is like a posterior density estimate, one could say that RBMs are trained using a Bayesian inference approach.</p><p>Although the Contrastive Divergence algorithm looks simple, one needs to be very careful in training RBMs, otherwise the model can result in overfitting. Readers who are interested in using RBMs in practical applications should refer to the technical report (reference 10 in the <span class="emphasis"><em>References</em></span> section of this chapter), where this is discussed in detail.</p></div><div class="section" title="Deep belief networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec46"/>Deep belief networks</h2></div></div></div><p>One can<a id="id396" class="indexterm"/> stack several RBMs, one on top of each other, such that the values of hidden units in the layer <span class="inlinemediaobject"><img src="../Images/image00597.jpeg" alt="Deep belief networks"/></span> would become values of visible units in the <span class="emphasis"><em>n</em></span><sup>th</sup> layer <span class="inlinemediaobject"><img src="../Images/image00598.jpeg" alt="Deep belief networks"/></span>, and so on. The resulting network is called a deep belief network. It was one of the main architectures used in early deep learning networks for pretraining. The idea of pretraining a NN is the following: in the standard three-layer (input-hidden-output) NN, one can start with random initial values for the weights and using the backpropagation algorithm, can find a good minimum of the log-likelihood function. However, when the number of layers increases, the straightforward application of backpropagation does not work because starting from output layer, as we compute the gradient values for the layers deep inside, their magnitude becomes very small. This is called the <a id="id397" class="indexterm"/>
<span class="strong"><strong>gradient vanishing</strong></span> problem. As a result, the network will get trapped in some <a id="id398" class="indexterm"/>poor local minima. Backpropagation still works if we are starting from the neighborhood of a good minimum. To achieve this, a DNN is often pretrained in an unsupervised way, using a DBN. Instead of starting from random values of weights, train a DBN in an unsupervised way and use weights from the DBN as initial weights for a corresponding supervised DNN. It was seen that such DNNs pretrained using DBNs perform much better (reference 8 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>The layer-wise pretraining of a DBN proceeds as follows. Start with the first RBM and train it using input data in the visible layer and the CD algorithm (or its latest better variants). Then, stack a second RBM on top of this. For this RBM, use values sample from <span class="inlinemediaobject"><img src="../Images/image00599.jpeg" alt="Deep belief networks"/></span> as the values for the visible layer. Continue this process for the desired number of layers. The outputs of hidden units from the top layer can also be used as inputs for training a supervised model. For this, add a conventional NN layer at the top of DBN with the desired number of classes as the number of output nodes. Input for this NN would be the output from the top layer of DBN. This is called <a id="id399" class="indexterm"/>
<span class="strong"><strong>DBN-DNN architecture</strong></span>. Here, a DBN's role is generating highly efficient features (the output of the top layer of DBN) automatically from the input data for the supervised NN in the top layer. </p><p>The architecture of a five-layer DBN-DNN for a binary classification task is shown in the following figure:</p><div class="mediaobject"><img src="../Images/image00600.jpeg" alt="Deep belief networks"/></div><p style="clear:both; height: 1em;"> </p><p>The last layer <a id="id400" class="indexterm"/>is trained using the backpropagation algorithm in a supervised manner for the two classes <span class="inlinemediaobject"><img src="../Images/image00601.jpeg" alt="Deep belief networks"/></span> and <span class="inlinemediaobject"><img src="../Images/image00602.jpeg" alt="Deep belief networks"/></span>. We will illustrate the training and classification with such a DBN-DNN using the darch R package.</p></div><div class="section" title="The darch R package"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec47"/>The darch R package</h2></div></div></div><p>The <a id="id401" class="indexterm"/>darch package, written by Martin Drees, is one <a id="id402" class="indexterm"/>of the R packages using which one can begin doing deep learning in R. It implements the DBN described in the previous section (references 5 and 7 in the <span class="emphasis"><em>References</em></span> section of this chapter). The package can be downloaded from <a class="ulink" href="https://cran.r-project.org/web/packages/darch/index.html">https://cran.r-project.org/web/packages/darch/index.html</a>.</p><p>The main <a id="id403" class="indexterm"/>class in the darch package implements deep architectures and provides the ability to train them with Contrastive Divergence and fine-tune with backpropagation, resilient backpropagation, and conjugate gradients. The new instances of the class are created with the <code class="literal">newDArch</code> constructor. It is called with the following arguments: a vector containing the number of nodes in each layers, the batch size, a Boolean variable to indicate whether to use the <span class="strong"><strong>ff</strong></span> package for computing weights and outputs, and the name of the function for generating the weight matrices. Let us create a network having two input units, four hidden units, and one output unit:</p><div class="informalexample"><pre class="programlisting">install.packages("darch") #one time
&gt;library(darch)
&gt;darch &lt;- newDArch(c(2,4,1),batchSize = 2,genWeightFunc = generateWeights)
INFO [2015-07-19 18:50:29] Constructing a darch with 3 layers.
INFO [2015-07-19 18:50:29] Generating RBMs.
INFO [2015-07-19 18:50:29] Construct new RBM instance with 2 visible and 4 hidden units.
INFO [2015-07-19 18:50:29] Construct new RBM instance with 4 visible and 1 hidden units.</pre></div><p>Let us train the DBN with a toy dataset. We are using this because for training any realistic examples, it would take a long time: hours, if not days. Let us create an input data set containing two columns and four rows:</p><div class="informalexample"><pre class="programlisting">&gt;inputs &lt;- matrix(c(0,0,0,1,1,0,1,1),ncol=2,byrow=TRUE)
&gt;outputs &lt;- matrix(c(0,1,1,0),nrow=4)</pre></div><p>Now, let us pretrain the DBN, using the input data:</p><div class="informalexample"><pre class="programlisting">&gt;darch &lt;- preTrainDArch(darch,inputs,maxEpoch=1000)</pre></div><p>We can have a look at the weights learned at any layer using the <code class="literal">getLayerWeights( )</code> function. Let us see how the hidden layer looks:</p><div class="informalexample"><pre class="programlisting">&gt;getLayerWeights(darch,index=1)
[[1]]
          [,1]        [,2]       [,3]       [,4]
[1,]   8.167022    0.4874743  -7.563470  -6.951426
[2,]   2.024671  -10.7012389   1.313231   1.070006
[3,]  -5.391781    5.5878931   3.254914   3.000914</pre></div><p>Now, let's do a backpropagation for supervised learning. For this, we need to first set the layer functions to <code class="literal">sigmoidUnitDerivatives</code>:</p><div class="informalexample"><pre class="programlisting">&gt;layers &lt;- getLayers(darch)
&gt;for(i in length(layers):1){
     layers[[i]][[2]] &lt;- sigmoidUnitDerivative
    }
&gt;setLayers(darch) &lt;- layers
&gt;rm(layers)</pre></div><p>Finally, the<a id="id404" class="indexterm"/> following two lines perform the backpropagation:</p><div class="informalexample"><pre class="programlisting">&gt;setFineTuneFunction(darch) &lt;- backpropagation
&gt;darch &lt;- fineTuneDArch(darch,inputs,outputs,maxEpoch=1000)</pre></div><p>We can see the prediction quality of DBN on the training data itself by running <code class="literal">darch</code> as follows:</p><div class="informalexample"><pre class="programlisting">&gt;darch &lt;- getExecuteFunction(darch)(darch,inputs)
&gt;outputs_darch &lt;- getExecOutputs(darch)
&gt;outputs_darch[[2]]
        [,1]
[1,] 9.998474e-01
[2,] 4.921130e-05
[3,] 9.997649e-01
[4,] 3.796699e-05</pre></div><p>Comparing with the actual output, DBN has predicted the wrong output for the first and second input rows. Since this example was just to illustrate how to use the darch package, we are not worried about the 50% accuracy here.</p></div><div class="section" title="Other deep learning packages in R"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec48"/>Other deep learning packages in R</h2></div></div></div><p>Although<a id="id405" class="indexterm"/> there are other deep learning packages in R, such as<a id="id406" class="indexterm"/> <span class="strong"><strong>deepnet</strong></span> and<a id="id407" class="indexterm"/> <span class="strong"><strong>RcppDL</strong></span>, compared with libraries in other languages such as <a id="id408" class="indexterm"/>
<span class="strong"><strong>Cuda</strong></span> (C++) and <span class="strong"><strong>Theano</strong></span> (Python), R<a id="id409" class="indexterm"/> yet does not have good native libraries for deep learning. The only available package is a wrapper for the Java-based deep learning open source project H2O. This R package, <span class="strong"><strong>h2o</strong></span>, allows running H2O via its REST API from within R. Readers who are interested in serious deep learning projects and applications should use H2O using h2o packages in R. One needs to install H2O in your machine to use h2o. We will cover H2O in the next chapter when we discuss Big Data and the distributed computing platform called Spark.</p></div></div>
<div class="section" title="Exercises" id="aid-1TVKI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For <a id="id410" class="indexterm"/>the Auto MPG dataset, compare the performance of predictive models using ordinary regression, Bayesian GLM, and Bayesian neural networks.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-1UU541"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">MacKay D. J. C. <span class="emphasis"><em>Information Theory, Inference and Learning Algorithms</em></span>. Cambridge University Press. 2003. ISBN-10: 0521642981</li><li class="listitem">MacKayD. J. C. "The Evidence Framework Applied to Classification Networks". Neural Computation. Volume 4(3), 698-714. 1992</li><li class="listitem">MacKay D. J. C. "Probable Networks and Plausible Predictions – a review of practical Bayesian methods for supervised neural networks". Network: Computation in neural systems</li><li class="listitem">Hinton G. E., Rumelhart D. E., and Williams R. J. "Learning Representations by Back Propagating Errors". Nature. Volume 323, 533-536. 1986</li><li class="listitem">MacKay D. J. C. "Bayesian Interpolation". Neural Computation. Volume 4(3), 415-447. 1992</li><li class="listitem">Hinton G. E., Krizhevsky A., and Sutskever I. "ImageNet Classification with Deep Convolutional Neural Networks". Advances In Neural Information Processing Systems (NIPS). 2012</li><li class="listitem">Hinton G., Osindero S., and Teh Y. "A Fast Learning Algorithm for Deep Belief Nets". Neural Computation. 18:1527–1554. 2006</li><li class="listitem">Hinton G. and Salakhutdinov R. "Reducing the Dimensionality of Data with Neural Networks". Science. 313(5786):504–507. 2006</li><li class="listitem">Li Deng and Dong Yu. <span class="emphasis"><em>Deep Learning: Methods and Applications (Foundations and Trends(r) in Signal Processing)</em></span>. Now Publishers Inc. Vol 7, Issue 3-4. 2014. ISBN-13: 978-1601988140</li><li class="listitem">Hinton G. "A Practical Guide to Training Restricted Boltzmann Machines". UTML Tech Report 2010-003. Univ. Toronto. 2010</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-1VSLM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Summary</h1></div></div></div><p>In this chapter, we learned about an important class of machine learning model, namely neural networks, and their Bayesian implementation. These models are inspired by the architecture of the human brain and they continue to be an area of active research and development. We also learned one of the latest advances in neural networks that is called deep learning. It can be used to solve many problems such as computer vision and natural language processing that involves highly cognitive elements. The artificial intelligent systems using deep learning were able to achieve accuracies comparable to human intelligence in tasks such as speech recognition and image classification. With this chapter, we have covered important classes of Bayesian machine learning models. In the next chapter, we will look at a different aspect: large scale machine learning and some of its applications in Bayesian models.</p></div></body></html>