["```py\nfrom sagemaker.pytorch import PyTorch\n\npt_dist_estimator = PyTorch(\n                entry_point=\"train_pytorch_dist.py\",\n               … \n              distribution={\n                    \"smdistributed\": {\"dataparallel\": {\"enabled\": True}}\n              }\n)\n```", "```py\n    import smdistributed.dataparallel.torch.distributed as dist\n    from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n    dist.init_process_group()\n    ```", "```py\n    torch.cuda.set_device(dist_get_local_rank())\n    ```", "```py\n    batch_size //= dist.get_world_size()\n    batch_size = max(batch_size, 1)\n    ```", "```py\n    model = DDP(model)\n    ```", "```py\n    pt_dist_estimator.fit()\n    ```", "```py\nmpi_options = {\n    \"enabled\": True,\n   \"processes_per_host\": 4\n  }\n\ndist_options = {\n    \"modelparallel\":{\n       \"enabled\": True,\n       \"parameters\": {\n           \"partitions\": 4,  # we'll partition the model among the 4 GPUs \n           \"microbatches\": 8,  # Mini-batchs are split in micro-batch to increase parallelism\n           \"optimize\": \"memory\" # The automatic model partitioning can optimize speed or memory\n           }\n       }\n}\npt_model_dist_estimator = PyTorch(\n    entry_point=\"train_pytorch_model_dist.py\",\n    ...\n    distribution={\"mpi\": mpi_options, \"smdistributed\": dist_options}\n)\n```", "```py\n    import smdistributed.modelparallel.torch as smp\n    smp.init()\n    ```", "```py\n    model = smp.DistributedModel(model)\n    optimizer = smp.DistributedOptimizer(optimizer)\n    ```", "```py\n    @smp.step\n    def train_step(model, data, target):\n        output = model(data)\n        long_target = target.long()\n        loss = F.nll_loss(output, long_target, reduction=\"mean\")\n        model.backward(loss)\n        return output, loss\n    ```", "```py\n    pt_dist_estimator.fit()\n    ```", "```py\n    from sagemaker.tuner import (\n     IntegerParameter,\n     CategoricalParameter,\n     ContinuousParameter,\n     HyperparameterTuner, \n    ) \n    hyperparameter_ranges = { \n     \"eta\": ContinuousParameter(0, 1),\n     \"min_child_weight\": ContinuousParameter(1, 10),\n     \"alpha\": ContinuousParameter(0, 2), \n     \"max_depth\": IntegerParameter(1, 10)\n    }\n    ```", "```py\n    estimator_hpo = \\ sagemaker.estimator.Estimator( \n    image_uri=xgboost_container, \n    hyperparameters=hyperparameters, \n    role=sagemaker.get_execution_role(), \n    instance_count=1, \n    instance_type='ml.m5.12xlarge', \n    volume_size=200, # 5 GB \n    output_path=output_path \n    ) \n    ```", "```py\n    tuner = HyperparameterTuner(\n                 estimator_hpo, \n         objective_metric_name,\n         hyperparameter_ranges, \n         max_jobs=10,\n         max_parallel_jobs=2,\n         objective_type = 'Minimize'\n    )\n    ```", "```py\n    tuner.fit({'train': train_input, \n               'validation': validation_input})\n    ```", "```py\n    weather_experiment = Experiment.create(\n        experiment_name=f\"weather-experiment-{int(time.time())}\",  \n        description=\"Weather Data Prediction\", \n        sagemaker_boto_client=sm)\n    ```", "```py\n    with Tracker.create(display_name=\"Training\", sagemaker_boto_client=sm) as tracker:\n        # Log the location of the training dataset\n        tracker.log_input(name=\"weather-training-dataset\", \n      media_type=\"s3/uri\", \n     value=\"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'train')) \n    ```", "```py\n    for i, max_depth in enumerate([2, 5]):\n        # create trial\n        trial_name = f\"xgboost-training-job-trial-{max_depth}-max-depth-{int(time.time())}\"\n        xgboost_trial = Trial.create(\n            trial_name=trial_name, \n            experiment_name=weather_experiment.experiment_name,\n            sagemaker_boto_client=sm,\n        )\n        max_depth_trial_name_map[max_depth] = trial_name\n\n        xgboost_training_job_name = \"xgboost-training-job-{}\".format(int(time.time()))\n\n    ```", "```py\n    # Now associate the estimator with the Experiment and Trial\n        estimator.fit(\n            inputs={'training': train_input}, \n            job_name=xgboost_training_job_name,\n            experiment_config={\n                \"TrialName\": xgboost_trial.trial_name,\n                \"TrialComponentDisplayName\": \"Training\",\n            },\n            wait=False,\n        )\n    ```", "```py\n    trial_component_analytics = \\ ExperimentAnalytics(sagemaker_session=sagemaker_session, experiment_name=experiment_name ) \n    trial_component_analytics.dataframe()\n    ```"]