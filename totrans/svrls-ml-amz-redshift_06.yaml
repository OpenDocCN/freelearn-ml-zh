- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about classification algorithms used in **machine**
    **learning** (**ML**). You will learn about the various methods that Redshift
    offers when you create classification models. This chapter will provide detailed
    examples of both **binary** and **multi-class classification models** and show
    you how to solve business problems with these modeling techniques. By the end
    of this chapter, you will be in a position to identify whether a business problem
    is a classification or not, identify the right method that Redshift offers in
    training, and build a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to classification algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model syntax with user guidance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a binary classification model using the XGBoost algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a multi-class classification model using the Linear Learner model type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon Redshift Serverless endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing the *Getting started with Amazon Redshift Serverless* section in
    [*Chapter 1*](B19071_01.xhtml#_idTextAnchor015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code used in this chapter here: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/).'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to classification algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Classification** is the process of categorizing any kind of entity or class
    so that it is better understood and analyzed. The classifying process usually
    happens as part of a pre-setup business process (for example, tagging a product
    as defective or good after observing it), or through a return process (for example,
    tagging a product as defective after the customer returned it as defective). In
    either event, the important point is classifying an entity – in this case, a product
    into a class (i.e., defective or not).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.1* shows data that has been classified into two classes using three
    input variables. The figure shows where a pair of **Input** and **Output** data
    points are categorized into two classes. When output labels consist of only two
    classes, it is called a **binary** **classification** problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Binary classification](img/B19071_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Binary classification
  prefs: []
  type: TYPE_NORMAL
- en: 'If the output variable consists of more than two classes – for example, predicting
    whether a fruit is an apple, an orange, or a pear – then it is called **multi-class
    classification**. *Figure 6**.2* shows data that has been classified into multiple
    classes based on a set of three input variables. The figure shows a multi-class
    classification chart, illustrating how input and output pairs are classified into
    three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Multi-class classification](img/B19071_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Multi-class classification
  prefs: []
  type: TYPE_NORMAL
- en: The classification process can also happen on data that does not have classes
    defined yet. Let us continue to understand how this is possible.
  prefs: []
  type: TYPE_NORMAL
- en: It is not always the case that your entities are grouped or categorized in a
    certain way. For example, if you want to analyze your customers’ purchase history
    or clickstream activity, or if you want to group similar customers based on demographics
    or shopping behavior, then classification algorithms come in handy to analyze
    the data and group similar data points into clusters. This type of classification
    modeling is called **unsupervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing classes helps the analysis process – for example, once products
    are tagged to a class label, you can easily retrieve a list of defective products
    that are returned and then further study the characteristics, such as store location,
    the demographics of the customer who returned the product, and the season when
    a product was returned most. How and when classes are defined and established
    enables businesses to conduct a deep-dive analysis, not only answering questions
    such as where and what but also training an ML model on historical data and classes,
    and predicting which class an entity will fall into.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common use cases where classification models are useful include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer behavior prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document or image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will show you how to create different classification models
    that Redshift offers you. Amazon Redshift provides **XGBoost**, **multilayer perceptron**
    (**MLP**), and **Linear Learner algorithms** to train and build a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will begin the journey of learning about supervised classification
    models by building binary classification models, using XGBoost, and a multi-class
    classification model, using linear learner. MLP models will be covered in [*Chapter
    9*](B19071_09.xhtml#_idTextAnchor157), whereas unsupervised classification modeling
    will be covered in [*Chapter 8*](B19071_08.xhtml#_idTextAnchor139).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will walk you through the detailed syntax of creating models with Redshift
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the Redshift CREATE MODEL syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B19071_04.xhtml#_idTextAnchor057)*,* we saw different variations
    of the Redshift `CREATE MODEL` command and how a data analyst, citizen data scientist,
    or data scientist can operate the `CREATE MODEL` command, with varying degrees
    of complexity. In this section, we will introduce you to a citizen data scientist
    persona, who is not fully aware of statistics but has good knowledge about identifying
    what algorithm to use and what problem type can be applied to a business problem.
    In the Redshift ML world, this type of model creation is known as **CREATE MODEL
    with** **user guidance**.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to explore the model type and problem type parameters of the `CREATE
    MODEL` statement. As part of *CREATE MODEL with user guidance*, you also have
    the option of setting a preprocessor, but we will leave that topic for [*Chapter
    10*](B19071_10.xhtml#_idTextAnchor178).
  prefs: []
  type: TYPE_NORMAL
- en: As an ML model creator, you will decide what algorithm to use and what problem
    type to address. Redshift ML still performs the feature engineering of independent
    variables behind the scenes. For example, out of 20 features, Redshift ML will
    automatically identify the categorical variables and numeric variables and create
    one-hot-encoded value or standardization of numerical variables where applicable,
    along with various other tasks required to complete the model training.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, you let Redshift ML handle the bulk of data preparation tasks for
    ML. As a model creator, you come up with an algorithm to be used and a problem
    type to be solved. By preselecting an algorithm type and problem type, Redshift
    ML will reduce the training type, as it trains the model on other algorithms and
    problem types. Compared to the full `AUTO` CREATE MODEL statement that we created
    in [*Chapter 5*](B19071_05.xhtml#_idTextAnchor068), *CREATE MODEL with user guidance*
    takes less time.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the previous section, we will use the XGBoost algorithm for
    binary classification and the linear learner algorithm for multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about XGBoost here: [https://docs.aws.amazon.com/sagemaker/latest/dg/XGBoost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/XGBoost.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'And you can learn more about Linear Learner here: [https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Using a simple `CREATE MODEL` statement, Redshift ML will use SageMaker Autopilot
    to automatically determine the problem type, algorithm, and the best model type
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Redshift ML, you can influence a model by providing user guidance. You
    can choose `model_type`, `problem_type`, and `objective` when you issue the `CREATE
    MODEL` statement. You can find more details on the syntax and options here: [https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html](https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html).'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed the basics of the Redshift ML `CREATE MODEL` syntax
    and how you can provide guidance, such as model type and objective, or choose
    to let Redshift ML automatically choose these for you.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you will learn how to create a binary classification model and specify
    the XGBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Training a binary classification model using the XGBoost algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Binary classification models are used to solve the problem of predicting one
    class of two possible classes – for example, predicting whether it will rain or
    not. The goal is to learn about past data points and figure out which one of the
    target buckets a particular data point will fall into. The typical use cases of
    a binary classification model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting whether a patient suffers from a disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting whether a customer will churn or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting behavior – for example, whether a customer will file an appeal or
    not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next few sections, we will go through the following steps to achieve
    our goal of creating a binary classification model to be used to run inference
    queries:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the business problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uploading and analyzing data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running prediction queries against the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing the business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build our binary classification problem, we will take a look at a banking
    campaign issue. Banks spend a lot of money on marketing campaigns targeted toward
    their customers so that they will subscribe to their products. It is very important
    that banks build efficiency into their campaign, and this can be done by learning
    the last campaign dataset and predicting future campaign results. We will work
    on predicting whether a banking customer will subscribe to a banking product offer
    of a term deposit.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading and analyzing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to work on a bank marketing dataset in this section. The data is
    related to direct marketing campaigns of a Portuguese banking institution. Imagine
    you are a marketing analyst and your goal is to increase the amount of deposits
    by offering a term deposit to your customers. It is very important that marketing
    campaigns target customers appropriately. You will create a model using Redshift
    ML to predict whether a customer is likely to accept the term deposit offer. This
    dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: '[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. *A Data-Driven Approach
    to Predict the Success of Bank Telemarketing. Decision Support Systems*, Elsevier,
    62:22–31, June 2014'
  prefs: []
  type: TYPE_NORMAL
- en: The classification goal is to predict whether the client will subscribe (yes/no)
    to a term deposit (the *y* variable).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has columns such as age, job, marital status, education level, and
    employment status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metadata about these columns can also be found at the UCI ML repository website
    here: [https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding link, there are 20 independent variables and
    1 dependent variable (*y*). We can use any or all of these independent variables
    as input to our `CREATE MODEL` statement to be able to predict the outcome, *y*,
    which indicates whether the customer is likely to accept the offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully connecting to Redshift as an admin or database developer,
    create the schema and load data into Amazon Redshift using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Redshift **query editor v2**, and connect to the **Serverless**
    endpoint and the **dev** database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the `Chap6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the serverless connection, the database set
    to **dev**, and the query editor page saved as **Chap6**:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.3 – Query Editor \uFEFFv2](img/B19071_06_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Query Editor v2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, using the following line of code, create the schema. This schema is where
    all the tables and data needed for this chapter will be created and maintained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see output like this, indicating that your schema is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Schema created](img/B19071_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Schema created
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will create the `bank_details_training` table to store data
    to train the model, and the `bank_details_inference` table to store data to run
    the inference queries. Note that we have already split our input dataset into
    these two datasets for you. All of the SQL commands used in this chapter can be
    found here: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code from GitHub to create the training and inference tables
    in Query Editor v2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see output like this to verify that your tables have been created
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Tables created successfully](img/B19071_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Tables created successfully
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created the tables, run the commands in *step 5* using Query
    Editor v2 to load the data, using the S3 buckets provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the sample data into the tables created in *step 4* by using the following
    command, which can be found on GitHub. Note that we use the `COPY` command to
    load this data from Amazon S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Analyze the customer term deposit subscription table by creating a histogram
    chart. First, run the following command again using Query Editor v2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see in the result set that **36548** customers did not choose the bank’s
    offer and **4640** did accept. You can also use the chart feature in Query Editor
    v2 to create a bar chart. Click on the **Chart** option found on the right-hand
    side in the **Result** pane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The subscription results and the Chart option](img/B19071_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The subscription results and the Chart option
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get the following result after choosing **Bar** for **Type**, **y**
    for the **X** value, and **customer_count** for the **Y** value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – A chart of customer acceptance](img/B19071_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – A chart of customer acceptance
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our data loaded, we can create our model.
  prefs: []
  type: TYPE_NORMAL
- en: Using XGBoost to train a binary classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will specify `MODEL_TYPE` and `PROBLEM_TYPE` to create
    a binary classification model using the XGBoost algorithm. We will now address
    the banking campaign problem. The goal of this model is to predict whether a customer
    will subscribe to a term deposit or not.
  prefs: []
  type: TYPE_NORMAL
- en: We will set `MODEL_TYPE` as `XGBoost` and `PROBLEM_TYPE` as `BINARY_CLASSIFICATION`.
    We will use the default `IAM_ROLE`. We also need to specify the S3 bucket where
    the model artifacts will be stored and, additionally, set `MAX_RUNTIME` to `3600`
    (in seconds).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to create the model. You will find the complete code
    along with all the SQL commands needed for the chapter at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter6.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter6.sql):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By setting `MODEL_TYPE` to `XGBoost` and `PROBLEM_TYPE` to `BINARY_CLASSIFICATION`,
    we guide Redshift ML to only search for the best XGBoost model in this training
    run. If this is left as default, Redshift ML checks whether other classification
    models can be applied to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Since the **SageMaker AutoPilot algorithm** does not have to test other model
    types or determine the problem type, the end result will be less training time.
    In this example, SageMaker Autopilot takes care of selecting the objective type,
    adjusting hyperparameters, and handling the data preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the status of the model, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Showing the model output](img/B19071_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Showing the model output
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, we can see that the model is still under training.
    Also, note that Redshift ML picks up the `CREATE MODEL` statement. Other parameters,
    such as the objective, hyperparameters, and preprocessing, are still auto-handled
    by Redshift ML.
  prefs: []
  type: TYPE_NORMAL
- en: The **predict_term_deposit** parameter under **Function Name** is used to generate
    predictions, which we will use in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `SHOW MODEL` command again after some time to check whether model training
    is complete. From the following screenshot, you can see that **Model State** is
    **READY** and **F1** has been selected as the objective for model evaluation.
    The **F1** score is **0.646200**, or 64%. The closer this number is to 1, the
    better the model score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Showing the model output](img/B19071_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Showing the model output
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the following query against our training data to validate the F1
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see in the following output that our accuracy is very good at almost
    94%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The accuracy results](img/B19071_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – The accuracy results
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model training is complete, we will use the function created to
    run prediction queries.
  prefs: []
  type: TYPE_NORMAL
- en: Running predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us run some predictions on our inference dataset to see how many customers
    are predicted to subscribe to the term deposit. Run the following SQL statement
    in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Prediction results](img/B19071_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Prediction results
  prefs: []
  type: TYPE_NORMAL
- en: We can see that **642** customers are predicted to accept the offer to subscribe
    to the term deposit, and **3477** are predicted to not accept the offer.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Redshift ML now provides the capability to get the probability of a
    prediction for binary and multi-class classification problems. Note that in the
    output of the `SHOW MODEL` command in *Figure 6**.9*, an additional function name
    has been created called `predict_term_deposit_prob`. Run the following query to
    check the probability that married customers with management jobs and between
    35 and 45 years of age will accept the term deposit offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Probability results](img/B19071_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Probability results
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the first row a **0.99985629** probability of a *false* prediction
    and only a **0.00014372** probability of a *true* prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also modify the preceding query to see the probability of the customers
    that are predicted to accept the term deposit offer. Run the following SQL command
    in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see similar results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – The probability results for customers accepting the term offer](img/B19071_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – The probability results for customers accepting the term offer
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 5*](B19071_05.xhtml#_idTextAnchor068), you learned how to determine
    feature importance by running an explainability report. Run the following query
    to see which inputs contributed most to the model prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the result and copy it to the editor so that it is easier to read, as
    shown in *Figure 6**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – The explainability report](img/B19071_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The explainability report
  prefs: []
  type: TYPE_NORMAL
- en: This shows that `pdays` has the most importance and that `poutcome` has the
    least.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have built a binary classification model, let us move on and try
    building a multi-class classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a multi-class classification model using the Linear Learner model type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to build a multi-class classification model
    in Amazon Redshift ML using the linear learner algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will use a customer segmentation dataset from Kaggle: [https://www.kaggle.com/datasets/vetrirah/customer](https://www.kaggle.com/datasets/vetrirah/customer).'
  prefs: []
  type: TYPE_NORMAL
- en: You will use this dataset to train a model to classify customers into one of
    four segments (`A`, `B`, `C`, or `D`). By segmenting customers, you can better
    understand the customer and do targeted marketing to customers, with product offerings
    that are relevant to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data has already been split into training and testing sets and is stored
    in the following S3 locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`s3://packt-serverless-ml-redshift/chapter06/segmentation/train.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3://packt-serverless-ml-redshift/chapter06/segmentation/test.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After successfully connecting to Redshift as an admin or database developer,
    load data into Amazon Redshift as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Redshift **query editor v2**, and connect to the **Serverless**
    endpoint and the **dev** database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same schema and query editor page you created for the binary classification
    exercise (see the *Uploading and analyzing the* *data* section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the train and test tables and load the data using the following SQL commands
    in Query Editor v2\. These SQL commands can be found at
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now that the data has loaded, let’s do some analysis of our training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyze the training data by executing the following SQL command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Segmentation](img/B19071_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Segmentation
  prefs: []
  type: TYPE_NORMAL
- en: Our training dataset has a total of 8,068 customer records. From this sample,
    we can see that segments **C**, **B**, and **A** are very similar and that more
    customers are in segment **D**.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the input from the training dataset to predict the customer segment,
    using the linear learner algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using Linear Learner to predict the customer segment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear learner** is a supervised learning algorithm and one of the model
    types you can use to solve classification or regression problems.'
  prefs: []
  type: TYPE_NORMAL
- en: For multi-class classification problems, we have more than two labels (or targets)
    that we will try to predict, compared to exactly two labels for binary classification
    problems. We will show you how to use linear learner to solve regression problems
    in [*Chapter 7*](B19071_07.xhtml#_idTextAnchor111).
  prefs: []
  type: TYPE_NORMAL
- en: With linear learner, you can achieve a significant increase in speed compared
    to traditional hyperparameter optimization techniques, making it very convenient.
  prefs: []
  type: TYPE_NORMAL
- en: We will provide a training set with data that contains our input or observations
    about the data, and the label that represents the value we want to predict. We
    can optionally provide certain combinations of preprocessors to certain sets of
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will apply user guidance techniques by providing `MODEL_TYPE`,
    `PROBLEM_TYPE`, and `OBJECTIVE` to create a multi-class classification model using
    the linear learner algorithm. The goal of this model is to predict the segment
    for each customer.
  prefs: []
  type: TYPE_NORMAL
- en: We will set `MODEL_TYPE` as `LINEAR_LEARNER` and `PROBLEM_TYPE` as `MULTICLASS_CLASSIFICATION`.
    We will leave other options as default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us execute the following code in Query Editor v2 to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the status of the model, run the following command in Query Editor
    v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Showing the model output](img/B19071_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Showing the model output
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the model is now in the `CREATE` `MODEL` statement.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model is trained, it is time to evaluate its quality.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you issue the `CREATE MODEL` command, Amazon SageMaker will automatically
    divide your data into testing and training in the background so that it can determine
    the accuracy of the model. If you look at the `validation:multiclass_accuracy`
    key from the `SHOW MODEL` output, you will see a value of **0.535028**, which
    means our model can correctly pick the segment 53% of the time. Ideally, we prefer
    a value closer to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also run a validation query to check our accuracy rates. In the following
    query, note that we select the actual segmentation, and then we use the function
    that was generated by our `CREATE MODEL` command to get the predicted segmentation
    to do the comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The model accuracy](img/B19071_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – The model accuracy
  prefs: []
  type: TYPE_NORMAL
- en: This output shows that we are very close to the score of **.535028** when we
    compare the number of times the model correctly predicted the segment against
    the total number of input records.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have checked the model accuracy, we are ready to run prediction
    queries against the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Running prediction queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our model and have done validation, we can run our prediction
    query against our test set so that we can segment our prospective customers, based
    on customer IDs. You can see that we now use our function against the test table
    to get the predicted segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 10 customers are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – The predicted segment](img/B19071_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – The predicted segment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the new prospective customers are spread across the various segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see here how many prospective customers are in each segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – The customer count by segment](img/B19071_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – The customer count by segment
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have this information, your marketing team is ready to target their
    efforts on these prospective customers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at some other options you can use to solve this multi-class
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other CREATE MODEL options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also create this model in a couple of different ways, which we will explore
    in the following sections. It is important to understand the different options
    available so that you can experiment and choose the approach that gives you the
    best model.
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, we will not provide any user guidance, such as specifying
    `MODEL_TYPE`, `PROBLEM_TYPE`, or `OBJECTIVE`. Use this approach if you are new
    to ML and want to let SageMaker Autopilot determine this for you.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the next example, you can see how you can provide `PROBLEM_TYPE` and
    `OBJECTIVE`. As a more experienced user of ML, you should now recognize which
    `PROBLEM_TYPE` and `OBJECTIVE` instances are best for your use case. When you
    provide these inputs, it will speed up the model training process, since SageMaker
    Autopilot will only train using the provided user guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model with no user guidance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, we let SageMaker Autopilot choose `MODEL_TYPE`, `PROBLEM_TYPE`,
    and `OBJECTIVE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have only provided the basic settings. We did not specify `MODEL_TYPE`,
    `PROBLEM_TYPE`, or `OBJECTIVE`. Amazon Redshift ML and SageMaker will automatically
    figure out that this is a multi-class classification problem and use the best
    model type. As an additional exercise, run this `CREATE MODEL` command, and then
    run the `SHOW MODEL` command. It will show you the `MODEL_TYPE` parameter that
    Amazon SageMaker used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model with some user guidance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will provide `PROBLEM_TYPE` and `OBJECTIVE`, but we will
    let Amazon SageMaker determine `MODEL_TYPE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we let Amazon Redshift ML and Amazon SageMaker determine `MODEL_TYPE`,
    and we pass in `PROBLEM_TYPE` and `OBJECTIVE`. When you have some free time, experiment
    with the different methods of creating the models, and note the differences you
    see in the time it takes to train the model, and also compare the accuracy and
    other outputs of the `SHOW` `MODEL` command.
  prefs: []
  type: TYPE_NORMAL
- en: You can also create multi-class classification models using XGBoost, which we
    will cover in [*Chapter 10*](B19071_10.xhtml#_idTextAnchor178).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed classification models in detail and looked at
    their common use cases. We also explained the `CREATE MODEL` syntax for classification
    models, where you provide guidance to train a model by supplying the model type
    and objective.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to do binary classification and multi-class classification with
    Amazon Redshift ML and how to use the XGBoost and linear learner algorithms. We
    also showed you how to check the status of your models, validate them for accuracy,
    and write SQL queries to run predictions on your test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will show you how to build regression models using Amazon
    Redshift ML.
  prefs: []
  type: TYPE_NORMAL
