- en: Part I. Module 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分。模块1
- en: '**Scala for Data Science**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Scala数据科学**'
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Leverage the power of Scala with different tools to build scalable, robust
    data science applications*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*利用Scala的不同工具构建可扩展、健壮的数据科学应用程序*'
- en: Chapter 1. Scala and Data Science
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。Scala和数据科学
- en: The second half of the 20^(th) century was the age of silicon. In fifty years,
    computing power went from extremely scarce to entirely mundane. The first half
    of the 21^(st) century is the age of the Internet. The last 20 years have seen
    the rise of giants such as Google, Twitter, and Facebook—giants that have forever
    changed the way we view knowledge.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪后半叶是硅的时代。在50年里，计算能力从极其稀缺到完全平凡。21世纪前半叶是互联网的时代。在过去的20年里，谷歌、推特和Facebook等巨头崛起——这些巨头永远改变了我们看待知识的方式。
- en: The Internet is a vast nexus of information. Ninety percent of the data generated
    by humanity has been generated in the last 18 months. The programmers, statisticians,
    and scientists who can harness this glut of data to derive real understanding
    will have an ever greater influence on how businesses, governments, and charities
    make decisions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网是一个庞大的信息枢纽。人类产生的90%的数据在过去18个月内已经生成。能够利用这些数据洪流以获得真正理解的程序员、统计学家和科学家将对商业、政府和慈善机构如何做出决策产生越来越大的影响。
- en: This book strives to introduce some of the tools that you will need to synthesize
    the avalanche of data to produce true insight.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在介绍一些您将需要的工具，以从数据洪流中综合提炼出真正的洞察力。
- en: Data science
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: 'Data science is the process of extracting useful information from data. As
    a discipline, it remains somewhat ill-defined, with nearly as many definitions
    as there are experts. Rather than add yet another definition, I will follow *Drew
    Conway''s* description ([http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)).
    He describes data science as the culmination of three orthogonal sets of skills:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是从数据中提取有用信息的过程。作为一个学科，它仍然有些模糊不清，定义的数量几乎与专家的数量一样多。而不是再添加另一个定义，我将遵循**德鲁·康威**的描述（[http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)）。他描述数据科学为三个正交技能集的融合：
- en: 'Data scientists must have *hacking skills*. Data is stored and transmitted
    through computers. Computers, programming languages, and libraries are the hammers
    and chisels of data scientists; they must wield them with confidence and accuracy
    to sculpt the data as they please. This is where Scala comes in: it''s a powerful
    tool to have in your programming toolkit.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家必须具备**黑客技能**。数据存储和传输通过计算机进行。计算机、编程语言和库是数据科学家的锤子和凿子；他们必须自信且准确地使用它们来塑造数据。这正是Scala发挥作用的地方：它是编程工具箱中一个强大的工具。
- en: Data scientists must have a sound understanding of *statistics and numerical
    algorithms*. Good data scientists will understand how machine learning algorithms
    function and how to interpret results. They will not be fooled by misleading metrics,
    deceptive statistics, or misinterpreted causal links.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家必须对**统计学和数值算法**有扎实的理解。优秀的数据科学家将理解机器学习算法的工作原理以及如何解读结果。他们不会被误导的指标、欺骗性的统计数据或误解的因果关系所迷惑。
- en: A good data scientist must have a sound understanding of the *problem domain*.
    The data science process involves building and discovering knowledge about the
    problem domain in a scientifically rigorous manner. The data scientist must, therefore,
    ask the right questions, be aware of previous results, and understand how the
    data science effort fits in the wider business or research context.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个优秀的数据科学家必须对**问题领域**有扎实的理解。数据科学过程涉及以科学严谨的方式构建和发现关于问题领域知识。因此，数据科学家必须提出正确的问题，了解以往的结果，并理解数据科学努力如何融入更广泛的商业或研究背景。
- en: 'Drew Conway summarizes this elegantly with a Venn diagram showing data science
    at the intersection of hacking skills, maths and statistics knowledge, and substantive
    expertise:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 德鲁·康威用一张维恩图优雅地总结了这一点，展示了数据科学位于黑客技能、数学和统计学知识以及实质性专业知识交汇处：
- en: '![Data science](img/image01158.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学](img/image01158.jpeg)'
- en: It is, of course, rare for people to be experts in more than one of these areas.
    Data scientists often work in cross-functional teams, with different members providing
    the expertise for different areas. To function effectively, every member of the
    team must nevertheless have a general working knowledge of all three areas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，人们精通这些领域中的多个领域是罕见的。数据科学家通常在跨职能团队中工作，不同成员为不同领域提供专业知识。然而，为了有效运作，团队中的每个成员都必须对所有三个领域有一个一般的工作知识。
- en: 'To give a more concrete overview of the workflow in a data science project,
    let''s imagine that we are trying to write an application that analyzes the public
    perception of a political campaign. This is what the data science pipeline might
    look like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地概述数据科学项目中的工作流程，让我们设想我们正在尝试编写一个分析公众对政治运动看法的应用程序。数据科学管道可能看起来是这样的：
- en: '**Obtaining data**: This might involve extracting information from text files,
    polling a sensor network or querying a web API. We could, for instance, query
    the Twitter API to obtain lists of tweets with the relevant hashtags.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取数据**：这可能涉及从文本文件中提取信息、轮询传感器网络或查询Web API。例如，我们可以查询Twitter API以获取包含相关标签的推文列表。'
- en: '**Data ingestion**: Data often comes from many different sources and might
    be unstructured or semi-structured. Data ingestion involves moving data from the
    data source, processing it to extract structured information, and storing this
    information in a database. For tweets, for instance, we might extract the username,
    the names of other users mentioned in the tweet, the hashtags, text of the tweet,
    and whether the tweet contains certain keywords.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取**：数据通常来自许多不同的来源，可能是非结构化或半结构化的。数据摄取涉及将数据从数据源移动到处理它以提取结构化信息，并将这些信息存储在数据库中。例如，对于推文，我们可能会提取用户名、推文中提到的其他用户的名称、标签、推文文本以及推文是否包含某些关键词。'
- en: '**Exploring data**: We often have a clear idea of what information we want
    to extract from the data but very little idea how. For instance, let''s imagine
    that we have ingested thousands of tweets containing hashtags relevant to our
    political campaign. There is no clear path to go from our database of tweets to
    the end goal: insight into the overall public perception of our campaign. Data
    exploration involves mapping out how we are going to get there. This step will
    often uncover new questions or sources of data, which requires going back to the
    first step of the pipeline. For our tweet database, we might, for instance, decide
    that we need to have a human manually label a thousand or more tweets as expressing
    "positive" or "negative" sentiments toward the political campaign. We could then
    use these tweets as a training set to construct a model.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索数据**：我们通常对想要从数据中提取的信息有一个明确的想法，但对如何做到这一点却知之甚少。例如，让我们设想我们已经摄取了包含与我们政治运动相关的标签的数千条推文。从我们的推文数据库到最终目标——了解公众对我们运动的总体看法——没有明确的路径。数据探索涉及规划我们如何到达那里。这一步骤通常会揭示新的问题或数据来源，这需要回到管道的第一步。例如，对于我们的推文数据库，我们可能会决定需要有人手动标记一千条或更多的推文，以表达对政治运动的“积极”或“消极”情绪。然后我们可以使用这些推文作为训练集来构建模型。'
- en: '**Feature building**: A machine learning algorithm is only as good as the features
    that enter it. A significant fraction of a data scientist''s time involves transforming
    and combining existing features to create new features more closely related to
    the problem that we are trying to solve. For instance, we might construct a new
    feature corresponding to the number of "positive" sounding words or pairs of words
    in a tweet.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建特征**：机器学习算法的好坏取决于进入它的特征。数据科学家的大部分时间都花在转换和组合现有特征以创建与我们要解决的问题更紧密相关的新特征上。例如，我们可能会构建一个新特征，对应于推文中“积极”语气单词或单词对的数量。'
- en: '**Model construction and training**: Having built the features that enter the
    model, the data scientist can now train machine learning algorithms on their datasets.
    This will often involve trying different algorithms and optimizing model **hyperparameters**.
    We might, for instance, settle on using a random forest algorithm to decide whether
    a tweet is "positive" or "negative" about the campaign. Constructing the model
    involves choosing the right number of trees and how to calculate impurity measures.
    A sound understanding of statistics and the problem domain will help inform these
    decisions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型构建和训练**：在构建了进入模型的特征之后，数据科学家现在可以在他们的数据集上训练机器学习算法。这通常涉及尝试不同的算法和优化模型的**超参数**。例如，我们可能会决定使用随机森林算法来决定一条推文是对活动“正面”还是“负面”的看法。构建模型涉及选择合适的树的数量以及如何计算不纯度度量。对统计学和问题领域的良好理解将有助于这些决策。'
- en: '**Model extrapolation and prediction**: The data scientists can now use their
    new model to try and infer information about previously unseen data points. They
    might pass a new tweet through their model to ascertain whether it speaks positively
    or negatively of the political campaign.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型外推和预测**：数据科学家现在可以使用他们新的模型来尝试推断关于先前未见数据点的信息。他们可能会将一条新的推文通过他们的模型来确认它是否对政治活动持正面或负面的看法。'
- en: '**Distillation of intelligence and insight from the model**: The data scientists
    combine the outcome of the data analysis process with knowledge of the business
    domain to inform business decisions. They might discover that specific messages
    resonate better with the target audience, or with specific segments of the target
    audience, leading to more accurate targeting. A key part of informing stakeholders
    involves data visualization and presentation: data scientists create graphs, visualizations,
    and reports to help make the insights derived clear and compelling.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从模型中提取智能和洞察力**：数据科学家将数据分析过程的结果与业务领域的知识相结合，以指导业务决策。他们可能会发现某些信息与目标受众或目标受众的特定部分产生更好的共鸣，从而实现更精确的目标。向利益相关者提供信息的关键部分涉及数据可视化和展示：数据科学家创建图表、可视化和报告，以帮助使得出的见解清晰且引人入胜。'
- en: 'This is far from a linear pipeline. Often, insights gained at one stage will
    require the data scientists to backtrack to a previous stage of the pipeline.
    Indeed, the generation of business insights from raw data is normally an iterative
    process: the data scientists might do a rapid first pass to verify the premise
    of the problem and then gradually refine the approach by adding new data sources
    or new features or trying new machine learning algorithms.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这远非一个线性的管道。通常，在一个阶段获得的见解将要求数据科学家回溯到管道的先前阶段。确实，从原始数据生成业务见解通常是一个迭代的过程：数据科学家可能会进行快速的第一遍扫描以验证问题的前提，然后通过添加新的数据源或新特征或尝试新的机器学习算法来逐步完善方法。
- en: In this book, you will learn how to deal with each step of the pipeline in Scala,
    leveraging existing libraries to build robust applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你将学习如何在Scala中处理管道的每个步骤，利用现有库构建健壮的应用程序。
- en: Programming in data science
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中的编程
- en: This book is not a book about data science. It is a book about how to use Scala,
    a programming language, for data science. So, where does programming come in when
    processing data?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是一本关于数据科学的书。它是一本关于如何使用Scala编程语言进行数据科学的书。那么，在处理数据时编程的作用在哪里呢？
- en: Computers are involved at every step of the data science pipeline, but not necessarily
    in the same manner. The style of programs that we build will be drastically different
    if we are just writing throwaway scripts to explore data or trying to build a
    scalable application that pushes data through a well-understood pipeline to continuously
    deliver business intelligence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在数据科学管道的每个步骤中都发挥作用，但并不一定是同样的方式。如果我们只是编写临时脚本以探索数据或试图构建一个可扩展的应用程序，该应用程序通过一个被充分理解的管道推送数据以持续提供业务智能，那么我们构建的程序的风格将会有很大的不同。
- en: Let's imagine that we work for a company making games for mobile phones in which
    you can purchase in-game benefits. The majority of users never buy anything, but
    a small fraction is likely to spend a lot of money. We want to build a model that
    recognizes big spenders based on their play patterns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们为一家制作手机游戏的公司工作，在这些游戏中你可以购买游戏内福利。大多数用户从不购买任何东西，但一小部分人可能会花很多钱。我们想要构建一个模型，根据他们的游戏模式识别大额消费者。
- en: The first step is to explore data, find the right features, and build a model
    based on a subset of the data. In this exploration phase, we have a clear goal
    in mind but little idea of how to get there. We want a light, flexible language
    with strong libraries to get us a working model as soon as possible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是探索数据，找到正确的特征，并在数据的一个子集上构建模型。在这个探索阶段，我们有一个明确的目标，但几乎没有关于如何实现目标的想法。我们希望有一个轻量级、灵活的语言，以及强大的库，以便尽快得到一个工作模型。
- en: 'Once we have a working model, we need to deploy it on our gaming platform to
    analyze the usage patterns of all the current users. This is a very different
    problem: we have a relatively clear understanding of the goals of the program
    and of how to get there. The challenge comes in designing software that will scale
    out to handle all the users and be robust to future changes in usage patterns.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了工作模型，我们需要将其部署到我们的游戏平台上，以分析所有当前用户的用法模式。这是一个非常不同的问题：我们对程序的目标和如何实现目标有相对清晰的理解。挑战在于设计能够扩展以处理所有用户并适应未来用法模式变化的软件。
- en: In practice, the type of software that we write typically lies on a spectrum
    ranging from a single throwaway script to production-level code that must be proof
    against future expansion and load increases. Before writing any code, the data
    scientist must understand where their software lies on this spectrum. Let's call
    this the **permanence** **spectrum**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们编写的软件类型通常位于从单个一次性脚本到必须能够抵御未来扩展和负载增加的生产级代码的连续谱上。在编写任何代码之前，数据科学家必须了解他们的软件在这个谱上的位置。让我们称这个为**持久性**谱。
- en: Why Scala?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Scala？
- en: You want to write a program that handles data. Which language should you choose?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你想编写一个处理数据的程序。你应该选择哪种语言？
- en: There are a few different options. You might choose a dynamic language such
    as Python or R or a more traditional object-oriented language such as Java. In
    this section, we will explore how Scala differs from these languages and when
    it might make sense to use it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的选择。你可能会选择一种动态语言，如Python或R，或者一种更传统的面向对象语言，如Java。在本节中，我们将探讨Scala与这些语言的差异以及何时使用它可能是有意义的。
- en: 'When choosing a language, the architect''s trade-off lies in a balance of provable
    correctness versus development speed. Which of these aspects you need to emphasize
    will depend on the application requirements and where on the permanence spectrum
    your program lies. Is this a short script that will be used by a few people who
    can easily fix any problems that arise? If so, you can probably permit a certain
    number of bugs in rarely used code paths: when a developer hits a snag, they can
    just fix the problem as it arises. By contrast, if you are developing a database
    engine that you plan on releasing to the wider world, you will, in all likelihood,
    favor correctness over rapid development. The SQLite database engine, for instance,
    is famous for its extensive test suite, with 800 times as much testing code as
    application code ([https://www.sqlite.org/testing.html](https://www.sqlite.org/testing.html)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择语言时，架构师需要在可证明的正确性和开发速度之间进行权衡。你需要强调哪些方面将取决于应用程序的要求以及你的程序在持久性谱上的位置。这是一个短脚本，将被少数人使用，他们可以轻松修复任何出现的问题？如果是这样，你可能在很少使用的代码路径中允许一定数量的错误：当开发者遇到问题时，他们可以立即修复问题。相比之下，如果你正在开发一个计划发布给更广泛世界的数据库引擎，你很可能会更重视正确性而不是快速开发。例如，SQLite数据库引擎以其广泛的测试套件而闻名，测试代码量是应用代码的800倍（[https://www.sqlite.org/testing.html](https://www.sqlite.org/testing.html)）。
- en: What matters, when estimating the *correctness* of a program, is not the perceived
    absence of bugs, it is the degree to which you can prove that certain bugs are
    absent.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在估计程序**正确性**时，重要的是不是感知到没有错误，而是你能够证明某些错误确实不存在的程度。
- en: 'There are several ways of proving the absence of bugs before the code has even
    run:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码运行之前，有几种方法可以证明不存在错误：
- en: Static type checking occurs at compile time in statically typed languages, but
    this can also be used in strongly typed dynamic languages that support type annotations
    or type hints. Type checking helps verify that we are using functions and classes
    as intended.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静态类型语言中，静态类型检查发生在编译时，但这也可以用于支持类型注解或类型提示的强类型动态语言。类型检查有助于验证我们是否按预期使用函数和类。
- en: Static analyzers and linters that check for undefined variables or suspicious
    behavior (such as parts of the code that can never be reached).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态分析器和代码检查器可以检查未定义的变量或可疑的行为（例如，代码中永远无法到达的部分）。
- en: Declaring some attributes as immutable or constant in compiled languages.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编译语言中声明某些属性为不可变或常量。
- en: Unit testing to demonstrate the absence of bugs along particular code paths.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试以证明特定代码路径上没有bug。
- en: 'There are several more ways of checking for the absence of some bugs at runtime:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以检查运行时某些错误的缺失：
- en: Dynamic type checking in both statically typed and dynamic languages
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静态类型和动态语言中均支持动态类型检查
- en: Assertions verifying supposed program invariants or expected contracts
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断言验证假设的程序不变性或预期契约
- en: In the next sections, we will examine how Scala compares to other languages
    in data science.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨Scala在数据科学领域与其他语言的比较。
- en: Static typing and type inference
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态类型和类型推断
- en: Scala's static typing system is very versatile. A lot of information as to the
    program's behavior can be encoded in types, allowing the compiler to guarantee
    a certain level of correctness. This is particularly useful for code paths that
    are rarely used. A dynamic language cannot catch errors until a particular branch
    of execution runs, so a bug can persist for a long time until the program runs
    into it. In a statically typed language, any bug that can be caught by the compiler
    will be caught at compile time, before the program has even started running.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的静态类型系统非常灵活。程序行为的大量信息可以编码在类型中，使得编译器能够保证一定程度的正确性。这对于很少使用的代码路径特别有用。动态语言无法在特定执行分支运行之前捕获错误，因此错误可能长时间存在，直到程序遇到它。在静态类型语言中，任何编译器可以捕获的bug都会在程序开始运行之前在编译时被捕获。
- en: 'Statically typed object-oriented languages have often been criticized for being
    needlessly verbose. Consider the initialization of an instance of the `Example`
    class in Java:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 静态类型面向对象语言常因冗余而被批评。以Java中`Example`类实例的初始化为例：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have to repeat the class name twice—once to define the compile-time type
    of the `myInstance` variable and once to construct the instance itself. This feels
    like unnecessary work: the compiler knows that the type of `myInstance` is `Example`
    (or a superclass of `Example`) as we are binding a value of the `Example` type.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须重复两次类名——一次是为了定义`myInstance`变量的编译时类型，另一次是为了构造实例本身。这感觉像是多余的工作：编译器知道`myInstance`的类型是`Example`（或`Example`的父类），因为我们绑定了一个`Example`类型的值。
- en: 'Scala, like most functional languages, uses type inference to allow the compiler
    to infer the type of variables from the instances bound to them. We would write
    the equivalent line in Scala as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Scala，像大多数函数式语言一样，使用类型推断来允许编译器从绑定到它们的实例中推断变量的类型。我们可以在Scala中这样写等效的行：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Scala compiler infers that `myInstance` has the `Example` type at compile
    time. A lot of the time, it is enough to specify the types of the arguments and
    of the return value of a function. The compiler can then infer types for all the
    variables defined in the body of the function. Scala code is usually much more
    concise and readable than the equivalent Java code, without compromising any of
    the type safety.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Scala编译器在编译时推断`myInstance`具有`Example`类型。很多时候，指定函数的参数和返回值的类型就足够了。然后编译器可以推断函数体中定义的所有变量的类型。Scala代码通常比等效的Java代码更简洁、更易读，而不牺牲任何类型安全性。
- en: Scala encourages immutability
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala鼓励不可变性
- en: 'Scala encourages the use of immutable objects. In Scala, it is very easy to
    define an attribute as immutable:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Scala鼓励使用不可变对象。在Scala中，定义一个属性为不可变非常容易：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The default collections are immutable:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 默认集合是不可变的：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Having immutable objects removes a common source of bugs. Knowing that some
    objects cannot be changed once instantiated reduces the number of places bugs
    can creep in. Instead of considering the lifetime of the object, we can narrow
    in on the constructor.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不可变对象消除了常见的错误来源。知道某些对象一旦实例化后就不能改变，可以减少错误可能潜入的地方。而不是考虑对象的生命周期，我们可以专注于构造函数。
- en: Scala and functional programs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala和函数式程序
- en: 'Scala encourages functional code. A lot of Scala code consists of using higher-order
    functions to transform collections. You, as a programmer, do not have to deal
    with the details of iterating over the collection. Let''s write an `occurrencesOf`
    function that returns the indices at which an element occurs in a list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Scala鼓励使用函数式代码。大量的Scala代码由使用高阶函数来转换集合组成。作为程序员，你不需要处理遍历集合的细节。让我们编写一个`occurrencesOf`函数，它返回元素在列表中出现的索引：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'How does this work? We first declare a new list, `collection.zipWithIndex`,
    whose elements are `(collection(0), 0)`, `(collection(1), 1)`, and so on: pairs
    of the collection''s elements and their indexes.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？我们首先声明一个新的列表，`collection.zipWithIndex`，其元素是`(collection(0), 0)`、`(collection(1),
    1)`等等：集合的元素及其索引的配对。
- en: We then tell Scala that we want to iterate over this collection, binding the
    `currentElem` variable to the current element and `index` to the index. We apply
    a filter on the iteration, selecting only those elements for which `currentElem
    == elem`. We then tell Scala to just return the `index` variable.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们告诉Scala我们想要遍历这个集合，将`currentElem`变量绑定到当前元素，将`index`绑定到索引。我们对迭代应用一个过滤器，只选择那些`currentElem
    == elem`的元素。然后我们告诉Scala只返回`index`变量。
- en: 'We did not need to deal with the details of the iteration process in Scala.
    The syntax is very declarative: we tell the compiler that we want the index of
    every element equal to `elem` in collection and let the compiler worry about how
    to iterate over collection.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，我们不需要处理迭代过程的细节。语法非常声明式：我们告诉编译器我们想要集合中每个等于`elem`的元素的索引，然后让编译器去担心如何遍历集合。
- en: 'Consider the equivalent in Java:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑Java中的等效代码：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In Java, you start by defining a (mutable) list in which to put occurrences
    as you find them. You then iterate over the collection by defining a counter,
    considering each element in turn and adding its index to the list of occurrences,
    if need be. There are many more moving parts that we need to get right for this
    method to work. These moving parts exist because we must tell Java how to iterate
    over the collection, and they represent a common source of bugs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中，你首先定义一个（可变的）列表，用于存放你找到的实例。然后通过定义一个计数器来遍历这个集合，依次考虑每个元素，并在需要时将其索引添加到实例列表中。为了使这个方法正常工作，我们需要正确处理许多其他部分。这些部分的存在是因为我们必须告诉Java如何遍历集合，并且它们是bug的常见来源。
- en: 'Furthermore, as a lot of code is taken up by the iteration mechanism, the line
    that defines the logic of the function is harder to find:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于大量的代码被迭代机制占用，定义函数逻辑的行更难找到：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that this is not meant as an attack on Java. In fact, Java 8 adds a slew
    of functional constructs, such as lambda expressions, the `Optional` type that
    mirrors Scala's `Option`, or stream processing. Rather, it is meant to demonstrate
    the benefit of functional approaches in minimizing the potential for errors and
    maximizing clarity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这并不是对Java的攻击。事实上，Java 8增加了一系列函数式构造，如lambda表达式、与Scala的`Option`相对应的`Optional`类型或流处理。相反，这是为了展示函数式方法在最小化错误潜力和最大化清晰度方面的好处。
- en: Null pointer uncertainty
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空指针不确定性
- en: We often need to represent the possible absence of a value. For instance, imagine
    that we are reading a list of usernames from a CSV file. The CSV file contains
    name and e-mail information. However, some users have declined to enter their
    e-mail into the system, so this information is absent. In Java, one would typically
    represent the e-mail as a string or an `Email` class and represent the absence
    of e-mail information for a particular user by setting that reference to `null`.
    Similarly, in Python, we might use `None` to demonstrate the absence of a value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要表示值的可能不存在。例如，假设我们正在从CSV文件中读取用户名列表。CSV文件包含姓名和电子邮件信息。然而，一些用户选择不将他们的电子邮件输入到系统中，因此这些信息不存在。在Java中，人们通常会使用字符串或`Email`类来表示电子邮件，并通过将那个引用设置为`null`来表示特定用户的电子邮件信息不存在。同样，在Python中，我们可能会使用`None`来表示值的缺失。
- en: This approach is dangerous because we are not encoding the possible absence
    of e-mail information. In any nontrivial program, deciding whether an instance
    attribute can be `null` requires considering every occasion in which this instance
    is defined. This quickly becomes impractical, so programmers either assume that
    a variable is not null or code too defensively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很危险，因为我们没有编码电子邮件信息的可能缺失。在任何非平凡程序中，决定实例属性是否可以为`null`需要考虑这个实例定义的每一个场合。这很快就会变得不切实际，因此程序员要么假设变量不是`null`，要么编写过于防御性的代码。
- en: 'Scala (following the lead of other functional languages) introduces the `Option[T]`
    type to represent an attribute that might be absent. We might then write the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Scala（跟随其他函数式语言的趋势）引入了`Option[T]`类型来表示可能缺失的属性。然后我们可能会写出以下内容：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have now encoded the possible absence of e-mail in the type information.
    It is obvious to any programmer using the `User` class that e-mail information
    is possibly absent. Even better, the compiler knows that the `email` field can
    be absent, forcing us to deal with the problem rather than recklessly ignoring
    it to have the application burn at runtime in a conflagration of null pointer
    exceptions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在类型信息中编码了电子邮件可能不存在的情况。对于使用`User`类的任何程序员来说，电子邮件信息可能不存在是显而易见的。更好的是，编译器知道`email`字段可能不存在，这迫使我们处理这个问题，而不是鲁莽地忽略它，让应用程序在运行时因为空指针异常而崩溃。
- en: All this goes back to achieving a certain level of provable correctness. Never
    using `null`, we know that we will never run into null pointer exceptions. Achieving
    the same level of correctness in languages without `Option[T]` requires writing
    unit tests on the client code to verify that it behaves correctly when the e-mail
    attribute is null.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都回到了实现一定程度的可证明正确性的目标。从不使用`null`，我们知道我们永远不会遇到空指针异常。在没有`Option[T]`的语言中实现相同级别的正确性需要编写单元测试来验证当电子邮件属性为`null`时客户端代码的行为是否正确。
- en: 'Note that it is possible to achieve this in Java using, for instance, Google''s
    Guava library ([https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained](https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained))
    or the `Optional` class in Java 8\. It is more a matter of convention: using `null`
    in Java to denote the absence of a value has long been the norm.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Java中，可以使用例如Google的Guava库([https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained](https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained))或Java
    8中的`Optional`类来实现这一点。这更多是一个约定：在Java中使用`null`来表示值的缺失已经很长时间是规范了。
- en: Easier parallelism
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更容易的并行性
- en: Writing programs that take advantage of parallel architectures is challenging.
    It is nevertheless necessary to tackle all but the simplest data science problems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 编写利用并行架构的程序具有挑战性。尽管如此，解决除了最简单的数据科学问题之外的所有问题仍然是必要的。
- en: Parallel programming is difficult because we, as programmers, tend to think
    sequentially. Reasoning about the order in which different events can happen in
    a concurrent program is very challenging.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程困难，因为我们作为程序员，往往倾向于按顺序思考。在并发程序中推理不同事件可能发生的顺序是非常具有挑战性的。
- en: Scala provides several abstractions that greatly facilitate the writing of parallel
    code. These abstractions work by imposing constraints on the way parallelism is
    achieved. For instance, parallel collections force the user to phrase the computation
    as a sequence of operations (such as **map**, **reduce**, and **filter**) on collections.
    Actor systems require the developer to think in terms of actors that encapsulate
    the application state and communicate by passing messages.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Scala提供了几个抽象，这些抽象极大地简化了并行代码的编写。这些抽象通过限制实现并行性的方式来工作。例如，并行集合强制用户将计算表述为对集合的操作序列（如**map**、**reduce**和**filter**）。演员系统要求开发者从封装应用程序状态并通过传递消息进行通信的演员的角度来思考。
- en: It might seem paradoxical that restricting the programmer's freedom to write
    parallel code as they please avoids many of the problems associated with concurrency.
    However, limiting the number of ways in which a program behaves facilitates thinking
    about its behavior. For instance, if an actor is misbehaving, we know that the
    problem lies either in the code for this actor or in one of the messages that
    the actor receives.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 限制程序员自由编写他们想要的并行代码似乎有些矛盾，但这可以避免与并发相关的大多数问题。然而，限制程序行为的方式有助于思考其行为。例如，如果一个演员表现不佳，我们知道问题要么在于这个演员的代码，要么在于演员收到的某个消息。
- en: 'As an example of the power afforded by having coherent, restrictive abstractions,
    let''s use parallel collections to solve a simple probability problem. We will
    calculate the probability of getting at least 60 heads out of 100 coin tosses.
    We can estimate this using Monte Carlo: we simulate 100 coin tosses by drawing
    100 random Boolean values and check whether the number of true values is at least
    60\. We repeat this until results have converged to the required accuracy, or
    we get bored of waiting.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具有一致、限制性抽象的强大功能的例子，让我们使用并行集合来解决一个简单的概率问题。我们将计算在100次抛硬币中至少得到60次正面的概率。我们可以使用蒙特卡洛方法来估计这一点：通过抽取100个随机的布尔值来模拟100次抛硬币，并检查真值的数量是否至少为60。我们重复这个过程，直到结果收敛到所需的精度，或者我们等得不耐烦了。
- en: 'Let''s run through this in a Scala console:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Scala控制台中演示这个过程：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `trial` function runs a single set of 100 throws, returning the number
    of heads:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`trial`函数运行一组100次投掷，返回正面朝上的次数：'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To get our answer, we just need to repeat `trial` as many times as we can and
    aggregate the results. Repeating the same set of operations is ideally suited
    to parallel collections:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到我们的答案，我们只需要尽可能多地重复`trial`，并汇总结果。重复相同的操作集非常适合并行集合：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The probability is thus approximately 2.5% to 3%. All we had to do to distribute
    the calculation over every CPU in our computer is use the `par` method to parallelize
    the range `(0 until nTrials)`. This demonstrates the benefits of having a coherent
    abstraction: parallel collections let us trivially parallelize any computation
    that can be phrased in terms of higher-order functions on collections.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率大约是2.5%到3%。我们只需使用`par`方法并行化范围`(0 until nTrials)`，就可以将计算分布到我们计算机的每个CPU上。这证明了具有一致抽象的好处：并行集合使我们能够轻易地将任何可以用集合上的高阶函数表述的计算并行化。
- en: Clearly, not every problem is as easy to parallelize as a simple Monte Carlo
    problem. However, by offering a rich set of intuitive abstractions, Scala makes
    writing parallel applications manageable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，并非每个问题都像简单的蒙特卡洛问题那样容易并行化。然而，通过提供丰富的直观抽象，Scala使得编写并行应用程序变得可行。
- en: Interoperability with Java
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Java的互操作性
- en: Scala runs on the Java virtual machine. The Scala compiler compiles programs
    to Java byte code. Thus, Scala developers have access to Java libraries natively.
    Given the phenomenal number of applications written in Java, both open source
    and as part of the legacy code in organizations, the interoperability of Scala
    and Java helps explain the rapid uptake of Scala.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Scala运行在Java虚拟机上。Scala编译器将程序编译成Java字节码。因此，Scala开发者可以原生地访问Java库。鉴于用Java编写的应用程序数量庞大，无论是开源的还是作为组织中的遗留代码的一部分，Scala和Java的互操作性有助于解释Scala的快速采用。
- en: 'Interoperability has not just been unidirectional: some Scala libraries, such
    as the Play framework, are becoming increasingly popular among Java developers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 互操作性不仅仅是单向的：一些Scala库，如Play框架，在Java开发者中越来越受欢迎。
- en: When not to use Scala
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时不使用Scala
- en: In the previous sections, we described how Scala's strong type system, preference
    for immutability, functional capabilities, and parallelism abstractions make it
    easy to write reliable programs and minimize the risk of unexpected behavior.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们描述了Scala的强类型系统、对不可变性的偏好、函数能力和并行抽象如何使得编写可靠的程序变得容易，并最小化意外行为的风险。
- en: What reasons might you have to avoid Scala in your next project? One important
    reason is familiarity. Scala introduces many concepts such as implicits, type
    classes, and composition using traits that might not be familiar to programmers
    coming from the object-oriented world. Scala's type system is very expressive,
    but getting to know it well enough to use its full power takes time and requires
    adjusting to a new programming paradigm. Finally, dealing with immutable data
    structures can feel alien to programmers coming from Java or Python.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有哪些理由在下一个项目中避免使用Scala？一个重要的原因是熟悉度。Scala引入了许多概念，例如隐式参数、类型类和通过特质使用组合，这些可能对来自面向对象世界的程序员来说并不熟悉。Scala的类型系统非常强大，但要充分了解它以发挥其全部功能需要时间，并需要适应新的编程范式。最后，对于来自Java或Python的程序员来说，处理不可变数据结构可能会感到不适应。
- en: Nevertheless, these are all drawbacks that can be overcome with time. Scala
    does fall short of the other data science languages in library availability. The
    IPython Notebook, coupled with matplotlib, is an unparalleled resource for data
    exploration. There are ongoing efforts to provide similar functionality in Scala
    (Spark Notebooks or Apache Zeppelin, for instance), but there are no projects
    with the same level of maturity. The type system can also be a minor hindrance
    when one is exploring data or trying out different models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些都是可以通过时间克服的缺点。Scala在库可用性方面确实不如其他数据科学语言。IPython Notebook与matplotlib结合使用，是数据探索的无与伦比的资源。有持续的努力在Scala中提供类似的功能（例如Spark
    Notebooks或Apache Zeppelin），但没有项目达到相同的成熟度。当探索数据或尝试不同的模型时，类型系统也可能成为轻微的障碍。
- en: Thus, in this author's biased opinion, Scala excels for more *permanent* programs.
    If you are writing a throwaway script or exploring data, you might be better served
    with Python. If you are writing something that will need to be reused and requires
    a certain level of provable correctness, you will find Scala extremely powerful.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the obligatory introduction is over, it is time to write some Scala
    code. In the next chapter, you will learn about leveraging Breeze for numerical
    computations with Scala. For our first foray into data science, we will use logistic
    regression to predict the gender of a person given their height and weight.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By far, the best book on Scala is *Programming in Scala* by *Martin Odersky*,
    *Lex Spoon*, and *Bill Venners*. Besides being authoritative (*Martin Odersky*
    is the driving force behind Scala), this book is also approachable and readable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '*Scala Puzzlers* by *Andrew Phillips* and *Nermin Šerifović* provides a fun
    way to learn more advanced Scala.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '*Scala for Machine Learning* by *Patrick R. Nicholas* provides examples of
    how to write machine learning algorithms with Scala.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2. Manipulating Data with Breeze
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data science is, by and large, concerned with the manipulation of structured
    data. A large fraction of structured datasets can be viewed as tabular data: each
    row represents a particular instance, and columns represent different attributes
    of that instance. The ubiquity of tabular representations explains the success
    of spreadsheet programs like Microsoft Excel, or of tools like SQL databases.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: To be useful to data scientists, a language must support the manipulation of
    columns or tables of data. Python does this through NumPy and pandas, for instance.
    Unfortunately, there is no single, coherent ecosystem for numerical computing
    in Scala that quite measures up to the SciPy ecosystem in Python.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce Breeze, a library for fast linear algebra
    and manipulation of data arrays as well as many other features necessary for scientific
    computing and data science.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Code examples
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to access the code examples in this book is to clone the GitHub
    repository:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The code samples for each chapter are in a single, standalone folder. You may
    also browse the code online on GitHub.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Installing Breeze
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have downloaded the code examples for this book, the easiest way of using
    Breeze is to go into the `chap02` directory and type `sbt console` at the command
    line. This will open a Scala console in which you can import Breeze.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build a standalone project, the most common way of installing
    Breeze (and, indeed, any Scala module) is through SBT. To fetch the dependencies
    required for this chapter, copy the following lines to a file called `build.sbt`,
    taking care to leave an empty line after `scalaVersion`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Open a Scala console in the same directory as your `build.sbt` file by typing
    `sbt console` in a terminal. You can check that Breeze is working correctly by
    importing Breeze from the Scala prompt:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Getting help on Breeze
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gives a reasonably detailed introduction to Breeze, but it does
    not aim to give a complete API reference.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: To get a full list of Breeze's functionality, consult the Breeze Wiki page on
    GitHub at [https://github.com/scalanlp/breeze/wiki](https://github.com/scalanlp/breeze/wiki).
    This is very complete for some modules and less complete for others. The source
    code ([https://github.com/scalanlp/breeze/](https://github.com/scalanlp/breeze/))
    is detailed and gives a lot of information. To understand how a particular function
    is meant to be used, look at the unit tests for that function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Basic Breeze data types
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breeze is an extensive library providing fast and easy manipulation of arrays
    of data, routines for optimization, interpolation, linear algebra, signal processing,
    and numerical integration.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The basic linear algebra operations underlying Breeze rely on the `netlib-java`
    library, which can use system-optimized **BLAS** and **LAPACK** libraries, if
    present. Thus, linear algebra operations in Breeze are often extremely fast. Breeze
    is still undergoing rapid development and can, therefore, be somewhat unstable.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Breeze makes manipulating one- and two-dimensional data structures easy. To
    start, open a Scala console through SBT and import Breeze:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s dive straight in and define a vector:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We have just defined a three-element vector, `v`. Vectors are just one-dimensional
    arrays of data exposing methods tailored to numerical uses. They can be indexed
    like other Scala collections:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'They support element-wise operations with a scalar:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'They also support element-wise operations with another vector:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Breeze makes writing vector operations intuitive and considerably more readable
    than the native Scala equivalent.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Breeze will refuse (at compile time) to coerce operands to the correct
    type:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It will also refuse (at runtime) to add vectors together if they have different
    lengths:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Basic manipulation of vectors in Breeze will feel natural to anyone used to
    working with NumPy, MATLAB, or R.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have only looked at *element-wise* operators. These are all prefixed
    with a colon. All the usual suspects are present: `:+`, `:*`, `:-`, `:/`, `:%`
    (remainder), and `:^` (power) as well as Boolean operators. To see the full list
    of operators, have a look at the API documentation for `DenseVector` or `DenseMatrix`
    ([https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet](https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet)).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides element-wise operations, Breeze vectors support the operations you
    might expect of mathematical vectors, such as the dot product:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pitfalls of element-wise operators**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the `:+` and `:-` operators for element-wise addition and subtraction
    that we have seen so far, we can also use the more traditional `+` and `-` operators:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'One must, however, be very careful with operator precedence rules when mixing
    `:+` or `:*` with `:+` operators. The `:+` and `:*` operators have very low operator
    precedence, so they will be evaluated last. This can lead to some counter-intuitive
    behavior:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By contrast, if we use `:+` instead of `+`, the mathematical precedence of
    operators is respected:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In summary, one should avoid mixing the `:+` style operators with the `+` style
    operators as much as possible.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Dense and sparse vectors and the vector trait
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the vectors we have looked at thus far have been dense vectors. Breeze also
    supports sparse vectors. When dealing with arrays of numbers that are mostly zero,
    it may be more computationally efficient to use sparse vectors. The point at which
    a vector has enough zeros to warrant switching to a sparse representation depends
    strongly on the type of operations, so you should run your own benchmarks to determine
    which type to use. Nevertheless, a good heuristic is that, if your vector is about
    90% zero, you may benefit from using a sparse representation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Sparse vectors are available in Breeze as the `SparseVector` and `HashVector`
    classes. Both these types support many of the same operations as `DenseVector`
    but use a different internal implementation. The `SparseVector` instances are
    very memory-efficient, but adding non-zero elements is slow. `HashVector` is more
    versatile, at the cost of an increase in memory footprint and computational time
    for iterating over non-zero elements. Unless you need to squeeze the last bits
    of memory out of your application, I recommend using `HashVector`. We will not
    discuss these further in this book, but the reader should find them straightforward
    to use if needed. `DenseVector`, `SparseVector`, and `HashVector` all implement
    the `Vector` trait, giving them a common interface.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Breeze remains very experimental and, as of this writing, somewhat unstable.
    I have found dealing with specific implementations of the `Vector` trait, such
    as `DenseVector` or `SparseVector`, to be more reliable than dealing with the
    `Vector` trait directly. In this chapter, we will explicitly type every vector
    as `DenseVector`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Breeze allows the construction and manipulation of two-dimensional arrays in
    a similar manner:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Building vectors and matrices
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen how to explicitly build vectors and matrices by passing their
    values to the constructor (or rather, to the companion object''s `apply` method):
    `DenseVector(1.0, 2.0, 3.0)`. Breeze offers several other powerful ways of building
    vectors and matrices:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `linspace` method (available in the `breeze.linalg` package object) creates
    a `Double` vector of equally spaced values. For instance, to create a vector of
    10 values distributed uniformly between `0` and `1`, perform the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `tabulate` method lets us construct vectors and matrices from functions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first argument to `DenseVector.tabulate` is the size of the vector, and
    the second is a function returning the value of the vector at a particular position.
    This is useful for creating ranges of data, among other things.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rand` function lets us create random vectors and matrices:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we can construct vectors from Scala arrays:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To construct vectors from other Scala collections, you must use the *splat*
    operator, `:_ *`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Advanced indexing and slicing
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen how to select a particular element in a vector `v` by its
    index with, for instance, `v(2)`. Breeze also offers several powerful methods
    for selecting parts of a vector.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a vector to play around with:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Unlike native Scala collections, Breeze vectors support negative indexing:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Breeze lets us slice the vector using a range:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Tip
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Indexing by a range returns a *view* of the original vector: when running `val
    v2 = v(1 to 3)`, no data is copied. This means that slicing is extremely efficient.
    Taking a slice of a huge vector does not increase the memory footprint at all.
    It also means that one should be careful updating a slice, since it will also
    update the original vector. We will discuss mutating vectors and matrices in a
    subsequent section in this chapter.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Breeze also lets us select an arbitrary set of elements from a vector:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This creates a `SliceVector`, which behaves like a `DenseVector` (both implement
    the `Vector` interface), but does not actually have memory allocated for values:
    it just knows how to map from its indices to values in its parent vector. One
    should think of `vSlice` as a specific view of `v`. We can materialize the view
    (give it its own data rather than acting as a lens through which `v` is viewed)
    by converting it to `DenseVector`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note that if an element of a slice is out of bounds, an exception will only
    be thrown when that element is accessed:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, one can index vectors using Boolean arrays. Let''s start by defining
    an array:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, `v(mask)` results in a view containing the elements of `v` for which
    `mask` is `true`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This can be used as a way of filtering certain elements in a vector. For instance,
    to select the elements of `v` which are less than `3.0`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Matrices can be indexed in much the same way as vectors. Matrix indexing functions
    take two arguments—the first argument selects the row(s) and the second one slices
    the column(s):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can also mix different slicing types for rows and columns:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Note how, in this case, Breeze returns a vector. In general, slicing returns
    the following objects:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: A scalar when single indices are passed as the row and column arguments
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector when the row argument is a range and the column argument is a single
    index
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector transpose when the column argument is a range and the row argument
    is a single index
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matrix otherwise
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The symbol `::` can be used to indicate *every element along a particular direction*.
    For instance, we can select the second column of `m`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Mutating vectors and matrices
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Breeze vectors and matrices are mutable. Most of the slicing operations described
    above can also be used to set elements of a vector or matrix:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We are not limited to mutating single elements. In fact, all the indexing operations
    outlined above can be used to set the elements of vectors or matrices. When mutating
    slices of vectors or matrices, use the element-wise assignment operator, `:=`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The assignment operator, `:=`, works like other element-wise operators in Breeze.
    If the right-hand side is a scalar, it will automatically be broadcast to a vector
    of the given shape:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'All element-wise operators have an update counterpart. For instance, the `:+=`
    operator acts like the element-wise addition operator `:+`, but also updates its
    left-hand operand:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Notice how the update operator updates the vector in place and returns it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We have learnt how to slice vectors and matrices in Breeze to create new views
    of the original data. These views are not independent of the vector they were
    created from—updating the view will update the underlying vector and vice-versa.
    This is best illustrated with an example:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This quickly becomes intuitive if we remember that, when we create a vector
    or matrix, we are creating a view of an underlying data array rather than creating
    the data itself:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Mutating vectors and matrices](img/image01159.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'A vector slice `v(0 to 6 by 2)` of the `v` vector is just a different view
    of the array underlying `v`. The view itself contains no data. It just contains
    pointers to the data in the original array. Internally, the view is just stored
    as a pointer to the underlying data and a recipe for iterating over that data:
    in the case of this slice, the recipe is just "start at the first element of the
    underlying data and go to the seventh element of the underlying data in steps
    of two".'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Breeze offers a `copy` function for when we want to create independent copies
    of data. In the previous example, we can construct a copy of `viewEvens` as:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can now update `copyEvens` independently of `v`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication, transposition, and the orientation of vectors
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have mostly looked at element-wise operations on vectors and matrices.
    Let's now look at matrix multiplication and related operations.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The matrix multiplication operator is `*`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Besides matrix-matrix multiplication, we can use the matrix multiplication
    operator between matrices and vectors. All vectors in Breeze are column vectors.
    This means that, when multiplying matrices and vectors together, a vector should
    be viewed as an (*n * 1*) matrix. Let''s walk through an example of matrix-vector
    multiplication. We want the following operation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix multiplication, transposition, and the orientation of vectors](img/image01160.jpeg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'By contrast, if we wanted:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix multiplication, transposition, and the orientation of vectors](img/image01161.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'We must convert `v` to a row vector. We can do this using the transpose operation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note that the type of `v.t` is `Transpose[DenseVector[_]]`. A `Transpose[DenseVector[_]]`
    behaves in much the same way as a `DenseVector` as far as element-wise operations
    are concerned, but it does not support mutation or slicing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing and feature engineering
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now discovered the basic components of Breeze. In the next few sections,
    we will apply them to real examples to understand how they fit together to form
    a robust base for data science.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: An important part of data science involves preprocessing datasets to construct
    useful features. Let's walk through an example of this. To follow this example
    and access the data, you will need to download the code examples for the book
    ([www.github.com/pbugnion/s4ds](http://www.github.com/pbugnion/s4ds)).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find, in directory `chap02/data/` of the code attached to this book,
    a CSV file with true heights and weights as well as self-reported heights and
    weights for 181 men and women. The original dataset was collected as part of a
    study on body image. Refer to the following link for more information: [http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html](http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a helper function in the package provided with the book to load the
    data into Breeze arrays:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `data` object contains five vectors, each 181 element long:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '`data.genders`: A `Char` vector describing the gender of the participants'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.heights`: A `Double` vector of the true height of the participants'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.weights`: A `Double` vector of the true weight of the participants'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.reportedHeights`: A `Double` vector of the self-reported height of the
    participants'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.reportedWeights`: A `Double` vector of the self-reported weight of the
    participants'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start by counting the number of men and women in the study. We will
    define an array that contains just `''M''` and do an element-wise comparison with
    `data.genders`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `isMale` vector is the same length as `data.genders`. It is `true` where
    the participant is male, and `false` otherwise. We can use this Boolean array
    as a mask for the other arrays in the dataset (remember that `vector(mask)` selects
    the elements of `vector` where mask is `true`). Let''s get the height of the men
    in our dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To count the number of men in our dataset, we can use the indicator function.
    This transforms a Boolean array into an array of doubles, mapping `false` to `0.0`
    and `true` to `1.0`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s calculate the `mean` height of men and women in the experiment. We can
    calculate the mean of a vector using `mean(v)`, which we can access by importing
    `breeze.stats._`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To calculate the `mean` height of the men, we can use our `isMale` array to
    slice `data.heights`; `data.heights(isMale)` is a view of the `data.heights` array
    with all the height values for the men:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'As a somewhat more involved example, let''s look at the discrepancy between
    real and reported weight for both men and women in this experiment. We can get
    an array of the percentage difference between the reported weight and the true
    weight:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Notice how Breeze's overloading of mathematical operators allows us to manipulate
    data arrays easily and elegantly.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now calculate the mean and standard deviation of this array for men:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can also calculate the fraction of men who overestimated their height:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: There are thus ten men who believe they are taller than they actually are. The
    element-wise AND operator `:&` returns a vector that is true for all indices for
    which both its arguments are true. The vector `overReportMask :& isMale` is thus
    true for all participants that are male and over-reported their height.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Breeze – function optimization
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having studied feature engineering, let's now look at the other end of the data
    science pipeline. Typically, a machine learning algorithm defines a loss function
    that is a function of a set of parameters. The value of the loss function represents
    how well the model fits the data. The parameters are then optimized to minimize
    (or maximize) the loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed Machine Learning
    with MLlib"), *Distributed Machine Learning with MLlib*, we will look at **MLlib**,
    a machine learning library that contains many well-known algorithms. Often, we
    don't need to worry about optimizing loss functions directly since we can rely
    on the machine learning algorithms provided by MLlib. It is nevertheless useful
    to have a basic knowledge of optimization.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Breeze has an `optimize` module that contains functions for finding a local
    minimum:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s create a toy function that we want to optimize:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![Breeze – function optimization](img/image01162.jpeg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: 'We can represent this function in Scala as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Most local optimizers also require the gradient of the function being optimized.
    The gradient is a vector of the same dimension as the arguments to the function.
    In our case, the gradient is:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Breeze – function optimization](img/image01163.jpeg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'We can represent the gradient in Breeze with a function that takes a vector
    argument and returns a vector of the same length:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For instance, at the point `(1, 1, 1)`, we have:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s set up the optimization problem. Breeze''s optimization methods require
    that we pass in an implementation of the `DiffFunction` trait with a single method,
    `calculate`. This method must return a tuple of the function and its gradient:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We are now ready to run the optimization. The optimize module provides a `minimize`
    function that does just what we want. We pass it `optTrait` and a starting point
    for the optimization:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The true minimum is at `(0.0, 0.0, 0.0)`. The optimizer therefore correctly
    finds the minimum.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The `minimize` function uses the **L-BFGS** method to run the optimization by
    default. It takes several additional arguments to control the optimization. We
    will explore these in the next sections.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Numerical derivatives
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we specified the gradient of `f` explicitly. While
    this is generally good practice, calculating the gradient of a function can often
    be tedious. Breeze provides a gradient approximation function using finite differences.
    Reusing the same objective function `def f(xs:DenseVector[Double]) = sum(xs :^
    2.0)` as in the previous section:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The trait `approxOptTrait` has a `gradientAt` method that returns an approximation
    to the gradient at a point:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Note that this can be quite inaccurate. The `ApproximateGradientFunction` constructor
    takes an `epsilon` optional argument that controls the size of the step taken
    when calculating the finite differences. Changing the value of `epsilon` can improve
    the accuracy of the finite difference algorithm.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ApproximateGradientFunction` instance implements the `DiffFunction` trait.
    It can therefore be passed to `minimize` directly:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This, again, gives a result close to zero, but somewhat further away than when
    we specified the gradient explicitly. In general, it will be significantly more
    efficient and more accurate to calculate the gradient of a function analytically
    than to rely on Breeze's numerical gradient. It is probably best to only use the
    numerical gradient during data exploration or to check analytical gradients.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `minimize` function takes many optional arguments relevant to machine learning
    algorithms. In particular, we can instruct the optimizer to use a regularization
    parameter when performing the optimization. Regularization introduces a penalty
    in the loss function to prevent the parameters from growing arbitrarily. This
    is useful to avoid overfitting. We will discuss regularization in greater detail
    in [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed Machine Learning
    with MLlib"), *Distributed Machine Learning with MLlib*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, to use `L2Regularization` with a hyperparameter of `0.5`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The regularization makes no difference in this case, since the parameters are
    zero at the minimum.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: To see a list of optional arguments that can be passed to `minimize`, consult
    the Breeze documentation online.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: An example – logistic regression
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now imagine we want to build a classifier that takes a person''s **height**
    and **weight** and assigns a probability to their being **Male** or **Female**.
    We will reuse the height and weight data introduced earlier in this chapter. Let''s
    start by plotting the dataset:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01164.jpeg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: Height versus weight data for 181 men and women
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: There are many different algorithms for classification. A first glance at the
    data shows that we can, approximately, separate men from women by drawing a straight
    line across the plot. A linear method is therefore a reasonable initial attempt
    at classification. In this section, we will use logistic regression to build a
    classifier.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: A detailed explanation of logistic regression is beyond the scope of this book.
    The reader unfamiliar with logistic regression is referred to *The Elements of
    Statistical Learning* by *Hastie*, *Tibshirani*, and *Friedman*. We will just
    give a brief summary here.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression estimates the probability of a given *height* and *weight*
    belonging to a *male* with the following sigmoid function:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01165.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f* is a linear function:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01166.jpeg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![An example – logistic regression](img/image01167.jpeg) is an array
    of parameters that we need to determine using the training set. If we consider
    the height and weight as a *features = (height, weight)* matrix, we can re-write
    the sigmoid kernel *f* as a matrix multiplication of the *features* matrix with
    the *params* vector:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01168.jpeg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'To simplify this expression further, it is common to add a dummy feature whose
    value is always *1* to the *features* matrix. We can then multiply *params(0)*
    by this feature, allowing us to write the entire sigmoid kernel *f* as a single
    matrix-vector multiplication:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01169.jpeg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: The feature matrix, *features*, is now a (*181 * 3*) matrix, where each row
    is *(1, height, weight)* for a particular participant.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the optimal values of the parameters, we can maximize the likelihood
    function, *L(params|features)*. The likelihood takes a given set of parameter
    values as input and returns the probability that these particular parameters gave
    rise to the training set. For a set of parameters and associated probability function
    *P(male|features[i]),* the likelihood is:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01170.jpeg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: If we magically know, ahead of time, the gender of everyone in the population,
    we can assign *P(male)=1* for the men and *P(male)=0* for the women. The likelihood
    function would then be **1**. Conversely, any uncertainty leads to a reduction
    in the likelihood function. If we choose a set of parameters that consistently
    lead to classification errors (low *P(male)* for men or high *P(male)* for women),
    the likelihood function drops to *0*.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: The maximum likelihood corresponds to those values of the parameters most likely
    to describe the observed data. Thus, to find the parameters that best describe
    our training set, we just need to find parameters that maximize *L(params|features)*.
    However, maximizing the likelihood function itself is very rarely done, since
    it involves multiplying many small values together, which quickly leads to floating
    point underflow. It is best to maximize the log of the likelihood, which has the
    same maximum as the likelihood. Finally, since most optimization algorithms are
    geared to minimize a function rather than maximize it, we will minimize![An example
    – logistic regression](img/image01171.jpeg).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'For logistic regression, this is equivalent to minimizing:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01172.jpeg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: Here, the sum runs over all participants in the training data, ![An example
    – logistic regression](img/image01173.jpeg) is a vector ![An example – logistic
    regression](img/image01174.jpeg) of the *i*-th observation in the training set,
    and ![An example – logistic regression](img/image01175.jpeg) is *1* if the person
    is male, and *0* if the participant is female.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize the *Cost* function, we must also know its gradient with respect
    to the parameters. This is:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01176.jpeg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: We will start by rescaling the height and weight by their mean and standard
    deviation. While this is not strictly necessary for logistic regression, it is
    generally good practice. It facilitates the optimization and would become necessary
    if we wanted to use regularization methods or build superlinear features (features
    that allow the boundary separating men from women to be curved rather than a straight
    line).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will move away from the Scala shell and write a standalone
    Scala script. Here''s the full code listing. Don''t worry if this looks daunting.
    We will break it up into manageable chunks in a minute:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'That was a mouthful! Let''s take this one step at a time. After the obvious
    imports, we start with:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'By extending the built-in `App` trait, we tell Scala to treat the entire object
    as a `main` function. This just cuts out `def main(args:Array[String])` boilerplate.
    We then load the data and rescale the height and weight to have a `mean` of zero
    and a standard deviation of one:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The `rescaledHeights` and `rescaledWeights` vectors will be the features of
    our model. We can now build the training set matrix for this model. This is a
    (*181 * 3*) matrix, for which the *i*-th row is `(1, height(i), weight(i))`, corresponding
    to the values of the height and weight for the *i*th participant. We start by
    transforming both `rescaledHeights` and `rescaledWeights` from vectors to (*181
    * 1*) matrices
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We must also create a (*181 * 1*) matrix containing just *1* to act as the
    dummy feature. We can do this using:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We now need to combine our three (*181 * 1*) matrices together into a single
    feature matrix of shape (*181 * 3*). We can use the `horzcat` method to concatenate
    the three matrices together:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The final step in the data preprocessing stage is to create the target variable.
    We need to convert the `data.genders` vector to a vector of ones and zeros. We
    assign a value of one for men and zero for women. Thus, our classifier will predict
    the probability that any given person is male. We will use the `.values.map` method,
    a method equivalent to the `.map` method on Scala collections:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Note that we could also have used the indicator function which we discovered
    earlier:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This results in the allocation of a temporary array, `maleVector`, and might
    therefore increase the program's memory footprint if there were many participants
    in the experiment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a matrix representing the training set and a vector denoting the
    target variable. We can write the loss function that we want to minimize. As mentioned
    previously, we will minimize ![An example – logistic regression](img/image01177.jpeg).
    The loss function takes as input a set of values for the linear coefficients and
    returns a number indicating how well those values of the linear coefficients fit
    the training data:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Note that we use `log1p(x)` to calculate *log(1+x)*. This is robust to underflow
    for small values of `x`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the cost function:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: We can see that the cost function is somewhat lower for slightly positive values
    of the height and weight parameters. This indicates that the likelihood function
    is larger for slightly positive values of the height and weight. This, in turn,
    implies (as we expect from the plot) that people who are taller and heavier than
    average are more likely to be male.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need a function that calculates the gradient of the loss function,
    since that will help with the optimization:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Having defined the loss function and gradient, we are now in a position to
    set up the optimization:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'All that is left now is to run the optimization. The cost function for logistic
    regression is convex (it has a single minimum), so the starting point for optimization
    is irrelevant in principle. In practice, it is common to start with a coefficient
    vector that is zero everywhere (equating to assigning a 0.5 probability of being
    male to every participant):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This returns the vector of optimal parameters:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How can we interpret the values of the optimal parameters? The coefficients
    for the height and weight are both positive, indicating that people who are taller
    and heavier are more likely to be male.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get the decision boundary (the line separating (height, weight)
    pairs more likely to belong to a woman from (height, weight) pairs more likely
    to belong to a man) directly from the coefficients. The decision boundary is:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '![An example – logistic regression](img/image01178.jpeg)![An example – logistic
    regression](img/image01179.jpeg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: Height and weight data (shifted by the mean and rescaled by the standard deviation).
    The orange line is the logistic regression decision boundary. Logistic regression
    predicts that individuals above the boundary are male.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Towards re-usable code
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we performed all of the computation in a single script.
    While this is fine for data exploration, it means that we cannot reuse the logistic
    regression code that we have built. In this section, we will start the construction
    of a machine learning library that you can reuse across different projects.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'We will factor the logistic regression algorithm out into its own class. We
    construct a `LogisticRegression` class:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The class takes, as input, a matrix representing the training set and a vector
    denoting the target variable. Notice how we assign these to `vals`, meaning that
    they are set on class creation and will remain the same until the class is destroyed.
    Of course, the `DenseMatrix` and `DenseVector` objects are mutable, so the values
    that `training` and `target` point to might change. Since programming best practice
    dictates that mutable state makes reasoning about program behavior difficult,
    we will avoid taking advantage of this mutability.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a method that calculates the cost function and its gradient:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We are now all set up to run the optimization to calculate the coefficients
    that best reproduce the training set. In traditional object-oriented languages,
    we might define a `getOptimalCoefficients` method that returns a `DenseVector`
    of the coefficients. Scala, however, is more elegant. Since we have defined the
    `training` and `target` attributes as `vals`, there is only one possible set of
    values of the optimal coefficients. We could, therefore, define a `val optimalCoefficients
    = ???` class attribute that holds the optimal coefficients. The problem with this
    is that it forces all the computation to happen when the instance is constructed.
    This will be unexpected for the user and might be wasteful: if the user is only
    interested in accessing the cost function, for instance, the time spent minimizing
    it will be wasted. The solution is to use a `lazy val`. This value will only be
    evaluated when the client code requests it:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'To help with the calculation of the coefficients, we will define a private
    helper method:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: We have refactored the logistic regression into its own class, that we can reuse
    across different projects.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: If we were planning on reusing the height-weight data, we could, similarly,
    refactor it into a class of its own that facilitates data loading, feature scaling,
    and any other functionality that we find ourselves reusing often.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives to Breeze
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Breeze is the most feature-rich and approachable Scala framework for linear
    algebra and numeric computation. However, do not take my word for it: experiment
    with other libraries for tabular data. In particular, I recommend trying *Saddle*,
    which provides a `Frame` object similar to data frames in pandas or R. In the
    Java world, the *Apache Commons Maths library* provides a very rich toolkit for
    numerical computation. In [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark*, [Chapter
    11](part0106.xhtml#aid-352RK2 "Chapter 11. Spark SQL and DataFrames"), *Spark
    SQL and DataFrames*, and [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed
    Machine Learning with MLlib"), *Distributed Machine Learning with MLlib*, we will
    explore *Spark* and *MLlib*, which allow the user to run distributed machine learning
    algorithms.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes our brief overview of Breeze. We have learned how to manipulate
    basic Breeze data types, how to use them for linear algebra, and how to perform
    convex optimization. We then used our knowledge to clean a real dataset and performed
    logistic regression on it.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss breeze-viz, a plotting library for Scala.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Elements of Statistical Learning*, by *Hastie*, *Tibshirani*, and *Friedman*,
    gives a lucid, practical description of the mathematical underpinnings of machine
    learning. Anyone aspiring to do more than mindlessly apply machine learning algorithms
    as black boxes ought to have a well-thumbed copy of this book.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '*Scala for Machine Learning*, by *Patrick R. Nicholas*, describes practical
    implementations of many useful machine learning algorithms in Scala.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: The Breeze documentation ([https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart)),
    API docs ([http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package)),
    and source code ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze))
    provide the most up-to-date sources of documentation on Breeze.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3. Plotting with breeze-viz
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data visualization is an integral part of data science. Visualization needs
    fall into two broad categories: during the development and validation of new models
    and, at the end of the pipeline, to distill meaning from the data and the models
    to provide insight to external stakeholders.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'The two types of visualizations are quite different. At the data exploration
    and model development stage, the most important feature of a visualization library
    is its ease of use. It should take as few steps as possible to go from having
    data as arrays of numbers (or CSVs or in a database) to having data displayed
    on a screen. The lifetime of graphs is also quite short: once the data scientist
    has learned all he can from the graph or visualization, it is normally discarded.
    By contrast, when developing visualization widgets for external stakeholders,
    one is willing to tolerate increased development time for greater flexibility.
    The visualizations can have significant lifetime, especially if the underlying
    data changes over time.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: The tool of choice in Scala for the first type of visualization is breeze-viz.
    When developing visualizations for external stakeholders, web-based visualizations
    (such as D3) and Tableau tend to be favored.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore breeze-viz. In [Chapter 14](part0139.xhtml#aid-44HU61
    "Chapter 14. Visualization with D3 and the Play Framework"), *Visualization with
    D3 and the Play Framework*, we will learn how to build Scala backends for JavaScript
    visualizations.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Breeze-viz is (no points for guessing) Breeze's visualization library. It wraps
    **JFreeChart**, a very popular Java charting library. Breeze-viz is still very
    experimental. In particular, it is much less feature-rich than matplotlib in Python,
    or R or MATLAB. Nevertheless, breeze-viz allows access to the underlying JFreeChart
    objects so one can always fall back to editing these objects directly. The syntax
    for breeze-viz is inspired by MATLAB and matplotlib.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Diving into Breeze
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started. We will work in the Scala console, but a program similar
    to this example is available in `BreezeDemo.scala` in the examples corresponding
    to this chapter. Create a `build.sbt` file with the following lines:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Start an `sbt` console:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let''s start by plotting a sigmoid curve, ![Diving into Breeze](img/image01180.jpeg).
    We will first generate the data using Breeze. Recall that the `linspace` method
    creates a vector of doubles, uniformly distributed between two values:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We now have the data ready for plotting. The first step is to create a figure:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This creates an empty Java Swing window (which may appear on your taskbar or
    equivalent). A figure can contain one or more plots. Let''s add a plot to our
    figure:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'For now, let''s ignore the `0` passed as argument to `.subplot`. We can add
    data points to our `plot`:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The `plot` function takes two arguments, corresponding to the *x* and *y* values
    of the data series to be plotted. To view the changes, you need to refresh the
    figure:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Look at the Swing window now. You should see a beautiful sigmoid, similar to
    the one below. Right-clicking on the window lets you interact with the plot and
    save the image as a PNG:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving into Breeze](img/image01181.jpeg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'You can also save the image programmatically as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Breeze-viz currently only supports exporting to PNG.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Customizing plots
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have a curve on our chart. Let''s add a few more:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Looking at the figure now, you should see all three curves in different colors.
    Notice that we named the data series as we added them to the plot, using the `name=""`
    keyword argument. To view the names, we must set the `legend` attribute:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '![Customizing plots](img/image01182.jpeg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: 'Our plot still leaves a lot to be desired. Let''s start by restricting the
    range of the *x* axis to remove the bands of white space on either side of the
    plot:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Now, notice how, while the *x* ticks are sensibly spaced, there are only two
    *y* ticks: at *0* and *1*. It would be useful to have ticks every *0.1* increment.
    Breeze does not provide a way to set this directly. Instead, it exposes the underlying
    JFreeChart Axis object belonging to the current plot:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The `Axis` object supports a `.setTickUnit` method that lets us set the tick
    spacing:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: JFreeChart allows extensive customization of the `Axis` object. For a full list
    of methods available, consult the JFreeChart documentation ([http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html](http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html)).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also add a vertical line at *x=0* and a horizontal line at *f(x)=1*.
    We will need to access the underlying JFreeChart plot to add these lines. This
    is available (somewhat confusingly) as the `.plot` attribute in our Breeze `Plot`
    object:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We can use the `.addDomainMarker` and `.addRangeMarker` methods to add vertical
    and horizontal lines to JFreeChart `XYPlot` objects:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Let''s also add labels to the axes:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'If you have run all these commands, you should have a graph that looks like
    this:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing plots](img/image01183.jpeg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
- en: We now know how to customize the basic building blocks of a graph. The next
    step is to learn how to change how curves are drawn.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the line type
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have just plotted lines using the default settings. Breeze lets us
    customize how lines are drawn, at least to some extent.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will use the height-weight data discussed in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze*.
    We will use the Scala shell here for demonstrative purposes, but you will find
    a program in `BreezeDemo.scala` that follows the example shell session.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The code examples for this chapter come with a module for loading the data,
    `HWData.scala`, that loads the data from the CSVs:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Let''s create a scatter plot of the heights against the weights:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'This produces a scatter-plot of the height-weight data:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing the line type](img/image01184.jpeg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
- en: 'Note that we passed a third argument to the `plot` method, `''+''`. This controls
    the plotting style. As of this writing, there are three available styles: `''-''`
    (the default), `''+''`, and `''.''`. Experiment with these to see what they do.
    Finally, we pass a `colorcode="black"` argument to control the color of the line.
    This is either a color name or an RGB triple, written as a string. Thus, to plot
    red points, we could have passed `colorcode="[255,0,0]"`.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the height-weight plot, there is clearly a trend between height
    and weight. Let''s try and fit a straight line through the data points. We will
    fit the following function:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing the line type](img/image01185.jpeg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scientific literature suggests that it would be better to fit something more
    like ![Customizing the line type](img/image01186.jpeg). You should find it straightforward
    to fit a quadratic line to the data, should you wish to.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Breeze''s least squares function to find the values of `a` and
    `b`. The `leastSquares` method expects an input matrix of features and a target
    vector, just like the `LogisticRegression` class that we defined in the previous
    chapter. Recall that in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating
    Data with Breeze"), *Manipulating Data with Breeze*, when we prepared the training
    set for logistic regression classification, we introduced a dummy feature that
    was one for every participant to provide the degree of freedom for the *y* intercept.
    We will use the same approach here. Our feature matrix, therefore, contains two
    columns—one that is `1` everywhere and one for the height:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The `leastSquares` method returns an instance of `LeastSquareRegressionResult`,
    which contains a `coefficients` attribute containing the coefficients that best
    fit the data:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The best-fit line is therefore:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing the line type](img/image01187.jpeg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: 'Let''s extract the coefficients. An elegant way of doing this is to use Scala''s
    pattern matching capabilities:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: By writing `val Array(a, b) = ...`, we are telling Scala that the right-hand
    side of the expression is a two-element array and to bind the first element of
    that array to the value `a` and the second to the value `b`. See [Appendix](part0149.xhtml#aid-4E33Q2
    "Appendix A. Pattern Matching and Extractors"), *Pattern Matching and Extractors*,
    for a discussion of pattern matching.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add the best-fit line to our graph. We start by generating evenly-spaced
    dummy height values:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Let''s also add the equation for the best-fit line to the graph as an annotation.
    We will first generate the label:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'To add an annotation, we must access the underlying JFreeChart plot:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The `XYTextAnnotation` constructor takes three parameters: the annotation string
    and a pair of (*x*, *y*) coordinates defining the centre of the annotation on
    the graph. The coordinates of the annotation are expressed in the coordinate system
    of the data. Thus, calling `new XYTextAnnotation(label, 175.0, 105.0)` generates
    an annotation whose centroid is at the point corresponding to a height of 175
    cm and weight of 105 kg:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing the line type](img/image01188.jpeg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
- en: More advanced scatter plots
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breeze-viz offers a `scatter` function that adds a significant degree of customization
    to scatter plots. In particular, we can use the size and color of the marker points
    to add additional dimensions of information to the plot.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: The `scatter` function takes, as its first two arguments, collections of *x*
    and *y* points. The third argument is a function mapping an integer `i` to a `Double`
    indicating the size of the *ith* point. The size of the point is measured in units
    of the *x* axis. If you have the sizes as a Scala collection or a Breeze vector,
    you can use that collection's `apply` method as the function. Let's see how this
    works in practice.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the previous examples, we will use the REPL, but you can find a sample
    program in `BreezeDemo.scala`:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Selecting custom colors works in a similar manner: we pass in a `colors` argument
    that maps an integer index to a `java.awt.Paint` object. Using these directly
    can be cumbersome, so Breeze provides some default palettes. For instance, the
    `GradientPaintScale` maps doubles in a given domain to a uniform color gradient.
    Let''s map doubles in the range `0.0` to `1.0` to the colors between red and green:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Besides the `GradientPaintScale`, breeze-viz provides a `CategoricalPaintScale`
    class for categorical palettes. For an overview of the different palettes, consult
    the source file `PaintScale.scala` at `scala`: [https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala).'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our newfound knowledge to draw a multicolor scatter plot. We will
    assume the same initialization as the previous example. We will assign a random
    color to each point:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '![More advanced scatter plots](img/image01189.jpeg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: Multi-plot example – scatterplot matrix plots
  id: totrans-481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to have several plots in the same figure.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 'The key new method that allows multiple plots in the same figure is `fig.subplot(nrows,
    ncols, plotIndex)`. This method, an overloaded version of the `fig.subplot` method
    we have been using up to now, both sets the number of rows and columns in the
    figure and returns a specific subplot. It takes three arguments:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '`nrows`: The number of rows of subplots in the figure'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ncols`: The number of columns of subplots in the figure'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plotIndex`: The index of the plot to return'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Users familiar with MATLAB or matplotlib will note that the `.subplot` method
    is identical to the eponymous methods in these frameworks. This might seem a little
    complex, so let''s look at an example (you will find the code for this in `BreezeDemo.scala`):'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Running this example produces the following plot:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-plot example – scatterplot matrix plots](img/image01190.jpeg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
- en: Now that we have a basic grasp of how to add several subplots to the same figure,
    let's do something a little more interesting. We will write a class to draw scatterplot
    matrices. These are useful for exploring correlations between different features.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with scatterplot matrices, have a look at the figure
    at the end of this section for an idea of what we are constructing. The idea is
    to build a square matrix of scatter plots for each pair of features. Element (*i*,
    *j*) in the matrix is a scatter plot of feature *i* against feature *j*. Since
    a scatter plot of a variable against itself is of limited use, one normally draws
    histograms of each feature along the diagonal. Finally, since a scatter plot of
    feature *i* against feature *j* contains the same information as a scatter plot
    of feature *j* against feature *i*, one normally only plots the upper triangle
    or the lower triangle of the matrix.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by writing functions for the individual plots. These will take
    a `Plot` object referencing the correct subplot and vectors of the data to plot:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Notice the use of `hist(data)` to draw a histogram. The argument to `hist` must
    be a vector of data points. The `hist` method will bin these and represent them
    as a histogram.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the machinery for drawing individual plots, we just need to
    wire everything together. The tricky part is to know how to select the correct
    subplot for a given row and column position in the matrix. We can select a single
    plot by calling `fig.subplot(nrows, ncolumns, plotIndex)`, but translating from
    a (*row*, *column*) index pair to a single `plotIndex` is not obvious. The plots
    are numbered in increasing order, first from left to right, then from top to bottom:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Let''s write a short function to select a plot at a (*row*, *column*) index
    pair:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'We are now in a position to draw the matrix plot itself:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Let''s write an example for our class. We will use the height-weight data again:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Running this through SBT produces the following plot:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-plot example – scatterplot matrix plots](img/image01191.jpeg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
- en: Managing without documentation
  id: totrans-506
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Breeze-viz is unfortunately rather poorly documented. This can make the learning
    curve somewhat steep. Fortunately, it is still quite a small project: at the time
    of writing, there are just ten source files ([https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot](https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot)).
    A good way to understand exactly what breeze-viz does is to read the source code.
    For instance, to see what methods are available on a `Plot` object, read the source
    file `Plot.scala`. If you need functionality beyond that provided by Breeze, consult
    the documentation for JFreeChart to discover if you can implement what you need
    by accessing the underlying JFreeChart objects.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: Breeze-viz reference
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Writing a reference in a programming book is a dangerous exercise: you quickly
    become out of date. Nevertheless, given the paucity of documentation for breeze-viz,
    this section becomes more relevant – it is easier to compete against something
    that does not exist. Take this section with a pinch of salt, and if a command
    in this section does not work, head over to the source code:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '| Command | Description |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| `plt += plot(xs, ys)` | This plots a series of (`xs`, `ys`) values. The `xs`
    and `ys` values must be collection-like objects (Breeze vectors, Scala arrays,
    or lists, for instance). |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| `plt += scatter(xs, ys, size)``plt += scatter(xs, ys, size, color)` | This
    plots a series of (`xs`, `ys`) values as a scatter plot. The `size` argument is
    an `(Int) => Double` function mapping the index of a point to its size (in the
    same units as the *x* axis). The `color` argument is an `(Int) => java.awt.Paint`
    function mapping from integers to colors. Read the *more advanced scatter plots*
    section for further details. |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| `plt += hist(xs)``plt += hist(xs, bins=10)` | This bins `xs` and plots a
    histogram. The `bins` argument controls the number of bins. |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| `plt += image(mat)` | This plots an image or matrix. The `mat` argument should
    be `Matrix[Double]`. Read the `package.scala` source file in `breeze.plot` for
    details ([https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala)).
    |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: 'It is also useful to summarize the options available on a `plot` object:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Description |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| `plt.xlabel = "x-label"``plt.ylabel = "y-label"` | This sets the axis label
    |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| `plt.xlim = (0.0, 1.0)``plt.ylim = (0.0, 1.0)` | This sets the axis maximum
    and minimum value |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| `plt.logScaleX = true``plt.logScaleY = true` | This switches the axis to
    a log scale |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| `plt.title = "title"` | This sets the plot title |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: Data visualization beyond breeze-viz
  id: totrans-523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Other tools for data visualization in Scala are emerging: Spark notebooks ([https://github.com/andypetrella/spark-notebook#description](https://github.com/andypetrella/spark-notebook#description))
    based on the IPython notebook and Apache Zeppelin ([https://zeppelin.incubator.apache.org](https://zeppelin.incubator.apache.org)).
    Both of these rely on Apache Spark, which we will explore later in this book.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to draw simple charts with breeze-viz. In the
    last chapter of this book, we will learn how to build interactive visualizations
    using JavaScript libraries.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about basic Scala concurrency constructs—specifically, parallel
    collections.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4. Parallel Collections and Futures
  id: totrans-528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science often involves processing medium or large amounts of data. Since
    the previously exponential growth in the speed of individual CPUs has slowed down
    and the amount of data continues to increase, leveraging computers effectively
    must entail parallel computation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at ways of parallelizing computation and data
    processing over a single computer. Virtually all new computers have more than
    one processing unit, and distributing a calculation over these cores can be an
    effective way of hastening medium-sized calculations.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing calculations over a single chip is suitable for calculations involving
    gigabytes or a few terabytes of data. For larger data flows, we must resort to
    distributing the computation over several computers in parallel. We will discuss
    Apache Spark, a framework for parallel data processing in [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will look at three common ways of leveraging parallel architectures
    in a single machine: parallel collections, futures, and actors. We will consider
    the first two in this chapter, and leave the study of actors to [Chapter 9](part0077.xhtml#aid-29DRA1
    "Chapter 9. Concurrency with Akka"), *Concurrency with Akka*.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Parallel collections
  id: totrans-533
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel collections offer an extremely easy way to parallelize independent
    tasks. The reader, being familiar with Scala, will know that many tasks can be
    phrased as operations on collections, such as *map*, *reduce*, *filter*, or *groupBy*.
    Parallel collections are an implementation of Scala collections that parallelize
    these operations to run over several threads.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with an example. We want to calculate the frequency of occurrence
    of each letter in a sentence:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Let''s start by converting our sentence from a string to a vector of characters:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'We can now convert `characters` to a *parallel* vector, a `ParVector`. To do
    this, we use the `par` method:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '`ParVector` collections support the same operations as regular vectors, but
    their methods are executed in parallel over several threads.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by filtering out the spaces in `charactersPar`:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Notice how Scala hides the execution details. The `filter` operation was performed
    using multiple threads, and you barely even noticed! The interface and behavior
    of a parallel vector is identical to its serial counterpart, save for a few details
    that we will explore in the next section.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use the `toLower` function to make the letters lowercase:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'As before, the `map` method was applied in parallel. To find the frequency
    of occurrence of each letter, we use the `groupBy` method to group characters
    into vectors containing all the occurrences of that character:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Note how the `groupBy` method has created a `ParMap` instance, the parallel
    equivalent of an immutable map. To get the number of occurrences of each letter,
    we do a `mapValues` call on `intermediateMap`, replacing each vector by its length:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Congratulations! We've written a multi-threaded algorithm for finding the frequency
    of occurrence of each letter in a few lines of code. You should find it straightforward
    to adapt this to find the frequency of occurrence of each word in a document,
    a common preprocessing problem for analyzing text data.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel collections make it very easy to parallelize some operation pipelines:
    all we had to do was call `.par` on the `characters` vector. All subsequent operations
    were parallelized. This makes switching from a serial to a parallel implementation
    very easy.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of parallel collections
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Part of the power and the appeal of parallel collections is that they present
    the same interface as their serial counterparts: they have a `map` method, a `foreach`
    method, a `filter` method, and so on. By and large, these methods work in the
    same way on parallel collections as they do in serial. There are, however, some
    notable caveats. The most important one has to do with side effects. If an operation
    on a parallel collection has a side effect, this may result in a race condition:
    a situation in which the final result depends on the order in which the threads
    perform their operations.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: 'Side effects in collections arise most commonly when we update a variable defined
    outside of the collection. To give a trivial example of unexpected behavior, let''s
    define a `count` variable and increment it a thousand times using a parallel range:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'What happened here? The function passed to `foreach` has a side effect: it
    increments `count`, a variable outside of the scope of the function. This is a
    problem because the `+=` operator is a sequence of two operations:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the value of `count` and add one to it
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign the result back to `count`
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand why this causes unexpected behavior, let''s imagine that the
    `foreach` loop has been parallelized over two threads. **Thread A** might read
    the **count** variable when it is **832** and add one to it to give **833**. Before
    it has time to reassign **833** to **count**, **Thread B** reads **count**, still
    at **832**, and adds one to give **833**. **Thread A** then assigns **833** to
    **count**. **Thread B** then assigns **833** to **count**. We''ve run through
    two updates but only incremented the count by one. The problem arises because
    `+=` can be separated into two instructions: it is not *atomic*. This leaves room
    for threads to interleave their operations:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '![Limitations of parallel collections](img/image01192.jpeg)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
- en: 'The anatomy of a race condition: both thread A and thread B are trying to update
    `count` concurrently, resulting in one of the updates being overwritten. The final
    value of `count` is 833 instead of 834.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a somewhat more realistic example of problems caused by non-atomicity,
    let''s look at a different method for counting the frequency of occurrence of
    each letter in our sentence. We define a mutable `Char -> Int` hash map outside
    of the loop. Each time we encounter a letter, we increment the corresponding integer
    in the map:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: The discrepancy occurs because of the non-atomicity of the operations in the
    `foreach` loop.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, it is good practice to avoid side effects in higher-order functions
    on collections. They make the code harder to understand and preclude switching
    from serial to parallel collections. It is also good practice to avoid exposing
    mutable state: immutable objects can be shared freely between threads and cannot
    be affected by side effects.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: 'Another limitation of parallel collections occurs in reduction (or folding)
    operations. The function used to combine items together must be *associative*.
    For instance:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'The *minus* operator, `–`, is not associative. The order in which consecutive
    operations are applied matters: `(a – b) – c` is not the same as `a – (b – c)`.
    The function used to reduce a parallel collection must be associative because
    the order in which the reduction occurs is not tied to the order of the collection.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In single-threaded programs, exception handling is relatively straightforward:
    if an exception occurs, the function can either handle it or escalate it. This
    is not nearly as obvious when parallelism is introduced: a single thread might
    fail, but the others might return successfully.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel collection methods will throw an exception if they fail on any element,
    just like their serial counterparts:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: There are cases when this isn't the behavior that we want. For instance, we
    might be using a parallel collection to retrieve a large number of web pages in
    parallel. We might not mind if a few of the pages cannot be fetched.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: 'Scala''s `Try` type was designed for sandboxing code that might throw exceptions.
    It is similar to `Option` in that it is a one-element container:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Unlike the `Option` type, which indicates whether an expression has a useful
    value, the `Try` type indicates whether an expression can be executed without
    throwing an exception. It takes on the following two values:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '`Try { 2 + 2 } == Success(4)` if the expression in the `Try` statement is evaluated
    successfully'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Try { 2 / 0 } == Failure(java.lang.ArithmeticException: / by zero)` if the
    expression in the `Try` block results in an exception'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This will make more sense with an example. To see the `Try` type in action,
    we will try to fetch web pages in a fault tolerant manner. We will use the built-in
    `Source.fromURL` method which fetches a web page and opens an iterator of the
    page''s content. If it fails to fetch the web page, it throws an error:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Instead of letting the expression propagate out and crash the rest of our code,
    we can wrap the call to `Source.fromURL` in `Try`:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'To see the power of our `Try` statement, let''s now retrieve a list of URLs
    in parallel in a fault tolerant manner:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'We can then use a `collect` statement to act on the pages we could fetch successfully.
    For instance, to get the number of characters on each page:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: By making good use of Scala's built-in `Try` classes and parallel collections,
    we have built a fault tolerant, multithreaded URL retriever in a few lines of
    code. (Compare this to the myriad of Java/C++ books that prefix code examples
    with 'error handling is left out for clarity'.)
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The Try type versus try/catch statements**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Programmers with imperative or object-oriented backgrounds will be more familiar
    with try/catch blocks for handling exceptions. We could have accomplished similar
    functionality here by wrapping the code for fetching URLs in a try block, returning
    null if the call raises an exception.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 'However, besides being more verbose, returning null is less satisfactory: we
    lose all information about the exception and null is less expressive than `Failure(exception)`.
    Furthermore, returning a `Try[T]` type forces the caller to consider the possibility
    that the function might fail, by encoding this possibility in the type of the
    return value. In contrast, just returning `T` and coding failure with a null value
    allows the caller to ignore failure, raising the possibility of a confusing `NullPointerException`
    being thrown at a completely different point in the program.'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: In short, `Try[T]` is just another higher-order type, like `Option[T]` or `List[T]`.
    Treating the possibility of failure in the same way as the rest of the code adds
    coherence to the program and encourages programmers to tackle the possibility
    of exceptions explicitly.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: Setting the parallelism level
  id: totrans-594
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have considered parallel collections as black boxes: add `par` to
    a normal collection and all the operations are performed in parallel. Often, we
    will want more control over how the tasks are executed.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: Internally, parallel collections work by distributing an operation over multiple
    threads. Since the threads share memory, parallel collections do not need to copy
    any data. Changing the number of threads available to the parallel collection
    will change the number of CPUs that are used to perform the tasks.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel collections have a `tasksupport` attribute that controls task execution:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: The task support object of a collection is an *execution context*, an abstraction
    capable of executing Scala expressions in a separate thread. By default, the execution
    context in Scala 2.11 is a *work-stealing thread pool*. When a parallel collection
    submits tasks, the context allocates these tasks to its threads. If a thread finds
    that it has finished its queued tasks, it will try and steal outstanding tasks
    from the other threads. The default execution context maintains a thread pool
    with number of threads equal to the number of CPUs.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of threads over which the parallel collection distributes the work
    can be changed by changing the task support. For instance, to parallelize the
    operations performed by a range over four threads:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: An example – cross-validation with parallel collections
  id: totrans-602
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's apply what you have learned so far to solve data science problems. There
    are many parts of a machine learning pipeline that can be parallelized trivially.
    One such part is cross-validation.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: We will give a brief description of cross-validation here, but you can refer
    to *The Elements of Statistical Learning*, by *Hastie*, *Tibshirani*, and *Friedman*
    for a more in-depth discussion.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a supervised machine learning problem involves training an algorithm
    over a training set. For instance, when we built a model to calculate the probability
    of a person being male based on their height and weight, the training set was
    the (height, weight) data for each participant, together with the male/female
    label for each row. Once the algorithm is trained on the training set, we can
    use it to classify new data. This process only really makes sense if the training
    set is representative of the new data that we are likely to encounter.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: The training set has a finite number of entries. It will thus, inevitably, have
    idiosyncrasies that are not representative of the population at large, merely
    due to its finite nature. These idiosyncrasies will result in prediction errors
    when predicting whether a new person is male or female, over and above the prediction
    error of the algorithm on the training set itself. Cross-validation is a tool
    for estimating the error caused by the idiosyncrasies of the training set that
    do not reflect the population at large.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-validation works by dividing the training set in two parts: a smaller,
    new training set and a cross-validation set. The algorithm is trained on the reduced
    training set. We then see how well the algorithm models the cross-validation set.
    Since we know the right answer for the cross-validation set, we can measure how
    well our algorithm is performing when shown new information. We repeat this procedure
    many times with different cross-validation sets.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different types of cross-validation, which differ in how
    we choose the cross-validation set. In this chapter, we will look at repeated
    random subsampling: we select *k* rows at random from the training data to form
    the cross-validation set. We do this many times, calculating the cross-validation
    error for each subsample. Since each iteration is independent of the previous
    ones, we can parallelize this process trivially. It is therefore a good candidate
    for parallel collections. We will look at an alternative form of cross-validation,
    *k-fold cross-validation*, in [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed
    Machine Learning with MLlib"), *Distributed Machine Learning with MLlib*.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a class that performs cross-validation in parallel. I encourage
    you to write the code as you go, but you will find the source code corresponding
    to these examples on GitHub ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).We
    will use parallel collections to handle the parallelism and Breeze data types
    in the inner loop. The `build.sbt` file is identical to the one we used in [Chapter
    2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data with Breeze") , *Manipulating
    Data with Breeze*:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'We will build a `RandomSubsample` class. The class exposes a type alias, `CVFunction`,
    for a function that takes two lists of indices—the first corresponding to the
    reduced training set and the second to the validation set—and returns a `Double`
    corresponding to the cross-validation error:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'The `RandomSubsample` class will expose a single method, `mapSamples`, which
    takes a `CVFunction`, repeatedly passes it different partitions of indices, and
    returns a vector of the errors. This is what the class looks like:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Let''s look at what happens in more detail, starting with the arguments passed
    to the constructor:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: We pass the total number of elements in the training set and the number of elements
    to leave out for cross-validation in the class constructor. Thus, passing 100
    to `nElems` and 20 to `nCrossValidation` implies that our training set will have
    80 random elements of the total data and that the test set will have 20 elements.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: 'We then construct a list of all integers between `0` and `nElems`:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: For each iteration of the cross-validation, we will shuffle this list and take
    the first `nCrossValidation` elements to be the indices of rows in our test set
    and the remaining to be the indices of rows in our training set.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: 'Our class exposes a single method, `mapSamples`, that takes two curried arguments:
    `nShuffles`, the number of times to perform random subsampling, and `f`, a `CVFunction`:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'With all this set up, the code for doing cross-validation is deceptively simple.
    We generate a parallel range from `0` to `nShuffles` and, for each item in the
    range, generate a new train-test split and calculate the cross-validation error:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: The only tricky part of this function is splitting the shuffled index list into
    a list of indices for the training set and a list of indices for the test set.
    We use Breeze's `split` method. This takes a vector as its first argument and
    a list of split-points as its second, and returns a list of fragments of the original
    vector. We then use pattern matching to extract the individual parts.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, `mapSamples` converts `cvResults` to a Breeze vector:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Let''s see this in action. We can test our class by running cross-validation
    on the logistic regression example developed in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze") , *Manipulating Data with Breeze*.
    In that chapter, we developed a `LogisticRegression` class that takes a training
    set (in the form of a `DenseMatrix`) and target (in the form of a `DenseVector`)
    at construction time. The class then calculates the parameters that best represent
    the training set. We will first add two methods to the `LogisticRegression` class
    to use the trained model to classify previously unseen examples:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: The `predictProbabilitiesMany` method uses the trained model to calculate the
    probability of having the target variable set to one. In the context of our example,
    this is the probability of being male, given a height and weight.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `classifyMany` method assigns classification labels (one or zero) to members
    of a test set. We will assign a one if `predictProbabilitiesMany` returns a value
    greater than `0.5`.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these two functions, our `LogisticRegression` class becomes:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: We can now put together an example program for our `RandomSubsample` class.
    We will use the same height-weight data as in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze") , *Manipulating Data with Breeze*.
    The data preprocessing will be similar. The code examples for this chapter provide
    a helper module, `HWData`, to load the height-weight data into Breeze vectors.
    The data itself is in the `data/` directory of the code examples for this chapter
    (available on GitHub at [https://github.com/pbugnion/s4ds/tree/master/chap04](https://github.com/pbugnion/s4ds/tree/master/chap04)).
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: 'For each new subsample, we create a new `LogisticRegression` instance, train
    it on the subset of the training set to get the best coefficients for this train-test
    split, and use `classifyMany` to generate predictions on the cross-validation
    set in this split. We then calculate the classification error and report the average
    classification error over every train-test split:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Running this program on the height-weight data gives a classification error
    of 10%.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: We now have a fully working, parallelized cross-validation class. Scala's parallel
    range made it simple to repeatedly compute the same function in different threads.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  id: totrans-638
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Parallel collections offer a simple, yet powerful, framework for parallel operations.
    However, they are limited in one respect: the total amount of work must be known
    in advance, and each thread must perform the same function (possibly on different
    inputs).'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we want to write a program that fetches a web page (or queries
    a web API) every few seconds and extracts data for further processing from this
    web page. A typical example might involve querying a web API to maintain an up-to-date
    value of a particular stock price. Fetching data from an external web page takes
    a few hundred milliseconds, typically. If we perform this operation on the main
    thread, it will needlessly waste CPU cycles waiting for the web server to reply.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to wrap the code for fetching the web page in a *future*. A
    future is a one-element container containing the future result of a computation.
    When you create a future, the computation in it gets off-loaded to a different
    thread in order to avoid blocking the main thread. When the computation finishes,
    the result is written to the future and thus made accessible to the main thread.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we will write a program that queries the "Markit on demand"
    API to fetch the price of a given stock. For instance, the URL for the current
    price of a Google share is [http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG).
    Go ahead and paste this in the address box of your web browser. You will see an
    XML string appear with, among other things, the current stock price. Let''s fetch
    this programmatically without resorting to a future first:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Notice how it takes a little bit of time to query the API. Let''s now do the
    same, but using a future (don''t worry about the imports for now, we will discuss
    what they mean in more detail further on):'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'If you run this, you will notice that control returns to the shell instantly
    before the API has had a chance to respond. To make this evident, let''s simulate
    a slow connection by adding a call to `Thread.sleep`:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'When you run this, you do not have to wait for ten seconds for the next prompt
    to appear: you regain control of the shell straightaway. The bit of code in the
    future is executed asynchronously: its execution is independent of the main program
    flow.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we retrieve the result of the computation? We note that `response` has
    type `Future[String]`. We can check whether the computation wrapped in the future
    has finished by querying the future''s `isCompleted` attribute:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'The future exposes a `value` attribute that contains the computation result:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: The `value` attribute of a future has type `Option[Try[T]]`. We have already
    seen how to use the `Try` type to handle exceptions gracefully in the context
    of parallel collections. It is used in the same way here. A future's `value` attribute
    is `None` until the future is complete, then it is set to `Some(Success(value))`
    if the future ran successfully, or `Some(Failure(error))` if an exception was
    thrown.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeatedly calling `f.value` until the future completes works well in the shell,
    but it does not generalize to more complex programs. Instead, we want to tell
    the computer to do something once the future is complete: we want to bind a *callback*
    function to the future. We can do this by setting the future''s `onComplete` attribute.
    Let''s tell the future to print the API response when it completes:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The function passed to `onComplete` runs when the future is finished. It takes
    a single argument of type `Try[T]` containing the result of the future.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Failure is normal: how to build resilient applications**'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: By wrapping the output of the code that it runs in a `Try` type, futures force
    the client code to consider the possibility that the code might fail. The client
    can isolate the effect of failure to avoid crashing the whole application. They
    might, for instance, log the exception. In the case of a web API query, they might
    add the offending URL to be queried again at a later date. In the case of a database
    failure, they might roll back the transaction.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: By treating failure as a first-class citizen rather than through exceptional
    control flow bolted on at the end, we can build applications that are much more
    resilient.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: Future composition – using a future's result
  id: totrans-661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you learned about the `onComplete` method to bind a
    callback to a future. This is useful to cause a side effect to happen when the
    future is complete. It does not, however, let us transform the future's return
    value easily.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: 'To carry on with our stocks example, let''s imagine that we want to convert
    the query response from a string to an XML object. Let''s start by including the
    `scala-xml` library as a dependency in `build.sbt`:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Let''s restart the console and reimport the dependencies on `scala.concurrent._`,
    `scala.concurrent.ExecutionContext.Implicits.global`, and `scala.io._`. We also
    want to import the `XML` library:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'We will use the same URL as in the previous section:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: '[http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG)'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes useful to think of a future as a collection that either contains
    one element if a calculation has been successful, or zero elements if it has failed.
    For instance, if the web API has been queried successfully, our future contains
    a string representation of the response. Like other container types in Scala,
    futures support a `map` method that applies a function to the element contained
    in the future, returning a new future, and does nothing if the calculation in
    the future failed. But what does this mean in the context of a computation that
    might not be finished yet? The map method gets applied as soon as the future is
    complete, like the `onComplete` method.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: We can use the future's `map` method to apply a transformation to the result
    of the future asynchronously. Let's poll the "Markit on demand" API again. This
    time, instead of printing the result, we will parse it as XML.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: By registering subsequent maps on futures, we are providing a road map to the
    executor running the future for what to do.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of the steps fail, the failed `Try` instance containing the exception
    gets propagated instead:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  id: totrans-674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: This behavior makes sense if you think of a failed future as an empty container.
    When applying a map to an empty list, it returns the same empty list. Similarly,
    when applying a map to an empty (failed) future, the empty future is returned.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: Blocking until completion
  id: totrans-676
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code for fetching stock prices works fine in the shell. However, if you
    paste it in a standalone program, you will notice that nothing gets printed and
    the program finishes straightaway. Let''s look at a trivial example of this:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'The program stops running as soon as the main thread has completed its tasks,
    which, in this example, just involves creating the futures. In particular, the
    line `"future completed"` is never printed. If we want the main thread to wait
    for a future to execute, we must explicitly tell it to block execution until the
    future has finished running. This is done using the `Await.ready` or `Await.result`
    methods. Both these methods block the execution of the main thread until the future
    completes. We could make the above program work as intended by adding this line:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: The `Await` methods take the future as their first argument and a `Duration`
    object as the second. If the future takes longer to complete than the specified
    duration, a `TimeoutException` is thrown. Pass `Duration.Inf` to set an infinite
    timeout.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: The difference between `Await.ready` and `Await.result` is that the latter returns
    the value inside the future. In particular, if the future resulted in an exception,
    that exception will get thrown. In contrast, `Await.ready` returns the future
    itself.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, one should try to avoid blocking as much as possible: the whole
    point of futures is to run code in background threads in order to keep the main
    thread of execution responsive. However, a common, legitimate use case for blocking
    is at the end of a program. If we are running a large-scale integration process,
    we might dispatch several futures to query web APIs, read from text files, or
    insert data into a database. Embedding the code in futures is more scalable than
    performing these operations sequentially. However, as the majority of the intensive
    work is running in background threads, we are left with many outstanding futures
    when the main thread completes. It makes sense, at this stage, to block until
    all the futures have completed.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: Controlling parallel execution with execution contexts
  id: totrans-684
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know how to define futures, let's look at controlling how they run.
    In particular, you might want to control the number of threads to use when running
    a large number of futures.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: When a future is defined, it is passed an *execution context*, either directly
    or implicitly. An execution context is an object that exposes an `execute` method
    that takes a block of code and runs it, possibly asynchronously. By changing the
    execution context, we can change the "backend" that runs the futures. We have
    already seen how to use execution contexts to control the execution of parallel
    collections.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have just been using the default execution context by importing `scala.concurrent.ExecutionContext.Implicits.global`.
    This is a fork / join thread pool with as many threads as there are underlying
    CPUs.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define a new execution context that uses sixteen threads:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Having defined the execution context, we can pass it explicitly to futures
    as they are defined:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Alternatively, we can define the execution context implicitly:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'It is then passed as an implicit parameter to all new futures as they are constructed:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'You can shut the execution context down to destroy the thread pool:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: When an execution context receives a shutdown command, it will finish executing
    its current tasks but will refuse any new tasks.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: Futures example – stock price fetcher
  id: totrans-699
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s bring some of the concepts that we covered in this section together
    to build a command-line application that prompts the user for the name of a stock
    and fetches the value of that stock. The catch is that, to keep the UI responsive,
    we will fetch the stock using a future:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Try running the program and entering the code for some stocks:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: Let's summarize how the code works. when you enter a stock, the main thread
    constructs a future that fetches the stock information from the API, converts
    it to XML, and extracts the price. We use `(r \ "LastPrice").text` to extract
    the text inside the `LastPrice` tag from the XML node `r`. We then convert the
    value to a big decimal. When the transformations are complete, the result is printed
    to screen by binding a callback through `onComplete`. Exception handling is handled
    naturally through our use of `.map` methods to handle transformations.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: By wrapping the code for fetching a stock price in a future, we free up the
    main thread to just respond to the user. This means that the user interface does
    not get blocked if we have, for instance, a slow internet connection.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: 'This example is somewhat artificial, but you could easily wrap much more complicated
    logic: stock prices could be written to a database and we could add additional
    commands to plot the stock price over time, for instance.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: We have only scratched the surface of what futures can offer in this section.
    We will revisit futures in more detail when we look at polling web APIs in [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs* and [Chapter 9](part0077.xhtml#aid-29DRA1
    "Chapter 9. Concurrency with Akka"), *Concurrency with Akka*.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: Futures are a key part of the data scientist's toolkit for building scalable
    systems. Moving expensive computation (either in terms of CPU time or wall time)
    to background threads improves scalability greatly. For this reason, futures are
    an important part of many Scala libraries such as **Akka** and the **Play** framework.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By providing high-level concurrency abstractions, Scala makes writing parallel
    code intuitive and straightforward. Parallel collections and futures form an invaluable
    part of a data scientist's toolbox, allowing them to parallelize their code with
    minimal effort. However, while these high-level abstractions obviate the need
    to deal directly with threads, an understanding of the internals of Scala's concurrency
    model is necessary to avoid race conditions.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will put concurrency on hold and study how to interact
    with SQL databases. However, this is only temporary: futures will play an important
    role in many of the remaining chapters in this book.'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-712
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Aleksandar Prokopec*, *Learning Concurrent Programming in Scala*. This is
    a detailed introduction to the basics of concurrent programming in Scala. In particular,
    it explores parallel collections and futures in much greater detail than this
    chapter.'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: 'Daniel Westheide''s blog gives an excellent introduction to many Scala concepts,
    in particular:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: '**Futures**: [http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html](http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html)'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Try** **type**: [http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html](http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html)'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a discussion of cross-validation, see *The Elements of Statistical Learning*
    by *Hastie*, *Tibshirani*, and *Friedman*.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5. Scala and SQL through JDBC
  id: totrans-718
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of data science's raison d'être is the difficulty of manipulating large
    datasets. Much of the data of interest to a company or research group cannot fit
    conveniently in a single computer's RAM. Storing the data in a way that is easy
    to query is therefore a complex problem.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases have been successful at solving the data storage problem.
    Originally proposed in 1970 ([http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf](http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf)),
    the overwhelming majority of databases in active use today are still relational.
    In that time, the price of RAM per megabyte has decreased by a factor of a hundred
    million. Similarly, hard drive capacity has increased from tens or hundreds of
    megabytes to terabytes. It is remarkable that, despite this exponential growth
    in data storage capacity, the relational model has remained dominant.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: 'Virtually all relational databases are described and queried with variants
    of **SQL** (**Structured Query Language**). With the advent of distributed computing,
    the position of SQL databases as the de facto data storage standard is being challenged
    by other types of databases, commonly grouped under the umbrella term NoSQL. Many
    NoSQL databases are more partition-tolerant than SQL databases: they can be split
    into several parts residing on different computers. While this author expects
    that NoSQL databases will become increasingly popular, SQL databases are likely
    to remain prevalent as a data persistence mechanism; hence, a significant portion
    of this book is devoted to interacting with SQL from Scala.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: While SQL is standardized, most implementations do not follow the full standard.
    Additionally, most implementations provide extensions to the standard. This means
    that, while many of the concepts in this book will apply to all SQL backends,
    the exact syntax will need to be adjusted. We will consider only the MySQL implementation
    here.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to interact with SQL databases from Scala
    using JDBC, a bare bones Java API. In the next chapter, we will consider Slick,
    an **Object Relational** **Mapper** (**ORM**) that gives a more Scala-esque feel
    to interacting with SQL.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is roughly composed of two sections: we will first discuss the
    basic functionality for connecting and interacting with SQL databases, and then
    discuss useful functional patterns that can be used to create an elegant, loosely
    coupled, and coherent data access layer.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that you have a basic working knowledge of SQL. If you
    do not, you would be better off first reading one of the reference books mentioned
    at the end of the chapter.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with JDBC
  id: totrans-726
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JDBC is an API for connecting to SQL databases in Java. It remains the simplest
    way of connecting to SQL databases from Scala. Furthermore, the majority of higher-level
    abstractions for interacting with databases still use JDBC as a backend.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: JDBC is not a library in itself. Rather, it exposes a set of interfaces to interact
    with databases. Relational database vendors then provide specific implementations
    of these interfaces.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a `build.sbt` file. We will declare a dependency on
    the MySQL JDBC connector:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: First steps with JDBC
  id: totrans-731
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by connecting to JDBC from the command line. To follow with the
    examples, you will need access to a running MySQL server. If you added the MySQL
    connector to the list of dependencies, open a Scala console by typing the following
    command:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: 'Let''s import JDBC:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'We then need to tell JDBC to use a specific connector. This is normally done
    using reflection, loading the driver at runtime:'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: This loads the appropriate driver into the namespace at runtime. If this seems
    somewhat magical to you, it's probably not worth worrying about exactly how this
    works. This is the only example of reflection that we will consider in this book,
    and it is not particularly idiomatic Scala.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to a database server
  id: totrans-739
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having specified the SQL connector, we can now connect to a database. Let''s
    assume that we have a database called `test` on host `127.0.0.1`, listening on
    port `3306`. We create a connection as follows:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: The first argument to `getConnection` is a URL-like string with `jdbc:mysql://host[:port]/database`.
    The second and third arguments are the username and password. Pass in an empty
    string if you can connect without a password.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: Creating tables
  id: totrans-743
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a database connection, let''s interact with the server. For
    these examples, you will find it useful to have a MySQL shell open (or a MySQL
    GUI such as **MySQLWorkbench**) as well as the Scala console. You can open a MySQL
    shell by typing the following command in a terminal:'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  id: totrans-745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'As an example, we will create a small table to keep track of famous physicists.
    In a `mysql` shell, we would run the following command:'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'To achieve the same with Scala, we send a JDBC statement to the connection:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: Let's ignore the return value of `executeUpdate` for now.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data
  id: totrans-751
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have created a table, let''s insert some data into it. We can do
    this with a SQL `INSERT` statement:'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  id: totrans-753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: In this case, `executeUpdate` returns `1`. When inserting rows, it returns the
    number of rows that were inserted. Similarly, if we had used a `SQL UPDATE` statement,
    this would return the number of rows that were updated. For statements that do
    not manipulate rows directly (such as the `CREATE TABLE` statement in the previous
    section), `executeUpdate` just returns `0`.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just jump into a `mysql` shell to verify the insertion performed correctly:'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Let''s quickly summarize what we have seen so far: to execute SQL statements
    that do not return results, use the following:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  id: totrans-758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'In the context of data science, we frequently need to insert or update many
    rows at a time. For instance, we might have a list of physicists:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'We want to insert all of these into the database. While we could create a statement
    for each physicist and send it to the database, this is quite inefficient. A better
    solution is to create a *batch* of statements and send them to the database together.
    We start by creating a statement template:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  id: totrans-762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'This is identical to the previous `prepareStatement` calls, except that we
    replaced the physicist''s name with a `?` placeholder. We can set the placeholder
    value with the `statement.setString` method:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'This replaces the first placeholder in the statement with the string `Richard
    Feynman`:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  id: totrans-766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Note that JDBC, somewhat counter-intuitively, counts the placeholder positions
    from 1 rather than 0.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now created the first statement in the batch of updates. Run the following
    command:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  id: totrans-769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'By running the preceding command, we initiate a batch insert: the statement
    is added to a temporary buffer that will be executed when we run the `executeBatch`
    method. Let''s add all the physicists in our list:'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'We can now execute all the statements in the batch:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: The return value of `executeBatch` is an array of the number of rows altered
    or inserted by each item in the batch.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: Note that we used `statement.setString` to fill in the template with a particular
    name. The `PreparedStatement` object has `setXXX` methods for all basic types.
    To get a complete list, read the `PreparedStatement` API documentation ([http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html)).
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: Reading data
  id: totrans-776
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know how to insert data into a database, let''s look at the converse:
    reading data. We use SQL `SELECT` statements to query the database. Let''s do
    this in the MySQL shell first:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  id: totrans-778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'To extract this information in Scala, we define a `PreparedStatement`:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  id: totrans-780
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'We execute this statement by running the following command:'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: This returns a JDBC `ResultSet` instance. The `ResultSet` is an abstraction
    representing a set of rows from the database. Note that we used `statement.executeQuery`
    rather than `statement.executeUpdate`. In general, one should execute statements
    that return data (in the form of `ResultSet`) with `executeQuery`. Statements
    that modify the database without returning data (insert, create, alter, or update
    statements, among others) are executed with `executeUpdate`.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ResultSet` object behaves somewhat like an iterator. It exposes a `next`
    method that advances itself to the next record, returning `true` if there are
    records left in `ResultSet`:'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  id: totrans-785
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'When the `ResultSet` instance points to a record, we can extract fields in
    this record by passing in the field name:'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  id: totrans-787
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'We can also extract fields using positional arguments. The fields are indexed
    from one:'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  id: totrans-789
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: 'When we are done with a particular record, we call the `next` method to advance
    the `ResultSet` to the next record:'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '![Reading data](img/image01193.jpeg)'
  id: totrans-792
  prefs: []
  type: TYPE_IMG
- en: A ResultSet object supports the getXXX(fieldName) methods to access the fields
    of a record and a `next` method to advance to the next record in the result set.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: 'One can iterate over a result set using a `while` loop:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE195]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Tip
  id: totrans-796
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A word of warning applies to reading fields that are nullable. While one might
    expect JDBC to return null when faced with a null SQL field, the return type depends
    on the `getXXX` command used. For instance, `getInt` and `getLong` will return
    `0` for any field that is null. Similarly, `getDouble` and `getFloat` return `0.0`.
    This can lead to some subtle bugs in code. In general, one should be careful with
    getters that return Java value types (`int`, `long`) rather than objects. To find
    out if a value is `null` in the database, query it first with `getInt` (or `getLong`
    or `getDouble`, as appropriate), then use the `wasNull` method that returns a
    Boolean if the last read value was null:'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  id: totrans-798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: This (surprising) behavior makes reading from `ResultSet` instances error-prone.
    One of the goals of the second part of this chapter is to give you the tools to
    build an abstraction layer on top of the `ResultSet` interface to avoid having
    to call methods such as `getInt` directly.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: Reading values directly from `ResultSet` objects feels quite unnatural in Scala.
    We will look, further on in this chapter, at constructing a layer through which
    you can access the result set using type classes.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: 'We now know how to read and write to a database. Having finished with the database
    for now, we close the result sets, prepared statements, and connections:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  id: totrans-802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: While closing statements and connections is not important in the Scala shell
    (they will get closed when you exit), it is important when you run programs; otherwise,
    the objects will persist, leading to "out of memory exceptions". In the next sections,
    we will look at establishing connections and statements with the **loan pattern**,
    a design pattern that closes a resource automatically when we finish using it.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: JDBC summary
  id: totrans-804
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have an overview of JDBC. The rest of this chapter will concentrate on
    writing abstractions that sit above JDBC, making database accesses feel more natural.
    Before we do this, let's summarize what we have seen so far.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used three JDBC classes:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Connection` class represents a connection to a specific SQL database.
    Instantiate a connection as follows:'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE198]'
  id: totrans-808
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'Our main use of `Connection` instances has been to generate `PreparedStatement`
    objects:'
  id: totrans-809
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE199]'
  id: totrans-810
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: 'A `PreparedStatement` instance represents a SQL statement about to be sent
    to the database. It also represents the template for a SQL statement with placeholders
    for values yet to be filled in. The class exposes the following methods:'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `statement.executeUpdate` | This sends the statement to the database. Use
    this for SQL statements that modify the database and do not return any data, such
    as `INSERT`, `UPDATE`, `DELETE`, and `CREATE` statements. |'
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `val results = statement.executeQuery` | This sends the statement to the
    database. Use this for SQL statements that return data (predominantly, the `SELECT`
    statements). This returns a `ResultSet` instance. |'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `statement.addBatch``statement.executeBatch` | The `addBatch` method adds
    the current statement to a batch of statements, and `executeBatch` sends the batch
    of statements to the database. |'
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `statement.setString(1, "Scala")``statement.setInt(1, 42)``statement.setBoolean(1,
    true)` | Fill in the placeholder values in the `PreparedStatement`. The first
    argument is the position in the statement (counting from 1). The second argument
    is the value.One common use case for these is in a batch update or insert: we
    might have a Scala list of objects that we want to insert into the database. We
    fill in the placeholders for each object in the list using the `.setXXX` methods,
    then add this statement to the batch using `.addBatch`. We can then send the entire
    batch to the database using `.executeBatch`. |'
  id: totrans-815
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `statement.setNull(1, java.sql.Types.BOOLEAN)` | This sets a particular item
    in the statement to `NULL`. The second argument specifies the `NULL` type. If
    we are setting a cell in a Boolean column, for instance, this should be `Types.BOOLEAN`.
    A full list of types is given in the API documentation for the `java.sql.Types`
    package ([http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html)).
    |'
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'A `ResultSet` instance represents a set of rows returned by a `SELECT` or `SHOW`
    statement. `ResultSet` exposes methods to access fields in the current row:'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `rs.getString(i)``rs.getInt(i)` | These methods get the value of the `ith`
    field in the current row; `i` is measured from 1. |'
  id: totrans-818
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `rs.getString("name")``rs.getInt("age")` | These methods get the value of
    a specific field, which is indexed by the column name. |'
  id: totrans-819
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `rs.wasNull` | This returns whether the last column read was `NULL.` This
    is particularly important when reading Java value types, such as `getInt`, `getBoolean`,
    or `getDouble`, as these return a default value when reading a `NULL` value. |'
  id: totrans-820
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The `ResultSet` instance exposes the `.next` method to move to the next row;
    `.next` returns `true` until the `ResultSet` has advanced to just beyond the last
    row.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: Functional wrappers for JDBC
  id: totrans-822
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a basic overview of the tools afforded by JDBC. All the objects
    that we have interacted with so far feel somewhat clunky and out of place in Scala.
    They do not encourage a functional style of programming.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, elegance is not necessarily a goal in itself (or, at least, you
    will probably struggle to convince your CEO that he should delay the launch of
    a product because the code lacks elegance). However, it is usually a symptom:
    either the code is not extensible or too tightly coupled, or it is easy to introduce
    bugs. The latter is particularly the case for JDBC. Forgot to check `wasNull`?
    That will come back to bite you. Forgot to close your connections? You''ll get
    an "out of memory exception" (hopefully not in production).'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will look at patterns that we can use to wrap JDBC
    types in order to mitigate many of these risks. The patterns that we introduce
    here are used very commonly in Scala libraries and applications. Thus, besides
    writing robust classes to interact with JDBC, learning about these patterns will,
    I hope, give you greater understanding of Scala programming.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: Safer JDBC connections with the loan pattern
  id: totrans-826
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen how to connect to a JDBC database and send statements
    to the database for execution. This technique, however, is somewhat error prone:
    you have to remember to close statements; otherwise, you will quickly run out
    of memory. In more traditional imperative style, we write the following try-finally
    block around every connection:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'Scala, with first-class functions, provides us with an alternative: the *loan
    pattern*. We write a function that is responsible for opening the connection,
    loaning it to the client code to do something interesting with it, and then closing
    it when the client code is done. Thus, the client code is not responsible for
    closing the connection any more.'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new `SqlUtils` object with a `usingConnection` method that
    leverages the loan pattern:'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  id: totrans-831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Let''s see this function in action:'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  id: totrans-833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: Thus, the client doesn't have to remember to close the connection, and the resultant
    code (for the client) feels much more like Scala.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: 'How does our `usingConnection` function work? The function definition is `def
    usingConnection( ... )(f : Connection => T ):T`. It takes, as its second set of
    arguments, a function that acts on a `Connection` object. The body of `usingConnection`
    creates the connection, then passes it to `f`, and finally closes the connection.
    This syntax is somewhat similar to code blocks in Ruby or the `with` statement
    in Python.'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-836
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful when mixing the loan pattern with lazy operations. This applies particularly
    to returning iterators, streams, and futures from `f`. As soon as the thread of
    execution leaves `f`, the connection will be closed. Any data structure that is
    not materialized at this point will not be able to carry on accessing the connection.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: 'The loan pattern is, of course, not exclusive to database connections. It is
    useful whenever you have the following pattern, in pseudocode:'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: Enriching JDBC statements with the "pimp my library" pattern
  id: totrans-840
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how to create self-closing connections with
    the loan pattern. This allows us to open connections to the database without having
    to remember to close them. However, we still have to remember to close any `ResultSet`
    and `PreparedStatement` that we open:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: 'Having to open and close the statement is somewhat ugly and error prone. This
    is another natural use case for the loan pattern. Ideally, we would like to write
    the following:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: How can we define a `.withQuery` method on the `Connection` class? We do not
    control the `Connection` class definition as it is part of the JDBC API. We would
    like to be able to somehow reopen the `Connection` class definition to add the
    `withQuery` method.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: Scala does not let us reopen classes to add new methods (a practice known as
    monkey-patching). We can still, however, enrich existing libraries with implicit
    conversions using the **pimp** **my library** pattern ([http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)).
    We first define a `RichConnection` class that contains the `withQuery` method.
    This `RichConnection` class is created from an existing `Connection` instance.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: 'We could use this class by just wrapping every `Connection` instance in a `RichConnection`
    instance:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: 'This adds unnecessary boilerplate: we have to remember to convert every connection
    instance to `RichConnection` to use `withQuery`. Fortunately, Scala provides an
    easier way with implicit conversions: we tell Scala how to convert from `Connection`
    to `RichConnection` and vice versa, and tell it to perform this conversion automatically
    (implicitly), if necessary:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: Now, whenever `pimpConnection` and `depimpConnection` are in the current scope,
    Scala will automatically use them to convert from `Connection` instances to `RichConnection`
    and back as needed.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now write the following (I have added type information for emphasis):'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  id: totrans-854
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: This might look like magic, so let's step back and look at what happens when
    we call `withQuery` on a `Connection` instance. The Scala compiler will first
    look to see if the class definition of `Connection` defines a `withQuery` method.
    When it finds that it does not, it will look for implicit methods that convert
    a `Connection` instance to a class that defines `withQuery`. It will find that
    the `pimpConnection` method allows conversion from `Connection` to `RichConnection`,
    which defines `withQuery`. The Scala compiler automatically uses `pimpConnection`
    to transform the `Connection` instance to `RichConnection`.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: Note that we used the names `pimpConnection` and `depimpConnection` for the
    conversion functions, but they could have been anything. We never call these methods
    explicitly.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize how to use the *pimp my library* pattern to add methods to
    an existing class:'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a class that wraps the class you want to enrich: `class` `RichConnection(val
    underlying:Connection)`. Add all the methods that you wish the original class
    had.'
  id: totrans-858
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a method to convert from your original class to your enriched class as
    part of an object called (conventionally) `Implicits`. Make sure that you tell
    Scala to use this conversion automatically with the `implicit` keyword: `implicit
    def pimpConnection(conn:Connection):RichConnection`. You can also tell Scala to
    automatically convert back from the enriched class to the original class by adding
    the reverse conversion method.'
  id: totrans-859
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Allow implicit conversions by importing the implicit conversion methods: `import
    Implicits._`.'
  id: totrans-860
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrapping result sets in a stream
  id: totrans-861
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The JDBC `ResultSet` object plays very badly with Scala collections. The only
    real way of doing anything useful with it is to loop through it directly with
    a `while` loop. For instance, to get a list of the names of physicists in our
    database, we could write the following code:'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  id: totrans-863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'The `ResultSet` interface feels unnatural because it behaves very differently
    from Scala collections. In particular, it does not support the higher-order functions
    that we take for granted in Scala: no `map`, `filter`, `fold`, or `for` comprehensions.
    Thankfully, writing a *stream* that wraps `ResultSet` is quite straightforward.
    A Scala stream is a lazily evaluated list: it evaluates the next element in the
    collection when it is needed and forgets previous elements when they are no longer
    used.'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a `stream` method that wraps `ResultSet` as follows:'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'This might look quite confusing, so let''s take it slowly. We define a `stream`
    method that wraps `ResultSet`, returning a `Stream[ResultSet]`. When the client
    calls `stream` on an empty result set, this just returns an empty stream. When
    the client calls `stream` on a non-empty `ResultSet`, the `ResultSet` instance
    is advanced by one row, and the client gets back `results #:: stream(results)`.
    The `#::` operator on a stream is similar to the cons operator, `::`, on a list:
    it prepends `results` to an existing `Stream`. The critical difference is that,
    unlike a list, `stream(results)` does not get evaluated until necessary. This,
    therefore, avoids duplicating the entire `ResultSet` in memory.'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our brand new `stream` function to get the name of all the physicists
    in our database:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: Streaming the results, rather than using the result set directly, lets us interact
    with the data much more naturally as we are now dealing with just a Scala collection.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: When you use `stream` in a `withQuery` block (or, generally, in a block that
    automatically closes the result set), you must always materialize the stream within
    the function, hence the call to `toVector`. Otherwise, the stream will wait until
    its elements are needed to materialize them, and by then, the `ResultSet` instance
    will be closed.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: Looser coupling with type classes
  id: totrans-872
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been reading and writing simple types to the database. Let''s
    imagine that we want to add a `gender` column to our database. We will store the
    gender as an enumeration in our physicists database. Our table is now as follows:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  id: totrans-874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: 'How can we represent genders in Scala? A good way of doing this is with an
    enumeration:'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE214]'
  id: totrans-876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: 'However, we now have a problem when deserializing objects from the database:
    JDBC has no built-in mechanism to convert from a SQL `ENUM` type to a Scala `Gender`
    type. We could achieve this by just converting manually every time we need to
    read gender information:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: However, we would need to write this everywhere that we want to read the `gender`
    field. This goes against the DRY (don't repeat yourself) principle, leading to
    code that is difficult to maintain. If we decide to change the way gender is stored
    in the database, we would need to find every instance in the code where we read
    the `gender` field and change it.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: 'A somewhat better solution would be to add a `getGender` method to the `ResultSet`
    class using the pimp my library idiom that we used extensively in this chapter.
    This solution is still not optimal. We are adding unnecessary specificity to `ResultSet`:
    it is now coupled to the structure of our databases.'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: 'We could create a subclass of `ResultSet` using inheritance, such as `PhysicistResultSet`,
    that can read the fields in a specific table. However, this approach is not composable:
    if we had another table that kept track of pets, with name, species, and gender
    fields, we would have to either reimplement the code for reading gender in a new
    `PetResultSet` or factor out a `GenderedResultSet` superclass. As the number of
    tables grows, the inheritance hierarchy would become unmanageable. A better approach
    would let us compose the functionality that we need. In particular, we want to
    decouple the process of extracting Scala objects from a result set from the code
    for iterating over a result set.'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: Type classes
  id: totrans-882
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scala provides an elegant solution using *type classes*. Type classes are a
    very powerful arrow in the Scala architect's quiver. However, they can present
    a bit of a learning curve, especially as there is no direct equivalent in object-oriented
    programming.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of presenting an abstract explanation, I will dive into an example:
    I will describe how we can leverage type classes to convert fields in a `ResultSet`
    to Scala types. The aim is to define a `read[T](field)` method on `ResultSet`
    that knows exactly how to deserialize to objects of type `T`. This method will
    replace and extend the `getXXX` methods in `ResultSet`:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: 'We start by defining an abstract `SqlReader[T]` trait that exposes a `read`
    method to read a specific field from a `ResultSet` and return an instance of type
    `T`:'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: 'We now need to provide a concrete implementation of `SqlReader[T]` for every
    `T` type that we want to read. Let''s provide concrete implementations for the
    `Gender` and `String` fields. We will place the implementation in a `SqlReader`
    companion object:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: 'We could now use our `ReadableXXX` objects to read from a result set:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  id: totrans-891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: 'This is already somewhat better than using the following:'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: 'This is because the code to map from a `ResultSet` field to `Gender.Value`
    is centralized in a single place: `ReadableGender`. However, it would be great
    if we could tell Scala to use `ReadableGender` whenever it needs to read `Gender.Value`,
    and use `ReadableString` whenever it needs to read a String value. This is exactly
    what type classes do.'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: Coding against type classes
  id: totrans-895
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We defined a `Readable[T]` interface that abstracts how to read an object of
    type `T` from a field in a `ResultSet`. How do we tell Scala that it needs to
    use this `Readable` object to convert from the `ResultSet` fields to the appropriate
    Scala type?
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: 'The key is the `implicit` keyword that we used to prefix the `GenderReader`
    and `StringReader` object definitions. It lets us write:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: 'By writing `implicitly[SqlReader[T]]`, we are telling the Scala compiler to
    find a class (or an object) that extends `SqlReader[T]` that is marked for implicit
    use. Try this out by pasting the following in the command line, for instance:'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  id: totrans-900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: 'Of course, using `implicitly[SqlReader[T]]` everywhere is not particularly
    elegant. Let''s use the pimp my library idiom to add a `read[T]` method to `ResultSet`.
    We first define a `RichResultSet` class that we can use to "pimp" the `ResultSet`
    class:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: 'The only unfamiliar part of this should be the `read[T : SqlReader]` generic
    definition. We are stating here that `read` will accept any `T` type, provided
    an instance of `SqlReader[T]` exists. This is called a *context bound.*'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: 'We must also add implicit methods to the `Implicits` object to convert from
    `ResultSet` to `RichResultSet`. You should be familiar with this now, so I will
    not bore you with the details. You can now call `results.read[T](fieldName)` for
    any `T` for which you have a `SqlReader[T]` implicit object defined:'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'Let''s summarize the steps needed for type classes to work. We will do this
    in the context of deserializing from SQL, but you will be able to adapt these
    steps to solve other problems:'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: Define an abstract generic trait that provides the interface for the type class,
    for example, `SqlReader[T]`. Any functionality that is independent of `T` can
    be added to this base trait.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the companion object for the base trait and add implicit objects extending
    the trait for each `T`, for example,
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE225]'
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Type classes are always used in generic methods. A method that relies on the
    existence of a type class for an argument must contain a context bound in the
    generic definition, for example, `def read[T : SqlReader](field:String):T`. To
    access the type class in this method, use the `implicitly` keyword: `implicitly[SqlReader[T]]`.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use type classes
  id: totrans-911
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Type classes are useful when you need a particular behavior for many different
    types, but exactly how this behavior is implemented varies between these types.
    For instance, we need to be able to read several different types from `ResultSet`,
    but exactly how each type is read differs between types: for strings, we must
    read from `ResultSet` using `getString`, whereas for integers, we must use `getInt`
    followed by `wasNull`.'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: A good rule of thumb is when you start thinking "Oh, I could just write a generic
    method to do this. Ah, but wait, I will have to write the `Int` implementation
    as a specific edge case as it behaves differently. Oh, and the `Gender` implementation.
    I wonder if there's a better way?", then type classes might be useful.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of type classes
  id: totrans-914
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data scientists frequently have to deal with new input streams, changing requirements,
    and new data types. Having an object-relational mapping layer that is easy to
    extend or alter is therefore critical to responding to changes efficiently. Minimizing
    coupling between code entities and separation of concerns are the only ways to
    ensure that the code can be changed in response to new data.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: 'With type classes, we maintain orthogonality between accessing records in the
    database (through the `ResultSet` class) and how individual fields are transformed
    to Scala objects: both can vary independently. The only coupling between these
    two concerns is through the `SqlReader[T]` interface.'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that both concerns can evolve independently: to read a new data
    type, we just need to implement a `SqlReader[T]` object. Conversely, we can add
    functionality to `ResultSet` without needing to reimplement how fields are converted.
    For instance, we could add a `getColumn` method that returns a `Vector[T]` of
    all the values of a field in a `ResultSet` instance:'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  id: totrans-918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: Note how we could do this without increasing the coupling to the way in which
    individual fields are read.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data access layer
  id: totrans-920
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's bring together everything that we have seen and build a *data-mapper*
    class for fetching `Physicist` objects from the database. These classes (also
    called *data access objects*) are useful to decouple the internal representation
    of an object from its representation in the database.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the `Physicist` class:'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  id: totrans-923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'The data access object will expose a single method, `readAll`, that returns
    a `Vector[Physicist]` of all the physicists in our database:'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  id: totrans-925
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: 'The data access layer can be used by client code as in the following example:'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  id: totrans-927
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: Summary
  id: totrans-928
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to interact with SQL databases using JDBC. We
    wrote a library to wrap native JDBC objects, aiming to give them a more functional
    interface.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about Slick, a Scala library that provides
    functional wrappers to interact with relational databases.
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-931
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The API documentation for JDBC is very complete: [http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html](http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html)'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: The API documentation for the `ResultSet` interface ([http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html](http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html)),
    for the `PreparedStatement` class ([http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html))
    and the `Connection` class ([http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html))
    is particularly relevant.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: The data mapper pattern is described extensively in Martin Fowler's *Patterns
    of Enterprise Application Architecture*. A brief description is also available
    on his website ([http://martinfowler.com/eaaCatalog/dataMapper.html](http://martinfowler.com/eaaCatalog/dataMapper.html)).
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: For an introduction to SQL, I suggest *Learning SQL* by *Alan Beaulieu* (*O'Reilly*).
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: For another discussion of type classes, read [http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html](http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html).
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
- en: 'This post describes how some common object-oriented design patterns can be
    reimplemented more elegantly in Scala using type classes:'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
- en: '[https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/](https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/)'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
- en: 'This post by *Martin Odersky* details the *Pimp my Library* pattern:'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6. Slick – A Functional Interface for SQL
  id: totrans-941
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*, we investigated how to access SQL databases with
    JDBC. As interacting with JDBC feels somewhat unnatural, we extended JDBC using
    custom wrappers. The wrappers were developed to provide a functional interface
    to hide the imperative nature of JDBC.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
- en: With the difficulty of interacting directly with JDBC from Scala and the ubiquity
    of SQL databases, you would expect there to be existing Scala libraries that wrap
    JDBC. *Slick* is such a library.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: Slick styles itself as a *functional-relational mapping* library, a play on
    the more traditional *object-relational mapping* name used to denote libraries
    that build objects from relational databases. It presents a functional interface
    to SQL databases, allowing the client to interact with them in a manner similar
    to native Scala collections.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
- en: FEC data
  id: totrans-945
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use a somewhat more involved example dataset. The **Federal
    Electoral Commission of the United** **States** (**FEC**) records all donations
    to presidential candidates greater than $200\. These records are publicly available.
    We will look at the donations for the campaign leading up to the 2012 general
    elections that resulted in Barack Obama's re-election. The data includes donations
    to the two presidential candidates, Obama and Romney, and also to the other contenders
    in the Republican primaries (there were no Democrat primaries).
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take the transaction data provided by the FEC, store
    it in a table, and learn how to query and analyze it.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to acquire the data. If you have downloaded the code samples
    from the Packt website, you should already have two CSVs in the `data` directory
    of the code samples for this chapter. If not, you can download the files using
    the following links:'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
- en: '`data.scala4datascience.com/fec/ohio.csv.gz` (or `ohio.csv.zip`)'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.scala4datascience.com/fec/us.csv.gz` (or `us.csv.zip`)'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decompress the two files and place them in a directory called `data/` in the
    same location as the source code examples for this chapter. The data files correspond
    to the following:'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
- en: The `ohio.csv` file is a CSV of all the donations made by donors in Ohio.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `us.csv` file is a CSV of all the donations made by donors across the country.
    This is quite a large file, with six million rows.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two CSV files contain identical columns. Use the Ohio dataset for more responsive
    behavior, or the nationwide data file if you want to wrestle with a larger dataset.
    The dataset is adapted from a list of contributions downloaded from [http://www.fec.gov/disclosurep/PDownload.do](http://www.fec.gov/disclosurep/PDownload.do).
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a Scala case class to represent a transaction. In
    the context of this chapter, a transaction is a single donation from an individual
    to a candidate:'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  id: totrans-956
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: 'The code repository for this chapter includes helper functions in an `FECData`
    singleton object to load the data from CSVs:'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  id: totrans-958
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: 'Calling `FECData.loadOhio` or `FECData.loadAll` will create an `FECData` object
    with a single attribute, `transactions`, which is an iterator over all the donations
    coming from Ohio or the entire United States:'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  id: totrans-960
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: Now that we have some data to play with, let's try and put it in the database
    so that we can run some useful queries on it.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
- en: Importing Slick
  id: totrans-962
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To add Slick to the list of dependencies, you will need to add `"com.typesafe.slick"
    %% "slick" % "2.1.0"` to the list of dependencies in your `build.sbt` file. You
    will also need to make sure that Slick has access to a JDBC driver. In this chapter,
    we will connect to a MySQL database, and must, therefore, add the MySQL connector
    `"mysql" % "mysql-connector-java" % "5.1.37"` to the list of dependencies.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
- en: 'Slick is imported by importing a specific database driver. As we are using
    MySQL, we must import the following:'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: To connect to a different flavor of SQL database, import the relevant driver.
    The easiest way of seeing what drivers are available is to consult the API documentation
    for the `slick.driver` package, which is available at [http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package).
    All the common SQL flavors are supported (including **H2**, **PostgreSQL**, **MS
    SQL Server**, and **SQLite**).
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: Defining the schema
  id: totrans-967
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s create a table to represent our transactions. We will use the following
    schema:'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: Note that the donation amount is in *cents*. This allows us to use an integer
    field (rather than a fixed point decimal, or worse, a float).
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-971
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You should never use a floating point format to represent money or, in fact,
    any discrete quantity because floats cannot represent most fractions exactly:'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: This seemingly nonsensical result occurs because there is no way to store 0.3
    exactly in doubles.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: 'This post gives an extensive discussion of the limitations of the floating
    point format:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Slick with tables in our database, we first need to tell Slick about
    the database schema. We do this by creating a class that extends the `Table` abstract
    class. The way in which a schema is defined is quite straightforward, so let''s
    dive straight into the code. We will store our schema in a `Tables` singleton.
    We define a `Transactions` class that provides the mapping to go from collections
    of `Transaction` instances to SQL tables structured like the `transactions` table:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE236]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: Let's go through this line by line. We first define a `Transactions` class,
    which must take a Slick `Tag` object as its first argument. The `Tag` object is
    used by Slick internally to construct SQL statements. The `Transactions` class
    extends a `Table` object, passing it the tag and name of the table in the database.
    We could, optionally, have added a database name by extending `Table[Transaction](tag,
    Some("fec"), "transactions")` rather than just `Table[Transaction](tag, "transactions")`.
    The `Table` type is parametrized by `Transaction`. This means that running `SELECT`
    statements on the database returns `Transaction` objects. Similarly, we will insert
    data into the database by passing a transaction or list of transactions to the
    relevant Slick methods.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the `Transactions` class definition in more detail. The body
    of the class starts by listing the database columns. For instance, the `id` column
    is defined as follows:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: We tell Slick that it should read the column called `id` and transform it to
    a Scala integer. Additionally, we tell Slick that this column is the primary key
    and that it is auto-incrementing. The Slick documentation contains a list of available
    options for `column`.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: 'The `candidate` and `contributor` columns are straightforward: we tell Slick
    to read these as `String` from the database. The `contributor_state` column is
    a little more interesting. Besides specifying that it should be read from the
    database as a `String`, we also tell Slick that it should be stored in the database
    with type `VARCHAR(2)`.'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: 'The `contributor_occupation` column in our table can contain `NULL` values.
    When defining the schema, we pass the `Option[String]` type to the column method:'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE238]'
  id: totrans-985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: When reading from the database, a `NULL` field will get converted to `None`
    for columns specified as `Option[T]`. Conversely, if the field has a value, it
    will be returned as `Some(value)`.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: 'The last line of the class body is the most interesting part: it specifies
    how to transform the raw data read from the database into a `Transaction` object
    and how to convert a `Transaction` object to raw fields ready for insertion:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: 'The first part is just a tuple of fields to be read from the database: `(id.?,
    candidate, contributor, contributorState, contributorOccupation, amount, date)`,
    with a small amount of metadata. The second part is a pair of functions that describe
    how to transform this tuple into a `Transaction` object and back. In this case,
    as `Transaction` is a case class, we can take advantage of the `Transaction.tupled`
    and `Transaction.unapply` methods automatically provided for case classes.'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we followed the `id` entry with `.?`. In our `Transaction` class,
    the donation `id` has the `Option[Int]` type, but the column in the database has
    the `INT` type with the additional `O.AutoInc` option. The `.?` suffix tells Slick
    to use the default value provided by the database (in this case, the database's
    auto-increment) if `id` is `None`.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we define the value:'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: 'This is the handle that we use to actually interact with the database. For
    instance, as we will see later, to get a list of donations to Barack Obama, we
    run the following query (don''t worry about the details of the query for now):'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  id: totrans-994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'Let''s summarize the parts of our `Transactions` mapper class:'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Transactions` class must extend the `Table` abstract class parametrized
    by the type that we want to return: `Table[Transaction]`.'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define the columns to read from the database explicitly using `column`, for
    example, `def contributorState = column[String]("contributor_state", O.DBType("VARCHAR(2)"))`.
    The `[String]` type parameter defines the Scala type that this column gets read
    as. The first argument is the SQL column name. Consult the Slick documentation
    for a full list of additional arguments ([http://slick.typesafe.com/doc/2.1.0/schemas.html](http://slick.typesafe.com/doc/2.1.0/schemas.html)).
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We describe how to convert from a tuple of the column values to a Scala object
    and vice versa using `def * = (id.?, candidate, ...) <> (Transaction.tupled, Transaction.unapply)`.
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to the database
  id: totrans-999
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you have learned how to define `Table` classes that encode the transformation
    from rows in a SQL table to Scala case classes. To move beyond table definitions
    and start interacting with a database server, we must connect to a database. As
    in the previous chapter, we will assume that there is a MySQL server running on
    localhost on port `3306`.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the console to demonstrate the functionality in this chapter, but
    you can find an equivalent sample program in `SlickDemo.scala`. Let''s open a
    Scala console and connect to the database running on port `3306`:'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: If you have read the previous chapter, you will recognize the first argument
    as a JDBC-style URL. The URL starts by defining a protocol, in this case, `jdbc:mysql`,
    followed by the IP address and port of the database server, followed by the database
    name (`test`, here).
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: The second argument to `forURL` is the class name of the JDBC driver. This driver
    is imported at runtime using reflection. Note that the driver specified here must
    match the Slick driver imported statically.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: 'Having defined the database, we can now use it to create a connection:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE243]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: 'Slick functions that require access to the database take a `Session` argument
    implicitly: if a `Session` instance marked as implicit is available in scope,
    they will use it. Thus, preceding `session` with the `implicit` keyword saves
    us having to pass `session` explicitly every time we run an operation on the database.'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have read the previous chapter, you will recognize that Slick deals
    with the need to close connections with the *loan pattern*: a database connection
    is created in the form of a `session` object and passed temporarily to the client.
    When the client code returns, the session is closed, ensuring that all opened
    connections are closed. The client code is therefore spared the responsibility
    of closing the connection.'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
- en: 'The loan pattern is very useful in production code, but it can be somewhat
    cumbersome in the shell. Slick lets us create a session explicitly as follows:'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]'
  id: totrans-1010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: Creating tables
  id: totrans-1011
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use our new connection to create the transaction table in the database.
    We can access methods to create and drop tables using the `ddl` attribute on our
    `TableQuery[Transactions]` instance:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  id: totrans-1013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: 'If you jump into a `mysql` shell, you will see that a `transactions` table
    has been created:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: The `ddl` attribute also includes a `drop` method to drop the table. Incidentally,
    `ddl` stands for "data-definition language" and is commonly used to refer to the
    parts of SQL relevant to schema and constraint definitions.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data
  id: totrans-1017
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Slick `TableQuery` instances let us interact with SQL tables with an interface
    similar to Scala collections.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a transaction first. We will pretend that a donation occurred
    on the 22nd of June, 2010\. Unfortunately, the code to create dates in Scala and
    pass these to JDBC is particularly clunky. We first create a `java.util.Date`
    instance, which we must then convert to a `java.sql.Date` to use in our newly
    created transaction:'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  id: totrans-1020
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: 'Much of the interface provided by the `TableQuery` instance mirrors that of
    a mutable list. To insert a single row in the transaction table, we can use the
    `+=` operator:'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: Under the hood, this will create a JDBC prepared statement and run this statement's
    `executeUpdate` method.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are committing many rows at a time, you should use Slick''s bulk insert
    operator: `++=`. This takes a `List[Transaction]` as input and inserts all the
    transactions in a single batch by taking advantage of JDBC''s `addBatch` and `executeBatch`
    functionality.'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s insert all the FEC transactions so that we have some data to play with
    when running queries in the next section. We can load an iterator of transactions
    for Ohio by calling the following:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: 'We can also load the transactions for the whole of United States:'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE250]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: 'To avoid materializing all the transactions in a single fell swoop—thus potentially
    exceeding our computer''s available memory—we will take batches of transactions
    from the iterator and insert them:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  id: totrans-1030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: An iterator's `grouped` method splits the iterator into batches. It is useful
    to split a long collection or iterator into manageable batches that can be processed
    one after the other. This is important when integrating or processing large datasets.
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: 'All that we have to do now is iterate over our batches, inserting them into
    the database as we go:'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  id: totrans-1033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: 'While this works, it is sometimes useful to see progress reports when doing
    long-running integration processes. As we have split the integration into batches,
    we know (to the nearest batch) how far into the integration we are. Let''s print
    the progress information at the beginning of every batch:'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE253]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: We use the `.zipWithIndex` method to transform our iterator over batches into
    an iterator of (*batch*, *current* *index*) pairs. In a full-scale application,
    the progress information would probably be written to a log file rather than to
    the screen.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: Slick's well-designed interface makes inserting data very intuitive, integrating
    well with native Scala types.
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: Querying data
  id: totrans-1038
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we used Slick to insert donation data into our database.
    Let's explore this data now.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: 'When defining the `Transactions` class, we defined a `TableQuery` object, `transactions`,
    that acts as the handle for accessing the transaction table. It exposes an interface
    similar to Scala iterators. For instance, to see the first five elements in our
    database, we can call `take(5)`:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: 'Internally, Slick implements the `.take` method using a SQL `LIMIT`. We can,
    in fact, get the SQL statement using the `.selectStatement` method on the query:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  id: totrans-1043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: 'Our Slick query is made up of the following two parts:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: '`.take(n)`: This part is called the *invoker*. Invokers build up the SQL statement
    but do not actually fire it to the database. You can chain many invokers together
    to build complex SQL statements.'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.list`: This part sends the statement prepared by the invoker to the database
    and converts the result to Scala object. This takes a `session` argument, possibly
    implicitly.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invokers
  id: totrans-1047
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Invokers** are the components of a Slick query that build up the SQL select
    statement. Slick exposes a variety of invokers that allow the construction of
    complex queries. Let''s look at some of these invokers here:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: 'The `map` invoker is useful to select individual columns or apply operations
    to columns:'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE256]'
  id: totrans-1050
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE256]'
- en: 'The `filter` invoker is the equivalent of the `WHERE` statements in SQL. Note
    that Slick fields must be compared using `===`:'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE257]'
  id: totrans-1052
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE257]'
- en: 'Similarly, to filter out donations to Barack Obama, use the `=!=` operator:'
  id: totrans-1053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE258]'
  id: totrans-1054
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE258]'
- en: 'The `sortBy` invoker is the equivalent of the `ORDER BY` statement in SQL:'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE259]'
  id: totrans-1056
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE259]'
- en: The `leftJoin`, `rightJoin`, `innerJoin`, and `outerJoin` invokers are used
    for joining tables. As we do not cover interactions between multiple tables in
    this tutorial, we cannot demonstrate joins. See the Slick documentation ([http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping](http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping))
    for examples of these.
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggregation invokers such as `length`, `min`, `max`, `sum`, and `avg` can be
    used for computing summary statistics. These must be executed using `.run`, rather
    than `.list`, as they return single numbers. For instance, to get the total donations
    to Barack Obama:'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE260]'
  id: totrans-1059
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE260]'
- en: Operations on columns
  id: totrans-1060
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you learned about the different invokers and how they
    mapped to SQL statements. We brushed over the methods supported by columns themselves,
    however: we can compare for equality using `===`, but what other operations are
    supported by Slick columns?'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the SQL functions are supported. For instance, to get the total donations
    to candidates whose name starts with `"O"`, we could run the following:'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE261]'
  id: totrans-1063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: 'Similarly, to count donations that happened between January 1, 2011 and February
    1, 2011, we can use the `.between` method on the `date` column:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE262]'
  id: totrans-1065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: 'The equivalent of the SQL `IN (...)` operator that selects values in a specific
    set is `inSet`. For instance, to select all transactions to Barack Obama and Mitt
    Romney, we can use the following:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE263]'
  id: totrans-1067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: So, between them, Mitt Romney and Barack Obama received over 28 million dollars
    in registered donations.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also negate a Boolean column with the `!` operator. For instance, to
    calculate the total amount of donations received by all candidates apart from
    Barack Obama and Mitt Romney:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: Column operations are added by implicit conversion on the base `Column` instances.
    For a full list of methods available on String columns, consult the API documentation
    for the `StringColumnExtensionMethods` class ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods)).
    For the methods available on Boolean columns, consult the API documentation for
    the `BooleanColumnExtensionMethods` class ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods)).
    For the methods available on numeric columns, consult the API documentation for
    `NumericColumnExtensionMethods` ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods)).
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations with "Group by"
  id: totrans-1072
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Slick also provides a `groupBy` method that behaves like the `groupBy` method
    of native Scala collections. Let''s get a list of candidates with all the donations
    for each candidate:'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: Let's break this down. The first statement, `transactions.groupBy { _.candidate
    }`, specifies the key by which to group. You can think of this as building an
    intermediate list of `(String, List[Transaction])` tuples mapping the group key
    to a list of all the table rows that satisfy this key. This behavior is identical
    to calling `groupBy` on a Scala collection.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to `groupBy` must be followed by a `map` that aggregates the groups.
    The function passed to `map` must take the tuple `(String, List[Transaction])`
    pair created by the `groupBy` call as its sole argument. The `map` call is responsible
    for aggregating the `List[Transaction]` object. We choose to first pick out the
    `amount` field of each transaction, and then to run a sum over these. Finally,
    we call `.list` on the whole pipeline to actually run the query. This just returns
    a Scala list. Let''s convert the total donations from cents to dollars:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE266]'
  id: totrans-1077
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: Accessing database metadata
  id: totrans-1078
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Commonly, especially during development, you might start the script by dropping
    the table if it exists, then recreating it. We can find if a table is defined
    by accessing the database metadata through the `MTable` object. To get a list
    of tables with name matching a certain pattern, we can run `MTable.getTables(pattern)`:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  id: totrans-1080
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: 'Thus, to drop the transactions table if it exists, we can run the following:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  id: totrans-1082
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: 'The `MTable` instance contains a lot of metadata about the table. Go ahead
    and recreate the `transactions` table if you dropped it in the previous example.
    Then, to find information about the table''s primary keys:'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE269]'
  id: totrans-1084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: For a full list of methods available on `MTable` instances, consult the Slick
    documentation ([http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable](http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable)).
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: Slick versus JDBC
  id: totrans-1086
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter and the previous one introduced two different ways of interacting
    with SQL. In the previous chapter, we described how to use JDBC and build extensions
    on top of JDBC to make it more usable. In this chapter, we introduced Slick, a
    library that provides a functional interface on top of JDBC.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: Which method should you choose? If you are starting a new project, you should
    consider using Slick. Even if you spend a considerable amount of time writing
    wrappers that sit on top of JDBC, it is unlikely that you will achieve the fluidity
    that Slick offers.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: If you are working on an existing project that makes extensive use of JDBC,
    I hope that the previous chapter demonstrates that, with a little time and effort,
    you can write JDBC wrappers that reduce the impedance between the imperative style
    of JDBC and Scala's functional approach.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1090
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, we looked extensively at how to query relational
    databases from Scala. In this chapter, you learned how to use Slick, a "functional-relational"
    mapper that allows interacting with SQL databases as one would with Scala collections.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to ingest data by querying web APIs.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-1093
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about Slick, you can refer to the Slick documentation ([http://slick.typesafe.com/doc/2.1.0/](http://slick.typesafe.com/doc/2.1.0/))
    and its API documentation ([http://slick.typesafe.com/doc/2.1.0/api/#package](http://slick.typesafe.com/doc/2.1.0/api/#package)).
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7. Web APIs
  id: totrans-1095
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists and data engineers get data from a variety of different sources.
    Often, data might come as CSV files or database dumps. Sometimes, we have to obtain
    the data through a web API.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: 'An individual or organization sets up a web API to distribute data to programs
    over the Internet (or an internal network). Unlike websites, where the data is
    intended to be consumed by a web browser and shown to the user, the data provided
    by a web API is agnostic to the type of program querying it. Web servers serving
    HTML and web servers backing an API are queried in essentially the same way: through
    HTTP requests.'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen an example of a web API in [Chapter 4](part0036.xhtml#aid-12AK82
    "Chapter 4. Parallel Collections and Futures"), *Parallel Collections and Futures*,
    where we queried the "Markit on demand" API for current stock prices. In this
    chapter, we will explore how to interact with web APIs in more detail; specifically,
    how to convert the data returned by the API to Scala objects and how to add additional
    information to the request through HTTP headers (for authentication, for instance).
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
- en: The "Markit on demand" API returned the data formatted as an XML object, but
    increasingly, new web APIs return data formatted as JSON. We will therefore focus
    on JSON in this chapter, but the concepts will port easily to XML.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: JSON is a language for formatting structured data. Many readers will have come
    across JSON in the past, but if not, there is a brief introduction to the syntax
    and concepts later on in this chapter. You will find it quite straightforward.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will poll the GitHub API. GitHub has, over the last few
    years, become the de facto tool for collaborating on open source software. It
    provides a powerful, feature-rich API that gives programmatic access to nearly
    all the data available through the website.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get a taste of what we can do. Type `api.github.com/users/odersky` in
    your web browser address bar. This will return the data offered by the API on
    a particular user (Martin Odersky, in this case):'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: The data is returned as a JSON object. This chapter is devoted to learning how
    to access and parse this data programmatically. In [Chapter 13](part0125.xhtml#aid-3N6MA1
    "Chapter 13. Web APIs with Play"), *Web APIs with Play*, you will learn how to
    build your own web API.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-1105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GitHub API is extensive and very well-documented. We will explore some of
    the features of the API in this chapter. To see the full extent of the API, visit
    the documentation ([https://developer.github.com/v3/](https://developer.github.com/v3/)).
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: A whirlwind tour of JSON
  id: totrans-1107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JSON is a format for transferring structured data. It is flexible, easy for
    computers to generate and parse, and relatively readable for humans. It has become
    very common as a means of persisting program data structures and transferring
    data between programs.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: 'JSON has four basic types: **Numbers**, **Strings**, **Booleans**, and **null**,
    and two compound types: **Arrays** and **Objects**. Objects are unordered collections
    of key-value pairs, where the key is always a string and the value can be any
    simple or compound type. We have already seen a JSON object: the data returned
    by the API call `api.github.com/users/odersky`.'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: 'Arrays are ordered lists of simple or compound types. For instance, type [api.github.com/users/odersky/repos](http://api.github.com/users/odersky/repos)
    in your browser to get an array of objects, each representing a GitHub repository:'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  id: totrans-1111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: We can construct complex structures by nesting objects within other objects
    or arrays. Nevertheless, most web APIs return JSON structures with no more than
    one or two levels of nesting. If you are not familiar with JSON, I encourage you
    to explore the GitHub API through your web browser.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
- en: Querying web APIs
  id: totrans-1113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way of querying a web API from Scala is to use `Source.fromURL`.
    We have already used this in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, when we queried
    the "Markit on demand" API. `Source.fromURL` presents an interface similar to
    `Source.fromFile`:'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE272]'
  id: totrans-1115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '`Source.fromURL` returns an iterator over the characters of the response. We
    materialize the iterator into a string using its `.mkString` method. We now have
    the response as a Scala string. The next step is to parse the string with a JSON
    parser.'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: JSON in Scala – an exercise in pattern matching
  id: totrans-1117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several libraries for manipulating JSON in Scala. We prefer json4s,
    but if you are a die-hard fan of another JSON library, you should be able to readily
    adapt the examples in this chapter. Let''s create a `build.sbt` file with a dependency
    on `json4s`:'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  id: totrans-1119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: 'We can then import `json4s` into an SBT console session with:'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE274]'
  id: totrans-1121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: 'Let''s use `json4s` to parse the response to our GitHub API query:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE275]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: The `parse` method takes a string (that contains well-formatted JSON) and converts
    it to a `JValue`, a supertype for all `json4s` objects. The runtime type of the
    response to this particular query is `JObject`, which is a `json4s` type representing
    a JSON object.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: '`JObject` is a wrapper around a `List[JField]`, and `JField` represents an
    individual key-value pair in the object. We can use *extractors* to access this
    list:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  id: totrans-1126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: 'What''s happened here? By writing `val JObject(fields) = ...`, we are telling
    Scala:'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: The right-hand side has runtime type of `JObject`
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go into the `JObject` instance and bind the list of fields to the constant `fields`
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Readers familiar with Python might recognize the similarity with tuple unpacking,
    though Scala extractors are much more powerful and versatile. Extractors are used
    extensively to extract Scala types from `json4s` types.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-1131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pattern matching using case classes**'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: 'How exactly does the Scala compiler know what to do with an extractor such
    as:'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE277]'
  id: totrans-1134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '`JObject` is a case class with the following constructor:'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE278]'
  id: totrans-1136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: Case classes all come with an extractor that reverses the constructor exactly.
    Thus, writing `val JObject(fields)` will bind `fields` to the `obj` attribute
    of the `JObject`. For further details on how extractors work, read [Appendix](part0149.xhtml#aid-4E33Q2
    "Appendix A. Pattern Matching and Extractors"), *Pattern Matching and Extractors*.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now extracted `fields`, a (plain old Scala) list of fields from the
    `JObject`. A `JField` is a key-value pair, with the key being a string and value
    being a subtype of `JValue`. Again, we can use extractors to extract the values
    in the field:'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  id: totrans-1139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: We matched the right-hand side against the pattern `JField(_, JString(_))`,
    binding the first element to `key` and the second to `value`. What happens if
    the right-hand side does not match the pattern?
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  id: totrans-1141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: 'The code throws a `MatchError` at runtime. These examples demonstrate the power
    of nested pattern matching: in a single line, we managed to verify the type of
    `firstField`, that its value has type `JString`, and we have bound the key and
    value to the `key` and `value` variables, respectively. As another example, if
    we *know* that the first field is the login field, we can both verify this and
    extract the value:'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE281]'
  id: totrans-1143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: 'Notice how this style of programming is *declarative* rather than imperative:
    we declare that we want a `JField("login", JString(_))` variable on the right-hand
    side. We then let the language figure out how to check the variable types. Pattern
    matching is a recurring theme in functional languages.'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use pattern matching in a for loop when looping over fields. When
    used in a for loop, a pattern match defines a *partial function*: only elements
    that match the pattern pass through the loop. This lets us filter the collection
    for elements that match a pattern and also apply a transformation to these elements.
    For instance, we can extract every string field in our `fields` list:'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  id: totrans-1146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: 'We can use this to search for specific fields. For instance, to extract the
    `"followers"` field:'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  id: totrans-1148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: We first extracted all fields that matched the pattern `JField("follower", JInt(_))`,
    returning the integer inside the `JInt`. As the source collection, `fields`, is
    a list, this returns a list of integers. We then extract the first value from
    this list using `headOption`, which returns the head of the list if the list has
    at least one element, or `None` if the list is empty.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not limited to extracting a single field at a time. For instance, to
    extract the `"id"` and `"login"` fields together:'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE284]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: Scala's pattern matching and extractors provide you with an extremely powerful
    way of traversing the `json4s` tree, extracting the fields that we need.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: JSON4S types
  id: totrans-1153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already discovered parts of `json4s`''s type hierarchy: strings are
    wrapped in `JString` objects, integers (or big integers) are wrapped in `JInt`,
    and so on. In this section, we will take a step back and formalize the type structure
    and what Scala types they extract to. These are the `json4s` runtime types:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: '`val JString(s) // => extracts to a String`'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JDouble(d) // => extracts to a Double`'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JDecimal(d) // => extracts to a BigDecimal`'
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JInt(i) // => extracts to a BigInt`'
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JBool(b) // => extracts to a Boolean`'
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JObject(l) // => extracts to a List[JField]`'
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val JArray(l) // => extracts to a List[JValue]`'
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JNull // => represents a JSON null`'
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these types are subclasses of `JValue`. The compile-time result of `parse`
    is `JValue`, which you normally need to cast to a concrete type using an extractor.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: 'The last type in the hierarchy is `JField`, which represents a key-value pair.
    `JField` is just a type alias for the `(String, JValue)` tuple. It is thus not
    a subtype of `JValue`. We can extract the key and value using the following extractor:'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE285]'
  id: totrans-1165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: Extracting fields using XPath
  id: totrans-1166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, you learned how to traverse JSON objects using extractors.
    In this section, we will look at a different way of traversing JSON objects and
    extracting specific fields: the *XPath DSL* (domain-specific language). XPath
    is a query language for traversing tree-like structures. It was originally designed
    for addressing specific nodes in an XML document, but it works just as well with
    JSON. We have already seen an example of XPath syntax when we extracted the stock
    price from the XML document returned by the "Markit on demand" API in [Chapter
    4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and Futures"), *Parallel
    Collections and Futures*. We extracted the node with tag `"LastPrice"` using `r
    \ "LastPrice"`. The `\` operator was defined by the `scala.xml` package.'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: 'The `json4s` package exposes a similar DSL to extract fields from `JObject`
    instances. For instance, we can extract the `"login"` field from the JSON object
    `jsonResponse`:'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE286]'
  id: totrans-1169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: 'This returns a `JValue` that we can transform into a Scala string using an
    extractor:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: 'Notice the similarity between the XPath DSL and traversing a filesystem: we
    can think of `JObject` instances as directories. Field names correspond to file
    names and the field value to the content of the file. This is more evident for
    nested structures. The `users` endpoint of the GitHub API does not have nested
    documents, so let''s try another endpoint. We will query the API for the repository
    corresponding to this book: "[https://api.github.com/repos/pbugnion/s4ds](https://api.github.com/repos/pbugnion/s4ds)".
    The response has the following structure:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  id: totrans-1173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: 'Let''s fetch this document and use the XPath syntax to extract the repository
    owner''s login name:'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  id: totrans-1175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: 'Again, this is much like traversing a filesystem: `jsonResponse \ "owner"`
    returns a `JObject` corresponding to the `"owner"` object. This `JObject` can,
    in turn, be queried for the `"login"` field, returning the value `JString(pbugnion)`
    associated with this key.'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: 'What if the API response is an array? The filesystem analogy breaks down somewhat.
    Let''s query the API endpoint listing Martin Odersky''s repositories: [https://api.github.com/users/odersky/repos](https://api.github.com/users/odersky/repos).
    The response is an array of JSON objects, each of which represents a repository:'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  id: totrans-1178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: 'Let''s fetch this and parse it as JSON:'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  id: totrans-1180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: 'This returns a `JArray`. The XPath DSL works in the same way on a `JArray`
    as on a `JObject`, but now, instead of returning a single `JValue`, it returns
    an array of fields matching the path in every object in the array. Let''s get
    the size of all Martin Odersky''s repositories:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  id: totrans-1182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'We now have a `JArray` of the values corresponding to the `"size"` field in
    every repository. We can iterate over this array with a `for` comprehension and
    use extractors to convert elements to Scala objects:'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: Thus, combining extractors with the XPath DSL gives us powerful, complementary
    tools to extract information from JSON objects.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: There is much more to the XPath syntax than we have space to cover here, including
    the ability to extract fields nested at any level of depth below the current root
    or fields that match a predicate or a certain type. We find that well-designed
    APIs obviate the need for many of these more powerful functions, but do consult
    the documentation (`json4s.org`) to get an overview of what you can do.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at extracting JSON directly into case classes.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: Extraction using case classes
  id: totrans-1188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we extracted specific fields from the JSON response
    using Scala extractors. We can do one better and extract full case classes.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: 'When moving beyond the REPL, programming best practice dictates that we move
    from `json4s` types to Scala objects as soon as possible rather than passing `json4s`
    types around the program. Converting from `json4s` types to Scala types (or case
    classes representing domain objects) is good practice because:'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: It decouples the program from the structure of the data that we receive from
    the API, something we have little control over.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It improves type safety: a `JObject` is, as far as the compiler is concerned,
    always a `JObject`, whatever fields it contains. By contrast, the compiler will
    never mistake a `User` for a `Repository`.'
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Json4s` lets us extract case classes directly from `JObject` instances, making
    writing the layer converting `JObject` instances to custom types easy.'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a case class representing a GitHub user:'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE294]'
  id: totrans-1195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: 'To extract a case class from a `JObject`, we must first define an implicit
    `Formats` value that defines how simple types should be serialized and deserialized.
    We will use the default `DefaultFormats` provided with `json4s`:'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE295]'
  id: totrans-1197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: 'We can now extract instances of `User`. Let''s do this for Martin Odersky:'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  id: totrans-1199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: 'This works as long as the object is well-formatted. The `extract` method looks
    for fields in the `JObject` that match the attributes of `User`. In this case,
    `extract` will note that the `JObject` contains the `"login": "odersky"` field
    and that `JString("odersky")` can be converted to a Scala string, so it binds
    `"odersky"` to the `login` attribute in `User`.'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
- en: 'What if the attribute names differ from the field names in the JSON object?
    We must first transform the object to have the correct fields. For instance, let''s
    rename the `login` attribute to `userName` in our `User` class:'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  id: totrans-1202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: 'If we try to use `extract[User]` on `jsonResponse`, we will get a mapping error
    because the deserializer is missing a `login` field in the response. We can fix
    this using the `transformField` method on `jsonResponse` to rename the `login`
    field:'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  id: totrans-1204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: 'What about optional fields? Let''s assume that the JSON object returned by
    the GitHub API does not always contain the login field. We could symbolize this
    in our object model by giving the `login` parameter the type `Option[String]`
    rather than `String`:'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE299]'
  id: totrans-1206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: 'This works just as you would expect. When the response contains a non-null
    `login` field, calling `extract[User]` will deserialize it to `Some(value)`, and
    when it''s missing or `JNull`, it will produce `None`:'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE300]'
  id: totrans-1208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: 'Let''s wrap this up in a small program. The program will take a single command-line
    argument, the user''s login name, extract a `User` instance, and print it to screen:'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE301]'
  id: totrans-1210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'We can run this from an SBT console as follows:'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  id: totrans-1212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: Concurrency and exception handling with futures
  id: totrans-1213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the program that we wrote in the previous section works, it is very brittle.
    It will crash if we enter a non-existent user name or the GitHub API changes or
    returns a badly-formatted response. We need to make it fault-tolerant.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: What if we also wanted to fetch multiple users? The program, as written, is
    entirely single-threaded. The `fetchUserFromUrl` method fires a call to the API
    and blocks until the API sends data back. A better solution would be to fetch
    multiple users in parallel.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
- en: 'As you learned in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, there are two straightforward
    ways to implement both fault tolerance and parallel execution: we can either put
    all the user names in a parallel collection and wrap the code for fetching and
    extracting the user in a `Try` block or we can wrap each query in a future.'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
- en: When querying web APIs, it is sometimes the case that a request can take abnormally
    long. To prevent this from blocking the other threads, it is preferable to rely
    on futures rather than parallel collections for concurrency, as we saw in the
    *Parallel collection or Future?* section at the end of [Chapter 4](part0036.xhtml#aid-12AK82
    "Chapter 4. Parallel Collections and Futures"), *Parallel Collections and Futures*.
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite the code from the previous section to handle fetching multiple
    users concurrently in a fault-tolerant manner. We will change the `fetchUserFromUrl`
    method to query the API asynchronously. This is not terribly different from [Chapter
    4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and Futures"), *Parallel
    Collections and Futures*, in which we queried the "Markit on demand" API:'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE303]'
  id: totrans-1219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: 'Let''s run the code through `sbt`:'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE304]'
  id: totrans-1221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: 'The code itself should be straightforward. All the concepts used here have
    been explored in this chapter or in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, apart from the
    last line:'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE305]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE305]'
- en: This statement tells the program to wait until all futures in our list have
    been completed. `Await.ready(..., 1 minute)` takes a future as its first argument
    and blocks execution until this future returns. The second argument is a time-out
    on this future. The only catch is that we need to pass a single future to `Await`
    rather than a list of futures. We can use `Future.sequence` to merge a collection
    of futures into a single future. This future will be completed when all the futures
    in the sequence have completed.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
- en: Authentication – adding HTTP headers
  id: totrans-1225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been using the GitHub API without authentication. This limits
    us to sixty requests per hour. Now that we can query the API in parallel, we could
    exceed this limit in seconds.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, GitHub is much more generous if you authenticate when you query
    the API. The limit increases to 5,000 requests per hour. You must have a GitHub
    user account to authenticate, so go ahead and create one now if you need to. After
    creating an account, navigate to [https://github.com/settings/tokens](https://github.com/settings/tokens)
    and click on the **Generate new token** button. Accept the default settings and
    enter a token description and a long hexadecimal number should appear on the screen.
    Copy the token for now.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: HTTP – a whirlwind overview
  id: totrans-1228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before using our newly generated token, let's take a few minutes to review how
    HTTP works.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: HTTP is a protocol for transferring information between different computers.
    It is the protocol that we have been using throughout the chapter, though Scala
    hid the details from us in the call to `Source.fromURL`. It is also the protocol
    that you use when you point your web browser to a website, for instance.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
- en: In HTTP, a computer will typically make a *request* to a remote server, and
    the server will send back a *response*. Requests contain a *verb*, which defines
    the type of request, and a URL identifying a *resource*. For instance, when we
    typed [api.github.com/users/pbugnion](http://api.github.com/users/pbugnion) in
    our browsers, this was translated into a GET (the verb) request for the `users/pbugnion`
    resource. All the calls that we have made so far have been GET requests. You might
    use a different type of request, for instance, a POST request, to modify (rather
    than just view) some content on GitHub.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the verb and resource, there are two more parts to an HTTP request:'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
- en: 'The *headers* include metadata about the request, such as the expected format
    and character set of the response or the authentication credentials. Headers are
    just a list of key-value pairs. We will pass the OAuth token that we have just
    generated to the API using the `Authorization` header. This Wikipedia article
    lists commonly used header fields: [en.wikipedia.org/wiki/List_of_HTTP_header_fields](http://en.wikipedia.org/wiki/List_of_HTTP_header_fields).'
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request body is not used in GET requests but becomes important for requests
    that modify the resource they query. For instance, if I wanted to create a new
    repository on GitHub programmatically, I would send a POST request to `/pbugnion/repos`.
    The POST body would then be a JSON object describing the new repository. We will
    not use the request body in this chapter.
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding headers to HTTP requests in Scala
  id: totrans-1235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will pass the OAuth token as a header with our HTTP request. Unfortunately,
    the `Source.fromURL` method is not particularly suited to adding headers when
    creating a GET request. We will, instead, use a library, `scalaj-http`.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add `scalaj-http` to the dependencies in our `build.sbt`:'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE306]'
  id: totrans-1238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: 'We can now import `scalaj-http`:'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  id: totrans-1240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: 'We start by creating an `HttpRequest` object:'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  id: totrans-1242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: 'We can now add the authorization header to the request (add your own token
    string here):'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  id: totrans-1244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: Tip
  id: totrans-1245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `.header` method returns a new `HttpRequest` instance. It does not modify
    the request in place. Thus, just calling `request.header(...)` does not actually
    add the header to request itself, which can be a source of confusion.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s fire the request. We do this through the request''s `asString` method,
    which queries the API, fetches the response, and parses it as a Scala `String`:'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  id: totrans-1248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: 'The response is made up of three components:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
- en: 'The status code, which should be `200` for a successful request:'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE311]'
  id: totrans-1251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE311]'
- en: 'The response body, which is the part that we are interested in:'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE312]'
  id: totrans-1253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE312]'
- en: 'The response headers (metadata about the response):'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE313]'
  id: totrans-1255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE313]'
- en: 'To verify that the authorization was successful, query the `X-RateLimit-Limit`
    header:'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE314]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: This value is the maximum number of requests per hour that you can make to the
    GitHub API from a single IP address.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have some understanding of how to add authentication to GET requests,
    let''s modify our script for fetching users to use the OAuth token for authentication.
    We first need to import `scalaj-http`:'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE315]'
  id: totrans-1260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: 'Injecting the value of the token into the code can be somewhat tricky. You
    might be tempted to hardcode it, but this prohibits you from sharing the code.
    A better solution is to use an *environment variable*. Environment variables are
    a set of variables present in your terminal session that are accessible to all
    processes running in that session. To get a list of the current environment variables,
    type the following on Linux or Mac OS:'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE316]'
  id: totrans-1262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: 'On Windows, the equivalent command is `SET`. Let''s add the GitHub token to
    the environment. Use the following command on Mac OS or Linux:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE317]'
  id: totrans-1264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: 'On Windows, use the following command:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: If you were to reuse this environment variable across many projects, entering
    `export GHTOKEN=...` in the shell for every session gets old quite quickly. A
    more permanent solution is to add `export GHTOKEN="e83638…"` to your shell configuration
    file (your `.bashrc` file if you are using Bash). This is safe provided your `.bashrc`
    is readable by the user only. Any new shell session will have access to the `GHTOKEN`
    environment variable.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: 'We can access environment variables from a Scala program using `sys.env`, which
    returns a `Map[String, String]` of the variables. Let''s add a `lazy val token`
    to our class, containing the `token` value:'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE319]'
  id: totrans-1269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: 'Now that we have the token, the only part of the code that must change, to
    add authentication, is the `fetchUserFromUrl` method:'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE320]'
  id: totrans-1271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: Additionally, we can, to gain clearer error messages, check that the response's
    status code is 200\. As this is straightforward, it is left as an exercise.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to query the GitHub API, converting the response
    to Scala objects. Of course, merely printing results to screen is not terribly
    interesting. In the next chapter, we will look at the next step of the data ingestion
    process: storing data in a database. We will query the GitHub API and store the
    results in a MongoDB database.'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](part0125.xhtml#aid-3N6MA1 "Chapter 13. Web APIs with Play"),
    *Web APIs with Play*, we will look at building our own simple web API.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-1276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GitHub API, with its extensive documentation, is a good place to explore
    how a rich API is constructed. It has a **Getting Started** section that is worth
    reading:'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developer.github.com/guides/getting-started/](https://developer.github.com/guides/getting-started/)'
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this is not specific to Scala: it uses cURL to query the API.'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
- en: Read the documentation ([http://json4s.org](http://json4s.org)) and source code
    ([https://github.com/json4s/json4s](https://github.com/json4s/json4s)) for `json4s`
    for a complete reference. There are many parts of this package that we have not
    explored, in particular, how to build JSON from Scala.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8. Scala and MongoDB
  id: totrans-1281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*, and [Chapter 6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick
    – A Functional Interface for SQL"), *Slick – A Functional Interface for SQL*,
    you learned how to insert, transform, and read data in SQL databases. These databases
    remain (and are likely to remain) very popular in data science, but NoSQL databases
    are emerging as strong contenders.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: 'The needs for data storage are growing rapidly. Companies are producing and
    storing more data points in the hope of acquiring better business intelligence.
    They are also building increasingly large teams of data scientists, who all need
    to access the data store. Maintaining constant access time as the data load increases
    requires taking advantage of parallel architectures: we need to distribute the
    database across several computers so that, as the load on the server increases,
    we can just add more machines to improve throughput.'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
- en: In MySQL databases, the data is naturally split across different tables. Complex
    queries necessitate joining across several tables. This makes partitioning the
    database across different computers difficult. NoSQL databases emerged to fill
    this gap.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn to interact with MongoDB, an open source database
    that offers high performance and can be distributed easily. MongoDB is one of
    the more popular NoSQL databases with a strong community. It offers a reasonable
    balance of speed and flexibility, making it a natural alternative to SQL for storing
    large datasets with uncertain query requirements, as might happen in data science.
    Many of the concepts and recipes in this chapter will apply to other NoSQL databases.
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB
  id: totrans-1286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MongoDB is a *document-oriented* database. It contains collections of documents.
    Each document is a JSON-like object:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE321]'
  id: totrans-1288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: Just as in JSON, a document is a set of key-value pairs, where the values can
    be strings, numbers, Booleans, dates, arrays, or subdocuments. Documents are grouped
    in collections, and collections are grouped in databases.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be thinking that this is not very different from SQL: a document
    is similar to a row and a collection corresponds to a table. There are two important
    differences:'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: The values in documents can be simple values, arrays, subdocuments, or arrays
    of subdocuments. This lets us encode one-to-many and many-to-many relationships
    in a single collection. For instance, consider the wizard collection. In SQL,
    if we wanted to store pseudonyms for each wizard, we would have to use a separate
    `wizard2pseudonym` table with a row for each wizard-pseudonym pair. In MongoDB,
    we can just use an array. In practice, this means that we can normally use a single
    document to represent an entity (a customer, transaction, or wizard, for instance).
    In SQL, we would normally have to join across several tables to retrieve all the
    information on a specific entity.
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MongoDB is *schemaless*. Documents in a collection can have varying sets of
    fields with different types for the same field across different documents. In
    practice, MongoDB collections have a loose schema enforced either client side
    or by convention: most documents will have a subset of the same fields, and fields
    will, in general, contain the same data type. Having a flexible schema makes adjusting
    the data structure easy as there is no need for time-consuming `ALTER` `TABLE`
    statements. The downside is that there is no easy way of enforcing our flexible
    schema on the database side.'
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note the `_id` field: this is a unique key. MongoDB will generate one automatically
    if we insert a document without an `_id` field.'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
- en: This chapter gives recipes for interacting with a MongoDB database from Scala,
    including maintaining type safety and best practices. We will not cover advanced
    MongoDB functionality (such as aggregation or distributing the database). We will
    assume that you have MongoDB installed on your computer ([http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)).
    It will also help to have a very basic knowledge of MongoDB (we discuss some references
    at the end of this chapter, but any basic tutorial available online will be sufficient
    for the needs of this chapter).
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to MongoDB with Casbah
  id: totrans-1295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official MongoDB driver for Scala is called **Casbah**. Rather than a fully-fledged
    driver, Casbah wraps the Java Mongo driver, providing a more functional interface.
    There are other MongoDB drivers for Scala, which we will discuss briefly at the
    end of this chapter. For now, we will stick to Casbah.
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by adding Casbah to our `build.sbt` file:'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE322]'
  id: totrans-1298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: 'Casbah also expects `slf4j` bindings (a Scala logging framework) to be available,
    so let''s also add `slf4j-nop`:'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE323]'
  id: totrans-1300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: 'We can now start an SBT console and import Casbah in the Scala shell:'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE324]'
  id: totrans-1302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: 'This connects to a MongoDB server on the default host (`localhost`) and default
    port (`27017`). To connect to a different server, pass the host and port as arguments
    to `MongoClient`:'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE325]'
  id: totrans-1304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE325]'
- en: 'Note that creating a client is a lazy operation: it does not attempt to connect
    to the server until it needs to. This means that if you enter the wrong URL or
    password, you will not know about it until you try and access documents on the
    server.'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a connection to the server, accessing a database is as simple
    as using the client''s `apply` method. For instance, to access the `github` database:'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE326]'
  id: totrans-1307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE326]'
- en: 'We can then access the `"users"` collection:'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE327]'
  id: totrans-1309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE327]'
- en: Connecting with authentication
  id: totrans-1310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB supports several different authentication mechanisms. In this section,
    we will assume that your server is using the **SCRAM-SHA-1** mechanism, but you
    should find adapting the code to a different type of authentication straightforward.
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way of authenticating is to pass `username` and `password` in the
    URI when connecting:'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE328]'
  id: totrans-1313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE328]'
- en: 'In general, you will not want to put your password in plain text in the code.
    You can either prompt for a password on the command line or pass it through environment
    variables, as we did with the GitHub OAuth token in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*. The following code snippet demonstrates how
    to pass credentials through the environment:'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE329]'
  id: totrans-1315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE329]'
- en: 'You can run it through SBT as follows:'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE330]'
  id: totrans-1317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE330]'
- en: Inserting documents
  id: totrans-1318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s insert some documents into our newly created database. We want to store
    information about GitHub users, using the following document structure:'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE331]'
  id: totrans-1320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE331]'
- en: 'Casbah provides a `DBObject` class to represent MongoDB documents (and subdocuments)
    in Scala. Let''s start by creating a `DBObject` instance for each repository subdocument:'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE332]'
  id: totrans-1322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE332]'
- en: As you can see, a `DBObject` is just a list of key-value pairs, where the keys
    are strings. The values have compile-time type `AnyRef`, but Casbah will fail
    (at runtime) if you try to add a value that cannot be serialized.
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create `DBObject` instances from lists of key-value pairs directly.
    This is particularly useful when converting from a Scala map to a `DBObject`:'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE333]'
  id: totrans-1325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE333]'
- en: 'The `DBObject` class provides many of the same methods as a map. For instance,
    we can address individual fields:'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE334]'
- en: 'We can construct a new object by adding a field to an existing object:'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE335]'
  id: totrans-1329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE335]'
- en: 'Note the return type: `mutable.Map[String,Any]`. Rather than implementing methods
    such as `+` directly, Casbah adds them to `DBObject` by providing an implicit
    conversion to and from `mutable.Map`.'
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
- en: 'New `DBObject` instances can also be created by concatenating two existing
    instances:'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: '`DBObject` instances can then be inserted into a collection using the `+=`
    operator. Let''s insert our first document into the `user` collection:'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  id: totrans-1334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: A database containing a single document is a bit boring, so let's add a few
    more documents queried directly from the GitHub API. You learned how to query
    the GitHub API in the previous chapter, so we won't dwell on how to do this here.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code examples for this chapter, we have provided a class called `GitHubUserIterator`
    that queries the GitHub API (specifically the `/users` endpoint) for user documents,
    converts them to a case class, and offers them as an iterator. You will find the
    class in the code examples for this chapter (available on GitHub at [https://github.com/pbugnion/s4ds/tree/master/chap08](https://github.com/pbugnion/s4ds/tree/master/chap08))
    in the `GitHubUserIterator.scala` file. The easiest way to have access to the
    class is to open an SBT console in the directory of the code examples for this
    chapter. The API then fetches users in increasing order of their login ID:'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE338]'
  id: totrans-1337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: '`GitHubUserIterator` returns instances of the `User` case class, defined as
    follows:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: Let's write a short program to fetch 500 users and insert them into the MongoDB
    database. We will need to authenticate with the GitHub API to retrieve these users.
    The constructor for `GitHubUserIterator` takes the GitHub OAuth token as an optional
    argument. We will inject the token through the environment, as we did in the previous
    chapter.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: We first give the entire code listing before breaking it down—if you are typing
    this out, you will need to copy `GitHubUserIterator.scala` from the code examples
    for this chapter to the directory in which you are running this to access the
    `GitHubUserIterator` class. The class relies on `scalaj-http` and `json4s`, so
    either copy the `build.sbt` file from the code examples or specify those packages
    as dependencies in your `build.sbt` file.
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE340]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE340]'
- en: 'Before diving into the details of how this program works, let''s run it through
    SBT. You will want to query the API with authentication to avoid hitting the rate
    limit. Recall that we need to set the `GHTOKEN` environment variable:'
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE341]'
  id: totrans-1344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE341]'
- en: 'The program will take about five minutes to run (depending on your Internet
    connection). To verify that the program works, we can query the number of documents
    in the `users` collection of the `github` database:'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE342]'
  id: totrans-1346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE342]'
- en: Let's break the code down. We first load the OAuth token to authenticate with
    the GithHub API. The token is stored as an environment variable, `GHTOKEN`. The
    `token` variable is a `lazy val`, so the token is loaded only when we formulate
    the first request to the API. We have already used this pattern in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*.
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define two methods to transform from classes in the domain model to
    `DBObject` instances:'
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE343]'
  id: totrans-1349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE343]'
- en: 'Armed with these two methods, we can add users to our MongoDB collection easily:'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE344]'
  id: totrans-1351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE344]'
- en: 'We used currying to split the arguments of `insertUsers`. This lets us use
    `insertUsers` as a function factory:'
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE345]'
  id: totrans-1353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE345]'
- en: 'This creates a new method, `inserter`, with signature `Iterable[User] => Unit`
    that inserts users into `coll`. To see how this might come in useful, let''s write
    a function to wrap the whole data ingestion process. This is how a first attempt
    at this function could look:'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE346]'
  id: totrans-1355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE346]'
- en: 'Notice how `ingestUsers` takes a method that specifies how the list of users
    is inserted into the database as its second argument. This function encapsulates
    the entire code specific to insertion into a MongoDB collection. If we decide,
    at some later date, that we hate MongoDB and must insert the documents into a
    SQL database or write them to a flat file, all we need to do is pass a different
    `inserter` function to `ingestUsers`. The rest of the code remains the same. This
    demonstrates the increased flexibility afforded by using higher-order functions:
    we can easily build a framework and let the client code plug in the components
    that it needs.'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ingestUsers` method, as defined previously, has one problem: if the `nusers`
    value is large, it will consume a lot of memory in constructing the entire list
    of users. A better solution would be to break it down into batches: we fetch a
    batch of users from the API, insert them into the database, and move on to the
    next batch. This allows us to control memory usage by changing the batch size.
    It is also more fault tolerant: if the program crashes, we can just restart from
    the last successfully inserted batch.'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.grouped` method, available on all iterables, is useful for batching.
    It returns an iterator over fragments of the original iterable:'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE347]'
  id: totrans-1359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE347]'
- en: 'Let''s rewrite our `ingestUsers` method to use batches. We will also add a
    progress report after each batch in order to give the user some feedback:'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE348]'
  id: totrans-1361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE348]'
- en: 'Let''s look at the highlighted line more closely. We start from the user iterator,
    `it`. We then take the first `nusers`. This returns an `Iterator[User]` that,
    instead of happily churning through every user in the GitHub database, will terminate
    after `nusers`. We then group this iterator into batches of 100 users. The `.grouped`
    method returns `Iterator[Iterator[User]]`. We then zip each batch with its index
    so that we know which batch we are currently processing (we use this in the `print`
    statement). The `.zipWithIndex` method returns `Iterator[(Iterator[User], Int)]`.
    We unpack this tuple in the loop using a case statement that binds `users` to
    `Iterator[User]` and `batchNumber` to the index. Let''s run this through SBT:'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE349]'
  id: totrans-1363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE349]'
- en: Extracting objects from the database
  id: totrans-1364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have a database populated with a few users. Let''s query this database
    from the REPL:'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE350]'
  id: totrans-1366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE350]'
- en: 'The `findOne` method returns a single `DBObject` object wrapped in an option,
    unless the collection is empty, in which case it returns `None`. We must therefore
    use the `get` method to extract the object:'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE351]'
  id: totrans-1368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE351]'
- en: 'As you learned earlier in this chapter, `DBObject` is a map-like object with
    keys of type `String` and values of type `AnyRef`:'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE352]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE352]'
- en: 'In general, we want to restore compile-time type information as early as possible
    when importing objects from the database: we do not want to pass `AnyRef`s around
    when we can be more specific. We can use the `getAs` method to extract a field
    and cast it to a specific type:'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE353]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE353]'
- en: 'If the field is missing in the document or if the value cannot be cast, `getAs`
    will return `None`:'
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE354]'
  id: totrans-1374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE354]'
- en: The astute reader may note that the interface provided by `getAs[T]` is similar
    to the `read[T]` method that we defined on a JDBC result set in [Chapter 5](part0040.xhtml#aid-164MG1
    "Chapter 5. Scala and SQL through JDBC"), *Scala and SQL through JDBC*.
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
- en: 'If `getAs` fails (for instance, because the field is missing), we can use the
    `orElse` partial function to recover:'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE355]'
  id: totrans-1377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE355]'
- en: 'The `getAsOrElse` method allows us to substitute a default value if the cast
    fails:'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE356]'
  id: totrans-1379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: 'Note that we can also use `getAsOrElse` to throw an exception:'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  id: totrans-1381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: 'Arrays embedded in documents can be cast to `List[T]` objects, where `T` is
    the type of elements in the array:'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE358]'
  id: totrans-1383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: 'Retrieving a single document at a time is not very useful. To retrieve all
    the documents in a collection, use the `.find` method:'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE359]'
  id: totrans-1385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: 'This returns an iterator of `DBObject`s. To actually fetch the documents from
    the database, you need to materialize the iterator by transforming it into a collection,
    using, for instance, `.toList`:'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE360]'
  id: totrans-1387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE360]'
- en: 'Let''s bring all of this together. We will write a toy program that prints
    the average number of repositories per user in our collection. The code works
    by fetching every document in the collection, extracting the number of repositories
    from each document, and then averaging over these:'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE361]'
  id: totrans-1389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE361]'
- en: 'Let''s run this through SBT:'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  id: totrans-1391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE362]'
- en: The code starts with the `extractNumber` function, which extracts the number
    of repositories from each `DBObject`. The return value is `None` if the document
    does not contain the `repos` field.
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
- en: The main body of the code starts by creating an iterator over `DBObject`s in
    the collection. This iterator is then mapped through the `extractNumber` function,
    which transforms it into an iterator of `Option[Int]`. We then run `.collect`
    on this iterator to collect all the values that are not `None`, converting from
    `Option[Int]` to `Int` in the process. Only then do we materialize the iterator
    to a list using `.toList`. The resulting list, `wellFormattedNumbers`, has the
    `List[Int]` type. We then just take the mean of this list and print it to screen.
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, besides the `extractNumber` function, none of this program deals
    with Casbah-specific types: the iterator returned by `.find()` is just a Scala
    iterator. This makes Casbah straightforward to use: the only data type that you
    need to familiarize yourself with is `DBObject` (compare this with JDBC''s `ResultSet`,
    which we had to explicitly wrap in a stream, for instance).'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: Complex queries
  id: totrans-1395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know how to convert `DBObject` instances to custom Scala classes. In
    this section, you will learn how to construct queries that only return a subset
    of the documents in the collection.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, you learned to retrieve all the documents in a collection
    as follows:'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE363]'
  id: totrans-1398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: The `collection.find()` method returns an iterator over all the documents in
    the collection. By calling `.toList` on this iterator, we materialize it to a
    list.
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: 'We can customize which documents are returned by passing a query document to
    the `.find` method. For instance, we can retrieve documents for a specific login
    name:'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE364]'
  id: totrans-1401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE364]'
- en: 'MongoDB queries are expressed as `DBObject` instances. Keys in the `DBObject`
    correspond to fields in the collection''s documents, and the values are expressions
    controlling the allowed values of this field. Thus, `DBObject("login" -> "mojombo")`
    will select all the documents for which the `login` field is `mojombo`. Using
    a `DBObject` instance to represent a query might seem a little obscure, but it
    will quickly make sense if you read the MongoDB documentation ([https://docs.mongodb.org/manual/core/crud-introduction/](https://docs.mongodb.org/manual/core/crud-introduction/)):
    queries are themselves just JSON objects in MongoDB. Thus, the fact that the query
    in Casbah is represented as a `DBObject` is consistent with other MongoDB client
    implementations. It also allows someone familiar with MongoDB to start writing
    Casbah queries in no time.'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
- en: 'MongoDB supports more complex queries. For instance, to query everyone with
    `"github_id"` between `20` and `30`, we can write the following query:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE365]'
  id: totrans-1404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE365]'
- en: We limit the range of values that `github_id` can take with `DBObject("$gte"
    -> 20, "$lt" -> 30)`. The `"$gte"` string indicates that `github_id` must be greater
    or equal to `20`. Similarly, `"$lt"` denotes the *less than* operator. To get
    a full list of operators that you can use when querying, consult the MongoDB reference
    documentation ([http://docs.mongodb.org/manual/reference/operator/query/](http://docs.mongodb.org/manual/reference/operator/query/)).
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have only looked at queries on top-level fields. Casbah also lets
    us query fields in subdocuments and arrays using the *dot* notation. In the context
    of array values, this will return all the documents for which at least one value
    in the array matches the query. For instance, to retrieve all users who have a
    repository whose main language is Scala:'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE366]'
  id: totrans-1407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE366]'
- en: Casbah query DSL
  id: totrans-1408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using `DBObject` instances to express queries can be very verbose and somewhat
    difficult to read. Casbah provides a DSL to express queries much more succinctly.
    For instance, to get all the documents with the `github_id` field between `20`
    and `30`, we would write the following:'
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE367]'
  id: totrans-1410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE367]'
- en: The operators provided by the DSL will automatically construct `DBObject` instances.
    Using the DSL operators as much as possible generally leads to much more readable
    and maintainable code.
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
- en: 'Going into the full details of the query DSL is beyond the scope of this chapter.
    You should find it quite easy to use. For a full list of the operators supported
    by the DSL, refer to the Casbah documentation at [http://mongodb.github.io/casbah/3.0/reference/query_dsl/](http://mongodb.github.io/casbah/3.0/reference/query_dsl/).
    We summarize the most important operators here:'
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
- en: '| Operators | Description |'
  id: totrans-1413
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-1414
  prefs: []
  type: TYPE_TB
- en: '| `"login" $eq "mojombo"` | This selects documents whose `login` field is exactly
    `mojombo` |'
  id: totrans-1415
  prefs: []
  type: TYPE_TB
- en: '| `"login" $ne "mojombo"` | This selects documents whose `login` field is not
    `mojombo` |'
  id: totrans-1416
  prefs: []
  type: TYPE_TB
- en: '| `"github_id" $gt 1 $lt 20` | This selects documents with `github_id` greater
    than `1` and less than `20` |'
  id: totrans-1417
  prefs: []
  type: TYPE_TB
- en: '| `"github_id" $gte 1 $lte 20` | This selects documents with `github_id` greater
    than or equal to `1` and less than or equal to `20` |'
  id: totrans-1418
  prefs: []
  type: TYPE_TB
- en: '| `"login" $in ("mojombo", "defunkt")` | The `login` field is either `mojombo`
    or `defunkt` |'
  id: totrans-1419
  prefs: []
  type: TYPE_TB
- en: '| `"login" $nin ("mojombo", "defunkt")` | The `login` field is not `mojombo`
    or `defunkt` |'
  id: totrans-1420
  prefs: []
  type: TYPE_TB
- en: '| `"login" $regex "^moj.*"` | The `login` field matches the particular regular
    expression |'
  id: totrans-1421
  prefs: []
  type: TYPE_TB
- en: '| `"login" $exists true` | The `login` field exists |'
  id: totrans-1422
  prefs: []
  type: TYPE_TB
- en: '| `$or("login" $eq "mojombo", "github_id" $gte 22)` | Either the `login` field
    is `mojombo` or the `github_id` field is greater or equal to `22` |'
  id: totrans-1423
  prefs: []
  type: TYPE_TB
- en: '| `$and("login" $eq "mojombo", "github_id" $gte 22)` | The `login` field is
    `mojombo` and the `github_id` field is greater or equal to `22` |'
  id: totrans-1424
  prefs: []
  type: TYPE_TB
- en: 'We can also use the *dot* notation to query arrays and subdocuments. For instance,
    the following query will count all the users who have a repository in Scala:'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE368]'
  id: totrans-1426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE368]'
- en: Custom type serialization
  id: totrans-1427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only tried to serialize and deserialize simple types. What
    if we wanted to decode the language field in the repository array to an enumeration
    rather than a string? We might, for instance, define the following enumeration:'
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE369]'
  id: totrans-1429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE369]'
- en: 'Casbah lets us define custom serializers tied to a specific Scala type: we
    can inform Casbah that whenever it encounters an instance of the `Language.Value`
    type in a `DBObject`, the instance should be passed through a custom transformer
    that will convert it to, for instance, a string, before writing it to the database.'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: 'To define a custom serializer, we need to define a class that extends the `Transformer`
    trait. This trait exposes a single method, `transform(o:AnyRef):AnyRef`. Let''s
    define a `LanguageTransformer` trait that transforms from `Language.Value` to
    `String`:'
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE370]'
  id: totrans-1432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE370]'
- en: 'We now need to register the trait to be used whenever an instance of type `Language.Value`
    needs to be decoded. We can do this using the `addEncodingHook` method:'
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE371]'
  id: totrans-1434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE371]'
- en: 'We can now construct `DBObject` instances containing values of the `Language`
    enumeration:'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE372]'
  id: totrans-1436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE372]'
- en: 'What about the reverse? How do we tell Casbah to read the `"language"` field
    as `Language.Value`? This is not possible with custom deserializers: `"Scala"`
    is now stored as a string in the database. Thus, when it comes to deserialization,
    `"Scala"` is no different from, say, `"mojombo"`. We thus lose type information
    when `"Scala"` is serialized.'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, while custom encoding hooks are useful for serialization, they are much
    less useful when deserializing. A cleaner, more consistent alternative to customize
    both serialization and deserialization is to use *type classes*. We have already
    covered how to use these extensively in [Chapter 5](part0040.xhtml#aid-164MG1
    "Chapter 5. Scala and SQL through JDBC"), *Scala and SQL through JDBC*, in the
    context of serializing to and from SQL. The procedure here would be very similar:'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
- en: Define a `MongoReader[T]` type class with a `read(v:Any)`:`T` method.
  id: totrans-1439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define concrete implementations of `MongoReader` in the `MongoReader` companion
    object for all types of interest, such as `String`, `Language.Value`.
  id: totrans-1440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enrich `DBObject` with a `read[T:MongoReader]` method using the *pimp my library*
    pattern.
  id: totrans-1441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, the implementation of `MongoReader` for `Language.Value` would
    be as follows:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE373]'
  id: totrans-1443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE373]'
- en: We could then do the same with a `MongoWriter` type class. Using type classes
    is an idiomatic and extensible approach to custom serialization and deserialization.
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
- en: We provide a complete example of type classes in the code examples associated
    with this chapter (in the `typeclass` directory).
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Casbah
  id: totrans-1446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have only considered Casbah in this chapter. There are, however, other drivers
    for MongoDB.
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
- en: '*ReactiveMongo* is a driver that focusses on asynchronous read and writes to
    and from the database. All queries return a future, forcing asynchronous behavior.
    This fits in well with data streams or web applications.'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
- en: '*Salat* sits at a higher level than Casbah and aims to provide easy serialization
    and deserialization of case classes.'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: A full list of drivers is available at [https://docs.mongodb.org/ecosystem/drivers/scala/](https://docs.mongodb.org/ecosystem/drivers/scala/).
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1451
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to interact with a MongoDB database. By weaving
    the constructs learned in the previous chapter—pulling information from a web
    API—with those learned in this chapter, we can now build a concurrent, reactive
    program for data ingestion.
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn to build distributed, concurrent structures
    with greater flexibility using Akka actors.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-1454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*MongoDB: The Definitive Guide*, by *Kristina Chodorow*, is a good introduction
    to MongoDB. It does not cover interacting with MongoDB in Scala at all, but Casbah
    is intuitive enough for anyone familiar with MongoDB.'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the MongoDB documentation ([https://docs.mongodb.org/manual/](https://docs.mongodb.org/manual/))
    provides an in-depth discussion of MongoDB.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
- en: Casbah itself is well-documented ([http://mongodb.github.io/casbah/3.0/](http://mongodb.github.io/casbah/3.0/)).
    There is a *Getting Started* guide that is somewhat similar to this chapter and
    a complete reference guide that will fill in the gaps left by this chapter.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: 'This gist, [https://gist.github.com/switzer/4218526](https://gist.github.com/switzer/4218526),
    implements type classes to serialize and deserialize objects in the domain model
    to `DBObject`s. The premise is a little different from the suggested usage of
    type classes in this chapter: we are converting from Scala types to `AnyRef` to
    be used as values in `DBObject`. However, the two approaches are complementary:
    one could imagine a set of type classes to convert from `User` or `Repo` to `DBObject`
    and another to convert from `Language.Value` to `AnyRef`.'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9. Concurrency with Akka
  id: totrans-1459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much of this book focusses on taking advantage of multicore and distributed
    architectures. In [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections
    and Futures"), *Parallel Collections and Futures*, you learned how to use parallel
    collections to distribute batch processing problems over several threads and how
    to perform asynchronous computations using futures. In [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, we applied this knowledge to query the GitHub
    API with several concurrent threads.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency abstractions such as futures and parallel collections simplify the
    enormous complexity of concurrent programming by limiting what you can do. Parallel
    collections, for instance, force you to phrase your parallelization problem as
    a sequence of pure functions on collections.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
- en: Actors offer a different way of thinking about concurrency. Actors are very
    good at encapsulating *state*. Managing state shared between different threads
    of execution is probably the most challenging part of developing concurrent applications,
    and, as we will discover in this chapter, actors make it manageable.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
- en: GitHub follower graph
  id: totrans-1463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, we explored the GitHub API, learning how to query
    the API and parse the results using *json-4s*.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that we want to extract the GitHub follower graph: we want a
    program that will start from a particular user, extract this user followers, and
    then extract their followers until we tell it to stop. The catch is that we don''t
    know ahead of time what URLs we need to fetch: when we download the login names
    of a particular user''s followers, we need to verify whether we have fetched these
    users previously. If not, we add them to a queue of users whose followers we need
    to fetch. Algorithm aficionados might recognize this as *breadth-first search*.'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s outline how we might write this in a single-threaded way. The central
    components are a set of visited users and queue of future users to visit:'
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE374]'
  id: totrans-1467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE374]'
- en: Here, the `fetchFollowersForUser` method has signature `String => Iterable[String]`
    and is responsible for taking a login name, transforming it into a URL in the
    GitHub API, querying the API, and extracting a list of followers from the response.
    We will not implement it here, but you can find a complete example in the `chap09/single_threaded`
    directory of the code examples for this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    You should have all the tools to implement this yourself if you have read [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: While this works, it will be painfully slow. The bottleneck is clearly the `fetchFollowersForUser`
    method, in particular, the part that queries the GitHub API. This program does
    not lend itself to the concurrency constructs that we have seen earlier in the
    book because we need to protect the state of the program, embodied by the user
    queue and set of fetched users, from race conditions. Note that it is not just
    a matter of making the queue and set thread-safe. We must also keep the two synchronized.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
- en: '*Actors* offer an elegant abstraction to encapsulate state. They are lightweight
    objects that each perform a single task (possibly repeatedly) and communicate
    with each other by passing messages. The internal state of an actor can only be
    changed from within the actor itself. Importantly, actors only process messages
    one at a time, effectively preventing race conditions.'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
- en: 'By hiding program state inside actors, we can reason about the program more
    effectively: if a bug is introduced that makes this state inconsistent, the culprit
    will be localized entirely in that actor.'
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
- en: Actors as people
  id: totrans-1472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you learned that an actor encapsulates state, interacting
    with the outside world through messages. Actors make concurrent programming more
    intuitive because they behave a little bit like an ideal workforce.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s think of an actor system representing a start-up with five people. There''s
    Chris, the CEO, and Mark, who''s in charge of marketing. Then there''s Sally,
    who heads the engineering team. Sally has two minions, Bob and Kevin. As every
    good organization needs an organizational chart, refer to the following diagram:'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
- en: '![Actors as people](img/image01194.jpeg)'
  id: totrans-1475
  prefs: []
  type: TYPE_IMG
- en: Let's say that Chris receives an order. He will look at the order, decide whether
    it is something that he can process himself, and if not, he will forward it to
    Mark or Sally. Let's assume that the order asks for a small program so Bob forwards
    the order to Sally. Sally is very busy working on a backlog of orders so she cannot
    process the order message straightaway, and it will just sit in her mailbox for
    a short while. When she finally gets round to processing the order, she might
    decide to split the order into several parts, some of which she will give to Kevin
    and some to Bob.
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
- en: As Bob and Kevin complete items, they will send messages back to Sally to inform
    her. When every part of the order is fulfilled, Sally will aggregate the parts
    together and message either the customer directly or Chris with the results.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of keeping track of which jobs must be fulfilled to complete the order
    rests with Sally. When she receives messages from Bob and Kevin, she must update
    her list of tasks in progress and check whether every task related to this order
    is complete. This sort of coordination would be more challenging with traditional
    *synchronize* blocks: every access to the list of tasks in progress and to the
    list of completed tasks would need to be synchronized. By embedding this logic
    in Sally, who can only process a single message at a time, we can be sure that
    there will not be race conditions.'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: 'Our start-up works well because each person is responsible for doing a single
    thing: Chris either delegates to Mark or Sally, Sally breaks up orders into several
    parts and assigns them to Bob and Kevin, and Bob and Kevin fulfill each part.
    You might think "hold on, all the logic is embedded in Bob and Kevin, the employees
    at the bottom of the ladder who do all the actual work". Actors, unlike employees,
    are cheap, so if the logic embedded in an actor gets too complicated, it is easy
    to introduce additional layers of delegation until tasks get simple enough.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
- en: The employees in our start-up refuse to multitask. When they get a piece of
    work, they process it completely and then move on to the next task. This means
    that they cannot get muddled by the complexities of multitasking. Actors, by processing
    a single message at a time, greatly reduce the scope for introducing concurrency
    errors such as race conditions.
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, by offering an abstraction that programmers can intuitively
    understand—that of human workers—Akka makes reasoning about concurrency easier.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
- en: Hello world with Akka
  id: totrans-1482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s install Akka. We add it as a dependency to our `build.sbt` file:'
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE375]'
  id: totrans-1484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE375]'
- en: 'We can now import Akka as follows:'
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE376]'
  id: totrans-1486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE376]'
- en: 'For our first foray into the world of actors, we will build an actor that echoes
    every message it receives. The code examples for this section are in a directory
    called `chap09/hello_akka` in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)):'
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE377]'
  id: totrans-1488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE377]'
- en: Let's pick this example apart, starting with the constructor. Our actor class
    must extend `Actor`. We also add `ActorLogging`, a utility trait that adds the
    `log` attribute.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: The `Echo` actor exposes a single method, `receive`. This is the actor's only
    way of communicating with the external world. To be useful, all actors must expose
    a `receive` method. The `receive` method is a partial function, typically implemented
    with multiple `case` statements. When an actor starts processing a message, it
    will match it against every `case` statement until it finds one that matches.
    It will then execute the corresponding block.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
- en: Our echo actor accepts a single type of message, a plain string. When this message
    gets processed, the actor waits for half a second and then echoes the message
    to the log file.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s instantiate a couple of Echo actors and send them messages:'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE378]'
  id: totrans-1493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE378]'
- en: 'Running this gives us the following output:'
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE379]'
  id: totrans-1495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE379]'
- en: 'Note that the `echo1` and `echo2` actors are clearly acting concurrently: `hello
    echo1` and `hello echo2` are logged at the same time. The second message, passed
    to `echo1`, gets processed after the actor has finished processing `hello echo1`.'
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few different things to note:'
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
- en: To start instantiating actors, we must first create an actor system. There is
    typically a single actor system per application.
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The way in which we instantiate actors looks a little strange. Instead of calling
    the constructor, we create an actor properties object, `Props[T]`. We then ask
    the actor system to create an actor with these properties. In fact, we never instantiate
    actors with `new`: they are either created by calling the `actorOf` method in
    the actor system or a similar method from within another actor (more on this later).'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We never call an actor's methods from outside that actor. The only way to interact
    with the actor is to send messages to it. We do this using the *tell* operator,
    `!`. There is thus no way to mess with an actor's internals from outside that
    actor (or at least, Akka makes it difficult to mess with an actor's internals).
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
- en: Case classes as messages
  id: totrans-1501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our "hello world" example, we constructed an actor that is expected to receive
    a string as message. Any object can be passed as a message, provided it is immutable.
    It is very common to use case classes to represent messages. This is better than
    using strings because of the additional type safety: the compiler will catch a
    typo in a case class but not in a string.'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite our `EchoActor` to accept instances of case classes as messages.
    We will make it accept two different messages: `EchoMessage(message)` and `EchoHello`,
    which just echoes a default message. The examples for this section and the next
    are in the `chap09/hello_akka_case_classes` directory in the sample code provided
    with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: 'A common Akka pattern is to define the messages that an actor can receive in
    the actor''s companion object:'
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE380]'
  id: totrans-1505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE380]'
- en: 'Let''s change the actor definition to accept these messages:'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE381]'
  id: totrans-1507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE381]'
- en: 'We can now send `EchoHello` and `EchoMessage` to our actors:'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE382]'
  id: totrans-1509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE382]'
- en: Actor construction
  id: totrans-1510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actor construction is a common source of difficulty for people new to Akka.
    Unlike (most) ordinary objects, you never instantiate actors explicitly. You would
    never write, for instance, `val echo = new EchoActor`. In fact, if you try this,
    Akka raises an exception.
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating actors in Akka is a two-step process: you first create a `Props` object,
    which encapsulates the properties needed to construct an actor. The way to construct
    a `Props` object differs depending on whether the actor takes constructor arguments.
    If the constructor takes no arguments, we simply pass the actor class as a type
    parameter to `Props`:'
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE383]'
  id: totrans-1513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE383]'
- en: 'If we have an actor whose constructor does take arguments, we must pass these
    as additional arguments when defining the `Props` object. Let''s consider the
    following actor, for instance:'
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE384]'
  id: totrans-1515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE384]'
- en: 'We pass the constructor arguments to the `Props` object as follows:'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE385]'
  id: totrans-1517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE385]'
- en: 'The `Props` instance just embodies the configuration for creating an actor.
    It does not actually create anything. To create an actor, we pass the `Props`
    instance to the `system.actorOf` method, defined on the `ActorSystem` instance:'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE386]'
  id: totrans-1519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE386]'
- en: 'The `name` parameter is optional but is useful for logging and error messages.
    The value returned by `.actorOf` is not the actor itself: it is a *reference*
    to the actor (it helps to think of it as an address that the actor lives at) and
    has the `ActorRef` type. `ActorRef` is immutable, but it can be serialized and
    duplicated without affecting the underlying actor.'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another way to create actors besides calling `actorOf` on the actor
    system: each actor exposes a `context.actorOf` method that takes a `Props` instance
    as its argument. The context is only accessible from within the actor:'
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE387]'
  id: totrans-1522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE387]'
- en: 'The difference between an actor created from the actor system and an actor
    created from another actor''s context lies in the actor hierarchy: each actor
    has a parent. Any actor created within another actor''s context will have that
    actor as its parent. An actor created by the actor system has a predefined actor,
    called the *user guardian*, as its parent. We will understand the importance of
    the actor hierarchy when we study the actor lifecycle at the end of this chapter.'
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
- en: 'A very common idiom is to define a `props` method in an actor''s companion
    object that acts as a factory method for `Props` instances for that actor. Let''s
    amend the `EchoActor` companion object:'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE388]'
  id: totrans-1525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE388]'
- en: 'We can then instantiate the actor as follows:'
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE389]'
  id: totrans-1527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE389]'
- en: Anatomy of an actor
  id: totrans-1528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into a full-blown application, let''s look at the different components
    of the actor framework and how they fit together:'
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
- en: '**Mailbox**: A mailbox is basically a queue. Each actor has its own mailbox.
    When you send a message to an actor, the message lands in its mailbox and does
    nothing until the actor takes it off the queue and passes it through its `receive`
    method.'
  id: totrans-1530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messages**: Messages make synchronization between actors possible. A message
    can have any type with the sole requirement that it should be immutable. In general,
    it is better to use case classes or case objects to gain the compiler''s help
    in checking message types.'
  id: totrans-1531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actor reference**: When we create an actor using `val echo1 = system.actorOf(Props[EchoActor])`,
    `echo1` has type `ActorRef`. An `ActorRef` is a proxy for an actor and is what
    the rest of the world interacts with: when you send a message, you send it to
    the `ActorRef`, not to the actor directly. In fact, you can never obtain a handle
    to an actor directly in Akka. An actor can obtain an `ActorRef` for itself using
    the `.self` method.'
  id: totrans-1532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actor context**: Each actor has a `context` attribute through which you can
    access methods to create or access other actors and find information about the
    outside world. We have already seen how to create new actors with `context.actorOf(props)`.
    We can also obtain a reference to an actor''s parent through `context.parent`.
    An actor can also stop another actor with `context.stop(actorRef)`, where `actorRef`
    is a reference to the actor that we want to stop.'
  id: totrans-1533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dispatcher**: The dispatcher is the machine that actually executes the code
    in an actor. The default dispatcher uses a fork/join thread pool. Akka lets us
    use different dispatchers for different actors. Tweaking the dispatcher can be
    useful to optimize the performance and give priority to certain actors. The dispatcher
    that an actor runs on is accessible through `context.dispatcher`. Dispatchers
    implement the `ExecutionContext` interface so they can be used to run futures.'
  id: totrans-1534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follower network crawler
  id: totrans-1535
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The end game for this chapter is to build a crawler to explore GitHub's follower
    graph. We have already outlined how we can do this in a single-threaded manner
    earlier in this chapter. Let's design an actor system to do this concurrently.
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
- en: The moving parts in the code are the data structures managing which users have
    been fetched or are being fetched. These need to be encapsulated in an actor to
    avoid race conditions arising from multiple actors trying to change them concurrently.
    We will therefore create a *fetcher manager* actor whose job is to keep track
    of which users have been fetched and which users we are going to fetch next.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
- en: The part of the code that is likely to be a bottleneck is querying the GitHub
    API. We therefore want to be able to scale the number of workers doing this concurrently.
    We will create a pool of *fetchers*, actors responsible for querying the API for
    the followers of a particular user. Finally, we will create an actor whose responsibility
    is to interpret the API's response. This actor will forward its interpretation
    of the response to another actor who will extract the followers and give them
    to the fetcher manager.
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the architecture of the program will look like:'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
- en: '![Follower network crawler](img/image01195.jpeg)'
  id: totrans-1540
  prefs: []
  type: TYPE_IMG
- en: Actor system for our GitHub API crawler
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
- en: 'Each actor in our program performs a single task: fetchers just query the GitHub
    API and the queue manager just distributes work to the fetchers. Akka best practice
    dictates giving actors as narrow an area of responsibility as possible. This enables
    better granularity when scaling out (for instance, by adding more fetcher actors,
    we just parallelize the bottleneck) and better resilience: if an actor fails,
    it will only affect his area of responsibility. We will explore actor failure
    later on in this chapter.'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build the app in several steps, exploring the Akka toolkit as we write
    the program. Let''s start with the `build.sbt` file. Besides Akka, we will mark
    `scalaj-http` and `json4s` as dependencies:'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE390]'
  id: totrans-1544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE390]'
- en: Fetcher actors
  id: totrans-1545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The workhorse of our application is the fetcher, the actor responsible for fetching
    the follower details from GitHub. In the first instance, our actor will accept
    a single message, `Fetch(user)`. It will fetch the followers corresponding to
    `user` and log the response to screen. We will use the recipes developed in [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*, to query the
    GitHub API with an OAuth token. We will inject the token through the actor constructor.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the companion object. This will contain the definition of
    the `Fetch(user)` message and two factory methods to create the `Props` instances.
    You can find the code examples for this section in the `chap09/fetchers_alone`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)):'
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE391]'
  id: totrans-1548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE391]'
- en: 'Let''s now define the fetcher itself. We will wrap the call to the GitHub API
    in a future. This avoids a single slow request blocking the actor. When our actor
    receives a `Fetch` request, it wraps this request into a future, sends it off,
    and can then process the next message. Let''s go ahead and implement our actor:'
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE392]'
  id: totrans-1550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE392]'
- en: 'Let''s instantiate an actor system and four fetchers to check whether our actor
    is working as expected. We will read the GitHub token from the environment, as
    described in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web
    APIs*, then create four actors and ask each one to fetch the followers of a particular
    GitHub user. We wait five seconds for the requests to get completed, and then
    shut the system down:'
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE393]'
  id: totrans-1552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE393]'
- en: 'Let''s run the code through SBT:'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE394]'
  id: totrans-1554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE394]'
- en: Notice how we explicitly need to shut the actor system down using `system.shutdown`.
    The program hangs until the system is shut down. However, shutting down the system
    will stop all the actors, so we need to make sure that they have finished working.
    We do this by inserting a call to `Thread.sleep`.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
- en: Using `Thread.sleep` to wait until the API calls have finished to shut down
    the actor system is a little crude. A better approach could be to let the actors
    signal back to the system that they have completed their task. We will see examples
    of this pattern later when we implement the *fetcher manager* actor.
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
- en: 'Akka includes a feature-rich *scheduler* to schedule events. We can use the
    scheduler to replace the call to `Thread.sleep` by scheduling a system shutdown
    five seconds in the future. This is preferable as the scheduler does not block
    the calling thread, unlike `Thread.sleep`. To use the scheduler, we need to import
    a global execution context and the duration module:'
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE395]'
  id: totrans-1558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE395]'
- en: 'We can then schedule a system shutdown by replacing our call to `Thread.sleep`
    with the following:'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE396]'
  id: totrans-1560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE396]'
- en: Besides `scheduleOnce`, the scheduler also exposes a `schedule` method that
    lets you schedule events to happen regularly (every two seconds, for instance).
    This is useful for heartbeat checks or monitoring systems. For more information,
    read the API documentation on the scheduler available at [http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html](http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html).
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are actually cheating a little bit here by not fetching every follower.
    The response to the follower's query is actually paginated, so we would need to
    fetch several pages to fetch all the followers. Adding logic to the actor to do
    this is not terribly complicated. We will ignore this for now and assume that
    users are capped at 100 followers each.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  id: totrans-1563
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we created four fetchers and dispatched messages to
    them, one after the other. We have a pool of identical actors among which we distribute
    tasks. Manually routing the messages to the right actor to maximize the utilization
    of our pool is painful and error-prone. Fortunately, Akka provides us with several
    routing strategies that we can use to distribute work among our pool of actors.
    Let's rewrite the previous example with automatic routing. You can find the code
    examples for this section in the `chap09/fetchers_routing` directory in the sample
    code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    We will reuse the same definition of `Fetchers` and its companion object as we
    did in the previous section.
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the routing package:'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE397]'
  id: totrans-1566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE397]'
- en: 'A *router* is an actor that forwards the messages that it receives to its children.
    The easiest way to define a pool of actors is to tell Akka to create a router
    and pass it a `Props` object for its children. The router will then manage the
    creation of the workers directly. In our example (we will only comment on the
    parts that differ from the previous example in the text, but you can find the
    full code in the `fetchers_routing` directory with the examples for this chapter),
    we replace the custom `Fetcher` creation code with the following:'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE398]'
  id: totrans-1568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE398]'
- en: 'We can then send the fetch messages directly to the router. The router will
    route the messages to the children in a round-robin manner:'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE399]'
  id: totrans-1570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE399]'
- en: We used a round-robin router in this example. Akka offers many different types
    of routers, including routers with dynamic pool size, to cater to different types
    of load balancing. Head over to the Akka documentation for a list of all the available
    routers, at [http://doc.akka.io/docs/akka/snapshot/scala/routing.html](http://doc.akka.io/docs/akka/snapshot/scala/routing.html).
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
- en: Message passing between actors
  id: totrans-1572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Merely logging the API response is not very useful. To traverse the follower
    graph, we must perform the following:'
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
- en: Check the return code of the response to make sure that the GitHub API was happy
    with our request
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parse the response as JSON
  id: totrans-1575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the login names of the followers and, if we have not fetched them already,
    push them into the queue
  id: totrans-1576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You learned how to do all these things in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, but not in the context of actors.
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
- en: 'We could just add the additional processing steps to the `receive` method of
    our `Fetcher` actor: we could add further transformations to the API response
    by future composition. However, having actors do several different things, and
    possibly failing in several different ways, is an anti-pattern: when we learn
    about managing the actor life cycle, we will see that it becomes much more difficult
    to reason about our actor systems if the actors contain several bits of logic.'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
- en: 'We will therefore use a pipeline of three different actors:'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
- en: The fetchers, which we have already encountered, are responsible just for fetching
    a URL from GitHub. They will fail if the URL is badly formatted or they cannot
    access the GitHub API.
  id: totrans-1580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response interpreter is responsible for taking the response from the GitHub
    API and parsing it to JSON. If it fails at any step, it will just log the error
    (in a real application, we might take different corrective actions depending on
    the type of failure). If it manages to extract JSON successfully, it will pass
    the JSON array to the follower extractor.
  id: totrans-1581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The follower extractor will extract the followers from the JSON array and pass
    them on to the queue of users whose followers we need to fetch.
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already built the fetchers, though we will need to modify them to forward
    the API response to the response interpreter rather than just logging it.
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples for this section in the `chap09/all_workers`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).The
    first step is to modify the fetchers so that, instead of logging the response,
    they forward the response to the response interpreter. To be able to forward the
    response to the response interpreter, the fetchers will need a reference to this
    actor. We will just pass the reference to the response interpreter through the
    fetcher constructor, which is now:'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE400]'
  id: totrans-1585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE400]'
- en: 'We must also modify the `Props` factory method in the companion object:'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE401]'
  id: totrans-1587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE401]'
- en: 'We must also modify the `receive` method to forward the HTTP response to the
    interpreter rather than just logging it:'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE402]'
  id: totrans-1589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE402]'
- en: The *response interpreter* takes the response, decides if it is valid, parses
    it to JSON, and forwards it to a follower extractor. The response interpreter
    will need a reference to the follower extractor, which we will pass in the constructor.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the `ResponseInterpreter` companion. It will just
    contain the definition of the messages that the response interpreter can receive
    and a factory to create a `Props` object to help with instantiation:'
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE403]'
  id: totrans-1592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE403]'
- en: 'The body of `ResponseInterpreter` should feel familiar: when the actor receives
    a message giving it a response to interpret, it parses it to JSON using the techniques
    that you learned in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"),
    *Web APIs*. If we parse the response successfully, we forward the parsed JSON
    to the follower extractor. If we fail to parse the response (possibly because
    it was badly formatted), we just log the error. We could recover from this in
    other ways, for instance, by re-adding this login to the queue manager to be fetched
    again:'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE404]'
  id: totrans-1594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE404]'
- en: 'We now have two-thirds of our worker actors. The last link is the follower
    extractor. This actor''s job is simple: it takes the `JArray` passed to it by
    the response interpreter and converts it to a list of followers. For now, we will
    just log this list, but when we build our fetcher manager, the follower extractor
    will send messages asking the manager to add the followers to its queue of logins
    to fetch.'
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, the companion just defines the messages that this actor can receive
    and a Props factory method:'
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE405]'
  id: totrans-1597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE405]'
- en: 'The `FollowerExtractor` class receives `Extract` messages containing a `JArray`
    of information representing a follower. It extracts the `login` field and logs
    it:'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE406]'
  id: totrans-1599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE406]'
- en: 'Let''s write a new `main` method to exercise all our actors:'
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE407]'
  id: totrans-1601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE407]'
- en: 'Let''s run this through SBT:'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE408]'
  id: totrans-1603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE408]'
- en: Queue control and the pull pattern
  id: totrans-1604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now defined the three worker actors in our crawler application. The
    next step is to define the manager. The *fetcher manager* is responsible for keeping
    a queue of logins to fetch as well as a set of login names that we have already
    seen in order to avoid fetching the same logins more than once.
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
- en: 'A first attempt might involve building an actor that keeps a set of users that
    we have already seen and just dispatches it to a round-robin router for fetchers
    when it is given a new user to fetch. The problem with this approach is that the
    number of messages in the fetchers'' mailboxes would accumulate quickly: for each
    API query, we are likely to get tens of followers, each of which is likely to
    make it back to a fetcher''s inbox. This gives us very little control over the
    amount of work piling up.'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
- en: 'The first problem that this is likely to cause involves the GitHub API rate
    limit: even with authentication, we are limited to 5,000 requests per hour. It
    would be useful to stop queries as soon as we hit this threshold. We cannot be
    responsive if each fetcher has a backlog of hundreds of users that they need to
    fetch.'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
- en: 'A better alternative is to use a *pull* system: the fetchers request work from
    a central queue when they find themselves idle. Pull systems are common in Akka
    when we have a producer that produces work faster than consumers can process it
    (refer to [http://www.michaelpollmeier.com/akka-work-pulling-pattern/](http://www.michaelpollmeier.com/akka-work-pulling-pattern/)).'
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversations between the manager and fetchers will proceed as follows:'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
- en: If the manager goes from a state of having no work to having work, it sends
    a `WorkAvailable` message to all the fetchers.
  id: totrans-1610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever a fetcher receives a `WorkAvailable` message or when it completes an
    item of work, it sends a `GiveMeWork` message to the queue manager.
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the queue manager receives a `GiveMeWork` message, it ignores the request
    if no work is available or it is throttled. If it has work, it sends a `Fetch(user)`
    message to the actor.
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start by modifying our fetcher. You can find the code examples for this
    section in the `chap09/ghub_crawler` directory in the sample code provided with
    this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    We will pass a reference to the fetcher manager through the constructor. We need
    to change the companion object to add the `WorkAvailable` message and the `props`
    factory to include the reference to the manager:'
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE409]'
  id: totrans-1614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE409]'
- en: We also need to change the `receive` method so that it queries the `FetcherManager`
    asking for more work once it's done processing a request or when it receives a
    `WorkAvailable` message.
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the final version of the fetchers:'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE410]'
  id: totrans-1617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE410]'
- en: Now that we have a working definition of the fetchers, let's build the `FetcherManager`.
    This is the most complex actor that we have built so far, and, before we dive
    into building it, we need to learn a bit more about the components of the Akka
    toolkit.
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the sender of a message
  id: totrans-1619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When our fetcher manager receives a `GiveMeWork` request, we will need to send
    work back to the correct fetcher. We can access the actor who sent a message using
    the `sender` method, which is a method of `Actor` that returns the `ActorRef`
    corresponding to the actor who sent the message currently being processed. The
    `case` statement corresponding to `GiveMeWork` in the fetcher manager is therefore:'
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE411]'
  id: totrans-1621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE411]'
- en: 'As `sender` is a *method*, its return value will change for every new incoming
    message. It should therefore only be used synchronously with the `receive` method.
    In particular, using it in a future is dangerous:'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE412]'
  id: totrans-1623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE412]'
- en: The problem is that when the future is completed 20 seconds after the message
    is processed, the actor will, in all likelihood, be processing a different message
    so the return value of `sender` will have changed. We will thus send the `Complete`
    message to a completely different actor.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to reply to a message outside of the `receive` method, such as
    when a future completes, you should bind the value of the current sender to a
    variable:'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE413]'
  id: totrans-1626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE413]'
- en: Stateful actors
  id: totrans-1627
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The behavior of the fetcher manager depends on whether it has work to give
    out to the fetchers:'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
- en: If it has work to give, it needs to respond to `GiveMeWork` messages with a
    `Fetcher.Fetch` message
  id: totrans-1629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it does not have work, it must ignore the `GiveMeWork` messages and, if work
    gets added, it must send a `WorkAvailable` message to the fetchers
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoding the notion of state is straightforward in Akka. We specify different
    `receive` methods and switch from one to the other depending on the state. We
    will define the following `receive` methods for our fetcher manager, corresponding
    to each of the states:'
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE414]'
  id: totrans-1632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE414]'
- en: 'Note that we must define the return type of the receive methods as `Receive`.
    To switch the actor from one method to the other, we can use `context.become(methodName)`.
    Thus, for instance, when the last login name is popped off the queue, we can transition
    to using the `receiveWhileEmpty` method with `context.become(receiveWhileEmpty)`.
    We set the initial state by assigning `receiveWhileEmpty` to the `receive` method:'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE415]'
  id: totrans-1634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE415]'
- en: Follower network crawler
  id: totrans-1635
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to code up the remaining pieces of our network crawler. The
    largest missing piece is the fetcher manager. Let''s start with the companion
    object. As with the worker actors, this just contains the definitions of the messages
    that the actor can receive and a factory to create the `Props` instance:'
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE416]'
  id: totrans-1637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE416]'
- en: 'The manager can receive two messages: `AddToQueue`, which tells it to add a
    username to the queue of users whose followers need to be fetched, and `GiveMeWork`,
    emitted by the fetchers when they are unemployed.'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
- en: 'The manager will be responsible for launching the fetchers, response interpreter,
    and follower extractor, as well as maintaining an internal queue of usernames
    and a set of usernames that we have seen:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE417]'
  id: totrans-1640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE417]'
- en: 'We now have a fetcher manager. The rest of the code can remain the same, apart
    from the follower extractor. Instead of logging followers names, it must send
    `AddToQueue` messages to the manager. We will pass a reference to the manager
    at construction time:'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE418]'
  id: totrans-1642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE418]'
- en: 'The `main` method running all this is remarkably simple as all the code to
    instantiate actors has been moved to the `FetcherManager`. We just need to instantiate
    the manager and give it the first node in the network, and it will do the rest:'
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE419]'
  id: totrans-1644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE419]'
- en: 'Notice how we do not attempt to shut down the actor system anymore. We will
    just let it run, crawling the network, until we stop it or hit the authentication
    limit. Let''s run this through SBT:'
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE420]'
  id: totrans-1646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE420]'
- en: Our program does not actually do anything useful with the followers that it
    retrieves besides logging them. We could replace the `log.info` call to, for instance,
    store the nodes in a database or draw the graph to screen.
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance
  id: totrans-1648
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real programs fail, and they fail in unpredictable ways. Akka, and the Scala
    community in general, favors planning explicitly for failure rather than trying
    to write infallible applications. A *fault tolerant* system is a system that can
    continue to operate when one or more of its components fails. The failure of an
    individual subsystem does not necessarily mean the failure of the application.
    How does this apply to Akka?
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor model provides a natural unit to encapsulate failure: the actor.
    When an actor throws an exception while processing a message, the default behavior
    is for the actor to restart, but the exception does not leak out and affect the
    rest of the system. For instance, let''s introduce an arbitrary failure in the
    response interpreter. We will modify the `receive` method to throw an exception
    when it is asked to interpret the response for `misto`, one of Martin Odersky''s
    followers:'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE421]'
  id: totrans-1651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE421]'
- en: 'If you rerun the code through SBT, you will notice that an error gets logged.
    The program does not crash, however. It just continues as normal:'
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE422]'
  id: totrans-1653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE422]'
- en: 'None of the followers of `misto` will get added to the queue: he never made
    it past the `ResponseInterpreter` stage. Let''s step through what happens when
    the exception gets thrown:'
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
- en: The interpreter is sent the `InterpretResponse("misto", ...)` message. This
    causes it to throw an exception and it dies. None of the other actors are affected
    by the exception.
  id: totrans-1655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fresh instance of the response interpreter is created with the same Props
    instance as the recently deceased actor.
  id: totrans-1656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the response interpreter has finished initializing, it gets bound to the
    same `ActorRef` as the deceased actor. This means that, as far as the rest of
    the system is concerned, nothing has changed.
  id: totrans-1657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mailbox is tied to `ActorRef` rather than the actor, so the new response
    interpreter will have the same mailbox as its predecessor, without the offending
    message.
  id: totrans-1658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, if, for whatever reason, our crawler crashes when fetching or parsing
    the response for a user, the application will be minimally affected—we will just
    not fetch this user's followers.
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
- en: 'Any internal state that an actor carries is lost when it restarts. Thus, if,
    for instance, the fetcher manager died, we would lose the current value of the
    queue and visited users. The risks associated with losing the internal state can
    be mitigated by the following:'
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
- en: 'Adopting a different strategy for failure: we can, for instance, carry on processing
    messages without restarting the actor in the event of failure. Of course, this
    is of little use if the actor died because its internal state is inconsistent.
    In the next section, we will discuss how to change the failure recovery strategy.'
  id: totrans-1661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up the internal state by writing it to disk periodically and loading
    from the backup on restart.
  id: totrans-1662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Protecting actors that carry critical state by ensuring that all "risky" operations
    are delegated to other actors. In our crawler example, all the interactions with
    external services, such as querying the GitHub API and parsing the response, happen
    with actors that carry no internal state. As we saw in the previous example, if
    one of these actors dies, the application is minimally affected. By contrast,
    the precious fetcher manager is only allowed to interact with sanitized inputs.
    This is called the *error kernel* pattern: code likely to cause errors is delegated
    to kamikaze actors.'
  id: totrans-1663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom supervisor strategies
  id: totrans-1664
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default strategy of restarting an actor on failure is not always what we
    want. In particular, for actors that carry a lot of data, we might want to resume
    processing after an exception rather than restarting the actor. Akka lets us customize
    this behavior by setting a *supervisor strategy* in the actor's supervisor.
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
- en: Recall that all actors have parents, including the top-level actors, who are
    children of a special actor called the *user guardian*. By default, an actor's
    supervisor is his parent, and it is the supervisor who decides what happens to
    the actor on failure.
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to change how an actor reacts to failure, you must set its parent''s
    supervisor strategy. You do this by setting the `supervisorStrategy` attribute.
    The default strategy is equivalent to the following:'
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE423]'
  id: totrans-1668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE423]'
- en: 'There are two components to a supervisor strategy:'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
- en: '`OneForOneStrategy` determines that the strategy applies only to the actor
    that failed. By contrast, we can use `AllForOneStrategy`, which applies the same
    strategy to all the supervisees. If a single child fails, all the children will
    be restarted (or stopped or resumed).'
  id: totrans-1670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A partial function mapping `Throwables` to a `Directive`, which is an instruction
    on what to do in response to a failure. The default strategy, for instance, maps
    `ActorInitializationException` (which happens if the constructor fails) to the
    `Stop` directive and (almost all) other exceptions to `Restart`.
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are four directives:'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
- en: '`Restart`: This destroys the faulty actor and restarts it, binding the newborn
    actor to the old `ActorRef`. This clears the internal state of the actor, which
    may be a good thing (the actor might have failed because of some internal inconsistency).'
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Resume`: The actor just moves on to processing the next message in its inbox.'
  id: totrans-1674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stop`: The actor stops and is not restarted. This is useful in throwaway actors
    that you use to complete a single operation: if this operation fails, the actor
    is not needed any more.'
  id: totrans-1675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Escalate`: The supervisor itself rethrows the exception, hoping that its supervisor
    will know what to do with it.'
  id: totrans-1676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A supervisor does not have access to which of its children failed. Thus, if
    an actor has children that might require different recovery strategies, it is
    best to create a set of intermediate supervisor actors to supervise the different
    groups of children.
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of setting the supervisor strategy, let''s tweak the `FetcherManager`
    supervisor strategy to adopt an all-for-one strategy and stop its children when
    one of them fails. We start with the relevant imports:'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE424]'
  id: totrans-1679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE424]'
- en: 'Then, we just need to set the `supervisorStrategy` attribute in the `FetcherManager`
    definition:'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE425]'
  id: totrans-1681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE425]'
- en: If you run this through SBT, you will notice that when the code comes across
    the custom exception thrown by the response interpreter, the system halts. This
    is because all the actors apart from the fetcher manager are now defunct.
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
- en: Life-cycle hooks
  id: totrans-1683
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Akka lets us specify code that runs in response to specific events in an actor''s
    life, through *life-cycle hooks*. Akka defines the following hooks:'
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
- en: '`preStart()`: This runs after the actor''s constructor has finished but before
    it starts processing messages. This is useful to run initialization code that
    depends on the actor being fully constructed.'
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`postStop()`: This runs when the actor dies after it has stopped processing
    messages. This is useful to run cleanup code before terminating the actor.'
  id: totrans-1686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preRestart(reason: Throwable, message: Option[Any])`: This is called just
    after an actor receives an order to restart. The `preRestart` method has access
    to the exception that was thrown and to the offending message, allowing for corrective
    action. The default behavior of `preRestart` is to stop each child and then call
    `postStop`.'
  id: totrans-1687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`postRestart(reason:Throwable)`: This is called after an actor has restarted.
    The default behavior is to call `preStart()`.'
  id: totrans-1688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s use system hooks to persist the state of `FetcherManager` between runs
    of the programs. You can find the code examples for this section in the `chap09/ghub_crawler_fault_tolerant`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    This will make the fetcher manager fault-tolerant. We will use `postStop` to write
    the current queue and set of visited users to text files and `preStart` to read
    these text files from the disk. Let''s start by importing the libraries necessary
    to read and write files:'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE426]'
  id: totrans-1690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE426]'
- en: 'We will store the names of the two text files in which we persist the state
    in the `FetcherManager` companion object (a better approach would be to store
    them in a configuration file):'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE427]'
  id: totrans-1692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE427]'
- en: 'In the `preStart` method, we load both the set of fetched users and the backlog
    of users to fetch from the text files, and in the `postStop` method, we overwrite
    these files with the new values of these data structures:'
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE428]'
  id: totrans-1694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE428]'
- en: Now that we save the state of the crawler when it shuts down, we can put a better
    termination condition for the program than simply interrupting the program once
    we get bored. In production, we might halt the crawler when we have enough names
    in a database, for instance. In this example, we will simply let the crawler run
    for 30 seconds and then shut it down.
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify the `main` method:'
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE429]'
  id: totrans-1697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE429]'
- en: After 30 seconds, we just call `system.shutdown`, which stops all the actors
    recursively. This will stop the fetcher manager, calling the `postStop` life cycle
    hook. After one run of the program, I have 2,164 names in the `fetched-users.txt`
    file. Running it again increases this number to 3,728 users.
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
- en: We could improve fault tolerance further by making the fetcher manager dump
    the data structures at regular intervals while the code runs. As writing to the
    disk (or to a database) carries a certain element of risk (What if the database
    server goes down or the disk is full?) it would be better to delegate writing
    the data structures to a custom actor rather than endangering the manager.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
- en: 'Our crawler has one minor problem: when the fetcher manager stops, it stops
    the fetcher actors, response interpreter, and follower extractor. However, none
    of the users currently going through these actors are stored. This also results
    in a small number of undelivered messages at the end of the code: if the response
    interpreter stops before a fetcher, the fetcher will try to deliver to a non-existent
    actor. This only accounts for a small number of users. To recover these login
    names, we can create a reaper actor whose job is to coordinate the killing of
    all the worker actors in the correct order and harvest their internal state. This
    pattern is documented in a blog post by *Derek Wyatt* ([http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2](http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2)).'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
- en: What we have not talked about
  id: totrans-1701
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Akka is a very rich ecosystem, far too rich to do it justice in a single chapter.
    There are some important parts of the toolkit that you will need, but we have
    not covered them here. We will give brief descriptions, but you can refer to the
    Akka documentation for more details:'
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
- en: The ask operator, `?`, offers an alternative to the tell operator, `!`, that
    we have used to send messages to actors. Unlike "tell", which just fires a message
    to an actor, the ask operator expects a response. This is useful when we need
    to ask actors questions rather than just telling them what to do. The ask pattern
    is documented at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future).
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deathwatch allows actors to watch another actor and receive a message when it
    dies. This is useful for actors that might depend on another actor but not be
    its direct supervisor. This is documented at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch).
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our crawler, we passed references to actors explicitly through the constructor.
    We can also look up actors using the actor hierarchy with a syntax reminiscent
    of files in a filesystem at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection).
  id: totrans-1705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We briefly explored how to implement stateful actors with different receive
    methods and using `context.become` to switch between them. Akka offers a more
    powerful alternative, based on finite state machines, to encode a more complex
    set of states and transitions: [http://doc.akka.io/docs/akka/snapshot/scala/fsm.html](http://doc.akka.io/docs/akka/snapshot/scala/fsm.html).'
  id: totrans-1706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have not discussed distributing actor systems across several nodes in this
    chapter. The message passing architecture works well with distributed setups:
    [http://doc.akka.io/docs/akka/2.4.0/common/cluster.html](http://doc.akka.io/docs/akka/2.4.0/common/cluster.html).'
  id: totrans-1707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1708
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to weave actors together to tackle a difficult
    concurrent problem. More importantly, we saw how Akka's actor framework encourages
    us to think about concurrent problems in terms of many separate chunks of encapsulated
    mutable data, synchronized through message passing. Akka makes concurrent programming
    easier to reason about and more fun.
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-1710
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Derek Wyatt''s* book, *Akka Concurrency*, is a fantastic introduction to Akka.
    It should definitely be the first stop for anyone wanting to do serious Akka programming.'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
- en: The **LET IT CRASH** blog ([http://letitcrash.com](http://letitcrash.com)) is
    the official Akka blog, and contains many examples of idioms and patterns to solve
    common issues.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10. Distributed Batch Processing with Spark
  id: totrans-1713
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and
    Futures"), *Parallel Collections and Futures*, we discovered how to use parallel
    collections for "embarrassingly" parallel problems: problems that can be broken
    down into a series of tasks that require no (or very little) communication between
    the tasks.'
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark provides behavior similar to Scala parallel collections (and much
    more), but, instead of distributing tasks across different CPUs on the same computer,
    it allows the tasks to be distributed across a computer cluster. This provides
    arbitrary horizontal scalability, since we can simply add more computers to the
    cluster.
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the basics of Apache Spark and use it to explore
    a set of emails, extracting features with the view of building a spam filter.
    We will explore several ways of actually building a spam filter in [Chapter 12](part0117.xhtml#aid-3FIHQ2
    "Chapter 12. Distributed Machine Learning with MLlib"), *Distributed Machine Learning
    with MLlib*.
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark
  id: totrans-1717
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we included dependencies by specifying them in a `build.sbt`
    file, and relying on SBT to fetch them from the Maven Central repositories. For
    Apache Spark, downloading the source code or pre-built binaries explicitly is
    more common, since Spark ships with many command line scripts that greatly facilitate
    launching jobs and interacting with a cluster.
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
- en: Head over to [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    and download Spark 1.5.2, choosing the "pre-built for Hadoop 2.6 or later" package.
    You can also build Spark from source if you need customizations, but we will stick
    to the pre-built version since it requires no configuration.
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking **Download** will download a tarball, which you can unpack with the
    following command:'
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE430]'
  id: totrans-1721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE430]'
- en: This will create a `spark-1.5.2-bin-hadoop2.6` directory. To verify that Spark
    works correctly, navigate to `spark-1.5.2-bin-hadoop2.6/bin` and launch the Spark
    shell using `./spark-shell`. This is just a Scala shell with the Spark libraries
    loaded.
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
- en: 'You may want to add the `bin/` directory to your system path. This will let
    you call the scripts in that directory from anywhere on your system, without having
    to reference the full path. On Linux or Mac OS, you can add variables to the system
    path by entering the following line in your shell configuration file (`.bash_profile`
    on Mac OS, and `.bashrc` or `.bash_profile` on Linux):'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE431]'
  id: totrans-1724
  prefs: []
  type: TYPE_PRE
  zh: '[PRE431]'
- en: 'The changes will take effect in new shell sessions. On Windows (if you use
    PowerShell), you need to enter this line in the `profile.ps1` file in the `WindowsPowerShell`
    folder in `Documents`:'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE432]'
  id: totrans-1726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE432]'
- en: If this worked correctly, you should be able to open a Spark shell in any directory
    on your system by just typing `spark-shell` in a terminal.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring the example data
  id: totrans-1728
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the Ling-Spam email dataset (The original dataset
    is described at [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html)).
    Download the dataset from [http://data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)
    (or [ling-spam.zip](http://ling-spam.zip), depending on your preferred mode of
    compression), and unpack the contents to the directory containing the code examples
    for this chapter. The archive contains two directories, `spam/` and `ham/`, containing
    the spam and legitimate emails, respectively.
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
- en: Resilient distributed datasets
  id: totrans-1730
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark expresses all computations as a sequence of transformations and actions
    on distributed collections, called **Resilient Distributed Datasets** (**RDD**).
    Let''s explore how RDDs work with the Spark shell. Navigate to the examples directory
    and open a Spark shell as follows:'
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE433]'
  id: totrans-1732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE433]'
- en: 'Let''s start by loading an email in an RDD:'
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE434]'
  id: totrans-1734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE434]'
- en: '`email` is an RDD, with each element corresponding to a line in the input file.
    Notice how we created the RDD by calling the `textFile` method on an object called
    `sc`:'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE435]'
  id: totrans-1736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE435]'
- en: '`sc` is a `SparkContext` instance, an object representing the entry point to
    the Spark cluster (for now, just our local machine). When we start a Spark shell,
    a context is created and bound to the variable `sc` automatically.'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the email into words using `flatMap`:'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE436]'
  id: totrans-1739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE436]'
- en: 'This will feel natural if you are familiar with collections in Scala: the `email`
    RDD behaves just like a list of strings. Here, we split using the regular expression
    `\s`, denoting white space characters. Instead of using `flatMap` explicitly,
    we can also manipulate RDDs using Scala''s syntactic sugar:'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE437]'
  id: totrans-1741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE437]'
- en: 'Let''s inspect the results. We can use `.take(n)` to extract the first *n*
    elements of an RDD:'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE438]'
  id: totrans-1743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE438]'
- en: 'We can also use `.count` to get the number of elements in an RDD:'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE439]'
  id: totrans-1745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE439]'
- en: 'RDDs support many of the operations supported by collections. Let''s use `filter`
    to remove punctuation from our email. We will remove all words that contain any
    non-alphanumeric character. We can do this by filtering out elements that match
    this *regular expression* anywhere in the word: `[^a-zA-Z0-9]`.'
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE440]'
  id: totrans-1747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE440]'
- en: 'In this example, we created an RDD from a text file. We can also create RDDs
    from Scala iterables using the `sc.parallelize` method available on a Spark context:'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE441]'
  id: totrans-1749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE441]'
- en: 'This is useful for debugging and for trialling behavior in the shell. The counterpart
    to parallelize is the `.collect` method, which converts an RDD to a Scala array:'
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE442]'
  id: totrans-1751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE442]'
- en: The `.collect` method requires the entire RDD to fit in memory on the master
    node. It is thus either used for debugging with a reduced dataset, or at the end
    of a pipeline that trims down a dataset.
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, RDDs offer an API much like Scala iterables. The critical difference
    is that RDDs are *distributed* and *resilient*. Let's explore what this means
    in practice.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are immutable
  id: totrans-1754
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You cannot change an RDD once it is created. All operations on RDDs either create
    new RDDs or other Scala objects.
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are lazy
  id: totrans-1756
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you execute operations like map and filter on a Scala collection in the
    interactive shell, the REPL prints the values of the new collection to screen.
    The same isn''t true of Spark RDDs. This is because operations on RDDs are lazy:
    they are only evaluated when needed.'
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, when we write:'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE443]'
  id: totrans-1759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE443]'
- en: 'We are creating an RDD, `words` that knows how to build itself from its parent
    RDD, `email`, which, in turn, knows that it needs to read a text file and split
    it into lines. However, none of the commands actually happen until we force the
    evaluation of the RDDs by calling an *action* to return a Scala object. This is
    most evident if we try to read from a non-existent text file:'
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE444]'
  id: totrans-1761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE444]'
- en: 'We can create the RDD without a hitch. We can even define further transformations
    on the RDD. The program crashes only when these transformations are finally evaluated:'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE445]'
  id: totrans-1763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE445]'
- en: The action `.count` is expected to return the number of elements in our RDD
    as an integer. Spark has no choice but to evaluate `inp`, which results in an
    exception.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it is probably more appropriate to think of an RDD as a pipeline of operations,
    rather than a more traditional collection.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
- en: RDDs know their lineage
  id: totrans-1766
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RDDs can only be constructed from stable storage (for instance, by loading data
    from a file that is present on every node in the Spark cluster), or through a
    set of transformations based on other RDDs. Since RDDs are lazy, they need to
    know how to build themselves when needed. They do this by knowing who their parent
    RDD is, and what operation they need to apply to the parent. This is a well-defined
    process since the parent RDD is immutable.
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
- en: 'The `toDebugString` method provides a diagram of how an RDD is constructed:'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE446]'
  id: totrans-1769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE446]'
- en: RDDs are resilient
  id: totrans-1770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you run an application on a single computer, you generally don''t need to
    worry about hardware failure in your application: if the computer fails, your
    application is doomed anyway.'
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed architectures should, by contrast, be fault-tolerant: the failure
    of a single machine should not crash the entire application. Spark RDDs are built
    with fault tolerance in mind. Let''s imagine that one of the worker nodes fails,
    causing the destruction of some of the data associated with an RDD. Since the
    Spark RDD knows how to build itself from its parent, there is no permanent data
    loss: the elements that were lost can just be re-computed when needed on another
    computer.'
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are distributed
  id: totrans-1773
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you construct an RDD, for instance from a text file, Spark will split the
    RDD into a number of partitions. Each partition will be entirely localized on
    a single machine (though there is, in general, more than one partition per machine).
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
- en: 'Many transformations on RDDs can be executed on each partition independently.
    For instance, when performing a `.map` operation, a given element in the output
    RDD depends on a single element in the parent: data does not need to be moved
    between partitions. The same is true of `.flatMap` and `.filter` operations. This
    means that the partition in the RDD produced by one of these operations depends
    on a single partition in the parent RDD.'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a `.distinct` transformation, which removes all duplicate
    elements from an RDD, requires the data in a given partition to be compared to
    the data in every other partition. This requires *shuffling* the data across the
    nodes. Shuffling, especially for large datasets, is an expensive operation and
    should be avoided if possible.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
- en: Transformations and actions on RDDs
  id: totrans-1777
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The set of operations supported by an RDD can be split into two categories:'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations** create a new RDD from the current one. Transformations
    are lazy: they are not evaluated immediately.'
  id: totrans-1779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions** force the evaluation of an RDD, and normally return a Scala object,
    rather than an RDD, or have some form of side-effect. Actions are evaluated immediately,
    triggering the execution of all the transformations that make up this RDD.'
  id: totrans-1780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the tables below, we give some examples of useful transformations and actions.
    For a full, up-to-date list, consult the Spark documentation ([http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)).
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
- en: 'For the examples in these tables, we assume that you have created an RDD with:'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE447]'
  id: totrans-1783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE447]'
- en: 'The following table lists common transformations on an RDD. Recall that transformations
    always generate a new RDD, and that they are lazy operations:'
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Notes | **Example (assuming** `rdd` **is** `{ "quick", "brown",
    "quick", "dog" }`) |'
  id: totrans-1785
  prefs: []
  type: TYPE_TB
- en: '| `rdd.map(func)` |   | `rdd.map { _.size } // => { 5, 5, 5, 3 }` |'
  id: totrans-1786
  prefs: []
  type: TYPE_TB
- en: '| `rdd.filter(pred)` |   | `rdd.filter { _.length < 4 } // => { "dog" }` |'
  id: totrans-1787
  prefs: []
  type: TYPE_TB
- en: '| `rdd.flatMap(func)` |   | `rdd.flatMap { _.toCharArray } // => { ''q'', ''u'',
    ''i'', ''c'', ''k'', ''b'', ''r'', ''o'' … }` |'
  id: totrans-1788
  prefs: []
  type: TYPE_TB
- en: '| `rdd.distinct()` | Remove duplicate elements in RDD. | `rdd.distinct // =>
    { "dog", "brown", "quick" }` |'
  id: totrans-1789
  prefs: []
  type: TYPE_TB
- en: '| `rdd.pipe(command, [envVars])` | Pipe through an external program. RDD elements
    are written, line-by-line, to the process''s `stdin`. The output is read from
    `stdout`. | `rdd.pipe("tr a-z A-Z") // => { "QUICK", "BROWN", "QUICK", "DOG" }`
    |'
  id: totrans-1790
  prefs: []
  type: TYPE_TB
- en: The following table describes common actions on RDDs. Recall that actions always
    generate a Scala type or cause a side-effect, rather than creating a new RDD.
    Actions force the evaluation of the RDD, triggering the execution of the transformations
    underpinning the RDD.
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
- en: '| Action | Nodes | **Example (assuming** `rdd` **is** `{ "quick", "brown",
    "quick", "dog" }`) |'
  id: totrans-1792
  prefs: []
  type: TYPE_TB
- en: '| `rdd.first` | First element in the RDD. | `rdd.first // => quick` |'
  id: totrans-1793
  prefs: []
  type: TYPE_TB
- en: '| `rdd.collect` | Transform the RDD to an array (the array must be able to
    fit in memory on the master node). | `rdd.collect // => Array[String]("quick",
    "brown", "quick", "dog")` |'
  id: totrans-1794
  prefs: []
  type: TYPE_TB
- en: '| `rdd.count` | Number of elements in the RDD. | `rdd.count // => 4` |'
  id: totrans-1795
  prefs: []
  type: TYPE_TB
- en: '| `rdd.countByValue` | Map of element to the number of times this element occurs.
    The map must fit on the master node. | `rdd.countByValue // => Map(quick -> 2,
    brown -> 1, dog -> 1)` |'
  id: totrans-1796
  prefs: []
  type: TYPE_TB
- en: '| `rdd.take(n)` | Return an array of the first *n* elements in the RDD. | `rdd.take(2)
    // => Array(quick, brown)` |'
  id: totrans-1797
  prefs: []
  type: TYPE_TB
- en: '| `rdd.takeOrdered(n:Int)(implicit ordering: Ordering[T])` | Top *n* elements
    in the RDD according to the element''s default ordering, or the ordering passed
    as second argument. See the Scala docs for `Ordering` for how to define custom
    comparison functions ([http://www.scala-lang.org/api/current/index.html#scala.math.Ordering](http://www.scala-lang.org/api/current/index.html#scala.math.Ordering)).
    | `rdd.takeOrdered(2) // => Array(brown, dog)``rdd.takeOrdered(2) (Ordering.by
    { _.size }) // => Array[String] = Array(dog, quick)` |'
  id: totrans-1798
  prefs: []
  type: TYPE_TB
- en: '| `rdd.reduce(func)` | Reduce the RDD according to the specified function.
    Uses the first element in the RDD as the base. `func` should be commutative and
    associative. | `rdd.map { _.size }.reduce { _ + _ } // => 18` |'
  id: totrans-1799
  prefs: []
  type: TYPE_TB
- en: '| `rdd.aggregate(zeroValue)(seqOp, combOp)` | Reduction for cases where the
    reduction function returns a value of type different to the RDD''s type. In this
    case, we need to provide a function for reducing within a single partition (`seqOp`)
    and a function for combining the value of two partitions (`combOp`). | `rdd.aggregate(0)
    ( _ + _.size, _ + _ ) // => 18` |'
  id: totrans-1800
  prefs: []
  type: TYPE_TB
- en: Persisting RDDs
  id: totrans-1801
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned that RDDs only retain the sequence of operations needed to
    construct the elements, rather than the values themselves. This, of course, drastically
    reduces memory usage since we do not need to keep intermediate versions of our
    RDDs in memory. For instance, let''s assume we want to trawl through transaction
    logs to identify all the transactions that occurred on a particular account:'
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE448]'
  id: totrans-1803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE448]'
- en: The set of all transactions will be large, while the set of transactions on
    the account of interest will be much smaller. Spark's policy of remembering *how*
    to construct a dataset, rather than the dataset itself, means that we never have
    all the lines of our input file in memory at any one time.
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two situations in which we may want to avoid re-computing the elements
    of an RDD every time we use it:'
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
- en: 'For interactive use: we might have detected fraudulent behavior on account
    "123456", and we want to investigate how this might have arisen. We will probably
    want to perform many different exploratory calculations on this RDD, without having
    to re-read the entire log file every time. It therefore makes sense to persist
    `interestingTransactions`.'
  id: totrans-1806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an algorithm re-uses an intermediate result, or a dataset. A canonical
    example is logistic regression. In logistic regression, we normally use an iterative
    algorithm to find the 'optimal' coefficients that minimize the loss function.
    At every step in our iterative algorithm, we must calculate the loss function
    and its gradient from the training set. We should avoid re-computing the training
    set (or re-loading it from an input file) if at all possible.
  id: totrans-1807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark provides a `.persist` method on RDDs to achieve this. By calling `.persist`
    on an RDD, we tell Spark to keep the dataset in memory next time it is computed.
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE449]'
  id: totrans-1809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE449]'
- en: 'Spark supports different levels of persistence, which you can tune by passing
    arguments to `.persist`:'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE450]'
  id: totrans-1811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE450]'
- en: 'Spark provides several persistence levels, including:'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY`: the default storage level. The RDD is stored in RAM. If the
    RDD is too big to fit in memory, parts of it will not persist, and will need to
    be re-computed on the fly.'
  id: totrans-1813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK`: As much of the RDD is stored in memory as possible. If the
    RDD is too big, it will spill over to disk. This is only worthwhile if the RDD
    is expensive to compute. Otherwise, re-computing it may be faster than reading
    from the disk.'
  id: totrans-1814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you persist several RDDs and run out of memory, Spark will clear the least
    recently used out of memory (either discarding them or saving them to disk, depending
    on the chosen persistence level). RDDs also expose an `unpersist` method to explicitly
    tell Spark than an RDD is not needed any more.
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
- en: Persisting RDDs can have a drastic impact on performance. What and how to persist
    therefore becomes very important when tuning a Spark application. Finding the
    best persistence level generally requires some tinkering, benchmarking and experimentation.
    The Spark documentation provides guidelines on when to use which persistence level
    ([http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)),
    as well as general tips on tuning memory usage ([http://spark.apache.org/docs/latest/tuning.html](http://spark.apache.org/docs/latest/tuning.html)).
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, the `persist` method does not force the evaluation of the RDD.
    It just notifies the Spark engine that, next time the values in this RDD are computed,
    they should be saved rather than discarded.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
- en: Key-value RDDs
  id: totrans-1818
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have only considered RDDs of Scala value types. RDDs of more complex
    data types support additional operations. Spark adds many operations for *key-value
    RDDs*: RDDs whose type parameter is a tuple `(K, V)`, for any type `K` and `V`.'
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to our sample email:'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE451]'
  id: totrans-1821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE451]'
- en: 'Let''s persist the `words` RDD in memory to avoid having to re-read the `email`
    file from disk repeatedly:'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE452]'
  id: totrans-1823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE452]'
- en: 'To access key-value operations, we just need to apply a transformation to our
    RDD that creates key-value pairs. Let''s use the words as keys. For now, we will
    just use 1 for every value:'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE453]'
  id: totrans-1825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE453]'
- en: 'Key-value RDDs support several operations besides the core RDD operations.
    These are added through an implicit conversion, using the "pimp my library" pattern
    that we explored in [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and
    SQL through JDBC"), *Scala and SQL through JDBC*. These additional transformations
    fall into two broad categories: *by-key* transformations and *joins* between RDDs.'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
- en: 'By-key transformations are operations that aggregate the values corresponding
    to the same key. For instance, we can count the number of times each word appears
    in our email using `reduceByKey`. This method takes all the values that belong
    to the same key and combines them using a user-supplied function:'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE454]'
  id: totrans-1828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE454]'
- en: 'Note that `reduceByKey` requires (in general) shuffling the RDD, since not
    every occurrence of a given key will be in the same partition:'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE455]'
  id: totrans-1830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE455]'
- en: 'Note that key-value RDDs are not like Scala Maps: the same key can occur multiple
    times, and they do not support *O(1)* lookup. A key-value RDD can be transformed
    to a Scala map using the `.collectAsMap` action:'
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE456]'
  id: totrans-1832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE456]'
- en: This requires pulling the entire RDD onto the main Spark node. You therefore
    need to have enough memory on the main node to house the map. This is often the
    last stage in a pipeline that filters a large RDD to just the information that
    we need.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many by-key operations, which we describe in the table below. For
    the examples in the table, we assume that `rdd` is created as follows:'
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE457]'
  id: totrans-1835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE457]'
- en: '| Transformation | Notes | **Example (assumes** `rdd` **is** `{ quick -> 5,
    brown -> 5, quick -> 5, dog -> 3 }`) |'
  id: totrans-1836
  prefs: []
  type: TYPE_TB
- en: '| `rdd.mapValues` | Apply an operation to the values. | `rdd.mapValues { _
    * 2 } // => { quick -> 10, brown -> 10, quick -> 10, dog ->6 }` |'
  id: totrans-1837
  prefs: []
  type: TYPE_TB
- en: '| `rdd.groupByKey` | Return a key-value RDD in which values corresponding to
    the same key are grouped into iterables. | `rdd.groupByKey // => { quick -> Iterable(5,
    5), brown -> Iterable(5), dog -> Iterable(3) }` |'
  id: totrans-1838
  prefs: []
  type: TYPE_TB
- en: '| `rdd.reduceByKey(func)` | Return a key-value RDD in which values corresponding
    to the same key are combined using a user-supplied function. | `rdd.reduceByKey
    { _ + _ } // => { quick -> 10, brown -> 5, dog -> 3 }` |'
  id: totrans-1839
  prefs: []
  type: TYPE_TB
- en: '| `rdd.keys` | Return an RDD of the keys. | `rdd.keys // => { quick, brown,
    quick, dog }` |'
  id: totrans-1840
  prefs: []
  type: TYPE_TB
- en: '| `rdd.values` | Return an RDD of the values. | `rdd.values // => { 5, 5, 5,
    3 }` |'
  id: totrans-1841
  prefs: []
  type: TYPE_TB
- en: 'The second category of operations on key-value RDDs involves joining different
    RDDs together by key. This is somewhat similar to SQL joins, where the keys are
    the column being joined on. Let''s load a spam email and apply the same transformations
    we applied to our ham email:'
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE458]'
  id: totrans-1843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE458]'
- en: 'Both `spamWordCounts` and `wordCounts` are key-value RDDs for which the keys
    correspond to unique words in the message, and the values are the number of times
    that word occurs. There will be some overlap in keys between `spamWordCounts`
    and `wordCounts`, since the emails will share many of the same words. Let''s do
    an *inner join* between those two RDDs to get the words that occur in both emails:'
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE459]'
  id: totrans-1845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE459]'
- en: The values in the RDD resulting from an inner join will be pairs. The first
    element in the pair is the value for that key in the first RDD, and the second
    element is the value for that key in the second RDD. Thus, the word *call* occurs
    three times in the legitimate email and once in the spam email.
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark supports all four join types. For instance, let''s perform a left join:'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE460]'
  id: totrans-1848
  prefs: []
  type: TYPE_PRE
  zh: '[PRE460]'
- en: 'Notice that the second element in our pair has type `Option[Int]`, to accommodate
    keys absent in `spamWordCounts`. The word *paper*, for instance, occurs twice
    in the legitimate email and never in the spam email. In this case, it is more
    useful to have zeros to indicate absence, rather than `None`. Replacing `None`
    with a default value is simple with `getOrElse`:'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE461]'
  id: totrans-1850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE461]'
- en: 'The table below lists the most common joins on key-value RDDs:'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transformation** | **Result (assuming** `rdd1` is `{ quick -> 1, brown
    -> 2, quick -> 3, dog -> 4 }` **and** `rdd2` **is** `{ quick -> 78, brown -> 79,
    fox -> 80 }`) |'
  id: totrans-1852
  prefs: []
  type: TYPE_TB
- en: '| `rdd1.join(rdd2)` | `{ quick -> (1, 78), quick -> (3, 78), brown -> (2, 79)
    }` |'
  id: totrans-1853
  prefs: []
  type: TYPE_TB
- en: '| `rdd1.leftOuterJoin(rdd2)` | `{ dog -> (4, None), quick -> (1, Some(78)),
    quick -> (3, Some(78)), brown -> (2, Some(79)) }` |'
  id: totrans-1854
  prefs: []
  type: TYPE_TB
- en: '| `rdd1.rightOuterJoin(rdd2)` | `{ quick -> (Some(1), 78), quick -> (Some(3),
    78), brown -> (Some(2), 79), fox -> (None, 80) }` |'
  id: totrans-1855
  prefs: []
  type: TYPE_TB
- en: '| `rdd1.fullOuterJoin(rdd2)` | `{ dog -> (Some(4), None), quick -> (Some(1),
    Some(78)), quick -> (Some(3), Some(78)), brown -> (Some(2), Some(79)), fox ->
    (None, Some(80)) }` |'
  id: totrans-1856
  prefs: []
  type: TYPE_TB
- en: For a complete list of transformations, consult the API documentation for `PairRDDFunctions`,
    [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions).
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
- en: Double RDDs
  id: totrans-1858
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we saw that Spark adds functionality to key-value
    RDDs through an implicit conversion. Similarly, Spark adds statistics functionality
    to RDDs of doubles. Let''s extract the word frequencies for the ham message, and
    convert the values from integers to doubles:'
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE462]'
  id: totrans-1860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE462]'
- en: 'We can then get summary statistics using the `.stats` action:'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE463]'
  id: totrans-1862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE463]'
- en: 'Thus, the most common word appears 72 times. We can also use the `.histogram`
    action to get an idea of the distribution of values:'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE464]'
  id: totrans-1864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE464]'
- en: 'The `.histogram` method returns a pair of arrays. The first array indicates
    the bounds of the histogram bins, and the second is the count of elements in that
    bin. Thus, there are `391` words that appear less than `15.2` times. The distribution
    of words is very skewed, such that a histogram with regular-sized bin is not really
    appropriate. We can, instead, pass in custom bins by passing an array of bin edges
    to the `histogram` method. For instance, we might distribute the bins logarithmically:'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE465]'
  id: totrans-1866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE465]'
- en: Building and running standalone programs
  id: totrans-1867
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have interacted exclusively with Spark through the Spark shell. In
    the section that follows, we will build a standalone application and launch a
    Spark program either locally or on an EC2 cluster.
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
- en: Running Spark applications locally
  id: totrans-1869
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to write the `build.sbt` file, as you would if you were running
    a standard Scala script. The Spark binary that we downloaded needs to be run against
    Scala 2.10 (You need to compile Spark from source to run against Scala 2.11\.
    This is not difficult to do, just follow the instructions on [http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211](http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211)).
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE466]'
  id: totrans-1871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE466]'
- en: We then run `sbt package` to compile and build a jar of our program. The jar
    will be built in `target/scala-2.10/`, and called `spam_mi_2.10-0.1-SNAPSHOT.jar`.
    You can try this with the example code provided for this chapter.
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then run the jar locally using the `spark-submit` shell script, available
    in the `bin/` folder in the Spark installation directory:'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE467]'
  id: totrans-1874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE467]'
- en: The resources allocated to Spark can be controlled by passing arguments to `spark-submit`.
    Use `spark-submit --help` to see the full list of arguments.
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
- en: 'If the Spark programs has dependencies (for instance, on other Maven packages),
    it is easiest to bundle them into the application jar using the *SBT* *assembly*
    plugin. Let''s imagine that our application depends on breeze-viz. The `build.sbt`
    file now looks like:'
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE468]'
  id: totrans-1877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE468]'
- en: 'SBT assembly is an SBT plugin that builds *fat* jars: jars that contain not
    only the program itself, but all the dependencies for the program.'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we marked Spark as "provided" in the list of dependencies, which
    means that Spark itself will not be included in the jar (it is provided by the
    Spark environment anyway). To include the SBT assembly plugin, create a file called
    `assembly.sbt` in the `project/` directory, with the following line:'
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE469]'
  id: totrans-1880
  prefs: []
  type: TYPE_PRE
  zh: '[PRE469]'
- en: You will need to re-start SBT for the changes to take effect. You can then create
    the assembly jar using the `assembly` command in SBT. This will create a jar called
    `spam_mi-assembly-0.1-SNAPSHOT.jar` in the `target/scala-2.10` directory. You
    can run this jar using `spark-submit`.
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
- en: Reducing logging output and Spark configuration
  id: totrans-1882
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark is, by default, very verbose. The default log-level is set to `INFO`.
    To avoid missing important messages, it is useful to change the log settings to
    `WARN`. To change the default log level system-wide, go into the `conf` directory
    in the directory in which you installed Spark. You should find a file called `log4j.properties.template`.
    Rename this file to `log4j.properties` and look for the following line:'
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE470]'
  id: totrans-1884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE470]'
- en: 'Change this line to:'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE471]'
  id: totrans-1886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE471]'
- en: There are several other configuration files in that directory that you can use
    to alter Spark's default behavior. For a full list of configuration options, head
    over to [http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html).
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
- en: Running Spark applications on EC2
  id: totrans-1888
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running Spark locally is useful for testing, but the whole point of using a
    distributed framework is to run programs harnessing the power of several different
    computers. We can set Spark up on any set of computers that can communicate with
    each other using HTTP. In general, we also need to set up a distributed file system
    like HDFS, so that we can share input files across the cluster. For the purpose
    of this example, we will set Spark up on an Amazon EC2 cluster.
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark comes with a shell script, `ec2/spark-ec2`, for setting up an EC2 cluster
    and installing Spark. It will also install HDFS. You will need an account with
    Amazon Web Services (AWS) to follow these examples ([https://aws.amazon.com](https://aws.amazon.com)).
    You will need the AWS access key and secret key, which you can access through
    the **Account** / **Security Credentials** / **Access Credentials** menu in the
    AWS web console. You need to make these available to the `spark-ec2` script through
    environment variables. Inject them into your current session as follows:'
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE472]'
  id: totrans-1891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE472]'
- en: You can also write these lines into the configuration script for your shell
    (your `.bashrc` file, or equivalent), to avoid having to re-enter them every time
    you run the `setup-ec2` script. We discussed environment variables in [Chapter
    6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick – A Functional Interface for SQL"),
    *Slick – A Functional Interface for SQL*.
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need to create a key pair by clicking on **Key Pairs** in the
    EC2 web console, creating a new key pair and downloading the certificate file.
    I will assume you named the key pair `test_ec2` and the certificate file `test_ec2.pem`.
    Make sure that the key pair is created in the *N*. Virginia region (by choosing
    the correct region in the upper right corner of the EC2 Management console), to
    avoid having to specify the region explicitly in the rest of this chapter. You
    will need to set access permissions on the certificate file to user-readable only:'
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  id: totrans-1894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE473]'
- en: 'We are now ready to launch the cluster. Navigate to the `ec2` directory and
    run:'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE474]'
  id: totrans-1896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE474]'
- en: This will create a cluster called `test_cluster` with a master and two slaves.
    The number of slaves is set through the `-s` command line argument. The cluster
    will take a while to start up, but you can verify that the instances are launching
    correctly by looking at the **Instances** window in the EC2 Management Console.
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
- en: The setup script supports many options for customizing the type of instances,
    the number of hard drives and so on. You can explore these options by passing
    the `--help` command line option to `spark-ec2`.
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
- en: 'The life cycle of the cluster can be controlled by passing different commands
    to the `spark-ec2` script, such as:'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE475]'
  id: totrans-1900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE475]'
- en: For more detail on using Spark on EC2, consult the official documentation at
    [http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications](http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications).
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering
  id: totrans-1902
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put all we''ve learned to good use and do some data exploration for
    our spam filter. We will use the Ling-Spam email dataset: [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html).
    The dataset contains 2412 ham emails and 481 spam emails, all of which were received
    by a mailing list on linguistics. We will extract the words that are most informative
    of whether an email is spam or ham.'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
- en: 'The first steps in any natural language processing workflow are to remove stop
    words and lemmatization. Removing stop words involves filtering very common words
    such as *the*, *this* and so on. Lemmatization involves replacing different forms
    of the same word with a canonical form: both *colors* and *color* would be mapped
    to *color*, and *organize*, *organizing* and *organizes* would be mapped to *organize*.
    Removing stop words and lemmatization is very challenging, and beyond the scope
    of this book (if you do need to remove stop words and lemmatize a dataset, your
    go-to tool should be the Stanford NLP toolkit: [http://nlp.stanford.edu/software/corenlp.shtml](http://nlp.stanford.edu/software/corenlp.shtml)).
    Fortunately, the Ling-Spam e-mail dataset has been cleaned and lemmatized already
    (which is why the text in the emails looks strange).'
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
- en: 'When we do build the spam filter, we will use the presence of a particular
    word in an email as the feature for our model. We will use a *bag-of-words* approach:
    we consider which words appear in an email, but not the word order.'
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, some words will be more important than others when deciding whether
    an email is spam. For instance, an email that contains *language* is likely to
    be ham, since the mailing list was for linguistics discussions, and *language*
    is a word unlikely to be used by spammers. Conversely, words which are common
    to both message types, for instance *hello*, are unlikely to be much use.
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of quantifying the importance of a word in determining whether a message
    is spam is through the **Mutual Information** (**MI**). The mutual information
    is the gain in information about whether a message is ham or spam if we know that
    it contains a particular word. For instance, the presence of *language* in a particular
    email is very informative as to whether that email is spam or ham. Similarly,
    the presence of the word *dollar* is informative since it appears often in spam
    messages and only infrequently in ham messages. By contrast, the presence of the
    word *morning* is uninformative, since it is approximately equally common in both
    spam and ham messages. The formula for the mutual information between the presence
    of a particular word in an email, and whether that email is spam or ham is:'
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
- en: '![Spam filtering](img/image01196.jpeg)'
  id: totrans-1908
  prefs: []
  type: TYPE_IMG
- en: where ![Spam filtering](img/image01197.jpeg) is the joint probability of an
    email containing a particular word and being of that class (either ham or spam),
    ![Spam filtering](img/image01198.jpeg) is the probability that a particular word
    is present in an email, and ![Spam filtering](img/image01199.jpeg) is the probability
    that any email is of that class. The MI is commonly used in decision trees.
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1910
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The derivation of the expression for the mutual information is beyond the scope
    of this book. The interested reader is directed to *David MacKay's* excellent
    *Information Theory, Inference, and Learning Algorithms*, especially the chapter
    *Dependent Random Variables*.
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
- en: A key component of our MI calculation is evaluating the probability that a word
    occurs in spam or ham messages. The best approximation to this probability, given
    our data set, is the fraction of messages a word appears in. Thus, for instance,
    if *language* appears in 40% of messages, we will assume that the probability
    ![Spam filtering](img/image01200.jpeg) of language being present in any message
    is 0.4\. Similarly, if 40% of the messages are ham, and *language* appears in
    50% of those, we will assume that the probability of language being present in
    an email, and that email being ham is ![Spam filtering](img/image01201.jpeg).
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
- en: Let's write a `wordFractionInFiles` function to calculate the fraction of messages
    in which each word appears, for all the words in a given corpus. Our function
    will take, as argument, a path with a shell wildcard identifying a set of files,
    such as `ham/*`, and it will return a key-value RDD, where the keys are words
    and the values are the probability that that word occurs in any of those files.
    We will put the function in an object called `MutualInformation`.
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
- en: 'We first give the entire code listing for this function. Don''t worry if this
    doesn''t all make sense straight-away: we explain the tricky parts in more detail
    just after the code. You may find it useful to type some of these commands in
    the shell, replacing `fileGlob` with, for instance `"ham/*"`:'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE476]'
  id: totrans-1915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE476]'
- en: 'Let''s play with this function in the Spark shell. To be able to access this
    function from the shell, we need to create a jar with the `MutualInformation`
    object. Write a `build.sbt` file similar to the one presented in the previous
    section and package the code into a jar using `sbt package`. Then, open a Spark
    shell with:'
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE477]'
  id: totrans-1917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE477]'
- en: 'This will open a Spark shell with our newly created jar on the classpath. Let''s
    run our `wordFractionInFiles` method on the `ham` emails:'
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE478]'
  id: totrans-1919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE478]'
- en: 'Let''s get a snapshot of the `fractions` RDD:'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE479]'
  id: totrans-1921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE479]'
- en: 'It would be nice to see the words that come up most often in ham messages.
    We can use the `.takeOrdered` action to take the top values of an RDD, with a
    custom ordering. `.takeOrdered` expects, as its second argument, an instance of
    the type class `Ordering[T]`, where `T` is the type parameter of our RDD: `(String,
    Double)` in this case. `Ordering[T]` is a trait with a single `compare(a:T, b:T)`
    method describing how to compare `a` and `b`. The easiest way of creating an `Ordering[T]`
    is through the companion object''s `by` method, which defines a key by which to
    compare the elements of our RDD.'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to order the elements in our key-value RDD by the value and, since
    we want the most common words, rather than the least, we need to reverse that
    ordering:'
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE480]'
  id: totrans-1924
  prefs: []
  type: TYPE_PRE
  zh: '[PRE480]'
- en: Unsurprisingly, `language` is present in 67% of ham emails, `university` in
    60% of ham emails and so on. A similar investigation on spam messages reveals
    that the exclamation mark character *!* is present in 83% of spam emails, *our*
    is present in 61% and *free* in 57%.
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
- en: We are now in a position to start writing the body of our application to calculate
    the mutual information between each word and whether a message is spam or ham.
    We will put the body of the code in the `MutualInformation` object, which already
    contains the `wordFractionInFiles` method.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a Spark context:'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE481]'
  id: totrans-1928
  prefs: []
  type: TYPE_PRE
  zh: '[PRE481]'
- en: Note that we did not need to do this when we were using the Spark shell because
    the shell comes with a pre-built context bound to the variable `sc`.
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now calculate the conditional probabilities of a message containing
    a particular word given that it is *spam*, ![Spam filtering](img/image01202.jpeg).
    This is just the fraction of messages containing that word in the *spam* corpus.
    This, in turn, lets us infer the joint probability of a message containing a certain
    word and being *spam* ![Spam filtering](img/image01203.jpeg). We will do this
    for all four combinations of classes: whether any given word is present or absent
    in a message, and whether that message is spam or ham:'
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE482]'
  id: totrans-1931
  prefs: []
  type: TYPE_PRE
  zh: '[PRE482]'
- en: 'We will re-use these RDDs in several places in the calculation, so let''s tell
    Spark to keep them in memory to avoid having to re-calculate them:'
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE483]'
  id: totrans-1933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE483]'
- en: We now need to calculate the probabilities of words being present, ![Spam filtering](img/image01198.jpeg).
    This is just the sum of `pPresentAndSpam` and `pPresentAndHam`, for each word.
    The tricky part is that not all words are present in both the ham and spam messages.
    We must therefore do a full outer join of those RDDs. This will give an RDD mapping
    each word to a pair of `Option[Double]` values. For words absent in either the
    ham or spam messages, we must use a default value. A sensible default is ![Spam
    filtering](img/image01204.jpeg) for spam messages (a more rigorous approach would
    be to use *additive smoothing*). This implies that the word would appear once
    if the corpus was twice as large.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE484]'
  id: totrans-1935
  prefs: []
  type: TYPE_PRE
  zh: '[PRE484]'
- en: Note that we could also have chosen 0 as the default value. This complicates
    the information gain calculation somewhat, since we cannot just take the log of
    a zero value, and it seems unlikely that a particular word has exactly zero probability
    of occurring in an email.
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now construct an RDD mapping words to ![Spam filtering](img/image01198.jpeg),
    the probability that a word exists in either a spam or a ham message:'
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE485]'
  id: totrans-1938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE485]'
- en: We now have all the RDDs that we need to calculate the mutual information between
    the presence of a word in a message and whether it is ham or spam. We need to
    bring them all together using the equation for the mutual information outlined
    earlier.
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining a helper method that, given an RDD of joint probabilities
    *P(X, Y)* and marginal probabilities *P(X)* and *P(Y)*, calculates ![Spam filtering](img/image01205.jpeg).
    Here, *P(X)* could, for instance, be the probability of a word being present in
    a message ![Spam filtering](img/image01198.jpeg) and *P(Y)* would be the probability
    that that message is *spam*, ![Spam filtering](img/image01206.jpeg):'
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE486]'
  id: totrans-1941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE486]'
- en: 'We can use our function to calculate the four terms in the mutual information
    sum:'
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE487]'
  id: totrans-1943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE487]'
- en: 'Finally, we just need to sum those four terms together:'
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE488]'
  id: totrans-1945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE488]'
- en: 'The RDD `mutualInformation` is a key-value RDD mapping each word to a measure
    of how informative the presence of that word is in discerning whether a message
    is spam or ham. Let''s print out the twenty words that are most informative of
    whether a message is ham or spam:'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE489]'
  id: totrans-1947
  prefs: []
  type: TYPE_PRE
  zh: '[PRE489]'
- en: 'Let''s run this using `spark-submit`:'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE490]'
  id: totrans-1949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE490]'
- en: Thus, we find that the presence of words like `language` or `free` or `!` carry
    the most information, because they are almost exclusively present in either just
    spam messages or just ham messages. A very simple classification algorithm could
    just take the top 10 (by mutual information) spam words, and the top 10 ham words
    and see whether a message contains more spam words or ham words. We will explore
    machine learning algorithms for classification in more depth in [Chapter 12](part0117.xhtml#aid-3FIHQ2
    "Chapter 12. Distributed Machine Learning with MLlib"), *Distributed Machine Learning
    with MLlib*.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
- en: Lifting the hood
  id: totrans-1951
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section of this chapter, we will discuss, very briefly, how Spark
    works internally. For a more detailed discussion, see the *References* section
    at the end of the chapter.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
- en: When you open a Spark context, either explicitly or by launching the Spark shell,
    Spark starts a web UI with details of how the current task and past tasks have
    executed. Let's see this in action for the example mutual information program
    we wrote in the last section. To prevent the context from shutting down when the
    program completes, you can insert a call to `readLine` as the last line of the
    `main` method (after the call to `takeOrdered`). This expects input from the user,
    and will therefore pause program execution until you press *enter*.
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
- en: To access the UI, point your browser to `127.0.0.1:4040`. If you have other
    instances of the Spark shell running, the port may be `4041`, or `4042` and so
    on.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
- en: '![Lifting the hood](img/image01207.jpeg)'
  id: totrans-1955
  prefs: []
  type: TYPE_IMG
- en: 'The first page of the UI tells us that our application contains three *jobs*.
    A job occurs as the result of an action. There are, indeed, three actions in our
    application: the first two are called within the `wordFractionInFiles` function:'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE491]'
  id: totrans-1957
  prefs: []
  type: TYPE_PRE
  zh: '[PRE491]'
- en: The last job results from the call to `takeOrdered`, which forces the execution
    of the entire pipeline of RDD transformations that calculate the mutual information.
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
- en: 'The web UI lets us delve deeper into each job. Click on the `takeOrdered` job
    in the job table. You will get taken to a page that describes the job in more
    detail:'
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
- en: '![Lifting the hood](img/image01208.jpeg)'
  id: totrans-1960
  prefs: []
  type: TYPE_IMG
- en: Of particular interest is the **DAG visualization** entry. This is a graph of
    the execution plan to fulfill the action, and provides a glimpse of the inner
    workings of Spark.
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
- en: 'When you define a job by calling an action on an RDD, Spark looks at the RDD''s
    lineage and constructs a graph mapping the dependencies: each RDD in the lineage
    is represented by a node, with directed edges going from this RDD''s parent to
    itself. This type of graph is called a **directed** **acyclic graph** (**DAG**),
    and is a data structure useful for dependency resolution. Let''s explore the DAG
    for the `takeOrdered` job in our program using the web UI. The graph is quite
    complex, and it is therefore easy to get lost, so here is a simplified reproduction
    that only lists the RDDs bound to variable names in the program.'
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
- en: '![Lifting the hood](img/image01209.jpeg)'
  id: totrans-1963
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, at the bottom of the graph, we have the `mutualInformation`
    RDD. This is the RDD that we need to construct for our action. This RDD depends
    on the intermediate elements in the sum, `igFragment1`, `igFragment2`, and so
    on. We can work our way back through the list of dependencies until we reach the
    other end of the graph: RDDs that do not depend on other RDDs, only on external
    sources.'
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
- en: Once the graph is built, the Spark engines formulates a plan to execute the
    job. The plan starts with the RDDs that only have external dependencies (such
    as RDDs built by loading files from disk or fetching from a database) or RDDs
    that already have cached data. Each arrow along the graph is translated to a set
    of *tasks*, with each task applying a transformation to a partition of the data.
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
- en: Tasks are grouped into *stages*. A stage consists of a set of tasks that can
    all be performed without needing an intermediate shuffle.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
- en: Data shuffling and partitions
  id: totrans-1967
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand data shuffling in Spark, we first need to understand how data
    is partitioned in RDDs. When we create an RDD by, for instance, loading a file
    from HDFS, or reading a file in local storage, Spark has no control over what
    bits of data are distributed in which partitions. This becomes a problem for key-value
    RDDs: these often require knowing where occurrences of a particular key are, for
    instance to perform a join. If the key can occur anywhere in the RDD, we have
    to look through every partition to find the key.'
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
- en: To prevent this, Spark allows the definition of a *partitioner* on key-value
    RDDs. A partitioner is an attribute of the RDD that determines which partition
    a particular key lands in. When an RDD has a partitioner set, the location of
    a key is entirely determined by the partitioner, and not by the RDD's history,
    or the number of keys. Two different RDDs with the same partitioner will map the
    same key to the same partition.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
- en: 'Partitions impact performance through their effect on transformations. There
    are two types of transformations on key-value RDDs:'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
- en: Narrow transformations, like `mapValues`. In narrow transformations, the data
    to compute a partition in the child RDD resides on a single partition in the parent.
    The data processing for a narrow transformation can therefore be performed entirely
    locally, without needing to communicate data between nodes.
  id: totrans-1971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wide transformations, like `reduceByKey`. In wide transformations, the data
    to compute any single partition can reside on all the partitions in the parent.
    The RDD resulting from a wide transformation will, in general, have a partitioner
    set. For instance, the output of a `reduceByKey` transformation are hash-partitioned
    by default: the partition that a particular key ends up in is determined by `hash(key)
    % numPartitions`.'
  id: totrans-1972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, in our mutual information example, the RDDs `pPresentAndSpam` and `pPresentAndHam`
    will have the same partition structure since they both have the default hash partitioner.
    All descendent RDDs retain the same keys, all the way down to `mutualInformation`.
    The word `language`, for instance, will be in the same partition for each RDD.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
- en: 'Why does all this matter? If an RDD has a partitioner set, this partitioner
    is retained through all subsequent narrow transformations originating from this
    RDD. Let''s go back to our mutual information example. The RDDs `pPresentGivenHam`
    and `pPresentGivenSpam` both originate from `reduceByKey` operations, and they
    both have string keys. They will therefore both have the same hash-partitioner
    (unless we explicitly set a different partitioner). This partitioner is retained
    as we construct `pPresentAndSpam` and `pPresentAndHam`. When we construct `pPresent`,
    we perform a full outer join of `pPresentAndSpam` and `pPresentAndHam`. Since
    both these RDDs have the same partitioner, the child RDD `pPresent` has narrow
    dependencies: we can just join the first partition of `pPresentAndSpam` with the
    first partition of `pPresentAndHam`, the second partition of `pPresentAndSpam`
    with the second partition of `pPresentAndHam` and so on, since any string key
    will be hashed to the same partition in both RDDs. By contrast, without partitioner,
    we would have to join the data in each partition of `pPresentAndSpam` with every
    partition of `pPresentAndSpam`. This would require sending data across the network
    to all the nodes holding `pPresentAndSpam`, a time-consuming exercise.'
  id: totrans-1974
  prefs: []
  type: TYPE_NORMAL
- en: This process of having to send the data to construct a child RDD across the
    network, as a result of wide dependencies, is called *shuffling*. Much of the
    art of optimizing a Spark program involves reducing shuffling and, when shuffling
    is necessary, reducing the amount of shuffling.
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1976
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the basics of Spark and learned how to construct
    and manipulate RDDs. In the next chapter, we will learn about Spark SQL and DataFrames,
    a set of implicit conversions that allow us to manipulate RDDs in a manner similar
    to pandas DataFrames, and how to interact with different data sources using Spark.
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  id: totrans-1978
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Spark*, by *Holden Karau*, *Andy Konwinski*, *Patrick Wendell*, and
    *Matei Zaharia*, *O''Reilly*, provides a much more complete introduction to Spark
    that this chapter can provide. I thoroughly recommend it.'
  id: totrans-1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested in learning more about information theory, I recommend
    *David MacKay's* book *Information Theory, Inference, and Learning Algorithms*.
  id: totrans-1980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information Retrieval*, by *Manning*, *Raghavan*, and *Schütze*, describes
    how to analyze textual data (including lemmatization and stemming). An online'
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the Ling-Spam dataset, and how to analyze it: [http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf](http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf).'
  id: totrans-1982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This blog post delves into the Spark Web UI in more detail. [https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html).
  id: totrans-1983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This blog post, by *Sandy Ryza*, is the first in a two-part series discussing
    Spark internals, and how to leverage them to improve performance: [http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/).'
  id: totrans-1984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 11. Spark SQL and DataFrames
  id: totrans-1985
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to build a simple distributed application
    using Spark. The data that we used took the form of a set of e-mails stored as
    text files.
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that Spark was built around the concept of **resilient distributed
    datasets** (**RDDs**). We explored several types of RDDs: simple RDDs of strings,
    key-value RDDs, and RDDs of doubles. In the case of key-value RDDs and RDDs of
    doubles, Spark added functionality beyond that of the simple RDDs through implicit
    conversions. There is one important type of RDD that we have not explored yet:
    **DataFrames** (previously called **SchemaRDD**). DataFrames allow the manipulation
    of objects significantly more complex than those we have explored to date.'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
- en: A DataFrame is a distributed tabular data structure, and is therefore very useful
    for representing and manipulating structured data. In this chapter, we will first
    investigate DataFrames through the Spark shell, and then use the Ling-spam e-mail
    dataset, presented in the previous chapter, to see how DataFrames can be integrated
    in a machine learning pipeline.
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames – a whirlwind introduction
  id: totrans-1989
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by opening a Spark shell:'
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE492]'
  id: totrans-1991
  prefs: []
  type: TYPE_PRE
  zh: '[PRE492]'
- en: Let's imagine that we are interested in running analytics on a set of patients
    to estimate their overall health level. We have measured, for each patient, their
    height, weight, age, and whether they smoke.
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
- en: 'We might represent the readings for each patient as a case class (you might
    wish to write some of this in a text editor and paste it into the Scala shell
    using `:paste`):'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE493]'
  id: totrans-1994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE493]'
- en: 'We would, typically, have many thousands of patients, possibly stored in a
    database or a CSV file. We will worry about how to interact with external sources
    later in this chapter. For now, let''s just hard-code a few readings directly
    in the shell:'
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE494]'
  id: totrans-1996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE494]'
- en: 'We can convert `readings` to an RDD by using `sc.parallelize`:'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE495]'
  id: totrans-1998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE495]'
- en: 'Note that the type parameter of our RDD is `PatientReadings`. Let''s convert
    the RDD to a DataFrame using the `.toDF` method:'
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE496]'
  id: totrans-2000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE496]'
- en: 'We have created a DataFrame where each row corresponds to the readings for
    a specific patient, and the columns correspond to the different features:'
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE497]'
  id: totrans-2002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE497]'
- en: The easiest way to create a DataFrame is to use the `toDF` method on an RDD.
    We can convert any `RDD[T]`, where `T` is a case class or a tuple, to a DataFrame.
    Spark will map each attribute of the case class to a column of the appropriate
    type in the DataFrame. It uses reflection to discover the names and types of the
    attributes. There are several other ways of constructing DataFrames, both from
    RDDs and from external sources, which we will explore later in this chapter.
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrames support many operations for manipulating the rows and columns. For
    instance, let''s add a column for the **Body Mass Index** (**BMI**). The BMI is
    a common way of aggregating *height* and *weight* to decide if someone is overweight
    or underweight. The formula for the BMI is:'
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrames – a whirlwind introduction](img/image01210.jpeg)'
  id: totrans-2005
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start by creating a column of the height in meters:'
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE498]'
  id: totrans-2007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE498]'
- en: '`heightM` has data type `Column`, representing a column of data in a DataFrame.
    Columns support many arithmetic and comparison operators that apply element-wise
    across the column (similarly to Breeze vectors encountered in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze*).
    Operations on columns are lazy: the `heightM` column is not actually computed
    when defined. Let''s now define a BMI column:'
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE499]'
  id: totrans-2009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE499]'
- en: 'It would be useful to add the `bmi` column to our readings DataFrame. Since
    DataFrames, like RDDs, are immutable, we must define a new DataFrame that is identical
    to `readingsDF`, but with an additional column for the BMI. We can do this using
    the `withColumn` method, which takes, as its arguments, the name of the new column
    and a `Column` instance:'
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE500]'
  id: totrans-2011
  prefs: []
  type: TYPE_PRE
  zh: '[PRE500]'
- en: 'All the operations we have seen so far are *transformations*: they define a
    pipeline of operations that create new DataFrames. These transformations are executed
    when we call an **action**, such as `show`:'
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE501]'
  id: totrans-2013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE501]'
- en: 'Besides creating additional columns, DataFrames also support filtering rows
    that satisfy a certain predicate. For instance, we can select all smokers:'
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE502]'
  id: totrans-2015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE502]'
- en: 'Or, to select everyone who weighs more than 70 kgs:'
  id: totrans-2016
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE503]'
  id: totrans-2017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE503]'
- en: 'It can become cumbersome to keep repeating the DataFrame name in an expression.
    Spark defines the operator `$` to refer to a column in the current DataFrame.
    Thus, the `filter` expression above could have been written more succinctly using:'
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE504]'
  id: totrans-2019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE504]'
- en: 'The `.filter` method is overloaded. It accepts either a column of Boolean values,
    as above, or a string identifying a Boolean column in the current DataFrame. Thus,
    to filter our `readingsWithBmiDF` DataFrame to sub-select smokers, we could also
    have used the following:'
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE505]'
  id: totrans-2021
  prefs: []
  type: TYPE_PRE
  zh: '[PRE505]'
- en: 'When comparing for equality, you must compare columns with the special *triple-equals*
    operator:'
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE506]'
  id: totrans-2023
  prefs: []
  type: TYPE_PRE
  zh: '[PRE506]'
- en: 'Similarly, you must use `!==` to select rows that are not equal to a value:'
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE507]'
  id: totrans-2025
  prefs: []
  type: TYPE_PRE
  zh: '[PRE507]'
- en: Aggregation operations
  id: totrans-2026
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen how to apply an operation to every row in a DataFrame to create
    a new column, and we have seen how to use filters to build new DataFrames with
    a sub-set of rows from the original DataFrame. The last set of operations on DataFrames
    is grouping operations, equivalent to the `GROUP BY` statement in SQL. Let''s
    calculate the average BMI for smokers and non-smokers. We must first tell Spark
    to group the DataFrame by a column (the `isSmoker` column, in this case), and
    then apply an aggregation operation (averaging, in this case) to reduce each group:'
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE508]'
  id: totrans-2028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE508]'
- en: 'This has created a new DataFrame with two columns: the grouping column and
    the column over which we aggregated. Let''s show this DataFrame:'
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE509]'
  id: totrans-2030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE509]'
- en: 'Besides averaging, there are several operators for performing the aggregation
    across each group. We outline some of the more important ones in the table below,
    but, for a full list, consult the *Aggregate functions* section of [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions%24):'
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
- en: '| Operator | Notes |'
  id: totrans-2032
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-2033
  prefs: []
  type: TYPE_TB
- en: '| `avg(column)` | Group averages of the values in the specified column. |'
  id: totrans-2034
  prefs: []
  type: TYPE_TB
- en: '| `count(column)` | Number of elements in each group in the specified column.
    |'
  id: totrans-2035
  prefs: []
  type: TYPE_TB
- en: '| `countDistinct(column, ... )` | Number of distinct elements in each group.
    This can also accept multiple columns to return the count of unique elements across
    several columns. |'
  id: totrans-2036
  prefs: []
  type: TYPE_TB
- en: '| `first(column), last(column)` | First/last element in each group |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
- en: '| `max(column), min(column)` | Largest/smallest element in each group |'
  id: totrans-2038
  prefs: []
  type: TYPE_TB
- en: '| `sum(column)` | Sum of the values in each group |'
  id: totrans-2039
  prefs: []
  type: TYPE_TB
- en: 'Each aggregation operator takes either the name of a column, as a string, or
    an expression of type `Column`. The latter allows aggregation of compound expressions.
    If we wanted the average height, in meters, of the smokers and non-smokers in
    our sample, we could use:'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE510]'
  id: totrans-2041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE510]'
- en: 'We can also use compound expressions to define the column on which to group.
    For instance, to count the number of patients in each `age` group, increasing
    by decade, we can use:'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  id: totrans-2043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE511]'
- en: We have used the short-hand `"*"` to indicate a count over every column.
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
- en: Joining DataFrames together
  id: totrans-2045
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only considered operations on a single DataFrame. Spark also
    offers SQL-like joins to combine DataFrames. Let''s assume that we have another
    DataFrame mapping the patient id to a (systolic) blood pressure measurement. We
    will assume we have the data as a list of pairs mapping patient IDs to blood pressures:'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE512]'
  id: totrans-2047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE512]'
- en: 'We can construct a DataFrame from this RDD of tuples. However, unlike when
    constructing DataFrames from RDDs of case classes, Spark cannot infer column names.
    We must therefore pass these explicitly to `.toDF`:'
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE513]'
  id: totrans-2049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE513]'
- en: 'Let''s join `bloodPressureDF` with `readingsDF`, using the patient ID as the
    join key:'
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE514]'
  id: totrans-2051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE514]'
- en: 'This performs an *inner join*: only patient IDs present in both DataFrames
    are included in the result. The type of join can be passed as an extra argument
    to `join`. For instance, we can perform a *left join*:'
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE515]'
  id: totrans-2053
  prefs: []
  type: TYPE_PRE
  zh: '[PRE515]'
- en: Possible join types are `inner`, `outer`, `leftouter`, `rightouter`, or `leftsemi`.
    These should all be familiar, apart from `leftsemi`, which corresponds to a *left
    semi join*.This is the same as an inner join, but only the columns on the left-hand
    side are retained after the join. It is thus a way to filter a DataFrame for rows
    which are present in another DataFrame.
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
- en: Custom functions on DataFrames
  id: totrans-2055
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only used built-in functions to operate on DataFrame columns.
    While these are often sufficient, we sometimes need greater flexibility. Spark
    lets us apply custom transformations to every row through **user-defined functions**
    (**UDFs**). Let''s assume that we want to use the equation that we derived in
    [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data with Breeze"),
    *Manipulating Data with Breeze*, for the probability of a person being male, given
    their height and weight. We calculated that the decision boundary was given by:'
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
- en: '![Custom functions on DataFrames](img/image01211.jpeg)'
  id: totrans-2057
  prefs: []
  type: TYPE_IMG
- en: 'Any person with *f > 0* is more likely to be male than female, given their
    height and weight and the training set used for [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze* (which
    was based on students, so is unlikely to be representative of the population as
    a whole). To convert from a height in centimeters to the normalized height, *rescaledHeight*,
    we can use this formula:'
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
- en: '![Custom functions on DataFrames](img/image01212.jpeg)'
  id: totrans-2059
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, to convert a weight (in kilograms) to the normalized weight, *rescaledWeight*,
    we can use:'
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
- en: '![Custom functions on DataFrames](img/image01213.jpeg)'
  id: totrans-2061
  prefs: []
  type: TYPE_IMG
- en: 'The average and standard deviation of the *height* and *weight* are calculated
    from the training set. Let''s write a Scala function that returns whether a person
    is more likely to be male, given their height and weight:'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE516]'
  id: totrans-2063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE516]'
- en: 'To use this function on Spark DataFrames, we need to register it as a **user-defined
    function** (**UDF**). This transforms our function, which accepts integer arguments,
    into one that accepts column arguments:'
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE517]'
  id: totrans-2065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE517]'
- en: To register a UDF, we must have access to a `sqlContext` instance. The SQL context
    provides the entry point for DataFrame operations. The Spark shell creates a SQL
    context at startup, bound to the variable `sqlContext`, and destroys it when the
    shell session is closed.
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
- en: 'The first argument passed to the `register` function is the name of the UDF
    (we will use the UDF name later when we write SQL statements on the DataFrame,
    but you can ignore it for now). We can then use the UDF just like the built-in
    transformations included in Spark:'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE518]'
  id: totrans-2068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE518]'
- en: 'As you can see, Spark applies the function underlying the UDF to every row
    in the DataFrame. We are not limited to using UDFs to create new columns. We can
    also use them in `filter` expressions. For instance, to select rows likely to
    correspond to women:'
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE519]'
  id: totrans-2070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE519]'
- en: Using UDFs lets us define arbitrary Scala functions to transform rows, giving
    tremendous additional power for data manipulation.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame immutability and persistence
  id: totrans-2072
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataFrames, like RDDs, are immutable. When you define a transformation on a
    DataFrame, this always creates a new DataFrame. The original DataFrame cannot
    be modified in place (this is notably different to pandas DataFrames, for instance).
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
- en: 'Operations on DataFrames can be grouped into two: *transformations*, which
    result in the creation of a new DataFrame, and *actions*, which usually return
    a Scala type or have a side-effect. Methods like `filter` or `withColumn` are
    transformations, while methods like `show` or `head` are actions.'
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
- en: Transformations are lazy, much like transformations on RDDs. When you generate
    a new DataFrame by transforming an existing DataFrame, this results in the elaboration
    of an execution plan for creating the new DataFrame, but the data itself is not
    transformed immediately. You can access the execution plan with the `queryExecution`
    method.
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
- en: 'When you call an action on a DataFrame, Spark processes the action as if it
    were a regular RDD: it implicitly builds a direct acyclic graph to resolve dependencies,
    processing the transformations needed to build the DataFrame on which the action
    was called.'
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like RDDs, we can persist DataFrames in memory or on disk:'
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE520]'
  id: totrans-2078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE520]'
- en: 'This works in the same way as persisting RDDs: next time the RDD is calculated,
    it will be kept in memory (provided there is enough space), rather than discarded.
    The level of persistence can also be set:'
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE521]'
  id: totrans-2080
  prefs: []
  type: TYPE_PRE
  zh: '[PRE521]'
- en: SQL statements on DataFrames
  id: totrans-2081
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you will have noticed that many operations on DataFrames are inspired
    by SQL operations. Additionally, Spark allows us to register DataFrames as tables
    and query them with SQL statements directly. We can therefore build a temporary
    database as part of the program flow.
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s register `readingsDF` as a temporary table:'
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE522]'
  id: totrans-2084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE522]'
- en: This registers a temporary table that can be used in SQL queries. Registering
    a temporary table relies on the presence of a SQL context. The temporary tables
    are destroyed when the SQL context is destroyed (when we close the shell, for
    instance).
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore what we can do with our temporary tables and the SQL context.
    We can first get a list of all the tables currently registered with the context:'
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE523]'
  id: totrans-2087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE523]'
- en: 'This returns a DataFrame. In general, all operations on a SQL context that
    return data return DataFrames:'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE524]'
  id: totrans-2089
  prefs: []
  type: TYPE_PRE
  zh: '[PRE524]'
- en: 'We can query this table by passing SQL statements to the SQL context:'
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE525]'
  id: totrans-2091
  prefs: []
  type: TYPE_PRE
  zh: '[PRE525]'
- en: 'Any UDFs registered with the `sqlContext` are available through the name given
    to them when they were registered. We can therefore use them in SQL queries:'
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE526]'
  id: totrans-2093
  prefs: []
  type: TYPE_PRE
  zh: '[PRE526]'
- en: You might wonder why one would want to register DataFrames as temporary tables
    and run SQL queries on those tables, when the same functionality is available
    directly on DataFrames. The main reason is for interacting with external tools.
    Spark can run a SQL engine that exposes a JDBC interface, meaning that programs
    that know how to interact with a SQL database will be able to make use of the
    temporary tables.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
- en: We don't have the space to cover how to set up a distributed SQL engine in this
    book, but you can find details in the Spark documentation ([http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine](http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)).
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
- en: Complex data types – arrays, maps, and structs
  id: totrans-2096
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, all the elements in our DataFrames were simple types. DataFrames support
    three additional collection types: arrays, maps, and structs.'
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
- en: Structs
  id: totrans-2098
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first compound type that we will look at is the **struct**. A struct is
    similar to a case class: it stores a set of key-value pairs, with a fixed set
    of keys. If we convert an RDD of a case class containing nested case classes to
    a DataFrame, Spark will convert the nested objects to a struct.'
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that we want to serialize Lords of the Ring characters. We might
    use the following object model:'
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE527]'
  id: totrans-2101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE527]'
- en: 'We want to create a DataFrame of `LotrCharacter` instances. Let''s create some
    dummy data:'
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE528]'
  id: totrans-2103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE528]'
- en: 'The `weapon` attribute in the case class was converted to a struct column in
    the DataFrame. To extract sub-fields from a struct, we can pass the field name
    to the column''s `.apply` method:'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE529]'
  id: totrans-2105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE529]'
- en: 'We can use this derived column just as we would any other column. For instance,
    let''s filter our DataFrame to only contain characters who wield a sword:'
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE530]'
  id: totrans-2107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE530]'
- en: Arrays
  id: totrans-2108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s return to the earlier example, and assume that, besides height, weight,
    and age measurements, we also have phone numbers for our patients. Each patient
    might have zero, one, or more phone numbers. We will define a new case class and
    new dummy data:'
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE531]'
  id: totrans-2110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE531]'
- en: 'The `List[String]` array in our case class gets translated to an `array<string>`
    data type:'
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE532]'
  id: totrans-2112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE532]'
- en: 'As with structs, we can construct a column for a specific index the array.
    For instance, we can select the first element in each array:'
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE533]'
  id: totrans-2114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE533]'
- en: Maps
  id: totrans-2115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last compound data type is the map. Maps are similar to structs inasmuch
    as they store key-value pairs, but the set of keys is not fixed when the DataFrame
    is created. They can thus store arbitrary key-value pairs.
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
- en: Scala maps will be converted to DataFrame maps when the DataFrame is constructed.
    They can then be queried in a manner similar to structs.
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with data sources
  id: totrans-2118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A major challenge in data science or engineering is dealing with the wealth
    of input and output formats for persisting data. We might receive or send data
    as CSV files, JSON files, or through a SQL database, to name a few.
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a unified API for serializing and de-serializing DataFrames to
    and from different data sources.
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
- en: JSON files
  id: totrans-2121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark supports loading data from JSON files, provided that each line in the
    JSON file corresponds to a single JSON object. Each object will be mapped to a
    DataFrame row. JSON arrays are mapped to arrays, and embedded objects are mapped
    to structs.
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
- en: This section would be a little dry without some data, so let's generate some
    from the GitHub API. Unfortunately, the GitHub API does not return JSON formatted
    as a single object per line. The code repository for this chapter contains a script,
    `FetchData.scala` which will download and format JSON entries for Martin Odersky's
    repositories, saving the objects to a file named `odersky_repos.json` (go ahead
    and change the GitHub user in `FetchData.scala` if you want). You can also download
    a pre-constructed data file from [data.scala4datascience.com/odersky_repos.json](http://data.scala4datascience.com/odersky_repos.json).
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive into the Spark shell and load this data into a DataFrame. Reading
    from a JSON file is as simple as passing the file name to the `sqlContext.read.json`
    method:'
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE534]'
  id: totrans-2125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE534]'
- en: 'Reading from a JSON file loads data as a DataFrame. Spark automatically infers
    the schema from the JSON documents. There are many columns in our DataFrame. Let''s
    sub-select a few to get a more manageable DataFrame:'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE535]'
  id: totrans-2127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE535]'
- en: 'Let''s save the DataFrame back to JSON:'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE536]'
  id: totrans-2129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE536]'
- en: If you look at the files present in the directory in which you are running the
    Spark shell, you will notice a `repos_short.json` directory. Inside it, you will
    see files named `part-000000`, `part-000001`, and so on. When serializing JSON,
    each partition of the DataFrame is serialized independently. If you are running
    this on several machines, you will find parts of the serialized output on each
    computer.
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
- en: 'You may, optionally, pass a `mode` argument to control how Spark deals with
    the case of an existing `repos_short.json` file:'
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE537]'
  id: totrans-2132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE537]'
- en: Available save modes are `ErrorIfExists`, `Append` (only available for Parquet
    files), `Overwrite`, and `Ignore` (do not save if the file exists already).
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
- en: Parquet files
  id: totrans-2134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Parquet is a popular file format well-suited for storing tabular data.
    It is often used for serialization in the Hadoop ecosystem, since it allows for
    efficient extraction of specific columns and rows without having to read the entire
    file.
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
- en: 'Serialization and deserialization of Parquet files is identical to JSON, with
    the substitution of `json` with `parquet`:'
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE538]'
  id: totrans-2137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE538]'
- en: In general, Parquet will be more space-efficient than JSON for storing large
    collections of objects. Parquet is also much more efficient at retrieving specific
    columns or rows, if the partition can be inferred from the row. Parquet is thus
    advantageous over JSON unless you need the output to be human-readable, or de-serializable
    by an external program.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
- en: Standalone programs
  id: totrans-2139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been using Spark SQL and DataFrames through the Spark shell.
    To use it in standalone programs, you will need to create it explicitly, from
    a Spark context:'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE539]'
  id: totrans-2141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE539]'
- en: 'Additionally, importing the `implicits` object nested in `sqlContext` allows
    the conversions of RDDs to DataFrames:'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE540]'
  id: totrans-2143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE540]'
- en: We will use DataFrames extensively in the next chapter to manipulate data to
    get it ready for use with MLlib.
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-2145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Spark SQL and DataFrames. DataFrames add a rich
    layer of abstraction on top of Spark's core engine, greatly facilitating the manipulation
    of tabular data. Additionally, the source API allows the serialization and de-serialization
    of DataFrames from a rich variety of data files.
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on our knowledge of Spark and DataFrames
    to build a spam filter using MLlib.
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-2148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataFrames are a relatively recent addition to Spark. There is thus still a
    dearth of literature and documentation. The first port of call should be the Scala
    docs, available at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame).'
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scaladocs for operations available on the DataFrame `Column` type can be
    found at: [http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column).'
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also extensive documentation on the Parquet file format: [https://parquet.apache.org](https://parquet.apache.org).'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 12. Distributed Machine Learning with MLlib
  id: totrans-2152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning describes the construction of algorithms that make predictions
    from data. It is a core component of most data science pipelines, and is often
    seen to be the component adding the most value: the accuracy of the machine learning
    algorithm determines the success of the data science endeavor. It is also, arguably,
    the section of the data science pipeline that requires the most knowledge from
    fields beyond software engineering: a machine learning expert will be familiar,
    not just with algorithms, but also with statistics and with the business domain.'
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
- en: Choosing and tuning a machine learning algorithm to solve a particular problem
    involves significant exploratory analysis to try and determine which features
    are relevant, how features are correlated, whether there are outliers in the dataset,
    and so on. Designing suitable machine learning pipelines is difficult. Add on
    an additional layer of complexity resulting from the size of datasets and the
    need for scalability, and you have a real challenge.
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
- en: '**MLlib** helps mitigate this difficulty. MLlib is a component of Spark that
    provides machine learning algorithms on top of the core Spark libraries. It offers
    a set of learning algorithms that parallelize well over distributed datasets.'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
- en: 'MLlib has evolved into two separate layers. MLlib itself contains the core
    algorithms, and **ml**, also called the *pipeline API*, defines an API for gluing
    algorithms together and provides a higher level of abstraction. The two libraries
    differ in the data types on which they operate: the original MLlib predates the
    introduction of DataFrames, and acts mainly on RDDs of feature vectors. The pipeline
    API operates on DataFrames.'
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study the newer pipeline API, diving into MLlib only
    when the functionality is missing from the pipeline API.
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
- en: This chapter does not try to teach the machine learning fundamentals behind
    the algorithms that we present. We assume that the reader has a good enough grasp
    of machine learning tools and techniques to understand, at least superficially,
    what the algorithms presented here do, and we defer to better authors for in-depth
    explanations of the mechanics of statistical learning (we present several references
    at the end of the chapter).
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
- en: MLlib is a rich library that is evolving rapidly. This chapter does not aim
    to give a complete overview of the library. We will work through the construction
    of a machine learning pipeline to train a spam filter, learning about the parts
    of MLlib that we need along the way. Having read this chapter, you will have an
    understanding of how the different parts of the library fit together, and can
    use the online documentation, or a more specialized book (see references at the
    end of this chapter) to learn about the parts of MLlib not covered here.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MLlib – Spam classification
  id: totrans-2160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's introduce MLlib with a concrete example. We will look at spam classification
    using the Ling-Spam dataset that we used in the [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*. We will create a spam filter that uses logistic regression to estimate
    the probability that a given message is spam.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
- en: We will run through examples using the Spark shell, but you will find an analogous
    program in `LogisticRegressionDemo.scala` among the examples for this chapter.
    If you have not installed Spark, refer to [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*, for installation instructions.
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by loading the e-mails in the Ling-Spam dataset. If you have not
    done this for [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark*, download
    the data from [data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)
    or [data.scala4datascience.com/ling-spam.zip](http://data.scala4datascience.com/ling-spam.zip),
    depending on whether you want a `tar.gz` file or a `zip` file, and unpack the
    archive. This will create a `spam` directory and a `ham` directory containing
    spam and ham messages, respectively.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `wholeTextFiles` method to load spam and ham e-mails:'
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE541]'
  id: totrans-2165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE541]'
- en: 'The `wholeTextFiles` method creates a key-value RDD where the keys are the
    file names and the values are the contents of the files:'
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE542]'
  id: totrans-2167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE542]'
- en: 'The algorithms in the pipeline API work on DataFrames. We must therefore convert
    our key-value RDDs to DataFrames. We define a new case class, `LabelledDocument`,
    which contains a message text and a category label identifying whether a message
    is `spam` or `ham`:'
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE543]'
  id: totrans-2169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE543]'
- en: 'To create models, we will need all the documents in a single DataFrame. Let''s
    therefore take the union of our two `LabelledDocument` RDDs, and transform that
    to a DataFrame. The `union` method concatenates RDDs together:'
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE544]'
  id: totrans-2171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE544]'
- en: Let's do some basic checks to verify that we have loaded all the documents.
    We start by persisting the DataFrame in memory to avoid having to re-create it
    from the raw text files.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE545]'
  id: totrans-2173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE545]'
- en: Let's now split the DataFrame into a training set and a test set. We will use
    the test set to validate the model that we build. For now, we will just use a
    single split, training the model on 70% of the data and testing it on the remaining
    30%. In the next section, we will look at cross-validation, which provides more
    rigorous way to check the accuracy of our models.
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve this 70-30 split using the DataFrame''s `.randomSplit` method:'
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE546]'
  id: totrans-2176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE546]'
- en: 'The `.randomSplit` method takes an array of weights and returns an array of
    DataFrames, of approximately the size specified by the weights. For instance,
    we passed weights `0.7` and `0.3`, indicating that any given row has a 70% chance
    of ending up in `trainDF`, and a 30% chance of ending up in `testDF`. Note that
    this means the split DataFrames are not of fixed size: `trainDF` is approximately,
    but not exactly, 70% the size of `documentsDF`:'
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE547]'
  id: totrans-2178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE547]'
- en: If you need a fixed size sample, use the DataFrame's `.sample` method to obtain
    `trainDF` and filter `documentDF` for rows not in `trainDF`.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now in a position to start using MLlib. Our attempt at classification
    will involve performing logistic regression on *term-frequency vectors*: we will
    count how often each word appears in each message, and use the frequency of occurrence
    as a feature. Before jumping into the code, let''s take a step back and discuss
    the structure of machine learning pipelines.'
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline components
  id: totrans-2181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipelines consist of a set of components joined together such that the DataFrame
    produced by one component is used as input for the next component. The components
    available are split into two classes: *transformers* and *estimators*.'
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  id: totrans-2183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Transformers** transform one DataFrame into another, normally by appending
    one or more columns.'
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in our spam classification algorithm is to split each message
    into an array of words. This is called **tokenization**. We can use the `Tokenizer`
    transformer, provided by MLlib:'
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE548]'
  id: totrans-2186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE548]'
- en: 'The behavior of transformers can be customized through getters and setters.
    The easiest way of obtaining a list of the parameters available is to call the
    `.explainParams` method:'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE549]'
  id: totrans-2188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE549]'
- en: 'We see that the behavior of a `Tokenizer` instance can be customized using
    two parameters: `inputCol` and `outputCol`, describing the header of the column
    containing the input (the string to be tokenized) and the output (the array of
    words), respectively. We can set these parameters using the `setInputCol` and
    `setOutputCol` methods.'
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
- en: 'We set `inputCol` to `"text"`, since that is what the column is called in our
    training and test DataFrames. We will set `outputCol` to `"words"`:'
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE550]'
  id: totrans-2191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE550]'
- en: In due course, we will integrate `tokenizer` into a pipeline, but, for now,
    let's just use it to transform the training DataFrame, to verify that it works
    correctly.
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE551]'
  id: totrans-2193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE551]'
- en: The `tokenizer` transformer produces a new DataFrame with an additional column,
    `words`, containing an array of the words in the `text` column.
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we can use our `tokenizer` to transform any DataFrame with the correct
    schema. We could, for instance, use it on the test set. Much of machine learning
    involves calling the same (or a very similar) pipeline on different data sets.
    By providing the pipeline abstraction, MLlib facilitates reasoning about complex
    machine learning algorithms consisting of many cleaning, transformation, and modeling
    components.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our pipeline is to calculate the frequency of occurrence of
    each word in each message. We will eventually use these frequencies as features
    in our algorithm. We will use the `HashingTF` transformer to transform from arrays
    of words to word frequency vectors for each message.
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
- en: The `HashingTF` transformer constructs a sparse vector of word frequencies from
    input iterables. Each element in the word array gets transformed to a hash code.
    This hash code is truncated to a value between *0* and a large number *n*, the
    total number of elements in the output vector. The term frequency vector is just
    the number of occurrences of the truncated hash.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through an example manually to understand how this works. We will
    calculate the term frequency vector for `Array("the", "dog", "jumped", "over",
    "the")`. Let''s set *n*, the number of elements in the sparse output vector, to
    16 for this example. The first step is to calculate the hash code for each element
    in our array. We can use the built-in `##` method, which calculates a hash code
    for any object:'
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE552]'
  id: totrans-2199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE552]'
- en: 'To transform the hash codes into valid vector indices, we take the modulo of
    each hash by the size of the vector (`16`, in this case):'
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE553]'
  id: totrans-2201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE553]'
- en: 'We can then create a mapping from indices to the number of times that index
    appears:'
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE554]'
  id: totrans-2203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE554]'
- en: 'Finally, we can convert this map to a sparse vector, where the value at each
    element in the vector is the frequency with which this particular index occurs:'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE555]'
  id: totrans-2205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE555]'
- en: 'Note that the `.toString` output for a sparse vector consists of three elements:
    the total size of the vector, followed by two lists: the first is a series of
    indices, and the second is a series of values at those indices.'
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a sparse vector provides a compact and efficient way of representing
    the frequency of occurrence of words in the message, and is exactly how `HashingTF`
    works under the hood. The disadvantage is that the mapping from words to indices
    is not necessarily unique: truncating hash codes by the length of the vector will
    map different strings to the same index. This is known as a *collision*. The solution
    is to make *n* large enough that the frequency of collisions is minimized.'
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-2208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HashingTF` is similar to building a hash table (for example, a Scala map)
    whose keys are words and whose values are the number of times that word occurs
    in the message, with one important difference: it does not attempt to deal with
    hash collisions. Thus, if two words map to the same hash, they will have the wrong
    frequency. There are two advantages to using this algorithm over just constructing
    a hash table:'
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
- en: We do not have to maintain a list of distinct words in memory.
  id: totrans-2210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each e-mail can be transformed to a vector independently of all others: we
    do not have to reduce over different partitions to get the set of keys in the
    map. This greatly eases applying this algorithm to each e-mail in a distributed
    manner, since we can apply the `HashingTF` transformation on each partition independently.'
  id: totrans-2211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main disadvantage is that we must use machine learning algorithms that can
    take advantage of the sparse representation efficiently. This is the case with
    logistic regression, which we will use here.
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, the `HashingTF` transformer takes, as parameters, the input
    and output columns. It also takes a parameter defining the number of distinct
    hash buckets in the vector. Increasing the number of buckets decreases the number
    of collisions. In practice, a value between ![Transformers](img/image01214.jpeg)
    and ![Transformers](img/image01215.jpeg) is recommended.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE556]'
  id: totrans-2214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE556]'
- en: 'Each element in the `features` column is a sparse vector:'
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE557]'
  id: totrans-2216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE557]'
- en: 'We can thus interpret our vector as: the word that hashes to element `33` occurs
    three times, the word that hashes to element `36` occurs four times etc.'
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
- en: Estimators
  id: totrans-2218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have the features ready for logistic regression. The last step prior
    to running logistic regression is to create the target variable. We will transform
    the `category` column in our DataFrame to a binary 0/1 target column. Spark provides
    a `StringIndexer` class that replaces a set of strings in a column with doubles.
    A `StringIndexer` is not a transformer: it must first be ''fitted'' to a set of
    categories to calculate the mapping from string to numeric value. This introduces
    the second class of components in the pipeline API: *estimators*.'
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a transformer, which works "out of the box", an estimator must be fitted
    to a DataFrame. For our string indexer, the fitting process involves obtaining
    the list of unique strings (`"spam"` and `"ham"`) and mapping each of these to
    a double. The fitting process outputs a transformer which can be used on subsequent
    DataFrames.
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE558]'
  id: totrans-2221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE558]'
- en: 'The transformer produced by the fitting process has a `labels` attribute describing
    the mapping it applies:'
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE559]'
  id: totrans-2223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE559]'
- en: 'Each label will get mapped to its index in the array: thus, our transformer
    maps `ham` to `0` and `spam` to `1`:'
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE560]'
  id: totrans-2225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE560]'
- en: 'We now have the feature vectors and classification labels in the correct format
    for logistic regression. The component for performing logistic regression is an
    estimator: it is fitted to a training DataFrame to create a trained model. The
    model can then be used to transform test DataFrames.'
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE561]'
  id: totrans-2227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE561]'
- en: 'The `LogisticRegression` estimator expects the feature column to be named `"features"`
    and the label column (the target) to be named `"label"`, by default. There is
    no need to set these explicitly, since they match the column names set by `hashingTF`
    and `indexer`. There are several parameters that can be set to control how logistic
    regression works:'
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE562]'
  id: totrans-2229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE562]'
- en: 'For now, we just set the `maxIter` parameter. We will look at the effect of
    other parameters, such as regularization, later on. Let''s now fit the classifier
    to `labelledDF`:'
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE563]'
  id: totrans-2231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE563]'
- en: 'This produces a transformer that we can use on a DataFrame with a `features`
    column. The transformer appends a `prediction` column and a `probability` column.
    We can, for instance use `trainedClassifier` to transform `labelledDF`, the training
    set itself:'
  id: totrans-2232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE564]'
  id: totrans-2233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE564]'
- en: 'A quick way of checking the performance of our model is to just count the number
    of misclassified messages:'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE565]'
  id: totrans-2235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE565]'
- en: In this case, logistic regression managed to correctly classify every message
    but one in the training set. This is perhaps unsurprising, given the large number
    of features and the relatively clear demarcation between the words used in spam
    and legitimate e-mails.
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the real test of a model is not how well it performs on the training
    set, but how well it performs on a test set. To test this, we could just push
    the test DataFrame through the same stages that we used to train the model, replacing
    estimators with the fitted transformer that they produced. MLlib provides the
    *pipeline* abstraction to facilitate this: we wrap an ordered list of transformers
    and estimators in a pipeline. This pipeline is then fitted to a DataFrame corresponding
    to the training set. The fitting produces a `PipelineModel` instance, equivalent
    to the pipeline but with estimators replaced by transformers, as shown in this
    diagram:'
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimators](img/image01216.jpeg)'
  id: totrans-2238
  prefs: []
  type: TYPE_IMG
- en: 'Let''s construct the pipeline for our logistic regression spam filter:'
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE566]'
  id: totrans-2240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE566]'
- en: 'Once the pipeline is defined, we fit it to the DataFrame holding the training
    set:'
  id: totrans-2241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE567]'
  id: totrans-2242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE567]'
- en: 'When fitting a pipeline to a DataFrame, estimators and transformers are treated
    differently:'
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are applied to the DataFrame and copied, as is, into the pipeline
    model.
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimators are fitted to the DataFrame, producing a transformer. The transformer
    is then applied to the DataFrame, and appended to the pipeline model.
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now apply the pipeline model to the test set:'
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE568]'
  id: totrans-2247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE568]'
- en: 'This has added a `prediction` column to the DataFrame with the predictions
    of our logistic regression model. To measure the performance of our algorithm,
    we calculate the classification error on the test set:'
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE569]'
  id: totrans-2249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE569]'
- en: Thus, our naive logistic regression algorithm, with no model selection, or regularization,
    mis-classifies 2.3% of e-mails. You may, of course, get slightly different results,
    since the train-test split was random.
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s save the training and test DataFrames, with predictions, as `parquet`
    files:'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE570]'
  id: totrans-2252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE570]'
- en: Tip
  id: totrans-2253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In spam classification, a false positive is considerably worse than a false
    negative: it is much worse to classify a legitimate message as spam, than it is
    to let a spam message through. To account for this, we could increase the threshold
    for classification: only messages that score, for instance, 0.7 or above would
    get classified as spam. This raises the obvious question of choosing the right
    threshold. One way to do this would be to investigate the false positive rate
    incurred in the test set for different thresholds, and choosing the lowest threshold
    to give us an acceptable false positive rate. A good way of visualizing this is
    to use ROC curves, which we will investigate in the next section.'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  id: totrans-2255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, the functionality for evaluating model quality in the pipeline
    API remains limited, as of version 1.5.2\. Logistic regression does output a summary
    containing several evaluation metrics (available through the `summary` attribute
    on the trained model), but these are calculated on the training set. In general,
    we want to evaluate the performance of the model both on the training set and
    on a separate test set. We will therefore dive down to the underlying MLlib layer
    to access evaluation metrics.
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
- en: MLlib provides a module, `org.apache.spark.mllib.evaluation`, with a set of
    classes for assessing the quality of a model. We will use the `BinaryClassificationMetrics`
    class here, since spam classification is a binary classification problem. Other
    evaluation classes provide metrics for multi-class models, regression models and
    ranking models.
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous section, we will illustrate the concepts in the shell, but
    you will find analogous code in the `ROC.scala` script in the code examples for
    this chapter. We will use *breeze-viz* to plot curves, so, when starting the shell,
    we must ensure that the relevant libraries are on the classpath. We will use SBT
    assembly, as described in [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark* (specifically,
    the *Building and running standalone programs* section), to create a JAR with
    the required dependencies. We will then pass this JAR to the Spark shell, allowing
    us to import breeze-viz. Let''s write a `build.sbt` file that declares a dependency
    on breeze-viz:'
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE571]'
  id: totrans-2259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE571]'
- en: 'Package the dependencies into a jar with:'
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE572]'
  id: totrans-2261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE572]'
- en: 'This will create a jar called `spam_filter-assembly-0.1-SNAPSHOT.jar` in the
    `target/scala-2.10`/ directory. To include this jar in the Spark shell, re-start
    the shell with the `--jars` command line argument:'
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE573]'
  id: totrans-2263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE573]'
- en: 'To verify that the packaging worked correctly, try to import `breeze.plot`:'
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE574]'
  id: totrans-2265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE574]'
- en: 'Let''s load the test set, with predictions, which we created in the previous
    section and saved as a `parquet` file:'
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE575]'
  id: totrans-2267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE575]'
- en: 'The `BinaryClassificationMetrics` object expects an `RDD[(Double, Double)]`
    object of pairs of scores (the probability assigned by the classifier that a particular
    e-mail is spam) and labels (whether an e-mail is actually spam). We can extract
    this RDD from our DataFrame:'
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE576]'
  id: totrans-2269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE576]'
- en: 'We can now construct the `BinaryClassificationMetrics` instance:'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE577]'
  id: totrans-2271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE577]'
- en: The `BinaryClassificationMetrics` objects contain many useful metrics for evaluating
    the performance of a classification model. We will look at the **receiver operating**
    **characteristic** (**ROC**) curve.
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-2273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**ROC Curves**'
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine gradually decreasing, from 1.0, the probability threshold at which
    we assume a particular e-mail is spam. Clearly, when the threshold is set to 1.0,
    no e-mails will get classified as spam. This means that there will be no **false
    positives** (ham messages which we incorrectly classify as spam), but it also
    means that there will be no **true positives** (spam messages that we correctly
    identify as spam): all spam e-mails will be incorrectly identified as ham.'
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
- en: As we gradually lower the probability threshold at which we assume a particular
    e-mail is spam, our spam filter will, hopefully, start identifying a large fraction
    of e-mails as spam. The vast majority of these will, if our algorithm is well-designed,
    be real spam. Thus, our rate of true positives increases. As we gradually lower
    the threshold, we start classifying messages about which we are less sure of as
    spam. This will increase the number of messages correctly identified as spam,
    but it will also increase the number of false positives.
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve plots, for each threshold value, the fraction of true positives
    against the fraction of false positives. In the best case, the curve is always
    1: this happens when all spam messages are given a score of 1.0, and all ham messages
    are given a score of 0.0\. By contrast, the worst case happens when the curve
    is a diagonal *P(true positive) = P(false positive)*, which occurs when our algorithm
    does no better than random. In general, ROC curves fall somewhere in between,
    forming a convex shell above the diagonal. The deeper this shell, the better our
    algorithm.'
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation](img/image01217.jpeg)'
  id: totrans-2278
  prefs: []
  type: TYPE_IMG
- en: '(left) ROC curve for a model performing much better than random: the curve
    reaches very high true positive rates for a low false positive rate.'
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
- en: (middle) ROC curve for a model performing significantly better than random.
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
- en: '(right) ROC curve for a model performing only marginally better than random:
    the true positive rate is only marginally larger than the rate of false positives,
    for any given threshold, meaning that nearly half the examples are misclassified.'
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate an array of points on the ROC curve using the `.roc` method
    on our `BinaryClassificationMetrics` instance. This returns an `RDD[(Double, Double)]`
    of (*false positive*, *true positive*) fractions for each threshold value. We
    can collect this as an array:'
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE578]'
  id: totrans-2283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE578]'
- en: 'Of course, an array of numbers is not very enlightening, so let''s plot the
    ROC curve with breeze-viz. We start by transforming our array of pairs into two
    arrays, one of false positives and one of true positives:'
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE579]'
  id: totrans-2285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE579]'
- en: 'Let''s plot these two arrays:'
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE580]'
  id: totrans-2287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE580]'
- en: 'The ROC curve hits *1.0* for a small value of x: that is, we retrieve all true
    positives at the cost of relatively few false positives. To visualize the curve
    more accurately, it is instructive to limit the range on the *x*-axis from *0*
    to *0.1*.'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE581]'
  id: totrans-2289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE581]'
- en: 'We also need to tell breeze-viz to use appropriate tick spacing, which requires
    going down to the JFreeChart layer underlying breeze-viz:'
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE582]'
  id: totrans-2291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE582]'
- en: 'We can now save the graph:'
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE583]'
  id: totrans-2293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE583]'
- en: 'This produces the following graph, stored in `roc.png`:'
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation](img/image01218.jpeg)'
  id: totrans-2295
  prefs: []
  type: TYPE_IMG
- en: ROC curve for spam classification with logistic regression. Note that we have
    limited the false positive axis at 0.1
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the graph, we see that we can filter out 85% of spam without a
    single **false positive**. Of course, we would need a larger test set to really
    validate this assumption.
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph is useful to really understand the behavior of a model. Sometimes,
    however, we just want to have a single measure of the quality of a model. The
    area under the ROC curve can be a good such metric:'
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE584]'
  id: totrans-2299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE584]'
- en: 'This can be interpreted as follows: given any two messages randomly drawn from
    the test set, one of which is ham, and one of which is spam, there is a 99.8%
    probability that the model assigned a greater likelihood of spam to the spam message
    than to the ham message.'
  id: totrans-2300
  prefs: []
  type: TYPE_NORMAL
- en: 'Other useful measures of model quality are the precision and recall for particular
    thresholds, or the F1 score. All of these are provided by the `BinaryClassificationMetrics`
    instance. The API documentation lists the methods available: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).'
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in logistic regression
  id: totrans-2302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the dangers of machine learning is over-fitting: the algorithm captures
    not only the signal in the training set, but also the statistical noise that results
    from the finite size of the training set.'
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
- en: 'A way to mitigate over-fitting in logistic regression is to use regularization:
    we impose a penalty for large values of the parameters when optimizing. We can
    do this by adding a penalty to the cost function that is proportional to the magnitude
    of the parameters. Formally, we re-write the logistic regression cost function
    (described in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data
    with Breeze"), *Manipulating Data with Breeze*) as:'
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization in logistic regression](img/image01219.jpeg)'
  id: totrans-2305
  prefs: []
  type: TYPE_IMG
- en: 'where ![Regularization in logistic regression](img/image01220.jpeg) is the
    normal logistic regression cost function:'
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization in logistic regression](img/image01221.jpeg)'
  id: totrans-2307
  prefs: []
  type: TYPE_IMG
- en: 'Here, *params* is the vector of parameters, ![Regularization in logistic regression](img/image01222.jpeg)
    is the vector of features for the *i^(th)* training example, and ![Regularization
    in logistic regression](img/image01223.jpeg) is *1* if the *i* *th* training example
    is spam, and *0* otherwise. This is identical to the logistic regression cost-function
    introduced in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data
    with Breeze"), *Manipulating data with Breeze*, apart from the addition of the
    regularization term ![Regularization in logistic regression](img/image01224.jpeg),
    the ![Regularization in logistic regression](img/image01225.jpeg) norm of the
    parameter vector. The most common value of *n* is 2, in which case ![Regularization
    in logistic regression](img/image01226.jpeg) is just the magnitude of the parameter
    vector:'
  id: totrans-2308
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization in logistic regression](img/image01227.jpeg)'
  id: totrans-2309
  prefs: []
  type: TYPE_IMG
- en: The additional regularization term drives the algorithm to reduce the magnitude
    of the parameter vector. When using regularization, features must all have comparable
    magnitude. This is commonly achieved by normalizing the features. The logistic
    regression estimator provided by MLlib normalizes all features by default. This
    can be turned off with the `setStandardization` parameter.
  id: totrans-2310
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark has two hyperparameters that can be tweaked to control regularization:'
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
- en: The type of regularization, set with the `elasticNetParam` parameter. A value
    of 0 indicates ![Regularization in logistic regression](img/image01228.jpeg) regularization.
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The degree of regularization (![Regularization in logistic regression](img/image01229.jpeg)
    in the cost function), set with the `regParam` parameter. A high value of the
    regularization parameter indicates a strong regularization. In general, the greater
    the danger of over-fitting, the larger the regularization parameter ought to be.
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create a new logistic regression instance that uses regularization:'
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE585]'
  id: totrans-2315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE585]'
- en: To choose the appropriate value of ![Regularization in logistic regression](img/image01229.jpeg),
    we fit the pipeline to the training set and calculate the classification error
    on the test set for several values of ![Regularization in logistic regression](img/image01229.jpeg).
    Further on in the chapter, we will learn about cross-validation in MLlib, which
    provides a much more rigorous way of choosing hyper-parameters.
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE586]'
  id: totrans-2317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE586]'
- en: For our example, we see that any attempt to add L[2] regularization leads to
    a decrease in classification accuracy.
  id: totrans-2318
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation and model selection
  id: totrans-2319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we validated our approach by withholding 30% of the
    data when training, and testing on this subset. This approach is not particularly
    rigorous: the exact result changes depending on the random train-test split. Furthermore,
    if we wanted to test several different hyperparameters (or different models) to
    choose the best one, we would, unwittingly, choose the model that best reflects
    the specific rows in our test set, rather than the population as a whole.'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
- en: This can be overcome with *cross-validation*. We have already encountered cross-validation
    in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and
    Futures"), *Parallel Collections and Futures*. In that chapter, we used random
    subsample cross-validation, where we created the train-test split randomly.
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use **k-fold cross-validation**: we split the training
    set into *k* parts (where, typically, *k* is *10* or *3*) and use *k-1* parts
    as the training set and the last as the test set. The train/test cycle is repeated
    *k* times, keeping a different part as test set each time.'
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is commonly used to choose the best set of hyperparameters
    for a model. To illustrate choosing suitable hyperparameters, we will go back
    to our regularized logistic regression example. Instead of intuiting the hyper-parameters
    ourselves, we will choose the hyper-parameters that give us the best cross-validation
    score.
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore setting both the regularization type (through `elasticNetParam`)
    and the degree of regularization (through `regParam`). A crude, but effective
    way to find good values of the parameters is to perform a grid search: we calculate
    the cross-validation score for every pair of values of the regularization parameters
    of interest.'
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
- en: We can build a grid of parameters using MLlib's `ParamGridBuilder`.
  id: totrans-2325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE587]'
  id: totrans-2326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE587]'
- en: 'To add hyper-parameters over which to optimize to the grid, we use the `addGrid`
    method:'
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE588]'
  id: totrans-2328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE588]'
- en: 'Once all the dimensions are added, we can just call the `build` method on the
    builder to build the grid:'
  id: totrans-2329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE589]'
  id: totrans-2330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE589]'
- en: As we can see, the grid is just a one-dimensional array of sets of parameters
    to pass to the logistic regression model prior to fitting.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step in setting up the cross-validation pipeline is to define a metric
    for comparing model performance. Earlier in the chapter, we saw how to use `BinaryClassificationMetrics`
    to estimate the quality of a model. Unfortunately, the `BinaryClassificationMetrics`
    class is part of the core MLLib API, rather than the new pipeline API, and is
    thus not (easily) compatible. The pipeline API offers a `BinaryClassificationEvaluator`
    class instead. This class works directly on DataFrames, and thus fits perfectly
    into the pipeline API flow:'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE590]'
  id: totrans-2333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE590]'
- en: 'From the parameter list, we see that the `BinaryClassificationEvaluator` class
    supports two metrics: the area under the ROC curve, and the area under the precision-recall
    curve. It expects, as input, a DataFrame containing a `label` column (the model
    truth) and a `rawPrediction` column (the column containing the probability that
    an e-mail is spam or ham).'
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have all the parameters we need to run cross-validation. We first build
    the pipeline, and then pass the pipeline, the evaluator and the array of parameters
    over which to run the cross-validation to an instance of `CrossValidator`:'
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE591]'
  id: totrans-2336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE591]'
- en: 'We will now fit `crossval` to `trainDF`:'
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE592]'
  id: totrans-2338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE592]'
- en: 'This step can take a fairly long time (over an hour on a single machine). This
    creates a transformer, `cvModel`, corresponding to the logistic regression object
    with the parameters that best represent `trainDF`. We can use it to predict the
    classification error on the test DataFrame:'
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE593]'
  id: totrans-2340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE593]'
- en: 'Cross-validation has therefore resulted in a model that performs identically
    to the original, naive logistic regression model with no hyper-parameters. `cvModel`
    also contains a list of the evaluation score for each set of parameter in the
    parameter grid:'
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE594]'
  id: totrans-2342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE594]'
- en: 'The easiest way to relate this to the hyper-parameters is to zip it with `cvModel.getEstimatorParamMaps`.
    This gives us a list of (*hyperparameter values*, *cross-validation score*) pairs:'
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE595]'
  id: totrans-2344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE595]'
- en: The best set of hyper-parameters correspond to L[2] regularization with a regularization
    parameter of `1E-10`, though this only corresponds to a tiny improvement in AUC.
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes our spam filter example. We have successfully trained a spam
    filter for this particular Ling-Spam dataset. To obtain better results, one could
    experiment with better feature extraction: we could remove stop words or use TF-IDF
    vectors, rather than just term frequency vectors as features, and we could add
    additional features like the length of messages, or even *n-grams*. We could also
    experiment with non-linear algorithms, such as random forest. All of these steps
    would be straightforward to add to the pipeline.'
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
- en: Beyond logistic regression
  id: totrans-2347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have concentrated on logistic regression in this chapter, but MLlib offers
    many alternative algorithms that will capture non-linearity in the data more effectively.
    The consistency of the pipeline API makes it easy to try out different algorithms
    and see how they perform. The pipeline API offers decision trees, random forest
    and gradient boosted trees for classification, as well as a simple feed-forward
    neural network, which is still experimental. It offers lasso and ridge regression
    and decision trees for regression, as well as PCA for dimensionality reduction.
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
- en: The lower level MLlib API also offers principal component analysis for dimensionality
    reduction, several clustering methods including *k*-means and latent Dirichlet
    allocation and recommender systems using alternating least squares.
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-2350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib tackles the challenge of devising scalable machine learning algorithms
    head-on. In this chapter, we used it to train a simple scalable spam filter. MLlib
    is a vast, rapidly evolving library. The best way to learn more about what it
    can offer is to try and port code that you might have written using another library
    (such as scikit-learn).
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to build web APIs and interactive visualizations
    to share our results with the rest of the world.
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-2353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best reference is the online documentation, including:'
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline API: [http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)'
  id: totrans-2355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full list of transformers: [http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines](http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines)'
  id: totrans-2356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced Analytics with Spark*, by *Sandy Ryza*, *Uri Laserson*, *Sean Owen*
    and *Josh Wills* provides a detailed and up-to-date introduction to machine learning
    with Spark.'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
- en: There are several books that introduce machine learning in more detail than
    we can here. We have mentioned *The Elements of Statistical Learning*, by *Friedman*,
    *Tibshirani* and *Hastie* several times in this book. It is one of the most complete
    introductions to the mathematical underpinnings of machine learning currently
    available.
  id: totrans-2358
  prefs: []
  type: TYPE_NORMAL
- en: Andrew Ng's Machine Learning course on [https://www.coursera.org/](https://www.coursera.org/)
    provides a good introduction to machine learning. It uses Octave/MATLAB as the
    programming language, but should be straightforward to adapt to Breeze and Scala.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 13. Web APIs with Play
  id: totrans-2360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first 12 chapters of this book, we introduced basic tools and libraries
    for anyone wanting to build data science applications: we learned how to interact
    with SQL and MongoDB databases, how to build fast batch processing applications
    using Spark, how to apply state-of-the-art machine learning algorithms using MLlib,
    and how to build modular concurrent applications in Akka.'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last chapters of this book, we will branch out to look at a web framework:
    *Play*. You might wonder why a web framework would feature in a data science book;
    surely such topics are best left to software engineers or web developers. Data
    scientists, however, rarely exist in a vacuum. They often need to communicate
    results or insights to stakeholders. As compelling as an ROC curve may be to someone
    well versed in statistics, it may not carry as much weight with less technical
    people. Indeed, it can be much easier to sell insights when they are accompanied
    by an engaging visualization.'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
- en: Many modern interactive data visualization applications are web applications
    running in a web browser. Often, these involve **D3.js**, a JavaScript library
    for building data-driven web pages. In this chapter and the next, we will look
    at integrating D3 with Scala.
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing a web application is a complex endeavor. We will split this task over
    this chapter and the next. In this chapter, we will learn how to write a REST
    API that we can use as backend for our application, or query in its own right.
    In the next chapter, we will look at integrating front-end code with Play to query
    the API exposed by the backend and display it using D3\. We assume at least a
    basic familiarity with HTTP in this chapter: you should have read [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, at least.'
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
- en: Many data scientists or aspiring data scientists are unlikely to be familiar
    with the inner workings of web technologies. Learning how to build complex websites
    or web APIs can be daunting. This chapter therefore starts with a general discussion
    of dynamic websites and the architecture of web applications. If you are already
    familiar with server-side programming and with web frameworks, you can easily
    skip over the first few sections.
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
- en: Client-server applications
  id: totrans-2366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A website works through the interaction between two computers: the client and
    the server. If you enter the URL [www.github.com/pbugnion/s4ds/graphs](http://www.github.com/pbugnion/s4ds/graphs)
    in a web browser, your browser queries one of the GitHub servers. The server will
    look though its database for information concerning the repository that you are
    interested in. It will serve this information as HTML, CSS, and JavaScript to
    your computer. Your browser is then responsible for interpreting this response
    in the correct way.'
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the URL in question, you will notice that there are several graphs
    on that page. Unplug your internet connection and you can still interact with
    the graphs. All the information necessary for interacting with the graphs was
    transferred, as JavaScript, when you loaded that webpage. When you play with the
    graphs, the CPU cycles necessary to make those changes happen are spent on *your*
    computer, not a GitHub server. The code is executed *client-side*. Conversely,
    when you request information about a new repository, that request is handled by
    a GitHub server. It is said to be handled *server-side*.
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
- en: 'A web framework like Play can be used on the server. For client-side code,
    we can only use a language that the client browser will understand: HTML for the
    layout, CSS for the styling and JavaScript, or languages that can compile to JavaScript,
    for the logic.'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to web frameworks
  id: totrans-2370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is a brief introduction to how modern web applications are designed.
    Go ahead and skip it if you already feel comfortable writing backend code.
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
- en: Loosely, a web framework is a set of tools and code libraries for building web
    applications. To understand what a web framework provides, let's take a step back
    and think about what you would need to do if you did not have one.
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
- en: 'You want to write a program that listens on port 80 and sends HTML (or JSON
    or XML) back to clients that request it. This is simple if you are serving the
    same file back to every client: just load the HTML from file when you start the
    server, and send it to clients who request it.'
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good. But what if you now want to customize the HTML based on the
    client request? You might choose to respond differently based on part of the URL
    that the client put in his browser, or based on specific elements in the HTTP
    request. For instance, the product page on [amazon.com](http://amazon.com) is
    different to the payment page. You need to write code to parse the URL and the
    request, and then route the request to the relevant handler.
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
- en: You might now want to customize the HTML returned dynamically, based on specific
    elements of the request. The page for every product on [amazon.com](http://amazon.com)
    follows the same outline, but specific elements are different. It would be wasteful
    to store the entire HTML content for every product. A better way is to store the
    details for each product in a database and inject them into an HTML template when
    a client requests information on that product. You can do this with a *template
    processor*. Of course, writing a good template processor is difficult.
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
- en: You might deploy your web framework and realize that it cannot handle the traffic
    directed to it. You decide that handlers responding to client requests should
    run asynchronously. You now have to deal with concurrency.
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
- en: A web framework essentially provides the wires to bind everything together.
    Besides bundling an HTTP server, most frameworks will have a router that automatically
    routes a request, based on the URL, to the correct handler. In most cases, the
    handler will run asynchronously, giving you much better scalability. Many frameworks
    have a template processor that lets you write HTML (or sometimes JSON or XML)
    templates intuitively. Some web frameworks also provide functionality for accessing
    a database, for parsing JSON or XML, for formulating HTTP requests and for localization
    and internationalization.
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
- en: Model-View-Controller architecture
  id: totrans-2378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many web frameworks impose program architectures: it is difficult to provide
    wires to bind disparate components together without making some assumptions about
    what those components are. The **Model-View-Controller** (**MVC**) architecture
    is particularly popular on the Web, and it is the architecture the Play framework
    assumes. Let''s look at each component in turn:'
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
- en: The model is the data underlying the application. For example, I expect the
    application underlying GitHub has models for users, repositories, organizations,
    pull requests and so on. In the Play framework, a model is often an instance of
    a case class. The core responsibility of the model is to remember the current
    state of the application.
  id: totrans-2380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Views are representations of a model or a set of models on the screen.
  id: totrans-2381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The controller handles client interactions, possibly changing the model. For
    instance, if you *star* a project on GitHub, the controller will update the relevant
    models. Controllers normally carry very little application state: remembering
    things is the job of the models.![Model-View-Controller architecture](img/image01230.jpeg)'
  id: totrans-2382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MVC architecture: the state of the application is provided by the model. The
    view provides a visual representation of the model to the user, and the controller
    handles logic: what to do when the user presses a button or submits a form.'
  id: totrans-2383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The MVC framework works well because it decouples the user interface from the
    underlying data and structures the flow of actions: a controller can update the
    model state or the view, a model can send signals to the view to tell it to update,
    and the view merely displays that information. The model carries no information
    related to the user interface. This separation of concerns results in an easier
    mental model of information flow, better encapsulation and greater testability.'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
- en: Single page applications
  id: totrans-2385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The client-server duality adds a degree of complication to the elegant MVC architecture.
    Where should the model reside? What about the controller? Traditionally, the model
    and the controller ran almost entirely on the server, which just pushed the relevant
    HTML view to the client.
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
- en: 'The growth in client-side JavaScript frameworks, such AngularJS, has resulted
    in a gradual shift to putting more code in the client. Both the controller and
    a temporary version of the model typically run client-side. The server just functions
    as a web API: if, for instance, the user updates the model, the controller will
    send an HTTP request to the server informing it of the change.'
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
- en: 'It then makes sense to think of the program running server-side and the one
    running client-side as two separate applications: the server persists data in
    databases, for instance, and provides a programmatic interface to this data, usually
    as a web service returning JSON or XML data. The client-side program maintains
    its own model and controller, and polls the server whenever it needs a new model,
    or whenever it needs to inform the server that the persistent view of the model
    should be changed.'
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
- en: Taken to the extreme, this results in **Single-Page Applications**. In a single-page
    application, the first time the client requests a page from the server, he receives
    the HTML and the JavaScript necessary to build the framework for the entire application.
    If the client needs further data from the server, he will poll the server's API.
    This data is returned as JSON or XML.
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
- en: 'This might seem a little complicated in the abstract, so let''s think how the
    Amazon website might be structured as a single-page application. We will just
    concern ourselves with the products page here, since that''s complicated enough.
    Let''s imagine that you are on the home page, and you hit a link for a particular
    product. The application running on your computer knows how to display products,
    for instance through an HTML template. The JavaScript also has a prototype for
    the model, such as:'
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE596]'
  id: totrans-2391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE596]'
- en: 'What it''s currently missing is knowledge of what data to put in those fields
    for the product you have just selected: there is no way that information could
    have been sent to your computer when the website loaded, since there was no way
    to know what product you might click on (and sending information about every product
    would be prohibitively costly). So the Amazon client sends a request to the server
    for information on that product. The Amazon server replies with a JSON object
    (or maybe XML). The client then updates its model with that information. When
    the update is complete, an event is fired to update the view:'
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
- en: '![Single page applications](img/image01231.jpeg)'
  id: totrans-2393
  prefs: []
  type: TYPE_IMG
- en: 'Client-server communications in a single-page application: when the client
    first accesses the website, it receives HTML, CSS and JavaScript files that contain
    the entire logic for the application. From then on, the client only uses the server
    as an API when it requests additional data. The application running in the user''s
    web browser and the one running on the server are nearly independent. The only
    coupling is through the structure of the API exposed by the server.'
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
- en: Building an application
  id: totrans-2395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter and the next, we will build a single-page application that
    relies on an API written in Play. We will build a webpage that looks like this:'
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an application](img/image01232.jpeg)'
  id: totrans-2397
  prefs: []
  type: TYPE_IMG
- en: The user enters the name of someone on GitHub and can view a list of their repositories
    and a chart summarizing what language they use. You can find the application deployed
    at `app.scala4datascience.com`. Go ahead and give it a whirl.
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a glimpse of the innards, type `app.scala4datascience.com/api/repos/odersky`.
    This returns a JSON object like:'
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE597]'
  id: totrans-2400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE597]'
- en: We will build the API in this chapter, and write the front-end code in the next
    chapter.
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
- en: The Play framework
  id: totrans-2402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Play framework is a web framework built on top of Akka. It has a proven
    track record in industry, and is thus a reliable choice for building scalable
    web applications.
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
- en: 'Play is an *opinionated* web framework: it expects you to follow the MVC architecture,
    and it has a strong opinion about the tools you should be using. It comes bundled
    with its own JSON and XML parsers, with its own tools for accessing external APIs,
    and with recommendations for how to access databases.'
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
- en: 'Web applications are much more complex than the command line scripts we have
    been developing in this book, because there are many more components: the backend
    code, routing information, HTML templates, JavaScript files, images, and so on.
    The Play framework makes strong assumptions about the directory structure for
    your project. Building that structure from scratch is both mind-numbingly boring
    and easy to get wrong. Fortunately, we can use **Typesafe activators** to bootstrap
    the project (you can also download the code from the Git repository in [https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)
    but I encourage you to start the project from a basic activator structure and
    code along instead, using the finished version as an example).'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
- en: 'Typesafe activator is a custom version of SBT that includes templates to get
    Scala programmers up and running quickly. To install activator, you can either
    download a JAR from [https://www.typesafe.com/activator/download](https://www.typesafe.com/activator/download),
    or, on Mac OS, via homebrew:'
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE598]'
  id: totrans-2407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE598]'
- en: 'You can then launch the activator console from the terminal. If you downloaded
    activator:'
  id: totrans-2408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE599]'
  id: totrans-2409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE599]'
- en: 'Or, if you installed via Homebrew:'
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE600]'
  id: totrans-2411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE600]'
- en: This starts a new project in the current directory. It starts by asking what
    template you want to start with. Choose `play-scala`. It then asks for a name
    for your application. I chose `ghub-display`, but go ahead and be creative!
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the newly created project structure (I have only retained the
    most important files):'
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE601]'
  id: totrans-2414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE601]'
- en: 'Let''s run the app:'
  id: totrans-2415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE602]'
  id: totrans-2416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE602]'
- en: Head over to your browser and navigate to the URL `127.0.0.1:9000/`. The page
    may take a few seconds to load. Once it is loaded, you should see a default page
    that says **Your application is ready**.
  id: totrans-2417
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we modify anything, let''s walk through how this happens. When you ask
    your browser to take you to `127.0.0.1:9000/`, your browser sends an HTTP request
    to the server listening at that address (in this case, the Netty server bundled
    with Play). The request is a GET request for the route `/`. The Play framework
    looks in `conf/routes` to see if it has a route satisfying `/`:'
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE603]'
  id: totrans-2419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE603]'
- en: 'We see that the `conf/routes` file does contain the route `/` for GET requests.
    The second part of that line, `controllers.Application.index`, is the name of
    a Scala function to handle that route (more on that in a moment). Let''s experiment.
    Change the route end-point to `/hello`. Refresh your browser without changing
    the URL. This will trigger recompilation of the application. You should now see
    an error page:'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
- en: '![The Play framework](img/image01233.jpeg)'
  id: totrans-2421
  prefs: []
  type: TYPE_IMG
- en: The error page tells you that the app does not have an action for the route
    `/` any more. If you navigate to `127.0.0.1:9000/hello`, you should see the landing
    page again.
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides learning a little of how routing works, we have also learned two things
    about developing Play applications:'
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
- en: In development mode, code gets recompiled when you refresh your browser and
    there have been code changes
  id: totrans-2424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compilation and runtime errors get propagated to the web page
  id: totrans-2425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's change the route back to `/`. There is a lot more to say on routing, but
    it can wait till we start building our application.
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
- en: 'The `conf/routes` file tells the Play framework to use the method `controllers.Application.index`
    to handle requests to `/`. Let''s look at the `Application.scala` file in `app/controllers`,
    where the `index` method is defined:'
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE604]'
  id: totrans-2428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE604]'
- en: 'We see that `controllers.Application.index` refers to the method `index` in
    the class `Application`. This method has return type `Action`. An `Action` is
    just a function that maps HTTP requests to responses. Before explaining this in
    more detail, let''s change the action to:'
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE605]'
  id: totrans-2430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE605]'
- en: Refresh your browser and you should see the landing page replaced with `"hello
    world"`. By having our action return `Ok("hello, world")`, we are asking Play
    to return an HTTP response with status code 200 (indicating that the request was
    successful) and the body `"hello world"`.
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to the original content of `index`:'
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE606]'
  id: totrans-2433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE606]'
- en: 'We can see that this calls the method `views.html.index`. This might appear
    strange, because there is no `views` package anywhere. However, if you look at
    the `app/views` directory, you will notice two files: `index.scala.html` and `main.scala.html`.
    These are templates, which, at compile time, get transformed into Scala functions.
    Let''s have a look at `main.scala.html`:'
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE607]'
  id: totrans-2435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE607]'
- en: 'At compile time, this template is compiled to a function `main(title:String)(content:Html)`
    in the package `views.html`. Notice that the function package and name comes from
    the template file name, and the function arguments come from the first line of
    the template. The template contains embedded `@title` and `@content` values, which
    get filled in by the arguments to the function. Let''s experiment with this in
    a Scala console:'
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE608]'
  id: totrans-2437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE608]'
- en: We can call `views.html.main`, just like we would call a normal Scala function.
    The arguments we pass in get embedded in the correct place, as defined by the
    template in `views/main.scala.html`.
  id: totrans-2438
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our introductory tour of Play. Let''s briefly go over what we
    have learnt: when a request reaches the Play server, the server reads the URL
    and the HTTP verb and checks that these exist in its `conf/routes` file. It will
    then pass the request to the `Action` defined by the controller for that route.
    This `Action` returns an HTTP response that gets fed back to the browser. In constructing
    the response, the `Action` may make use of a template, which, as far as it is
    concerned is just a function `(arguments list) => String` or `(arguments list)
    => HTML`.'
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing
  id: totrans-2440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Routing, as we saw, is the mapping of HTTP requests to Scala handlers. Routes
    are stored in `conf/routes`. A route is defined by an HTTP verb, followed by the
    end-point, followed by a Scala function:'
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE609]'
  id: totrans-2442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE609]'
- en: 'We learnt to add new routes by just adding lines to the `routes` file. We are
    not limited to static routes, however. The Play framework lets us include wild
    cards in routes. The value of the wild card can be passed as an argument to the
    controller. To see how this works, let''s create a controller that takes the name
    of a person as argument. In the `Application` object in `app.controllers`, add:'
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE610]'
  id: totrans-2444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE610]'
- en: 'We can now define a route handled by this controller:'
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE611]'
  id: totrans-2446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE611]'
- en: If you now point your browser to `127.0.0.1:9000/hello/Jim`, you will see **hello,
    Jim** appear on the screen.
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
- en: 'Any string between `:` and the following `/` is treated as a wild card: it
    will match any combination of characters. The value of the wild card can be passed
    to the controller. Note that the wild card can appear anywhere in the URL, and
    there can be more than one wild card. The following are all valid route definitions,
    for instance:'
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE612]'
  id: totrans-2449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE612]'
- en: 'There are many other options for selecting routes and passing arguments to
    the controller. Consult the documentation for the Play framework for a full discussion
    on the routing possibilities: [https://www.playframework.com/documentation/2.4.x/ScalaRouting](https://www.playframework.com/documentation/2.4.x/ScalaRouting).'
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-2451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**URL design**'
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
- en: 'It is generally considered best practice to leave the URL as simple as possible.
    The URL should reflect the hierarchical structure of the information of the website,
    rather than the underlying implementation. GitHub is a very good example of this:
    its URLs make intuitive sense. For instance, the URL for the repository for this
    book is:'
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)'
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
- en: To access the issues page for that repository, add `/issues` to the route. To
    access the first issue, add `/1` to that route. These are called **semantic URLs**
    ([https://en.wikipedia.org/wiki/Semantic_URL](https://en.wikipedia.org/wiki/Semantic_URL)).
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  id: totrans-2456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have talked about routes, and how to pass parameters to controllers. Let's
    now talk about what we can do with the controller.
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
- en: The method defined in the route must return a `play.api.mvc.Action` instance.
    The `Action` type is a thin wrapper around the type `Request[A] => Result`, where
    `Request[A]` identifies an HTTP request and `Result` is an HTTP response.
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
- en: Composing the response
  id: totrans-2459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An HTTP response, as we saw in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web
    APIs"), *Web APIs*, is composed of:'
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
- en: the status code (such as 200 for a successful response, or 404 for a missing
    page)
  id: totrans-2461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the response headers, a key-value list indicating metadata related to the response
  id: totrans-2462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response body. This can be HTML for web pages, or JSON, XML or plain text
    (or many other formats). This is generally the bit that we are really interested
    in.
  id: totrans-2463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Play framework defines a `play.api.mvc.Result` object that symbolizes a
    response. The object contains a `header` attribute with the status code and the
    headers, and a `body` attribute containing the body.
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to generate a `Result` is to use one of the factory methods
    in `play.api.mvc.Results`. We have already seen the `Ok` method, which generates
    a response with status code 200:'
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE613]'
  id: totrans-2466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE613]'
- en: 'Let''s take a step back and open a Scala console so we can understand how this
    works:'
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE614]'
  id: totrans-2468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE614]'
- en: 'We can see how the `Results.Ok(...)` creates a `Result` object with status
    `200` and (in this case), a single header denoting the content type. The body
    is a bit more complicated: it is an enumerator that can be pushed onto the output
    stream when needed. The enumerator contains the argument passed to `Ok`: `"hello,
    world"`, in this case.'
  id: totrans-2469
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many factory methods in `Results` for returning different status
    codes. Some of the more relevant ones are:'
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
- en: '`Action { Results.NotFound }`'
  id: totrans-2471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Action { Results.BadRequest("bad request") }`'
  id: totrans-2472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Action { Results.InternalServerError("error") }`'
  id: totrans-2473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Action { Results.Forbidden }`'
  id: totrans-2474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Action { Results.Redirect("/home") }`'
  id: totrans-2475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a full list of `Result` factories, consult the API documentation for Results
    ([https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results](https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results)).
  id: totrans-2476
  prefs: []
  type: TYPE_NORMAL
- en: 'We have, so far, been limiting ourselves to passing strings as the content
    of the `Ok` result: `Ok("hello, world")`. We are not, however, limited to passing
    strings. We can pass a JSON object:'
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE615]'
  id: totrans-2478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE615]'
- en: 'We will cover interacting with JSON in more detail when we start building the
    API. We can also pass HTML as the content. This is most commonly the case when
    returning a view:'
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE616]'
  id: totrans-2480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE616]'
- en: Note how the `Content-Type` header is set based on the type of content passed
    to `Ok`. The `Ok` factory uses the `Writeable` type class to convert its argument
    to the body of the response. Thus, any content type for which a `Writeable` type
    class exists can be used as argument to `Ok`. If you are unfamiliar with type
    classes, you might want to read the *Looser coupling with type classes* section
    in [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*.
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and parsing the request
  id: totrans-2482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now know how to formulate (basic) responses. The other half of the equation
    is the HTTP request. Recall that an `Action` is just a function mapping `Request
    => Result`. We can access the request using:'
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE617]'
  id: totrans-2484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE617]'
- en: 'One of the reasons for needing a reference to the request is to access parameters
    in the query string. Let''s modify the `Hello, <name>` example that we wrote earlier
    to, optionally, include a title in the query string. Thus, a URL could be formatted
    as `/hello/Jim?title=Dr`. The `request` instance exposes the `getQueryString`
    method for accessing specific keys in the query string. This method returns `Some[String]`
    if the key is present in the query, or `None` otherwise. We can re-write our `hello`
    controller as:'
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE618]'
  id: totrans-2486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE618]'
- en: Try this out by accessing the URL `127.0.0.1:9000/hello/Odersky?title=Dr` in
    your browser. The browser should display `Hello, Dr Odersky`.
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
- en: 'We have, so far, been concentrating on GET requests. These do not have a body.
    Other types of HTTP request, most commonly POST requests, do contain a body. Play
    lets the user pass *body parsers* when defining the action. The request body will
    be passed through the body parser, which will convert it from a byte stream to
    a Scala type. As a very simple example, let''s define a new route that accepts
    POST requests:'
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE619]'
  id: totrans-2489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE619]'
- en: 'We will apply the predefined `parse.text` body parser to the incoming request
    body. This converts the body of the request to a string. The `helloPost` controller
    looks like:'
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE620]'
  id: totrans-2491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE620]'
- en: Tip
  id: totrans-2492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You cannot test POST requests easily in the browser. You can use cURL instead.
    cURL is a command line utility for dispatching HTTP requests. It is installed
    by default on Mac OS and should be available via the package manager on Linux
    distributions. The following will send a POST request with `"I think that Scala
    is great"` in the body:'
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE621]'
  id: totrans-2494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE621]'
- en: 'This prints the following line to the terminal:'
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
- en: '`Hello. You told me: I think that Scala is great`'
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of built-in body parsers:'
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
- en: '`parse.file(new File("filename.txt"))` will save the body to a file.'
  id: totrans-2498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse.json` will parse the body as JSON (we will learn more about interacting
    with JSON in the next section).'
  id: totrans-2499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse.xml` will parse the body as XML.'
  id: totrans-2500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse.urlFormEncoded` will parse the body as returned by submitting an HTML
    form. The `request.body` attribute is a Scala map from `String` to `Seq[String]`,
    mapping each form element to its value(s).'
  id: totrans-2501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a full list of body parsers, the best source is the Scala API documentation
    for `play.api.mvc.BodyParsers.parse` available at: [https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers$parse$](https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers%24parse%24).'
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with JSON
  id: totrans-2503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JSON, as we discovered in previous chapters, is becoming the de-facto language
    for communicating structured data over HTTP. If you develop a web application
    or a web API, it is likely that you will have to consume or emit JSON, or both.
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*,
    we learned how to parse JSON through `json4s`. The Play framework includes its
    own JSON parser and emitter. Fortunately, it behaves in much the same way as `json4s`.
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine that we are building an API that summarizes information about
    GitHub repositories. Our API will emit a JSON array listing a user's repositories
    when queried about a specific user (much like the GitHub API, but with just a
    subset of fields).
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining a model for the repository. In Play applications,
    models are normally stored in the folder `app/models`, in the `models` package:'
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE622]'
  id: totrans-2508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE622]'
- en: 'Let''s add a route to our application that serves arrays of repos for a particular
    user. In `conf/routes`, add the following line:'
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE623]'
  id: totrans-2510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE623]'
- en: 'Let''s now implement the framework for the controller. We will create a new
    controller for our API, imaginatively called `Api`. For now, we will just have
    the controller return dummy data. This is what the code looks like (we will explain
    the details shortly):'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE624]'
  id: totrans-2512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE624]'
- en: 'If you point your web browser to `127.0.0.1:9000/api/repos/odersky`, you should
    now see the following JSON object:'
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE625]'
  id: totrans-2514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE625]'
- en: The only tricky part of this code is the conversion from `Repo` to JSON. We
    call `Json.toJson` on `data`, an instance of type `List[Repo]`. The `toJson` method
    relies on the existence of a type class `Writes[T]` for the type `T` passed to
    it.
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
- en: 'The Play framework makes extensive use of type classes to define how to convert
    models to specific formats. Recall that we learnt how to write type classes in
    the context of SQL and MongoDB. The Play framework''s expectations are very similar:
    for the `Json.toJson` method to work on an instance of type `Repo`, there must
    be a `Writes[Repo]` implementation available that specifies how to transform `Repo`
    objects to JSON.'
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Play framework, the `Writes[T]` type class defines a single method:'
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE626]'
  id: totrans-2518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE626]'
- en: '`Writes` methods for built-in simple types and for collections are already
    built into the Play framework, so we do not need to worry about defining `Writes[Boolean]`,
    for instance.'
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
- en: The `Writes[Repo]` instance is commonly defined either directly in the controller,
    if it is just used for that controller, or in the `Repo` companion object, where
    it can be used across several controllers. For simplicity, we just embedded it
    in the controller.
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
- en: Note how type-classes allow for separation of concerns. The model just defines
    the `Repo` type, without attaching any behavior. The `Writes[Repo]` type class
    just knows how to convert from a `Repo` instance to JSON, but knows nothing of
    the context in which it is used. Finally, the controller just knows how to create
    a JSON HTTP response.
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you have just defined a web API that returns JSON! In the next
    section, we will learn how to fetch data from the GitHub web API to avoid constantly
    returning the same array.
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
- en: Querying external APIs and consuming JSON
  id: totrans-2523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learnt how to provide the user with a dummy JSON array of repositories
    in response to a request to `/api/repos/:username`. In this section, we will replace
    the dummy data with the user's actual repositories, dowloaded from GitHub.
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*,
    we learned how to query the GitHub API using Scala's `Source.fromURL` method and
    `scalaj-http`. It should come as no surprise that the Play framework implements
    its own library for interacting with external web services.
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s edit the `Api` controller to fetch information about a user''s repositories
    from GitHub, rather than using dummy data. When called with a username as argument,
    the controller will:'
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
- en: Send a GET request to the GitHub API for that user's repositories.
  id: totrans-2527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the response, converting the body from a JSON object to a `List[Repo]`.
  id: totrans-2528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert from the `List[Repo]` to a JSON array, forming the response.
  id: totrans-2529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We start by giving the full code listing before explaining the thornier parts
    in detail:'
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE627]'
  id: totrans-2531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE627]'
- en: 'If you have written all this, point your browser to, for instance, `127.0.0.1:9000/api/repos/odersky`
    to see the list of repositories owned by Martin Odersky:'
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE628]'
  id: totrans-2533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE628]'
- en: This code sample is a lot to take in, so let's break it down.
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
- en: Calling external web services
  id: totrans-2535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in querying external APIs is to import the `WS` object, which
    defines factory methods for creating HTTP requests. These factory methods rely
    on a reference to an implicit Play application in the namespace. The easiest way
    to ensure this is the case is to import `play.api.Play.current`, a reference to
    the current application.
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s ignore the `readsRepoFromGithub` type class for now and jump straight
    to the controller body. The URL that we want to hit with a GET request is `"https://api.github.com/users/$username/repos"`,
    with the appropriate value for `$username`. We create a GET request with `WS.url(url).get()`.
    We can also add headers to an existing request. For instance, to specify the content
    type, we could have written:'
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE629]'
  id: totrans-2538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE629]'
- en: 'We can use headers to pass a GitHub OAuth token using:'
  id: totrans-2539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE630]'
  id: totrans-2540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE630]'
- en: To formulate a POST request, rather than a GET request, replace the final `.get()`
    with `.post(data)`. Here, `data` can be JSON, XML or a string.
  id: totrans-2541
  prefs: []
  type: TYPE_NORMAL
- en: Adding `.get` or `.post` fires the request, returning a `Future[WSResponse]`.
    You should, by now, be familiar with futures. By writing `response.map { r =>
    ... }`, we specify a transformation to be executed on the future result, when
    it returns. The transformation verifies the response's status, returning `NotFound`
    if the status code of the response is anything but 200.
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
- en: Parsing JSON
  id: totrans-2543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the status code is 200, the callback parses the response body to JSON and
    converts the parsed JSON to a `List[Repo]` instance. We already know how to convert
    from a `Repo` object to JSON using the `Writes[Repo]` type class. The converse,
    going from JSON to a `Repo` object, is a little more challenging, because we have
    to account for incorrectly formatted JSON. To this effect, the Play framework
    provides the `.validate[T]` method on JSON objects. This method tries to convert
    the JSON to an instance of type `T`, returning `JsSuccess` if the JSON is well-formatted,
    or `JsError` otherwise (similar to Scala''s `Try` object). The `.validate` method
    relies on the existence of a type class `Reads[Repo]`. Let''s experiment with
    a Scala console:'
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE631]'
  id: totrans-2545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE631]'
- en: 'Using `Json.parse` converts a string to an instance of `JsValue`, the super-type
    for JSON instances. We can access specific fields in `parsedJson` using XPath-like
    syntax (if you are not familiar with XPath-like syntax, you might want to read
    [Chapter 6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick – A Functional Interface
    for SQL"), *Slick – A Functional Interface for SQL*):'
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE632]'
  id: totrans-2547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE632]'
- en: 'XPath-like lookups return an instance with type `JsLookupResult`. This takes
    two values: either `JsDefined`, if the path is valid, or `JsUndefined` if it is
    not:'
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE633]'
  id: totrans-2549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE633]'
- en: 'To go from a `JsLookupResult` instance to a String in a type-safe way, we can
    use the `.validate[String]` method:'
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE634]'
  id: totrans-2551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE634]'
- en: 'The `.validate[T]` method returns either `JsSuccess` if the `JsDefined` instance
    could be successfully cast to `T`, or `JsError` otherwise. To illustrate the latter,
    let''s try validating this as an `Int`:'
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE635]'
  id: totrans-2553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE635]'
- en: 'Calling `.validate` on an instance of type `JsUndefined` also returns in a
    `JsError`:'
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE636]'
  id: totrans-2555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE636]'
- en: 'To convert from an instance of `JsResult[T]` to an instance of type `T`, we
    can use pattern matching:'
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE637]'
  id: totrans-2557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE637]'
- en: We can now use `.validate` to cast JSON to simple types in a type-safe manner.
    But, in the code example, we used `.validate[Repo]`. This works provided a `Reads[Repo]`
    type class is implicitly available in the namespace.
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common way of defining `Reads[T]` type classes is through a DSL provided
    in `import play.api.libs.functional.syntax._`. The DSL works by chaining operations
    returning either `JsSuccess` or `JsError` together. Discussing exactly how this
    DSL works is outside the scope of this chapter (see, for instance, the Play framework
    documentation page on JSON combinators: [https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators](https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators)).
    We will stick to discussing the syntax.'
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE638]'
  id: totrans-2560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE638]'
- en: 'The `Reads` type class is defined in two stages. The first chains together
    `read[T]` methods with `and`, combining successes and errors. The second uses
    the apply method of the companion object of a case class (or `Tuple` instance)
    to construct the object, provided the first stage completed successfully. Now
    that we have defined the type class, we can call `validate[Repo]` on a `JsValue`
    object:'
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE639]'
  id: totrans-2562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE639]'
- en: 'We can then use pattern matching to extract the `Repo` object from the `JsSuccess`
    instance:'
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE640]'
  id: totrans-2564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE640]'
- en: We have, so far, only talked about validating single repos. The Play framework
    defines type classes for collection types, so, provided `Reads[Repo]` is defined,
    `Reads[List[Repo]]` will also be defined.
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to extract Scala objects from JSON, let's get back
    to the code. If we manage to successfully convert the repositories to a `List[Repo]`,
    we emit it again as JSON. Of course, converting from GitHub's JSON representation
    of a repository to a Scala object, and from that Scala object directly to our
    JSON representation of the object, might seem convoluted. However, if this were
    a real application, we would have additional logic. We could, for instance, store
    repos in a cache, and try and fetch from that cache instead of querying the GitHub
    API. Converting from JSON to Scala objects as early as possible decouples the
    code that we write from the way GitHub returns repositories.
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous actions
  id: totrans-2567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last bit of the code sample that is new is the call to `Action.async`, rather
    than just `Action`. Recall that an `Action` instance is a thin wrapper around
    a `Request => Result` method. Our code, however, returns a `Future[Result]`, rather
    than a `Result`. When that is the case, use the `Action.async` to construct the
    action, rather than `Action` directly. Using `Action.async` tells the Play framework
    that the code creating the `Action` is asynchronous.
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating APIs with Play: a summary'
  id: totrans-2569
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we deployed an API that responds to GET requests. Since
    this is a lot to take in, let''s summarize how to go about API creation:'
  id: totrans-2570
  prefs: []
  type: TYPE_NORMAL
- en: Define appropriate routes in `/conf/routes`, using wildcards in the URL as needed.
  id: totrans-2571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create Scala case classes in `/app/models` to represent the models used by the
    API.
  id: totrans-2572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create `Write[T]` methods to write models to JSON or XML so that they can be
    returned by the API.
  id: totrans-2573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bind the routes to controllers. If the controllers need to do more than a trivial
    amount a work, wrap the work in a future to avoid blocking the server.
  id: totrans-2574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many more useful components of the Play framework that you are likely
    to need, such as, for instance, how to use Slick to access SQL databases. We do
    not, unfortunately, have time to cover these in this introduction. The Play framework
    has extensive, well-written documentation that will fill the gaping holes in this
    tutorial.
  id: totrans-2575
  prefs: []
  type: TYPE_NORMAL
- en: 'Rest APIs: best practice'
  id: totrans-2576
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the Internet matures, REST (representational state transfer) APIs are emerging
    as the most reliable design pattern for web APIs. An API is described as *RESTful*
    if it follows these guiding principles:'
  id: totrans-2577
  prefs: []
  type: TYPE_NORMAL
- en: The API is designed as a set of resources. For instance, the GitHub API provides
    information about users, repositories, followers, etc. Each user, or repository,
    is a specific resource. Each resource can be addressed through a different HTTP
    end-point.
  id: totrans-2578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The URLs should be simple and should identify the resource clearly. For instance,
    `api.github.com/users/odersky` is simple and tells us clearly that we should expect
    information about the user Martin Odersky.
  id: totrans-2579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no *world resource* that contains all the information about the system.
    Instead, top-level resources contain links to more specialized resources. For
    instance, the user resource in the GitHub API contains links to that user's repositories
    and that user's followers, rather than having all that information embedded in
    the user resource directly.
  id: totrans-2580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API should be discoverable. The response to a request for a specific resource
    should contain URLs for related resources. When you query the user resource on
    GitHub, the response contains the URL for accessing that user's followers, repositories
    etc. The client should use the URLs provided by the API, rather than attempting
    to construct them client-side. This makes the client less brittle to changes in
    the API.
  id: totrans-2581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There should be as little state maintained on the server as possible. For instance,
    when querying the GitHub API, we must pass the authentication token with every
    request, rather than expecting our authentication status to be *remembered* on
    the server. Having each interaction be independent of the history provides much
    better scalability: if any interaction can be handled by any server, load balancing
    is much easier.'
  id: totrans-2582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-2583
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the Play framework as a tool for building web
    APIs. We built an API that returns a JSON array of a user's GitHub repositories.
    In the next chapter, we will build on this API and construct a single-page application
    to represent this data graphically.
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-2585
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Wikipedia page gives information on semantic URLs: [https://en.wikipedia.org/wiki/Semantic_URL](https://en.wikipedia.org/wiki/Semantic_URL)
    and [http://apiux.com/2013/04/03/url-design-restful-web-services/](http://apiux.com/2013/04/03/url-design-restful-web-services/).'
  id: totrans-2586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a much more in depth discussion of the Play framework, I suggest *Play Framework
    Essentials* by *Julien Richard-Foy*.
  id: totrans-2587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*REST in Practice: Hypermedia and Systems Architecture*, by *Jim Webber*, *Savas
    Parastatidis* and *Ian Robinson* describes how to architect REST APIs.'
  id: totrans-2588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 14. Visualization with D3 and the Play Framework
  id: totrans-2589
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the Play framework, a web framework
    for Scala. We built an API that returns a JSON array describing a user's GitHub
    repositories.
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will construct a fully-fledged web application that displays
    a table and a chart describing a user's repositories. We will learn to integrate
    **D3.js**, a JavaScript library for building data-driven web pages, with the Play
    framework. This will set you on the path to building compelling interactive visualizations
    that showcase results obtained with machine learning.
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that you are familiar with HTML, CSS, and JavaScript. We
    present references at the end of the chapter. You should also have read the previous
    chapter.
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
- en: GitHub user data
  id: totrans-2593
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will build a single-page application that uses, as its backend, the API
    developed in the previous chapter. The application contains a form where the user
    enters the login name for a GitHub account. The application queries the API to
    get a list of repositories for that user and displays them on the screen as both
    a table and a pie chart summarizing programming language use for that user:'
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
- en: '![GitHub user data](img/image01234.jpeg)'
  id: totrans-2595
  prefs: []
  type: TYPE_IMG
- en: To see a live version of the application, head over to [http://app.scala4datascience.com](http://app.scala4datascience.com).
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
- en: Do I need a backend?
  id: totrans-2597
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned about the client-server model that underpins
    how the internet works: when you enter a website URL in your browser, the server
    serves HTML, CSS, and JavaScript to your browser, which then renders it in the
    appropriate manner.'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
- en: What does this all mean for you? Arguably the second question that you should
    be asking yourself when building a web application is whether you need to do any
    server-side processing (right after "is this really going to be worth the effort?").
    Could you just create an HTML web-page with some JavaScript?
  id: totrans-2599
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get away without a backend if the data needed to build the whole application
    is small enough: typically a few megabytes. If your application is larger, you
    will need a backend to transfer just the data the client currently needs. Surprisingly,
    you can often build visualizations without a backend: while data science is accustomed
    to dealing with terabytes of data, the goal of the data science process is often
    condensing these huge data sets to a few meaningful numbers.'
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a backend also lets you include logic invisible to the client. If you
    need to validate a password, you clearly cannot send the code to do that to the
    client computer: it needs to happen out of sight, on the server.'
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
- en: If your application is small enough and you do not need to do any server-side
    processing, stop reading this chapter, brush up on your JavaScript if you have
    to, and forget about Scala for now. Not having to worry about building a backend
    will make your life easier.
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, however, we do not have that freedom for the application that we want
    to build: the user could enter the name of anyone on GitHub. Finding information
    about that user requires a backend with access to tremendous storage and querying
    capacity (which we simulate by just forwarding the request to the GitHub API and
    re-interpreting the response).'
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript dependencies through web-jars
  id: totrans-2604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the challenges of developing web applications is that we are writing
    two quasi-separate programs: the server-side program and the client-side program.
    These generally require different technologies. In particular, for any but the
    most trivial application, we must keep track of JavaScript libraries, and integrate
    processing the JavaScript code (for instance, for minification) in the build process.'
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
- en: 'The Play framework manages JavaScript dependencies through *web-jars*. These
    are just JavaScript libraries packaged as jars. They are deployed on Maven Central,
    which means that we can just add them as dependencies to our `build.sbt` file.
    For this application, we will need the following JavaScript libraries:'
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
- en: Require.js, a library for writing modular JavaScript
  id: totrans-2607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JQuery
  id: totrans-2608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrap
  id: totrans-2609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underscore.js, a library that adds many functional constructs and client-side
    templating.
  id: totrans-2610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D3, the graph plotting library
  id: totrans-2611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVD3, a graph library built on top of D3
  id: totrans-2612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are planning on coding up the examples provided in this chapter, the
    easiest will be for you to start from the code for the previous chapter (You can
    download the code for [Chapter 13](part0125.xhtml#aid-3N6MA1 "Chapter 13. Web
    APIs with Play"), *Web APIs with Play*, from GitHub: [https://github.com/pbugnion/s4ds/tree/master/chap13](https://github.com/pbugnion/s4ds/tree/master/chap13)).
    We will assume this as a starting point here onwards.'
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s include the dependencies on the web-jars in the `build.sbt` file:'
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE641]'
  id: totrans-2615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE641]'
- en: Fetch the modules by running `activator` `update`. Once you have done this,
    you will notice the JavaScript libraries in `target/web/public/main/lib`.
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
- en: 'Towards a web application: HTML templates'
  id: totrans-2617
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we briefly saw how to construct HTML templates by interleaving
    Scala snippets in an HTML file. We saw that templates are compiled to Scala functions,
    and we learned how to call these functions from the controllers.
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
- en: In single-page applications, the majority of the logic governing what is actually
    displayed in the browser resides in the client-side JavaScript, not in the server.
    The pages served by the server contain the bare-bones HTML framework.
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the HTML layout for our application. We will save this in `views/index.scala.html`.
    The template will just contain the layout for the application, but will not contain
    any information about any user's repositories. To fetch that information, the
    application will have to query the API developed in the previous chapter. The
    template does not take any parameters, since all the dynamic HTML generation will
    happen client-side.
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
- en: We use the Bootstrap grid layout to control the HTML layout. If you are not
    familiar with Bootstrap layouts, consult the documentation at [http://getbootstrap.com/css/#grid-example-basic](http://getbootstrap.com/css/#grid-example-basic).
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE642]'
  id: totrans-2622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE642]'
- en: In the HTML head, we link the CSS stylesheets that we need for the application.
    Instead of specifying the path explicitly, we use the `@routes.Assets.versioned(...)`
    function. This resolves to a URI corresponding to the location where the assets
    are stored post-compilation. The argument passed to the function should be the
    path from `target/web/public/main` to the asset you need.
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to serve the compiled version of this view when the user accesses the
    route `/` on our server. We therefore need to add this route to `conf/routes`:'
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE643]'
  id: totrans-2625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE643]'
- en: 'The route is served by the `index` function in the `Application` controller.
    All this controller needs to do is serve the `index` view:'
  id: totrans-2626
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE644]'
  id: totrans-2627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE644]'
- en: Start the Play framework by running `activator run` in the root directory of
    the application and point your web browser to `127.0.0.1:9000/`. You should see
    the framework for our web application. Of course, the application does not do
    anything yet, since we have not written any of the JavaScript logic yet.
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
- en: '![Towards a web application: HTML templates](img/image01235.jpeg)'
  id: totrans-2629
  prefs: []
  type: TYPE_IMG
- en: Modular JavaScript through RequireJS
  id: totrans-2630
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way of injecting JavaScript libraries into the namespace is to
    add them to the HTML framework via `<script>...</script>` tags in the HTML header.
    For instance, to add JQuery, we would add the following line to the head of the
    document:'
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE645]'
  id: totrans-2632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE645]'
- en: While this works, it does not scale well to large applications, since every
    library gets imported into the global namespace. Modern client-side JavaScript
    frameworks such as AngularJS provide an alternative way of defining and loading
    modules that preserve encapsulation.
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use RequireJS. In a nutshell, RequireJS lets us encapsulate JavaScript
    modules through functions. For instance, if we wanted to write a module `example`
    that contains a function for hiding a `div`, we would define the module as follows:'
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE646]'
  id: totrans-2635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE646]'
- en: 'We encapsulate our module as a callback in a function called `define`. The
    `define` function takes two arguments: a list of dependencies, and a function
    definition. The `define` function binds the dependencies to the arguments list
    of the callback: in this case, functions in JQuery will be bound to `$` and functions
    in Underscore will be bound to `_`. This creates a module which exposes whatever
    the callback function returns. In this case, we export the `hide` function, binding
    it to the name `"hide"`. Our example module thus exposes the `hide` function.'
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
- en: 'To load this module, we pass it as a dependency to the module in which we want
    to use it:'
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE647]'
  id: totrans-2638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE647]'
- en: Notice how the functions in `example` are encapsulated, rather than existing
    in the global namespace. We call them through `example.<function-name>`. Furthermore,
    any functions or variables defined internally to the `example` module remain private.
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we want JavaScript code to exist outside of modules. This is often
    the case for the script that bootstraps the application. For these, replace `define`
    with `require`:'
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE648]'
  id: totrans-2641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE648]'
- en: 'Now that we have an overview of RequireJS, how do we use it in the Play framework?
    The first step is to add the dependency on the RequireJS web jar, which we have
    done. The Play framework also adds a RequireJS SBT plugin ([https://github.com/sbt/sbt-rjs](https://github.com/sbt/sbt-rjs)),
    which should be installed by default if you used the `play-scala` activator. If
    this is missing, it can be added with the following line in `plugins.sbt`:'
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE649]'
  id: totrans-2643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE649]'
- en: 'We also need to add the plugin to the list of stages. This allows the plugin
    to manipulate the JavaScript assets when packaging the application as a jar. Add
    the following line to `build.sbt`:'
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE650]'
  id: totrans-2645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE650]'
- en: You will need to restart the activator for the changes to take effect.
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to use RequireJS in our application. We can use it by adding
    the following line in the head section of our view:'
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE651]'
  id: totrans-2648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE651]'
- en: 'When the view is compiled, this is resolved to tags like:'
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE652]'
  id: totrans-2650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE652]'
- en: The argument passed to `data-main` is the entry point for our application. When
    RequireJS loads, it will execute `main.js`. That script must therefore bootstrap
    our application. In particular, it should contain a configuration object for RequireJS,
    to make it aware of where all the libraries are.
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping the applications
  id: totrans-2652
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we linked `require.js` to our application, we told it to use `main.js`
    as our entry point. To test that this works, let''s start by entering a dummy
    `main.js`. JavaScript files in Play applications go in `/public/javascripts`:'
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE653]'
  id: totrans-2654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE653]'
- en: To verify that this worked, head to `127.0.0.1:9000` and open the browser console.
    You should see `"hello, JavaScript"` in the console.
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now write a more useful `main.js`. We will start by configuring RequireJS,
    giving it the location of modules we will use in our application. Unfortunately,
    NVD3, the graph library that we use, does not play very well with RequireJS so
    we have to use an ugly hack to make it work. This complicates our `main.js` file
    somewhat:'
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE654]'
  id: totrans-2657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE654]'
- en: Now that we have the configuration in place, we can dig into the JavaScript
    part of the application.
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
- en: Client-side program architecture
  id: totrans-2659
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic idea is simple: the user searches for the name of someone on GitHub
    in the input box. When he enters a name, we fire a request to the API designed
    earlier in this chapter. When the response from the API returns, the program binds
    that response to a model and emits an event notifying that the model has been
    changed. The views listen for this event and refresh from the model in response.'
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
- en: Designing the model
  id: totrans-2661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by defining the client-side model. The model holds information regarding
    the repos of the user currently displayed. It gets filled in after the first search.
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE655]'
  id: totrans-2663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE655]'
- en: 'To see a populated value of the model, head to the complete application example
    on `app.scala4datascience.com`, open a JavaScript console in your browser, search
    for a user (for example, `odersky`) in the application and type the following
    in the console:'
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE656]'
  id: totrans-2665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE656]'
- en: These import the `"model"` module, bind it to the variable `model`, and then
    print information to the console.
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
- en: The event bus
  id: totrans-2667
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a mechanism for informing the views when the model is updated, since
    the views need to refresh from the new model. This is commonly handled through
    *events* in web applications. JQuery lets us bind callbacks to specific events.
    The callback is executed when that event occurs.
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, to bind a callback to the event `"custom-event"`, enter the following
    in a JavaScript console:'
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE657]'
  id: totrans-2670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE657]'
- en: 'We can fire the event using:'
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE658]'
  id: totrans-2672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE658]'
- en: 'Events in JQuery require an *event* bus, a DOM element on which the event is
    registered. In this case, we used the `window` DOM element as our event bus, but
    any JQuery element would have served. Centralizing event definitions to a single
    module is helpful. We will, therefore, create an `events` module containing two
    functions: `trigger`, which triggers an event (specified by a string) and `on`,
    which binds a callback to a specific event:'
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE659]'
  id: totrans-2674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE659]'
- en: 'We can now emit and receive events using the `events` module. You can test
    this out in a JavaScript console on the live version of the application (at `app.scala4datascience.com`).
    Let''s start by registering a listener:'
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE660]'
  id: totrans-2676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE660]'
- en: 'If we now trigger the event `"hello_event"`, the listener prints `"Received
    event"`:'
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE661]'
  id: totrans-2678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE661]'
- en: Using events allows us to decouple the controller from the views. The controller
    does not need to know anything about the views, and vice-versa. The controller
    just needs to emit a `"model_updated"` event when the model is updated, and the
    views need to refresh from the model when they receive that event.
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
- en: AJAX calls through JQuery
  id: totrans-2680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now write the controller for our application. When the user enters a
    name in the text input, we query the API, update the model and trigger a `model_updated`
    event.
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
- en: 'We use JQuery''s `$.getJSON` function to query our API. This function takes
    a URL as its first argument, and a callback as its second argument. The API call
    is asynchronous: `$.getJSON` returns immediately after execution. All request
    processing must, therefore, be done in the callback. The callback is called if
    the request is successful, but we can define additional handlers that are always
    called, or called on failure. Let''s try this out in the browser console (either
    your own, if you are running the API developed in the previous chapter, or on
    `app.scala4datascience.com`). Recall that the API is listening to the end-point
    `/api/repos/:user`:'
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE662]'
  id: totrans-2683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE662]'
- en: '`getJSON` returns immediately. A few tenths of a second later, the API responds,
    at which point the response gets fed through the callback.'
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
- en: 'The callback only gets executed on success. It takes, as its argument, the
    JSON object returned by the API. To bind a callback that is executed when the
    API request fails, call the `.fail` method on the return value of `getJSON`:'
  id: totrans-2685
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE663]'
  id: totrans-2686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE663]'
- en: We can also use the `.always` method on the return value of `getJSON` to specify
    a callback that is executed, whether the API query was successful or not.
  id: totrans-2687
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how to use `$.getJSON` to query our API, we can write the
    controller. The controller listens for changes to the `#user-selection` input
    field. When a change occurs, it fires an AJAX request to the API for information
    on that user. It binds a callback which updates the model when the API replies
    with a list of repositories. We will define a `controller` module that exports
    a single function, `initialize`, that creates the event listeners:'
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE664]'
  id: totrans-2689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE664]'
- en: 'Our controller module just exposes the `initialize` method. Once the initialization
    is performed, the controller interacts with the rest of the application through
    event listeners. We will call the controller''s `initialize` method in `main.js`.
    Currently, the last lines of that file are just an empty `require` block. Let''s
    import our controller and initialize it:'
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE665]'
  id: totrans-2691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE665]'
- en: 'To test that this works, we can bind a dummy listener to the `"model_updated"`
    event. For instance, we could log the current model to the browser JavaScript
    console with the following snippet (which you can write directly in the JavaScript
    console):'
  id: totrans-2692
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE666]'
  id: totrans-2693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE666]'
- en: If you then search for a user, the model will be printed to the console. We
    now have the controller in place. The last step is writing the views.
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
- en: Response views
  id: totrans-2695
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the request fails, we just display **Not found** in the response div. This
    part is the easiest to code up, so let''s do that first. We define an `initialize`
    method that generates the view. The view then listens for the `"model_updated"`
    event, which is fired by the controller after it updates the model. Once the initialization
    is complete, the only way to interact with the response view is through `"model_updated"`
    events:'
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE667]'
  id: totrans-2697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE667]'
- en: 'To bootstrap the view, we must call the initialize function from `main.js`.
    Just add a dependency on `responseView` in the require block, and call `responseView.initialize()`.
    With these modifications, the final `require` block in `main.js` is:'
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE668]'
  id: totrans-2699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE668]'
- en: You can check that this all works by entering junk in the user input to deliberately
    cause the API request to fail.
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
- en: When the user enters a valid GitHub login name and the API returns a list of
    repos, we must display those on the screen. We display a table and a pie chart
    that aggregates the repository sizes by language. We will define the pie chart
    and the table in two separate modules, called `repoGraph.js` and `repoTable.js`.
    Let's assume those exist for now and that they expose a `build` method that accepts
    a `model` and the name of a `div` in which to appear.
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s update the code for `responseView` to accommodate the user entering
    a valid GitHub user name:'
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE669]'
  id: totrans-2703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE669]'
- en: 'Let''s walk through what happens in the event of a successful API call. We
    inject the following bit of HTML in the `#response` div:'
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE670]'
  id: totrans-2705
  prefs: []
  type: TYPE_PRE
  zh: '[PRE670]'
- en: This adds two HTML divs, one for the table of repositories, and the other for
    the graph. We use Bootstrap classes to split the response div vertically.
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now turn our attention to the table view, which needs to expose a single
    `build` method, as described in the previous section. We will just display the
    repositories in an HTML table. We will use *Underscore templates* to build the
    table dynamically. Underscore templates work much like string interpolation in
    Scala: we define a template with placeholders. Let''s try this in a browser console:'
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE671]'
  id: totrans-2708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE671]'
- en: 'This creates a `myTemplate` function which accepts an object with attributes
    `title` and `name`:'
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE672]'
  id: totrans-2710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE672]'
- en: 'Underscore templates thus provide a convenient mechanism for formatting an
    object as a string. We will create a template for each row in our table, and pass
    the model for each repository to the template:'
  id: totrans-2711
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE673]'
  id: totrans-2712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE673]'
- en: Drawing plots with NVD3
  id: totrans-2713
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: D3 is a library that offers low-level components for building interactive visualizations
    in JavaScript. By offering the low-level components, it gives a huge degree of
    flexibility to the developer. The learning curve can, however, be quite steep.
    In this example, we will use NVD3, a library which provides pre-made graphs for
    D3\. This can greatly speed up initial development. We will place the code in
    the file `repoGraph.js` and expose a single method, `build`, which takes, as arguments,
    a model and a div and draws a pie chart in that div. The pie chart will aggregate
    language use across all the user's repositories.
  id: totrans-2714
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating a pie chart is nearly identical to the example given
    in the NVD3 documentation, available at [http://nvd3.org/examples/pie.html](http://nvd3.org/examples/pie.html).
    The data passed to the graph must be available as an array of objects. Each object
    must contain a `label` field and a `size` field. The `label` field identifies
    the language, and the `size` field is the total size of all the repositories for
    that user written in that language. The following would be a valid data array:'
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE674]'
  id: totrans-2716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE674]'
- en: 'To get the data in this format, we must aggregate sizes across the repositories
    written in a particular language in our model. We write the `generateDataFromModel`
    function to transform the `repos` array in the model to an array suitable for
    NVD3\. The crux of the aggregation is performed by a call to Underscore''s `groupBy`
    method, to group repositories by language. This method works exactly like Scala''s
    `groupBy` method. With this in mind, the `generateDataFromModel` function is:'
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE675]'
  id: totrans-2718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE675]'
- en: 'We can now build the pie chart, using NVD3''s `addGraph` method:'
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE676]'
  id: totrans-2720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE676]'
- en: This was the last component of our application. Point your browser to `127.0.0.1:9000`
    and you should see the application running.
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We have built a fully-functioning single-page web application.
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-2723
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to write a fully-featured web application with
    the Play framework. Congratulations on making it this far. Building web applications
    are likely to push many data scientists beyond their comfort zone, but knowing
    enough about the web to build basic applications will allow you to share your
    results in a compelling, engaging manner, as well as facilitate communications
    with software engineers and web developers.
  id: totrans-2724
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our whistle stop tour of Scala libraries. Over the course of
    this book, we have learned how to tackle linear algebra and optimization problems
    efficiently using Breeze, how to insert and query data in SQL databases in a functional
    manner, and both how to interact with web APIs and how to create them. We have
    reviewed some of tools available to the data scientist for writing concurrent
    or parallel applications, from parallel collections and futures to Spark via Akka.
    We have seen how pervasive these constructs are in Scala libraries, from futures
    in the Play framework to Akka as the backbone of Spark. If you have read this
    far, pat yourself on the back.
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
- en: This books gives you the briefest of introduction to the libraries it covers,
    hopefully just enough to give you a taste of what each tool is good for, what
    you could accomplish with it, and how it fits in the wider Scala ecosystem. If
    you decide to use any of these in your data science pipeline, you will need to
    read the documentation in more detail, or a more complete reference book. The
    references listed at the end of each chapter should provide a good starting point.
  id: totrans-2726
  prefs: []
  type: TYPE_NORMAL
- en: 'Both Scala and data science are evolving rapidly. Do not stay wedded to a particular
    toolkit or concept. Remain on top of current developments and, above all, remain
    pragmatic: find the right tool for the right job. Scala and the libraries discussed
    here will often be that tool, but not always: sometimes, a shell command or a
    short Python script will be more effective. Remember also that programming skills
    are but one aspect of the data scientist''s body of knowledge. Even if you want
    to specialize in the engineering side of data science, learn about the problem
    domain and the mathematical underpinnings of machine learning.'
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, if you have taken the time to read this book, it is likely
    that you view programming and data science as more than a day job. Coding in Scala
    can be satisfying and rewarding, so have fun and be awesome!
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-2729
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are thousands of HTML and CSS tutorials dotted around the web. A simple
    Google search will give you a much better idea of the resources available than
    any list of references I can provide.
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
- en: 'Mike Bostock''s website has a wealth of beautiful D3 visualizations: [http://bost.ocks.org/mike/.](http://bost.ocks.org/mike/.)
    To understand a bit more about D3, I recommend *Scott Murray''s Interactive Data
    Visualization for the Web*.'
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
- en: You may also wish to consult the references given in the previous chapter for
    reference books on the Play framework and designing REST APIs.
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A. Pattern Matching and Extractors
  id: totrans-2733
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pattern matching is a powerful tool for control flow in Scala. It is often underused
    and under-estimated by people coming to Scala from imperative languages.
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a few examples of pattern matching before diving into the
    theory. We start by defining a tuple:'
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE677]'
  id: totrans-2736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE677]'
- en: 'We can use pattern matching to extract the elements of this tuple and bind
    them to variables:'
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE678]'
  id: totrans-2738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE678]'
- en: 'We just extracted the two elements of the `names` tuple, binding them to the
    variables `firstName` and `lastName`. Notice how the left-hand side defines a
    pattern that the right-hand side must match: we are declaring that the variable
    `names` must be a two-element tuple. To make the pattern more specific, we could
    also have specified the expected types of the elements in the tuple:'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE679]'
  id: totrans-2740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE679]'
- en: What happens if the pattern on the left-hand side does not match the right-hand
    side?
  id: totrans-2741
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE680]'
  id: totrans-2742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE680]'
- en: This results in a compile error. Other types of pattern matching failures result
    in runtime errors.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
- en: 'Pattern matching is very expressive. To achieve the same behavior without pattern
    matching, you would have to do the following explicitly:'
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the variable `names` is a two-element tuple
  id: totrans-2745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the first element and bind it to `firstName`
  id: totrans-2746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the second element and bind it to `lastName`
  id: totrans-2747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we expect certain elements in the tuple to have specific values, we can
    verify this as part of the pattern match. For instance, we can verify that the
    first element of the `names` tuple matches `"Pascal"`:'
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE681]'
  id: totrans-2749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE681]'
- en: 'Besides tuples, we can also match on Scala collections:'
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE682]'
  id: totrans-2751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE682]'
- en: 'Notice the similarity between this pattern matching and array construction:'
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE683]'
  id: totrans-2753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE683]'
- en: Syntactically, Scala expresses pattern matching as the reverse process to instance
    construction. We can think of pattern matching as the deconstruction of an object,
    binding the object's constituent parts to variables.
  id: totrans-2754
  prefs: []
  type: TYPE_NORMAL
- en: 'When matching against collections, one is sometimes only interested in matching
    the first element, or the first few elements, and discarding the rest of the collection,
    whatever its length. The operator `_*` will match against any number of elements:'
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE684]'
  id: totrans-2756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE684]'
- en: 'By default, the part of the pattern matched by the `_*` operator is not bound
    to a variable. We can capture it as follows:'
  id: totrans-2757
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE685]'
  id: totrans-2758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE685]'
- en: 'Besides tuples and collections, we can also match against case classes. Let''s
    start by defining a case representing a name:'
  id: totrans-2759
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE686]'
  id: totrans-2760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE686]'
- en: 'We can match against instances of `Name` in much the same way we matched against
    tuples:'
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE687]'
  id: totrans-2762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE687]'
- en: 'All these patterns can also be used in `match` statements:'
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE688]'
  id: totrans-2764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE688]'
- en: Pattern matching in for comprehensions
  id: totrans-2765
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pattern matching is useful in *for* comprehensions for extracting items from
    a collection that match a specific pattern. Let''s build a collection of `Name`
    instances:'
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE689]'
  id: totrans-2767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE689]'
- en: 'We can use pattern matching to extract the internals of the class in a for-comprehension:'
  id: totrans-2768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE690]'
  id: totrans-2769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE690]'
- en: So far, nothing terribly ground-breaking. But what if we wanted to extract the
    surname of everyone whose first name is `"Martin"`?
  id: totrans-2770
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE691]'
  id: totrans-2771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE691]'
- en: Writing `Name("Martin", last) <- names` extracts the elements of names that
    match the pattern. You might think that this is a contrived example, and it is,
    but the examples in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"),
    *Web APIs* demonstrate the usefulness and versatility of this language pattern,
    for instance, for extracting specific fields from JSON objects.
  id: totrans-2772
  prefs: []
  type: TYPE_NORMAL
- en: Pattern matching internals
  id: totrans-2773
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you define a case class, as we saw with `Name`, you get pattern matching
    against the constructor *for free*. You should be using case classes to represent
    your data as much as possible, thus reducing the need to implement your own pattern
    matching. It is nevertheless useful to understand how pattern matching works.
  id: totrans-2774
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create a case class, Scala automatically builds a companion object:'
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE692]'
  id: totrans-2776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE692]'
- en: The method used (internally) for pattern matching is `unapply`. This method
    takes, as argument, an object and returns `Option[T],` where `T` is a tuple of
    the values of the case class.
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE693]'
  id: totrans-2778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE693]'
- en: 'The `unapply` method is an *extractor*. It plays the opposite role of the constructor:
    it takes an object and extracts the list of parameters needed to construct that
    object. When you write `val Name(firstName, lastName)`, or when you use `Name`
    as a case in a match statement, Scala calls `Name.unapply` on what you are matching
    against. A value of `Some[(String, String)]` implies a pattern match, while a
    value of `None` implies that the pattern fails.'
  id: totrans-2779
  prefs: []
  type: TYPE_NORMAL
- en: 'To write custom extractors, you just need an object with an `unapply` method.
    While `unapply` normally resides in the companion object of a class that you are
    deconstructing, this need not be the case. In fact, it does not need to correspond
    to an existing class at all. For instance, let''s define a `NonZeroDouble` extractor
    that matches any non-zero double:'
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE694]'
  id: totrans-2781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE694]'
- en: We defined an extractor for `NonZeroDouble`, despite the absence of a corresponding
    `NonZeroDouble` class.
  id: totrans-2782
  prefs: []
  type: TYPE_NORMAL
- en: 'This `NonZeroDouble` extractor would be useful in a match object. For instance,
    let''s define a `safeDivision` function that returns a default value when the
    denominator is zero:'
  id: totrans-2783
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE695]'
  id: totrans-2784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE695]'
- en: 'This is a trivial example because the `NonZeroDouble.unapply` method is so
    simple, but you can hopefully see the usefulness and expressiveness, if we were
    to define a more complex test. Defining custom extractors lets you define powerful
    control flow constructs to leverage `match` statements. More importantly, they
    enable the client using the extractors to think about control flow declaratively:
    the client can declare that they need a `NonZeroDouble`, rather than instructing
    the compiler to check whether the value is zero.'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
- en: Extracting sequences
  id: totrans-2786
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous section explains extraction from case classes, and how to write
    custom extractors, but it does not explain how extraction works on sequences:'
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE696]'
  id: totrans-2788
  prefs: []
  type: TYPE_PRE
  zh: '[PRE696]'
- en: 'Rather than relying on an `unapply` method, sequences rely on an `unapplySeq`
    method defined in the companion object. This is expected to return an `Option[Seq[A]]`:'
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE697]'
  id: totrans-2790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE697]'
- en: 'Let''s write an example. We will write an extractor for Breeze vectors (which
    do not currently support pattern matching). To avoid clashing with the `DenseVector`
    companion object, we will write our `unapplySeq` in a separate object, called
    `DV`. All our `unapplySeq` method needs to do is convert its argument to a Scala
    `Vector` instance. To avoid muddying the concepts with generics, we will write
    this implementation for `[Double]` vectors only:'
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE698]'
  id: totrans-2792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE698]'
- en: 'Let''s try our new extractor implementation:'
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE699]'
  id: totrans-2794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE699]'
- en: Summary
  id: totrans-2795
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pattern matching is a powerful tool for control flow. It encourages the programmer
    to think declaratively: declare that you expect a variable to match a certain
    pattern, rather than explicitly tell the computer how to check that it matches
    this pattern. This can save many lines of code and enhance clarity.'
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  id: totrans-2797
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For an overview of pattern matching in Scala, there is no better reference
    than *Programming in Scala*, by *Martin Odersky*, *Bill Venners*, and *Lex Spoon*.
    An online version of the first edition is available at: [https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html](https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html).'
  id: totrans-2798
  prefs: []
  type: TYPE_NORMAL
- en: '*Daniel Westheide''s* blog covers slightly more advanced Scala constructs,
    and is a very useful read: [http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html](http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html).'
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
