- en: Part I. Module 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分。模块1
- en: '**Scala for Data Science**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Scala数据科学**'
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Leverage the power of Scala with different tools to build scalable, robust
    data science applications*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*利用Scala的不同工具构建可扩展、健壮的数据科学应用程序*'
- en: Chapter 1. Scala and Data Science
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。Scala和数据科学
- en: The second half of the 20^(th) century was the age of silicon. In fifty years,
    computing power went from extremely scarce to entirely mundane. The first half
    of the 21^(st) century is the age of the Internet. The last 20 years have seen
    the rise of giants such as Google, Twitter, and Facebook—giants that have forever
    changed the way we view knowledge.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪后半叶是硅的时代。在50年里，计算能力从极其稀缺到完全平凡。21世纪前半叶是互联网的时代。在过去的20年里，谷歌、推特和Facebook等巨头崛起——这些巨头永远改变了我们看待知识的方式。
- en: The Internet is a vast nexus of information. Ninety percent of the data generated
    by humanity has been generated in the last 18 months. The programmers, statisticians,
    and scientists who can harness this glut of data to derive real understanding
    will have an ever greater influence on how businesses, governments, and charities
    make decisions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网是一个庞大的信息枢纽。人类产生的90%的数据在过去18个月内已经生成。能够利用这些数据洪流以获得真正理解的程序员、统计学家和科学家将对商业、政府和慈善机构如何做出决策产生越来越大的影响。
- en: This book strives to introduce some of the tools that you will need to synthesize
    the avalanche of data to produce true insight.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在介绍一些您将需要的工具，以从数据洪流中综合提炼出真正的洞察力。
- en: Data science
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: 'Data science is the process of extracting useful information from data. As
    a discipline, it remains somewhat ill-defined, with nearly as many definitions
    as there are experts. Rather than add yet another definition, I will follow *Drew
    Conway''s* description ([http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)).
    He describes data science as the culmination of three orthogonal sets of skills:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是从数据中提取有用信息的过程。作为一个学科，它仍然有些模糊不清，定义的数量几乎与专家的数量一样多。而不是再添加另一个定义，我将遵循**德鲁·康威**的描述（[http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)）。他描述数据科学为三个正交技能集的融合：
- en: 'Data scientists must have *hacking skills*. Data is stored and transmitted
    through computers. Computers, programming languages, and libraries are the hammers
    and chisels of data scientists; they must wield them with confidence and accuracy
    to sculpt the data as they please. This is where Scala comes in: it''s a powerful
    tool to have in your programming toolkit.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家必须具备**黑客技能**。数据存储和传输通过计算机进行。计算机、编程语言和库是数据科学家的锤子和凿子；他们必须自信且准确地使用它们来塑造数据。这正是Scala发挥作用的地方：它是编程工具箱中一个强大的工具。
- en: Data scientists must have a sound understanding of *statistics and numerical
    algorithms*. Good data scientists will understand how machine learning algorithms
    function and how to interpret results. They will not be fooled by misleading metrics,
    deceptive statistics, or misinterpreted causal links.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家必须对**统计学和数值算法**有扎实的理解。优秀的数据科学家将理解机器学习算法的工作原理以及如何解读结果。他们不会被误导的指标、欺骗性的统计数据或误解的因果关系所迷惑。
- en: A good data scientist must have a sound understanding of the *problem domain*.
    The data science process involves building and discovering knowledge about the
    problem domain in a scientifically rigorous manner. The data scientist must, therefore,
    ask the right questions, be aware of previous results, and understand how the
    data science effort fits in the wider business or research context.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个优秀的数据科学家必须对**问题领域**有扎实的理解。数据科学过程涉及以科学严谨的方式构建和发现关于问题领域知识。因此，数据科学家必须提出正确的问题，了解以往的结果，并理解数据科学努力如何融入更广泛的商业或研究背景。
- en: 'Drew Conway summarizes this elegantly with a Venn diagram showing data science
    at the intersection of hacking skills, maths and statistics knowledge, and substantive
    expertise:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 德鲁·康威用一张维恩图优雅地总结了这一点，展示了数据科学位于黑客技能、数学和统计学知识以及实质性专业知识交汇处：
- en: '![Data science](img/image01158.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学](img/image01158.jpeg)'
- en: It is, of course, rare for people to be experts in more than one of these areas.
    Data scientists often work in cross-functional teams, with different members providing
    the expertise for different areas. To function effectively, every member of the
    team must nevertheless have a general working knowledge of all three areas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，人们精通这些领域中的多个领域是罕见的。数据科学家通常在跨职能团队中工作，不同成员为不同领域提供专业知识。然而，为了有效运作，团队中的每个成员都必须对所有三个领域有一个一般的工作知识。
- en: 'To give a more concrete overview of the workflow in a data science project,
    let''s imagine that we are trying to write an application that analyzes the public
    perception of a political campaign. This is what the data science pipeline might
    look like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地概述数据科学项目中的工作流程，让我们设想我们正在尝试编写一个分析公众对政治运动看法的应用程序。数据科学管道可能看起来是这样的：
- en: '**Obtaining data**: This might involve extracting information from text files,
    polling a sensor network or querying a web API. We could, for instance, query
    the Twitter API to obtain lists of tweets with the relevant hashtags.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取数据**：这可能涉及从文本文件中提取信息、轮询传感器网络或查询Web API。例如，我们可以查询Twitter API以获取包含相关标签的推文列表。'
- en: '**Data ingestion**: Data often comes from many different sources and might
    be unstructured or semi-structured. Data ingestion involves moving data from the
    data source, processing it to extract structured information, and storing this
    information in a database. For tweets, for instance, we might extract the username,
    the names of other users mentioned in the tweet, the hashtags, text of the tweet,
    and whether the tweet contains certain keywords.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取**：数据通常来自许多不同的来源，可能是非结构化或半结构化的。数据摄取涉及将数据从数据源移动到处理它以提取结构化信息，并将这些信息存储在数据库中。例如，对于推文，我们可能会提取用户名、推文中提到的其他用户的名称、标签、推文文本以及推文是否包含某些关键词。'
- en: '**Exploring data**: We often have a clear idea of what information we want
    to extract from the data but very little idea how. For instance, let''s imagine
    that we have ingested thousands of tweets containing hashtags relevant to our
    political campaign. There is no clear path to go from our database of tweets to
    the end goal: insight into the overall public perception of our campaign. Data
    exploration involves mapping out how we are going to get there. This step will
    often uncover new questions or sources of data, which requires going back to the
    first step of the pipeline. For our tweet database, we might, for instance, decide
    that we need to have a human manually label a thousand or more tweets as expressing
    "positive" or "negative" sentiments toward the political campaign. We could then
    use these tweets as a training set to construct a model.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索数据**：我们通常对想要从数据中提取的信息有一个明确的想法，但对如何做到这一点却知之甚少。例如，让我们设想我们已经摄取了包含与我们政治运动相关的标签的数千条推文。从我们的推文数据库到最终目标——了解公众对我们运动的总体看法——没有明确的路径。数据探索涉及规划我们如何到达那里。这一步骤通常会揭示新的问题或数据来源，这需要回到管道的第一步。例如，对于我们的推文数据库，我们可能会决定需要有人手动标记一千条或更多的推文，以表达对政治运动的“积极”或“消极”情绪。然后我们可以使用这些推文作为训练集来构建模型。'
- en: '**Feature building**: A machine learning algorithm is only as good as the features
    that enter it. A significant fraction of a data scientist''s time involves transforming
    and combining existing features to create new features more closely related to
    the problem that we are trying to solve. For instance, we might construct a new
    feature corresponding to the number of "positive" sounding words or pairs of words
    in a tweet.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建特征**：机器学习算法的好坏取决于进入它的特征。数据科学家的大部分时间都花在转换和组合现有特征以创建与我们要解决的问题更紧密相关的新特征上。例如，我们可能会构建一个新特征，对应于推文中“积极”语气单词或单词对的数量。'
- en: '**Model construction and training**: Having built the features that enter the
    model, the data scientist can now train machine learning algorithms on their datasets.
    This will often involve trying different algorithms and optimizing model **hyperparameters**.
    We might, for instance, settle on using a random forest algorithm to decide whether
    a tweet is "positive" or "negative" about the campaign. Constructing the model
    involves choosing the right number of trees and how to calculate impurity measures.
    A sound understanding of statistics and the problem domain will help inform these
    decisions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型构建和训练**：在构建了进入模型的特征之后，数据科学家现在可以在他们的数据集上训练机器学习算法。这通常涉及尝试不同的算法和优化模型的**超参数**。例如，我们可能会决定使用随机森林算法来决定一条推文是对活动“正面”还是“负面”的看法。构建模型涉及选择合适的树的数量以及如何计算不纯度度量。对统计学和问题领域的良好理解将有助于这些决策。'
- en: '**Model extrapolation and prediction**: The data scientists can now use their
    new model to try and infer information about previously unseen data points. They
    might pass a new tweet through their model to ascertain whether it speaks positively
    or negatively of the political campaign.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型外推和预测**：数据科学家现在可以使用他们新的模型来尝试推断关于先前未见数据点的信息。他们可能会将一条新的推文通过他们的模型来确认它是否对政治活动持正面或负面的看法。'
- en: '**Distillation of intelligence and insight from the model**: The data scientists
    combine the outcome of the data analysis process with knowledge of the business
    domain to inform business decisions. They might discover that specific messages
    resonate better with the target audience, or with specific segments of the target
    audience, leading to more accurate targeting. A key part of informing stakeholders
    involves data visualization and presentation: data scientists create graphs, visualizations,
    and reports to help make the insights derived clear and compelling.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从模型中提取智能和洞察力**：数据科学家将数据分析过程的结果与业务领域的知识相结合，以指导业务决策。他们可能会发现某些信息与目标受众或目标受众的特定部分产生更好的共鸣，从而实现更精确的目标。向利益相关者提供信息的关键部分涉及数据可视化和展示：数据科学家创建图表、可视化和报告，以帮助使得出的见解清晰且引人入胜。'
- en: 'This is far from a linear pipeline. Often, insights gained at one stage will
    require the data scientists to backtrack to a previous stage of the pipeline.
    Indeed, the generation of business insights from raw data is normally an iterative
    process: the data scientists might do a rapid first pass to verify the premise
    of the problem and then gradually refine the approach by adding new data sources
    or new features or trying new machine learning algorithms.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这远非一个线性的管道。通常，在一个阶段获得的见解将要求数据科学家回溯到管道的先前阶段。确实，从原始数据生成业务见解通常是一个迭代的过程：数据科学家可能会进行快速的第一遍扫描以验证问题的前提，然后通过添加新的数据源或新特征或尝试新的机器学习算法来逐步完善方法。
- en: In this book, you will learn how to deal with each step of the pipeline in Scala,
    leveraging existing libraries to build robust applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你将学习如何在Scala中处理管道的每个步骤，利用现有库构建健壮的应用程序。
- en: Programming in data science
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中的编程
- en: This book is not a book about data science. It is a book about how to use Scala,
    a programming language, for data science. So, where does programming come in when
    processing data?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是一本关于数据科学的书。它是一本关于如何使用Scala编程语言进行数据科学的书。那么，在处理数据时编程的作用在哪里呢？
- en: Computers are involved at every step of the data science pipeline, but not necessarily
    in the same manner. The style of programs that we build will be drastically different
    if we are just writing throwaway scripts to explore data or trying to build a
    scalable application that pushes data through a well-understood pipeline to continuously
    deliver business intelligence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在数据科学管道的每个步骤中都发挥作用，但并不一定是同样的方式。如果我们只是编写临时脚本以探索数据或试图构建一个可扩展的应用程序，该应用程序通过一个被充分理解的管道推送数据以持续提供业务智能，那么我们构建的程序的风格将会有很大的不同。
- en: Let's imagine that we work for a company making games for mobile phones in which
    you can purchase in-game benefits. The majority of users never buy anything, but
    a small fraction is likely to spend a lot of money. We want to build a model that
    recognizes big spenders based on their play patterns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们为一家制作手机游戏的公司工作，在这些游戏中你可以购买游戏内福利。大多数用户从不购买任何东西，但一小部分人可能会花很多钱。我们想要构建一个模型，根据他们的游戏模式识别大额消费者。
- en: The first step is to explore data, find the right features, and build a model
    based on a subset of the data. In this exploration phase, we have a clear goal
    in mind but little idea of how to get there. We want a light, flexible language
    with strong libraries to get us a working model as soon as possible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是探索数据，找到正确的特征，并在数据的一个子集上构建模型。在这个探索阶段，我们有一个明确的目标，但几乎没有关于如何实现目标的想法。我们希望有一个轻量级、灵活的语言，以及强大的库，以便尽快得到一个工作模型。
- en: 'Once we have a working model, we need to deploy it on our gaming platform to
    analyze the usage patterns of all the current users. This is a very different
    problem: we have a relatively clear understanding of the goals of the program
    and of how to get there. The challenge comes in designing software that will scale
    out to handle all the users and be robust to future changes in usage patterns.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了工作模型，我们需要将其部署到我们的游戏平台上，以分析所有当前用户的用法模式。这是一个非常不同的问题：我们对程序的目标和如何实现目标有相对清晰的理解。挑战在于设计能够扩展以处理所有用户并适应未来用法模式变化的软件。
- en: In practice, the type of software that we write typically lies on a spectrum
    ranging from a single throwaway script to production-level code that must be proof
    against future expansion and load increases. Before writing any code, the data
    scientist must understand where their software lies on this spectrum. Let's call
    this the **permanence** **spectrum**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们编写的软件类型通常位于从单个一次性脚本到必须能够抵御未来扩展和负载增加的生产级代码的连续谱上。在编写任何代码之前，数据科学家必须了解他们的软件在这个谱上的位置。让我们称这个为**持久性**谱。
- en: Why Scala?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Scala？
- en: You want to write a program that handles data. Which language should you choose?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你想编写一个处理数据的程序。你应该选择哪种语言？
- en: There are a few different options. You might choose a dynamic language such
    as Python or R or a more traditional object-oriented language such as Java. In
    this section, we will explore how Scala differs from these languages and when
    it might make sense to use it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的选择。你可能会选择一种动态语言，如Python或R，或者一种更传统的面向对象语言，如Java。在本节中，我们将探讨Scala与这些语言的差异以及何时使用它可能是有意义的。
- en: 'When choosing a language, the architect''s trade-off lies in a balance of provable
    correctness versus development speed. Which of these aspects you need to emphasize
    will depend on the application requirements and where on the permanence spectrum
    your program lies. Is this a short script that will be used by a few people who
    can easily fix any problems that arise? If so, you can probably permit a certain
    number of bugs in rarely used code paths: when a developer hits a snag, they can
    just fix the problem as it arises. By contrast, if you are developing a database
    engine that you plan on releasing to the wider world, you will, in all likelihood,
    favor correctness over rapid development. The SQLite database engine, for instance,
    is famous for its extensive test suite, with 800 times as much testing code as
    application code ([https://www.sqlite.org/testing.html](https://www.sqlite.org/testing.html)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择语言时，架构师需要在可证明的正确性和开发速度之间进行权衡。你需要强调哪些方面将取决于应用程序的要求以及你的程序在持久性谱上的位置。这是一个短脚本，将被少数人使用，他们可以轻松修复任何出现的问题？如果是这样，你可能在很少使用的代码路径中允许一定数量的错误：当开发者遇到问题时，他们可以立即修复问题。相比之下，如果你正在开发一个计划发布给更广泛世界的数据库引擎，你很可能会更重视正确性而不是快速开发。例如，SQLite数据库引擎以其广泛的测试套件而闻名，测试代码量是应用代码的800倍（[https://www.sqlite.org/testing.html](https://www.sqlite.org/testing.html)）。
- en: What matters, when estimating the *correctness* of a program, is not the perceived
    absence of bugs, it is the degree to which you can prove that certain bugs are
    absent.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在估计程序**正确性**时，重要的是不是感知到没有错误，而是你能够证明某些错误确实不存在的程度。
- en: 'There are several ways of proving the absence of bugs before the code has even
    run:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码运行之前，有几种方法可以证明不存在错误：
- en: Static type checking occurs at compile time in statically typed languages, but
    this can also be used in strongly typed dynamic languages that support type annotations
    or type hints. Type checking helps verify that we are using functions and classes
    as intended.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静态类型语言中，静态类型检查发生在编译时，但这也可以用于支持类型注解或类型提示的强类型动态语言。类型检查有助于验证我们是否按预期使用函数和类。
- en: Static analyzers and linters that check for undefined variables or suspicious
    behavior (such as parts of the code that can never be reached).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态分析器和代码检查器可以检查未定义的变量或可疑的行为（例如，代码中永远无法到达的部分）。
- en: Declaring some attributes as immutable or constant in compiled languages.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编译语言中声明某些属性为不可变或常量。
- en: Unit testing to demonstrate the absence of bugs along particular code paths.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试以证明特定代码路径上没有bug。
- en: 'There are several more ways of checking for the absence of some bugs at runtime:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以检查运行时某些错误的缺失：
- en: Dynamic type checking in both statically typed and dynamic languages
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静态类型和动态语言中均支持动态类型检查
- en: Assertions verifying supposed program invariants or expected contracts
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断言验证假设的程序不变性或预期契约
- en: In the next sections, we will examine how Scala compares to other languages
    in data science.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨Scala在数据科学领域与其他语言的比较。
- en: Static typing and type inference
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态类型和类型推断
- en: Scala's static typing system is very versatile. A lot of information as to the
    program's behavior can be encoded in types, allowing the compiler to guarantee
    a certain level of correctness. This is particularly useful for code paths that
    are rarely used. A dynamic language cannot catch errors until a particular branch
    of execution runs, so a bug can persist for a long time until the program runs
    into it. In a statically typed language, any bug that can be caught by the compiler
    will be caught at compile time, before the program has even started running.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的静态类型系统非常灵活。程序行为的大量信息可以编码在类型中，使得编译器能够保证一定程度的正确性。这对于很少使用的代码路径特别有用。动态语言无法在特定执行分支运行之前捕获错误，因此错误可能长时间存在，直到程序遇到它。在静态类型语言中，任何编译器可以捕获的bug都会在程序开始运行之前在编译时被捕获。
- en: 'Statically typed object-oriented languages have often been criticized for being
    needlessly verbose. Consider the initialization of an instance of the `Example`
    class in Java:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 静态类型面向对象语言常因冗余而被批评。以Java中`Example`类实例的初始化为例：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have to repeat the class name twice—once to define the compile-time type
    of the `myInstance` variable and once to construct the instance itself. This feels
    like unnecessary work: the compiler knows that the type of `myInstance` is `Example`
    (or a superclass of `Example`) as we are binding a value of the `Example` type.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须重复两次类名——一次是为了定义`myInstance`变量的编译时类型，另一次是为了构造实例本身。这感觉像是多余的工作：编译器知道`myInstance`的类型是`Example`（或`Example`的父类），因为我们绑定了一个`Example`类型的值。
- en: 'Scala, like most functional languages, uses type inference to allow the compiler
    to infer the type of variables from the instances bound to them. We would write
    the equivalent line in Scala as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Scala，像大多数函数式语言一样，使用类型推断来允许编译器从绑定到它们的实例中推断变量的类型。我们可以在Scala中这样写等效的行：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Scala compiler infers that `myInstance` has the `Example` type at compile
    time. A lot of the time, it is enough to specify the types of the arguments and
    of the return value of a function. The compiler can then infer types for all the
    variables defined in the body of the function. Scala code is usually much more
    concise and readable than the equivalent Java code, without compromising any of
    the type safety.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Scala编译器在编译时推断`myInstance`具有`Example`类型。很多时候，指定函数的参数和返回值的类型就足够了。然后编译器可以推断函数体中定义的所有变量的类型。Scala代码通常比等效的Java代码更简洁、更易读，而不牺牲任何类型安全性。
- en: Scala encourages immutability
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala鼓励不可变性
- en: 'Scala encourages the use of immutable objects. In Scala, it is very easy to
    define an attribute as immutable:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Scala鼓励使用不可变对象。在Scala中，定义一个属性为不可变非常容易：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The default collections are immutable:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 默认集合是不可变的：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Having immutable objects removes a common source of bugs. Knowing that some
    objects cannot be changed once instantiated reduces the number of places bugs
    can creep in. Instead of considering the lifetime of the object, we can narrow
    in on the constructor.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不可变对象消除了常见的错误来源。知道某些对象一旦实例化后就不能改变，可以减少错误可能潜入的地方。而不是考虑对象的生命周期，我们可以专注于构造函数。
- en: Scala and functional programs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala和函数式程序
- en: 'Scala encourages functional code. A lot of Scala code consists of using higher-order
    functions to transform collections. You, as a programmer, do not have to deal
    with the details of iterating over the collection. Let''s write an `occurrencesOf`
    function that returns the indices at which an element occurs in a list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Scala鼓励使用函数式代码。大量的Scala代码由使用高阶函数来转换集合组成。作为程序员，你不需要处理遍历集合的细节。让我们编写一个`occurrencesOf`函数，它返回元素在列表中出现的索引：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'How does this work? We first declare a new list, `collection.zipWithIndex`,
    whose elements are `(collection(0), 0)`, `(collection(1), 1)`, and so on: pairs
    of the collection''s elements and their indexes.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？我们首先声明一个新的列表，`collection.zipWithIndex`，其元素是`(collection(0), 0)`、`(collection(1),
    1)`等等：集合的元素及其索引的配对。
- en: We then tell Scala that we want to iterate over this collection, binding the
    `currentElem` variable to the current element and `index` to the index. We apply
    a filter on the iteration, selecting only those elements for which `currentElem
    == elem`. We then tell Scala to just return the `index` variable.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们告诉Scala我们想要遍历这个集合，将`currentElem`变量绑定到当前元素，将`index`绑定到索引。我们对迭代应用一个过滤器，只选择那些`currentElem
    == elem`的元素。然后我们告诉Scala只返回`index`变量。
- en: 'We did not need to deal with the details of the iteration process in Scala.
    The syntax is very declarative: we tell the compiler that we want the index of
    every element equal to `elem` in collection and let the compiler worry about how
    to iterate over collection.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，我们不需要处理迭代过程的细节。语法非常声明式：我们告诉编译器我们想要集合中每个等于`elem`的元素的索引，然后让编译器去担心如何遍历集合。
- en: 'Consider the equivalent in Java:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑Java中的等效代码：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In Java, you start by defining a (mutable) list in which to put occurrences
    as you find them. You then iterate over the collection by defining a counter,
    considering each element in turn and adding its index to the list of occurrences,
    if need be. There are many more moving parts that we need to get right for this
    method to work. These moving parts exist because we must tell Java how to iterate
    over the collection, and they represent a common source of bugs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中，你首先定义一个（可变的）列表，用于存放你找到的实例。然后通过定义一个计数器来遍历这个集合，依次考虑每个元素，并在需要时将其索引添加到实例列表中。为了使这个方法正常工作，我们需要正确处理许多其他部分。这些部分的存在是因为我们必须告诉Java如何遍历集合，并且它们是bug的常见来源。
- en: 'Furthermore, as a lot of code is taken up by the iteration mechanism, the line
    that defines the logic of the function is harder to find:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于大量的代码被迭代机制占用，定义函数逻辑的行更难找到：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that this is not meant as an attack on Java. In fact, Java 8 adds a slew
    of functional constructs, such as lambda expressions, the `Optional` type that
    mirrors Scala's `Option`, or stream processing. Rather, it is meant to demonstrate
    the benefit of functional approaches in minimizing the potential for errors and
    maximizing clarity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这并不是对Java的攻击。事实上，Java 8增加了一系列函数式构造，如lambda表达式、与Scala的`Option`相对应的`Optional`类型或流处理。相反，这是为了展示函数式方法在最小化错误潜力和最大化清晰度方面的好处。
- en: Null pointer uncertainty
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空指针不确定性
- en: We often need to represent the possible absence of a value. For instance, imagine
    that we are reading a list of usernames from a CSV file. The CSV file contains
    name and e-mail information. However, some users have declined to enter their
    e-mail into the system, so this information is absent. In Java, one would typically
    represent the e-mail as a string or an `Email` class and represent the absence
    of e-mail information for a particular user by setting that reference to `null`.
    Similarly, in Python, we might use `None` to demonstrate the absence of a value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要表示值的可能不存在。例如，假设我们正在从CSV文件中读取用户名列表。CSV文件包含姓名和电子邮件信息。然而，一些用户选择不将他们的电子邮件输入到系统中，因此这些信息不存在。在Java中，人们通常会使用字符串或`Email`类来表示电子邮件，并通过将那个引用设置为`null`来表示特定用户的电子邮件信息不存在。同样，在Python中，我们可能会使用`None`来表示值的缺失。
- en: This approach is dangerous because we are not encoding the possible absence
    of e-mail information. In any nontrivial program, deciding whether an instance
    attribute can be `null` requires considering every occasion in which this instance
    is defined. This quickly becomes impractical, so programmers either assume that
    a variable is not null or code too defensively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很危险，因为我们没有编码电子邮件信息的可能缺失。在任何非平凡程序中，决定实例属性是否可以为`null`需要考虑这个实例定义的每一个场合。这很快就会变得不切实际，因此程序员要么假设变量不是`null`，要么编写过于防御性的代码。
- en: 'Scala (following the lead of other functional languages) introduces the `Option[T]`
    type to represent an attribute that might be absent. We might then write the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Scala（跟随其他函数式语言的趋势）引入了`Option[T]`类型来表示可能缺失的属性。然后我们可能会写出以下内容：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have now encoded the possible absence of e-mail in the type information.
    It is obvious to any programmer using the `User` class that e-mail information
    is possibly absent. Even better, the compiler knows that the `email` field can
    be absent, forcing us to deal with the problem rather than recklessly ignoring
    it to have the application burn at runtime in a conflagration of null pointer
    exceptions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在类型信息中编码了电子邮件可能不存在的情况。对于使用`User`类的任何程序员来说，电子邮件信息可能不存在是显而易见的。更好的是，编译器知道`email`字段可能不存在，这迫使我们处理这个问题，而不是鲁莽地忽略它，让应用程序在运行时因为空指针异常而崩溃。
- en: All this goes back to achieving a certain level of provable correctness. Never
    using `null`, we know that we will never run into null pointer exceptions. Achieving
    the same level of correctness in languages without `Option[T]` requires writing
    unit tests on the client code to verify that it behaves correctly when the e-mail
    attribute is null.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都回到了实现一定程度的可证明正确性的目标。从不使用`null`，我们知道我们永远不会遇到空指针异常。在没有`Option[T]`的语言中实现相同级别的正确性需要编写单元测试来验证当电子邮件属性为`null`时客户端代码的行为是否正确。
- en: 'Note that it is possible to achieve this in Java using, for instance, Google''s
    Guava library ([https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained](https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained))
    or the `Optional` class in Java 8\. It is more a matter of convention: using `null`
    in Java to denote the absence of a value has long been the norm.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Java中，可以使用例如Google的Guava库([https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained](https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained))或Java
    8中的`Optional`类来实现这一点。这更多是一个约定：在Java中使用`null`来表示值的缺失已经很长时间是规范了。
- en: Easier parallelism
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更容易的并行性
- en: Writing programs that take advantage of parallel architectures is challenging.
    It is nevertheless necessary to tackle all but the simplest data science problems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 编写利用并行架构的程序具有挑战性。尽管如此，解决除了最简单的数据科学问题之外的所有问题仍然是必要的。
- en: Parallel programming is difficult because we, as programmers, tend to think
    sequentially. Reasoning about the order in which different events can happen in
    a concurrent program is very challenging.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程困难，因为我们作为程序员，往往倾向于按顺序思考。在并发程序中推理不同事件可能发生的顺序是非常具有挑战性的。
- en: Scala provides several abstractions that greatly facilitate the writing of parallel
    code. These abstractions work by imposing constraints on the way parallelism is
    achieved. For instance, parallel collections force the user to phrase the computation
    as a sequence of operations (such as **map**, **reduce**, and **filter**) on collections.
    Actor systems require the developer to think in terms of actors that encapsulate
    the application state and communicate by passing messages.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Scala提供了几个抽象，这些抽象极大地简化了并行代码的编写。这些抽象通过限制实现并行性的方式来工作。例如，并行集合强制用户将计算表述为对集合的操作序列（如**map**、**reduce**和**filter**）。演员系统要求开发者从封装应用程序状态并通过传递消息进行通信的演员的角度来思考。
- en: It might seem paradoxical that restricting the programmer's freedom to write
    parallel code as they please avoids many of the problems associated with concurrency.
    However, limiting the number of ways in which a program behaves facilitates thinking
    about its behavior. For instance, if an actor is misbehaving, we know that the
    problem lies either in the code for this actor or in one of the messages that
    the actor receives.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 限制程序员自由编写他们想要的并行代码似乎有些矛盾，但这可以避免与并发相关的大多数问题。然而，限制程序行为的方式有助于思考其行为。例如，如果一个演员表现不佳，我们知道问题要么在于这个演员的代码，要么在于演员收到的某个消息。
- en: 'As an example of the power afforded by having coherent, restrictive abstractions,
    let''s use parallel collections to solve a simple probability problem. We will
    calculate the probability of getting at least 60 heads out of 100 coin tosses.
    We can estimate this using Monte Carlo: we simulate 100 coin tosses by drawing
    100 random Boolean values and check whether the number of true values is at least
    60\. We repeat this until results have converged to the required accuracy, or
    we get bored of waiting.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具有一致、限制性抽象的强大功能的例子，让我们使用并行集合来解决一个简单的概率问题。我们将计算在100次抛硬币中至少得到60次正面的概率。我们可以使用蒙特卡洛方法来估计这一点：通过抽取100个随机的布尔值来模拟100次抛硬币，并检查真值的数量是否至少为60。我们重复这个过程，直到结果收敛到所需的精度，或者我们等得不耐烦了。
- en: 'Let''s run through this in a Scala console:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Scala控制台中演示这个过程：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `trial` function runs a single set of 100 throws, returning the number
    of heads:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`trial`函数运行一组100次投掷，返回正面朝上的次数：'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To get our answer, we just need to repeat `trial` as many times as we can and
    aggregate the results. Repeating the same set of operations is ideally suited
    to parallel collections:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到我们的答案，我们只需要尽可能多地重复`trial`，并汇总结果。重复相同的操作集非常适合并行集合：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The probability is thus approximately 2.5% to 3%. All we had to do to distribute
    the calculation over every CPU in our computer is use the `par` method to parallelize
    the range `(0 until nTrials)`. This demonstrates the benefits of having a coherent
    abstraction: parallel collections let us trivially parallelize any computation
    that can be phrased in terms of higher-order functions on collections.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概率大约是2.5%到3%。我们只需使用`par`方法并行化范围`(0 until nTrials)`，就可以将计算分布到我们计算机的每个CPU上。这证明了具有一致抽象的好处：并行集合使我们能够轻易地将任何可以用集合上的高阶函数表述的计算并行化。
- en: Clearly, not every problem is as easy to parallelize as a simple Monte Carlo
    problem. However, by offering a rich set of intuitive abstractions, Scala makes
    writing parallel applications manageable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，并非每个问题都像简单的蒙特卡洛问题那样容易并行化。然而，通过提供丰富的直观抽象，Scala使得编写并行应用程序变得可行。
- en: Interoperability with Java
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Java的互操作性
- en: Scala runs on the Java virtual machine. The Scala compiler compiles programs
    to Java byte code. Thus, Scala developers have access to Java libraries natively.
    Given the phenomenal number of applications written in Java, both open source
    and as part of the legacy code in organizations, the interoperability of Scala
    and Java helps explain the rapid uptake of Scala.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Scala运行在Java虚拟机上。Scala编译器将程序编译成Java字节码。因此，Scala开发者可以原生地访问Java库。鉴于用Java编写的应用程序数量庞大，无论是开源的还是作为组织中的遗留代码的一部分，Scala和Java的互操作性有助于解释Scala的快速采用。
- en: 'Interoperability has not just been unidirectional: some Scala libraries, such
    as the Play framework, are becoming increasingly popular among Java developers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 互操作性不仅仅是单向的：一些Scala库，如Play框架，在Java开发者中越来越受欢迎。
- en: When not to use Scala
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时不使用Scala
- en: In the previous sections, we described how Scala's strong type system, preference
    for immutability, functional capabilities, and parallelism abstractions make it
    easy to write reliable programs and minimize the risk of unexpected behavior.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们描述了Scala的强类型系统、对不可变性的偏好、函数能力和并行抽象如何使得编写可靠的程序变得容易，并最小化意外行为的风险。
- en: What reasons might you have to avoid Scala in your next project? One important
    reason is familiarity. Scala introduces many concepts such as implicits, type
    classes, and composition using traits that might not be familiar to programmers
    coming from the object-oriented world. Scala's type system is very expressive,
    but getting to know it well enough to use its full power takes time and requires
    adjusting to a new programming paradigm. Finally, dealing with immutable data
    structures can feel alien to programmers coming from Java or Python.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有哪些理由在下一个项目中避免使用Scala？一个重要的原因是熟悉度。Scala引入了许多概念，例如隐式参数、类型类和通过特质使用组合，这些可能对来自面向对象世界的程序员来说并不熟悉。Scala的类型系统非常强大，但要充分了解它以发挥其全部功能需要时间，并需要适应新的编程范式。最后，对于来自Java或Python的程序员来说，处理不可变数据结构可能会感到不适应。
- en: Nevertheless, these are all drawbacks that can be overcome with time. Scala
    does fall short of the other data science languages in library availability. The
    IPython Notebook, coupled with matplotlib, is an unparalleled resource for data
    exploration. There are ongoing efforts to provide similar functionality in Scala
    (Spark Notebooks or Apache Zeppelin, for instance), but there are no projects
    with the same level of maturity. The type system can also be a minor hindrance
    when one is exploring data or trying out different models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些都是可以通过时间克服的缺点。Scala在库可用性方面确实不如其他数据科学语言。IPython Notebook与matplotlib结合使用，是数据探索的无与伦比的资源。有持续的努力在Scala中提供类似的功能（例如Spark
    Notebooks或Apache Zeppelin），但没有项目达到相同的成熟度。当探索数据或尝试不同的模型时，类型系统也可能成为轻微的障碍。
- en: Thus, in this author's biased opinion, Scala excels for more *permanent* programs.
    If you are writing a throwaway script or exploring data, you might be better served
    with Python. If you are writing something that will need to be reused and requires
    a certain level of provable correctness, you will find Scala extremely powerful.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个作者有偏见的观点中，Scala在编写更“永久”的程序方面表现出色。如果你正在编写一个一次性脚本或探索数据，你可能会发现Python更适合。如果你正在编写需要重用并需要一定程度的可证明正确性的东西，你会发现Scala非常强大。
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Now that the obligatory introduction is over, it is time to write some Scala
    code. In the next chapter, you will learn about leveraging Breeze for numerical
    computations with Scala. For our first foray into data science, we will use logistic
    regression to predict the gender of a person given their height and weight.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在必要的介绍已经结束，是时候编写一些Scala代码了。在下一章中，你将学习如何利用Breeze在Scala中进行数值计算。在我们的第一次数据科学探索中，我们将使用逻辑回归来预测给定一个人的身高和体重来预测其性别。
- en: References
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: By far, the best book on Scala is *Programming in Scala* by *Martin Odersky*,
    *Lex Spoon*, and *Bill Venners*. Besides being authoritative (*Martin Odersky*
    is the driving force behind Scala), this book is also approachable and readable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，关于Scala的最佳书籍是马丁·奥德斯基（Martin Odersky）、莱克斯·斯波恩（Lex Spoon）和比尔·文纳（Bill Venners）合著的《Programming
    in Scala》。这本书不仅权威（马丁·奥德斯基是Scala的推动力），而且易于接近和阅读。
- en: '*Scala Puzzlers* by *Andrew Phillips* and *Nermin Šerifović* provides a fun
    way to learn more advanced Scala.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 《Scala Puzzlers》由安德鲁·菲利普斯（Andrew Phillips）和内尔明·谢里福维奇（Nermin Šerifović）所著，提供了一种有趣的方式来学习更高级的Scala。
- en: '*Scala for Machine Learning* by *Patrick R. Nicholas* provides examples of
    how to write machine learning algorithms with Scala.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 《Scala for Machine Learning》由帕特里克·R·尼古拉斯（Patrick R. Nicholas）所著，提供了如何使用Scala编写机器学习算法的示例。
- en: Chapter 2. Manipulating Data with Breeze
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：使用Breeze操作数据
- en: 'Data science is, by and large, concerned with the manipulation of structured
    data. A large fraction of structured datasets can be viewed as tabular data: each
    row represents a particular instance, and columns represent different attributes
    of that instance. The ubiquity of tabular representations explains the success
    of spreadsheet programs like Microsoft Excel, or of tools like SQL databases.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学在很大程度上是关于结构化数据的操作。大量结构化数据集可以被视为表格数据：每一行代表一个特定的实例，而列代表该实例的不同属性。表格表示的普遍性解释了像Microsoft
    Excel这样的电子表格程序或像SQL数据库这样的工具的成功。
- en: To be useful to data scientists, a language must support the manipulation of
    columns or tables of data. Python does this through NumPy and pandas, for instance.
    Unfortunately, there is no single, coherent ecosystem for numerical computing
    in Scala that quite measures up to the SciPy ecosystem in Python.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要对数据科学家有用，一种语言必须支持数据列或表格的操作。例如，Python通过NumPy和pandas来实现这一点。不幸的是，在Scala中，没有一个单一、连贯的数值计算生态系统可以与Python中的SciPy生态系统相媲美。
- en: In this chapter, we will introduce Breeze, a library for fast linear algebra
    and manipulation of data arrays as well as many other features necessary for scientific
    computing and data science.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Breeze，这是一个用于快速线性代数和数据数组操作以及许多其他科学计算和数据科学所需特性的库。
- en: Code examples
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码示例
- en: 'The easiest way to access the code examples in this book is to clone the GitHub
    repository:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 访问本书中的代码示例最简单的方法是克隆GitHub仓库：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The code samples for each chapter are in a single, standalone folder. You may
    also browse the code online on GitHub.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每章的代码示例都在一个单独的独立文件夹中。你还可以在GitHub上在线浏览代码。
- en: Installing Breeze
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Breeze
- en: If you have downloaded the code examples for this book, the easiest way of using
    Breeze is to go into the `chap02` directory and type `sbt console` at the command
    line. This will open a Scala console in which you can import Breeze.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经下载了本书的代码示例，使用Breeze最简单的方法是进入`chap02`目录，并在命令行中输入`sbt console`。这将打开一个Scala控制台，你可以在其中导入Breeze。
- en: 'If you want to build a standalone project, the most common way of installing
    Breeze (and, indeed, any Scala module) is through SBT. To fetch the dependencies
    required for this chapter, copy the following lines to a file called `build.sbt`,
    taking care to leave an empty line after `scalaVersion`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要构建一个独立的项目，安装Breeze（以及任何Scala模块）最常见的方式是通过SBT。为了获取本章所需的依赖项，将以下行复制到名为`build.sbt`的文件中，注意在`scalaVersion`之后留一个空行：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Open a Scala console in the same directory as your `build.sbt` file by typing
    `sbt console` in a terminal. You can check that Breeze is working correctly by
    importing Breeze from the Scala prompt:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中输入`sbt console`，在`build.sbt`文件相同的目录中打开Scala控制台。您可以通过从Scala提示符导入Breeze来检查Breeze是否正常工作：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Getting help on Breeze
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Breeze的帮助
- en: This chapter gives a reasonably detailed introduction to Breeze, but it does
    not aim to give a complete API reference.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本章对Breeze进行了相当详细的介绍，但并不旨在提供完整的API参考。
- en: To get a full list of Breeze's functionality, consult the Breeze Wiki page on
    GitHub at [https://github.com/scalanlp/breeze/wiki](https://github.com/scalanlp/breeze/wiki).
    This is very complete for some modules and less complete for others. The source
    code ([https://github.com/scalanlp/breeze/](https://github.com/scalanlp/breeze/))
    is detailed and gives a lot of information. To understand how a particular function
    is meant to be used, look at the unit tests for that function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取Breeze功能的完整列表，请查阅GitHub上的Breeze Wiki页面[https://github.com/scalanlp/breeze/wiki](https://github.com/scalanlp/breeze/wiki)。对于某些模块来说，这是非常完整的，而对于其他模块来说则不那么完整。源代码（[https://github.com/scalanlp/breeze/](https://github.com/scalanlp/breeze/））详细且提供了大量信息。要了解特定函数的预期用法，请查看该函数的单元测试。
- en: Basic Breeze data types
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的Breeze数据类型
- en: Breeze is an extensive library providing fast and easy manipulation of arrays
    of data, routines for optimization, interpolation, linear algebra, signal processing,
    and numerical integration.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze是一个功能丰富的库，提供了对数据数组进行快速和简单操作的功能，包括优化、插值、线性代数、信号处理和数值积分的例程。
- en: The basic linear algebra operations underlying Breeze rely on the `netlib-java`
    library, which can use system-optimized **BLAS** and **LAPACK** libraries, if
    present. Thus, linear algebra operations in Breeze are often extremely fast. Breeze
    is still undergoing rapid development and can, therefore, be somewhat unstable.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze背后的基本线性代数操作依赖于`netlib-java`库，该库可以使用系统优化的**BLAS**和**LAPACK**库（如果存在）。因此，Breeze中的线性代数操作通常非常快。Breeze仍在快速发展中，因此可能有些不稳定。
- en: Vectors
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量
- en: 'Breeze makes manipulating one- and two-dimensional data structures easy. To
    start, open a Scala console through SBT and import Breeze:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze使得操作一维和二维数据结构变得简单。首先，通过SBT打开Scala控制台并导入Breeze：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s dive straight in and define a vector:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接定义一个向量：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We have just defined a three-element vector, `v`. Vectors are just one-dimensional
    arrays of data exposing methods tailored to numerical uses. They can be indexed
    like other Scala collections:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚定义了一个包含三个元素的向量，`v`。向量只是一维数据数组，提供了针对数值使用的定制方法。它们可以像其他Scala集合一样进行索引：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'They support element-wise operations with a scalar:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还支持与标量的逐元素操作：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'They also support element-wise operations with another vector:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还支持与另一个向量的逐元素操作：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Breeze makes writing vector operations intuitive and considerably more readable
    than the native Scala equivalent.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze使得编写向量操作直观，并且比Scala原生等价物更易于阅读。
- en: 'Note that Breeze will refuse (at compile time) to coerce operands to the correct
    type:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Breeze将在编译时拒绝将操作数强制转换为正确的类型：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It will also refuse (at runtime) to add vectors together if they have different
    lengths:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它还会在运行时拒绝将不同长度的向量相加：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Basic manipulation of vectors in Breeze will feel natural to anyone used to
    working with NumPy, MATLAB, or R.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在Breeze中对向量的基本操作对习惯于使用NumPy、MATLAB或R的人来说将感觉自然。
- en: 'So far, we have only looked at *element-wise* operators. These are all prefixed
    with a colon. All the usual suspects are present: `:+`, `:*`, `:-`, `:/`, `:%`
    (remainder), and `:^` (power) as well as Boolean operators. To see the full list
    of operators, have a look at the API documentation for `DenseVector` or `DenseMatrix`
    ([https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet](https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet)).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了*逐元素*运算符。这些运算符都是以冒号开头的。所有常见的运算符都存在：`:+`、`:*`、`:-`、`:/`、`:%`（余数）和`:^`（幂）以及布尔运算符。要查看运算符的完整列表，请查看`DenseVector`或`DenseMatrix`的API文档（[https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet](https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet)）。
- en: 'Besides element-wise operations, Breeze vectors support the operations you
    might expect of mathematical vectors, such as the dot product:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 除了逐元素操作外，Breeze向量还支持您可能期望的数学向量的操作，例如点积：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Pitfalls of element-wise operators**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**逐元素运算符的陷阱**'
- en: 'Besides the `:+` and `:-` operators for element-wise addition and subtraction
    that we have seen so far, we can also use the more traditional `+` and `-` operators:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们迄今为止看到的用于逐元素加法和减法的 `:+` 和 `:-` 运算符之外，我们还可以使用更传统的 `+` 和 `-` 运算符：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'One must, however, be very careful with operator precedence rules when mixing
    `:+` or `:*` with `:+` operators. The `:+` and `:*` operators have very low operator
    precedence, so they will be evaluated last. This can lead to some counter-intuitive
    behavior:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在将 `:+` 或 `:*` 与 `:+` 运算符混合使用时，必须非常小心运算符优先级规则。`:+` 和 `:*` 运算符的运算优先级非常低，因此它们将被最后评估。这可能会导致一些不符合直觉的行为：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By contrast, if we use `:+` instead of `+`, the mathematical precedence of
    operators is respected:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们使用 `:+` 而不是 `+`，则运算符的数学优先级将被尊重：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In summary, one should avoid mixing the `:+` style operators with the `+` style
    operators as much as possible.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，应尽可能避免将 `:+` 风格的运算符与 `+` 风格的运算符混合使用。
- en: Dense and sparse vectors and the vector trait
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稠密和稀疏矢量和矢量特性
- en: All the vectors we have looked at thus far have been dense vectors. Breeze also
    supports sparse vectors. When dealing with arrays of numbers that are mostly zero,
    it may be more computationally efficient to use sparse vectors. The point at which
    a vector has enough zeros to warrant switching to a sparse representation depends
    strongly on the type of operations, so you should run your own benchmarks to determine
    which type to use. Nevertheless, a good heuristic is that, if your vector is about
    90% zero, you may benefit from using a sparse representation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止查看的所有矢量都是稠密矢量。Breeze 也支持稀疏矢量。当处理主要由零组成的数字数组时，使用稀疏矢量可能更有效。矢量何时有足够的零以证明切换到稀疏表示取决于操作类型，因此你应该运行自己的基准测试以确定使用哪种类型。尽管如此，一个好的启发式方法是，如果你的矢量大约有
    90% 是零，那么使用稀疏表示可能会有所裨益。
- en: Sparse vectors are available in Breeze as the `SparseVector` and `HashVector`
    classes. Both these types support many of the same operations as `DenseVector`
    but use a different internal implementation. The `SparseVector` instances are
    very memory-efficient, but adding non-zero elements is slow. `HashVector` is more
    versatile, at the cost of an increase in memory footprint and computational time
    for iterating over non-zero elements. Unless you need to squeeze the last bits
    of memory out of your application, I recommend using `HashVector`. We will not
    discuss these further in this book, but the reader should find them straightforward
    to use if needed. `DenseVector`, `SparseVector`, and `HashVector` all implement
    the `Vector` trait, giving them a common interface.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矢量在 Breeze 中以 `SparseVector` 和 `HashVector` 类的形式提供。这两种类型都支持与 `DenseVector`
    相同的许多操作，但使用不同的内部实现。`SparseVector` 实例非常节省内存，但添加非零元素的速度较慢。`HashVector` 更灵活，但代价是内存占用和遍历非零元素的计算时间增加。除非你需要从应用程序中挤出最后一点内存，否则我建议使用
    `HashVector`。本书中不会进一步讨论这些内容，但如果需要，读者应该会发现它们的使用非常直观。`DenseVector`、`SparseVector`
    和 `HashVector` 都实现了 `Vector` 特性，从而提供了统一的接口。
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Breeze remains very experimental and, as of this writing, somewhat unstable.
    I have found dealing with specific implementations of the `Vector` trait, such
    as `DenseVector` or `SparseVector`, to be more reliable than dealing with the
    `Vector` trait directly. In this chapter, we will explicitly type every vector
    as `DenseVector`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze 仍然处于实验阶段，并且截至本文撰写时，有些不稳定。我发现处理 `Vector` 特性的特定实现（如 `DenseVector` 或 `SparseVector`）比直接处理
    `Vector` 特性更可靠。在本章中，我们将明确地将每个矢量类型指定为 `DenseVector`。
- en: Matrices
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵
- en: 'Breeze allows the construction and manipulation of two-dimensional arrays in
    a similar manner:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze 允许以类似的方式构建和操作二维数组：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Building vectors and matrices
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建矢量和矩阵
- en: 'We have seen how to explicitly build vectors and matrices by passing their
    values to the constructor (or rather, to the companion object''s `apply` method):
    `DenseVector(1.0, 2.0, 3.0)`. Breeze offers several other powerful ways of building
    vectors and matrices:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何通过将它们的值传递给构造函数（或者更确切地说，传递给伴随对象的 `apply` 方法）来显式构建矢量和矩阵：`DenseVector(1.0,
    2.0, 3.0)`。Breeze 提供了构建矢量和矩阵的几种其他强大方法：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `linspace` method (available in the `breeze.linalg` package object) creates
    a `Double` vector of equally spaced values. For instance, to create a vector of
    10 values distributed uniformly between `0` and `1`, perform the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`linspace` 方法（在 `breeze.linalg` 包对象中可用）创建一个等间隔值的 `Double` 矢量。例如，要创建一个在 `0`
    和 `1` 之间均匀分布的 10 个值的矢量，请执行以下操作：'
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `tabulate` method lets us construct vectors and matrices from functions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`tabulate` 方法允许我们通过函数构造向量和矩阵：'
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first argument to `DenseVector.tabulate` is the size of the vector, and
    the second is a function returning the value of the vector at a particular position.
    This is useful for creating ranges of data, among other things.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`DenseVector.tabulate` 的第一个参数是向量的长度，第二个参数是一个函数，它返回向量在特定位置的值。这有助于创建数据范围，以及其他用途。'
- en: 'The `rand` function lets us create random vectors and matrices:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`rand` 函数允许我们创建随机向量和矩阵：'
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we can construct vectors from Scala arrays:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以从 Scala 数组构造向量：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To construct vectors from other Scala collections, you must use the *splat*
    operator, `:_ *`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要从其他 Scala 集合构造向量，必须使用 *splat* 操作符，`:_ *`：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Advanced indexing and slicing
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级索引和切片
- en: We have already seen how to select a particular element in a vector `v` by its
    index with, for instance, `v(2)`. Breeze also offers several powerful methods
    for selecting parts of a vector.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何通过索引选择向量 `v` 中的特定元素，例如 `v(2)`。Breeze 还提供了几个强大的方法来选择向量的部分。
- en: 'Let''s start by creating a vector to play around with:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个向量来玩耍：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Unlike native Scala collections, Breeze vectors support negative indexing:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与原生 Scala 集合不同，Breeze 向量支持负索引：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Breeze lets us slice the vector using a range:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze 允许我们使用范围来切片向量：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Tip
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Indexing by a range returns a *view* of the original vector: when running `val
    v2 = v(1 to 3)`, no data is copied. This means that slicing is extremely efficient.
    Taking a slice of a huge vector does not increase the memory footprint at all.
    It also means that one should be careful updating a slice, since it will also
    update the original vector. We will discuss mutating vectors and matrices in a
    subsequent section in this chapter.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过范围进行索引返回原始向量的一个 *视图*：当运行 `val v2 = v(1 to 3)` 时，不会复制任何数据。这意味着切片非常高效。从大向量中取切片不会增加内存占用。这也意味着在更新切片时应该小心，因为它也会更新原始向量。我们将在本章后续部分讨论向量和矩阵的修改。
- en: 'Breeze also lets us select an arbitrary set of elements from a vector:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze 还允许我们从向量中选择任意一组元素：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This creates a `SliceVector`, which behaves like a `DenseVector` (both implement
    the `Vector` interface), but does not actually have memory allocated for values:
    it just knows how to map from its indices to values in its parent vector. One
    should think of `vSlice` as a specific view of `v`. We can materialize the view
    (give it its own data rather than acting as a lens through which `v` is viewed)
    by converting it to `DenseVector`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个 `SliceVector`，它类似于 `DenseVector`（两者都实现了 `Vector` 接口），但实际上并没有为值分配内存：它只是知道如何将其索引映射到父向量的值。可以将
    `vSlice` 视为 `v` 的一个特定视图。我们可以通过将其转换为 `DenseVector` 来具体化视图（给它自己的数据，而不是作为 `v` 的观察透镜）：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note that if an element of a slice is out of bounds, an exception will only
    be thrown when that element is accessed:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果切片中的某个元素超出了范围，只有在访问该元素时才会抛出异常：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, one can index vectors using Boolean arrays. Let''s start by defining
    an array:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以使用布尔数组来索引向量。让我们先定义一个数组：
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, `v(mask)` results in a view containing the elements of `v` for which
    `mask` is `true`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`v(mask)` 结果是一个包含 `v` 中 `mask` 为 `true` 的元素的视图：
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This can be used as a way of filtering certain elements in a vector. For instance,
    to select the elements of `v` which are less than `3.0`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用作过滤向量中某些元素的一种方式。例如，要选择小于 `3.0` 的 `v` 的元素：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Matrices can be indexed in much the same way as vectors. Matrix indexing functions
    take two arguments—the first argument selects the row(s) and the second one slices
    the column(s):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的索引方式与向量非常相似。矩阵索引函数接受两个参数——第一个参数选择行（s），第二个参数切片列（s）：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can also mix different slicing types for rows and columns:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以混合不同类型的行和列切片：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Note how, in this case, Breeze returns a vector. In general, slicing returns
    the following objects:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，Breeze 返回一个向量。一般来说，切片操作返回以下对象：
- en: A scalar when single indices are passed as the row and column arguments
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当传递单个索引作为行和列参数时，返回一个标量
- en: A vector when the row argument is a range and the column argument is a single
    index
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当行参数是一个范围，列参数是一个单个索引时，返回一个向量
- en: A vector transpose when the column argument is a range and the row argument
    is a single index
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当列参数是一个范围，行参数是一个单个索引时，返回一个向量的转置
- en: A matrix otherwise
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则返回一个矩阵
- en: 'The symbol `::` can be used to indicate *every element along a particular direction*.
    For instance, we can select the second column of `m`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 符号`::`可以用来表示*沿特定方向的每个元素*。例如，我们可以选择`m`的第二列：
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Mutating vectors and matrices
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改向量和矩阵
- en: 'Breeze vectors and matrices are mutable. Most of the slicing operations described
    above can also be used to set elements of a vector or matrix:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze中的向量和矩阵是可变的。上述描述的大多数切片操作也可以用来设置向量和矩阵的元素：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We are not limited to mutating single elements. In fact, all the indexing operations
    outlined above can be used to set the elements of vectors or matrices. When mutating
    slices of vectors or matrices, use the element-wise assignment operator, `:=`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于修改单个元素。实际上，上述概述的所有索引操作都可以用来设置向量和矩阵的元素。在修改向量和矩阵的切片时，使用逐元素赋值运算符`:=`：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The assignment operator, `:=`, works like other element-wise operators in Breeze.
    If the right-hand side is a scalar, it will automatically be broadcast to a vector
    of the given shape:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 赋值运算符`:=`在Breeze中像其他逐元素运算符一样工作。如果右侧是一个标量，它将自动广播到给定形状的向量：
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'All element-wise operators have an update counterpart. For instance, the `:+=`
    operator acts like the element-wise addition operator `:+`, but also updates its
    left-hand operand:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所有逐元素运算符都有一个更新对应的运算符。例如，`:+=`运算符类似于逐元素加法运算符`:+`，但也会更新其左侧的操作数：
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Notice how the update operator updates the vector in place and returns it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更新操作是如何就地更新向量并返回它的。
- en: 'We have learnt how to slice vectors and matrices in Breeze to create new views
    of the original data. These views are not independent of the vector they were
    created from—updating the view will update the underlying vector and vice-versa.
    This is best illustrated with an example:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何在Breeze中切片向量和矩阵来创建原始数据的新的视图。这些视图并不独立于它们创建的向量——更新视图将更新底层向量，反之亦然。这最好用一个例子来说明：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This quickly becomes intuitive if we remember that, when we create a vector
    or matrix, we are creating a view of an underlying data array rather than creating
    the data itself:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们记住，当我们创建一个向量和矩阵时，我们实际上是在创建一个底层数据数组的视图，而不是创建数据本身，这会很快变得直观：
- en: '![Mutating vectors and matrices](img/image01159.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![修改向量和矩阵](img/image01159.jpeg)'
- en: 'A vector slice `v(0 to 6 by 2)` of the `v` vector is just a different view
    of the array underlying `v`. The view itself contains no data. It just contains
    pointers to the data in the original array. Internally, the view is just stored
    as a pointer to the underlying data and a recipe for iterating over that data:
    in the case of this slice, the recipe is just "start at the first element of the
    underlying data and go to the seventh element of the underlying data in steps
    of two".'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`v`向量的切片`v(0 to 6 by 2)`只是`v`底层数组的另一种视图。这个视图本身不包含数据。它只包含指向原始数组中数据的指针。内部，视图只是存储为指向底层数据的指针和一个遍历该数据的配方：在这个切片的情况下，配方就是“从底层数据的第一元素开始，以两步的间隔到达底层数据的第七元素”。'
- en: 'Breeze offers a `copy` function for when we want to create independent copies
    of data. In the previous example, we can construct a copy of `viewEvens` as:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要创建数据的独立副本时，Breeze提供了一个`copy`函数。在之前的例子中，我们可以构建`viewEvens`的副本如下：
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can now update `copyEvens` independently of `v`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以独立于`v`更新`copyEvens`。
- en: Matrix multiplication, transposition, and the orientation of vectors
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵乘法、转置和向量的方向
- en: So far, we have mostly looked at element-wise operations on vectors and matrices.
    Let's now look at matrix multiplication and related operations.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注了向量和矩阵的逐元素操作。现在让我们看看矩阵乘法和相关操作。
- en: 'The matrix multiplication operator is `*`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法运算符是`*`：
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Besides matrix-matrix multiplication, we can use the matrix multiplication
    operator between matrices and vectors. All vectors in Breeze are column vectors.
    This means that, when multiplying matrices and vectors together, a vector should
    be viewed as an (*n * 1*) matrix. Let''s walk through an example of matrix-vector
    multiplication. We want the following operation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了矩阵-矩阵乘法之外，我们还可以在矩阵和向量之间使用矩阵乘法运算符。Breeze中的所有向量都是列向量。这意味着，当矩阵和向量相乘时，向量应被视为一个(*n
    * 1*)矩阵。让我们通过一个矩阵-向量乘法的例子来了解一下。我们想要以下操作：
- en: '![Matrix multiplication, transposition, and the orientation of vectors](img/image01160.jpeg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵乘法、转置和向量的方向](img/image01160.jpeg)'
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'By contrast, if we wanted:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们想要：
- en: '![Matrix multiplication, transposition, and the orientation of vectors](img/image01161.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵乘法、转置和向量的方向](img/image01161.jpeg)'
- en: 'We must convert `v` to a row vector. We can do this using the transpose operation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将 `v` 转换为一个行向量。我们可以使用转置操作来完成此操作：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note that the type of `v.t` is `Transpose[DenseVector[_]]`. A `Transpose[DenseVector[_]]`
    behaves in much the same way as a `DenseVector` as far as element-wise operations
    are concerned, but it does not support mutation or slicing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`v.t` 的类型是 `Transpose[DenseVector[_]]`。在逐元素操作方面，`Transpose[DenseVector[_]]`
    与 `DenseVector` 几乎以相同的方式表现，但它不支持修改或切片。
- en: Data preprocessing and feature engineering
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: We have now discovered the basic components of Breeze. In the next few sections,
    we will apply them to real examples to understand how they fit together to form
    a robust base for data science.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经发现了 Breeze 的基本组件。在接下来的几节中，我们将将它们应用于实际例子，以了解它们如何组合在一起形成数据科学的一个强大基础。
- en: An important part of data science involves preprocessing datasets to construct
    useful features. Let's walk through an example of this. To follow this example
    and access the data, you will need to download the code examples for the book
    ([www.github.com/pbugnion/s4ds](http://www.github.com/pbugnion/s4ds)).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的一个重要部分涉及预处理数据集以构建有用的特征。让我们通过一个例子来了解这个过程。为了跟随这个例子并访问数据，您需要下载本书的代码示例（[www.github.com/pbugnion/s4ds](http://www.github.com/pbugnion/s4ds)）。
- en: 'You will find, in directory `chap02/data/` of the code attached to this book,
    a CSV file with true heights and weights as well as self-reported heights and
    weights for 181 men and women. The original dataset was collected as part of a
    study on body image. Refer to the following link for more information: [http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html](http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在本书附带的代码的 `chap02/data/` 目录中找到一个 CSV 文件，其中包含 181 名男性和女性的真实身高和体重，以及自我报告的身高和体重。原始数据集是作为一项关于身体形象的研究的一部分收集的。有关更多信息，请参阅以下链接：[http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html](http://vincentarelbundock.github.io/Rdatasets/doc/car/Davis.html)。
- en: 'There is a helper function in the package provided with the book to load the
    data into Breeze arrays:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本书提供的包中有一个辅助函数，用于将数据加载到 Breeze 数组中：
- en: '[PRE53]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `data` object contains five vectors, each 181 element long:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 对象包含五个向量，每个向量长度为 181：'
- en: '`data.genders`: A `Char` vector describing the gender of the participants'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.genders`：一个描述参与者性别的 `Char` 向量'
- en: '`data.heights`: A `Double` vector of the true height of the participants'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.heights`：一个包含参与者真实身高的 `Double` 向量'
- en: '`data.weights`: A `Double` vector of the true weight of the participants'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.weights`：一个包含参与者真实体重的 `Double` 向量'
- en: '`data.reportedHeights`: A `Double` vector of the self-reported height of the
    participants'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.reportedHeights`：一个包含参与者自我报告身高的 `Double` 向量'
- en: '`data.reportedWeights`: A `Double` vector of the self-reported weight of the
    participants'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.reportedWeights`：一个包含参与者自我报告体重的 `Double` 向量'
- en: 'Let''s start by counting the number of men and women in the study. We will
    define an array that contains just `''M''` and do an element-wise comparison with
    `data.genders`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先计算研究中男性和女性的数量。我们将定义一个只包含 `'M'` 的数组，并与 `data.genders` 进行逐元素比较：
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `isMale` vector is the same length as `data.genders`. It is `true` where
    the participant is male, and `false` otherwise. We can use this Boolean array
    as a mask for the other arrays in the dataset (remember that `vector(mask)` selects
    the elements of `vector` where mask is `true`). Let''s get the height of the men
    in our dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`isMale` 向量与 `data.genders` 的长度相同。当参与者为男性时，它为 `true`，否则为 `false`。我们可以使用这个布尔数组作为数据集中其他数组的掩码（记住
    `vector(mask)` 选择 `vector` 中掩码为 `true` 的元素）。让我们获取我们数据集中男性的身高：'
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To count the number of men in our dataset, we can use the indicator function.
    This transforms a Boolean array into an array of doubles, mapping `false` to `0.0`
    and `true` to `1.0`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算我们数据集中男性的数量，我们可以使用指示函数。这个函数将布尔数组转换为一个双精度浮点数数组，将 `false` 映射到 `0.0`，将 `true`
    映射到 `1.0`：
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s calculate the `mean` height of men and women in the experiment. We can
    calculate the mean of a vector using `mean(v)`, which we can access by importing
    `breeze.stats._`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算实验中男性和女性的平均身高。我们可以使用 `mean(v)` 计算向量的平均值，通过导入 `breeze.stats._` 来访问它：
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To calculate the `mean` height of the men, we can use our `isMale` array to
    slice `data.heights`; `data.heights(isMale)` is a view of the `data.heights` array
    with all the height values for the men:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算男性的平均身高，我们可以使用我们的`isMale`数组来切片`data.heights`；`data.heights(isMale)`是`data.heights`数组的一个视图，其中包含所有男性的身高值：
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'As a somewhat more involved example, let''s look at the discrepancy between
    real and reported weight for both men and women in this experiment. We can get
    an array of the percentage difference between the reported weight and the true
    weight:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作为稍微复杂一点的例子，让我们看看在这个实验中男性和女性实际体重与报告体重之间的差异。我们可以得到一个报告体重与真实体重之间百分比差异的数组：
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Notice how Breeze's overloading of mathematical operators allows us to manipulate
    data arrays easily and elegantly.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Breeze对数学运算符的重载如何使我们能够轻松优雅地操作数据数组。
- en: 'We can now calculate the mean and standard deviation of this array for men:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算这个数组中男性的平均值和标准差：
- en: '[PRE60]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can also calculate the fraction of men who overestimated their height:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算高估自己身高的男性的比例：
- en: '[PRE61]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: There are thus ten men who believe they are taller than they actually are. The
    element-wise AND operator `:&` returns a vector that is true for all indices for
    which both its arguments are true. The vector `overReportMask :& isMale` is thus
    true for all participants that are male and over-reported their height.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有十名男性认为自己比实际更高。逐元素AND运算符`:&`返回一个向量，对于其两个参数都为真的所有索引，该向量是真实的。因此，向量`overReportMask
    :& isMale`对于所有报告身高超过实际身高的男性参与者都是真实的。
- en: Breeze – function optimization
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Breeze – 函数优化
- en: Having studied feature engineering, let's now look at the other end of the data
    science pipeline. Typically, a machine learning algorithm defines a loss function
    that is a function of a set of parameters. The value of the loss function represents
    how well the model fits the data. The parameters are then optimized to minimize
    (or maximize) the loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了特征工程之后，我们现在来看看数据科学管道的另一端。通常，机器学习算法定义了一个损失函数，该函数是一组参数的函数。损失函数的值表示模型拟合数据的程度。然后，参数被优化以最小化（或最大化）损失函数。
- en: In [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed Machine Learning
    with MLlib"), *Distributed Machine Learning with MLlib*, we will look at **MLlib**,
    a machine learning library that contains many well-known algorithms. Often, we
    don't need to worry about optimizing loss functions directly since we can rely
    on the machine learning algorithms provided by MLlib. It is nevertheless useful
    to have a basic knowledge of optimization.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第12章](part0117.xhtml#aid-3FIHQ2 "第12章。使用MLlib的分布式机器学习")《使用MLlib的分布式机器学习》中，我们将探讨**MLlib**，这是一个包含许多知名算法的机器学习库。通常，我们不需要担心直接优化损失函数，因为我们可以依赖MLlib提供的机器学习算法。然而，了解优化基础知识仍然很有用。
- en: 'Breeze has an `optimize` module that contains functions for finding a local
    minimum:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze有一个`optimize`模块，其中包含用于寻找局部最小值的函数：
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s create a toy function that we want to optimize:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个我们想要优化的玩具函数：
- en: '![Breeze – function optimization](img/image01162.jpeg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![Breeze – 函数优化](img/image01162.jpeg)'
- en: 'We can represent this function in Scala as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下方式在Scala中表示这个函数：
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Most local optimizers also require the gradient of the function being optimized.
    The gradient is a vector of the same dimension as the arguments to the function.
    In our case, the gradient is:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数局部优化器也要求提供正在优化的函数的梯度。梯度是与函数的参数相同维度的向量。在我们的例子中，梯度是：
- en: '![Breeze – function optimization](img/image01163.jpeg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![Breeze – 函数优化](img/image01163.jpeg)'
- en: 'We can represent the gradient in Breeze with a function that takes a vector
    argument and returns a vector of the same length:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个接受向量参数并返回相同长度向量的函数来表示梯度：
- en: '[PRE64]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For instance, at the point `(1, 1, 1)`, we have:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在点`(1, 1, 1)`处，我们有：
- en: '[PRE65]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s set up the optimization problem. Breeze''s optimization methods require
    that we pass in an implementation of the `DiffFunction` trait with a single method,
    `calculate`. This method must return a tuple of the function and its gradient:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置优化问题。Breeze的优化方法要求我们传递一个实现了`DiffFunction`特质的实现，该特质只有一个方法，即`calculate`。此方法必须返回一个包含函数及其梯度的元组：
- en: '[PRE66]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We are now ready to run the optimization. The optimize module provides a `minimize`
    function that does just what we want. We pass it `optTrait` and a starting point
    for the optimization:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行优化了。优化模块提供了一个`minimize`函数，它正好符合我们的需求。我们传递给它`optTrait`和优化的起始点：
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The true minimum is at `(0.0, 0.0, 0.0)`. The optimizer therefore correctly
    finds the minimum.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 真实最小值在`(0.0, 0.0, 0.0)`。因此，优化器正确地找到了最小值。
- en: The `minimize` function uses the **L-BFGS** method to run the optimization by
    default. It takes several additional arguments to control the optimization. We
    will explore these in the next sections.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`minimize`函数默认使用**L-BFGS**方法运行优化。它接受几个额外的参数来控制优化。我们将在下一节中探讨这些参数。'
- en: Numerical derivatives
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值微分
- en: 'In the previous example, we specified the gradient of `f` explicitly. While
    this is generally good practice, calculating the gradient of a function can often
    be tedious. Breeze provides a gradient approximation function using finite differences.
    Reusing the same objective function `def f(xs:DenseVector[Double]) = sum(xs :^
    2.0)` as in the previous section:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们明确指定了`f`的梯度。虽然这通常是好的做法，但计算函数的梯度往往很繁琐。Breeze提供了一个使用有限差分的梯度近似函数。重用与上一节相同的目标准函数`def
    f(xs:DenseVector[Double]) = sum(xs :^ 2.0)`：
- en: '[PRE68]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The trait `approxOptTrait` has a `gradientAt` method that returns an approximation
    to the gradient at a point:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 特性`approxOptTrait`有一个`gradientAt`方法，它返回在一点处的梯度近似值：
- en: '[PRE69]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Note that this can be quite inaccurate. The `ApproximateGradientFunction` constructor
    takes an `epsilon` optional argument that controls the size of the step taken
    when calculating the finite differences. Changing the value of `epsilon` can improve
    the accuracy of the finite difference algorithm.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这可能会相当不准确。`ApproximateGradientFunction`构造函数接受一个可选的`epsilon`参数，该参数控制计算有限差分时步长的大小。改变`epsilon`的值可以提高有限差分算法的准确性。
- en: 'The `ApproximateGradientFunction` instance implements the `DiffFunction` trait.
    It can therefore be passed to `minimize` directly:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`ApproximateGradientFunction`实例实现了`DiffFunction`特性。因此，它可以直接传递给`minimize`。'
- en: '[PRE70]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This, again, gives a result close to zero, but somewhat further away than when
    we specified the gradient explicitly. In general, it will be significantly more
    efficient and more accurate to calculate the gradient of a function analytically
    than to rely on Breeze's numerical gradient. It is probably best to only use the
    numerical gradient during data exploration or to check analytical gradients.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次给出了接近零的结果，但比我们明确指定梯度时稍远一些。一般来说，通过解析计算函数的梯度将比依赖Breeze的数值梯度更有效、更准确。可能最好只在数据探索期间或检查解析梯度时使用数值梯度。
- en: Regularization
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: The `minimize` function takes many optional arguments relevant to machine learning
    algorithms. In particular, we can instruct the optimizer to use a regularization
    parameter when performing the optimization. Regularization introduces a penalty
    in the loss function to prevent the parameters from growing arbitrarily. This
    is useful to avoid overfitting. We will discuss regularization in greater detail
    in [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed Machine Learning
    with MLlib"), *Distributed Machine Learning with MLlib*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`minimize`函数接受许多与机器学习算法相关的可选参数。特别是，我们可以指示优化器在执行优化时使用正则化参数。正则化在损失函数中引入惩罚，以防止参数任意增长。这有助于避免过拟合。我们将在[第12章](part0117.xhtml#aid-3FIHQ2
    "第12章。使用MLlib的分布式机器学习")中更详细地讨论正则化，*使用MLlib的分布式机器学习*。'
- en: 'For instance, to use `L2Regularization` with a hyperparameter of `0.5`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要使用具有超参数`0.5`的`L2Regularization`：
- en: '[PRE71]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The regularization makes no difference in this case, since the parameters are
    zero at the minimum.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，正则化没有区别，因为参数在最小值处为零。
- en: To see a list of optional arguments that can be passed to `minimize`, consult
    the Breeze documentation online.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可以传递给`minimize`的可选参数列表，请查阅在线的Breeze文档。
- en: An example – logistic regression
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个例子——逻辑回归
- en: 'Let''s now imagine we want to build a classifier that takes a person''s **height**
    and **weight** and assigns a probability to their being **Male** or **Female**.
    We will reuse the height and weight data introduced earlier in this chapter. Let''s
    start by plotting the dataset:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们想象我们想要构建一个分类器，它接受一个人的**身高**和**体重**，并为他们被分配为**男性**或**女性**的概率。我们将重用本章前面引入的身高和体重数据。让我们先绘制数据集：
- en: '![An example – logistic regression](img/image01164.jpeg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![一个例子——逻辑回归](img/image01164.jpeg)'
- en: Height versus weight data for 181 men and women
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 181名男性和女性的身高与体重数据
- en: There are many different algorithms for classification. A first glance at the
    data shows that we can, approximately, separate men from women by drawing a straight
    line across the plot. A linear method is therefore a reasonable initial attempt
    at classification. In this section, we will use logistic regression to build a
    classifier.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法有很多种。初步观察数据表明，我们可以通过在图上画一条直线来大致区分男性和女性。因此，线性方法是对分类的合理初步尝试。在本节中，我们将使用逻辑回归来构建分类器。
- en: A detailed explanation of logistic regression is beyond the scope of this book.
    The reader unfamiliar with logistic regression is referred to *The Elements of
    Statistical Learning* by *Hastie*, *Tibshirani*, and *Friedman*. We will just
    give a brief summary here.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的详细解释超出了本书的范围。对逻辑回归不熟悉的读者可参考 *Hastie*、*Tibshirani* 和 *Friedman* 所著的 *《统计学习的要素》*。我们在这里只做简要概述。
- en: 'Logistic regression estimates the probability of a given *height* and *weight*
    belonging to a *male* with the following sigmoid function:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归使用以下sigmoid函数估计给定身高和体重属于男性的概率：
- en: '![An example – logistic regression](img/image01165.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01165.jpeg)'
- en: 'Here, *f* is a linear function:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f* 是一个线性函数：
- en: '![An example – logistic regression](img/image01166.jpeg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01166.jpeg)'
- en: 'Here, ![An example – logistic regression](img/image01167.jpeg) is an array
    of parameters that we need to determine using the training set. If we consider
    the height and weight as a *features = (height, weight)* matrix, we can re-write
    the sigmoid kernel *f* as a matrix multiplication of the *features* matrix with
    the *params* vector:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![一个示例 – 逻辑回归](img/image01167.jpeg) 是我们需要使用训练集确定的参数数组。如果我们将身高和体重视为 *features
    = (height, weight)* 矩阵，我们可以将sigmoid核 *f* 重新写为 *features* 矩阵与 *params* 向量的矩阵乘法：
- en: '![An example – logistic regression](img/image01168.jpeg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01168.jpeg)'
- en: 'To simplify this expression further, it is common to add a dummy feature whose
    value is always *1* to the *features* matrix. We can then multiply *params(0)*
    by this feature, allowing us to write the entire sigmoid kernel *f* as a single
    matrix-vector multiplication:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步简化这个表达式，通常会在 *features* 矩阵中添加一个值始终为 *1* 的虚拟特征。然后我们可以将 *params(0)* 乘以这个特征，这样我们就可以将整个sigmoid核
    *f* 写作一个单一的矩阵-向量乘法：
- en: '![An example – logistic regression](img/image01169.jpeg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01169.jpeg)'
- en: The feature matrix, *features*, is now a (*181 * 3*) matrix, where each row
    is *(1, height, weight)* for a particular participant.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 特征矩阵，*features*，现在是一个 (*181 * 3*) 矩阵，其中每一行代表一个特定参与者的 *(1, height, weight)*。
- en: 'To find the optimal values of the parameters, we can maximize the likelihood
    function, *L(params|features)*. The likelihood takes a given set of parameter
    values as input and returns the probability that these particular parameters gave
    rise to the training set. For a set of parameters and associated probability function
    *P(male|features[i]),* the likelihood is:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到参数的最优值，我们可以最大化似然函数，*L(params|features)*。似然函数以一组给定的参数值作为输入，并返回这些特定参数导致训练集的概率。对于一组参数和相关的概率函数
    *P(male|features[i])*，似然函数为：
- en: '![An example – logistic regression](img/image01170.jpeg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01170.jpeg)'
- en: If we magically know, ahead of time, the gender of everyone in the population,
    we can assign *P(male)=1* for the men and *P(male)=0* for the women. The likelihood
    function would then be **1**. Conversely, any uncertainty leads to a reduction
    in the likelihood function. If we choose a set of parameters that consistently
    lead to classification errors (low *P(male)* for men or high *P(male)* for women),
    the likelihood function drops to *0*.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们神奇地提前知道人口中每个人的性别，我们可以将男性的概率分配为 *P(male)=1*，女性的概率分配为 *P(male)=0*。此时，似然函数将是
    **1**。相反，任何不确定性都会导致似然函数的降低。如果我们选择一组参数，这些参数始终导致分类错误（男性低 *P(male)* 或女性高 *P(male)*），则似然函数将降至
    *0*。
- en: The maximum likelihood corresponds to those values of the parameters most likely
    to describe the observed data. Thus, to find the parameters that best describe
    our training set, we just need to find parameters that maximize *L(params|features)*.
    However, maximizing the likelihood function itself is very rarely done, since
    it involves multiplying many small values together, which quickly leads to floating
    point underflow. It is best to maximize the log of the likelihood, which has the
    same maximum as the likelihood. Finally, since most optimization algorithms are
    geared to minimize a function rather than maximize it, we will minimize![An example
    – logistic regression](img/image01171.jpeg).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然对应于最有可能描述观察数据的参数值。因此，为了找到最能描述我们的训练集的参数，我们只需要找到最大化 *L(params|features)* 的参数。然而，最大化似然函数本身很少被做，因为它涉及到将许多小的值相乘，这很快就会导致浮点下溢。最好最大化似然的对数，它与似然有相同的最大值。最后，由于大多数优化算法都是针对最小化函数而不是最大化函数而设计的，因此我们将最小化
    ![一个示例 – 逻辑回归](img/image01171.jpeg)。
- en: 'For logistic regression, this is equivalent to minimizing:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，这相当于最小化：
- en: '![An example – logistic regression](img/image01172.jpeg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01172.jpeg)'
- en: Here, the sum runs over all participants in the training data, ![An example
    – logistic regression](img/image01173.jpeg) is a vector ![An example – logistic
    regression](img/image01174.jpeg) of the *i*-th observation in the training set,
    and ![An example – logistic regression](img/image01175.jpeg) is *1* if the person
    is male, and *0* if the participant is female.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，求和遍历训练数据中的所有参与者，![一个示例 – 逻辑回归](img/image01173.jpeg) 是训练集中第 *i*- 次观察的向量 ![一个示例
    – 逻辑回归](img/image01174.jpeg)，而 ![一个示例 – 逻辑回归](img/image01175.jpeg) 如果该人是男性则为 *1*，如果是女性则为
    *0*。
- en: 'To minimize the *Cost* function, we must also know its gradient with respect
    to the parameters. This is:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化 *成本* 函数，我们还必须知道其相对于参数的梯度。这是：
- en: '![An example – logistic regression](img/image01176.jpeg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 – 逻辑回归](img/image01176.jpeg)'
- en: We will start by rescaling the height and weight by their mean and standard
    deviation. While this is not strictly necessary for logistic regression, it is
    generally good practice. It facilitates the optimization and would become necessary
    if we wanted to use regularization methods or build superlinear features (features
    that allow the boundary separating men from women to be curved rather than a straight
    line).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过它们的平均值和标准差来缩放身高和体重。虽然这对逻辑回归来说不是严格必要的，但通常是一个好的实践。它有助于优化，如果我们想使用正则化方法或构建超线性特征（允许将男性和女性分开的边界是曲线而不是直线），则成为必要。
- en: 'For this example, we will move away from the Scala shell and write a standalone
    Scala script. Here''s the full code listing. Don''t worry if this looks daunting.
    We will break it up into manageable chunks in a minute:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将离开 Scala shell 并编写一个独立的 Scala 脚本。以下是完整的代码列表。不要担心这看起来令人畏惧。我们将在下一分钟将其分解成可管理的块：
- en: '[PRE72]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'That was a mouthful! Let''s take this one step at a time. After the obvious
    imports, we start with:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很复杂！让我们一步一步来。在明显的导入之后，我们开始：
- en: '[PRE73]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'By extending the built-in `App` trait, we tell Scala to treat the entire object
    as a `main` function. This just cuts out `def main(args:Array[String])` boilerplate.
    We then load the data and rescale the height and weight to have a `mean` of zero
    and a standard deviation of one:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩展内置的 `App` 特质，我们告诉 Scala 将整个对象视为一个 `main` 函数。这仅仅消除了 `def main(args:Array[String])`
    的样板代码。然后我们加载数据并将身高和体重缩放到具有 `mean` 为零和标准差为之一的值：
- en: '[PRE74]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The `rescaledHeights` and `rescaledWeights` vectors will be the features of
    our model. We can now build the training set matrix for this model. This is a
    (*181 * 3*) matrix, for which the *i*-th row is `(1, height(i), weight(i))`, corresponding
    to the values of the height and weight for the *i*th participant. We start by
    transforming both `rescaledHeights` and `rescaledWeights` from vectors to (*181
    * 1*) matrices
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`rescaledHeights` 和 `rescaledWeights` 向量将是我们的模型特征。现在我们可以为这个模型构建训练集矩阵。这是一个 (*181
    * 3*) 矩阵，其中第 *i*- 行是 `(1, height(i), weight(i))`，对应于第 *i* 个参与者的身高和体重值。我们首先将 `rescaledHeights`
    和 `rescaledWeights` 从向量转换为 (*181 * 1*) 矩阵'
- en: '[PRE75]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We must also create a (*181 * 1*) matrix containing just *1* to act as the
    dummy feature. We can do this using:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须创建一个只包含 *1* 的 (*181 * 1*) 矩阵作为虚拟特征。我们可以使用以下方法来完成：
- en: '[PRE76]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We now need to combine our three (*181 * 1*) matrices together into a single
    feature matrix of shape (*181 * 3*). We can use the `horzcat` method to concatenate
    the three matrices together:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将我们的三个 (*181 * 1*) 矩阵组合成一个形状为 (*181 * 3*) 的单个特征矩阵。我们可以使用 `horzcat` 方法将三个矩阵连接起来：
- en: '[PRE77]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The final step in the data preprocessing stage is to create the target variable.
    We need to convert the `data.genders` vector to a vector of ones and zeros. We
    assign a value of one for men and zero for women. Thus, our classifier will predict
    the probability that any given person is male. We will use the `.values.map` method,
    a method equivalent to the `.map` method on Scala collections:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理阶段的最后一步是创建目标变量。我们需要将 `data.genders` 向量转换为包含一和零的向量。我们将男性赋值为1，女性赋值为0。因此，我们的分类器将预测任何给定的人是男性的概率。我们将使用
    `.values.map` 方法，这是Scala集合上的 `.map` 方法的等价方法：
- en: '[PRE78]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Note that we could also have used the indicator function which we discovered
    earlier:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以使用我们之前发现的指示函数：
- en: '[PRE79]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This results in the allocation of a temporary array, `maleVector`, and might
    therefore increase the program's memory footprint if there were many participants
    in the experiment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致分配一个临时数组，`maleVector`，因此如果实验中有许多参与者，这可能会增加程序的内存占用。
- en: 'We now have a matrix representing the training set and a vector denoting the
    target variable. We can write the loss function that we want to minimize. As mentioned
    previously, we will minimize ![An example – logistic regression](img/image01177.jpeg).
    The loss function takes as input a set of values for the linear coefficients and
    returns a number indicating how well those values of the linear coefficients fit
    the training data:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个表示训练集的矩阵和一个表示目标变量的向量。我们可以写出我们想要最小化的损失函数。如前所述，我们将最小化 ![一个示例 - 逻辑回归](img/image01177.jpeg)。损失函数接受一组线性系数的值作为输入，并返回一个数字，表示这些线性系数的值如何拟合训练数据：
- en: '[PRE80]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Note that we use `log1p(x)` to calculate *log(1+x)*. This is robust to underflow
    for small values of `x`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用 `log1p(x)` 来计算 *log(1+x)*。这对于 `x` 的较小值是稳健的，不会下溢。
- en: 'Let''s explore the cost function:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索成本函数：
- en: '[PRE81]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: We can see that the cost function is somewhat lower for slightly positive values
    of the height and weight parameters. This indicates that the likelihood function
    is larger for slightly positive values of the height and weight. This, in turn,
    implies (as we expect from the plot) that people who are taller and heavier than
    average are more likely to be male.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于稍微正的身高和体重参数值，成本函数略低。这表明对于稍微正的身高和体重值，似然函数更大。这反过来又意味着（正如我们根据图所期望的），身高和体重高于平均值的人更有可能是男性。
- en: 'We also need a function that calculates the gradient of the loss function,
    since that will help with the optimization:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数来计算损失函数的梯度，因为这有助于优化：
- en: '[PRE82]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Having defined the loss function and gradient, we are now in a position to
    set up the optimization:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了损失函数和梯度之后，我们现在可以设置优化：
- en: '[PRE83]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'All that is left now is to run the optimization. The cost function for logistic
    regression is convex (it has a single minimum), so the starting point for optimization
    is irrelevant in principle. In practice, it is common to start with a coefficient
    vector that is zero everywhere (equating to assigning a 0.5 probability of being
    male to every participant):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是运行优化。逻辑回归的成本函数是凸函数（它有一个唯一的极小值），所以在原则上优化起点是不相关的。在实践中，通常从一个系数向量开始，该向量在所有地方都是零（相当于将每个参与者的男性概率赋值为0.5）：
- en: '[PRE84]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This returns the vector of optimal parameters:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回最优参数的向量：
- en: '[PRE85]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How can we interpret the values of the optimal parameters? The coefficients
    for the height and weight are both positive, indicating that people who are taller
    and heavier are more likely to be male.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释最优参数的值？身高和体重的系数都是正的，这表明身高和体重较高的人更有可能是男性。
- en: 'We can also get the decision boundary (the line separating (height, weight)
    pairs more likely to belong to a woman from (height, weight) pairs more likely
    to belong to a man) directly from the coefficients. The decision boundary is:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以直接从系数中得到决策边界（分隔更可能属于女性的（身高，体重）对和更可能属于男性的（身高，体重）对的线）。决策边界是：
- en: '![An example – logistic regression](img/image01178.jpeg)![An example – logistic
    regression](img/image01179.jpeg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例 - 逻辑回归](img/image01178.jpeg)![一个示例 - 逻辑回归](img/image01179.jpeg)'
- en: Height and weight data (shifted by the mean and rescaled by the standard deviation).
    The orange line is the logistic regression decision boundary. Logistic regression
    predicts that individuals above the boundary are male.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 身高和体重数据（通过均值偏移并按标准差缩放）。橙色线是逻辑回归决策边界。逻辑回归预测边界上方的个体是男性。
- en: Towards re-usable code
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向可重用代码迈进
- en: In the previous section, we performed all of the computation in a single script.
    While this is fine for data exploration, it means that we cannot reuse the logistic
    regression code that we have built. In this section, we will start the construction
    of a machine learning library that you can reuse across different projects.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们在单个脚本中执行了所有计算。虽然这对于数据探索来说是不错的，但这意味着我们无法重用我们已经构建的逻辑回归代码。在本节中，我们将开始构建一个机器学习库，你可以在不同的项目中重用它。
- en: 'We will factor the logistic regression algorithm out into its own class. We
    construct a `LogisticRegression` class:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逻辑回归算法提取到它自己的类中。我们构建一个`LogisticRegression`类：
- en: '[PRE86]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The class takes, as input, a matrix representing the training set and a vector
    denoting the target variable. Notice how we assign these to `vals`, meaning that
    they are set on class creation and will remain the same until the class is destroyed.
    Of course, the `DenseMatrix` and `DenseVector` objects are mutable, so the values
    that `training` and `target` point to might change. Since programming best practice
    dictates that mutable state makes reasoning about program behavior difficult,
    we will avoid taking advantage of this mutability.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 该类接受一个表示训练集的矩阵和一个表示目标变量的向量作为输入。注意我们如何将它们分配给`vals`，这意味着它们在类创建时设置，并且将在类销毁之前保持不变。当然，`DenseMatrix`和`DenseVector`对象是可变的，所以`training`和`target`指向的值可能会改变。由于编程最佳实践规定可变状态会使程序行为难以推理，我们将避免利用这种可变性。
- en: 'Let''s add a method that calculates the cost function and its gradient:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一个计算成本函数及其梯度的方法：
- en: '[PRE87]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We are now all set up to run the optimization to calculate the coefficients
    that best reproduce the training set. In traditional object-oriented languages,
    we might define a `getOptimalCoefficients` method that returns a `DenseVector`
    of the coefficients. Scala, however, is more elegant. Since we have defined the
    `training` and `target` attributes as `vals`, there is only one possible set of
    values of the optimal coefficients. We could, therefore, define a `val optimalCoefficients
    = ???` class attribute that holds the optimal coefficients. The problem with this
    is that it forces all the computation to happen when the instance is constructed.
    This will be unexpected for the user and might be wasteful: if the user is only
    interested in accessing the cost function, for instance, the time spent minimizing
    it will be wasted. The solution is to use a `lazy val`. This value will only be
    evaluated when the client code requests it:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好运行优化过程，以计算最佳系数，这些系数能够最好地重现训练集。在传统的面向对象语言中，我们可能会定义一个`getOptimalCoefficients`方法，该方法返回一个包含系数的`DenseVector`。然而，Scala却更加优雅。由于我们已经将`training`和`target`属性定义为`vals`，因此最优系数的可能值集只有一个。因此，我们可以定义一个`val
    optimalCoefficients = ???`类属性来保存最优系数。问题是这会强制所有计算都在实例构造时发生。这对用户来说可能是意外的，也可能造成浪费：如果用户只对访问成本函数感兴趣，例如，用于最小化的时间将会被浪费。解决方案是使用`lazy
    val`。这个值只有在客户端代码请求时才会被评估：
- en: '[PRE88]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'To help with the calculation of the coefficients, we will define a private
    helper method:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助计算系数，我们将定义一个私有辅助方法：
- en: '[PRE89]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: We have refactored the logistic regression into its own class, that we can reuse
    across different projects.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将逻辑回归重构为它自己的类，我们可以在不同的项目中重用它。
- en: If we were planning on reusing the height-weight data, we could, similarly,
    refactor it into a class of its own that facilitates data loading, feature scaling,
    and any other functionality that we find ourselves reusing often.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打算重用身高体重数据，我们可以将其重构为一个自己的类，该类便于数据加载、特征缩放以及任何我们经常重用的其他功能。
- en: Alternatives to Breeze
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Breeze的替代方案
- en: 'Breeze is the most feature-rich and approachable Scala framework for linear
    algebra and numeric computation. However, do not take my word for it: experiment
    with other libraries for tabular data. In particular, I recommend trying *Saddle*,
    which provides a `Frame` object similar to data frames in pandas or R. In the
    Java world, the *Apache Commons Maths library* provides a very rich toolkit for
    numerical computation. In [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark*, [Chapter
    11](part0106.xhtml#aid-352RK2 "Chapter 11. Spark SQL and DataFrames"), *Spark
    SQL and DataFrames*, and [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed
    Machine Learning with MLlib"), *Distributed Machine Learning with MLlib*, we will
    explore *Spark* and *MLlib*, which allow the user to run distributed machine learning
    algorithms.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze是线性代数和数值计算方面功能最丰富且易于使用的Scala框架。然而，不要仅凭我的话就下结论：尝试其他表格数据库。特别是，我推荐尝试*Saddle*，它提供了一个类似于pandas或R中的数据框的`Frame`对象。在Java领域，*Apache
    Commons Maths库*提供了一个非常丰富的数值计算工具包。在第10章（[part0097.xhtml#aid-2SG6I1 "第10章. 使用Spark进行分布式批量处理"](part0097.xhtml#aid-2SG6I1
    "第10章. 使用Spark进行分布式批量处理")）、第11章（[part0106.xhtml#aid-352RK2 "第11章. Spark SQL和DataFrames"](part0106.xhtml#aid-352RK2
    "第11章. Spark SQL和DataFrames")）和第12章（[part0117.xhtml#aid-3FIHQ2 "第12章. 使用MLlib进行分布式机器学习"](part0117.xhtml#aid-3FIHQ2
    "第12章. 使用MLlib进行分布式机器学习")）中，我们将探讨*Spark*和*MLlib*，它们允许用户运行分布式机器学习算法。
- en: Summary
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This concludes our brief overview of Breeze. We have learned how to manipulate
    basic Breeze data types, how to use them for linear algebra, and how to perform
    convex optimization. We then used our knowledge to clean a real dataset and performed
    logistic regression on it.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Breeze的简要概述。我们学习了如何操作基本的Breeze数据类型，如何使用它们进行线性代数，以及如何执行凸优化。然后我们运用我们的知识清理了一个真实的数据集，并在其上执行了逻辑回归。
- en: In the next chapter, we will discuss breeze-viz, a plotting library for Scala.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论breeze-viz，这是一个用于Scala的绘图库。
- en: References
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*The Elements of Statistical Learning*, by *Hastie*, *Tibshirani*, and *Friedman*,
    gives a lucid, practical description of the mathematical underpinnings of machine
    learning. Anyone aspiring to do more than mindlessly apply machine learning algorithms
    as black boxes ought to have a well-thumbed copy of this book.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 《*统计学习的要素*》，由*Hastie*、*Tibshirani*和*Friedman*著，对机器学习的数学基础进行了清晰、实用的描述。任何希望不仅仅盲目地将机器学习算法作为黑盒应用的人，都应该拥有一本翻阅得破旧的这本书。
- en: '*Scala for Machine Learning*, by *Patrick R. Nicholas*, describes practical
    implementations of many useful machine learning algorithms in Scala.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 《*Scala for Machine Learning*》，由*Patrick R. Nicholas*著，描述了许多有用的机器学习算法在Scala中的实际应用。
- en: The Breeze documentation ([https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart)),
    API docs ([http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package)),
    and source code ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze))
    provide the most up-to-date sources of documentation on Breeze.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze的文档（[https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart)）、API文档（[http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package)）和源代码（[https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)）提供了关于Breeze的最新文档资源。
- en: Chapter 3. Plotting with breeze-viz
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 使用breeze-viz进行绘图
- en: 'Data visualization is an integral part of data science. Visualization needs
    fall into two broad categories: during the development and validation of new models
    and, at the end of the pipeline, to distill meaning from the data and the models
    to provide insight to external stakeholders.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是数据科学的一个基本组成部分。可视化需求可以分为两大类：在开发和新模型验证期间，以及在管道末尾，从数据和模型中提取意义，为外部利益相关者提供洞察。
- en: 'The two types of visualizations are quite different. At the data exploration
    and model development stage, the most important feature of a visualization library
    is its ease of use. It should take as few steps as possible to go from having
    data as arrays of numbers (or CSVs or in a database) to having data displayed
    on a screen. The lifetime of graphs is also quite short: once the data scientist
    has learned all he can from the graph or visualization, it is normally discarded.
    By contrast, when developing visualization widgets for external stakeholders,
    one is willing to tolerate increased development time for greater flexibility.
    The visualizations can have significant lifetime, especially if the underlying
    data changes over time.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种可视化类型相当不同。在数据探索和模型开发阶段，可视化库最重要的特性是它的易用性。它应该尽可能少地执行步骤，从拥有数字数组（或CSV文件或在数据库中）的数据到在屏幕上显示数据。图表的寿命也相当短：一旦数据科学家从图表或可视化中学到了所有他能学到的知识，它通常就会被丢弃。相比之下，当为外部利益相关者开发可视化小部件时，人们愿意为了更大的灵活性而容忍增加的开发时间。可视化可以具有相当长的寿命，特别是如果底层数据随时间变化的话。
- en: The tool of choice in Scala for the first type of visualization is breeze-viz.
    When developing visualizations for external stakeholders, web-based visualizations
    (such as D3) and Tableau tend to be favored.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，用于第一种可视化的首选工具是breeze-viz。当为外部利益相关者开发可视化时，基于Web的可视化（如D3）和Tableau往往更受欢迎。
- en: In this chapter, we will explore breeze-viz. In [Chapter 14](part0139.xhtml#aid-44HU61
    "Chapter 14. Visualization with D3 and the Play Framework"), *Visualization with
    D3 and the Play Framework*, we will learn how to build Scala backends for JavaScript
    visualizations.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索breeze-viz。在[第14章](part0139.xhtml#aid-44HU61 "第14章。使用D3和Play Framework进行可视化")，*使用D3和Play
    Framework进行可视化*中，我们将学习如何为JavaScript可视化构建Scala后端。
- en: Breeze-viz is (no points for guessing) Breeze's visualization library. It wraps
    **JFreeChart**, a very popular Java charting library. Breeze-viz is still very
    experimental. In particular, it is much less feature-rich than matplotlib in Python,
    or R or MATLAB. Nevertheless, breeze-viz allows access to the underlying JFreeChart
    objects so one can always fall back to editing these objects directly. The syntax
    for breeze-viz is inspired by MATLAB and matplotlib.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze-viz是（无需猜测）Breeze的可视化库。它封装了**JFreeChart**，一个非常流行的Java图表库。Breeze-viz仍然处于实验阶段。特别是，它比Python中的matplotlib、R或MATLAB的功能要少得多。尽管如此，breeze-viz允许访问底层的JFreeChart对象，因此用户可以始终回退到直接编辑这些对象。breeze-viz的语法受到了MATLAB和matplotlib的启发。
- en: Diving into Breeze
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Breeze
- en: 'Let''s get started. We will work in the Scala console, but a program similar
    to this example is available in `BreezeDemo.scala` in the examples corresponding
    to this chapter. Create a `build.sbt` file with the following lines:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。我们将在Scala控制台中工作，但与这个例子类似的程序可以在本章对应的示例中的`BreezeDemo.scala`文件中找到。创建一个包含以下行的`build.sbt`文件：
- en: '[PRE90]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Start an `sbt` console:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 启动`sbt`控制台：
- en: '[PRE91]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let''s start by plotting a sigmoid curve, ![Diving into Breeze](img/image01180.jpeg).
    We will first generate the data using Breeze. Recall that the `linspace` method
    creates a vector of doubles, uniformly distributed between two values:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先绘制一个sigmoid曲线，![深入Breeze](img/image01180.jpeg)。我们将首先使用Breeze生成数据。回想一下，`linspace`方法创建了一个在两个值之间均匀分布的双精度浮点数向量：
- en: '[PRE92]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We now have the data ready for plotting. The first step is to create a figure:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了用于绘制的数据。第一步是创建一个图形：
- en: '[PRE93]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This creates an empty Java Swing window (which may appear on your taskbar or
    equivalent). A figure can contain one or more plots. Let''s add a plot to our
    figure:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个空的Java Swing窗口（它可能出现在你的任务栏或等效位置）。一个图形可以包含一个或多个绘图。让我们向我们的图形添加一个绘图：
- en: '[PRE94]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'For now, let''s ignore the `0` passed as argument to `.subplot`. We can add
    data points to our `plot`:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们忽略传递给`.subplot`的`0`作为参数。我们可以向我们的`plot`添加数据点：
- en: '[PRE95]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The `plot` function takes two arguments, corresponding to the *x* and *y* values
    of the data series to be plotted. To view the changes, you need to refresh the
    figure:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot`函数接受两个参数，对应于要绘制的数据系列的*x*和*y*值。要查看更改，您需要刷新图形：'
- en: '[PRE96]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Look at the Swing window now. You should see a beautiful sigmoid, similar to
    the one below. Right-clicking on the window lets you interact with the plot and
    save the image as a PNG:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看Swing窗口。你应该看到一个漂亮的sigmoid曲线，类似于下面的一个。在窗口上右键单击可以让你与绘图交互并将图像保存为PNG：
- en: '![Diving into Breeze](img/image01181.jpeg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![深入Breeze](img/image01181.jpeg)'
- en: 'You can also save the image programmatically as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以按以下方式程序化地保存图像：
- en: '[PRE97]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Breeze-viz currently only supports exporting to PNG.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze-viz 目前仅支持导出为 PNG 格式。
- en: Customizing plots
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义绘图
- en: 'We now have a curve on our chart. Let''s add a few more:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们图表上有一条曲线。让我们再添加几条：
- en: '[PRE98]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Looking at the figure now, you should see all three curves in different colors.
    Notice that we named the data series as we added them to the plot, using the `name=""`
    keyword argument. To view the names, we must set the `legend` attribute:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在查看这个图，你应该看到三种不同颜色的所有三条曲线。注意，我们在添加到绘图时命名了数据系列，使用 `name=""` 关键字参数。要查看名称，我们必须设置
    `legend` 属性：
- en: '[PRE99]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '![Customizing plots](img/image01182.jpeg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![自定义绘图](img/image01182.jpeg)'
- en: 'Our plot still leaves a lot to be desired. Let''s start by restricting the
    range of the *x* axis to remove the bands of white space on either side of the
    plot:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的绘图还有很多可以改进的地方。让我们首先将 *x* 轴的范围限制，以移除绘图两侧的空白带：
- en: '[PRE100]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Now, notice how, while the *x* ticks are sensibly spaced, there are only two
    *y* ticks: at *0* and *1*. It would be useful to have ticks every *0.1* increment.
    Breeze does not provide a way to set this directly. Instead, it exposes the underlying
    JFreeChart Axis object belonging to the current plot:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意，虽然 *x* 刻度合理地分布，但只有两个 *y* 刻度：在 *0* 和 *1* 处。每增加 *0.1* 就有一个刻度将很有用。Breeze
    没有提供直接设置此功能的方法。相反，它暴露了当前绘图所属的底层 JFreeChart 轴对象：
- en: '[PRE101]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The `Axis` object supports a `.setTickUnit` method that lets us set the tick
    spacing:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '`Axis` 对象支持一个 `.setTickUnit` 方法，允许我们设置刻度间距：'
- en: '[PRE102]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: JFreeChart allows extensive customization of the `Axis` object. For a full list
    of methods available, consult the JFreeChart documentation ([http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html](http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html)).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: JFreeChart 允许对 `Axis` 对象进行广泛的定制。有关可用方法的完整列表，请参阅 JFreeChart 文档 ([http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html](http://www.jfree.org/jfreechart/api/javadoc/org/jfree/chart/axis/Axis.html))。
- en: 'Let''s also add a vertical line at *x=0* and a horizontal line at *f(x)=1*.
    We will need to access the underlying JFreeChart plot to add these lines. This
    is available (somewhat confusingly) as the `.plot` attribute in our Breeze `Plot`
    object:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 *x=0* 处添加一条垂直线，在 *f(x)=1* 处添加一条水平线。我们需要访问底层 JFreeChart 绘图来添加这些线条。这在我们的
    Breeze `Plot` 对象中作为 `.plot` 属性（有些令人困惑）可用：
- en: '[PRE103]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We can use the `.addDomainMarker` and `.addRangeMarker` methods to add vertical
    and horizontal lines to JFreeChart `XYPlot` objects:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `.addDomainMarker` 和 `.addRangeMarker` 方法向 JFreeChart `XYPlot` 对象添加垂直和水平线条：
- en: '[PRE104]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Let''s also add labels to the axes:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也给坐标轴添加标签：
- en: '[PRE105]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'If you have run all these commands, you should have a graph that looks like
    this:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了所有这些命令，你应该有一个看起来像这样的图形：
- en: '![Customizing plots](img/image01183.jpeg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![自定义绘图](img/image01183.jpeg)'
- en: We now know how to customize the basic building blocks of a graph. The next
    step is to learn how to change how curves are drawn.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道如何自定义图形的基本构建块。下一步是学习如何更改曲线的绘制方式。
- en: Customizing the line type
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义线条类型
- en: So far, we have just plotted lines using the default settings. Breeze lets us
    customize how lines are drawn, at least to some extent.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是使用默认设置绘制了线条。Breeze 允许我们自定义线条的绘制方式，至少在一定程度上。
- en: For this example, we will use the height-weight data discussed in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze*.
    We will use the Scala shell here for demonstrative purposes, but you will find
    a program in `BreezeDemo.scala` that follows the example shell session.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用在 [第 2 章](part0018.xhtml#aid-H5A41 "第 2 章。使用 Breeze 操作数据") 中讨论的高度-体重数据，*使用
    Breeze 操作数据*。我们将在这里使用 Scala shell 进行演示，但你将在 `BreezeDemo.scala` 程序中找到一个遵循示例 shell
    会话的程序。
- en: 'The code examples for this chapter come with a module for loading the data,
    `HWData.scala`, that loads the data from the CSVs:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码示例附带一个用于加载数据的模块，`HWData.scala`，它从 CSV 文件中加载数据：
- en: '[PRE106]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Let''s create a scatter plot of the heights against the weights:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个身高与体重的散点图：
- en: '[PRE107]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'This produces a scatter-plot of the height-weight data:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了身高-体重数据的散点图：
- en: '![Customizing the line type](img/image01184.jpeg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![自定义线条类型](img/image01184.jpeg)'
- en: 'Note that we passed a third argument to the `plot` method, `''+''`. This controls
    the plotting style. As of this writing, there are three available styles: `''-''`
    (the default), `''+''`, and `''.''`. Experiment with these to see what they do.
    Finally, we pass a `colorcode="black"` argument to control the color of the line.
    This is either a color name or an RGB triple, written as a string. Thus, to plot
    red points, we could have passed `colorcode="[255,0,0]"`.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们传递了第三个参数给`plot`方法，`'+'`。这控制了绘图样式。截至本文写作时，有三种可用的样式：`'-'`（默认），`'+'`和`'.'`。尝试这些样式以查看它们的作用。最后，我们传递一个`colorcode="black"`参数来控制线的颜色。这可以是颜色名称或RGB三元组，以字符串形式编写。因此，要绘制红色点，我们可以传递`colorcode="[255,0,0]"`。
- en: 'Looking at the height-weight plot, there is clearly a trend between height
    and weight. Let''s try and fit a straight line through the data points. We will
    fit the following function:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 观察身高体重图，身高和体重之间显然存在趋势。让我们尝试通过数据点拟合一条直线。我们将拟合以下函数：
- en: '![Customizing the line type](img/image01185.jpeg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![自定义线型](img/image01185.jpeg)'
- en: Note
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Scientific literature suggests that it would be better to fit something more
    like ![Customizing the line type](img/image01186.jpeg). You should find it straightforward
    to fit a quadratic line to the data, should you wish to.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 科学文献表明，拟合类似![自定义线型](img/image01186.jpeg)的东西会更好。如果你愿意，应该可以轻松地将二次线拟合到数据上。
- en: 'We will use Breeze''s least squares function to find the values of `a` and
    `b`. The `leastSquares` method expects an input matrix of features and a target
    vector, just like the `LogisticRegression` class that we defined in the previous
    chapter. Recall that in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating
    Data with Breeze"), *Manipulating Data with Breeze*, when we prepared the training
    set for logistic regression classification, we introduced a dummy feature that
    was one for every participant to provide the degree of freedom for the *y* intercept.
    We will use the same approach here. Our feature matrix, therefore, contains two
    columns—one that is `1` everywhere and one for the height:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Breeze的最小二乘函数来找到`a`和`b`的值。`leastSquares`方法期望一个特征矩阵的输入和一个目标向量，就像我们在上一章中定义的`LogisticRegression`类一样。回想一下，在[第2章](part0018.xhtml#aid-H5A41
    "第2章。使用Breeze操作数据")中，*使用Breeze操作数据*，当我们为逻辑回归分类准备训练集时，我们引入了一个虚拟特征，每个参与者都是1，以提供*y*截距的自由度。我们在这里将使用相同的方法。因此，我们的特征矩阵包含两列——一列在所有地方都是`1`，另一列是身高：
- en: '[PRE108]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The `leastSquares` method returns an instance of `LeastSquareRegressionResult`,
    which contains a `coefficients` attribute containing the coefficients that best
    fit the data:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '`leastSquares`方法返回一个`LeastSquareRegressionResult`实例，它包含一个`coefficients`属性，包含最佳拟合数据的系数：'
- en: '[PRE109]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The best-fit line is therefore:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳拟合线是：
- en: '![Customizing the line type](img/image01187.jpeg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![自定义线型](img/image01187.jpeg)'
- en: 'Let''s extract the coefficients. An elegant way of doing this is to use Scala''s
    pattern matching capabilities:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取系数。一种优雅的方法是使用Scala的模式匹配功能：
- en: '[PRE110]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: By writing `val Array(a, b) = ...`, we are telling Scala that the right-hand
    side of the expression is a two-element array and to bind the first element of
    that array to the value `a` and the second to the value `b`. See [Appendix](part0149.xhtml#aid-4E33Q2
    "Appendix A. Pattern Matching and Extractors"), *Pattern Matching and Extractors*,
    for a discussion of pattern matching.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编写`val Array(a, b) = ...`，我们告诉Scala表达式右侧是一个包含两个元素的数组，并将该数组的第一个元素绑定到值`a`，第二个元素绑定到`b`。参见[附录](part0149.xhtml#aid-4E33Q2
    "附录A。模式匹配和提取器")，*模式匹配和提取器*，以了解模式匹配的讨论。
- en: 'We can now add the best-fit line to our graph. We start by generating evenly-spaced
    dummy height values:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将最佳拟合线添加到我们的图中。我们首先生成均匀间隔的虚拟身高值：
- en: '[PRE111]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Let''s also add the equation for the best-fit line to the graph as an annotation.
    We will first generate the label:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也将最佳拟合线的方程添加到图中作为注释。我们首先生成标签：
- en: '[PRE112]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'To add an annotation, we must access the underlying JFreeChart plot:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加注释，我们必须访问底层的JFreeChart绘图：
- en: '[PRE113]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The `XYTextAnnotation` constructor takes three parameters: the annotation string
    and a pair of (*x*, *y*) coordinates defining the centre of the annotation on
    the graph. The coordinates of the annotation are expressed in the coordinate system
    of the data. Thus, calling `new XYTextAnnotation(label, 175.0, 105.0)` generates
    an annotation whose centroid is at the point corresponding to a height of 175
    cm and weight of 105 kg:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '`XYTextAnnotation` 构造函数接受三个参数：注释字符串和定义注释在图上中心的 (*x*, *y*) 坐标对。注释的坐标以数据的坐标系表示。因此，调用
    `new XYTextAnnotation(label, 175.0, 105.0)` 将生成一个中心在对应 175 厘米高度和 105 公斤重量的点的注释：'
- en: '![Customizing the line type](img/image01188.jpeg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![自定义线型](img/image01188.jpeg)'
- en: More advanced scatter plots
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更高级的散点图
- en: Breeze-viz offers a `scatter` function that adds a significant degree of customization
    to scatter plots. In particular, we can use the size and color of the marker points
    to add additional dimensions of information to the plot.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze-viz 提供了一个 `scatter` 函数，为散点图添加了显著的定制程度。特别是，我们可以使用标记点的大小和颜色向图中添加额外的信息维度。
- en: The `scatter` function takes, as its first two arguments, collections of *x*
    and *y* points. The third argument is a function mapping an integer `i` to a `Double`
    indicating the size of the *ith* point. The size of the point is measured in units
    of the *x* axis. If you have the sizes as a Scala collection or a Breeze vector,
    you can use that collection's `apply` method as the function. Let's see how this
    works in practice.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`scatter` 函数接受其前两个参数为 *x* 和 *y* 点的集合。第三个参数是一个函数，它将整数 `i` 映射到表示 *第 i 个* 点大小的
    `Double` 值。点的大小以 *x* 轴的单位来衡量。如果你有 Scala 集合或 Breeze 向量的尺寸，你可以使用该集合的 `apply` 方法作为该函数。让我们看看这在实践中是如何工作的。'
- en: 'As with the previous examples, we will use the REPL, but you can find a sample
    program in `BreezeDemo.scala`:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，我们将使用 REPL，但你可以在 `BreezeDemo.scala` 中找到一个示例程序：
- en: '[PRE114]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Selecting custom colors works in a similar manner: we pass in a `colors` argument
    that maps an integer index to a `java.awt.Paint` object. Using these directly
    can be cumbersome, so Breeze provides some default palettes. For instance, the
    `GradientPaintScale` maps doubles in a given domain to a uniform color gradient.
    Let''s map doubles in the range `0.0` to `1.0` to the colors between red and green:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 选择自定义颜色的工作方式类似：我们传递一个 `colors` 参数，它将整数索引映射到 `java.awt.Paint` 对象。直接使用这些可能比较麻烦，所以
    Breeze 提供了一些默认调色板。例如，`GradientPaintScale` 将给定域内的双精度值映射到均匀的颜色渐变。让我们将范围 `0.0` 到
    `1.0` 的双精度值映射到红色和绿色之间的颜色：
- en: '[PRE115]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Besides the `GradientPaintScale`, breeze-viz provides a `CategoricalPaintScale`
    class for categorical palettes. For an overview of the different palettes, consult
    the source file `PaintScale.scala` at `scala`: [https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala).'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `GradientPaintScale`，breeze-viz 还提供了一个 `CategoricalPaintScale` 类用于分类调色板。有关不同调色板的概述，请参阅
    `PaintScale.scala` 源文件，位于 `scala`：[https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/PaintScale.scala)。
- en: 'Let''s use our newfound knowledge to draw a multicolor scatter plot. We will
    assume the same initialization as the previous example. We will assign a random
    color to each point:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用我们新获得的知识来绘制一个多彩散点图。我们将假设与上一个示例相同的初始化。我们将为每个点分配一个随机颜色：
- en: '[PRE116]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '![More advanced scatter plots](img/image01189.jpeg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![更高级的散点图](img/image01189.jpeg)'
- en: Multi-plot example – scatterplot matrix plots
  id: totrans-481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多图示例 – 散点矩阵图
- en: In this section, we will learn how to have several plots in the same figure.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在同一图中拥有多个图。
- en: 'The key new method that allows multiple plots in the same figure is `fig.subplot(nrows,
    ncols, plotIndex)`. This method, an overloaded version of the `fig.subplot` method
    we have been using up to now, both sets the number of rows and columns in the
    figure and returns a specific subplot. It takes three arguments:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 允许在同一图中绘制多个图的关键新方法是 `fig.subplot(nrows, ncols, plotIndex)`。这个方法是我们迄今为止一直在使用的
    `fig.subplot` 方法的重载版本，它既设置了图中的行数和列数，又返回一个特定的子图。它接受三个参数：
- en: '`nrows`: The number of rows of subplots in the figure'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nrows`：图中子图行数'
- en: '`ncols`: The number of columns of subplots in the figure'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ncols`：图中子图列数'
- en: '`plotIndex`: The index of the plot to return'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotIndex`：要返回的图的索引'
- en: 'Users familiar with MATLAB or matplotlib will note that the `.subplot` method
    is identical to the eponymous methods in these frameworks. This might seem a little
    complex, so let''s look at an example (you will find the code for this in `BreezeDemo.scala`):'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉MATLAB或matplotlib的用户会注意到`.subplot`方法与这些框架中的同名方法相同。这可能会显得有点复杂，所以让我们看一个例子（你可以在`BreezeDemo.scala`中找到这个代码）：
- en: '[PRE117]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Running this example produces the following plot:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例将生成以下图表：
- en: '![Multi-plot example – scatterplot matrix plots](img/image01190.jpeg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![多图示例 – 散点图矩阵图表](img/image01190.jpeg)'
- en: Now that we have a basic grasp of how to add several subplots to the same figure,
    let's do something a little more interesting. We will write a class to draw scatterplot
    matrices. These are useful for exploring correlations between different features.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经基本掌握了如何在同一张图上添加多个子图的方法，让我们做一些更有趣的事情。我们将编写一个类来绘制散点图矩阵。这些对于探索不同特征之间的相关性非常有用。
- en: If you are not familiar with scatterplot matrices, have a look at the figure
    at the end of this section for an idea of what we are constructing. The idea is
    to build a square matrix of scatter plots for each pair of features. Element (*i*,
    *j*) in the matrix is a scatter plot of feature *i* against feature *j*. Since
    a scatter plot of a variable against itself is of limited use, one normally draws
    histograms of each feature along the diagonal. Finally, since a scatter plot of
    feature *i* against feature *j* contains the same information as a scatter plot
    of feature *j* against feature *i*, one normally only plots the upper triangle
    or the lower triangle of the matrix.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉散点图矩阵，请查看本节末尾的图表，以了解我们正在构建的内容。想法是为每对特征构建一个散点图的正方形矩阵。矩阵中的元素（*i*，*j*）是特征*i*与特征*j*的散点图。由于一个变量与其自身的散点图用途有限，通常会在对角线上绘制每个特征的直方图。最后，由于特征*i*与特征*j*的散点图包含与特征*j*与特征*i*的散点图相同的信息，通常只绘制矩阵的上三角或下三角。
- en: 'Let''s start by writing functions for the individual plots. These will take
    a `Plot` object referencing the correct subplot and vectors of the data to plot:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先编写用于单个图表的函数。这些函数将接受一个`Plot`对象，它引用正确的子图，以及要绘制的数据的向量：
- en: '[PRE118]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Notice the use of `hist(data)` to draw a histogram. The argument to `hist` must
    be a vector of data points. The `hist` method will bin these and represent them
    as a histogram.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到使用了`hist(data)`来绘制直方图。`hist`函数的参数必须是一个数据点的向量。`hist`方法会将这些数据点分箱，并以直方图的形式表示它们。
- en: 'Now that we have the machinery for drawing individual plots, we just need to
    wire everything together. The tricky part is to know how to select the correct
    subplot for a given row and column position in the matrix. We can select a single
    plot by calling `fig.subplot(nrows, ncolumns, plotIndex)`, but translating from
    a (*row*, *column*) index pair to a single `plotIndex` is not obvious. The plots
    are numbered in increasing order, first from left to right, then from top to bottom:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了绘制单个图表的机制，我们只需要将所有东西连接起来。难点是知道如何根据矩阵中的给定行和列位置选择正确的子图。我们可以通过调用`fig.subplot(nrows,
    ncolumns, plotIndex)`来选择单个图表，但将（*行*，*列*）索引对转换为单个`plotIndex`并不明显。图表按顺序编号，首先是左到右，然后是上到下：
- en: '[PRE119]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Let''s write a short function to select a plot at a (*row*, *column*) index
    pair:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个简短的功能来选择一个在（*行*，*列*）索引对上的图表：
- en: '[PRE120]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'We are now in a position to draw the matrix plot itself:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经可以绘制矩阵图本身了：
- en: '[PRE121]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Let''s write an example for our class. We will use the height-weight data again:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的类编写一个示例。我们将再次使用身高体重数据：
- en: '[PRE122]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Running this through SBT produces the following plot:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在SBT中运行此代码将生成以下图表：
- en: '![Multi-plot example – scatterplot matrix plots](img/image01191.jpeg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![多图示例 – 散点图矩阵图表](img/image01191.jpeg)'
- en: Managing without documentation
  id: totrans-506
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无文档管理
- en: 'Breeze-viz is unfortunately rather poorly documented. This can make the learning
    curve somewhat steep. Fortunately, it is still quite a small project: at the time
    of writing, there are just ten source files ([https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot](https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot)).
    A good way to understand exactly what breeze-viz does is to read the source code.
    For instance, to see what methods are available on a `Plot` object, read the source
    file `Plot.scala`. If you need functionality beyond that provided by Breeze, consult
    the documentation for JFreeChart to discover if you can implement what you need
    by accessing the underlying JFreeChart objects.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze-viz 的文档不幸地相当不完善。这可能会使学习曲线变得有些陡峭。幸运的是，它仍然是一个非常小的项目：在撰写本文时，只有十个源文件（[https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot](https://github.com/scalanlp/breeze/tree/master/viz/src/main/scala/breeze/plot)）。了解
    breeze-viz 确切做什么的一个好方法是阅读源代码。例如，要查看 `Plot` 对象上可用的方法，请阅读源文件 `Plot.scala`。如果您需要
    Breeze 提供的功能之外的功能，请查阅 JFreeChart 的文档，以了解您是否可以通过访问底层的 JFreeChart 对象来实现所需的功能。
- en: Breeze-viz reference
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Breeze-viz 参考文档
- en: 'Writing a reference in a programming book is a dangerous exercise: you quickly
    become out of date. Nevertheless, given the paucity of documentation for breeze-viz,
    this section becomes more relevant – it is easier to compete against something
    that does not exist. Take this section with a pinch of salt, and if a command
    in this section does not work, head over to the source code:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程书中编写参考是一项危险的活动：您很快就会过时。尽管如此，鉴于 breeze-viz 的文档很少，这一部分变得更加相关——与不存在的东西竞争更容易。请带着一点盐来接受这一部分，如果本节中的某个命令不起作用，请查看源代码：
- en: '| Command | Description |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 描述 |'
- en: '| --- | --- |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `plt += plot(xs, ys)` | This plots a series of (`xs`, `ys`) values. The `xs`
    and `ys` values must be collection-like objects (Breeze vectors, Scala arrays,
    or lists, for instance). |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| `plt += plot(xs, ys)` | 这将一系列 (`xs`, `ys`) 值绘制出来。`xs` 和 `ys` 值必须是类似集合的对象（例如
    Breeze 向量、Scala 数组或列表）。|'
- en: '| `plt += scatter(xs, ys, size)``plt += scatter(xs, ys, size, color)` | This
    plots a series of (`xs`, `ys`) values as a scatter plot. The `size` argument is
    an `(Int) => Double` function mapping the index of a point to its size (in the
    same units as the *x* axis). The `color` argument is an `(Int) => java.awt.Paint`
    function mapping from integers to colors. Read the *more advanced scatter plots*
    section for further details. |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| `plt += scatter(xs, ys, size)` | `plt += scatter(xs, ys, size, color)` |
    这将一系列 (`xs`, `ys`) 值作为散点图绘制。`size` 参数是一个 `(Int) => Double` 函数，它将点的索引映射到其大小（与 *x*
    轴相同的单位）。`color` 参数是一个 `(Int) => java.awt.Paint` 函数，它将整数映射到颜色。阅读 *更高级的散点图* 部分以获取更多详细信息。|'
- en: '| `plt += hist(xs)``plt += hist(xs, bins=10)` | This bins `xs` and plots a
    histogram. The `bins` argument controls the number of bins. |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| `plt += hist(xs)` | `plt += hist(xs, bins=10)` | 这将 `xs` 分组并绘制直方图。`bins`
    参数控制分组的数量。|'
- en: '| `plt += image(mat)` | This plots an image or matrix. The `mat` argument should
    be `Matrix[Double]`. Read the `package.scala` source file in `breeze.plot` for
    details ([https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala)).
    |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| `plt += image(mat)` | 这将图像或矩阵绘制出来。`mat` 参数应该是 `Matrix[Double]`。有关详细信息，请阅读
    `breeze.plot` 中的 `package.scala` 源文件（[https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala](https://github.com/scalanlp/breeze/blob/master/viz/src/main/scala/breeze/plot/package.scala)）。|'
- en: 'It is also useful to summarize the options available on a `plot` object:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 总结 `plot` 对象上可用的选项也是有用的：
- en: '| Attribute | Description |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 描述 |'
- en: '| --- | --- |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `plt.xlabel = "x-label"``plt.ylabel = "y-label"` | This sets the axis label
    |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| `plt.xlabel = "x-label"` | `plt.ylabel = "y-label"` | 这设置轴标签 |'
- en: '| `plt.xlim = (0.0, 1.0)``plt.ylim = (0.0, 1.0)` | This sets the axis maximum
    and minimum value |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| `plt.xlim = (0.0, 1.0)` | `plt.ylim = (0.0, 1.0)` | 这设置轴的最大和最小值 |'
- en: '| `plt.logScaleX = true``plt.logScaleY = true` | This switches the axis to
    a log scale |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| `plt.logScaleX = true` | `plt.logScaleY = true` | 这将轴切换到对数刻度 |'
- en: '| `plt.title = "title"` | This sets the plot title |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| `plt.title = "title"` | 这设置绘图标题 |'
- en: Data visualization beyond breeze-viz
  id: totrans-523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Breeze-viz 之外的数据可视化
- en: 'Other tools for data visualization in Scala are emerging: Spark notebooks ([https://github.com/andypetrella/spark-notebook#description](https://github.com/andypetrella/spark-notebook#description))
    based on the IPython notebook and Apache Zeppelin ([https://zeppelin.incubator.apache.org](https://zeppelin.incubator.apache.org)).
    Both of these rely on Apache Spark, which we will explore later in this book.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Scala中用于数据可视化的其他工具正在出现：基于IPython笔记本的Spark笔记本([https://github.com/andypetrella/spark-notebook#description](https://github.com/andypetrella/spark-notebook#description))和Apache
    Zeppelin([https://zeppelin.incubator.apache.org](https://zeppelin.incubator.apache.org))。这两个都依赖于Apache
    Spark，我们将在本书的后面部分探讨。
- en: Summary
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to draw simple charts with breeze-viz. In the
    last chapter of this book, we will learn how to build interactive visualizations
    using JavaScript libraries.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用breeze-viz绘制简单的图表。在本书的最后一章中，我们将学习如何使用JavaScript库构建交互式可视化。
- en: Next, we will learn about basic Scala concurrency constructs—specifically, parallel
    collections.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习关于基本Scala并发构造的知识——特别是并行集合。
- en: Chapter 4. Parallel Collections and Futures
  id: totrans-528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。并行集合和Futures
- en: Data science often involves processing medium or large amounts of data. Since
    the previously exponential growth in the speed of individual CPUs has slowed down
    and the amount of data continues to increase, leveraging computers effectively
    must entail parallel computation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学通常涉及处理中等或大量数据。由于之前单个CPU速度的指数增长已经放缓，而数据量仍在增加，因此有效地利用计算机必须涉及并行计算。
- en: In this chapter, we will look at ways of parallelizing computation and data
    processing over a single computer. Virtually all new computers have more than
    one processing unit, and distributing a calculation over these cores can be an
    effective way of hastening medium-sized calculations.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在单台计算机上并行化计算和数据处理的方法。几乎所有的计算机都有多个处理单元，将这些核心的计算分布在这些核心上可以是一种加快中等规模计算的有效方式。
- en: Parallelizing calculations over a single chip is suitable for calculations involving
    gigabytes or a few terabytes of data. For larger data flows, we must resort to
    distributing the computation over several computers in parallel. We will discuss
    Apache Spark, a framework for parallel data processing in [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个芯片上并行化计算适合涉及千兆字节或几太字节数据的计算。对于更大的数据流，我们必须求助于将计算并行分布在多台计算机上。我们将在[第10章](part0097.xhtml#aid-2SG6I1
    "第10章。使用Spark的分布式批量处理")，*使用Spark的分布式批量处理*中讨论Apache Spark，这是一个并行数据处理框架。
- en: 'In this book, we will look at three common ways of leveraging parallel architectures
    in a single machine: parallel collections, futures, and actors. We will consider
    the first two in this chapter, and leave the study of actors to [Chapter 9](part0077.xhtml#aid-29DRA1
    "Chapter 9. Concurrency with Akka"), *Concurrency with Akka*.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将探讨在单台机器上利用并行架构的三个常见方法：并行集合、futures和actors。我们将在本章中考虑前两种，并将actors的研究留给[第9章](part0077.xhtml#aid-29DRA1
    "第9章。使用Akka的并发")，*使用Akka的并发*。
- en: Parallel collections
  id: totrans-533
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行集合
- en: Parallel collections offer an extremely easy way to parallelize independent
    tasks. The reader, being familiar with Scala, will know that many tasks can be
    phrased as operations on collections, such as *map*, *reduce*, *filter*, or *groupBy*.
    Parallel collections are an implementation of Scala collections that parallelize
    these operations to run over several threads.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合提供了一种极其简单的方式来并行化独立任务。由于读者熟悉Scala，他们将知道许多任务可以表述为集合上的操作，如*map*、*reduce*、*filter*或*groupBy*。并行集合是Scala集合的一种实现，它将这些操作并行化以在多个线程上运行。
- en: 'Let''s start with an example. We want to calculate the frequency of occurrence
    of each letter in a sentence:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从例子开始。我们想要计算句子中每个字母出现的频率：
- en: '[PRE123]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Let''s start by converting our sentence from a string to a vector of characters:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将我们的句子从字符串转换为字符向量：
- en: '[PRE124]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'We can now convert `characters` to a *parallel* vector, a `ParVector`. To do
    this, we use the `par` method:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将`characters`转换为*并行*向量，即`ParVector`。为此，我们使用`par`方法：
- en: '[PRE125]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '`ParVector` collections support the same operations as regular vectors, but
    their methods are executed in parallel over several threads.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '`ParVector`集合支持与常规向量相同的操作，但它们的方法是在多个线程上并行执行的。'
- en: 'Let''s start by filtering out the spaces in `charactersPar`:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从`charactersPar`中过滤掉空格：
- en: '[PRE126]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Notice how Scala hides the execution details. The `filter` operation was performed
    using multiple threads, and you barely even noticed! The interface and behavior
    of a parallel vector is identical to its serial counterpart, save for a few details
    that we will explore in the next section.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Scala如何隐藏执行细节。`filter`操作使用了多个线程，而你几乎都没有注意到！并行向量的接口和行为与其串行对应物相同，除了我们将在下一节中探讨的一些细节。
- en: 'Let''s now use the `toLower` function to make the letters lowercase:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`toLower`函数将字母转换为小写：
- en: '[PRE127]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'As before, the `map` method was applied in parallel. To find the frequency
    of occurrence of each letter, we use the `groupBy` method to group characters
    into vectors containing all the occurrences of that character:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，`map`方法是在并行中应用的。为了找到每个字母的出现频率，我们使用`groupBy`方法将字符分组到包含该字符所有出现的向量中：
- en: '[PRE128]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Note how the `groupBy` method has created a `ParMap` instance, the parallel
    equivalent of an immutable map. To get the number of occurrences of each letter,
    we do a `mapValues` call on `intermediateMap`, replacing each vector by its length:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`groupBy`方法创建了一个`ParMap`实例，它是不可变映射的并行等价物。为了得到每个字母出现的次数，我们在`intermediateMap`上执行`mapValues`调用，将每个向量替换为其长度：
- en: '[PRE129]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Congratulations! We've written a multi-threaded algorithm for finding the frequency
    of occurrence of each letter in a few lines of code. You should find it straightforward
    to adapt this to find the frequency of occurrence of each word in a document,
    a common preprocessing problem for analyzing text data.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们用几行代码编写了一个多线程算法，用于查找每个字母在文本中出现的频率。你应该会发现将其修改为查找文档中每个单词的出现频率非常简单，这是分析文本数据时常见的预处理问题。
- en: 'Parallel collections make it very easy to parallelize some operation pipelines:
    all we had to do was call `.par` on the `characters` vector. All subsequent operations
    were parallelized. This makes switching from a serial to a parallel implementation
    very easy.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合使得并行化某些操作流程变得非常容易：我们只需在`characters`向量上调用`.par`。所有后续操作都被并行化。这使得从串行实现切换到并行实现变得非常容易。
- en: Limitations of parallel collections
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行集合的限制
- en: 'Part of the power and the appeal of parallel collections is that they present
    the same interface as their serial counterparts: they have a `map` method, a `foreach`
    method, a `filter` method, and so on. By and large, these methods work in the
    same way on parallel collections as they do in serial. There are, however, some
    notable caveats. The most important one has to do with side effects. If an operation
    on a parallel collection has a side effect, this may result in a race condition:
    a situation in which the final result depends on the order in which the threads
    perform their operations.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合的部分力量和吸引力在于它们提供了与它们的串行对应物相同的接口：它们有`map`方法、`foreach`方法、`filter`方法等等。总的来说，这些方法在并行集合上的工作方式与在串行集合上相同。然而，也有一些值得注意的注意事项。其中最重要的一点与副作用有关。如果在并行集合上的操作有副作用，这可能会导致竞争条件：一种最终结果取决于线程执行操作顺序的情况。
- en: 'Side effects in collections arise most commonly when we update a variable defined
    outside of the collection. To give a trivial example of unexpected behavior, let''s
    define a `count` variable and increment it a thousand times using a parallel range:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 集合中的副作用最常见的情况是我们更新集合外部定义的变量。为了给出一个意外的行为的不平凡例子，让我们定义一个`count`变量，并使用并行范围将其增加一千次：
- en: '[PRE130]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'What happened here? The function passed to `foreach` has a side effect: it
    increments `count`, a variable outside of the scope of the function. This is a
    problem because the `+=` operator is a sequence of two operations:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？传递给`foreach`的函数有一个副作用：它增加`count`，这是一个在函数作用域之外的变量。这是一个问题，因为`+=`运算符是一系列两个操作：
- en: Retrieve the value of `count` and add one to it
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取`count`的值并将其加一
- en: Assign the result back to `count`
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果赋值回`count`
- en: 'To understand why this causes unexpected behavior, let''s imagine that the
    `foreach` loop has been parallelized over two threads. **Thread A** might read
    the **count** variable when it is **832** and add one to it to give **833**. Before
    it has time to reassign **833** to **count**, **Thread B** reads **count**, still
    at **832**, and adds one to give **833**. **Thread A** then assigns **833** to
    **count**. **Thread B** then assigns **833** to **count**. We''ve run through
    two updates but only incremented the count by one. The problem arises because
    `+=` can be separated into two instructions: it is not *atomic*. This leaves room
    for threads to interleave their operations:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这为什么会引起意外的行为，让我们想象`foreach`循环已经被并行化到两个线程上。**线程A**可能在变量为**832**时读取**count**变量，并将其加一得到**833**。在它有时间将**833**重新赋值给**count**之前，**线程B**读取**count**，仍然为**832**，然后加一得到**833**。然后**线程A**将**833**赋值给**count**。接着**线程B**也将**833**赋值给**count**。我们进行了两次更新，但只增加了计数一次。问题出现是因为`+=`可以被分解成两个指令：它不是**原子性**的。这为线程提供了交错操作的空间：
- en: '![Limitations of parallel collections](img/image01192.jpeg)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![并行集合的限制](img/image01192.jpeg)'
- en: 'The anatomy of a race condition: both thread A and thread B are trying to update
    `count` concurrently, resulting in one of the updates being overwritten. The final
    value of `count` is 833 instead of 834.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 竞态条件的解剖：线程A和线程B都在尝试并发更新`count`，导致其中一个更新被覆盖。`count`的最终值是833而不是834。
- en: 'To give a somewhat more realistic example of problems caused by non-atomicity,
    let''s look at a different method for counting the frequency of occurrence of
    each letter in our sentence. We define a mutable `Char -> Int` hash map outside
    of the loop. Each time we encounter a letter, we increment the corresponding integer
    in the map:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出一个稍微更现实的例子，说明非原子性引起的问题，让我们看看一种不同的方法来计算句子中每个字母出现的频率。我们在循环外部定义一个可变的`Char ->
    Int`哈希表。每次我们遇到一个字母时，我们就在映射中增加相应的整数：
- en: '[PRE131]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: The discrepancy occurs because of the non-atomicity of the operations in the
    `foreach` loop.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异是由于`foreach`循环中操作的非原子性造成的。
- en: 'In general, it is good practice to avoid side effects in higher-order functions
    on collections. They make the code harder to understand and preclude switching
    from serial to parallel collections. It is also good practice to avoid exposing
    mutable state: immutable objects can be shared freely between threads and cannot
    be affected by side effects.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，避免在集合的高阶函数中引入副作用是一个好习惯。它们使得代码更难以理解，并阻止从串行集合切换到并行集合。避免暴露可变状态也是一个好习惯：不可变对象可以在线程之间自由共享，并且不会受到副作用的影响。
- en: 'Another limitation of parallel collections occurs in reduction (or folding)
    operations. The function used to combine items together must be *associative*.
    For instance:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合的另一个限制出现在归约（或折叠）操作中。用于组合项的功能必须是**结合律**的。例如：
- en: '[PRE132]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'The *minus* operator, `–`, is not associative. The order in which consecutive
    operations are applied matters: `(a – b) – c` is not the same as `a – (b – c)`.
    The function used to reduce a parallel collection must be associative because
    the order in which the reduction occurs is not tied to the order of the collection.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '**减号**运算符，`–`，不是结合律的。连续操作应用的顺序很重要：`(a – b) – c`不等于`a – (b – c)`。用于归约并行集合的功能必须是结合律的，因为归约发生的顺序与集合的顺序无关。'
- en: Error handling
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误处理
- en: 'In single-threaded programs, exception handling is relatively straightforward:
    if an exception occurs, the function can either handle it or escalate it. This
    is not nearly as obvious when parallelism is introduced: a single thread might
    fail, but the others might return successfully.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在单线程程序中，异常处理相对简单：如果发生异常，函数可以处理它或将其升级。当引入并行性时，这并不那么明显：一个线程可能会失败，但其他线程可能仍然成功返回。
- en: 'Parallel collection methods will throw an exception if they fail on any element,
    just like their serial counterparts:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 如果并行集合方法在任何一个元素上失败，它们将抛出异常，就像它们的串行对应物一样：
- en: '[PRE133]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: There are cases when this isn't the behavior that we want. For instance, we
    might be using a parallel collection to retrieve a large number of web pages in
    parallel. We might not mind if a few of the pages cannot be fetched.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们并不希望这种行为。例如，我们可能正在使用并行集合并行检索大量网页。我们可能不介意如果有一些页面无法获取。
- en: 'Scala''s `Try` type was designed for sandboxing code that might throw exceptions.
    It is similar to `Option` in that it is a one-element container:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的`Try`类型是为了沙盒化可能会抛出异常的代码而设计的。它与`Option`类似，因为它是一个单元素容器：
- en: '[PRE134]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Unlike the `Option` type, which indicates whether an expression has a useful
    value, the `Try` type indicates whether an expression can be executed without
    throwing an exception. It takes on the following two values:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 与表示表达式是否有有用值的 `Option` 类型不同，`Try` 类型表示表达式是否可以在不抛出异常的情况下执行。它具有以下两个值：
- en: '`Try { 2 + 2 } == Success(4)` if the expression in the `Try` statement is evaluated
    successfully'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`尝试 { 2 + 2 } == Success(4)` 如果 `Try` 语句中的表达式成功评估'
- en: '`Try { 2 / 0 } == Failure(java.lang.ArithmeticException: / by zero)` if the
    expression in the `Try` block results in an exception'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`尝试 { 2 / 0 } == Failure(java.lang.ArithmeticException: / by zero)` 如果 `Try`
    块中的表达式导致异常'
- en: 'This will make more sense with an example. To see the `Try` type in action,
    we will try to fetch web pages in a fault tolerant manner. We will use the built-in
    `Source.fromURL` method which fetches a web page and opens an iterator of the
    page''s content. If it fails to fetch the web page, it throws an error:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个例子来解释会更清晰。为了看到 `Try` 类型的实际应用，我们将尝试以容错的方式获取网页。我们将使用内置的 `Source.fromURL` 方法来获取网页并打开页面内容的迭代器。如果它无法获取网页，它将抛出一个错误：
- en: '[PRE135]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Instead of letting the expression propagate out and crash the rest of our code,
    we can wrap the call to `Source.fromURL` in `Try`:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是让表达式传播出去并使我们的代码崩溃，我们可以将 `Source.fromURL` 的调用包裹在 `Try` 中：
- en: '[PRE136]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'To see the power of our `Try` statement, let''s now retrieve a list of URLs
    in parallel in a fault tolerant manner:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到我们的 `Try` 语句的力量，现在让我们以容错的方式并行检索 URL 列表：
- en: '[PRE137]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'We can then use a `collect` statement to act on the pages we could fetch successfully.
    For instance, to get the number of characters on each page:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `collect` 语句对成功获取的页面进行操作。例如，获取每个页面的字符数：
- en: '[PRE138]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: By making good use of Scala's built-in `Try` classes and parallel collections,
    we have built a fault tolerant, multithreaded URL retriever in a few lines of
    code. (Compare this to the myriad of Java/C++ books that prefix code examples
    with 'error handling is left out for clarity'.)
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 通过充分利用 Scala 的内置 `Try` 类和并行集合，我们只用几行代码就构建了一个容错、多线程的 URL 检索器。（与那些在代码示例前加上 '为了清晰起见省略错误处理'
    的 Java/C++ 书籍相比。）
- en: Tip
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**The Try type versus try/catch statements**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**`Try` 类型与 try/catch 语句的比较**'
- en: Programmers with imperative or object-oriented backgrounds will be more familiar
    with try/catch blocks for handling exceptions. We could have accomplished similar
    functionality here by wrapping the code for fetching URLs in a try block, returning
    null if the call raises an exception.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 具有命令式或面向对象背景的程序员会更熟悉用于处理异常的 try/catch 块。我们可以通过将获取 URL 的代码包裹在 try 块中，在调用引发异常时返回
    null 来实现类似的功能。
- en: 'However, besides being more verbose, returning null is less satisfactory: we
    lose all information about the exception and null is less expressive than `Failure(exception)`.
    Furthermore, returning a `Try[T]` type forces the caller to consider the possibility
    that the function might fail, by encoding this possibility in the type of the
    return value. In contrast, just returning `T` and coding failure with a null value
    allows the caller to ignore failure, raising the possibility of a confusing `NullPointerException`
    being thrown at a completely different point in the program.'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了更冗长之外，返回 null 的效果也不太令人满意：我们失去了有关异常的所有信息，而 null 的表达性不如 `Failure(exception)`。此外，返回
    `Try[T]` 类型迫使调用者考虑函数可能失败的可能性，通过将这种可能性编码在返回值的类型中。相比之下，只返回 `T` 并用 null 值编码失败，允许调用者忽略失败，从而增加了在程序中完全不同的点上抛出令人困惑的
    `NullPointerException` 的可能性。
- en: In short, `Try[T]` is just another higher-order type, like `Option[T]` or `List[T]`.
    Treating the possibility of failure in the same way as the rest of the code adds
    coherence to the program and encourages programmers to tackle the possibility
    of exceptions explicitly.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，`Try[T]` 只是一种更高阶的类型，就像 `Option[T]` 或 `List[T]` 一样。将失败的可能性以与代码中其他部分相同的方式处理，增加了程序的一致性，并鼓励程序员明确处理异常的可能性。
- en: Setting the parallelism level
  id: totrans-594
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置并行级别
- en: 'So far, we have considered parallel collections as black boxes: add `par` to
    a normal collection and all the operations are performed in parallel. Often, we
    will want more control over how the tasks are executed.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将并行集合视为黑盒：将 `par` 添加到普通集合中，所有操作都会并行执行。通常，我们可能希望对任务执行的细节有更多的控制。
- en: Internally, parallel collections work by distributing an operation over multiple
    threads. Since the threads share memory, parallel collections do not need to copy
    any data. Changing the number of threads available to the parallel collection
    will change the number of CPUs that are used to perform the tasks.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，并行集合通过在多个线程上分配操作来工作。由于线程共享内存，并行集合不需要复制任何数据。更改并行集合可用的线程数将更改用于执行任务的CPU数量。
- en: 'Parallel collections have a `tasksupport` attribute that controls task execution:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合有一个`tasksupport`属性，用于控制任务执行：
- en: '[PRE139]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: The task support object of a collection is an *execution context*, an abstraction
    capable of executing Scala expressions in a separate thread. By default, the execution
    context in Scala 2.11 is a *work-stealing thread pool*. When a parallel collection
    submits tasks, the context allocates these tasks to its threads. If a thread finds
    that it has finished its queued tasks, it will try and steal outstanding tasks
    from the other threads. The default execution context maintains a thread pool
    with number of threads equal to the number of CPUs.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 集合的任务支持对象是一个*执行上下文*，这是一个能够在一个单独的线程中执行Scala表达式的抽象。默认情况下，Scala 2.11中的执行上下文是一个*工作窃取线程池*。当一个并行集合提交任务时，上下文将这些任务分配给其线程。如果一个线程发现它已经完成了其队列中的任务，它将尝试从其他线程中窃取未完成的任务。默认执行上下文维护一个线程池，线程数等于CPU的数量。
- en: 'The number of threads over which the parallel collection distributes the work
    can be changed by changing the task support. For instance, to parallelize the
    operations performed by a range over four threads:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合分配工作的线程数可以通过更改任务支持来改变。例如，为了通过四个线程并行化由范围执行的操作：
- en: '[PRE140]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: An example – cross-validation with parallel collections
  id: totrans-602
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 例子 - 使用并行集合的交叉验证
- en: Let's apply what you have learned so far to solve data science problems. There
    are many parts of a machine learning pipeline that can be parallelized trivially.
    One such part is cross-validation.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将你到目前为止学到的知识应用到解决数据科学问题中。机器学习管道的许多部分可以轻易并行化。其中之一就是交叉验证。
- en: We will give a brief description of cross-validation here, but you can refer
    to *The Elements of Statistical Learning*, by *Hastie*, *Tibshirani*, and *Friedman*
    for a more in-depth discussion.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将简要介绍交叉验证，但你可以参考*《统计学习的要素》*，由*哈斯蒂*，*蒂布斯哈尼*和*弗里德曼*撰写，以获得更深入的讨论。
- en: Typically, a supervised machine learning problem involves training an algorithm
    over a training set. For instance, when we built a model to calculate the probability
    of a person being male based on their height and weight, the training set was
    the (height, weight) data for each participant, together with the male/female
    label for each row. Once the algorithm is trained on the training set, we can
    use it to classify new data. This process only really makes sense if the training
    set is representative of the new data that we are likely to encounter.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个监督式机器学习问题涉及在一个训练集上训练一个算法。例如，当我们构建一个基于身高和体重计算一个人性别概率的模型时，训练集是每个参与者的(身高，体重)数据，以及每行的男/女标签。一旦算法在训练集上训练完成，我们就可以用它来分类新的数据。这个过程只有在训练集能够代表我们可能遇到的新数据时才有意义。
- en: The training set has a finite number of entries. It will thus, inevitably, have
    idiosyncrasies that are not representative of the population at large, merely
    due to its finite nature. These idiosyncrasies will result in prediction errors
    when predicting whether a new person is male or female, over and above the prediction
    error of the algorithm on the training set itself. Cross-validation is a tool
    for estimating the error caused by the idiosyncrasies of the training set that
    do not reflect the population at large.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集有有限数量的条目。因此，不可避免地，它将具有一些不是代表整个人群的独特性，仅仅是因为其有限性。这些独特性将在预测新人是男性还是女性时导致预测错误，这超出了算法在训练集本身上的预测错误。交叉验证是估计由不反映整个人群的训练集独特性引起的错误的工具。
- en: 'Cross-validation works by dividing the training set in two parts: a smaller,
    new training set and a cross-validation set. The algorithm is trained on the reduced
    training set. We then see how well the algorithm models the cross-validation set.
    Since we know the right answer for the cross-validation set, we can measure how
    well our algorithm is performing when shown new information. We repeat this procedure
    many times with different cross-validation sets.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证通过将训练集分成两部分：一个较小的、新的训练集和一个交叉验证集来工作。算法在减少的训练集上训练。然后我们看看算法对交叉验证集的建模效果如何。由于我们知道交叉验证集的正确答案，我们可以衡量当展示新信息时我们的算法表现得多好。我们重复这个程序多次，使用不同的交叉验证集。
- en: 'There are several different types of cross-validation, which differ in how
    we choose the cross-validation set. In this chapter, we will look at repeated
    random subsampling: we select *k* rows at random from the training data to form
    the cross-validation set. We do this many times, calculating the cross-validation
    error for each subsample. Since each iteration is independent of the previous
    ones, we can parallelize this process trivially. It is therefore a good candidate
    for parallel collections. We will look at an alternative form of cross-validation,
    *k-fold cross-validation*, in [Chapter 12](part0117.xhtml#aid-3FIHQ2 "Chapter 12. Distributed
    Machine Learning with MLlib"), *Distributed Machine Learning with MLlib*.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的交叉验证类型，它们在如何选择交叉验证集方面有所不同。在本章中，我们将探讨重复随机子采样：我们从训练数据中随机选择*k*行来形成交叉验证集。我们这样做很多次，计算每个子样本的交叉验证误差。由于每个迭代都是相互独立的，我们可以简单地并行化这个过程。因此，它是一个并行集合的良好候选者。我们将在[第12章](part0117.xhtml#aid-3FIHQ2
    "第12章。使用MLlib进行分布式机器学习")中查看交叉验证的另一种形式，即*k*折交叉验证*，*使用MLlib进行分布式机器学习*。
- en: 'We will build a class that performs cross-validation in parallel. I encourage
    you to write the code as you go, but you will find the source code corresponding
    to these examples on GitHub ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).We
    will use parallel collections to handle the parallelism and Breeze data types
    in the inner loop. The `build.sbt` file is identical to the one we used in [Chapter
    2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data with Breeze") , *Manipulating
    Data with Breeze*:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个执行并行交叉验证的类。我鼓励你在编写代码的同时进行，但你将在GitHub上找到与这些示例对应的源代码（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。我们将使用并行集合来处理并行性，并在内部循环中使用Breeze数据类型。`build.sbt`文件与我们在[第2章](part0018.xhtml#aid-H5A41
    "第2章。使用Breeze操作数据")中使用的相同，即*使用Breeze操作数据*：
- en: '[PRE141]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'We will build a `RandomSubsample` class. The class exposes a type alias, `CVFunction`,
    for a function that takes two lists of indices—the first corresponding to the
    reduced training set and the second to the validation set—and returns a `Double`
    corresponding to the cross-validation error:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个`RandomSubsample`类。该类公开了一个类型别名`CVFunction`，用于一个接受两个索引列表的函数——第一个对应于减少的训练集，第二个对应于验证集——并返回一个与交叉验证误差相对应的`Double`值：
- en: '[PRE142]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'The `RandomSubsample` class will expose a single method, `mapSamples`, which
    takes a `CVFunction`, repeatedly passes it different partitions of indices, and
    returns a vector of the errors. This is what the class looks like:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomSubsample`类将公开一个名为`mapSamples`的单个方法，它接受一个`CVFunction`，重复传递不同的索引分区，并返回一个错误向量。这个类看起来是这样的：'
- en: '[PRE143]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Let''s look at what happens in more detail, starting with the arguments passed
    to the constructor:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看发生了什么，从传递给构造函数的参数开始：
- en: '[PRE144]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: We pass the total number of elements in the training set and the number of elements
    to leave out for cross-validation in the class constructor. Thus, passing 100
    to `nElems` and 20 to `nCrossValidation` implies that our training set will have
    80 random elements of the total data and that the test set will have 20 elements.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在类构造函数中传递训练集中元素的总数和为交叉验证留出的元素数。因此，将100传递给`nElems`，将20传递给`nCrossValidation`意味着我们的训练集将包含80个随机元素的总数据，而测试集将包含20个元素。
- en: 'We then construct a list of all integers between `0` and `nElems`:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们构建一个介于`0`和`nElems`之间的所有整数的列表：
- en: '[PRE145]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: For each iteration of the cross-validation, we will shuffle this list and take
    the first `nCrossValidation` elements to be the indices of rows in our test set
    and the remaining to be the indices of rows in our training set.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 对于交叉验证的每一次迭代，我们将对这个列表进行洗牌，并取前`nCrossValidation`个元素作为测试集行索引，剩余的作为训练集行索引。
- en: 'Our class exposes a single method, `mapSamples`, that takes two curried arguments:
    `nShuffles`, the number of times to perform random subsampling, and `f`, a `CVFunction`:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 我们公开了一个单一的方法`mapSamples`，它接受两个柯里化参数：`nShuffles`，表示执行随机子采样的次数，以及`f`，一个`CVFunction`：
- en: '[PRE146]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'With all this set up, the code for doing cross-validation is deceptively simple.
    We generate a parallel range from `0` to `nShuffles` and, for each item in the
    range, generate a new train-test split and calculate the cross-validation error:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好所有这些之后，进行交叉验证的代码看起来很简单。我们生成一个从`0`到`nShuffles`的并行范围，并对范围中的每个项目，生成一个新的训练-测试分割并计算交叉验证错误：
- en: '[PRE147]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: The only tricky part of this function is splitting the shuffled index list into
    a list of indices for the training set and a list of indices for the test set.
    We use Breeze's `split` method. This takes a vector as its first argument and
    a list of split-points as its second, and returns a list of fragments of the original
    vector. We then use pattern matching to extract the individual parts.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的唯一难点是将打乱后的索引列表分割成训练集索引列表和测试集索引列表。我们使用Breeze的`split`方法。这个方法接受一个向量作为其第一个参数，一个分割点列表作为其第二个参数，并返回原始向量的片段列表。然后我们使用模式匹配来提取各个部分。
- en: 'Finally, `mapSamples` converts `cvResults` to a Breeze vector:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`mapSamples`将`cvResults`转换为Breeze向量：
- en: '[PRE148]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Let''s see this in action. We can test our class by running cross-validation
    on the logistic regression example developed in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze") , *Manipulating Data with Breeze*.
    In that chapter, we developed a `LogisticRegression` class that takes a training
    set (in the form of a `DenseMatrix`) and target (in the form of a `DenseVector`)
    at construction time. The class then calculates the parameters that best represent
    the training set. We will first add two methods to the `LogisticRegression` class
    to use the trained model to classify previously unseen examples:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的。我们可以通过在[第2章](part0018.xhtml#aid-H5A41 "第2章. 使用Breeze操作数据")中开发的逻辑回归示例上运行交叉验证来测试我们的类，*使用Breeze操作数据*。在该章中，我们开发了一个`LogisticRegression`类，它在构造时接受一个训练集（以`DenseMatrix`的形式）和一个目标（以`DenseVector`的形式）。然后该类计算最能代表训练集的参数。我们首先将向`LogisticRegression`类添加两个方法，以使用训练好的模型对先前未见过的示例进行分类：
- en: The `predictProbabilitiesMany` method uses the trained model to calculate the
    probability of having the target variable set to one. In the context of our example,
    this is the probability of being male, given a height and weight.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictProbabilitiesMany`方法使用训练好的模型来计算目标变量被设置为1的概率。在我们的示例中，这是给定身高和体重为男性的概率。'
- en: The `classifyMany` method assigns classification labels (one or zero) to members
    of a test set. We will assign a one if `predictProbabilitiesMany` returns a value
    greater than `0.5`.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifyMany`方法将分类标签（一个或零）分配给测试集的成员。如果`predictProbabilitiesMany`返回的值大于`0.5`，我们将分配一个一。'
- en: 'With these two functions, our `LogisticRegression` class becomes:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个函数，我们的`LogisticRegression`类变为：
- en: '[PRE149]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: We can now put together an example program for our `RandomSubsample` class.
    We will use the same height-weight data as in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze") , *Manipulating Data with Breeze*.
    The data preprocessing will be similar. The code examples for this chapter provide
    a helper module, `HWData`, to load the height-weight data into Breeze vectors.
    The data itself is in the `data/` directory of the code examples for this chapter
    (available on GitHub at [https://github.com/pbugnion/s4ds/tree/master/chap04](https://github.com/pbugnion/s4ds/tree/master/chap04)).
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为我们的`RandomSubsample`类编写一个示例程序。我们将使用与[第2章](part0018.xhtml#aid-H5A41 "第2章.
    使用Breeze操作数据")中相同的身高-体重数据，*使用Breeze操作数据*。数据预处理将类似。本章的代码示例提供了一个辅助模块`HWData`，用于将身高-体重数据加载到Breeze向量中。数据本身位于本章代码示例的`data/`目录中（可在GitHub上找到[https://github.com/pbugnion/s4ds/tree/master/chap04](https://github.com/pbugnion/s4ds/tree/master/chap04))。
- en: 'For each new subsample, we create a new `LogisticRegression` instance, train
    it on the subset of the training set to get the best coefficients for this train-test
    split, and use `classifyMany` to generate predictions on the cross-validation
    set in this split. We then calculate the classification error and report the average
    classification error over every train-test split:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个新的子样本，我们创建一个新的`LogisticRegression`实例，在训练集的子集上对其进行训练，以获得此训练-测试分割的最佳系数，并使用`classifyMany`在此分割的交叉验证集上生成预测。然后我们计算分类错误并报告每个训练-测试分割的平均分类错误：
- en: '[PRE150]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Running this program on the height-weight data gives a classification error
    of 10%.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在高度-体重数据上运行此程序给出10%的分类错误。
- en: We now have a fully working, parallelized cross-validation class. Scala's parallel
    range made it simple to repeatedly compute the same function in different threads.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个完全工作、并行化的交叉验证类。Scala的并行范围使得在不同的线程中重复计算相同的函数变得简单。
- en: Futures
  id: totrans-638
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来
- en: 'Parallel collections offer a simple, yet powerful, framework for parallel operations.
    However, they are limited in one respect: the total amount of work must be known
    in advance, and each thread must perform the same function (possibly on different
    inputs).'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合提供了一个简单而强大的并行操作框架。然而，在一方面它们是有限的：必须预先知道总工作量，并且每个线程必须执行相同的函数（可能在不同的输入上）。
- en: Imagine that we want to write a program that fetches a web page (or queries
    a web API) every few seconds and extracts data for further processing from this
    web page. A typical example might involve querying a web API to maintain an up-to-date
    value of a particular stock price. Fetching data from an external web page takes
    a few hundred milliseconds, typically. If we perform this operation on the main
    thread, it will needlessly waste CPU cycles waiting for the web server to reply.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要编写一个程序，每隔几秒从网页（或查询一个Web API）中获取数据，并从该网页中提取数据以进行进一步处理。一个典型的例子可能涉及查询Web
    API以维护特定股票价格的最新值。从外部网页获取数据通常需要几百毫秒。如果我们在这个主线程上执行此操作，它将无谓地浪费CPU周期等待Web服务器回复。
- en: The solution is to wrap the code for fetching the web page in a *future*. A
    future is a one-element container containing the future result of a computation.
    When you create a future, the computation in it gets off-loaded to a different
    thread in order to avoid blocking the main thread. When the computation finishes,
    the result is written to the future and thus made accessible to the main thread.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将获取网页的代码包装在一个*未来*中。未来是一个包含计算未来结果的单一元素容器。当你创建一个未来时，其中的计算会被卸载到不同的线程，以避免阻塞主线程。当计算完成时，结果会被写入未来，从而使其对主线程可访问。
- en: 'As an example, we will write a program that queries the "Markit on demand"
    API to fetch the price of a given stock. For instance, the URL for the current
    price of a Google share is [http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG).
    Go ahead and paste this in the address box of your web browser. You will see an
    XML string appear with, among other things, the current stock price. Let''s fetch
    this programmatically without resorting to a future first:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将编写一个程序，查询“Markit on demand”API以获取给定股票的价格。例如，谷歌当前股价的URL是[http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG)。请将此粘贴到您网络浏览器的地址框中。您将看到一个XML字符串出现，其中包含当前股价等信息。让我们先不使用未来来以编程方式获取这个信息：
- en: '[PRE151]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Notice how it takes a little bit of time to query the API. Let''s now do the
    same, but using a future (don''t worry about the imports for now, we will discuss
    what they mean in more detail further on):'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 注意查询API需要一点时间。现在让我们做同样的事情，但使用一个未来（现在不用担心导入，我们将在后面详细讨论它们的含义）：
- en: '[PRE152]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'If you run this, you will notice that control returns to the shell instantly
    before the API has had a chance to respond. To make this evident, let''s simulate
    a slow connection by adding a call to `Thread.sleep`:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会在API有机会响应之前立即返回控制台。为了使这一点更加明显，让我们通过添加`Thread.sleep`调用来模拟一个慢速连接：
- en: '[PRE153]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'When you run this, you do not have to wait for ten seconds for the next prompt
    to appear: you regain control of the shell straightaway. The bit of code in the
    future is executed asynchronously: its execution is independent of the main program
    flow.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个程序时，你不需要等待十秒钟才能出现下一个提示符：你立即重新获得控制台的控制权。未来中的代码是异步执行的：它的执行与主程序流程独立。
- en: 'How do we retrieve the result of the computation? We note that `response` has
    type `Future[String]`. We can check whether the computation wrapped in the future
    has finished by querying the future''s `isCompleted` attribute:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何检索计算的成果？我们注意到`response`的类型是`Future[String]`。我们可以通过查询未来的`isCompleted`属性来检查未来中包装的计算是否完成：
- en: '[PRE154]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'The future exposes a `value` attribute that contains the computation result:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 未来暴露了一个包含计算结果的`value`属性：
- en: '[PRE155]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: The `value` attribute of a future has type `Option[Try[T]]`. We have already
    seen how to use the `Try` type to handle exceptions gracefully in the context
    of parallel collections. It is used in the same way here. A future's `value` attribute
    is `None` until the future is complete, then it is set to `Some(Success(value))`
    if the future ran successfully, or `Some(Failure(error))` if an exception was
    thrown.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: Future 的 `value` 属性类型为 `Option[Try[T]]`。我们已经在并行集合的上下文中看到了如何使用 `Try` 类型优雅地处理异常。在这里也是同样的用法。一个
    future 的 `value` 属性在 future 完成之前是 `None`，然后它被设置为 `Some(Success(value))` 如果 future
    成功运行，或者设置为 `Some(Failure(error))` 如果抛出了异常。
- en: 'Repeatedly calling `f.value` until the future completes works well in the shell,
    but it does not generalize to more complex programs. Instead, we want to tell
    the computer to do something once the future is complete: we want to bind a *callback*
    function to the future. We can do this by setting the future''s `onComplete` attribute.
    Let''s tell the future to print the API response when it completes:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在 shell 中重复调用 `f.value` 直到 future 完成，效果很好，但这并不能推广到更复杂的程序中。相反，我们希望告诉计算机一旦 future
    完成，就执行某些操作：我们希望将 *回调* 函数绑定到 future。我们可以通过设置 future 的 `onComplete` 属性来实现这一点。让我们告诉
    future 在完成时打印 API 响应：
- en: '[PRE156]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The function passed to `onComplete` runs when the future is finished. It takes
    a single argument of type `Try[T]` containing the result of the future.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `onComplete` 的函数在 future 完成时运行。它接受一个类型为 `Try[T]` 的单个参数，包含 future 的结果。
- en: Tip
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Failure is normal: how to build resilient applications**'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '**失败是正常的：如何构建弹性应用程序**'
- en: By wrapping the output of the code that it runs in a `Try` type, futures force
    the client code to consider the possibility that the code might fail. The client
    can isolate the effect of failure to avoid crashing the whole application. They
    might, for instance, log the exception. In the case of a web API query, they might
    add the offending URL to be queried again at a later date. In the case of a database
    failure, they might roll back the transaction.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将运行代码的输出包装在 `Try` 类型中，futures 强制客户端代码考虑代码可能失败的可能性。客户端可以隔离失败的影响，避免整个应用程序崩溃。例如，他们可能会记录异常。在
    Web API 查询的情况下，他们可能会将受影响的 URL 添加到稍后再次查询的列表中。在数据库失败的情况下，他们可能会回滚事务。
- en: By treating failure as a first-class citizen rather than through exceptional
    control flow bolted on at the end, we can build applications that are much more
    resilient.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将失败视为一等公民，而不是通过在末尾附加异常控制流来处理，我们可以构建出更加弹性的应用程序。
- en: Future composition – using a future's result
  id: totrans-661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Future 组合 – 使用 future 的结果
- en: In the previous section, you learned about the `onComplete` method to bind a
    callback to a future. This is useful to cause a side effect to happen when the
    future is complete. It does not, however, let us transform the future's return
    value easily.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了关于 `onComplete` 方法将回调绑定到 future 的内容。这在 future 完成时引发副作用非常有用。然而，它并不能让我们轻松地转换
    future 的返回值。
- en: 'To carry on with our stocks example, let''s imagine that we want to convert
    the query response from a string to an XML object. Let''s start by including the
    `scala-xml` library as a dependency in `build.sbt`:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续我们的股票示例，让我们想象我们想要将查询响应从字符串转换为 XML 对象。让我们首先在 `build.sbt` 中将 `scala-xml` 库作为依赖项包含进来：
- en: '[PRE157]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Let''s restart the console and reimport the dependencies on `scala.concurrent._`,
    `scala.concurrent.ExecutionContext.Implicits.global`, and `scala.io._`. We also
    want to import the `XML` library:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新启动控制台并重新导入 `scala.concurrent._`、`scala.concurrent.ExecutionContext.Implicits.global`
    和 `scala.io._` 这三个依赖。我们还想导入 `XML` 库：
- en: '[PRE158]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'We will use the same URL as in the previous section:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一节相同的 URL：
- en: '[http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG)'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG](http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG)'
- en: It is sometimes useful to think of a future as a collection that either contains
    one element if a calculation has been successful, or zero elements if it has failed.
    For instance, if the web API has been queried successfully, our future contains
    a string representation of the response. Like other container types in Scala,
    futures support a `map` method that applies a function to the element contained
    in the future, returning a new future, and does nothing if the calculation in
    the future failed. But what does this mean in the context of a computation that
    might not be finished yet? The map method gets applied as soon as the future is
    complete, like the `onComplete` method.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 有时将 `future` 视为一个集合是有用的，如果计算成功，则包含一个元素；如果失败，则包含零个元素。例如，如果 Web API 已成功查询，我们的
    `future` 包含响应的字符串表示。像 Scala 中的其他容器类型一样，`futures` 支持一个 `map` 方法，该方法将函数应用于 `future`
    中包含的元素，返回一个新的 `future`，如果 `future` 中的计算失败则不执行任何操作。但在计算可能尚未完成的情况下，这意味着什么呢？`map`
    方法在 `future` 完成时立即应用，就像 `onComplete` 方法一样。
- en: We can use the future's `map` method to apply a transformation to the result
    of the future asynchronously. Let's poll the "Markit on demand" API again. This
    time, instead of printing the result, we will parse it as XML.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `future` 的 `map` 方法异步地对 `future` 的结果应用转换。让我们再次轮询 "Markit on demand" API。这次，我们不会打印结果，而是将其解析为
    XML。
- en: '[PRE159]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: By registering subsequent maps on futures, we are providing a road map to the
    executor running the future for what to do.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 `future` 上注册后续的映射，我们为运行 `future` 的执行器提供了一个路线图，说明要执行的操作。
- en: 'If any of the steps fail, the failed `Try` instance containing the exception
    gets propagated instead:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何步骤失败，包含异常的失败 `Try` 实例将被传播：
- en: '[PRE160]'
  id: totrans-674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: This behavior makes sense if you think of a failed future as an empty container.
    When applying a map to an empty list, it returns the same empty list. Similarly,
    when applying a map to an empty (failed) future, the empty future is returned.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将失败的 `future` 视为一个空容器，这种行为是有意义的。当将映射应用于空列表时，它返回相同的空列表。同样，当将映射应用于空（失败）的 `future`
    时，返回空 `future`。
- en: Blocking until completion
  id: totrans-676
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阻塞直到完成
- en: 'The code for fetching stock prices works fine in the shell. However, if you
    paste it in a standalone program, you will notice that nothing gets printed and
    the program finishes straightaway. Let''s look at a trivial example of this:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 用于获取股票价格的代码在 shell 中运行良好。然而，如果你将其粘贴到独立程序中，你会注意到没有任何内容被打印出来，程序立即结束。让我们看看这个简单示例：
- en: '[PRE161]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'The program stops running as soon as the main thread has completed its tasks,
    which, in this example, just involves creating the futures. In particular, the
    line `"future completed"` is never printed. If we want the main thread to wait
    for a future to execute, we must explicitly tell it to block execution until the
    future has finished running. This is done using the `Await.ready` or `Await.result`
    methods. Both these methods block the execution of the main thread until the future
    completes. We could make the above program work as intended by adding this line:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 程序在主线程完成其任务后立即停止运行，在这个例子中，这仅仅涉及创建 `future`。特别是，"future completed" 这一行永远不会打印出来。如果我们想让主线程等待
    `future` 执行，我们必须明确告诉它阻塞执行，直到 `future` 完成。这是通过使用 `Await.ready` 或 `Await.result`
    方法来完成的。这两种方法都会阻塞主线程的执行，直到 `future` 完成。我们可以通过添加以下行来使上述程序按预期工作：
- en: '[PRE162]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: The `Await` methods take the future as their first argument and a `Duration`
    object as the second. If the future takes longer to complete than the specified
    duration, a `TimeoutException` is thrown. Pass `Duration.Inf` to set an infinite
    timeout.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '`Await` 方法将 `future` 作为第一个参数，将 `Duration` 对象作为第二个参数。如果 `future` 完成所需时间超过指定的持续时间，则抛出
    `TimeoutException`。传递 `Duration.Inf` 来设置无限超时。'
- en: The difference between `Await.ready` and `Await.result` is that the latter returns
    the value inside the future. In particular, if the future resulted in an exception,
    that exception will get thrown. In contrast, `Await.ready` returns the future
    itself.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '`Await.ready` 和 `Await.result` 之间的区别在于后者返回 `future` 内部的值。特别是，如果 `future` 导致异常，则该异常将被抛出。相比之下，`Await.ready`
    返回 `future` 本身。'
- en: 'In general, one should try to avoid blocking as much as possible: the whole
    point of futures is to run code in background threads in order to keep the main
    thread of execution responsive. However, a common, legitimate use case for blocking
    is at the end of a program. If we are running a large-scale integration process,
    we might dispatch several futures to query web APIs, read from text files, or
    insert data into a database. Embedding the code in futures is more scalable than
    performing these operations sequentially. However, as the majority of the intensive
    work is running in background threads, we are left with many outstanding futures
    when the main thread completes. It makes sense, at this stage, to block until
    all the futures have completed.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应尽可能避免阻塞：`future`的整个目的就是在后台线程中运行代码，以保持执行主线程的响应性。然而，在程序结束时进行阻塞是一个常见且合理的用例。如果我们正在运行一个大规模的集成过程，我们可能会调度几个`future`来查询Web
    API、从文本文件中读取或向数据库中插入数据。将代码嵌入到`future`中比按顺序执行这些操作更具有可扩展性。然而，由于大部分密集型工作都在后台线程中运行，当主线程完成时，我们会留下许多未完成的`future`。在这个阶段，阻塞直到所有`future`完成是有意义的。
- en: Controlling parallel execution with execution contexts
  id: totrans-684
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用执行上下文控制并行执行
- en: Now that we know how to define futures, let's look at controlling how they run.
    In particular, you might want to control the number of threads to use when running
    a large number of futures.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何定义`future`，让我们看看如何控制它们的运行。特别是，你可能想要控制运行大量`future`时使用的线程数量。
- en: When a future is defined, it is passed an *execution context*, either directly
    or implicitly. An execution context is an object that exposes an `execute` method
    that takes a block of code and runs it, possibly asynchronously. By changing the
    execution context, we can change the "backend" that runs the futures. We have
    already seen how to use execution contexts to control the execution of parallel
    collections.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义一个`future`时，它会传递一个`execution context`，无论是直接还是隐式。执行上下文是一个对象，它公开了一个`execute`方法，该方法接受一段代码并运行它，可能是异步的。通过更改执行上下文，我们可以更改运行`future`的“后端”。我们已经看到了如何使用执行上下文来控制并行集合的执行。
- en: So far, we have just been using the default execution context by importing `scala.concurrent.ExecutionContext.Implicits.global`.
    This is a fork / join thread pool with as many threads as there are underlying
    CPUs.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是通过导入`scala.concurrent.ExecutionContext.Implicits.global`来使用默认的执行上下文。这是一个具有与底层CPU数量相同的线程的fork/join线程池。
- en: 'Let''s now define a new execution context that uses sixteen threads:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一个新的使用十六个线程的执行上下文：
- en: '[PRE163]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Having defined the execution context, we can pass it explicitly to futures
    as they are defined:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了执行上下文后，我们可以显式地将它传递给正在定义的`future`：
- en: '[PRE164]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Alternatively, we can define the execution context implicitly:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以隐式地定义执行上下文：
- en: '[PRE165]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'It is then passed as an implicit parameter to all new futures as they are constructed:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将它作为隐式参数传递给所有新构建的`future`：
- en: '[PRE166]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'You can shut the execution context down to destroy the thread pool:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以关闭执行上下文来销毁线程池：
- en: '[PRE167]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: When an execution context receives a shutdown command, it will finish executing
    its current tasks but will refuse any new tasks.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行上下文收到关闭命令时，它将完成当前任务的执行，但会拒绝任何新的任务。
- en: Futures example – stock price fetcher
  id: totrans-699
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期货示例 - 股票价格获取器
- en: 'Let''s bring some of the concepts that we covered in this section together
    to build a command-line application that prompts the user for the name of a stock
    and fetches the value of that stock. The catch is that, to keep the UI responsive,
    we will fetch the stock using a future:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将本节中介绍的一些概念结合起来，构建一个命令行应用程序，该程序提示用户输入股票名称并获取该股票的价值。难点在于，为了保持用户界面响应，我们将使用`future`来获取股票信息：
- en: '[PRE168]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Try running the program and entering the code for some stocks:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行程序并输入一些股票的代码：
- en: '[PRE169]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: Let's summarize how the code works. when you enter a stock, the main thread
    constructs a future that fetches the stock information from the API, converts
    it to XML, and extracts the price. We use `(r \ "LastPrice").text` to extract
    the text inside the `LastPrice` tag from the XML node `r`. We then convert the
    value to a big decimal. When the transformations are complete, the result is printed
    to screen by binding a callback through `onComplete`. Exception handling is handled
    naturally through our use of `.map` methods to handle transformations.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下代码的工作原理。当你输入一个股票时，主线程构建一个期货，从API获取股票信息，将其转换为XML，并提取价格。我们使用`(r \ "LastPrice").text`从XML节点`r`中提取`LastPrice`标签内的文本。然后我们将值转换为大数据。当转换完成后，结果通过`onComplete`绑定回调打印到屏幕。异常处理通过我们使用`.map`方法处理转换来自然处理。
- en: By wrapping the code for fetching a stock price in a future, we free up the
    main thread to just respond to the user. This means that the user interface does
    not get blocked if we have, for instance, a slow internet connection.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将获取股票价格的代码包装在期货中，我们释放了主线程，使其仅用于响应用户。这意味着如果我们的互联网连接很慢，用户界面不会阻塞。
- en: 'This example is somewhat artificial, but you could easily wrap much more complicated
    logic: stock prices could be written to a database and we could add additional
    commands to plot the stock price over time, for instance.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子有些人为，但你很容易将其包装得更加复杂：股票价格可以写入数据库，我们还可以添加额外的命令来绘制股票价格随时间的变化，例如。
- en: We have only scratched the surface of what futures can offer in this section.
    We will revisit futures in more detail when we look at polling web APIs in [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs* and [Chapter 9](part0077.xhtml#aid-29DRA1
    "Chapter 9. Concurrency with Akka"), *Concurrency with Akka*.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们只是刚刚触及了期货所能提供的表面内容。当我们查看第7章（part0059.xhtml#aid-1O8H61 "第7章。Web API"）、*Web
    API*和第9章（part0077.xhtml#aid-29DRA1 "第9章。使用Akka的并发"）、*使用Akka的并发*时，我们将更详细地回顾期货。
- en: Futures are a key part of the data scientist's toolkit for building scalable
    systems. Moving expensive computation (either in terms of CPU time or wall time)
    to background threads improves scalability greatly. For this reason, futures are
    an important part of many Scala libraries such as **Akka** and the **Play** framework.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 期货是数据科学家构建可扩展系统工具箱的关键部分。将昂贵的计算（无论是CPU时间还是墙时间）移动到后台线程可以极大地提高可扩展性。因此，期货是许多Scala库（如**Akka**和**Play**框架）的重要组成部分。
- en: Summary
  id: totrans-709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: By providing high-level concurrency abstractions, Scala makes writing parallel
    code intuitive and straightforward. Parallel collections and futures form an invaluable
    part of a data scientist's toolbox, allowing them to parallelize their code with
    minimal effort. However, while these high-level abstractions obviate the need
    to deal directly with threads, an understanding of the internals of Scala's concurrency
    model is necessary to avoid race conditions.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供高级并发抽象，Scala使得编写并行代码直观且简单。并行集合和期货是数据科学家工具箱中不可或缺的部分，它们允许他们以最小的努力并行化代码。然而，尽管这些高级抽象消除了直接处理线程的需求，但理解Scala并发模型内部机制是必要的，以避免竞争条件。
- en: 'In the next chapter, we will put concurrency on hold and study how to interact
    with SQL databases. However, this is only temporary: futures will play an important
    role in many of the remaining chapters in this book.'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将暂停并发的研究，学习如何与SQL数据库交互。然而，这只是一个临时的举措：期货将在本书剩余的许多章节中扮演重要的角色。
- en: References
  id: totrans-712
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Aleksandar Prokopec*, *Learning Concurrent Programming in Scala*. This is
    a detailed introduction to the basics of concurrent programming in Scala. In particular,
    it explores parallel collections and futures in much greater detail than this
    chapter.'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '*Aleksandar Prokopec*，*在Scala中学习并发编程*。这是对Scala并发编程基础的一个详细介绍。特别是，它比本章更详细地探讨了并行集合和期货。'
- en: 'Daniel Westheide''s blog gives an excellent introduction to many Scala concepts,
    in particular:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Westheide的博客为许多Scala概念提供了出色的介绍，特别是：
- en: '**Futures**: [http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html](http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html)'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未来**: [http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html](http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html)'
- en: '**The Try** **type**: [http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html](http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html)'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试** **类型**: [http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html](http://danielwestheide.com/blog/2012/12/26/the-neophytes-guide-to-scala-part-6-error-handling-with-try.html)'
- en: For a discussion of cross-validation, see *The Elements of Statistical Learning*
    by *Hastie*, *Tibshirani*, and *Friedman*.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 关于交叉验证的讨论，请参阅Hastie、Tibshirani和Friedman合著的《统计学习基础》。
- en: Chapter 5. Scala and SQL through JDBC
  id: totrans-718
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：Scala和SQL通过JDBC
- en: One of data science's raison d'être is the difficulty of manipulating large
    datasets. Much of the data of interest to a company or research group cannot fit
    conveniently in a single computer's RAM. Storing the data in a way that is easy
    to query is therefore a complex problem.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的一个基本目标是处理大型数据集的困难。许多公司或研究组感兴趣的数据都无法方便地存储在单个计算机的RAM中。因此，以易于查询的方式存储数据是一个复杂的问题。
- en: Relational databases have been successful at solving the data storage problem.
    Originally proposed in 1970 ([http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf](http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf)),
    the overwhelming majority of databases in active use today are still relational.
    In that time, the price of RAM per megabyte has decreased by a factor of a hundred
    million. Similarly, hard drive capacity has increased from tens or hundreds of
    megabytes to terabytes. It is remarkable that, despite this exponential growth
    in data storage capacity, the relational model has remained dominant.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库在解决数据存储问题上取得了成功。最初于1970年提出([http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf](http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf))，今天仍在使用的绝大多数数据库仍然是关系型。在这段时间里，每兆字节RAM的价格下降了十亿倍。同样，硬盘容量也从几十或几百兆字节增加到太字节。值得注意的是，尽管数据存储容量呈指数增长，但关系型模型仍然占据主导地位。
- en: 'Virtually all relational databases are described and queried with variants
    of **SQL** (**Structured Query Language**). With the advent of distributed computing,
    the position of SQL databases as the de facto data storage standard is being challenged
    by other types of databases, commonly grouped under the umbrella term NoSQL. Many
    NoSQL databases are more partition-tolerant than SQL databases: they can be split
    into several parts residing on different computers. While this author expects
    that NoSQL databases will become increasingly popular, SQL databases are likely
    to remain prevalent as a data persistence mechanism; hence, a significant portion
    of this book is devoted to interacting with SQL from Scala.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有关系型数据库都使用SQL（**结构化查询语言**）的变体进行描述和查询。随着分布式计算的出现，SQL数据库作为事实上的数据存储标准的地位正受到其他类型数据库的挑战，这些数据库通常被统称为NoSQL。许多NoSQL数据库比SQL数据库更具有分区容错性：它们可以被分割成几个部分，分别存储在不同的计算机上。尽管作者预计NoSQL数据库将越来越受欢迎，但SQL数据库作为数据持久化机制可能仍然普遍存在；因此，本书的很大一部分内容将致力于从Scala与SQL交互。
- en: While SQL is standardized, most implementations do not follow the full standard.
    Additionally, most implementations provide extensions to the standard. This means
    that, while many of the concepts in this book will apply to all SQL backends,
    the exact syntax will need to be adjusted. We will consider only the MySQL implementation
    here.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SQL是标准化的，但大多数实现并不完全遵循该标准。此外，大多数实现都提供了对标准的扩展。这意味着，尽管本书中的许多概念将适用于所有SQL后端，但确切的语法可能需要调整。在这里，我们将仅考虑MySQL实现。
- en: In this chapter, you will learn how to interact with SQL databases from Scala
    using JDBC, a bare bones Java API. In the next chapter, we will consider Slick,
    an **Object Relational** **Mapper** (**ORM**) that gives a more Scala-esque feel
    to interacting with SQL.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用JDBC（一个基本的Java API）从Scala与SQL数据库交互。在下一章中，我们将考虑Slick，这是一个**对象关系映射器**（ORM），它为与SQL的交互提供了更符合Scala风格的感觉。
- en: 'This chapter is roughly composed of two sections: we will first discuss the
    basic functionality for connecting and interacting with SQL databases, and then
    discuss useful functional patterns that can be used to create an elegant, loosely
    coupled, and coherent data access layer.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 本章大致分为两部分：我们首先将讨论连接和与SQL数据库交互的基本功能，然后讨论可以用来创建优雅、松散耦合和一致的数据访问层的有用功能模式。
- en: This chapter assumes that you have a basic working knowledge of SQL. If you
    do not, you would be better off first reading one of the reference books mentioned
    at the end of the chapter.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设你具备基本的SQL工作知识。如果你不具备，你最好先阅读本章末尾提到的参考书籍之一。
- en: Interacting with JDBC
  id: totrans-726
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与JDBC交互
- en: JDBC is an API for connecting to SQL databases in Java. It remains the simplest
    way of connecting to SQL databases from Scala. Furthermore, the majority of higher-level
    abstractions for interacting with databases still use JDBC as a backend.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC是Java连接SQL数据库的API。它仍然是Scala连接SQL数据库的最简单方式。此外，大多数用于与数据库交互的高级抽象仍然使用JDBC作为后端。
- en: JDBC is not a library in itself. Rather, it exposes a set of interfaces to interact
    with databases. Relational database vendors then provide specific implementations
    of these interfaces.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC本身不是一个库。相反，它暴露了一组用于与数据库交互的接口。关系数据库供应商随后提供这些接口的特定实现。
- en: 'Let''s start by creating a `build.sbt` file. We will declare a dependency on
    the MySQL JDBC connector:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个`build.sbt`文件。我们将声明对MySQL JDBC连接器的依赖：
- en: '[PRE170]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: First steps with JDBC
  id: totrans-731
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JDBC的第一步
- en: 'Let''s start by connecting to JDBC from the command line. To follow with the
    examples, you will need access to a running MySQL server. If you added the MySQL
    connector to the list of dependencies, open a Scala console by typing the following
    command:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从命令行连接到JDBC开始。为了跟随示例，你需要访问一个正在运行的MySQL服务器。如果你已经将MySQL连接器添加到依赖列表中，可以通过输入以下命令打开Scala控制台：
- en: '[PRE171]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: 'Let''s import JDBC:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入JDBC：
- en: '[PRE172]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'We then need to tell JDBC to use a specific connector. This is normally done
    using reflection, loading the driver at runtime:'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要告诉JDBC使用特定的连接器。这通常是通过反射完成的，在运行时加载驱动：
- en: '[PRE173]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: This loads the appropriate driver into the namespace at runtime. If this seems
    somewhat magical to you, it's probably not worth worrying about exactly how this
    works. This is the only example of reflection that we will consider in this book,
    and it is not particularly idiomatic Scala.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作将在运行时将适当的驱动加载到命名空间中。如果你觉得这有点神奇，可能不值得担心它具体是如何工作的。这是本书中我们将考虑的唯一反射示例，而且它并不是特别符合Scala的惯例。
- en: Connecting to a database server
  id: totrans-739
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接到数据库服务器
- en: 'Having specified the SQL connector, we can now connect to a database. Let''s
    assume that we have a database called `test` on host `127.0.0.1`, listening on
    port `3306`. We create a connection as follows:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 已经指定了SQL连接器，我们现在可以连接到数据库。假设我们有一个名为`test`的数据库，它在`127.0.0.1`主机上监听端口`3306`。我们创建连接如下：
- en: '[PRE174]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: The first argument to `getConnection` is a URL-like string with `jdbc:mysql://host[:port]/database`.
    The second and third arguments are the username and password. Pass in an empty
    string if you can connect without a password.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '`getConnection`的第一个参数是一个类似URL的字符串，格式为`jdbc:mysql://host[:port]/database`。第二个和第三个参数是用户名和密码。如果你无需密码即可连接，请传入空字符串。'
- en: Creating tables
  id: totrans-743
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表
- en: 'Now that we have a database connection, let''s interact with the server. For
    these examples, you will find it useful to have a MySQL shell open (or a MySQL
    GUI such as **MySQLWorkbench**) as well as the Scala console. You can open a MySQL
    shell by typing the following command in a terminal:'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了数据库连接，让我们与服务器交互。对于这些示例，你将发现打开一个MySQL壳（或MySQL GUI，如**MySQLWorkbench**）以及Scala控制台很有用。你可以在终端中输入以下命令打开MySQL壳：
- en: '[PRE175]'
  id: totrans-745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'As an example, we will create a small table to keep track of famous physicists.
    In a `mysql` shell, we would run the following command:'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将创建一个小表来跟踪著名物理学家。在`mysql`壳中，我们会运行以下命令：
- en: '[PRE176]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'To achieve the same with Scala, we send a JDBC statement to the connection:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Scala中实现相同的功能，我们需要向连接发送一个JDBC语句：
- en: '[PRE177]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: Let's ignore the return value of `executeUpdate` for now.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们先忽略`executeUpdate`方法的返回值。
- en: Inserting data
  id: totrans-751
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插入数据
- en: 'Now that we have created a table, let''s insert some data into it. We can do
    this with a SQL `INSERT` statement:'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个表，让我们向其中插入一些数据。我们可以使用SQL `INSERT`语句来完成这个操作：
- en: '[PRE178]'
  id: totrans-753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: In this case, `executeUpdate` returns `1`. When inserting rows, it returns the
    number of rows that were inserted. Similarly, if we had used a `SQL UPDATE` statement,
    this would return the number of rows that were updated. For statements that do
    not manipulate rows directly (such as the `CREATE TABLE` statement in the previous
    section), `executeUpdate` just returns `0`.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`executeUpdate`返回`1`。当插入行时，它返回插入的行数。同样，如果我们使用了一个`SQL UPDATE`语句，这将返回更新的行数。对于不直接操作行的语句（如上一节中的`CREATE
    TABLE`语句），`executeUpdate`仅返回`0`。
- en: 'Let''s just jump into a `mysql` shell to verify the insertion performed correctly:'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接跳入`mysql` shell来验证插入是否正确执行：
- en: '[PRE179]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Let''s quickly summarize what we have seen so far: to execute SQL statements
    that do not return results, use the following:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速总结一下到目前为止我们所看到的：要执行不返回结果的SQL语句，请使用以下方法：
- en: '[PRE180]'
  id: totrans-758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'In the context of data science, we frequently need to insert or update many
    rows at a time. For instance, we might have a list of physicists:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学的背景下，我们经常需要一次插入或更新多行。例如，我们可能有一个物理学家列表：
- en: '[PRE181]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'We want to insert all of these into the database. While we could create a statement
    for each physicist and send it to the database, this is quite inefficient. A better
    solution is to create a *batch* of statements and send them to the database together.
    We start by creating a statement template:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将这些全部插入到数据库中。虽然我们可以为每个物理学家创建一个语句并将其发送到数据库，但这非常低效。更好的解决方案是创建一个*批量*语句并将它们一起发送到数据库。我们首先创建一个语句模板：
- en: '[PRE182]'
  id: totrans-762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'This is identical to the previous `prepareStatement` calls, except that we
    replaced the physicist''s name with a `?` placeholder. We can set the placeholder
    value with the `statement.setString` method:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的`prepareStatement`调用相同，只是我们将物理学家名字替换为`?`占位符。我们可以使用`statement.setString`方法设置占位符的值：
- en: '[PRE183]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'This replaces the first placeholder in the statement with the string `Richard
    Feynman`:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 这将用字符串`Richard Feynman`替换语句中的第一个占位符：
- en: '[PRE184]'
  id: totrans-766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Note that JDBC, somewhat counter-intuitively, counts the placeholder positions
    from 1 rather than 0.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，JDBC有些反直觉地，从1而不是0开始计算占位符位置。
- en: 'We have now created the first statement in the batch of updates. Run the following
    command:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已创建了更新批量的第一个语句。运行以下命令：
- en: '[PRE185]'
  id: totrans-769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'By running the preceding command, we initiate a batch insert: the statement
    is added to a temporary buffer that will be executed when we run the `executeBatch`
    method. Let''s add all the physicists in our list:'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行前面的命令，我们启动了一个批量插入：该语句被添加到一个临时缓冲区中，当我们运行`executeBatch`方法时将执行。让我们将列表中的所有物理学家添加进去：
- en: '[PRE186]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'We can now execute all the statements in the batch:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以执行批量中的所有语句：
- en: '[PRE187]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: The return value of `executeBatch` is an array of the number of rows altered
    or inserted by each item in the batch.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '`executeBatch`的返回值是一个数组，表示批量中每个项目更改或插入的行数。'
- en: Note that we used `statement.setString` to fill in the template with a particular
    name. The `PreparedStatement` object has `setXXX` methods for all basic types.
    To get a complete list, read the `PreparedStatement` API documentation ([http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html)).
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了`statement.setString`来用特定的名字填充模板。`PreparedStatement`对象为所有基本类型都有`setXXX`方法。要获取完整的列表，请阅读`PreparedStatement`
    API文档([http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html))。
- en: Reading data
  id: totrans-776
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'Now that we know how to insert data into a database, let''s look at the converse:
    reading data. We use SQL `SELECT` statements to query the database. Let''s do
    this in the MySQL shell first:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何将数据插入数据库，让我们看看相反的情况：读取数据。我们使用SQL `SELECT`语句来查询数据库。让我们首先在MySQL shell中这样做：
- en: '[PRE188]'
  id: totrans-778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'To extract this information in Scala, we define a `PreparedStatement`:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Scala中提取此信息，我们定义一个`PreparedStatement`：
- en: '[PRE189]'
  id: totrans-780
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'We execute this statement by running the following command:'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行以下命令来执行此语句：
- en: '[PRE190]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: This returns a JDBC `ResultSet` instance. The `ResultSet` is an abstraction
    representing a set of rows from the database. Note that we used `statement.executeQuery`
    rather than `statement.executeUpdate`. In general, one should execute statements
    that return data (in the form of `ResultSet`) with `executeQuery`. Statements
    that modify the database without returning data (insert, create, alter, or update
    statements, among others) are executed with `executeUpdate`.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回一个JDBC `ResultSet`实例。`ResultSet`是一个表示数据库中一组行的抽象。请注意，我们使用了`statement.executeQuery`而不是`statement.executeUpdate`。一般来说，应该使用`executeQuery`来执行返回数据（以`ResultSet`形式）的语句。修改数据库而不返回数据（插入、创建、更改或更新语句等）的语句使用`executeUpdate`执行。
- en: 'The `ResultSet` object behaves somewhat like an iterator. It exposes a `next`
    method that advances itself to the next record, returning `true` if there are
    records left in `ResultSet`:'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultSet`对象的行为有点像迭代器。它暴露了一个`next`方法，该方法将其自身推进到下一个记录，如果`ResultSet`中还有记录，则返回`true`：'
- en: '[PRE191]'
  id: totrans-785
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'When the `ResultSet` instance points to a record, we can extract fields in
    this record by passing in the field name:'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 当`ResultSet`实例指向一个记录时，我们可以通过传递字段名来提取该记录中的字段：
- en: '[PRE192]'
  id: totrans-787
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'We can also extract fields using positional arguments. The fields are indexed
    from one:'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用位置参数提取字段。字段从一开始索引：
- en: '[PRE193]'
  id: totrans-789
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: 'When we are done with a particular record, we call the `next` method to advance
    the `ResultSet` to the next record:'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成特定记录的处理时，我们调用 `next` 方法将 `ResultSet` 移动到下一个记录：
- en: '[PRE194]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '![Reading data](img/image01193.jpeg)'
  id: totrans-792
  prefs: []
  type: TYPE_IMG
  zh: '![读取数据](img/image01193.jpeg)'
- en: A ResultSet object supports the getXXX(fieldName) methods to access the fields
    of a record and a `next` method to advance to the next record in the result set.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultSet` 对象支持 `getXXX(fieldName)` 方法来访问记录的字段，以及一个 `next` 方法来移动到结果集中的下一个记录。'
- en: 'One can iterate over a result set using a `while` loop:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `while` 循环遍历结果集：
- en: '[PRE195]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Tip
  id: totrans-796
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'A word of warning applies to reading fields that are nullable. While one might
    expect JDBC to return null when faced with a null SQL field, the return type depends
    on the `getXXX` command used. For instance, `getInt` and `getLong` will return
    `0` for any field that is null. Similarly, `getDouble` and `getFloat` return `0.0`.
    This can lead to some subtle bugs in code. In general, one should be careful with
    getters that return Java value types (`int`, `long`) rather than objects. To find
    out if a value is `null` in the database, query it first with `getInt` (or `getLong`
    or `getDouble`, as appropriate), then use the `wasNull` method that returns a
    Boolean if the last read value was null:'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 关于读取可空字段有一个警告。虽然当面对 null SQL 字段时，人们可能期望 JDBC 返回 null，但返回类型取决于使用的 `getXXX` 命令。例如，`getInt`
    和 `getLong` 将对任何 null 字段返回 `0`。同样，`getDouble` 和 `getFloat` 返回 `0.0`。这可能导致代码中的一些微妙错误。一般来说，应该小心使用返回
    Java 值类型（`int`、`long`）而不是对象的 getter。要确定数据库中的值是否为 `null`，首先使用 `getInt`（或 `getLong`
    或 `getDouble`，视情况而定）查询它，然后使用返回布尔值的 `wasNull` 方法：
- en: '[PRE196]'
  id: totrans-798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: This (surprising) behavior makes reading from `ResultSet` instances error-prone.
    One of the goals of the second part of this chapter is to give you the tools to
    build an abstraction layer on top of the `ResultSet` interface to avoid having
    to call methods such as `getInt` directly.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 这种（令人惊讶的）行为使得从 `ResultSet` 实例中读取变得容易出错。本章第二部分的一个目标是为您提供构建在 `ResultSet` 接口之上的抽象层的工具，以避免直接调用如
    `getInt` 这样的方法。
- en: Reading values directly from `ResultSet` objects feels quite unnatural in Scala.
    We will look, further on in this chapter, at constructing a layer through which
    you can access the result set using type classes.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala 中直接从 `ResultSet` 对象读取值感觉很不自然。在本章的后续部分，我们将探讨通过构建一个层来访问结果集，您可以通过类型类来访问这个层。
- en: 'We now know how to read and write to a database. Having finished with the database
    for now, we close the result sets, prepared statements, and connections:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何读取和写入数据库。现在我们已经完成了对数据库的操作，我们将关闭结果集、预处理语句和连接：
- en: '[PRE197]'
  id: totrans-802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: While closing statements and connections is not important in the Scala shell
    (they will get closed when you exit), it is important when you run programs; otherwise,
    the objects will persist, leading to "out of memory exceptions". In the next sections,
    we will look at establishing connections and statements with the **loan pattern**,
    a design pattern that closes a resource automatically when we finish using it.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 Scala shell 中关闭语句和连接并不重要（它们将在你退出时关闭），但在运行程序时很重要；否则，对象将持续存在，导致“内存不足异常”。在下一节中，我们将探讨使用
    **贷款模式** 建立连接和语句，这是一种设计模式，在完成使用资源后自动关闭它。
- en: JDBC summary
  id: totrans-804
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JDBC 概述
- en: We now have an overview of JDBC. The rest of this chapter will concentrate on
    writing abstractions that sit above JDBC, making database accesses feel more natural.
    Before we do this, let's summarize what we have seen so far.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 JDBC 有了一个概述。本章的其余部分将专注于编写位于 JDBC 之上的抽象，使数据库访问感觉更自然。在我们这样做之前，让我们总结一下到目前为止我们所看到的。
- en: 'We have used three JDBC classes:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了三个 JDBC 类：
- en: 'The `Connection` class represents a connection to a specific SQL database.
    Instantiate a connection as follows:'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Connection` 类表示与特定 SQL 数据库的连接。以下是如何实例化连接的示例：'
- en: '[PRE198]'
  id: totrans-808
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'Our main use of `Connection` instances has been to generate `PreparedStatement`
    objects:'
  id: totrans-809
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们主要使用 `Connection` 实例来生成 `PreparedStatement` 对象：
- en: '[PRE199]'
  id: totrans-810
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: 'A `PreparedStatement` instance represents a SQL statement about to be sent
    to the database. It also represents the template for a SQL statement with placeholders
    for values yet to be filled in. The class exposes the following methods:'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PreparedStatement` 实例代表即将发送到数据库的 SQL 语句。它还代表一个 SQL 语句的模板，其中包含尚未填充的值占位符。该类公开以下方法：'
- en: '| `statement.executeUpdate` | This sends the statement to the database. Use
    this for SQL statements that modify the database and do not return any data, such
    as `INSERT`, `UPDATE`, `DELETE`, and `CREATE` statements. |'
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `statement.executeUpdate` | 这将语句发送到数据库。用于修改数据库且不返回任何数据的 SQL 语句，例如 `INSERT`、`UPDATE`、`DELETE`
    和 `CREATE` 语句。|'
- en: '| `val results = statement.executeQuery` | This sends the statement to the
    database. Use this for SQL statements that return data (predominantly, the `SELECT`
    statements). This returns a `ResultSet` instance. |'
  id: totrans-813
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `val results = statement.executeQuery` | 这将语句发送到数据库。用于返回数据的 SQL 语句（主要是 `SELECT`
    语句）。这返回一个 `ResultSet` 实例。|'
- en: '| `statement.addBatch``statement.executeBatch` | The `addBatch` method adds
    the current statement to a batch of statements, and `executeBatch` sends the batch
    of statements to the database. |'
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `statement.addBatch` `statement.executeBatch` | `addBatch` 方法将当前语句添加到语句批处理中，而
    `executeBatch` 将语句批处理发送到数据库。|'
- en: '| `statement.setString(1, "Scala")``statement.setInt(1, 42)``statement.setBoolean(1,
    true)` | Fill in the placeholder values in the `PreparedStatement`. The first
    argument is the position in the statement (counting from 1). The second argument
    is the value.One common use case for these is in a batch update or insert: we
    might have a Scala list of objects that we want to insert into the database. We
    fill in the placeholders for each object in the list using the `.setXXX` methods,
    then add this statement to the batch using `.addBatch`. We can then send the entire
    batch to the database using `.executeBatch`. |'
  id: totrans-815
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `statement.setString(1, "Scala")` `statement.setInt(1, 42)` `statement.setBoolean(1,
    true)` | 在 `PreparedStatement` 中填写占位符值。第一个参数是语句中的位置（从1开始计数）。第二个参数是值。这些方法的常见用例包括批量更新或插入：我们可能有一个想要插入数据库的
    Scala 对象列表。我们使用 `.setXXX` 方法为列表中的每个对象填写占位符，然后使用 `.addBatch` 将此语句添加到批处理中。然后我们可以使用
    `.executeBatch` 将整个批处理发送到数据库。|'
- en: '| `statement.setNull(1, java.sql.Types.BOOLEAN)` | This sets a particular item
    in the statement to `NULL`. The second argument specifies the `NULL` type. If
    we are setting a cell in a Boolean column, for instance, this should be `Types.BOOLEAN`.
    A full list of types is given in the API documentation for the `java.sql.Types`
    package ([http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html)).
    |'
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `statement.setNull(1, java.sql.Types.BOOLEAN)` | 这将语句中的特定项设置为 `NULL`。第二个参数指定
    `NULL` 类型。如果我们正在设置布尔列中的单元格，例如，这应该是 `Types.BOOLEAN`。`java.sql.Types` 包的 API 文档中提供了类型列表（[http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html)）。|'
- en: 'A `ResultSet` instance represents a set of rows returned by a `SELECT` or `SHOW`
    statement. `ResultSet` exposes methods to access fields in the current row:'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResultSet` 实例表示由 `SELECT` 或 `SHOW` 语句返回的一组行。`ResultSet` 提供了访问当前行字段的方法：'
- en: '| `rs.getString(i)``rs.getInt(i)` | These methods get the value of the `ith`
    field in the current row; `i` is measured from 1. |'
  id: totrans-818
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `rs.getString(i)` `rs.getInt(i)` | 这些方法获取当前行中第 `i` 个字段的值；`i` 从1开始计算。|'
- en: '| `rs.getString("name")``rs.getInt("age")` | These methods get the value of
    a specific field, which is indexed by the column name. |'
  id: totrans-819
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `rs.getString("name")` `rs.getInt("age")` | 这些方法获取特定字段的值，字段通过列名索引。|'
- en: '| `rs.wasNull` | This returns whether the last column read was `NULL.` This
    is particularly important when reading Java value types, such as `getInt`, `getBoolean`,
    or `getDouble`, as these return a default value when reading a `NULL` value. |'
  id: totrans-820
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `rs.wasNull` | 这返回最后读取的列是否为 `NULL`。当读取 Java 值类型（如 `getInt`、`getBoolean` 或
    `getDouble`）时，这尤其重要，因为这些在读取 `NULL` 值时返回默认值。|'
- en: The `ResultSet` instance exposes the `.next` method to move to the next row;
    `.next` returns `true` until the `ResultSet` has advanced to just beyond the last
    row.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultSet` 实例公开了 `.next` 方法以移动到下一行；`.next` 返回 `true`，直到 `ResultSet` 前进到最后一行之后。'
- en: Functional wrappers for JDBC
  id: totrans-822
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JDBC 的函数式包装
- en: We now have a basic overview of the tools afforded by JDBC. All the objects
    that we have interacted with so far feel somewhat clunky and out of place in Scala.
    They do not encourage a functional style of programming.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对 JDBC 提供的工具有一个基本的概述。到目前为止，我们与之交互的所有对象在 Scala 中都显得有些笨拙和不合适。它们并不鼓励函数式编程风格。
- en: 'Of course, elegance is not necessarily a goal in itself (or, at least, you
    will probably struggle to convince your CEO that he should delay the launch of
    a product because the code lacks elegance). However, it is usually a symptom:
    either the code is not extensible or too tightly coupled, or it is easy to introduce
    bugs. The latter is particularly the case for JDBC. Forgot to check `wasNull`?
    That will come back to bite you. Forgot to close your connections? You''ll get
    an "out of memory exception" (hopefully not in production).'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，优雅本身可能不是目标（或者至少，你可能很难说服你的CEO因为代码缺乏优雅而推迟产品的发布）。然而，它通常是一个症状：要么代码不可扩展或耦合度过高，要么容易引入错误。对于JDBC来说，后者尤其如此。忘记检查`wasNull`？这会反过来咬你。忘记关闭你的连接？你会得到一个“内存不足异常”（希望不是在生产环境中）。
- en: In the next sections, we will look at patterns that we can use to wrap JDBC
    types in order to mitigate many of these risks. The patterns that we introduce
    here are used very commonly in Scala libraries and applications. Thus, besides
    writing robust classes to interact with JDBC, learning about these patterns will,
    I hope, give you greater understanding of Scala programming.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨我们可以用来包装JDBC类型以减轻许多这些风险的模式。我们在这里引入的模式在Scala库和应用程序中非常常见。因此，除了编写健壮的类与JDBC交互之外，了解这些模式，我希望，将使你对Scala编程有更深入的理解。
- en: Safer JDBC connections with the loan pattern
  id: totrans-826
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用借款模式实现更安全的JDBC连接
- en: 'We have already seen how to connect to a JDBC database and send statements
    to the database for execution. This technique, however, is somewhat error prone:
    you have to remember to close statements; otherwise, you will quickly run out
    of memory. In more traditional imperative style, we write the following try-finally
    block around every connection:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何连接到JDBC数据库并向数据库发送执行语句。然而，这种技术有些容易出错：你必须记得关闭语句；否则，你会很快耗尽内存。在更传统的命令式风格中，我们在每个连接周围编写以下try-finally块：
- en: '[PRE200]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'Scala, with first-class functions, provides us with an alternative: the *loan
    pattern*. We write a function that is responsible for opening the connection,
    loaning it to the client code to do something interesting with it, and then closing
    it when the client code is done. Thus, the client code is not responsible for
    closing the connection any more.'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: Scala，凭借一等函数，为我们提供了一个替代方案：*借款模式*。我们编写一个负责打开连接、将连接借给客户端代码以执行一些有趣的操作，并在客户端代码完成后关闭连接的函数。因此，客户端代码不再负责关闭连接。
- en: 'Let''s create a new `SqlUtils` object with a `usingConnection` method that
    leverages the loan pattern:'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的`SqlUtils`对象，并使用`usingConnection`方法利用借款模式：
- en: '[PRE201]'
  id: totrans-831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Let''s see this function in action:'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个函数的实际应用：
- en: '[PRE202]'
  id: totrans-833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: Thus, the client doesn't have to remember to close the connection, and the resultant
    code (for the client) feels much more like Scala.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，客户端不需要记住关闭连接，对于客户端来说，代码感觉更像是Scala。
- en: 'How does our `usingConnection` function work? The function definition is `def
    usingConnection( ... )(f : Connection => T ):T`. It takes, as its second set of
    arguments, a function that acts on a `Connection` object. The body of `usingConnection`
    creates the connection, then passes it to `f`, and finally closes the connection.
    This syntax is somewhat similar to code blocks in Ruby or the `with` statement
    in Python.'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的`usingConnection`函数是如何工作的？函数定义是`def usingConnection( ... )(f : Connection
    => T ):T`。它接受第二组参数，即作用于`Connection`对象的功能。`usingConnection`的主体创建连接，然后将其传递给`f`，最后关闭连接。这种语法与Ruby中的代码块或Python中的`with`语句有些相似。'
- en: Tip
  id: totrans-836
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Be careful when mixing the loan pattern with lazy operations. This applies particularly
    to returning iterators, streams, and futures from `f`. As soon as the thread of
    execution leaves `f`, the connection will be closed. Any data structure that is
    not materialized at this point will not be able to carry on accessing the connection.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 当混合借款模式与延迟操作时要小心。这尤其适用于从`f`返回迭代器、流和未来。一旦执行线程离开`f`，连接就会被关闭。在此点之前未实例化的任何数据结构将无法继续访问连接。
- en: 'The loan pattern is, of course, not exclusive to database connections. It is
    useful whenever you have the following pattern, in pseudocode:'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 借款模式当然不仅仅局限于数据库连接。当你遇到以下模式时，它非常有用，以下为伪代码：
- en: '[PRE203]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: Enriching JDBC statements with the "pimp my library" pattern
  id: totrans-840
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用“pimp my library”模式丰富JDBC语句
- en: 'In the previous section, we saw how to create self-closing connections with
    the loan pattern. This allows us to open connections to the database without having
    to remember to close them. However, we still have to remember to close any `ResultSet`
    and `PreparedStatement` that we open:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何使用贷款模式创建自关闭的连接。这允许我们打开数据库连接，而无需记住关闭它们。然而，我们仍然需要记住关闭我们打开的任何`ResultSet`和`PreparedStatement`：
- en: '[PRE204]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: 'Having to open and close the statement is somewhat ugly and error prone. This
    is another natural use case for the loan pattern. Ideally, we would like to write
    the following:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 需要打开和关闭语句有些丑陋且容易出错。这也是贷款模式的另一个自然用例。理想情况下，我们希望编写以下内容：
- en: '[PRE205]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: How can we define a `.withQuery` method on the `Connection` class? We do not
    control the `Connection` class definition as it is part of the JDBC API. We would
    like to be able to somehow reopen the `Connection` class definition to add the
    `withQuery` method.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何能在`Connection`类上定义一个`.withQuery`方法？我们并不控制`Connection`类的定义，因为它属于JDBC API的一部分。我们希望能够以某种方式重新打开`Connection`类的定义来添加`withQuery`方法。
- en: Scala does not let us reopen classes to add new methods (a practice known as
    monkey-patching). We can still, however, enrich existing libraries with implicit
    conversions using the **pimp** **my library** pattern ([http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)).
    We first define a `RichConnection` class that contains the `withQuery` method.
    This `RichConnection` class is created from an existing `Connection` instance.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: Scala不允许我们重新打开类来添加新方法（这种做法被称为猴子补丁）。然而，我们仍然可以使用**pimp my library**模式通过隐式转换来增强现有库（[http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)）。我们首先定义一个包含`withQuery`方法的`RichConnection`类。这个`RichConnection`类是由现有的`Connection`实例创建的。
- en: '[PRE206]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: 'We could use this class by just wrapping every `Connection` instance in a `RichConnection`
    instance:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个`Connection`实例包装在一个`RichConnection`实例中来使用这个类：
- en: '[PRE207]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: 'This adds unnecessary boilerplate: we have to remember to convert every connection
    instance to `RichConnection` to use `withQuery`. Fortunately, Scala provides an
    easier way with implicit conversions: we tell Scala how to convert from `Connection`
    to `RichConnection` and vice versa, and tell it to perform this conversion automatically
    (implicitly), if necessary:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 这增加了不必要的样板代码：我们必须记住将每个连接实例转换为`RichConnection`才能使用`withQuery`。幸运的是，Scala提供了一个更简单的方法，即隐式转换：我们告诉Scala如何从`Connection`转换为`RichConnection`，反之亦然，并告诉它如果需要则自动（隐式）执行此转换：
- en: '[PRE208]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: Now, whenever `pimpConnection` and `depimpConnection` are in the current scope,
    Scala will automatically use them to convert from `Connection` instances to `RichConnection`
    and back as needed.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当`pimpConnection`和`depimpConnection`在当前作用域内时，Scala将自动使用它们将`Connection`实例转换为`RichConnection`，并在需要时将其转换回`Connection`。
- en: 'We can now write the following (I have added type information for emphasis):'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编写以下内容（我添加了类型信息以强调）：
- en: '[PRE209]'
  id: totrans-854
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: This might look like magic, so let's step back and look at what happens when
    we call `withQuery` on a `Connection` instance. The Scala compiler will first
    look to see if the class definition of `Connection` defines a `withQuery` method.
    When it finds that it does not, it will look for implicit methods that convert
    a `Connection` instance to a class that defines `withQuery`. It will find that
    the `pimpConnection` method allows conversion from `Connection` to `RichConnection`,
    which defines `withQuery`. The Scala compiler automatically uses `pimpConnection`
    to transform the `Connection` instance to `RichConnection`.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来像魔法，所以让我们退后一步，看看当我们对一个`Connection`实例调用`withQuery`时会发生什么。Scala编译器首先会查看`Connection`类的定义是否包含`withQuery`方法。当它发现没有时，它会寻找将`Connection`实例转换为定义`withQuery`的类的隐式方法。它会发现`pimpConnection`方法允许从`Connection`转换为定义`withQuery`的`RichConnection`。Scala编译器会自动使用`pimpConnection`将`Connection`实例转换为`RichConnection`。
- en: Note that we used the names `pimpConnection` and `depimpConnection` for the
    conversion functions, but they could have been anything. We never call these methods
    explicitly.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了`pimpConnection`和`depimpConnection`这样的名称作为转换函数，但它们可以是任何名称。我们从未明确调用这些方法。
- en: 'Let''s summarize how to use the *pimp my library* pattern to add methods to
    an existing class:'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下如何使用*pimp my library*模式向现有类添加方法：
- en: 'Write a class that wraps the class you want to enrich: `class` `RichConnection(val
    underlying:Connection)`. Add all the methods that you wish the original class
    had.'
  id: totrans-858
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个包装你想要增强的类的类：`class RichConnection(val underlying:Connection)`。添加你希望原始类拥有的所有方法。
- en: 'Write a method to convert from your original class to your enriched class as
    part of an object called (conventionally) `Implicits`. Make sure that you tell
    Scala to use this conversion automatically with the `implicit` keyword: `implicit
    def pimpConnection(conn:Connection):RichConnection`. You can also tell Scala to
    automatically convert back from the enriched class to the original class by adding
    the reverse conversion method.'
  id: totrans-859
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个方法将你的原始类转换为你的增强类，作为名为（传统上）`Implicits` 的对象的一部分。确保你告诉 Scala 使用 `implicit`
    关键字自动使用这个转换：`implicit def pimpConnection(conn:Connection):RichConnection`。你还可以告诉
    Scala 通过添加反向转换方法自动将增强类转换回原始类。
- en: 'Allow implicit conversions by importing the implicit conversion methods: `import
    Implicits._`.'
  id: totrans-860
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过导入隐式转换方法允许隐式转换：`import Implicits._`。
- en: Wrapping result sets in a stream
  id: totrans-861
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在流中封装结果集
- en: 'The JDBC `ResultSet` object plays very badly with Scala collections. The only
    real way of doing anything useful with it is to loop through it directly with
    a `while` loop. For instance, to get a list of the names of physicists in our
    database, we could write the following code:'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC `ResultSet` 对象与 Scala 集合配合得非常糟糕。真正能够用它做些有用的事情的唯一方法就是直接使用 `while` 循环遍历它。例如，为了获取我们数据库中物理学家的名字列表，我们可以编写以下代码：
- en: '[PRE210]'
  id: totrans-863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'The `ResultSet` interface feels unnatural because it behaves very differently
    from Scala collections. In particular, it does not support the higher-order functions
    that we take for granted in Scala: no `map`, `filter`, `fold`, or `for` comprehensions.
    Thankfully, writing a *stream* that wraps `ResultSet` is quite straightforward.
    A Scala stream is a lazily evaluated list: it evaluates the next element in the
    collection when it is needed and forgets previous elements when they are no longer
    used.'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultSet` 接口感觉很不自然，因为它与 Scala 集合的行为非常不同。特别是，它不支持我们在 Scala 中视为理所当然的高阶函数：没有
    `map`、`filter`、`fold` 或 `for` 语句。幸运的是，编写一个封装 `ResultSet` 的 *stream* 非常简单。Scala
    流是一个延迟评估的列表：它在需要时评估集合中的下一个元素，并在不再使用时忘记之前的元素。'
- en: 'We can define a `stream` method that wraps `ResultSet` as follows:'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个 `stream` 方法，如下封装 `ResultSet`：
- en: '[PRE211]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'This might look quite confusing, so let''s take it slowly. We define a `stream`
    method that wraps `ResultSet`, returning a `Stream[ResultSet]`. When the client
    calls `stream` on an empty result set, this just returns an empty stream. When
    the client calls `stream` on a non-empty `ResultSet`, the `ResultSet` instance
    is advanced by one row, and the client gets back `results #:: stream(results)`.
    The `#::` operator on a stream is similar to the cons operator, `::`, on a list:
    it prepends `results` to an existing `Stream`. The critical difference is that,
    unlike a list, `stream(results)` does not get evaluated until necessary. This,
    therefore, avoids duplicating the entire `ResultSet` in memory.'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '这可能看起来相当令人困惑，所以让我们慢慢来。我们定义一个 `stream` 方法来封装 `ResultSet`，返回一个 `Stream[ResultSet]`。当客户端在空结果集上调用
    `stream` 时，这只会返回一个空流。当客户端在非空 `ResultSet` 上调用 `stream` 时，`ResultSet` 实例会向前推进一行，客户端会得到
    `results #:: stream(results)`。流上的 `#::` 操作符类似于列表上的 `::` 操作符：它将 `results` 预先添加到现有的
    `Stream` 中。关键的区别是，与列表不同，`stream(results)` 不会在必要时进行评估。因此，这避免了在内存中重复整个 `ResultSet`。'
- en: 'Let''s use our brand new `stream` function to get the name of all the physicists
    in our database:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们全新的 `stream` 函数来获取我们数据库中所有物理学家的名字：
- en: '[PRE212]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: Streaming the results, rather than using the result set directly, lets us interact
    with the data much more naturally as we are now dealing with just a Scala collection.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 相比直接使用结果集，流式处理结果允许我们以更自然的方式与数据交互，因为我们现在处理的是 Scala 集合。
- en: When you use `stream` in a `withQuery` block (or, generally, in a block that
    automatically closes the result set), you must always materialize the stream within
    the function, hence the call to `toVector`. Otherwise, the stream will wait until
    its elements are needed to materialize them, and by then, the `ResultSet` instance
    will be closed.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 `withQuery` 块（或者更一般地，在自动关闭结果集的块）中使用 `stream` 时，你必须在函数内部始终将流具体化，因此调用了 `toVector`。否则，流将等待其元素被需要时才具体化它们，而那时，`ResultSet`
    实例将被关闭。
- en: Looser coupling with type classes
  id: totrans-872
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过类型类实现更松散的耦合
- en: 'So far, we have been reading and writing simple types to the database. Let''s
    imagine that we want to add a `gender` column to our database. We will store the
    gender as an enumeration in our physicists database. Our table is now as follows:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在数据库中读取和写入简单类型。让我们假设我们想要向我们的数据库添加一个 `gender` 列。我们将把性别作为枚举存储在我们的物理学家数据库中。我们的表现在如下所示：
- en: '[PRE213]'
  id: totrans-874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: 'How can we represent genders in Scala? A good way of doing this is with an
    enumeration:'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在Scala中表示性别？一种好的方法是使用枚举：
- en: '[PRE214]'
  id: totrans-876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: 'However, we now have a problem when deserializing objects from the database:
    JDBC has no built-in mechanism to convert from a SQL `ENUM` type to a Scala `Gender`
    type. We could achieve this by just converting manually every time we need to
    read gender information:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们现在在从数据库反序列化对象时遇到了问题：JDBC没有内置机制将SQL `ENUM`类型转换为Scala `Gender`类型。我们可以通过每次需要读取性别信息时手动转换来实现这一点：
- en: '[PRE215]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: However, we would need to write this everywhere that we want to read the `gender`
    field. This goes against the DRY (don't repeat yourself) principle, leading to
    code that is difficult to maintain. If we decide to change the way gender is stored
    in the database, we would need to find every instance in the code where we read
    the `gender` field and change it.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要在所有想要读取`gender`字段的地方都写下这些代码。这违反了DRY（不要重复自己）原则，导致代码难以维护。如果我们决定更改数据库中存储性别的方式，我们就需要找到代码中所有读取`gender`字段的地方并对其进行更改。
- en: 'A somewhat better solution would be to add a `getGender` method to the `ResultSet`
    class using the pimp my library idiom that we used extensively in this chapter.
    This solution is still not optimal. We are adding unnecessary specificity to `ResultSet`:
    it is now coupled to the structure of our databases.'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 一个稍微好一些的解决方案是在`ResultSet`类中添加一个`getGender`方法，使用我们在本章中广泛使用的pimp my library习语。这个解决方案仍然不是最优的。我们正在向`ResultSet`添加不必要的特定性：它现在与我们的数据库结构耦合。
- en: 'We could create a subclass of `ResultSet` using inheritance, such as `PhysicistResultSet`,
    that can read the fields in a specific table. However, this approach is not composable:
    if we had another table that kept track of pets, with name, species, and gender
    fields, we would have to either reimplement the code for reading gender in a new
    `PetResultSet` or factor out a `GenderedResultSet` superclass. As the number of
    tables grows, the inheritance hierarchy would become unmanageable. A better approach
    would let us compose the functionality that we need. In particular, we want to
    decouple the process of extracting Scala objects from a result set from the code
    for iterating over a result set.'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过继承`ResultSet`来创建一个子类，例如`PhysicistResultSet`，这样就可以读取特定表中的字段。然而，这种方法是不可组合的：如果我们还有另一个表，它跟踪宠物，包括名称、种类和性别字段，我们就必须要么在新的`PetResultSet`中重新实现读取性别的代码，要么提取一个`GenderedResultSet`超类。随着表数量的增加，继承层次结构将变得难以管理。更好的方法可以让我们组合所需的功能。特别是，我们希望将从一个结果集中提取Scala对象的过程与遍历结果集的代码解耦。
- en: Type classes
  id: totrans-882
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型类
- en: Scala provides an elegant solution using *type classes*. Type classes are a
    very powerful arrow in the Scala architect's quiver. However, they can present
    a bit of a learning curve, especially as there is no direct equivalent in object-oriented
    programming.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: Scala提供了一个优雅的解决方案，使用*类型类*。类型类是Scala架构师箭袋中的一个非常强大的箭头。然而，它们可能有一定的学习曲线，尤其是在面向对象编程中没有直接等效物。
- en: 'Instead of presenting an abstract explanation, I will dive into an example:
    I will describe how we can leverage type classes to convert fields in a `ResultSet`
    to Scala types. The aim is to define a `read[T](field)` method on `ResultSet`
    that knows exactly how to deserialize to objects of type `T`. This method will
    replace and extend the `getXXX` methods in `ResultSet`:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会提供抽象的解释，而是直接进入一个例子：我将描述如何利用类型类将`ResultSet`中的字段转换为Scala类型。目标是定义一个`read[T](field)`方法在`ResultSet`上，该方法知道如何精确地将对象反序列化为类型`T`。此方法将替换并扩展`ResultSet`中的`getXXX`方法：
- en: '[PRE216]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: 'We start by defining an abstract `SqlReader[T]` trait that exposes a `read`
    method to read a specific field from a `ResultSet` and return an instance of type
    `T`:'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个抽象的`SqlReader[T]`特质，它公开一个`read`方法，用于从`ResultSet`中读取特定字段并返回类型为`T`的实例：
- en: '[PRE217]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: 'We now need to provide a concrete implementation of `SqlReader[T]` for every
    `T` type that we want to read. Let''s provide concrete implementations for the
    `Gender` and `String` fields. We will place the implementation in a `SqlReader`
    companion object:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为每个我们想要读取的`T`类型提供一个`SqlReader[T]`的具体实现。让我们为`Gender`和`String`字段提供具体实现。我们将实现放在`SqlReader`伴生对象中：
- en: '[PRE218]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: 'We could now use our `ReadableXXX` objects to read from a result set:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用我们的`ReadableXXX`对象从结果集中读取：
- en: '[PRE219]'
  id: totrans-891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: 'This is already somewhat better than using the following:'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经比使用以下方法好一些：
- en: '[PRE220]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: 'This is because the code to map from a `ResultSet` field to `Gender.Value`
    is centralized in a single place: `ReadableGender`. However, it would be great
    if we could tell Scala to use `ReadableGender` whenever it needs to read `Gender.Value`,
    and use `ReadableString` whenever it needs to read a String value. This is exactly
    what type classes do.'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为将 `ResultSet` 字段映射到 `Gender.Value` 的代码集中在一个单独的位置：`ReadableGender`。然而，如果我们能告诉
    Scala 在需要读取 `Gender.Value` 时使用 `ReadableGender`，在需要读取字符串值时使用 `ReadableString`，那就太好了。这正是类型类的作用。
- en: Coding against type classes
  id: totrans-895
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向类型类的编码
- en: We defined a `Readable[T]` interface that abstracts how to read an object of
    type `T` from a field in a `ResultSet`. How do we tell Scala that it needs to
    use this `Readable` object to convert from the `ResultSet` fields to the appropriate
    Scala type?
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个 `Readable[T]` 接口，它抽象化了如何从 `ResultSet` 字段中读取类型为 `T` 的对象。我们如何告诉 Scala
    需要使用这个 `Readable` 对象将 `ResultSet` 字段转换为适当的 Scala 类型？
- en: 'The key is the `implicit` keyword that we used to prefix the `GenderReader`
    and `StringReader` object definitions. It lets us write:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是我们在 `GenderReader` 和 `StringReader` 对象定义前使用的 `implicit` 关键字。它允许我们编写：
- en: '[PRE221]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: 'By writing `implicitly[SqlReader[T]]`, we are telling the Scala compiler to
    find a class (or an object) that extends `SqlReader[T]` that is marked for implicit
    use. Try this out by pasting the following in the command line, for instance:'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编写 `implicitly[SqlReader[T]]`，我们是在告诉 Scala 编译器找到一个扩展 `SqlReader[T]` 并标记为隐式使用的类（或对象）。你可以通过在命令行粘贴以下内容来尝试，例如：
- en: '[PRE222]'
  id: totrans-900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: 'Of course, using `implicitly[SqlReader[T]]` everywhere is not particularly
    elegant. Let''s use the pimp my library idiom to add a `read[T]` method to `ResultSet`.
    We first define a `RichResultSet` class that we can use to "pimp" the `ResultSet`
    class:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在所有地方使用 `implicitly[SqlReader[T]]` 并不是特别优雅。让我们使用“pimp my library”惯用法向 `ResultSet`
    添加一个 `read[T]` 方法。我们首先定义一个 `RichResultSet` 类，我们可以用它来“pimp” `ResultSet` 类：
- en: '[PRE223]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: 'The only unfamiliar part of this should be the `read[T : SqlReader]` generic
    definition. We are stating here that `read` will accept any `T` type, provided
    an instance of `SqlReader[T]` exists. This is called a *context bound.*'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: '这里的唯一不熟悉的部分应该是 `read[T : SqlReader]` 泛型定义。我们在这里声明，如果存在 `SqlReader[T]` 的实例，`read`
    将接受任何 `T` 类型。这被称为*上下文限制*。'
- en: 'We must also add implicit methods to the `Implicits` object to convert from
    `ResultSet` to `RichResultSet`. You should be familiar with this now, so I will
    not bore you with the details. You can now call `results.read[T](fieldName)` for
    any `T` for which you have a `SqlReader[T]` implicit object defined:'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须在 `Implicits` 对象中添加隐式方法，以将 `ResultSet` 转换为 `RichResultSet`。你现在应该熟悉这个了，所以我就不会详细说明了。你现在可以为任何具有
    `SqlReader[T]` 隐式对象的 `T` 调用 `results.read[T](fieldName)`：
- en: '[PRE224]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'Let''s summarize the steps needed for type classes to work. We will do this
    in the context of deserializing from SQL, but you will be able to adapt these
    steps to solve other problems:'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下使类型类正常工作所需的步骤。我们将在从 SQL 反序列化的上下文中进行此操作，但你将能够将这些步骤适应以解决其他问题：
- en: Define an abstract generic trait that provides the interface for the type class,
    for example, `SqlReader[T]`. Any functionality that is independent of `T` can
    be added to this base trait.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个抽象的泛型特质，为类型类提供接口，例如，`SqlReader[T]`。任何与 `T` 无关的功能都可以添加到这个基本特质中。
- en: Create the companion object for the base trait and add implicit objects extending
    the trait for each `T`, for example,
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为基本特质创建伴随对象，并为每个 `T` 添加扩展特质的隐式对象，例如，
- en: '[PRE225]'
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Type classes are always used in generic methods. A method that relies on the
    existence of a type class for an argument must contain a context bound in the
    generic definition, for example, `def read[T : SqlReader](field:String):T`. To
    access the type class in this method, use the `implicitly` keyword: `implicitly[SqlReader[T]]`.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类型类始终用于泛型方法。一个依赖于类型类存在的参数的方法必须在泛型定义中包含上下文限制，例如，`def read[T : SqlReader](field:String):T`。要访问此方法中的类型类，使用
    `implicitly` 关键字：`implicitly[SqlReader[T]]`。'
- en: When to use type classes
  id: totrans-911
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用类型类
- en: 'Type classes are useful when you need a particular behavior for many different
    types, but exactly how this behavior is implemented varies between these types.
    For instance, we need to be able to read several different types from `ResultSet`,
    but exactly how each type is read differs between types: for strings, we must
    read from `ResultSet` using `getString`, whereas for integers, we must use `getInt`
    followed by `wasNull`.'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要为许多不同类型实现特定行为，但此行为的具体实现在这类之间有所不同时，类型类很有用。例如，我们需要能够从 `ResultSet` 中读取几种不同的类型，但每种类型的读取方式各不相同：对于字符串，我们必须使用
    `getString` 从 `ResultSet` 中读取，而对于整数，我们必须使用 `getInt` 后跟 `wasNull`。
- en: A good rule of thumb is when you start thinking "Oh, I could just write a generic
    method to do this. Ah, but wait, I will have to write the `Int` implementation
    as a specific edge case as it behaves differently. Oh, and the `Gender` implementation.
    I wonder if there's a better way?", then type classes might be useful.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的经验法则是当你开始想“哦，我完全可以写一个泛型方法来做这件事。啊，但是等等，我必须将 `Int` 实现作为一个特定的边缘情况来写，因为它的行为不同。哦，还有
    `Gender` 实现。我想知道是否有更好的方法？”时，类型类可能就很有用了。
- en: Benefits of type classes
  id: totrans-914
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型类的优势
- en: Data scientists frequently have to deal with new input streams, changing requirements,
    and new data types. Having an object-relational mapping layer that is easy to
    extend or alter is therefore critical to responding to changes efficiently. Minimizing
    coupling between code entities and separation of concerns are the only ways to
    ensure that the code can be changed in response to new data.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家经常必须处理新的输入流、变化的需求和新数据类型。因此，拥有一个易于扩展或更改的对象关系映射层对于有效地应对变化至关重要。最小化代码实体之间的耦合和关注点的分离是确保代码能够根据新数据更改的唯一方法。
- en: 'With type classes, we maintain orthogonality between accessing records in the
    database (through the `ResultSet` class) and how individual fields are transformed
    to Scala objects: both can vary independently. The only coupling between these
    two concerns is through the `SqlReader[T]` interface.'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类型类，我们保持了访问数据库中的记录（通过 `ResultSet` 类）和将单个字段转换为 Scala 对象的方式之间的正交性：这两者可以独立变化。这两个关注点之间的唯一耦合是通过
    `SqlReader[T]` 接口。
- en: 'This means that both concerns can evolve independently: to read a new data
    type, we just need to implement a `SqlReader[T]` object. Conversely, we can add
    functionality to `ResultSet` without needing to reimplement how fields are converted.
    For instance, we could add a `getColumn` method that returns a `Vector[T]` of
    all the values of a field in a `ResultSet` instance:'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这两个关注点可以独立进化：要读取新的数据类型，我们只需实现一个 `SqlReader[T]` 对象。相反，我们可以在不重新实现字段转换方式的情况下向
    `ResultSet` 添加功能。例如，我们可以添加一个 `getColumn` 方法，它返回 `ResultSet` 实例中一个字段的 `Vector[T]`
    所有值：
- en: '[PRE226]'
  id: totrans-918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: Note how we could do this without increasing the coupling to the way in which
    individual fields are read.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何在不增加对单个字段读取方式耦合的情况下完成这件事。
- en: Creating a data access layer
  id: totrans-920
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据访问层
- en: Let's bring together everything that we have seen and build a *data-mapper*
    class for fetching `Physicist` objects from the database. These classes (also
    called *data access objects*) are useful to decouple the internal representation
    of an object from its representation in the database.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们汇总我们所看到的一切，并为从数据库中检索 `Physicist` 对象构建一个 *数据映射器* 类。这些类（也称为 *数据访问对象*）有助于将对象的内部表示与其在数据库中的表示解耦。
- en: 'We start by defining the `Physicist` class:'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了 `Physicist` 类：
- en: '[PRE227]'
  id: totrans-923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'The data access object will expose a single method, `readAll`, that returns
    a `Vector[Physicist]` of all the physicists in our database:'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 数据访问对象将公开一个单一的方法，`readAll`，它返回包含我们数据库中所有物理学家的 `Vector[Physicist]`：
- en: '[PRE228]'
  id: totrans-925
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: 'The data access layer can be used by client code as in the following example:'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端代码可以使用数据访问层，如下例所示：
- en: '[PRE229]'
  id: totrans-927
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: Summary
  id: totrans-928
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to interact with SQL databases using JDBC. We
    wrote a library to wrap native JDBC objects, aiming to give them a more functional
    interface.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用 JDBC 与 SQL 数据库进行交互。我们编写了一个库来封装原生 JDBC 对象，目的是为它们提供一个更功能化的接口。
- en: In the next chapter, you will learn about Slick, a Scala library that provides
    functional wrappers to interact with relational databases.
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解 Slick，这是一个 Scala 库，它提供了与关系数据库交互的功能包装器。
- en: References
  id: totrans-931
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'The API documentation for JDBC is very complete: [http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html](http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html)'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 的 API 文档非常完整：[http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html](http://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html)
- en: The API documentation for the `ResultSet` interface ([http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html](http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html)),
    for the `PreparedStatement` class ([http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html))
    and the `Connection` class ([http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html))
    is particularly relevant.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResultSet` 接口的 API 文档（[http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html](http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html)）、`PreparedStatement`
    类（[http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html](http://docs.oracle.com/javase/7/docs/api/java/sql/PreparedStatement.html)）和
    `Connection` 类（[http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html](http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html)）的文档尤其相关。'
- en: The data mapper pattern is described extensively in Martin Fowler's *Patterns
    of Enterprise Application Architecture*. A brief description is also available
    on his website ([http://martinfowler.com/eaaCatalog/dataMapper.html](http://martinfowler.com/eaaCatalog/dataMapper.html)).
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 数据映射模式在 Martin Fowler 的《企业应用架构模式》中描述得非常详细。在他的网站上也有简要的描述（[http://martinfowler.com/eaaCatalog/dataMapper.html](http://martinfowler.com/eaaCatalog/dataMapper.html)）。
- en: For an introduction to SQL, I suggest *Learning SQL* by *Alan Beaulieu* (*O'Reilly*).
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SQL 的入门，我建议阅读 *Alan Beaulieu* 的《Learning SQL》(*O'Reilly*)。
- en: For another discussion of type classes, read [http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html](http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html).
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类型类的另一篇讨论，请阅读 [http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html](http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html)。
- en: 'This post describes how some common object-oriented design patterns can be
    reimplemented more elegantly in Scala using type classes:'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了如何使用类型类在 Scala 中更优雅地重新实现一些常见的面向对象设计模式：
- en: '[https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/](https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/)'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/](https://staticallytyped.wordpress.com/2013/03/24/gang-of-four-patterns-with-type-classes-and-implicits-in-scala-part-2/)'
- en: 'This post by *Martin Odersky* details the *Pimp my Library* pattern:'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇由 *Martin Odersky* 撰写的文章详细介绍了 *Pimp my Library* 模式：
- en: '[http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.artima.com/weblogs/viewpost.jsp?thread=179766](http://www.artima.com/weblogs/viewpost.jsp?thread=179766)'
- en: Chapter 6. Slick – A Functional Interface for SQL
  id: totrans-941
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章。Slick – SQL 的函数式接口
- en: In [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*, we investigated how to access SQL databases with
    JDBC. As interacting with JDBC feels somewhat unnatural, we extended JDBC using
    custom wrappers. The wrappers were developed to provide a functional interface
    to hide the imperative nature of JDBC.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 5 章](part0040.xhtml#aid-164MG1 "第 5 章。通过 JDBC 使用 Scala 和 SQL") 中，我们探讨了如何使用
    JDBC 访问 SQL 数据库。由于与 JDBC 交互感觉有些不自然，我们通过自定义封装扩展了 JDBC。这些封装是为了提供一个函数式接口，以隐藏 JDBC
    的命令式本质。
- en: With the difficulty of interacting directly with JDBC from Scala and the ubiquity
    of SQL databases, you would expect there to be existing Scala libraries that wrap
    JDBC. *Slick* is such a library.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接从 Scala 与 JDBC 交互的难度以及 SQL 数据库的普遍性，你可能会期待存在现有的 Scala 库来封装 JDBC。*Slick* 就是这样一个库。
- en: Slick styles itself as a *functional-relational mapping* library, a play on
    the more traditional *object-relational mapping* name used to denote libraries
    that build objects from relational databases. It presents a functional interface
    to SQL databases, allowing the client to interact with them in a manner similar
    to native Scala collections.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: Slick 自称为 *函数式-关系映射* 库，这是对更传统的 *对象-关系映射* 名称的一种戏谑，后者用来表示从关系数据库构建对象的库。它提供了一个函数式接口来访问
    SQL 数据库，允许客户端以类似于原生 Scala 集合的方式与之交互。
- en: FEC data
  id: totrans-945
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FEC 数据
- en: In this chapter, we will use a somewhat more involved example dataset. The **Federal
    Electoral Commission of the United** **States** (**FEC**) records all donations
    to presidential candidates greater than $200\. These records are publicly available.
    We will look at the donations for the campaign leading up to the 2012 general
    elections that resulted in Barack Obama's re-election. The data includes donations
    to the two presidential candidates, Obama and Romney, and also to the other contenders
    in the Republican primaries (there were no Democrat primaries).
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个稍微复杂一些的示例数据集。**美国联邦选举委员会（Federal Electoral Commission of the United
    States，简称 FEC**）记录了所有超过 200 美元的总统候选人捐款。这些记录是公开可用的。我们将查看导致巴拉克·奥巴马连任的 2012 年大选前的捐款情况。数据包括对两位总统候选人奥巴马和罗姆尼的捐款，以及共和党初选中其他竞争者的捐款（没有民主党初选）。
- en: In this chapter, we will take the transaction data provided by the FEC, store
    it in a table, and learn how to query and analyze it.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 FEC 提供的交易数据，将其存储在表中，并学习如何查询和分析它。
- en: 'The first step is to acquire the data. If you have downloaded the code samples
    from the Packt website, you should already have two CSVs in the `data` directory
    of the code samples for this chapter. If not, you can download the files using
    the following links:'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取数据。如果你已经从 Packt 网站下载了代码示例，你应该已经在本章代码示例的 `data` 目录中有两个 CSV 文件。如果没有，你可以使用以下链接下载文件：
- en: '`data.scala4datascience.com/fec/ohio.csv.gz` (or `ohio.csv.zip`)'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.scala4datascience.com/fec/ohio.csv.gz`（或 `ohio.csv.zip`）'
- en: '`data.scala4datascience.com/fec/us.csv.gz` (or `us.csv.zip`)'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.scala4datascience.com/fec/us.csv.gz`（或 `us.csv.zip`）'
- en: 'Decompress the two files and place them in a directory called `data/` in the
    same location as the source code examples for this chapter. The data files correspond
    to the following:'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 解压缩这两个文件，并将它们放置在本章源代码示例相同的 `data/` 目录中。数据文件对应以下内容：
- en: The `ohio.csv` file is a CSV of all the donations made by donors in Ohio.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ohio.csv` 文件是俄亥俄州所有捐赠者捐赠的 CSV 文件。'
- en: The `us.csv` file is a CSV of all the donations made by donors across the country.
    This is quite a large file, with six million rows.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`us.csv` 文件是全国范围内所有捐赠者捐赠的 CSV 文件。这是一个相当大的文件，有六百万行。'
- en: The two CSV files contain identical columns. Use the Ohio dataset for more responsive
    behavior, or the nationwide data file if you want to wrestle with a larger dataset.
    The dataset is adapted from a list of contributions downloaded from [http://www.fec.gov/disclosurep/PDownload.do](http://www.fec.gov/disclosurep/PDownload.do).
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个 CSV 文件包含相同的列。使用俄亥俄州数据集可以获得更快的响应，或者如果你想处理更大的数据集，可以使用全国数据文件。数据集是从 [http://www.fec.gov/disclosurep/PDownload.do](http://www.fec.gov/disclosurep/PDownload.do)
    下载的贡献列表中改编的。
- en: 'Let''s start by creating a Scala case class to represent a transaction. In
    the context of this chapter, a transaction is a single donation from an individual
    to a candidate:'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个 Scala case class 来表示一笔交易。在本章的上下文中，一笔交易是一个个人向候选人捐赠的单笔捐款：
- en: '[PRE230]'
  id: totrans-956
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: 'The code repository for this chapter includes helper functions in an `FECData`
    singleton object to load the data from CSVs:'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码仓库包含一个 `FECData` 单例对象中的辅助函数，用于从 CSV 文件中加载数据：
- en: '[PRE231]'
  id: totrans-958
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: 'Calling `FECData.loadOhio` or `FECData.loadAll` will create an `FECData` object
    with a single attribute, `transactions`, which is an iterator over all the donations
    coming from Ohio or the entire United States:'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `FECData.loadOhio` 或 `FECData.loadAll` 将创建一个具有单个属性 `transactions` 的 `FECData`
    对象，该属性是一个遍历来自俄亥俄州或整个美国的所有捐赠的迭代器：
- en: '[PRE232]'
  id: totrans-960
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: Now that we have some data to play with, let's try and put it in the database
    so that we can run some useful queries on it.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一些数据可以操作，让我们尝试将其放入数据库中，以便我们可以运行一些有用的查询。
- en: Importing Slick
  id: totrans-962
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入 Slick
- en: To add Slick to the list of dependencies, you will need to add `"com.typesafe.slick"
    %% "slick" % "2.1.0"` to the list of dependencies in your `build.sbt` file. You
    will also need to make sure that Slick has access to a JDBC driver. In this chapter,
    we will connect to a MySQL database, and must, therefore, add the MySQL connector
    `"mysql" % "mysql-connector-java" % "5.1.37"` to the list of dependencies.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Slick 添加到依赖项列表中，你需要在 `build.sbt` 文件中的依赖项列表中添加 `"com.typesafe.slick" %% "slick"
    % "2.1.0"`。你还需要确保 Slick 有权访问 JDBC 驱动程序。在本章中，我们将连接到 MySQL 数据库，因此必须将 MySQL 连接器 `"mysql"
    % "mysql-connector-java" % "5.1.37"` 添加到依赖项列表中。
- en: 'Slick is imported by importing a specific database driver. As we are using
    MySQL, we must import the following:'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入特定的数据库驱动程序来导入 Slick。由于我们使用 MySQL，我们必须导入以下内容：
- en: '[PRE233]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: To connect to a different flavor of SQL database, import the relevant driver.
    The easiest way of seeing what drivers are available is to consult the API documentation
    for the `slick.driver` package, which is available at [http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package).
    All the common SQL flavors are supported (including **H2**, **PostgreSQL**, **MS
    SQL Server**, and **SQLite**).
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到不同类型的SQL数据库，导入相关的驱动程序。查看可用的驱动程序的最简单方法是查阅`slick.driver`包的API文档，该文档可在[http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.driver.package)找到。所有常见的SQL类型都受支持（包括**H2**、**PostgreSQL**、**MS
    SQL Server**和**SQLite**）。
- en: Defining the schema
  id: totrans-967
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模式
- en: 'Let''s create a table to represent our transactions. We will use the following
    schema:'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个表来表示我们的交易。我们将使用以下模式：
- en: '[PRE234]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: Note that the donation amount is in *cents*. This allows us to use an integer
    field (rather than a fixed point decimal, or worse, a float).
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，捐赠金额是以*分*表示的。这允许我们使用整数字段（而不是定点小数，或者更糟糕的是浮点数）。
- en: Note
  id: totrans-971
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You should never use a floating point format to represent money or, in fact,
    any discrete quantity because floats cannot represent most fractions exactly:'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 你永远不应该使用浮点格式来表示金钱，实际上，任何离散量，因为浮点数无法精确表示大多数分数：
- en: '[PRE235]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: This seemingly nonsensical result occurs because there is no way to store 0.3
    exactly in doubles.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 这种看似荒谬的结果发生是因为在双精度浮点数中无法精确存储0.3。
- en: 'This post gives an extensive discussion of the limitations of the floating
    point format:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章广泛讨论了浮点格式限制：
- en: '[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)'
- en: 'To use Slick with tables in our database, we first need to tell Slick about
    the database schema. We do this by creating a class that extends the `Table` abstract
    class. The way in which a schema is defined is quite straightforward, so let''s
    dive straight into the code. We will store our schema in a `Tables` singleton.
    We define a `Transactions` class that provides the mapping to go from collections
    of `Transaction` instances to SQL tables structured like the `transactions` table:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 要在数据库中的表上使用Slick，我们首先需要告诉Slick有关数据库模式。我们通过创建一个扩展`Table`抽象类的类来实现这一点。定义模式的方式相当直接，所以让我们直接进入代码。我们将我们的模式存储在`Tables`单例中。我们定义一个`Transactions`类，它提供了从`Transaction`实例的集合到类似`transactions`表的SQL表的映射：
- en: '[PRE236]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: Let's go through this line by line. We first define a `Transactions` class,
    which must take a Slick `Tag` object as its first argument. The `Tag` object is
    used by Slick internally to construct SQL statements. The `Transactions` class
    extends a `Table` object, passing it the tag and name of the table in the database.
    We could, optionally, have added a database name by extending `Table[Transaction](tag,
    Some("fec"), "transactions")` rather than just `Table[Transaction](tag, "transactions")`.
    The `Table` type is parametrized by `Transaction`. This means that running `SELECT`
    statements on the database returns `Transaction` objects. Similarly, we will insert
    data into the database by passing a transaction or list of transactions to the
    relevant Slick methods.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析。我们首先定义一个`Transactions`类，它必须将其第一个参数作为Slick `Tag`对象。`Tag`对象由Slick内部用于构建SQL语句。`Transactions`类扩展了一个`Table`对象，传递给它标签和数据库中表的名称。我们可以选择性地通过扩展`Table[Transaction](tag,
    Some("fec"), "transactions")`而不是仅仅`Table[Transaction](tag, "transactions")`来添加数据库名称。`Table`类型由`Transaction`参数化。这意味着在数据库上运行`SELECT`语句返回`Transaction`对象。同样，我们将通过传递一个事务或事务列表到相关的Slick方法来在数据库中插入数据。
- en: 'Let''s look at the `Transactions` class definition in more detail. The body
    of the class starts by listing the database columns. For instance, the `id` column
    is defined as follows:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看`Transactions`类的定义。类的主体首先列出数据库列。例如，`id`列定义如下：
- en: '[PRE237]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: We tell Slick that it should read the column called `id` and transform it to
    a Scala integer. Additionally, we tell Slick that this column is the primary key
    and that it is auto-incrementing. The Slick documentation contains a list of available
    options for `column`.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 我们告诉Slick它应该读取名为`id`的列，并将其转换为Scala整数。此外，我们还告诉Slick该列是主键，并且它是自动增长的。Slick文档中包含`column`可用选项的列表。
- en: 'The `candidate` and `contributor` columns are straightforward: we tell Slick
    to read these as `String` from the database. The `contributor_state` column is
    a little more interesting. Besides specifying that it should be read from the
    database as a `String`, we also tell Slick that it should be stored in the database
    with type `VARCHAR(2)`.'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: '`candidate` 和 `contributor` 列很简单：我们告诉 Slick 从数据库中读取这些作为 `String`。`contributor_state`
    列有点更有趣。除了指定它应该作为 `String` 从数据库中读取外，我们还告诉 Slick 它应该以 `VARCHAR(2)` 类型存储在数据库中。'
- en: 'The `contributor_occupation` column in our table can contain `NULL` values.
    When defining the schema, we pass the `Option[String]` type to the column method:'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表中的 `contributor_occupation` 列可以包含 `NULL` 值。在定义模式时，我们将 `Option[String]` 类型传递给列方法：
- en: '[PRE238]'
  id: totrans-985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: When reading from the database, a `NULL` field will get converted to `None`
    for columns specified as `Option[T]`. Conversely, if the field has a value, it
    will be returned as `Some(value)`.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 当从数据库读取时，对于指定为 `Option[T]` 的列，`NULL` 字段将被转换为 `None`。相反，如果字段有值，它将以 `Some(value)`
    返回。
- en: 'The last line of the class body is the most interesting part: it specifies
    how to transform the raw data read from the database into a `Transaction` object
    and how to convert a `Transaction` object to raw fields ready for insertion:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 类体的最后一行是最有趣的部分：它指定了如何将读取的原始数据转换为 `Transaction` 对象以及如何将 `Transaction` 对象转换为准备插入的原始字段：
- en: '[PRE239]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: 'The first part is just a tuple of fields to be read from the database: `(id.?,
    candidate, contributor, contributorState, contributorOccupation, amount, date)`,
    with a small amount of metadata. The second part is a pair of functions that describe
    how to transform this tuple into a `Transaction` object and back. In this case,
    as `Transaction` is a case class, we can take advantage of the `Transaction.tupled`
    and `Transaction.unapply` methods automatically provided for case classes.'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分只是从数据库中读取的字段元组：`(id.?, candidate, contributor, contributorState, contributorOccupation,
    amount, date)`，附带少量元数据。第二部分是一对函数，描述了如何将此元组转换为 `Transaction` 对象以及如何反向转换。在这种情况下，由于
    `Transaction` 是一个案例类，我们可以利用为案例类自动提供的 `Transaction.tupled` 和 `Transaction.unapply`
    方法。
- en: Notice how we followed the `id` entry with `.?`. In our `Transaction` class,
    the donation `id` has the `Option[Int]` type, but the column in the database has
    the `INT` type with the additional `O.AutoInc` option. The `.?` suffix tells Slick
    to use the default value provided by the database (in this case, the database's
    auto-increment) if `id` is `None`.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何在 `id` 条目后面跟着 `.?` 的。在我们的 `Transaction` 类中，捐赠 `id` 具有类型 `Option[Int]`，但数据库中的列具有类型
    `INT`，并附加了 `O.AutoInc` 选项。`.?` 后缀告诉 Slick 如果 `id` 是 `None`，则使用数据库提供的默认值（在这种情况下，数据库的自动递增）。
- en: 'Finally, we define the value:'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了值：
- en: '[PRE240]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: 'This is the handle that we use to actually interact with the database. For
    instance, as we will see later, to get a list of donations to Barack Obama, we
    run the following query (don''t worry about the details of the query for now):'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实际与数据库交互的句柄。例如，正如我们稍后将会看到的，要获取巴拉克·奥巴马的捐赠列表，我们将运行以下查询（现在不用担心查询的细节）：
- en: '[PRE241]'
  id: totrans-994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'Let''s summarize the parts of our `Transactions` mapper class:'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们的 `Transactions` 映射器类的部分：
- en: 'The `Transactions` class must extend the `Table` abstract class parametrized
    by the type that we want to return: `Table[Transaction]`.'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transactions` 类必须扩展由我们想要返回的类型参数化的 `Table` 抽象类：`Table[Transaction]`。'
- en: We define the columns to read from the database explicitly using `column`, for
    example, `def contributorState = column[String]("contributor_state", O.DBType("VARCHAR(2)"))`.
    The `[String]` type parameter defines the Scala type that this column gets read
    as. The first argument is the SQL column name. Consult the Slick documentation
    for a full list of additional arguments ([http://slick.typesafe.com/doc/2.1.0/schemas.html](http://slick.typesafe.com/doc/2.1.0/schemas.html)).
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `column` 显式定义从数据库中读取的列，例如，`def contributorState = column[String]("contributor_state",
    O.DBType("VARCHAR(2)"))`。`[String]` 类型参数定义了此列读取为的 Scala 类型。第一个参数是 SQL 列名。请参阅 Slick
    文档以获取附加参数的完整列表（[http://slick.typesafe.com/doc/2.1.0/schemas.html](http://slick.typesafe.com/doc/2.1.0/schemas.html)）。
- en: We describe how to convert from a tuple of the column values to a Scala object
    and vice versa using `def * = (id.?, candidate, ...) <> (Transaction.tupled, Transaction.unapply)`.
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们描述了如何使用 `def * = (id.?, candidate, ...) <> (Transaction.tupled, Transaction.unapply)`
    将列值的元组转换为 Scala 对象，反之亦然。
- en: Connecting to the database
  id: totrans-999
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接到数据库
- en: So far, you have learned how to define `Table` classes that encode the transformation
    from rows in a SQL table to Scala case classes. To move beyond table definitions
    and start interacting with a database server, we must connect to a database. As
    in the previous chapter, we will assume that there is a MySQL server running on
    localhost on port `3306`.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何定义`Table`类，这些类将SQL表中的行转换为Scala案例类。为了超越表定义并开始与数据库服务器交互，我们必须连接到数据库。与上一章类似，我们将假设有一个MySQL服务器在本地主机的`3306`端口上运行。
- en: 'We will use the console to demonstrate the functionality in this chapter, but
    you can find an equivalent sample program in `SlickDemo.scala`. Let''s open a
    Scala console and connect to the database running on port `3306`:'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用控制台来演示本章的功能，但你可以在`SlickDemo.scala`中找到一个等效的示例程序。让我们打开一个Scala控制台并连接到在`3306`端口上运行的数据库：
- en: '[PRE242]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: If you have read the previous chapter, you will recognize the first argument
    as a JDBC-style URL. The URL starts by defining a protocol, in this case, `jdbc:mysql`,
    followed by the IP address and port of the database server, followed by the database
    name (`test`, here).
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了上一章，你会认出第一个参数是一个JDBC风格的URL。URL首先定义了一个协议，在本例中是`jdbc:mysql`，然后是数据库服务器的IP地址和端口，最后是数据库名称（这里为`test`）。
- en: The second argument to `forURL` is the class name of the JDBC driver. This driver
    is imported at runtime using reflection. Note that the driver specified here must
    match the Slick driver imported statically.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: '`forURL`的第二个参数是JDBC驱动程序的类名。此驱动程序在运行时使用反射导入。请注意，此处指定的驱动程序必须与静态导入的Slick驱动程序匹配。'
- en: 'Having defined the database, we can now use it to create a connection:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了数据库后，我们现在可以使用它来创建一个连接：
- en: '[PRE243]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: 'Slick functions that require access to the database take a `Session` argument
    implicitly: if a `Session` instance marked as implicit is available in scope,
    they will use it. Thus, preceding `session` with the `implicit` keyword saves
    us having to pass `session` explicitly every time we run an operation on the database.'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 需要访问数据库的流畅函数隐式地接受一个`Session`参数：如果作用域内可用一个标记为隐式的`Session`实例，它们将使用它。因此，在`session`前加上`implicit`关键字可以节省我们每次在数据库上运行操作时显式传递`session`。
- en: 'If you have read the previous chapter, you will recognize that Slick deals
    with the need to close connections with the *loan pattern*: a database connection
    is created in the form of a `session` object and passed temporarily to the client.
    When the client code returns, the session is closed, ensuring that all opened
    connections are closed. The client code is therefore spared the responsibility
    of closing the connection.'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了上一章，你会认出Slick处理关闭连接的需求使用的是*借款模式*：数据库连接以`session`对象的形式创建，并临时传递给客户端。当客户端代码返回时，会话被关闭，确保所有打开的连接都被关闭。因此，客户端代码免除了关闭连接的责任。
- en: 'The loan pattern is very useful in production code, but it can be somewhat
    cumbersome in the shell. Slick lets us create a session explicitly as follows:'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 借款模式在生产代码中非常有用，但在shell中可能会有些繁琐。Slick允许我们显式地创建一个会话，如下所示：
- en: '[PRE244]'
  id: totrans-1010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: Creating tables
  id: totrans-1011
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表格
- en: 'Let''s use our new connection to create the transaction table in the database.
    We can access methods to create and drop tables using the `ddl` attribute on our
    `TableQuery[Transactions]` instance:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们新的连接在数据库中创建事务表。我们可以通过`TableQuery[Transactions]`实例上的`ddl`属性访问创建和删除表的方法：
- en: '[PRE245]'
  id: totrans-1013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: 'If you jump into a `mysql` shell, you will see that a `transactions` table
    has been created:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跳入`mysql` shell，你会看到已经创建了一个`transactions`表：
- en: '[PRE246]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: The `ddl` attribute also includes a `drop` method to drop the table. Incidentally,
    `ddl` stands for "data-definition language" and is commonly used to refer to the
    parts of SQL relevant to schema and constraint definitions.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '`ddl`属性还包括一个`drop`方法来删除表。顺便提一下，`ddl`代表“数据定义语言”，通常用来指代与模式定义和约束定义相关的SQL部分。'
- en: Inserting data
  id: totrans-1017
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插入数据
- en: Slick `TableQuery` instances let us interact with SQL tables with an interface
    similar to Scala collections.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: Slick的`TableQuery`实例让我们可以通过与Scala集合类似的接口与SQL表交互。
- en: 'Let''s create a transaction first. We will pretend that a donation occurred
    on the 22nd of June, 2010\. Unfortunately, the code to create dates in Scala and
    pass these to JDBC is particularly clunky. We first create a `java.util.Date`
    instance, which we must then convert to a `java.sql.Date` to use in our newly
    created transaction:'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个交易。我们将假装在2010年6月22日发生了一次捐赠。不幸的是，在Scala中创建日期并将其传递给JDBC的代码特别繁琐。我们首先创建一个`java.util.Date`实例，然后我们必须将其转换为`java.sql.Date`以用于我们新创建的交易：
- en: '[PRE247]'
  id: totrans-1020
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: 'Much of the interface provided by the `TableQuery` instance mirrors that of
    a mutable list. To insert a single row in the transaction table, we can use the
    `+=` operator:'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '`TableQuery`实例提供的界面大部分与可变列表相似。为了在事务表中插入单行，我们可以使用`+=`运算符：'
- en: '[PRE248]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: Under the hood, this will create a JDBC prepared statement and run this statement's
    `executeUpdate` method.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，这将创建一个JDBC预处理语句并运行该语句的`executeUpdate`方法。
- en: 'If you are committing many rows at a time, you should use Slick''s bulk insert
    operator: `++=`. This takes a `List[Transaction]` as input and inserts all the
    transactions in a single batch by taking advantage of JDBC''s `addBatch` and `executeBatch`
    functionality.'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一次提交多行，你应该使用Slick的批量插入运算符：`++=`。它接受一个`List[Transaction]`作为输入，并利用JDBC的`addBatch`和`executeBatch`功能将所有交易批量插入：
- en: 'Let''s insert all the FEC transactions so that we have some data to play with
    when running queries in the next section. We can load an iterator of transactions
    for Ohio by calling the following:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们插入所有FEC交易，这样我们就有一些数据在下一节运行查询时可以操作。我们可以通过调用以下代码来加载俄亥俄州的交易迭代器：
- en: '[PRE249]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: 'We can also load the transactions for the whole of United States:'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以加载整个美国的交易：
- en: '[PRE250]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: 'To avoid materializing all the transactions in a single fell swoop—thus potentially
    exceeding our computer''s available memory—we will take batches of transactions
    from the iterator and insert them:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免一次性将所有交易实体化——从而可能超出我们计算机的可用内存——我们将从迭代器中取出交易批次并插入它们：
- en: '[PRE251]'
  id: totrans-1030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: An iterator's `grouped` method splits the iterator into batches. It is useful
    to split a long collection or iterator into manageable batches that can be processed
    one after the other. This is important when integrating or processing large datasets.
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器的`grouped`方法将迭代器分割成批次。将长集合或迭代器分割成可管理的批次，以便可以逐个处理，这在集成或处理大型数据集时非常重要。
- en: 'All that we have to do now is iterate over our batches, inserting them into
    the database as we go:'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要做的就是遍历我们的批次，在插入的同时将它们存入数据库：
- en: '[PRE252]'
  id: totrans-1033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: 'While this works, it is sometimes useful to see progress reports when doing
    long-running integration processes. As we have split the integration into batches,
    we know (to the nearest batch) how far into the integration we are. Let''s print
    the progress information at the beginning of every batch:'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样可行，但在进行长时间运行的集成过程时，有时查看进度报告是有用的。由于我们将集成分成了批次，我们知道（到最近的批次为止）我们集成到了哪里。让我们在每一个批次的开始处打印进度信息：
- en: '[PRE253]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: We use the `.zipWithIndex` method to transform our iterator over batches into
    an iterator of (*batch*, *current* *index*) pairs. In a full-scale application,
    the progress information would probably be written to a log file rather than to
    the screen.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`.zipWithIndex`方法将我们的批次迭代器转换为(*批次*, *当前索引*)对的迭代器。在一个完整规模的应用中，进度信息可能会被写入日志文件而不是屏幕。
- en: Slick's well-designed interface makes inserting data very intuitive, integrating
    well with native Scala types.
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: Slick精心设计的界面使得插入数据非常直观，与原生Scala类型很好地集成。
- en: Querying data
  id: totrans-1038
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询数据
- en: In the previous section, we used Slick to insert donation data into our database.
    Let's explore this data now.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用Slick将捐赠数据插入到我们的数据库中。现在让我们探索这些数据。
- en: 'When defining the `Transactions` class, we defined a `TableQuery` object, `transactions`,
    that acts as the handle for accessing the transaction table. It exposes an interface
    similar to Scala iterators. For instance, to see the first five elements in our
    database, we can call `take(5)`:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义`Transactions`类时，我们定义了一个`TableQuery`对象`transactions`，它作为访问交易表的句柄。它提供了一个类似于Scala迭代器的接口。例如，要查看数据库中的前五个元素，我们可以调用`take(5)`：
- en: '[PRE254]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: 'Internally, Slick implements the `.take` method using a SQL `LIMIT`. We can,
    in fact, get the SQL statement using the `.selectStatement` method on the query:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，Slick使用SQL `LIMIT`来实现`.take`方法。实际上，我们可以通过查询上的`.selectStatement`方法来获取SQL语句：
- en: '[PRE255]'
  id: totrans-1043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: 'Our Slick query is made up of the following two parts:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流畅查询由以下两个部分组成：
- en: '`.take(n)`: This part is called the *invoker*. Invokers build up the SQL statement
    but do not actually fire it to the database. You can chain many invokers together
    to build complex SQL statements.'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.take(n)`: 这一部分称为 *调用器*。调用器构建 SQL 语句，但不会实际将其发送到数据库。你可以将多个调用器链接在一起来构建复杂的 SQL
    语句。'
- en: '`.list`: This part sends the statement prepared by the invoker to the database
    and converts the result to Scala object. This takes a `session` argument, possibly
    implicitly.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.list`: 这一部分将调用器准备好的语句发送到数据库，并将结果转换为 Scala 对象。这需要一个 `session` 参数，可能是隐式的。'
- en: Invokers
  id: totrans-1047
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调用器
- en: '**Invokers** are the components of a Slick query that build up the SQL select
    statement. Slick exposes a variety of invokers that allow the construction of
    complex queries. Let''s look at some of these invokers here:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: '**调用器**是 Slick 查询的组成部分，用于构建 SQL 选择语句。Slick 提供了各种调用器，允许构建复杂的查询。让我们看看其中的一些调用器：'
- en: 'The `map` invoker is useful to select individual columns or apply operations
    to columns:'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 调用器用于选择单个列或对列应用操作：'
- en: '[PRE256]'
  id: totrans-1050
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE256]'
- en: 'The `filter` invoker is the equivalent of the `WHERE` statements in SQL. Note
    that Slick fields must be compared using `===`:'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter` 调用器等同于 SQL 中的 `WHERE` 语句。请注意，Slick 字段必须使用 `===` 进行比较：'
- en: '[PRE257]'
  id: totrans-1052
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE257]'
- en: 'Similarly, to filter out donations to Barack Obama, use the `=!=` operator:'
  id: totrans-1053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，要过滤掉对巴拉克·奥巴马的捐款，使用 `=!=` 运算符：
- en: '[PRE258]'
  id: totrans-1054
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE258]'
- en: 'The `sortBy` invoker is the equivalent of the `ORDER BY` statement in SQL:'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sortBy` 调用器等同于 SQL 中的 `ORDER BY` 语句：'
- en: '[PRE259]'
  id: totrans-1056
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE259]'
- en: The `leftJoin`, `rightJoin`, `innerJoin`, and `outerJoin` invokers are used
    for joining tables. As we do not cover interactions between multiple tables in
    this tutorial, we cannot demonstrate joins. See the Slick documentation ([http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping](http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping))
    for examples of these.
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leftJoin`、`rightJoin`、`innerJoin` 和 `outerJoin` 调用器用于连接表。由于本教程不涉及多表之间的交互，我们无法演示连接操作。请参阅
    Slick 文档（[http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping](http://slick.typesafe.com/doc/2.1.0/queries.html#joining-and-zipping)）以了解这些操作的示例。'
- en: 'Aggregation invokers such as `length`, `min`, `max`, `sum`, and `avg` can be
    used for computing summary statistics. These must be executed using `.run`, rather
    than `.list`, as they return single numbers. For instance, to get the total donations
    to Barack Obama:'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length`、`min`、`max`、`sum` 和 `avg` 等聚合调用器可用于计算汇总统计信息。这些操作必须使用 `.run` 而不是 `.list`
    执行，因为它们返回单个数字。例如，要获取巴拉克·奥巴马的总捐款：'
- en: '[PRE260]'
  id: totrans-1059
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE260]'
- en: Operations on columns
  id: totrans-1060
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列上的操作
- en: 'In the previous section, you learned about the different invokers and how they
    mapped to SQL statements. We brushed over the methods supported by columns themselves,
    however: we can compare for equality using `===`, but what other operations are
    supported by Slick columns?'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了不同的调用器以及它们如何映射到 SQL 语句。然而，我们只是简要地提到了列本身支持的方法：我们可以使用 `===` 进行相等比较，但
    Slick 列还支持哪些其他操作？
- en: 'Most of the SQL functions are supported. For instance, to get the total donations
    to candidates whose name starts with `"O"`, we could run the following:'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 SQL 函数都得到了支持。例如，要获取以 `"O"` 开头的候选人的总捐款，我们可以运行以下命令：
- en: '[PRE261]'
  id: totrans-1063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: 'Similarly, to count donations that happened between January 1, 2011 and February
    1, 2011, we can use the `.between` method on the `date` column:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要计算在 2011 年 1 月 1 日至 2011 年 2 月 1 日之间发生的捐款，我们可以在 `date` 列上使用 `.between` 方法：
- en: '[PRE262]'
  id: totrans-1065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: 'The equivalent of the SQL `IN (...)` operator that selects values in a specific
    set is `inSet`. For instance, to select all transactions to Barack Obama and Mitt
    Romney, we can use the following:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 等同于 SQL 中的 `IN (...)` 操作符，用于选择特定集合中的值的是 `inSet`。例如，要选择所有对巴拉克·奥巴马和米特·罗姆尼的交易，我们可以使用以下命令：
- en: '[PRE263]'
  id: totrans-1067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: So, between them, Mitt Romney and Barack Obama received over 28 million dollars
    in registered donations.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 因此， Mitt Romney 和 Barack Obama 之间，他们共收到了超过 2800 万美元的注册捐款。
- en: 'We can also negate a Boolean column with the `!` operator. For instance, to
    calculate the total amount of donations received by all candidates apart from
    Barack Obama and Mitt Romney:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `!` 运算符否定布尔列。例如，要计算除巴拉克·奥巴马和米特·罗姆尼之外所有候选人的总捐款金额：
- en: '[PRE264]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: Column operations are added by implicit conversion on the base `Column` instances.
    For a full list of methods available on String columns, consult the API documentation
    for the `StringColumnExtensionMethods` class ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods)).
    For the methods available on Boolean columns, consult the API documentation for
    the `BooleanColumnExtensionMethods` class ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods)).
    For the methods available on numeric columns, consult the API documentation for
    `NumericColumnExtensionMethods` ([http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods)).
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 列操作是通过在基`Column`实例上隐式转换来添加的。有关字符串列上可用的所有方法的完整列表，请参阅`StringColumnExtensionMethods`类的API文档（[http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.StringColumnExtensionMethods)）。对于布尔列上可用的方法，请参阅`BooleanColumnExtensionMethods`类的API文档（[http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.BooleanColumnExtensionMethods)）。对于数值列上可用的方法，请参阅`NumericColumnExtensionMethods`的API文档（[http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods](http://slick.typesafe.com/doc/2.1.0/api/#scala.slick.lifted.NumericColumnExtensionMethods)）。
- en: Aggregations with "Group by"
  id: totrans-1072
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用“按组分组”进行聚合
- en: 'Slick also provides a `groupBy` method that behaves like the `groupBy` method
    of native Scala collections. Let''s get a list of candidates with all the donations
    for each candidate:'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: Slick还提供了一个`groupBy`方法，其行为类似于原生Scala集合的`groupBy`方法。让我们获取每个候选人的所有捐款的候选人列表：
- en: '[PRE265]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: Let's break this down. The first statement, `transactions.groupBy { _.candidate
    }`, specifies the key by which to group. You can think of this as building an
    intermediate list of `(String, List[Transaction])` tuples mapping the group key
    to a list of all the table rows that satisfy this key. This behavior is identical
    to calling `groupBy` on a Scala collection.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下。第一条语句`transactions.groupBy { _.candidate }`指定了分组的关键字。你可以将其视为构建一个中间列表，其中包含`(String,
    List[Transaction])`元组，将组关键字映射到满足此关键字的表的所有行列表。这种行为与在Scala集合上调用`groupBy`相同。
- en: 'The call to `groupBy` must be followed by a `map` that aggregates the groups.
    The function passed to `map` must take the tuple `(String, List[Transaction])`
    pair created by the `groupBy` call as its sole argument. The `map` call is responsible
    for aggregating the `List[Transaction]` object. We choose to first pick out the
    `amount` field of each transaction, and then to run a sum over these. Finally,
    we call `.list` on the whole pipeline to actually run the query. This just returns
    a Scala list. Let''s convert the total donations from cents to dollars:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy`调用必须后跟一个`map`来聚合组。传递给`map`的函数必须将`groupBy`调用创建的`(String, List[Transaction])`元组对作为其唯一参数。`map`调用负责聚合`List[Transaction]`对象。我们选择首先提取每个交易的`amount`字段，然后对这些字段进行求和。最后，我们在整个管道上调用`.list`来实际运行查询。这仅仅返回一个Scala列表。让我们将总捐款从分转换为美元：'
- en: '[PRE266]'
  id: totrans-1077
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: Accessing database metadata
  id: totrans-1078
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问数据库元数据
- en: 'Commonly, especially during development, you might start the script by dropping
    the table if it exists, then recreating it. We can find if a table is defined
    by accessing the database metadata through the `MTable` object. To get a list
    of tables with name matching a certain pattern, we can run `MTable.getTables(pattern)`:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，尤其是在开发期间，你可能从删除（如果存在）表并重新创建它开始脚本。我们可以通过通过`MTable`对象访问数据库元数据来检查表是否已定义。要获取与特定模式匹配的表列表，我们可以运行`MTable.getTables(pattern)`：
- en: '[PRE267]'
  id: totrans-1080
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: 'Thus, to drop the transactions table if it exists, we can run the following:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了删除（如果存在）交易表，我们可以运行以下操作：
- en: '[PRE268]'
  id: totrans-1082
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: 'The `MTable` instance contains a lot of metadata about the table. Go ahead
    and recreate the `transactions` table if you dropped it in the previous example.
    Then, to find information about the table''s primary keys:'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: '`MTable`实例包含关于表的大量元数据。如果你在前一个示例中删除了它，现在就重新创建`transactions`表。然后，为了找到关于表的主键的信息：'
- en: '[PRE269]'
  id: totrans-1084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: For a full list of methods available on `MTable` instances, consult the Slick
    documentation ([http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable](http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable)).
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`MTable`实例上可用的方法完整列表，请参阅Slick文档([http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable](http://slick.typesafe.com/doc/2.1.0/api/index.html#scala.slick.jdbc.meta.MTable))。
- en: Slick versus JDBC
  id: totrans-1086
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Slick与JDBC的比较
- en: This chapter and the previous one introduced two different ways of interacting
    with SQL. In the previous chapter, we described how to use JDBC and build extensions
    on top of JDBC to make it more usable. In this chapter, we introduced Slick, a
    library that provides a functional interface on top of JDBC.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和上一章介绍了两种不同的与SQL交互的方式。在上一章中，我们描述了如何使用JDBC并在其之上构建扩展以使其更易于使用。在本章中，我们介绍了Slick，这是一个在JDBC之上提供函数式接口的库。
- en: Which method should you choose? If you are starting a new project, you should
    consider using Slick. Even if you spend a considerable amount of time writing
    wrappers that sit on top of JDBC, it is unlikely that you will achieve the fluidity
    that Slick offers.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该选择哪种方法？如果你正在启动一个新项目，你应该考虑使用Slick。即使你花费相当多的时间编写位于JDBC之上的包装器，你也不太可能达到Slick提供的流畅性。
- en: If you are working on an existing project that makes extensive use of JDBC,
    I hope that the previous chapter demonstrates that, with a little time and effort,
    you can write JDBC wrappers that reduce the impedance between the imperative style
    of JDBC and Scala's functional approach.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在对一个大量使用JDBC的现有项目进行工作，我希望上一章的示例表明，只需一点时间和努力，你就可以编写JDBC包装器，以减少JDBC的命令式风格和Scala的函数式方法之间的阻抗。
- en: Summary
  id: totrans-1090
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the previous two chapters, we looked extensively at how to query relational
    databases from Scala. In this chapter, you learned how to use Slick, a "functional-relational"
    mapper that allows interacting with SQL databases as one would with Scala collections.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们广泛地探讨了如何从Scala查询关系型数据库。在本章中，你学习了如何使用Slick，这是一个“函数式关系型”映射器，允许像与Scala集合交互一样与SQL数据库交互。
- en: In the next chapter, you will learn how to ingest data by querying web APIs.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何通过查询Web API来摄取数据。
- en: References
  id: totrans-1093
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: To learn more about Slick, you can refer to the Slick documentation ([http://slick.typesafe.com/doc/2.1.0/](http://slick.typesafe.com/doc/2.1.0/))
    and its API documentation ([http://slick.typesafe.com/doc/2.1.0/api/#package](http://slick.typesafe.com/doc/2.1.0/api/#package)).
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Slick的信息，你可以参考Slick文档([http://slick.typesafe.com/doc/2.1.0/](http://slick.typesafe.com/doc/2.1.0/))及其API文档([http://slick.typesafe.com/doc/2.1.0/api/#package](http://slick.typesafe.com/doc/2.1.0/api/#package))。
- en: Chapter 7. Web APIs
  id: totrans-1095
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。Web API
- en: Data scientists and data engineers get data from a variety of different sources.
    Often, data might come as CSV files or database dumps. Sometimes, we have to obtain
    the data through a web API.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和数据工程师从各种不同的来源获取数据。通常，数据可能以CSV文件或数据库转储的形式出现。有时，我们必须通过Web API获取数据。
- en: 'An individual or organization sets up a web API to distribute data to programs
    over the Internet (or an internal network). Unlike websites, where the data is
    intended to be consumed by a web browser and shown to the user, the data provided
    by a web API is agnostic to the type of program querying it. Web servers serving
    HTML and web servers backing an API are queried in essentially the same way: through
    HTTP requests.'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 个人或组织建立Web API以通过互联网（或内部网络）向程序分发数据。与数据旨在由Web浏览器消费并显示给用户的网站不同，Web API提供的数据对查询它的程序类型是中立的。服务于HTML的Web服务器和支撑API的Web服务器基本上以相同的方式进行查询：通过HTTP请求。
- en: We have already seen an example of a web API in [Chapter 4](part0036.xhtml#aid-12AK82
    "Chapter 4. Parallel Collections and Futures"), *Parallel Collections and Futures*,
    where we queried the "Markit on demand" API for current stock prices. In this
    chapter, we will explore how to interact with web APIs in more detail; specifically,
    how to convert the data returned by the API to Scala objects and how to add additional
    information to the request through HTTP headers (for authentication, for instance).
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第4章](part0036.xhtml#aid-12AK82 "第4章。并行集合和未来")中看到了一个Web API的例子，*并行集合和未来*，其中我们查询了“Markit
    on demand”API以获取当前的股票价格。在本章中，我们将更详细地探讨如何与Web API交互；具体来说，如何将API返回的数据转换为Scala对象，以及如何通过HTTP头（例如，用于身份验证）向请求添加额外信息。
- en: The "Markit on demand" API returned the data formatted as an XML object, but
    increasingly, new web APIs return data formatted as JSON. We will therefore focus
    on JSON in this chapter, but the concepts will port easily to XML.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: “按需Markit”API返回的数据格式化为XML对象，但越来越多的新Web API返回的数据格式化为JSON。因此，本章我们将重点关注JSON，但这些概念很容易应用到XML上。
- en: JSON is a language for formatting structured data. Many readers will have come
    across JSON in the past, but if not, there is a brief introduction to the syntax
    and concepts later on in this chapter. You will find it quite straightforward.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种用于格式化结构化数据的语言。许多读者在过去可能已经遇到过JSON，但如果没有，本章后面将简要介绍其语法和概念。你会发现它非常直观。
- en: In this chapter, we will poll the GitHub API. GitHub has, over the last few
    years, become the de facto tool for collaborating on open source software. It
    provides a powerful, feature-rich API that gives programmatic access to nearly
    all the data available through the website.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将轮询GitHub API。在过去的几年里，GitHub已经成为开源软件协作的事实上工具。它提供了一个功能强大、特性丰富的API，可以以编程方式访问网站上的几乎所有数据。
- en: 'Let''s get a taste of what we can do. Type `api.github.com/users/odersky` in
    your web browser address bar. This will return the data offered by the API on
    a particular user (Martin Odersky, in this case):'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来尝尝我们能做什么。在您的网络浏览器地址栏中输入`api.github.com/users/odersky`。这将返回API提供的特定用户（在这种情况下是Martin
    Odersky）的数据：
- en: '[PRE270]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: The data is returned as a JSON object. This chapter is devoted to learning how
    to access and parse this data programmatically. In [Chapter 13](part0125.xhtml#aid-3N6MA1
    "Chapter 13. Web APIs with Play"), *Web APIs with Play*, you will learn how to
    build your own web API.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以JSON对象的形式返回。本章致力于学习如何以编程方式访问和解析这些数据。在[第13章](part0125.xhtml#aid-3N6MA1 "第13章。使用Play的Web
    API")《使用Play的Web API》中，你将学习如何构建你自己的Web API。
- en: Tip
  id: totrans-1105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The GitHub API is extensive and very well-documented. We will explore some of
    the features of the API in this chapter. To see the full extent of the API, visit
    the documentation ([https://developer.github.com/v3/](https://developer.github.com/v3/)).
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub API非常广泛且文档齐全。在本章中，我们将探索API的一些功能。要查看API的完整范围，请访问文档([https://developer.github.com/v3/](https://developer.github.com/v3/))。
- en: A whirlwind tour of JSON
  id: totrans-1107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON快速浏览
- en: JSON is a format for transferring structured data. It is flexible, easy for
    computers to generate and parse, and relatively readable for humans. It has become
    very common as a means of persisting program data structures and transferring
    data between programs.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种用于传输结构化数据的格式。它灵活，易于计算机生成和解析，对于人类来说相对易于阅读。它已成为持久化程序数据结构和在程序之间传输数据的一种非常常见的方式。
- en: 'JSON has four basic types: **Numbers**, **Strings**, **Booleans**, and **null**,
    and two compound types: **Arrays** and **Objects**. Objects are unordered collections
    of key-value pairs, where the key is always a string and the value can be any
    simple or compound type. We have already seen a JSON object: the data returned
    by the API call `api.github.com/users/odersky`.'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: JSON有四种基本类型：**数字**、**字符串**、**布尔值**和**null**，以及两种复合类型：**数组**和**对象**。对象是无序的键值对集合，其中键始终是字符串，值可以是任何简单或复合类型。我们已经看到了一个JSON对象：API调用`api.github.com/users/odersky`返回的数据。
- en: 'Arrays are ordered lists of simple or compound types. For instance, type [api.github.com/users/odersky/repos](http://api.github.com/users/odersky/repos)
    in your browser to get an array of objects, each representing a GitHub repository:'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 数组是有序的简单或复合类型列表。例如，在您的浏览器中输入[api.github.com/users/odersky/repos](http://api.github.com/users/odersky/repos)，以获取一个对象数组，每个对象代表一个GitHub仓库：
- en: '[PRE271]'
  id: totrans-1111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: We can construct complex structures by nesting objects within other objects
    or arrays. Nevertheless, most web APIs return JSON structures with no more than
    one or two levels of nesting. If you are not familiar with JSON, I encourage you
    to explore the GitHub API through your web browser.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在对象或数组内部嵌套其他对象来构建复杂结构。然而，大多数Web API返回的JSON结构最多只有一两个嵌套层级。如果你不熟悉JSON，我鼓励你通过你的网络浏览器探索GitHub
    API。
- en: Querying web APIs
  id: totrans-1113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询Web API
- en: 'The easiest way of querying a web API from Scala is to use `Source.fromURL`.
    We have already used this in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, when we queried
    the "Markit on demand" API. `Source.fromURL` presents an interface similar to
    `Source.fromFile`:'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Scala 中查询 Web API 最简单的方法是使用 `Source.fromURL`。我们已经在 [第 4 章](part0036.xhtml#aid-12AK82
    "第 4 章。并行集合和未来")，*并行集合和未来* 中使用过它，当时我们查询了 "Markit on demand" API。`Source.fromURL`
    提供了一个类似于 `Source.fromFile` 的接口：
- en: '[PRE272]'
  id: totrans-1115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '`Source.fromURL` returns an iterator over the characters of the response. We
    materialize the iterator into a string using its `.mkString` method. We now have
    the response as a Scala string. The next step is to parse the string with a JSON
    parser.'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Source.fromURL` 返回响应字符的迭代器。我们使用其 `.mkString` 方法将迭代器实体化为一个字符串。现在我们有了作为 Scala
    字符串的响应。下一步是使用 JSON 解析器解析字符串。'
- en: JSON in Scala – an exercise in pattern matching
  id: totrans-1117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala 中的 JSON – 一个模式匹配练习
- en: 'There are several libraries for manipulating JSON in Scala. We prefer json4s,
    but if you are a die-hard fan of another JSON library, you should be able to readily
    adapt the examples in this chapter. Let''s create a `build.sbt` file with a dependency
    on `json4s`:'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 中有几个用于操作 JSON 的库。我们更喜欢 json4s，但如果你是另一个 JSON 库的死忠粉丝，你应该能够轻松地适应本章中的示例。让我们创建一个包含对
    `json4s` 依赖的 `build.sbt` 文件：
- en: '[PRE273]'
  id: totrans-1119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: 'We can then import `json4s` into an SBT console session with:'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以导入 `json4s` 到 SBT 控制台会话中：
- en: '[PRE274]'
  id: totrans-1121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: 'Let''s use `json4s` to parse the response to our GitHub API query:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `json4s` 解析 GitHub API 查询的响应：
- en: '[PRE275]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: The `parse` method takes a string (that contains well-formatted JSON) and converts
    it to a `JValue`, a supertype for all `json4s` objects. The runtime type of the
    response to this particular query is `JObject`, which is a `json4s` type representing
    a JSON object.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse` 方法接受一个字符串（包含格式良好的 JSON），并将其转换为 `JValue`，这是所有 `json4s` 对象的超类型。此特定查询的响应运行时类型为
    `JObject`，它是表示 JSON 对象的 `json4s` 类型。'
- en: '`JObject` is a wrapper around a `List[JField]`, and `JField` represents an
    individual key-value pair in the object. We can use *extractors* to access this
    list:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '`JObject` 是一个围绕 `List[JField]` 的包装器，`JField` 表示对象中的单个键值对。我们可以使用 *提取器* 来访问这个列表：'
- en: '[PRE276]'
  id: totrans-1126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: 'What''s happened here? By writing `val JObject(fields) = ...`, we are telling
    Scala:'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？通过编写 `val JObject(fields) = ...`，我们告诉 Scala：
- en: The right-hand side has runtime type of `JObject`
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧的运行时类型为 `JObject`
- en: Go into the `JObject` instance and bind the list of fields to the constant `fields`
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入 `JObject` 实例并将字段列表绑定到常量 `fields`
- en: Readers familiar with Python might recognize the similarity with tuple unpacking,
    though Scala extractors are much more powerful and versatile. Extractors are used
    extensively to extract Scala types from `json4s` types.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉 Python 的读者可能会注意到与元组解包的相似之处，尽管 Scala 提取器要强大得多，也更加灵活。提取器被广泛用于从 `json4s` 类型中提取
    Scala 类型。
- en: Tip
  id: totrans-1131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Pattern matching using case classes**'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用案例类进行模式匹配**'
- en: 'How exactly does the Scala compiler know what to do with an extractor such
    as:'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 编译器是如何知道如何处理像这样的提取器的：
- en: '[PRE277]'
  id: totrans-1134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '`JObject` is a case class with the following constructor:'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: '`JObject` 是一个具有以下构造函数的案例类：'
- en: '[PRE278]'
  id: totrans-1136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: Case classes all come with an extractor that reverses the constructor exactly.
    Thus, writing `val JObject(fields)` will bind `fields` to the `obj` attribute
    of the `JObject`. For further details on how extractors work, read [Appendix](part0149.xhtml#aid-4E33Q2
    "Appendix A. Pattern Matching and Extractors"), *Pattern Matching and Extractors*.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 所有案例类都附带一个提取器，它可以精确地反转构造函数。因此，编写 `val JObject(fields)` 将将 `fields` 绑定到 `JObject`
    的 `obj` 属性。有关提取器如何工作的更多详细信息，请参阅 [附录](part0149.xhtml#aid-4E33Q2 "附录 A. 模式匹配和提取器")，*模式匹配和提取器*。
- en: 'We have now extracted `fields`, a (plain old Scala) list of fields from the
    `JObject`. A `JField` is a key-value pair, with the key being a string and value
    being a subtype of `JValue`. Again, we can use extractors to extract the values
    in the field:'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从 `JObject` 中提取了 `fields`，这是一个（普通的旧 Scala）字段列表。`JField` 是一个键值对，键是一个字符串，值是
    `JValue` 的子类型。同样，我们可以使用提取器来提取字段中的值：
- en: '[PRE279]'
  id: totrans-1139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: We matched the right-hand side against the pattern `JField(_, JString(_))`,
    binding the first element to `key` and the second to `value`. What happens if
    the right-hand side does not match the pattern?
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将右侧与模式 `JField(_, JString(_))` 匹配，将第一个元素绑定到 `key`，第二个绑定到 `value`。如果右侧不匹配模式会发生什么？
- en: '[PRE280]'
  id: totrans-1141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: 'The code throws a `MatchError` at runtime. These examples demonstrate the power
    of nested pattern matching: in a single line, we managed to verify the type of
    `firstField`, that its value has type `JString`, and we have bound the key and
    value to the `key` and `value` variables, respectively. As another example, if
    we *know* that the first field is the login field, we can both verify this and
    extract the value:'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在运行时抛出`MatchError`。以下示例展示了嵌套模式匹配的强大功能：在一行代码中，我们成功验证了`firstField`的类型，确认其值为`JString`类型，并将键和值分别绑定到`key`和`value`变量。作为另一个例子，如果我们*知道*第一个字段是登录字段，我们既可以验证这一点，也可以提取其值：
- en: '[PRE281]'
  id: totrans-1143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: 'Notice how this style of programming is *declarative* rather than imperative:
    we declare that we want a `JField("login", JString(_))` variable on the right-hand
    side. We then let the language figure out how to check the variable types. Pattern
    matching is a recurring theme in functional languages.'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这种编程风格是*声明式*而不是*命令式*：我们在右侧声明我们想要一个`JField("login", JString(_))`变量。然后让语言找出如何检查变量类型。模式匹配是函数式语言中的一个常见主题。
- en: 'We can also use pattern matching in a for loop when looping over fields. When
    used in a for loop, a pattern match defines a *partial function*: only elements
    that match the pattern pass through the loop. This lets us filter the collection
    for elements that match a pattern and also apply a transformation to these elements.
    For instance, we can extract every string field in our `fields` list:'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在遍历字段时使用模式匹配。当在for循环中使用时，模式匹配定义了一个*部分函数*：只有与模式匹配的元素才会通过循环。这让我们能够过滤出匹配模式的元素集合，并对这些元素应用转换。例如，我们可以从我们的`fields`列表中提取每个字符串字段：
- en: '[PRE282]'
  id: totrans-1146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: 'We can use this to search for specific fields. For instance, to extract the
    `"followers"` field:'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用它来搜索特定字段。例如，提取`"followers"`字段：
- en: '[PRE283]'
  id: totrans-1148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: We first extracted all fields that matched the pattern `JField("follower", JInt(_))`,
    returning the integer inside the `JInt`. As the source collection, `fields`, is
    a list, this returns a list of integers. We then extract the first value from
    this list using `headOption`, which returns the head of the list if the list has
    at least one element, or `None` if the list is empty.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提取所有匹配模式`JField("follower", JInt(_))`的字段，返回`JInt`内部的整数。由于源集合`fields`是一个列表，这返回一个整数列表。然后我们使用`headOption`从该列表中提取第一个值，它如果列表至少有一个元素，则返回列表的头部，如果列表为空，则返回`None`。
- en: 'We are not limited to extracting a single field at a time. For instance, to
    extract the `"id"` and `"login"` fields together:'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于一次提取一个字段。例如，要一起提取`"id"`和`"login"`字段：
- en: '[PRE284]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: Scala's pattern matching and extractors provide you with an extremely powerful
    way of traversing the `json4s` tree, extracting the fields that we need.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的模式匹配和提取器为你提供了一种极其强大的方法来遍历`json4s`树，提取我们需要的字段。
- en: JSON4S types
  id: totrans-1153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON4S类型
- en: 'We have already discovered parts of `json4s`''s type hierarchy: strings are
    wrapped in `JString` objects, integers (or big integers) are wrapped in `JInt`,
    and so on. In this section, we will take a step back and formalize the type structure
    and what Scala types they extract to. These are the `json4s` runtime types:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经发现了`json4s`类型层次结构的一部分：字符串被包裹在`JString`对象中，整数（或大整数）被包裹在`JInt`中，依此类推。在本节中，我们将退后一步，正式化类型结构和它们提取到的Scala类型。这些都是`json4s`的运行时类型：
- en: '`val JString(s) // => extracts to a String`'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JString(s) // => 提取为String`'
- en: '`val JDouble(d) // => extracts to a Double`'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JDouble(d) // => 提取为Double`'
- en: '`val JDecimal(d) // => extracts to a BigDecimal`'
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JDecimal(d) // => 提取为BigDecimal`'
- en: '`val JInt(i) // => extracts to a BigInt`'
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JInt(i) // => 提取为BigInt`'
- en: '`val JBool(b) // => extracts to a Boolean`'
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JBool(b) // => 提取为布尔值`'
- en: '`val JObject(l) // => extracts to a List[JField]`'
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JObject(l) // => 提取为[JField]列表`'
- en: '`val JArray(l) // => extracts to a List[JValue]`'
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val JArray(l) // => 提取为[JValue]列表`'
- en: '`JNull // => represents a JSON null`'
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JNull // => 表示JSON null`'
- en: All these types are subclasses of `JValue`. The compile-time result of `parse`
    is `JValue`, which you normally need to cast to a concrete type using an extractor.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些类型都是`JValue`的子类。`parse`的编译时结果是`JValue`，你通常需要使用提取器将其转换为具体类型。
- en: 'The last type in the hierarchy is `JField`, which represents a key-value pair.
    `JField` is just a type alias for the `(String, JValue)` tuple. It is thus not
    a subtype of `JValue`. We can extract the key and value using the following extractor:'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 层次结构中的最后一个类型是`JField`，它表示键值对。`JField`只是`(String, JValue)`元组的类型别名。因此，它不是`JValue`的子类型。我们可以使用以下提取器提取键和值：
- en: '[PRE285]'
  id: totrans-1165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: Extracting fields using XPath
  id: totrans-1166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用XPath提取字段
- en: 'In the previous sections, you learned how to traverse JSON objects using extractors.
    In this section, we will look at a different way of traversing JSON objects and
    extracting specific fields: the *XPath DSL* (domain-specific language). XPath
    is a query language for traversing tree-like structures. It was originally designed
    for addressing specific nodes in an XML document, but it works just as well with
    JSON. We have already seen an example of XPath syntax when we extracted the stock
    price from the XML document returned by the "Markit on demand" API in [Chapter
    4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and Futures"), *Parallel
    Collections and Futures*. We extracted the node with tag `"LastPrice"` using `r
    \ "LastPrice"`. The `\` operator was defined by the `scala.xml` package.'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了如何使用提取器遍历 JSON 对象。在本节中，我们将探讨另一种遍历 JSON 对象和提取特定字段的方法：*XPath DSL*（领域特定语言）。XPath
    是一种用于遍历树状结构的查询语言。它最初是为在 XML 文档中定位特定节点而设计的，但它同样适用于 JSON。当我们从“Markit on demand”API返回的
    XML 文档中提取股票价格时，我们已经看到了 XPath 语法的示例，这在[第 4 章](part0036.xhtml#aid-12AK82 "第 4 章。并行集合和未来")中，*并行集合和未来*。我们使用
    `r \ "LastPrice"` 提取了标签为 `"LastPrice"` 的节点。`\` 操作符是由 `scala.xml` 包定义的。
- en: 'The `json4s` package exposes a similar DSL to extract fields from `JObject`
    instances. For instance, we can extract the `"login"` field from the JSON object
    `jsonResponse`:'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: '`json4s` 包提供了一个类似的 DSL 来从 `JObject` 实例中提取字段。例如，我们可以从 JSON 对象 `jsonResponse`
    中提取 `"login"` 字段：'
- en: '[PRE286]'
  id: totrans-1169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: 'This returns a `JValue` that we can transform into a Scala string using an
    extractor:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了一个 `JValue`，我们可以使用提取器将其转换为 Scala 字符串：
- en: '[PRE287]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: 'Notice the similarity between the XPath DSL and traversing a filesystem: we
    can think of `JObject` instances as directories. Field names correspond to file
    names and the field value to the content of the file. This is more evident for
    nested structures. The `users` endpoint of the GitHub API does not have nested
    documents, so let''s try another endpoint. We will query the API for the repository
    corresponding to this book: "[https://api.github.com/repos/pbugnion/s4ds](https://api.github.com/repos/pbugnion/s4ds)".
    The response has the following structure:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 XPath DSL 和遍历文件系统的相似性：我们可以将 `JObject` 实例视为目录。字段名对应文件名，字段值对应文件内容。这在嵌套结构中更为明显。GitHub
    API 的 `users` 端点没有嵌套文档，所以让我们尝试另一个端点。我们将查询与这本书对应的仓库的 API："[https://api.github.com/repos/pbugnion/s4ds](https://api.github.com/repos/pbugnion/s4ds)"。响应具有以下结构：
- en: '[PRE288]'
  id: totrans-1173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: 'Let''s fetch this document and use the XPath syntax to extract the repository
    owner''s login name:'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取这个文档，并使用 XPath 语法提取仓库所有者的登录名：
- en: '[PRE289]'
  id: totrans-1175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: 'Again, this is much like traversing a filesystem: `jsonResponse \ "owner"`
    returns a `JObject` corresponding to the `"owner"` object. This `JObject` can,
    in turn, be queried for the `"login"` field, returning the value `JString(pbugnion)`
    associated with this key.'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这类似于遍历文件系统：`jsonResponse \ "owner"` 返回与 `"owner"` 对象相对应的 `JObject`。这个 `JObject`
    可以进一步查询 `"login"` 字段，返回与该键关联的值 `JString(pbugnion)`。
- en: 'What if the API response is an array? The filesystem analogy breaks down somewhat.
    Let''s query the API endpoint listing Martin Odersky''s repositories: [https://api.github.com/users/odersky/repos](https://api.github.com/users/odersky/repos).
    The response is an array of JSON objects, each of which represents a repository:'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 API 响应是一个数组呢？文件系统类比就有些不适用了。让我们查询列出马丁·奥德斯基仓库的 API 端点：[https://api.github.com/users/odersky/repos](https://api.github.com/users/odersky/repos)。响应是一个包含
    JSON 对象的数组，每个对象代表一个仓库：
- en: '[PRE290]'
  id: totrans-1178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: 'Let''s fetch this and parse it as JSON:'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取这个文档并将其解析为 JSON：
- en: '[PRE291]'
  id: totrans-1180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: 'This returns a `JArray`. The XPath DSL works in the same way on a `JArray`
    as on a `JObject`, but now, instead of returning a single `JValue`, it returns
    an array of fields matching the path in every object in the array. Let''s get
    the size of all Martin Odersky''s repositories:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了一个 `JArray`。XPath DSL 在 `JArray` 上的工作方式与在 `JObject` 上相同，但现在，它返回的是一个与数组中每个对象的路径匹配的字段数组。让我们获取所有马丁·奥德斯基的仓库的大小：
- en: '[PRE292]'
  id: totrans-1182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'We now have a `JArray` of the values corresponding to the `"size"` field in
    every repository. We can iterate over this array with a `for` comprehension and
    use extractors to convert elements to Scala objects:'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个包含每个仓库中 `"size"` 字段值的 `JArray`。我们可以使用 `for` 理解遍历这个数组，并使用提取器将元素转换为 Scala
    对象：
- en: '[PRE293]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: Thus, combining extractors with the XPath DSL gives us powerful, complementary
    tools to extract information from JSON objects.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结合提取器和 XPath DSL，我们得到了从 JSON 对象中提取信息的有力、互补的工具。
- en: There is much more to the XPath syntax than we have space to cover here, including
    the ability to extract fields nested at any level of depth below the current root
    or fields that match a predicate or a certain type. We find that well-designed
    APIs obviate the need for many of these more powerful functions, but do consult
    the documentation (`json4s.org`) to get an overview of what you can do.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: XPath 语法远比这里能涵盖的要多，包括从当前根的任何深度提取嵌套字段的能力，或者匹配谓词或特定类型的字段。我们发现，设计良好的 API 可以消除许多这些更强大功能的需求，但请查阅文档（`json4s.org`）以了解您可以做什么的概述。
- en: In the next section, we will look at extracting JSON directly into case classes.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何直接将 JSON 提取到案例类中。
- en: Extraction using case classes
  id: totrans-1188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例类进行提取
- en: In the previous sections, we extracted specific fields from the JSON response
    using Scala extractors. We can do one better and extract full case classes.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用 Scala 提取器从 JSON 响应中提取了特定字段。我们可以做得更好，提取完整的案例类。
- en: 'When moving beyond the REPL, programming best practice dictates that we move
    from `json4s` types to Scala objects as soon as possible rather than passing `json4s`
    types around the program. Converting from `json4s` types to Scala types (or case
    classes representing domain objects) is good practice because:'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们超出 REPL 时，编程最佳实践规定我们应该尽快从 `json4s` 类型移动到 Scala 对象，而不是在程序中传递 `json4s` 类型。从
    `json4s` 类型转换为 Scala 类型（或表示域对象的案例类）是良好的实践，因为：
- en: It decouples the program from the structure of the data that we receive from
    the API, something we have little control over.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将程序与从 API 收到的数据结构解耦，我们对这些结构几乎没有控制权。
- en: 'It improves type safety: a `JObject` is, as far as the compiler is concerned,
    always a `JObject`, whatever fields it contains. By contrast, the compiler will
    never mistake a `User` for a `Repository`.'
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提高了类型安全性：从编译器的角度来看，`JObject` 总是 `JObject`，无论它包含哪些字段。相比之下，编译器永远不会将 `User` 错误地认为是
    `Repository`。
- en: '`Json4s` lets us extract case classes directly from `JObject` instances, making
    writing the layer converting `JObject` instances to custom types easy.'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Json4s` 允许我们直接从 `JObject` 实例中提取案例类，这使得将 `JObject` 实例转换为自定义类型层的编写变得简单。'
- en: 'Let''s define a case class representing a GitHub user:'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个表示 GitHub 用户的案例类：
- en: '[PRE294]'
  id: totrans-1195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: 'To extract a case class from a `JObject`, we must first define an implicit
    `Formats` value that defines how simple types should be serialized and deserialized.
    We will use the default `DefaultFormats` provided with `json4s`:'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 `JObject` 中提取案例类，我们首先必须定义一个隐式 `Formats` 值，该值定义了简单类型应该如何序列化和反序列化。我们将使用 `json4s`
    提供的默认 `DefaultFormats`：
- en: '[PRE295]'
  id: totrans-1197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: 'We can now extract instances of `User`. Let''s do this for Martin Odersky:'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以提取 `User` 的实例。让我们为马丁·奥德斯基（Martin Odersky）做这个操作：
- en: '[PRE296]'
  id: totrans-1199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: 'This works as long as the object is well-formatted. The `extract` method looks
    for fields in the `JObject` that match the attributes of `User`. In this case,
    `extract` will note that the `JObject` contains the `"login": "odersky"` field
    and that `JString("odersky")` can be converted to a Scala string, so it binds
    `"odersky"` to the `login` attribute in `User`.'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: '只要对象格式良好，这种方法就有效。`extract` 方法在 `JObject` 中寻找与 `User` 属性匹配的字段。在这种情况下，`extract`
    会注意到 `JObject` 包含 `"login": "odersky"` 字段，并且 `JString("odersky")` 可以转换为 Scala
    字符串，因此它将 `"odersky"` 绑定到 `User` 中的 `login` 属性。'
- en: 'What if the attribute names differ from the field names in the JSON object?
    We must first transform the object to have the correct fields. For instance, let''s
    rename the `login` attribute to `userName` in our `User` class:'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果属性名称与 JSON 对象中的字段名称不同怎么办？我们必须首先将对象转换为具有正确字段的形式。例如，让我们将 `User` 类中的 `login`
    属性重命名为 `userName`：
- en: '[PRE297]'
  id: totrans-1202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: 'If we try to use `extract[User]` on `jsonResponse`, we will get a mapping error
    because the deserializer is missing a `login` field in the response. We can fix
    this using the `transformField` method on `jsonResponse` to rename the `login`
    field:'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试在 `jsonResponse` 上使用 `extract[User]`，我们将得到一个映射错误，因为反序列化器在响应中缺少 `login`
    字段。我们可以通过在 `jsonResponse` 上使用 `transformField` 方法来重命名 `login` 字段来修复这个问题：
- en: '[PRE298]'
  id: totrans-1204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: 'What about optional fields? Let''s assume that the JSON object returned by
    the GitHub API does not always contain the login field. We could symbolize this
    in our object model by giving the `login` parameter the type `Option[String]`
    rather than `String`:'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可选字段怎么办？假设 GitHub API 返回的 JSON 对象并不总是包含 `login` 字段。我们可以在对象模型中通过将 `login` 参数的类型指定为
    `Option[String]` 而不是 `String` 来表示这一点：
- en: '[PRE299]'
  id: totrans-1206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: 'This works just as you would expect. When the response contains a non-null
    `login` field, calling `extract[User]` will deserialize it to `Some(value)`, and
    when it''s missing or `JNull`, it will produce `None`:'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 这正如你所期望的那样工作。当响应包含非空的`login`字段时，调用`extract[User]`会将其反序列化为`Some(value)`，如果它缺失或为`JNull`，则会产生`None`：
- en: '[PRE300]'
  id: totrans-1208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: 'Let''s wrap this up in a small program. The program will take a single command-line
    argument, the user''s login name, extract a `User` instance, and print it to screen:'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个功能封装在一个小的程序中。该程序将接受一个命令行参数，即用户的登录名，提取一个`User`实例，并将其打印到屏幕上：
- en: '[PRE301]'
  id: totrans-1210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'We can run this from an SBT console as follows:'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从SBT控制台按照以下方式运行此程序：
- en: '[PRE302]'
  id: totrans-1212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: Concurrency and exception handling with futures
  id: totrans-1213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用未来（futures）进行并发和异常处理
- en: While the program that we wrote in the previous section works, it is very brittle.
    It will crash if we enter a non-existent user name or the GitHub API changes or
    returns a badly-formatted response. We need to make it fault-tolerant.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在上一节中编写的程序可以工作，但它非常脆弱。如果我们输入一个不存在的用户名，或者GitHub API发生变化或返回格式错误的响应，它将会崩溃。我们需要使其具有容错性。
- en: What if we also wanted to fetch multiple users? The program, as written, is
    entirely single-threaded. The `fetchUserFromUrl` method fires a call to the API
    and blocks until the API sends data back. A better solution would be to fetch
    multiple users in parallel.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还想获取多个用户呢？按照目前的程序编写方式，它是完全单线程的。`fetchUserFromUrl`方法会向API发起调用并阻塞，直到API返回数据。一个更好的解决方案是并行地获取多个用户。
- en: 'As you learned in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, there are two straightforward
    ways to implement both fault tolerance and parallel execution: we can either put
    all the user names in a parallel collection and wrap the code for fetching and
    extracting the user in a `Try` block or we can wrap each query in a future.'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第4章](part0036.xhtml#aid-12AK82 "第4章。并行集合和未来")，*并行集合和未来*中学到的，实现容错性和并行执行有两种简单的方法：我们可以将所有用户名放入并行集合中，并将获取和提取用户的代码封装在`Try`块中，或者我们可以将每个查询封装在未来的框架中。
- en: When querying web APIs, it is sometimes the case that a request can take abnormally
    long. To prevent this from blocking the other threads, it is preferable to rely
    on futures rather than parallel collections for concurrency, as we saw in the
    *Parallel collection or Future?* section at the end of [Chapter 4](part0036.xhtml#aid-12AK82
    "Chapter 4. Parallel Collections and Futures"), *Parallel Collections and Futures*.
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询网络API时，有时请求可能会异常地长时间。为了避免这阻碍其他线程，最好依赖于未来（futures）而不是并行集合（parallel collections）来实现并发，正如我们在[第4章](part0036.xhtml#aid-12AK82
    "第4章。并行集合和未来")末尾的*并行集合或未来？*部分所看到的，*并行集合和未来*。
- en: 'Let''s rewrite the code from the previous section to handle fetching multiple
    users concurrently in a fault-tolerant manner. We will change the `fetchUserFromUrl`
    method to query the API asynchronously. This is not terribly different from [Chapter
    4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and Futures"), *Parallel
    Collections and Futures*, in which we queried the "Markit on demand" API:'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写上一节中的代码，以并行且容错地获取多个用户。我们将把`fetchUserFromUrl`方法改为异步查询API。这与[第4章](part0036.xhtml#aid-12AK82
    "第4章。并行集合和未来")中的内容没有太大区别，我们在其中查询了"Markit on demand" API：
- en: '[PRE303]'
  id: totrans-1219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: 'Let''s run the code through `sbt`:'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过`sbt`运行这段代码：
- en: '[PRE304]'
  id: totrans-1221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: 'The code itself should be straightforward. All the concepts used here have
    been explored in this chapter or in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel
    Collections and Futures"), *Parallel Collections and Futures*, apart from the
    last line:'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: 代码本身应该是直截了当的。这里使用的所有概念都已在本章或[第4章](part0036.xhtml#aid-12AK82 "第4章。并行集合和未来")中探讨过，除了最后一行：
- en: '[PRE305]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE305]'
- en: This statement tells the program to wait until all futures in our list have
    been completed. `Await.ready(..., 1 minute)` takes a future as its first argument
    and blocks execution until this future returns. The second argument is a time-out
    on this future. The only catch is that we need to pass a single future to `Await`
    rather than a list of futures. We can use `Future.sequence` to merge a collection
    of futures into a single future. This future will be completed when all the futures
    in the sequence have completed.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 这条语句指示程序等待我们列表中的所有未来都已完成。`Await.ready(..., 1 minute)`将一个未来作为其第一个参数，并在该未来返回之前阻塞执行。第二个参数是对这个未来的超时。唯一的缺点是我们需要将单个未来传递给`Await`，而不是未来列表。我们可以使用`Future.sequence`将一组未来合并为一个单一的未来。这个未来将在序列中的所有未来都完成后完成。
- en: Authentication – adding HTTP headers
  id: totrans-1225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 身份验证 – 添加HTTP头部
- en: So far, we have been using the GitHub API without authentication. This limits
    us to sixty requests per hour. Now that we can query the API in parallel, we could
    exceed this limit in seconds.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用未经身份验证的GitHub API。这限制了我们每小时只能进行六十次请求。现在我们可以在并行查询API的情况下，几秒钟内就能超过这个限制。
- en: Fortunately, GitHub is much more generous if you authenticate when you query
    the API. The limit increases to 5,000 requests per hour. You must have a GitHub
    user account to authenticate, so go ahead and create one now if you need to. After
    creating an account, navigate to [https://github.com/settings/tokens](https://github.com/settings/tokens)
    and click on the **Generate new token** button. Accept the default settings and
    enter a token description and a long hexadecimal number should appear on the screen.
    Copy the token for now.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如果您在查询API时进行身份验证，GitHub会慷慨得多。限制增加到每小时5,000次请求。您必须有一个GitHub用户账户才能进行身份验证，所以如果您需要，请现在就创建一个账户。创建账户后，导航到[https://github.com/settings/tokens](https://github.com/settings/tokens)并点击**生成新令牌**按钮。接受默认设置，并在屏幕上输入令牌描述，应该会出现一个长的十六进制数字。现在先复制令牌。
- en: HTTP – a whirlwind overview
  id: totrans-1228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTP – 快速概述
- en: Before using our newly generated token, let's take a few minutes to review how
    HTTP works.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用我们新生成的令牌之前，让我们花几分钟时间回顾一下HTTP是如何工作的。
- en: HTTP is a protocol for transferring information between different computers.
    It is the protocol that we have been using throughout the chapter, though Scala
    hid the details from us in the call to `Source.fromURL`. It is also the protocol
    that you use when you point your web browser to a website, for instance.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP是在不同计算机之间传输信息的协议。这是我们本章一直在使用的协议，尽管Scala在`Source.fromURL`调用中隐藏了这些细节。它也是您在将网络浏览器指向网站时使用的协议，例如。
- en: In HTTP, a computer will typically make a *request* to a remote server, and
    the server will send back a *response*. Requests contain a *verb*, which defines
    the type of request, and a URL identifying a *resource*. For instance, when we
    typed [api.github.com/users/pbugnion](http://api.github.com/users/pbugnion) in
    our browsers, this was translated into a GET (the verb) request for the `users/pbugnion`
    resource. All the calls that we have made so far have been GET requests. You might
    use a different type of request, for instance, a POST request, to modify (rather
    than just view) some content on GitHub.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTTP中，一台计算机通常会向远程服务器发送一个*请求*，服务器会返回一个*响应*。请求包含一个*动词*，它定义了请求的类型，以及一个标识*资源*的URL。例如，当我们在我们浏览器中键入[api.github.com/users/pbugnion](http://api.github.com/users/pbugnion)时，这被转换为一个针对`users/pbugnion`资源的GET（动词）请求。我们迄今为止所做的一切调用都是GET请求。您可能会使用不同类型的请求，例如POST请求，来修改（而不仅仅是查看）GitHub上的某些内容。
- en: 'Besides the verb and resource, there are two more parts to an HTTP request:'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了动词和资源之外，HTTP请求还有两个其他部分：
- en: 'The *headers* include metadata about the request, such as the expected format
    and character set of the response or the authentication credentials. Headers are
    just a list of key-value pairs. We will pass the OAuth token that we have just
    generated to the API using the `Authorization` header. This Wikipedia article
    lists commonly used header fields: [en.wikipedia.org/wiki/List_of_HTTP_header_fields](http://en.wikipedia.org/wiki/List_of_HTTP_header_fields).'
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*头部*包含了关于请求的元数据，例如响应的预期格式和字符集或身份验证凭据。头部只是一个键值对的列表。我们将使用`Authorization`头部将我们刚刚生成的OAuth令牌传递给API。这篇维基百科文章列出了常用的头部字段：[en.wikipedia.org/wiki/List_of_HTTP_header_fields](http://en.wikipedia.org/wiki/List_of_HTTP_header_fields)。'
- en: The request body is not used in GET requests but becomes important for requests
    that modify the resource they query. For instance, if I wanted to create a new
    repository on GitHub programmatically, I would send a POST request to `/pbugnion/repos`.
    The POST body would then be a JSON object describing the new repository. We will
    not use the request body in this chapter.
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求体在GET请求中不被使用，但在修改它们查询的资源时变得重要。例如，如果我想通过编程方式在GitHub上创建一个新的仓库，我会向`/pbugnion/repos`发送POST请求。POST体将是一个描述新仓库的JSON对象。在本章中，我们不会使用请求体。
- en: Adding headers to HTTP requests in Scala
  id: totrans-1235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scala中向HTTP请求添加头信息
- en: We will pass the OAuth token as a header with our HTTP request. Unfortunately,
    the `Source.fromURL` method is not particularly suited to adding headers when
    creating a GET request. We will, instead, use a library, `scalaj-http`.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在HTTP请求中传递OAuth令牌作为头信息。不幸的是，`Source.fromURL`方法在创建GET请求时添加头信息并不特别适合。我们将改用库，`scalaj-http`。
- en: 'Let''s add `scalaj-http` to the dependencies in our `build.sbt`:'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`build.sbt`的依赖项中添加`scalaj-http`：
- en: '[PRE306]'
  id: totrans-1238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: 'We can now import `scalaj-http`:'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以导入`scalaj-http`：
- en: '[PRE307]'
  id: totrans-1240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: 'We start by creating an `HttpRequest` object:'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个`HttpRequest`对象：
- en: '[PRE308]'
  id: totrans-1242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: 'We can now add the authorization header to the request (add your own token
    string here):'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以向请求添加授权头（在此处添加你自己的令牌字符串）：
- en: '[PRE309]'
  id: totrans-1244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: Tip
  id: totrans-1245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The `.header` method returns a new `HttpRequest` instance. It does not modify
    the request in place. Thus, just calling `request.header(...)` does not actually
    add the header to request itself, which can be a source of confusion.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: '`.header`方法返回一个新的`HttpRequest`实例。它不会就地修改请求。因此，仅仅调用`request.header(...)`实际上并没有将头信息添加到请求本身，这可能会引起混淆。'
- en: 'Let''s fire the request. We do this through the request''s `asString` method,
    which queries the API, fetches the response, and parses it as a Scala `String`:'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们发起请求。我们通过请求的`asString`方法来完成，该方法查询API，获取响应，并将其解析为Scala `String`：
- en: '[PRE310]'
  id: totrans-1248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: 'The response is made up of three components:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 响应由三个部分组成：
- en: 'The status code, which should be `200` for a successful request:'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态码，对于成功的请求应该是`200`：
- en: '[PRE311]'
  id: totrans-1251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE311]'
- en: 'The response body, which is the part that we are interested in:'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应体，这是我们感兴趣的部分：
- en: '[PRE312]'
  id: totrans-1253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE312]'
- en: 'The response headers (metadata about the response):'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应头（关于响应的元数据）：
- en: '[PRE313]'
  id: totrans-1255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE313]'
- en: 'To verify that the authorization was successful, query the `X-RateLimit-Limit`
    header:'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证授权是否成功，查询`X-RateLimit-Limit`头：
- en: '[PRE314]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: This value is the maximum number of requests per hour that you can make to the
    GitHub API from a single IP address.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值是你可以从单个IP地址向GitHub API发起的最大每小时请求数量。
- en: 'Now that we have some understanding of how to add authentication to GET requests,
    let''s modify our script for fetching users to use the OAuth token for authentication.
    We first need to import `scalaj-http`:'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对如何向GET请求添加认证有了些了解，让我们修改我们的用户获取脚本，使用OAuth令牌进行认证。我们首先需要导入`scalaj-http`：
- en: '[PRE315]'
  id: totrans-1260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: 'Injecting the value of the token into the code can be somewhat tricky. You
    might be tempted to hardcode it, but this prohibits you from sharing the code.
    A better solution is to use an *environment variable*. Environment variables are
    a set of variables present in your terminal session that are accessible to all
    processes running in that session. To get a list of the current environment variables,
    type the following on Linux or Mac OS:'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 将令牌的值注入到代码中可能有些棘手。你可能想将其硬编码，但这会阻止你共享代码。更好的解决方案是使用*环境变量*。环境变量是在你的终端会话中存在的一组变量，该会话中的所有进程都可以访问这些变量。要获取当前环境变量的列表，请在Linux或Mac
    OS上输入以下内容：
- en: '[PRE316]'
  id: totrans-1262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: 'On Windows, the equivalent command is `SET`. Let''s add the GitHub token to
    the environment. Use the following command on Mac OS or Linux:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，等效的命令是`SET`。让我们将GitHub令牌添加到环境变量中。在Mac OS或Linux上使用以下命令：
- en: '[PRE317]'
  id: totrans-1264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: 'On Windows, use the following command:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，使用以下命令：
- en: '[PRE318]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: If you were to reuse this environment variable across many projects, entering
    `export GHTOKEN=...` in the shell for every session gets old quite quickly. A
    more permanent solution is to add `export GHTOKEN="e83638…"` to your shell configuration
    file (your `.bashrc` file if you are using Bash). This is safe provided your `.bashrc`
    is readable by the user only. Any new shell session will have access to the `GHTOKEN`
    environment variable.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算在许多项目中重用这个环境变量，那么在每次会话中输入`export GHTOKEN=...`会很快变得令人厌烦。一个更持久的解决方案是将`export
    GHTOKEN="e83638…"`添加到你的shell配置文件中（如果你使用Bash，则是你的`.bashrc`文件）。只要你的`.bashrc`只能被用户读取，这将是安全的。任何新的shell会话都将能够访问`GHTOKEN`环境变量。
- en: 'We can access environment variables from a Scala program using `sys.env`, which
    returns a `Map[String, String]` of the variables. Let''s add a `lazy val token`
    to our class, containing the `token` value:'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sys.env`从Scala程序中访问环境变量，它返回一个包含变量的`Map[String, String]`。让我们在我们的类中添加一个`lazy
    val token`，包含`token`值：
- en: '[PRE319]'
  id: totrans-1269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: 'Now that we have the token, the only part of the code that must change, to
    add authentication, is the `fetchUserFromUrl` method:'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了令牌，唯一需要更改代码的部分，以添加身份验证，就是`fetchUserFromUrl`方法：
- en: '[PRE320]'
  id: totrans-1271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: Additionally, we can, to gain clearer error messages, check that the response's
    status code is 200\. As this is straightforward, it is left as an exercise.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了获得更清晰的错误信息，我们可以检查响应的状态码是否为200。由于这很简单，所以留作练习。
- en: Summary
  id: totrans-1273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, you learned how to query the GitHub API, converting the response
    to Scala objects. Of course, merely printing results to screen is not terribly
    interesting. In the next chapter, we will look at the next step of the data ingestion
    process: storing data in a database. We will query the GitHub API and store the
    results in a MongoDB database.'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何查询GitHub API，将响应转换为Scala对象。当然，仅仅将结果打印到屏幕上并不十分有趣。在下一章中，我们将探讨数据摄取过程的下一步：将数据存储在数据库中。我们将查询GitHub
    API并将结果存储在MongoDB数据库中。
- en: In [Chapter 13](part0125.xhtml#aid-3N6MA1 "Chapter 13. Web APIs with Play"),
    *Web APIs with Play*, we will look at building our own simple web API.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第13章](part0125.xhtml#aid-3N6MA1 "第13章. 使用Play构建Web API")，*使用Play构建Web API*中，我们将探讨构建我们自己的简单Web
    API。
- en: References
  id: totrans-1276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The GitHub API, with its extensive documentation, is a good place to explore
    how a rich API is constructed. It has a **Getting Started** section that is worth
    reading:'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub API，凭借其详尽的文档，是探索如何构建丰富API的好地方。它有一个**入门**部分值得一读：
- en: '[https://developer.github.com/guides/getting-started/](https://developer.github.com/guides/getting-started/)'
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://developer.github.com/guides/getting-started/](https://developer.github.com/guides/getting-started/)'
- en: 'Of course, this is not specific to Scala: it uses cURL to query the API.'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这不仅仅针对Scala：它使用cURL查询API。
- en: Read the documentation ([http://json4s.org](http://json4s.org)) and source code
    ([https://github.com/json4s/json4s](https://github.com/json4s/json4s)) for `json4s`
    for a complete reference. There are many parts of this package that we have not
    explored, in particular, how to build JSON from Scala.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 请阅读`json4s`的文档([http://json4s.org](http://json4s.org))和源代码([https://github.com/json4s/json4s](https://github.com/json4s/json4s))以获取完整的参考。这个包的许多部分我们还没有探索，特别是如何从Scala构建JSON。
- en: Chapter 8. Scala and MongoDB
  id: totrans-1281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. Scala和MongoDB
- en: In [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*, and [Chapter 6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick
    – A Functional Interface for SQL"), *Slick – A Functional Interface for SQL*,
    you learned how to insert, transform, and read data in SQL databases. These databases
    remain (and are likely to remain) very popular in data science, but NoSQL databases
    are emerging as strong contenders.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](part0040.xhtml#aid-164MG1 "第5章. 通过JDBC的Scala和SQL")，*通过JDBC的Scala和SQL*和[第6章](part0051.xhtml#aid-1GKCM2
    "第6章. Slick – SQL的功能接口")，*Slick – SQL的功能接口*中，你学习了如何在SQL数据库中插入、转换和读取数据。这些数据库在数据科学中仍然（并且可能仍然）非常受欢迎，但NoSQL数据库正在成为强劲的竞争者。
- en: 'The needs for data storage are growing rapidly. Companies are producing and
    storing more data points in the hope of acquiring better business intelligence.
    They are also building increasingly large teams of data scientists, who all need
    to access the data store. Maintaining constant access time as the data load increases
    requires taking advantage of parallel architectures: we need to distribute the
    database across several computers so that, as the load on the server increases,
    we can just add more machines to improve throughput.'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储的需求正在迅速增长。公司正在生产和存储更多的数据点，希望获得更好的商业智能。他们也在组建越来越大的数据科学家团队，所有这些人都需要访问数据存储。随着数据负载的增加，保持恒定的访问时间需要利用并行架构：我们需要将数据库分布在几台计算机上，这样当服务器负载增加时，我们只需添加更多机器来提高吞吐量。
- en: In MySQL databases, the data is naturally split across different tables. Complex
    queries necessitate joining across several tables. This makes partitioning the
    database across different computers difficult. NoSQL databases emerged to fill
    this gap.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 在MySQL数据库中，数据自然地分布在不同的表中。复杂的查询需要跨多个表进行连接。这使得在多台计算机上分区数据库变得困难。NoSQL数据库的出现填补了这一空白。
- en: In this chapter, you will learn to interact with MongoDB, an open source database
    that offers high performance and can be distributed easily. MongoDB is one of
    the more popular NoSQL databases with a strong community. It offers a reasonable
    balance of speed and flexibility, making it a natural alternative to SQL for storing
    large datasets with uncertain query requirements, as might happen in data science.
    Many of the concepts and recipes in this chapter will apply to other NoSQL databases.
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何与MongoDB交互，这是一个提供高性能且易于分布的开源数据库。MongoDB是更受欢迎的NoSQL数据库之一，拥有强大的社区。它提供了速度和灵活性的合理平衡，使其成为存储具有不确定查询要求的大型数据集（如数据科学中可能发生的情况）的SQL的自然替代品。本章中的许多概念和食谱也适用于其他NoSQL数据库。
- en: MongoDB
  id: totrans-1286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MongoDB
- en: 'MongoDB is a *document-oriented* database. It contains collections of documents.
    Each document is a JSON-like object:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB是一个*面向文档*的数据库。它包含文档集合。每个文档都是一个类似JSON的对象：
- en: '[PRE321]'
  id: totrans-1288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: Just as in JSON, a document is a set of key-value pairs, where the values can
    be strings, numbers, Booleans, dates, arrays, or subdocuments. Documents are grouped
    in collections, and collections are grouped in databases.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 正如JSON一样，文档是一组键值对，其中值可以是字符串、数字、布尔值、日期、数组或子文档。文档被分组在集合中，集合被分组在数据库中。
- en: 'You might be thinking that this is not very different from SQL: a document
    is similar to a row and a collection corresponds to a table. There are two important
    differences:'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这与SQL并没有太大的不同：一个文档类似于一行，一个集合对应一个表。但存在两个重要的区别：
- en: The values in documents can be simple values, arrays, subdocuments, or arrays
    of subdocuments. This lets us encode one-to-many and many-to-many relationships
    in a single collection. For instance, consider the wizard collection. In SQL,
    if we wanted to store pseudonyms for each wizard, we would have to use a separate
    `wizard2pseudonym` table with a row for each wizard-pseudonym pair. In MongoDB,
    we can just use an array. In practice, this means that we can normally use a single
    document to represent an entity (a customer, transaction, or wizard, for instance).
    In SQL, we would normally have to join across several tables to retrieve all the
    information on a specific entity.
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档中的值可以是简单值、数组、子文档或子文档数组。这使得我们可以在单个集合中编码一对一和多对多关系。例如，考虑巫师集合。在SQL中，如果我们想为每个巫师存储化名，我们必须使用一个单独的`wizard2pseudonym`表，并为每个巫师-化名对创建一行。在MongoDB中，我们只需使用一个数组。在实践中，这意味着我们可以通常用一个文档来表示一个实体（例如客户、交易或巫师）。在SQL中，我们通常需要跨多个表进行连接，以检索特定实体的所有信息。
- en: 'MongoDB is *schemaless*. Documents in a collection can have varying sets of
    fields with different types for the same field across different documents. In
    practice, MongoDB collections have a loose schema enforced either client side
    or by convention: most documents will have a subset of the same fields, and fields
    will, in general, contain the same data type. Having a flexible schema makes adjusting
    the data structure easy as there is no need for time-consuming `ALTER` `TABLE`
    statements. The downside is that there is no easy way of enforcing our flexible
    schema on the database side.'
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB是*无模式的*。集合中的文档可以具有不同的字段集，不同文档中同一字段的类型也可以不同。在实践中，MongoDB集合有一个松散的模式，由客户端或约定强制执行：大多数文档将具有相同字段的子集，字段通常将包含相同的数据类型。具有灵活的模式使得调整数据结构变得容易，因为不需要耗时的`ALTER
    TABLE`语句。缺点是，在数据库端没有简单的方法来强制执行我们的灵活模式。
- en: 'Note the `_id` field: this is a unique key. MongoDB will generate one automatically
    if we insert a document without an `_id` field.'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`_id`字段：这是一个唯一键。如果我们插入一个没有`_id`字段的文档，MongoDB将自动生成一个。
- en: This chapter gives recipes for interacting with a MongoDB database from Scala,
    including maintaining type safety and best practices. We will not cover advanced
    MongoDB functionality (such as aggregation or distributing the database). We will
    assume that you have MongoDB installed on your computer ([http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)).
    It will also help to have a very basic knowledge of MongoDB (we discuss some references
    at the end of this chapter, but any basic tutorial available online will be sufficient
    for the needs of this chapter).
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了从Scala与MongoDB数据库交互的配方，包括维护类型安全和最佳实践。我们不会涵盖高级MongoDB功能（如聚合或数据库的分布式）。我们假设您已在您的计算机上安装了MongoDB（[http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)）。对MongoDB有非常基本的了解也会有所帮助（我们将在本章末尾讨论一些参考资料，但任何在线可用的基本教程都足以满足本章的需求）。
- en: Connecting to MongoDB with Casbah
  id: totrans-1295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Casbah连接到MongoDB
- en: The official MongoDB driver for Scala is called **Casbah**. Rather than a fully-fledged
    driver, Casbah wraps the Java Mongo driver, providing a more functional interface.
    There are other MongoDB drivers for Scala, which we will discuss briefly at the
    end of this chapter. For now, we will stick to Casbah.
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的官方MongoDB驱动程序称为**Casbah**。Casbah不是完整的驱动程序，而是包装了Java Mongo驱动程序，提供了一个更函数式的接口。还有其他Scala的MongoDB驱动程序，我们将在本章末尾简要讨论。现在，我们将坚持使用Casbah。
- en: 'Let''s start by adding Casbah to our `build.sbt` file:'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向我们的`build.sbt`文件添加Casbah开始：
- en: '[PRE322]'
  id: totrans-1298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: 'Casbah also expects `slf4j` bindings (a Scala logging framework) to be available,
    so let''s also add `slf4j-nop`:'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: Casbah还期望`slf4j`绑定（一个Scala日志框架）可用，因此让我们也添加`slf4j-nop`：
- en: '[PRE323]'
  id: totrans-1300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: 'We can now start an SBT console and import Casbah in the Scala shell:'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动SBT控制台并在Scala shell中导入Casbah：
- en: '[PRE324]'
  id: totrans-1302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: 'This connects to a MongoDB server on the default host (`localhost`) and default
    port (`27017`). To connect to a different server, pass the host and port as arguments
    to `MongoClient`:'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 这将连接到默认主机（`localhost`）和默认端口（`27017`）上的MongoDB服务器。要连接到不同的服务器，将主机和端口作为参数传递给`MongoClient`：
- en: '[PRE325]'
  id: totrans-1304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE325]'
- en: 'Note that creating a client is a lazy operation: it does not attempt to connect
    to the server until it needs to. This means that if you enter the wrong URL or
    password, you will not know about it until you try and access documents on the
    server.'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建客户端是一个延迟操作：它不会在需要之前尝试连接到服务器。这意味着如果您输入了错误的URL或密码，您直到尝试访问服务器上的文档时才会知道。
- en: 'Once we have a connection to the server, accessing a database is as simple
    as using the client''s `apply` method. For instance, to access the `github` database:'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们与服务器建立了连接，访问数据库就像使用客户端的`apply`方法一样简单。例如，要访问`github`数据库：
- en: '[PRE326]'
  id: totrans-1307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE326]'
- en: 'We can then access the `"users"` collection:'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以访问`"users"`集合：
- en: '[PRE327]'
  id: totrans-1309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE327]'
- en: Connecting with authentication
  id: totrans-1310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用身份验证连接
- en: MongoDB supports several different authentication mechanisms. In this section,
    we will assume that your server is using the **SCRAM-SHA-1** mechanism, but you
    should find adapting the code to a different type of authentication straightforward.
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB支持多种不同的身份验证机制。在本节中，我们假设您的服务器正在使用**SCRAM-SHA-1**机制，但您应该会发现将代码适应不同类型的身份验证很简单。
- en: 'The easiest way of authenticating is to pass `username` and `password` in the
    URI when connecting:'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的身份验证方式是在连接时通过URI传递`username`和`password`：
- en: '[PRE328]'
  id: totrans-1313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE328]'
- en: 'In general, you will not want to put your password in plain text in the code.
    You can either prompt for a password on the command line or pass it through environment
    variables, as we did with the GitHub OAuth token in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*. The following code snippet demonstrates how
    to pass credentials through the environment:'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您不希望在代码中以明文形式放置密码。您可以在命令行中提示输入密码或通过环境变量传递，就像我们在[第7章](part0059.xhtml#aid-1O8H61
    "第7章。Web APIs")中处理GitHub OAuth令牌那样。以下代码片段演示了如何通过环境传递凭据：
- en: '[PRE329]'
  id: totrans-1315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE329]'
- en: 'You can run it through SBT as follows:'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式在SBT中运行它：
- en: '[PRE330]'
  id: totrans-1317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE330]'
- en: Inserting documents
  id: totrans-1318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插入文档
- en: 'Let''s insert some documents into our newly created database. We want to store
    information about GitHub users, using the following document structure:'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在新创建的数据库中插入一些文档。我们希望存储有关GitHub用户的信息，使用以下文档结构：
- en: '[PRE331]'
  id: totrans-1320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE331]'
- en: 'Casbah provides a `DBObject` class to represent MongoDB documents (and subdocuments)
    in Scala. Let''s start by creating a `DBObject` instance for each repository subdocument:'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: Casbah提供了一个`DBObject`类来表示Scala中的MongoDB文档（和子文档）。让我们首先为每个存储库子文档创建一个`DBObject`实例：
- en: '[PRE332]'
  id: totrans-1322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE332]'
- en: As you can see, a `DBObject` is just a list of key-value pairs, where the keys
    are strings. The values have compile-time type `AnyRef`, but Casbah will fail
    (at runtime) if you try to add a value that cannot be serialized.
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`DBObject` 只是一个键值对列表，其中键是字符串。值具有编译时类型 `AnyRef`，但如果您尝试添加无法序列化的值，Casbah将在运行时失败。
- en: 'We can also create `DBObject` instances from lists of key-value pairs directly.
    This is particularly useful when converting from a Scala map to a `DBObject`:'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以直接从键值对列表创建 `DBObject` 实例。这在将Scala映射转换为 `DBObject` 时特别有用：
- en: '[PRE333]'
  id: totrans-1325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE333]'
- en: 'The `DBObject` class provides many of the same methods as a map. For instance,
    we can address individual fields:'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: '`DBObject` 类提供了与映射相同的大多数方法。例如，我们可以访问单个字段：'
- en: '[PRE334]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE334]'
- en: 'We can construct a new object by adding a field to an existing object:'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向现有对象添加字段来构造一个新对象：
- en: '[PRE335]'
  id: totrans-1329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE335]'
- en: 'Note the return type: `mutable.Map[String,Any]`. Rather than implementing methods
    such as `+` directly, Casbah adds them to `DBObject` by providing an implicit
    conversion to and from `mutable.Map`.'
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意返回类型：`mutable.Map[String,Any]`。Casbah不是直接实现如 `+` 之类的方法，而是通过提供到和从 `mutable.Map`
    的隐式转换将它们添加到 `DBObject` 中。
- en: 'New `DBObject` instances can also be created by concatenating two existing
    instances:'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 `DBObject` 实例也可以通过连接两个现有实例来创建：
- en: '[PRE336]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: '`DBObject` instances can then be inserted into a collection using the `+=`
    operator. Let''s insert our first document into the `user` collection:'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: '`DBObject` 实例可以使用 `+=` 操作符插入到集合中。让我们将我们的第一个文档插入到 `user` 集合中：'
- en: '[PRE337]'
  id: totrans-1334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: A database containing a single document is a bit boring, so let's add a few
    more documents queried directly from the GitHub API. You learned how to query
    the GitHub API in the previous chapter, so we won't dwell on how to do this here.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 包含单个文档的数据库有点无聊，所以让我们添加一些直接从GitHub API查询的更多文档。您在上一章中学习了如何查询GitHub API，所以我们不会在这里详细说明如何进行此操作。
- en: 'In the code examples for this chapter, we have provided a class called `GitHubUserIterator`
    that queries the GitHub API (specifically the `/users` endpoint) for user documents,
    converts them to a case class, and offers them as an iterator. You will find the
    class in the code examples for this chapter (available on GitHub at [https://github.com/pbugnion/s4ds/tree/master/chap08](https://github.com/pbugnion/s4ds/tree/master/chap08))
    in the `GitHubUserIterator.scala` file. The easiest way to have access to the
    class is to open an SBT console in the directory of the code examples for this
    chapter. The API then fetches users in increasing order of their login ID:'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码示例中，我们提供了一个名为 `GitHubUserIterator` 的类，该类查询GitHub API（特别是 `/users` 端点）以获取用户文档，将它们转换为案例类，并将它们作为迭代器提供。您可以在本章的代码示例（可在GitHub上找到
    [https://github.com/pbugnion/s4ds/tree/master/chap08](https://github.com/pbugnion/s4ds/tree/master/chap08)）中的
    `GitHubUserIterator.scala` 文件中找到该类。访问该类最简单的方法是在本章代码示例的目录中打开一个SBT控制台。API随后按登录ID递增的顺序获取用户：
- en: '[PRE338]'
  id: totrans-1337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: '`GitHubUserIterator` returns instances of the `User` case class, defined as
    follows:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: '`GitHubUserIterator` 返回 `User` 案例类的实例，该类定义如下：'
- en: '[PRE339]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: Let's write a short program to fetch 500 users and insert them into the MongoDB
    database. We will need to authenticate with the GitHub API to retrieve these users.
    The constructor for `GitHubUserIterator` takes the GitHub OAuth token as an optional
    argument. We will inject the token through the environment, as we did in the previous
    chapter.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个简短的程序来获取500个用户并将它们插入到MongoDB数据库中。我们需要通过GitHub API进行身份验证来检索这些用户。`GitHubUserIterator`
    构造函数接受GitHub OAuth令牌作为可选参数。我们将通过环境注入令牌，就像我们在上一章中所做的那样。
- en: We first give the entire code listing before breaking it down—if you are typing
    this out, you will need to copy `GitHubUserIterator.scala` from the code examples
    for this chapter to the directory in which you are running this to access the
    `GitHubUserIterator` class. The class relies on `scalaj-http` and `json4s`, so
    either copy the `build.sbt` file from the code examples or specify those packages
    as dependencies in your `build.sbt` file.
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: 在分解代码之前，我们首先给出整个代码列表——如果您正在手动输入，您需要将 `GitHubUserIterator.scala` 从本章的代码示例复制到您运行此操作的目录中，以便访问
    `GitHubUserIterator` 类。该类依赖于 `scalaj-http` 和 `json4s`，因此您可以选择复制代码示例中的 `build.sbt`
    文件，或者在您的 `build.sbt` 文件中指定这些包作为依赖项。
- en: '[PRE340]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE340]'
- en: 'Before diving into the details of how this program works, let''s run it through
    SBT. You will want to query the API with authentication to avoid hitting the rate
    limit. Recall that we need to set the `GHTOKEN` environment variable:'
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解程序的工作原理之前，让我们通过SBT运行它。您将想要使用身份验证查询API以避免达到速率限制。回想一下，我们需要设置 `GHTOKEN` 环境变量：
- en: '[PRE341]'
  id: totrans-1344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE341]'
- en: 'The program will take about five minutes to run (depending on your Internet
    connection). To verify that the program works, we can query the number of documents
    in the `users` collection of the `github` database:'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行大约需要五分钟（取决于您的互联网连接）。为了验证程序是否工作，我们可以查询 `github` 数据库中 `users` 集合中的文档数量：
- en: '[PRE342]'
  id: totrans-1346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE342]'
- en: Let's break the code down. We first load the OAuth token to authenticate with
    the GithHub API. The token is stored as an environment variable, `GHTOKEN`. The
    `token` variable is a `lazy val`, so the token is loaded only when we formulate
    the first request to the API. We have already used this pattern in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*.
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下代码。我们首先加载 OAuth 令牌以验证 GitHub API。令牌存储为环境变量 `GHTOKEN`。`token` 变量是一个 `lazy
    val`，因此令牌只在形成对 API 的第一个请求时加载。我们已经在 [第 7 章](part0059.xhtml#aid-1O8H61 "第 7 章。Web
    APIs")，*Web APIs* 中使用了这种模式。
- en: 'We then define two methods to transform from classes in the domain model to
    `DBObject` instances:'
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义两个方法，将领域模型中的类转换成 `DBObject` 实例：
- en: '[PRE343]'
  id: totrans-1349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE343]'
- en: 'Armed with these two methods, we can add users to our MongoDB collection easily:'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这两个方法，我们可以轻松地将用户添加到我们的 MongoDB 集合中：
- en: '[PRE344]'
  id: totrans-1351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE344]'
- en: 'We used currying to split the arguments of `insertUsers`. This lets us use
    `insertUsers` as a function factory:'
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用柯里化来拆分 `insertUsers` 的参数。这使得我们可以将 `insertUsers` 作为函数工厂使用：
- en: '[PRE345]'
  id: totrans-1353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE345]'
- en: 'This creates a new method, `inserter`, with signature `Iterable[User] => Unit`
    that inserts users into `coll`. To see how this might come in useful, let''s write
    a function to wrap the whole data ingestion process. This is how a first attempt
    at this function could look:'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个新的方法 `inserter`，其签名是 `Iterable[User] => Unit`，用于将用户插入到 `coll` 中。为了了解这如何有用，让我们编写一个函数来包装整个数据导入过程。这是这个函数的第一个尝试可能看起来像这样：
- en: '[PRE346]'
  id: totrans-1355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE346]'
- en: 'Notice how `ingestUsers` takes a method that specifies how the list of users
    is inserted into the database as its second argument. This function encapsulates
    the entire code specific to insertion into a MongoDB collection. If we decide,
    at some later date, that we hate MongoDB and must insert the documents into a
    SQL database or write them to a flat file, all we need to do is pass a different
    `inserter` function to `ingestUsers`. The rest of the code remains the same. This
    demonstrates the increased flexibility afforded by using higher-order functions:
    we can easily build a framework and let the client code plug in the components
    that it needs.'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `ingestUsers` 方法如何将其第二个参数作为一个指定用户列表如何插入到数据库中的方法。这个函数封装了插入到 MongoDB 集合的整个特定代码。如果我们决定，在未来的某个时间点，我们讨厌
    MongoDB 并且必须将文档插入到 SQL 数据库或写入平面文件，我们只需要将不同的 `inserter` 函数传递给 `ingestUsers`。其余的代码保持不变。这展示了使用高阶函数带来的更高灵活性：我们可以轻松构建一个框架，并让客户端代码插入它需要的组件。
- en: 'The `ingestUsers` method, as defined previously, has one problem: if the `nusers`
    value is large, it will consume a lot of memory in constructing the entire list
    of users. A better solution would be to break it down into batches: we fetch a
    batch of users from the API, insert them into the database, and move on to the
    next batch. This allows us to control memory usage by changing the batch size.
    It is also more fault tolerant: if the program crashes, we can just restart from
    the last successfully inserted batch.'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述定义的 `ingestUsers` 方法有一个问题：如果 `nusers` 值很大，它将在构建整个用户列表时消耗大量内存。更好的解决方案是将它分解成批次：我们从
    API 获取一批用户，将它们插入到数据库中，然后继续处理下一批。这样我们可以通过改变批次大小来控制内存使用。它也更加容错：如果程序崩溃，我们只需从最后一个成功插入的批次重新启动。
- en: 'The `.grouped` method, available on all iterables, is useful for batching.
    It returns an iterator over fragments of the original iterable:'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: '`.grouped` 方法，适用于所有可迭代对象，用于批量处理。它返回一个遍历原始可迭代对象片段的迭代器：'
- en: '[PRE347]'
  id: totrans-1359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE347]'
- en: 'Let''s rewrite our `ingestUsers` method to use batches. We will also add a
    progress report after each batch in order to give the user some feedback:'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写我们的 `ingestUsers` 方法以使用批次。我们还会在每个批次后添加一个进度报告，以便给用户一些反馈：
- en: '[PRE348]'
  id: totrans-1361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE348]'
- en: 'Let''s look at the highlighted line more closely. We start from the user iterator,
    `it`. We then take the first `nusers`. This returns an `Iterator[User]` that,
    instead of happily churning through every user in the GitHub database, will terminate
    after `nusers`. We then group this iterator into batches of 100 users. The `.grouped`
    method returns `Iterator[Iterator[User]]`. We then zip each batch with its index
    so that we know which batch we are currently processing (we use this in the `print`
    statement). The `.zipWithIndex` method returns `Iterator[(Iterator[User], Int)]`.
    We unpack this tuple in the loop using a case statement that binds `users` to
    `Iterator[User]` and `batchNumber` to the index. Let''s run this through SBT:'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看高亮行。我们从用户迭代器 `it` 开始。然后我们取前 `nusers` 个用户。这返回一个 `Iterator[User]`，它不会在
    GitHub 数据库中的每个用户上愉快地运行，而是在 `nusers` 后终止。然后我们将这个迭代器分组为 100 个用户的批次。`.grouped` 方法返回
    `Iterator[Iterator[User]]`。然后我们将每个批次与其索引进行连接，这样我们就可以知道我们目前正在处理哪个批次（我们在 `print`
    语句中使用这个）。`.zipWithIndex` 方法返回 `Iterator[(Iterator[User], Int)]`。我们在循环中使用一个 case
    语句来解包这个元组，将 `users` 绑定到 `Iterator[User]`，将 `batchNumber` 绑定到索引。让我们通过 SBT 运行这个例子：
- en: '[PRE349]'
  id: totrans-1363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE349]'
- en: Extracting objects from the database
  id: totrans-1364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据库中提取对象
- en: 'We now have a database populated with a few users. Let''s query this database
    from the REPL:'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含一些用户的数据库。让我们从 REPL 中查询这个数据库：
- en: '[PRE350]'
  id: totrans-1366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE350]'
- en: 'The `findOne` method returns a single `DBObject` object wrapped in an option,
    unless the collection is empty, in which case it returns `None`. We must therefore
    use the `get` method to extract the object:'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: '`findOne` 方法返回一个包含在选项中的单个 `DBObject` 对象，除非集合为空，在这种情况下它返回 `None`。因此，我们必须使用 `get`
    方法来提取对象：'
- en: '[PRE351]'
  id: totrans-1368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE351]'
- en: 'As you learned earlier in this chapter, `DBObject` is a map-like object with
    keys of type `String` and values of type `AnyRef`:'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在本章早期所学的，`DBObject` 是一个类似于映射的对象，其键的类型为 `String`，值类型为 `AnyRef`：
- en: '[PRE352]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE352]'
- en: 'In general, we want to restore compile-time type information as early as possible
    when importing objects from the database: we do not want to pass `AnyRef`s around
    when we can be more specific. We can use the `getAs` method to extract a field
    and cast it to a specific type:'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在从数据库导入对象时，希望尽可能早地恢复编译时类型信息：我们不希望在可以更具体的情况下传递 `AnyRef`。我们可以使用 `getAs` 方法提取字段并将其转换为特定类型：
- en: '[PRE353]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE353]'
- en: 'If the field is missing in the document or if the value cannot be cast, `getAs`
    will return `None`:'
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文档中缺少字段或值无法转换，`getAs` 将返回 `None`：
- en: '[PRE354]'
  id: totrans-1374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE354]'
- en: The astute reader may note that the interface provided by `getAs[T]` is similar
    to the `read[T]` method that we defined on a JDBC result set in [Chapter 5](part0040.xhtml#aid-164MG1
    "Chapter 5. Scala and SQL through JDBC"), *Scala and SQL through JDBC*.
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会注意到，`getAs[T]` 提供的接口与我们定义在 [第 5 章](part0040.xhtml#aid-164MG1 "第 5 章。通过
    JDBC 的 Scala 和 SQL") 中的 JDBC 结果集上的 `read[T]` 方法类似，*通过 JDBC 的 Scala 和 SQL*。
- en: 'If `getAs` fails (for instance, because the field is missing), we can use the
    `orElse` partial function to recover:'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `getAs` 失败（例如，因为字段缺失），我们可以使用 `orElse` 部分函数来恢复：
- en: '[PRE355]'
  id: totrans-1377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE355]'
- en: 'The `getAsOrElse` method allows us to substitute a default value if the cast
    fails:'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: '`getAsOrElse` 方法允许我们在转换失败时替换默认值：'
- en: '[PRE356]'
  id: totrans-1379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: 'Note that we can also use `getAsOrElse` to throw an exception:'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还可以使用 `getAsOrElse` 抛出异常：
- en: '[PRE357]'
  id: totrans-1381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: 'Arrays embedded in documents can be cast to `List[T]` objects, where `T` is
    the type of elements in the array:'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中嵌入的数组可以转换为 `List[T]` 对象，其中 `T` 是数组中元素的类型：
- en: '[PRE358]'
  id: totrans-1383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: 'Retrieving a single document at a time is not very useful. To retrieve all
    the documents in a collection, use the `.find` method:'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: 一次检索一个文档并不很有用。要检索集合中的所有文档，请使用 `.find` 方法：
- en: '[PRE359]'
  id: totrans-1385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: 'This returns an iterator of `DBObject`s. To actually fetch the documents from
    the database, you need to materialize the iterator by transforming it into a collection,
    using, for instance, `.toList`:'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 `DBObject` 迭代器。要实际从数据库中获取文档，您需要通过将其转换为集合来具体化迭代器，例如使用 `.toList`：
- en: '[PRE360]'
  id: totrans-1387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE360]'
- en: 'Let''s bring all of this together. We will write a toy program that prints
    the average number of repositories per user in our collection. The code works
    by fetching every document in the collection, extracting the number of repositories
    from each document, and then averaging over these:'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些放在一起。我们将编写一个玩具程序，该程序打印我们集合中每个用户的平均存储库数量。代码通过获取集合中的每个文档，从每个文档中提取存储库数量，然后对这些数量进行平均来实现：
- en: '[PRE361]'
  id: totrans-1389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE361]'
- en: 'Let''s run this through SBT:'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 SBT 运行这个例子：
- en: '[PRE362]'
  id: totrans-1391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE362]'
- en: The code starts with the `extractNumber` function, which extracts the number
    of repositories from each `DBObject`. The return value is `None` if the document
    does not contain the `repos` field.
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从 `extractNumber` 函数开始，该函数从每个 `DBObject` 中提取存储库数量。如果文档不包含 `repos` 字段，则返回值是
    `None`。
- en: The main body of the code starts by creating an iterator over `DBObject`s in
    the collection. This iterator is then mapped through the `extractNumber` function,
    which transforms it into an iterator of `Option[Int]`. We then run `.collect`
    on this iterator to collect all the values that are not `None`, converting from
    `Option[Int]` to `Int` in the process. Only then do we materialize the iterator
    to a list using `.toList`. The resulting list, `wellFormattedNumbers`, has the
    `List[Int]` type. We then just take the mean of this list and print it to screen.
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的主体部分首先创建一个遍历集合中 `DBObject` 的迭代器。然后，这个迭代器通过 `extractNumber` 函数进行映射，将其转换为 `Option[Int]`
    的迭代器。然后我们对这个迭代器运行 `.collect`，收集所有不是 `None` 的值，在这个过程中将 `Option[Int]` 转换为 `Int`。然后我们才使用
    `.toList` 将迭代器实体化为列表。得到的列表 `wellFormattedNumbers` 具有类型 `List[Int]`。然后我们只取这个列表的平均值并将其打印到屏幕上。
- en: 'Note that, besides the `extractNumber` function, none of this program deals
    with Casbah-specific types: the iterator returned by `.find()` is just a Scala
    iterator. This makes Casbah straightforward to use: the only data type that you
    need to familiarize yourself with is `DBObject` (compare this with JDBC''s `ResultSet`,
    which we had to explicitly wrap in a stream, for instance).'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了 `extractNumber` 函数外，这个程序没有处理任何与 Casbah 特定的类型相关的事务：`.find()` 返回的迭代器只是一个
    Scala 迭代器。这使得 Casbah 的使用变得简单：你需要熟悉的唯一数据类型是 `DBObject`（与 JDBC 的 `ResultSet` 进行比较，我们必须显式地将其包装在流中，例如）。
- en: Complex queries
  id: totrans-1395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂查询
- en: We now know how to convert `DBObject` instances to custom Scala classes. In
    this section, you will learn how to construct queries that only return a subset
    of the documents in the collection.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道如何将 `DBObject` 实例转换为自定义 Scala 类。在本节中，你将学习如何构建只返回集合中部分文档的查询。
- en: 'In the previous section, you learned to retrieve all the documents in a collection
    as follows:'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了如何如下检索集合中的所有文档：
- en: '[PRE363]'
  id: totrans-1398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: The `collection.find()` method returns an iterator over all the documents in
    the collection. By calling `.toList` on this iterator, we materialize it to a
    list.
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: '`collection.find()` 方法返回一个遍历集合中所有文档的迭代器。通过在这个迭代器上调用 `.toList`，我们将其实体化为列表。'
- en: 'We can customize which documents are returned by passing a query document to
    the `.find` method. For instance, we can retrieve documents for a specific login
    name:'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递一个查询文档到 `.find` 方法来自定义返回哪些文档。例如，我们可以检索特定登录名的文档：
- en: '[PRE364]'
  id: totrans-1401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE364]'
- en: 'MongoDB queries are expressed as `DBObject` instances. Keys in the `DBObject`
    correspond to fields in the collection''s documents, and the values are expressions
    controlling the allowed values of this field. Thus, `DBObject("login" -> "mojombo")`
    will select all the documents for which the `login` field is `mojombo`. Using
    a `DBObject` instance to represent a query might seem a little obscure, but it
    will quickly make sense if you read the MongoDB documentation ([https://docs.mongodb.org/manual/core/crud-introduction/](https://docs.mongodb.org/manual/core/crud-introduction/)):
    queries are themselves just JSON objects in MongoDB. Thus, the fact that the query
    in Casbah is represented as a `DBObject` is consistent with other MongoDB client
    implementations. It also allows someone familiar with MongoDB to start writing
    Casbah queries in no time.'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 查询以 `DBObject` 实例的形式表达。`DBObject` 中的键对应于集合文档中的字段，而值是控制该字段允许值的表达式。因此，`DBObject("login"
    -> "mojombo")` 将选择所有 `login` 字段为 `mojombo` 的文档。使用 `DBObject` 实例表示查询可能看起来有些晦涩，但如果你阅读
    MongoDB 文档([https://docs.mongodb.org/manual/core/crud-introduction/](https://docs.mongodb.org/manual/core/crud-introduction/))，你会很快明白：查询在
    MongoDB 中本身就是 JSON 对象。因此，Casbah 中的查询表示为 `DBObject` 与其他 MongoDB 客户端实现保持一致。它还允许熟悉
    MongoDB 的人迅速开始编写 Casbah 查询。
- en: 'MongoDB supports more complex queries. For instance, to query everyone with
    `"github_id"` between `20` and `30`, we can write the following query:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 支持更复杂的查询。例如，要查询 `"github_id"` 在 `20` 到 `30` 之间的所有人，我们可以编写以下查询：
- en: '[PRE365]'
  id: totrans-1404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE365]'
- en: We limit the range of values that `github_id` can take with `DBObject("$gte"
    -> 20, "$lt" -> 30)`. The `"$gte"` string indicates that `github_id` must be greater
    or equal to `20`. Similarly, `"$lt"` denotes the *less than* operator. To get
    a full list of operators that you can use when querying, consult the MongoDB reference
    documentation ([http://docs.mongodb.org/manual/reference/operator/query/](http://docs.mongodb.org/manual/reference/operator/query/)).
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`DBObject("$gte" -> 20, "$lt" -> 30)`限制了`github_id`可以取的值的范围。`"$gte"`字符串表示`github_id`必须大于或等于`20`。同样，`"$lt"`表示小于操作符。要获取查询时可以使用的所有操作符的完整列表，请查阅MongoDB参考文档([http://docs.mongodb.org/manual/reference/operator/query/](http://docs.mongodb.org/manual/reference/operator/query/))。
- en: 'So far, we have only looked at queries on top-level fields. Casbah also lets
    us query fields in subdocuments and arrays using the *dot* notation. In the context
    of array values, this will return all the documents for which at least one value
    in the array matches the query. For instance, to retrieve all users who have a
    repository whose main language is Scala:'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了顶级字段的查询。Casbah还允许我们使用点符号查询子文档和数组中的字段。在数组值的上下文中，这将返回数组中至少有一个值与查询匹配的所有文档。例如，要检索所有在Scala中拥有主要语言为Scala的仓库的用户：
- en: '[PRE366]'
  id: totrans-1407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE366]'
- en: Casbah query DSL
  id: totrans-1408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Casbah查询DSL
- en: 'Using `DBObject` instances to express queries can be very verbose and somewhat
    difficult to read. Casbah provides a DSL to express queries much more succinctly.
    For instance, to get all the documents with the `github_id` field between `20`
    and `30`, we would write the following:'
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DBObject`实例来表示查询可能非常冗长且难以阅读。Casbah提供了一个DSL来更简洁地表示查询。例如，要获取所有`github_id`字段在`20`和`30`之间的文档，我们会编写以下代码：
- en: '[PRE367]'
  id: totrans-1410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE367]'
- en: The operators provided by the DSL will automatically construct `DBObject` instances.
    Using the DSL operators as much as possible generally leads to much more readable
    and maintainable code.
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: DSL提供的运算符将自动构造`DBObject`实例。尽可能多地使用DSL运算符通常会导致代码更易读、更易于维护。
- en: 'Going into the full details of the query DSL is beyond the scope of this chapter.
    You should find it quite easy to use. For a full list of the operators supported
    by the DSL, refer to the Casbah documentation at [http://mongodb.github.io/casbah/3.0/reference/query_dsl/](http://mongodb.github.io/casbah/3.0/reference/query_dsl/).
    We summarize the most important operators here:'
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 进入查询DSL的详细内容超出了本章的范围。您会发现使用它相当简单。要获取DSL支持的运算符的完整列表，请参阅Casbah文档[http://mongodb.github.io/casbah/3.0/reference/query_dsl/](http://mongodb.github.io/casbah/3.0/reference/query_dsl/)。我们在此总结了最重要的运算符：
- en: '| Operators | Description |'
  id: totrans-1413
  prefs: []
  type: TYPE_TB
  zh: '| 运算符 | 描述 |'
- en: '| --- | --- |'
  id: totrans-1414
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `"login" $eq "mojombo"` | This selects documents whose `login` field is exactly
    `mojombo` |'
  id: totrans-1415
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $eq "mojombo"` | 这将选择`login`字段正好是`mojombo`的文档 |'
- en: '| `"login" $ne "mojombo"` | This selects documents whose `login` field is not
    `mojombo` |'
  id: totrans-1416
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $ne "mojombo"` | 这将选择`login`字段不是`mojombo`的文档 |'
- en: '| `"github_id" $gt 1 $lt 20` | This selects documents with `github_id` greater
    than `1` and less than `20` |'
  id: totrans-1417
  prefs: []
  type: TYPE_TB
  zh: '| `"github_id" $gt 1 $lt 20` | 这将选择`github_id`大于`1`且小于`20`的文档 |'
- en: '| `"github_id" $gte 1 $lte 20` | This selects documents with `github_id` greater
    than or equal to `1` and less than or equal to `20` |'
  id: totrans-1418
  prefs: []
  type: TYPE_TB
  zh: '| `"github_id" $gte 1 $lte 20` | 这将选择`github_id`大于或等于`1`且小于或等于`20`的文档 |'
- en: '| `"login" $in ("mojombo", "defunkt")` | The `login` field is either `mojombo`
    or `defunkt` |'
  id: totrans-1419
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $in ("mojombo", "defunkt")` | `login`字段是`mojombo`或`defunkt` |'
- en: '| `"login" $nin ("mojombo", "defunkt")` | The `login` field is not `mojombo`
    or `defunkt` |'
  id: totrans-1420
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $nin ("mojombo", "defunkt")` | `login`字段不是`mojombo`或`defunkt` |'
- en: '| `"login" $regex "^moj.*"` | The `login` field matches the particular regular
    expression |'
  id: totrans-1421
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $regex "^moj.*"` | `login`字段匹配特定的正则表达式 |'
- en: '| `"login" $exists true` | The `login` field exists |'
  id: totrans-1422
  prefs: []
  type: TYPE_TB
  zh: '| `"login" $exists true` | `login`字段存在 |'
- en: '| `$or("login" $eq "mojombo", "github_id" $gte 22)` | Either the `login` field
    is `mojombo` or the `github_id` field is greater or equal to `22` |'
  id: totrans-1423
  prefs: []
  type: TYPE_TB
  zh: '| `$or("login" $eq "mojombo", "github_id" $gte 22)` | `login`字段是`mojombo`或`github_id`字段大于或等于`22`
    |'
- en: '| `$and("login" $eq "mojombo", "github_id" $gte 22)` | The `login` field is
    `mojombo` and the `github_id` field is greater or equal to `22` |'
  id: totrans-1424
  prefs: []
  type: TYPE_TB
  zh: '| `$and("login" $eq "mojombo", "github_id" $gte 22)` | `login`字段是`mojombo`且`github_id`字段大于或等于`22`
    |'
- en: 'We can also use the *dot* notation to query arrays and subdocuments. For instance,
    the following query will count all the users who have a repository in Scala:'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用点符号来查询数组和子文档。例如，以下查询将计算所有在Scala中拥有仓库的用户：
- en: '[PRE368]'
  id: totrans-1426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE368]'
- en: Custom type serialization
  id: totrans-1427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义类型序列化
- en: 'So far, we have only tried to serialize and deserialize simple types. What
    if we wanted to decode the language field in the repository array to an enumeration
    rather than a string? We might, for instance, define the following enumeration:'
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只尝试了序列化和反序列化简单类型。如果我们想将存储在仓库数组中的语言字段解码为枚举而不是字符串呢？例如，我们可以定义以下枚举：
- en: '[PRE369]'
  id: totrans-1429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE369]'
- en: 'Casbah lets us define custom serializers tied to a specific Scala type: we
    can inform Casbah that whenever it encounters an instance of the `Language.Value`
    type in a `DBObject`, the instance should be passed through a custom transformer
    that will convert it to, for instance, a string, before writing it to the database.'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: Casbah 允许我们定义与特定Scala类型相关的自定义序列化器：我们可以通知 Casbah，每当它在 `DBObject` 中遇到 `Language.Value`
    类型的实例时，该实例应通过一个自定义转换器进行转换，例如转换为字符串，然后再将其写入数据库。
- en: 'To define a custom serializer, we need to define a class that extends the `Transformer`
    trait. This trait exposes a single method, `transform(o:AnyRef):AnyRef`. Let''s
    define a `LanguageTransformer` trait that transforms from `Language.Value` to
    `String`:'
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个自定义序列化器，我们需要定义一个扩展 `Transformer` 特质的类。这个特质暴露了一个方法，`transform(o:AnyRef):AnyRef`。让我们定义一个
    `LanguageTransformer` 特质，它将 `Language.Value` 转换为 `String`：
- en: '[PRE370]'
  id: totrans-1432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE370]'
- en: 'We now need to register the trait to be used whenever an instance of type `Language.Value`
    needs to be decoded. We can do this using the `addEncodingHook` method:'
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要注册特质，以便在需要解码类型 `Language.Value` 的实例时使用。我们可以使用 `addEncodingHook` 方法来完成此操作：
- en: '[PRE371]'
  id: totrans-1434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE371]'
- en: 'We can now construct `DBObject` instances containing values of the `Language`
    enumeration:'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建包含 `Language` 枚举值的 `DBObject` 实例：
- en: '[PRE372]'
  id: totrans-1436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE372]'
- en: 'What about the reverse? How do we tell Casbah to read the `"language"` field
    as `Language.Value`? This is not possible with custom deserializers: `"Scala"`
    is now stored as a string in the database. Thus, when it comes to deserialization,
    `"Scala"` is no different from, say, `"mojombo"`. We thus lose type information
    when `"Scala"` is serialized.'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来呢？我们如何告诉 Casbah 将 `"language"` 字段读取为 `Language.Value`？这不可能通过自定义反序列化器实现：`"Scala"`
    现在作为字符串存储在数据库中。因此，在反序列化时，`"Scala"` 与 `"mojombo"` 没有区别。因此，当 `"Scala"` 被序列化时，我们失去了类型信息。
- en: 'Thus, while custom encoding hooks are useful for serialization, they are much
    less useful when deserializing. A cleaner, more consistent alternative to customize
    both serialization and deserialization is to use *type classes*. We have already
    covered how to use these extensively in [Chapter 5](part0040.xhtml#aid-164MG1
    "Chapter 5. Scala and SQL through JDBC"), *Scala and SQL through JDBC*, in the
    context of serializing to and from SQL. The procedure here would be very similar:'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然自定义编码钩子在序列化时很有用，但在反序列化时则不那么有用。一种更干净、更一致的替代方案是使用 *类型类* 来自定义序列化和反序列化。我们已经在
    [第5章](part0040.xhtml#aid-164MG1 "第5章。通过JDBC的Scala和SQL")，*通过JDBC的Scala和SQL* 的上下文中广泛介绍了如何使用这些类型类，用于将数据序列化和反序列化到SQL。这里的程序将非常相似：
- en: Define a `MongoReader[T]` type class with a `read(v:Any)`:`T` method.
  id: totrans-1439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个具有 `read(v:Any)`:`T` 方法的 `MongoReader[T]` 类型类。
- en: Define concrete implementations of `MongoReader` in the `MongoReader` companion
    object for all types of interest, such as `String`, `Language.Value`.
  id: totrans-1440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `MongoReader` 伴生对象中为所有感兴趣的类型定义具体的 `MongoReader` 实现，例如 `String`、`Language.Value`。
- en: Enrich `DBObject` with a `read[T:MongoReader]` method using the *pimp my library*
    pattern.
  id: totrans-1441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *pimp my library* 模式，为 `DBObject` 增强一个 `read[T:MongoReader]` 方法。
- en: 'For instance, the implementation of `MongoReader` for `Language.Value` would
    be as follows:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`Language.Value` 的 `MongoReader` 实现如下：
- en: '[PRE373]'
  id: totrans-1443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE373]'
- en: We could then do the same with a `MongoWriter` type class. Using type classes
    is an idiomatic and extensible approach to custom serialization and deserialization.
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用同样的方式使用 `MongoWriter` 类型类。使用类型类是自定义序列化和反序列化的惯用和可扩展的方法。
- en: We provide a complete example of type classes in the code examples associated
    with this chapter (in the `typeclass` directory).
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章相关的代码示例（在 `typeclass` 目录中）提供了一个类型类的完整示例。
- en: Beyond Casbah
  id: totrans-1446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 除此之外
- en: We have only considered Casbah in this chapter. There are, however, other drivers
    for MongoDB.
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们只考虑了 Casbah。然而，MongoDB 还有其他驱动程序。
- en: '*ReactiveMongo* is a driver that focusses on asynchronous read and writes to
    and from the database. All queries return a future, forcing asynchronous behavior.
    This fits in well with data streams or web applications.'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReactiveMongo* 是一个专注于数据库异步读写操作的驱动程序。所有查询都返回一个未来对象，强制执行异步行为。这非常适合数据流或Web应用程序。'
- en: '*Salat* sits at a higher level than Casbah and aims to provide easy serialization
    and deserialization of case classes.'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '*Salat*位于Casbah之上，旨在提供对case类进行简单序列化和反序列化的功能。'
- en: A full list of drivers is available at [https://docs.mongodb.org/ecosystem/drivers/scala/](https://docs.mongodb.org/ecosystem/drivers/scala/).
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的驱动程序列表可在[https://docs.mongodb.org/ecosystem/drivers/scala/](https://docs.mongodb.org/ecosystem/drivers/scala/)找到。
- en: Summary
  id: totrans-1451
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to interact with a MongoDB database. By weaving
    the constructs learned in the previous chapter—pulling information from a web
    API—with those learned in this chapter, we can now build a concurrent, reactive
    program for data ingestion.
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何与MongoDB数据库交互。通过将上一章学到的结构（从Web API中提取信息）与本章学到的结构结合起来，我们现在可以构建一个用于数据摄取的并发、响应式程序。
- en: In the next chapter, you will learn to build distributed, concurrent structures
    with greater flexibility using Akka actors.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用Akka演员构建更灵活的分布式、并发结构。
- en: References
  id: totrans-1454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*MongoDB: The Definitive Guide*, by *Kristina Chodorow*, is a good introduction
    to MongoDB. It does not cover interacting with MongoDB in Scala at all, but Casbah
    is intuitive enough for anyone familiar with MongoDB.'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: '《*MongoDB: The Definitive Guide*》，由*Kristina Chodorow*所著，是MongoDB的良好入门指南。它完全不涉及使用Scala与MongoDB交互，但对于熟悉MongoDB的人来说，Casbah足够直观。'
- en: Similarly, the MongoDB documentation ([https://docs.mongodb.org/manual/](https://docs.mongodb.org/manual/))
    provides an in-depth discussion of MongoDB.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，MongoDB文档([https://docs.mongodb.org/manual/](https://docs.mongodb.org/manual/))提供了对MongoDB的深入讨论。
- en: Casbah itself is well-documented ([http://mongodb.github.io/casbah/3.0/](http://mongodb.github.io/casbah/3.0/)).
    There is a *Getting Started* guide that is somewhat similar to this chapter and
    a complete reference guide that will fill in the gaps left by this chapter.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: Casbah本身有很好的文档([http://mongodb.github.io/casbah/3.0/](http://mongodb.github.io/casbah/3.0/))。有一个*入门指南*，与本章有些类似，还有一个完整的参考指南，将填补本章留下的空白。
- en: 'This gist, [https://gist.github.com/switzer/4218526](https://gist.github.com/switzer/4218526),
    implements type classes to serialize and deserialize objects in the domain model
    to `DBObject`s. The premise is a little different from the suggested usage of
    type classes in this chapter: we are converting from Scala types to `AnyRef` to
    be used as values in `DBObject`. However, the two approaches are complementary:
    one could imagine a set of type classes to convert from `User` or `Repo` to `DBObject`
    and another to convert from `Language.Value` to `AnyRef`.'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段，[https://gist.github.com/switzer/4218526](https://gist.github.com/switzer/4218526)，实现了类型类，用于将领域模型中的对象序列化和反序列化为`DBObject`。前提与本章中建议的类型类用法略有不同：我们将Scala类型转换为`AnyRef`，以便在`DBObject`中使用作为值。然而，这两种方法互为补充：可以想象有一组类型类将`User`或`Repo`转换为`DBObject`，另一组将`Language.Value`转换为`AnyRef`。
- en: Chapter 9. Concurrency with Akka
  id: totrans-1459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。使用Akka进行并发
- en: Much of this book focusses on taking advantage of multicore and distributed
    architectures. In [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections
    and Futures"), *Parallel Collections and Futures*, you learned how to use parallel
    collections to distribute batch processing problems over several threads and how
    to perform asynchronous computations using futures. In [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, we applied this knowledge to query the GitHub
    API with several concurrent threads.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都专注于利用多核和分布式架构。在[第4章](part0036.xhtml#aid-12AK82 "第4章。并行集合和未来")，*并行集合和未来*中，你学习了如何使用并行集合将批处理问题分布到多个线程上，以及如何使用未来执行异步计算。在[第7章](part0059.xhtml#aid-1O8H61
    "第7章。Web API")，*Web API*中，我们将这些知识应用于使用多个并发线程查询GitHub API。
- en: Concurrency abstractions such as futures and parallel collections simplify the
    enormous complexity of concurrent programming by limiting what you can do. Parallel
    collections, for instance, force you to phrase your parallelization problem as
    a sequence of pure functions on collections.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: 并发抽象，如未来和并行集合，通过限制你可以做的事情来简化并发编程的巨大复杂性。例如，并行集合强制你将并行化问题表述为集合上的纯函数序列。
- en: Actors offer a different way of thinking about concurrency. Actors are very
    good at encapsulating *state*. Managing state shared between different threads
    of execution is probably the most challenging part of developing concurrent applications,
    and, as we will discover in this chapter, actors make it manageable.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 演员提供了一种不同的并发思考方式。演员在封装 *状态* 方面非常出色。管理不同执行线程之间共享的状态可能是开发并发应用程序最具挑战性的部分，正如我们将在本章中发现的那样，演员使其变得可管理。
- en: GitHub follower graph
  id: totrans-1463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GitHub 关注者图
- en: In the previous two chapters, we explored the GitHub API, learning how to query
    the API and parse the results using *json-4s*.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们探讨了 GitHub API，学习了如何使用 *json-4s* 查询 API 并解析结果。
- en: 'Let''s imagine that we want to extract the GitHub follower graph: we want a
    program that will start from a particular user, extract this user followers, and
    then extract their followers until we tell it to stop. The catch is that we don''t
    know ahead of time what URLs we need to fetch: when we download the login names
    of a particular user''s followers, we need to verify whether we have fetched these
    users previously. If not, we add them to a queue of users whose followers we need
    to fetch. Algorithm aficionados might recognize this as *breadth-first search*.'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们想要提取 GitHub 关注者图：我们想要一个程序，它将从特定用户开始，提取该用户关注者，然后提取他们的关注者，直到我们告诉它停止。问题是，我们事先不知道需要获取哪些
    URL：当我们下载特定用户关注者的登录名时，我们需要验证我们是否已经获取了这些用户。如果没有，我们将它们添加到需要获取其关注者的用户队列中。算法爱好者可能会认出这作为
    *广度优先搜索*。
- en: 'Let''s outline how we might write this in a single-threaded way. The central
    components are a set of visited users and queue of future users to visit:'
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们概述一下我们如何以单线程方式编写它。核心组件是一组已访问用户和未来用户队列：
- en: '[PRE374]'
  id: totrans-1467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE374]'
- en: Here, the `fetchFollowersForUser` method has signature `String => Iterable[String]`
    and is responsible for taking a login name, transforming it into a URL in the
    GitHub API, querying the API, and extracting a list of followers from the response.
    We will not implement it here, but you can find a complete example in the `chap09/single_threaded`
    directory of the code examples for this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    You should have all the tools to implement this yourself if you have read [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`fetchFollowersForUser` 方法的签名是 `String => Iterable[String]`，它负责接受一个登录名，将其转换为
    GitHub API 中的 URL，查询 API，并从响应中提取关注者列表。我们在这里不会实现它，但您可以在本书代码示例的 `chap09/single_threaded`
    目录中找到一个完整的示例（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。如果您已经阅读了
    [第 7 章](part0059.xhtml#aid-1O8H61 "第 7 章。Web APIs"），您应该拥有所有实现它的工具。
- en: While this works, it will be painfully slow. The bottleneck is clearly the `fetchFollowersForUser`
    method, in particular, the part that queries the GitHub API. This program does
    not lend itself to the concurrency constructs that we have seen earlier in the
    book because we need to protect the state of the program, embodied by the user
    queue and set of fetched users, from race conditions. Note that it is not just
    a matter of making the queue and set thread-safe. We must also keep the two synchronized.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样可行，但速度会非常慢。瓶颈显然是 `fetchFollowersForUser` 方法，特别是查询 GitHub API 的部分。这个程序不适合我们在这本书前面看到的并发构造，因为我们需要保护程序的状态，即用户队列和已获取用户集合，以避免竞态条件。请注意，这不仅仅是使队列和集合线程安全的问题。我们还必须保持这两个同步。
- en: '*Actors* offer an elegant abstraction to encapsulate state. They are lightweight
    objects that each perform a single task (possibly repeatedly) and communicate
    with each other by passing messages. The internal state of an actor can only be
    changed from within the actor itself. Importantly, actors only process messages
    one at a time, effectively preventing race conditions.'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: '*演员* 提供了一种优雅的抽象，用于封装状态。它们是轻量级对象，每个执行单个任务（可能反复执行）并通过传递消息相互通信。演员的内部状态只能从演员本身进行更改。重要的是，演员一次只处理一条消息，有效地防止了竞态条件。'
- en: 'By hiding program state inside actors, we can reason about the program more
    effectively: if a bug is introduced that makes this state inconsistent, the culprit
    will be localized entirely in that actor.'
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将程序状态隐藏在演员中，我们可以更有效地对程序进行推理：如果引入了一个导致此状态不一致的错误，那么罪魁祸首将完全局限于该演员。
- en: Actors as people
  id: totrans-1472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员作为人
- en: In the previous section, you learned that an actor encapsulates state, interacting
    with the outside world through messages. Actors make concurrent programming more
    intuitive because they behave a little bit like an ideal workforce.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你了解到演员封装了状态，通过消息与外界交互。演员使并发编程更加直观，因为它们的行为有点像理想的劳动力。
- en: 'Let''s think of an actor system representing a start-up with five people. There''s
    Chris, the CEO, and Mark, who''s in charge of marketing. Then there''s Sally,
    who heads the engineering team. Sally has two minions, Bob and Kevin. As every
    good organization needs an organizational chart, refer to the following diagram:'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个由五个人组成的初创公司表示的演员系统。有Chris，CEO，以及负责市场营销的Mark。然后是Sally，她领导着工程团队。Sally有两个助手，Bob和Kevin。正如每个好的组织都需要一个组织结构图一样，请参考以下图表：
- en: '![Actors as people](img/image01194.jpeg)'
  id: totrans-1475
  prefs: []
  type: TYPE_IMG
  zh: '![演员如人](img/image01194.jpeg)'
- en: Let's say that Chris receives an order. He will look at the order, decide whether
    it is something that he can process himself, and if not, he will forward it to
    Mark or Sally. Let's assume that the order asks for a small program so Bob forwards
    the order to Sally. Sally is very busy working on a backlog of orders so she cannot
    process the order message straightaway, and it will just sit in her mailbox for
    a short while. When she finally gets round to processing the order, she might
    decide to split the order into several parts, some of which she will give to Kevin
    and some to Bob.
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 假设Chris接到了一个订单。他会查看订单，决定是否可以自己处理，如果不能，他会将订单转发给Mark或Sally。让我们假设订单要求一个小程序，所以Bob将订单转发给了Sally。Sally非常忙，正在处理一批积压的订单，因此她不能立即处理订单信息，它将在她的邮箱中短暂停留。当她最终开始处理订单时，她可能会决定将订单分成几个部分，其中一些部分她会分配给Kevin和Bob。
- en: As Bob and Kevin complete items, they will send messages back to Sally to inform
    her. When every part of the order is fulfilled, Sally will aggregate the parts
    together and message either the customer directly or Chris with the results.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 当Bob和Kevin完成任务时，他们会向Sally发送消息以通知她。当订单的每个部分都得到满足时，Sally会将这些部分汇总起来，直接向客户或向Chris发送结果的消息。
- en: 'The task of keeping track of which jobs must be fulfilled to complete the order
    rests with Sally. When she receives messages from Bob and Kevin, she must update
    her list of tasks in progress and check whether every task related to this order
    is complete. This sort of coordination would be more challenging with traditional
    *synchronize* blocks: every access to the list of tasks in progress and to the
    list of completed tasks would need to be synchronized. By embedding this logic
    in Sally, who can only process a single message at a time, we can be sure that
    there will not be race conditions.'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪哪些工作必须完成以完成订单的任务落在Sally身上。当她收到Bob和Kevin的消息时，她必须更新她正在进行的任务列表，并检查与这个订单相关的每个任务是否完成。这种协调在传统的*synchronize*块中会更具有挑战性：对正在进行的任务列表和已完成任务列表的每次访问都需要同步。通过将这种逻辑嵌入只能一次处理一个消息的Sally中，我们可以确保不会出现竞态条件。
- en: 'Our start-up works well because each person is responsible for doing a single
    thing: Chris either delegates to Mark or Sally, Sally breaks up orders into several
    parts and assigns them to Bob and Kevin, and Bob and Kevin fulfill each part.
    You might think "hold on, all the logic is embedded in Bob and Kevin, the employees
    at the bottom of the ladder who do all the actual work". Actors, unlike employees,
    are cheap, so if the logic embedded in an actor gets too complicated, it is easy
    to introduce additional layers of delegation until tasks get simple enough.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初创公司运作良好，因为每个人只负责做一件事：Chris要么委托给Mark或Sally，Sally将订单拆分成几个部分并分配给Bob和Kevin，而Bob和Kevin完成每个部分。你可能会想，“等等，所有的逻辑都嵌入在Bob和Kevin中，他们是底层的员工，做所有实际的工作”。与员工不同，演员成本低，所以如果嵌入在演员中的逻辑变得过于复杂，很容易引入额外的委托层，直到任务足够简单。
- en: The employees in our start-up refuse to multitask. When they get a piece of
    work, they process it completely and then move on to the next task. This means
    that they cannot get muddled by the complexities of multitasking. Actors, by processing
    a single message at a time, greatly reduce the scope for introducing concurrency
    errors such as race conditions.
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初创公司的员工拒绝进行多任务处理。当他们得到一份工作时，他们会完全处理它，然后转到下一个任务。这意味着他们不会因为多任务处理的复杂性而变得混乱。通过一次处理一个消息，演员大大减少了引入并发错误（如竞态条件）的范围。
- en: More importantly, by offering an abstraction that programmers can intuitively
    understand—that of human workers—Akka makes reasoning about concurrency easier.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，通过提供一个程序员可以直观理解的抽象——即人类工作者——Akka 使得关于并发的推理变得更加容易。
- en: Hello world with Akka
  id: totrans-1482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Akka 的 Hello World
- en: 'Let''s install Akka. We add it as a dependency to our `build.sbt` file:'
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装 Akka。我们将它添加到我们的 `build.sbt` 文件中：
- en: '[PRE375]'
  id: totrans-1484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE375]'
- en: 'We can now import Akka as follows:'
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按照以下方式导入 Akka：
- en: '[PRE376]'
  id: totrans-1486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE376]'
- en: 'For our first foray into the world of actors, we will build an actor that echoes
    every message it receives. The code examples for this section are in a directory
    called `chap09/hello_akka` in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)):'
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们第一次进入演员的世界，我们将构建一个接收并回显每个接收到的消息的演员。本节中的代码示例位于本书提供的示例代码目录 `chap09/hello_akka`
    中（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）：
- en: '[PRE377]'
  id: totrans-1488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE377]'
- en: Let's pick this example apart, starting with the constructor. Our actor class
    must extend `Actor`. We also add `ActorLogging`, a utility trait that adds the
    `log` attribute.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这个例子，从构造函数开始。我们的演员类必须扩展 `Actor`。我们还添加了 `ActorLogging`，这是一个实用特性，它添加了 `log`
    属性。
- en: The `Echo` actor exposes a single method, `receive`. This is the actor's only
    way of communicating with the external world. To be useful, all actors must expose
    a `receive` method. The `receive` method is a partial function, typically implemented
    with multiple `case` statements. When an actor starts processing a message, it
    will match it against every `case` statement until it finds one that matches.
    It will then execute the corresponding block.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: '`Echo` 演员公开一个单一的方法，`receive`。这是演员与外部世界通信的唯一方式。为了有用，所有演员都必须公开一个 `receive` 方法。`receive`
    方法是一个部分函数，通常使用多个 `case` 语句实现。当演员开始处理消息时，它将匹配每个 `case` 语句，直到找到匹配的一个。然后执行相应的代码块。'
- en: Our echo actor accepts a single type of message, a plain string. When this message
    gets processed, the actor waits for half a second and then echoes the message
    to the log file.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 echo 演员接受一种类型的消息，一个普通的字符串。当这个消息被处理时，演员会等待半秒钟，然后将消息回显到日志文件中。
- en: 'Let''s instantiate a couple of Echo actors and send them messages:'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化几个 Echo 演员，并发送它们消息：
- en: '[PRE378]'
  id: totrans-1493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE378]'
- en: 'Running this gives us the following output:'
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令会得到以下输出：
- en: '[PRE379]'
  id: totrans-1495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE379]'
- en: 'Note that the `echo1` and `echo2` actors are clearly acting concurrently: `hello
    echo1` and `hello echo2` are logged at the same time. The second message, passed
    to `echo1`, gets processed after the actor has finished processing `hello echo1`.'
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`echo1` 和 `echo2` 演员显然是并发执行的：`hello echo1` 和 `hello echo2` 同时被记录。传递给 `echo1`
    的第二个消息在演员完成处理 `hello echo1` 后才被处理。
- en: 'There are a few different things to note:'
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：
- en: To start instantiating actors, we must first create an actor system. There is
    typically a single actor system per application.
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要开始实例化演员，我们首先必须创建一个演员系统。通常每个应用程序只有一个演员系统。
- en: 'The way in which we instantiate actors looks a little strange. Instead of calling
    the constructor, we create an actor properties object, `Props[T]`. We then ask
    the actor system to create an actor with these properties. In fact, we never instantiate
    actors with `new`: they are either created by calling the `actorOf` method in
    the actor system or a similar method from within another actor (more on this later).'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实例化演员的方式看起来有点奇怪。我们不是调用构造函数，而是创建一个演员属性对象，`Props[T]`。然后我们要求演员系统使用这些属性创建一个演员。实际上，我们从不使用
    `new` 实例化演员：它们要么是通过调用演员系统中的 `actorOf` 方法或另一个演员内的类似方法（稍后详细介绍）创建的。
- en: We never call an actor's methods from outside that actor. The only way to interact
    with the actor is to send messages to it. We do this using the *tell* operator,
    `!`. There is thus no way to mess with an actor's internals from outside that
    actor (or at least, Akka makes it difficult to mess with an actor's internals).
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从不从外部调用演员的方法。与演员交互的唯一方式是向其发送消息。我们使用 *tell* 操作符，`!` 来这样做。因此，从外部无法干扰演员的内部结构（或者至少，Akka
    使得干扰演员的内部结构变得困难）。
- en: Case classes as messages
  id: totrans-1501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为消息的案例类
- en: 'In our "hello world" example, we constructed an actor that is expected to receive
    a string as message. Any object can be passed as a message, provided it is immutable.
    It is very common to use case classes to represent messages. This is better than
    using strings because of the additional type safety: the compiler will catch a
    typo in a case class but not in a string.'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 "hello world" 示例中，我们构建了一个预期接收字符串消息的演员。任何不可变的对象都可以作为消息传递。使用案例类来表示消息非常常见。这比使用字符串更好，因为增加了额外的类型安全性：编译器会在案例类中捕获错误，而不会在字符串中。
- en: 'Let''s rewrite our `EchoActor` to accept instances of case classes as messages.
    We will make it accept two different messages: `EchoMessage(message)` and `EchoHello`,
    which just echoes a default message. The examples for this section and the next
    are in the `chap09/hello_akka_case_classes` directory in the sample code provided
    with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写我们的 `EchoActor` 以接受案例类的实例作为消息。我们将使其接受两种不同的消息：`EchoMessage(message)` 和 `EchoHello`，后者只是回显默认消息。本节和下一节的示例位于本书提供的示例代码中的
    `chap09/hello_akka_case_classes` 目录（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。
- en: 'A common Akka pattern is to define the messages that an actor can receive in
    the actor''s companion object:'
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 定义演员可以接收的消息是 Akka 的一个常见模式：
- en: '[PRE380]'
  id: totrans-1505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE380]'
- en: 'Let''s change the actor definition to accept these messages:'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更改演员定义以接受这些消息：
- en: '[PRE381]'
  id: totrans-1507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE381]'
- en: 'We can now send `EchoHello` and `EchoMessage` to our actors:'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以向我们的演员发送 `EchoHello` 和 `EchoMessage`：
- en: '[PRE382]'
  id: totrans-1509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE382]'
- en: Actor construction
  id: totrans-1510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员构建
- en: Actor construction is a common source of difficulty for people new to Akka.
    Unlike (most) ordinary objects, you never instantiate actors explicitly. You would
    never write, for instance, `val echo = new EchoActor`. In fact, if you try this,
    Akka raises an exception.
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: 演员构建是 Akka 新手常见的难题来源。与（大多数）普通对象不同，你永远不会显式实例化演员。例如，你永远不会写 `val echo = new EchoActor`。实际上，如果你这样做，Akka
    会抛出异常。
- en: 'Creating actors in Akka is a two-step process: you first create a `Props` object,
    which encapsulates the properties needed to construct an actor. The way to construct
    a `Props` object differs depending on whether the actor takes constructor arguments.
    If the constructor takes no arguments, we simply pass the actor class as a type
    parameter to `Props`:'
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Akka 中创建演员是一个两步过程：首先创建一个 `Props` 对象，它封装了构建演员所需的属性。构建 `Props` 对象的方式取决于演员是否接受构造函数参数。如果构造函数不接受参数，我们只需将演员类作为类型参数传递给
    `Props`：
- en: '[PRE383]'
  id: totrans-1513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE383]'
- en: 'If we have an actor whose constructor does take arguments, we must pass these
    as additional arguments when defining the `Props` object. Let''s consider the
    following actor, for instance:'
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: 如果演员的构造函数接受参数，我们必须在定义 `Props` 对象时将这些参数作为额外的参数传递。让我们考虑以下演员，例如：
- en: '[PRE384]'
  id: totrans-1515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE384]'
- en: 'We pass the constructor arguments to the `Props` object as follows:'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下将构造函数参数传递给 `Props` 对象：
- en: '[PRE385]'
  id: totrans-1517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE385]'
- en: 'The `Props` instance just embodies the configuration for creating an actor.
    It does not actually create anything. To create an actor, we pass the `Props`
    instance to the `system.actorOf` method, defined on the `ActorSystem` instance:'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: '`Props` 实例只是封装了创建演员的配置。它实际上并没有创建任何东西。要创建演员，我们将 `Props` 实例传递给定义在 `ActorSystem`
    实例上的 `system.actorOf` 方法：'
- en: '[PRE386]'
  id: totrans-1519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE386]'
- en: 'The `name` parameter is optional but is useful for logging and error messages.
    The value returned by `.actorOf` is not the actor itself: it is a *reference*
    to the actor (it helps to think of it as an address that the actor lives at) and
    has the `ActorRef` type. `ActorRef` is immutable, but it can be serialized and
    duplicated without affecting the underlying actor.'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: '`name` 参数是可选的，但用于日志和错误消息很有用。`.actorOf` 返回的值不是演员本身：它是对演员的 *引用*（可以将其视为演员居住的地址），具有
    `ActorRef` 类型。`ActorRef` 是不可变的，但它可以被序列化和复制，而不会影响底层演员。'
- en: 'There is another way to create actors besides calling `actorOf` on the actor
    system: each actor exposes a `context.actorOf` method that takes a `Props` instance
    as its argument. The context is only accessible from within the actor:'
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在演员系统中调用 `actorOf` 之外，还有另一种创建演员的方法：每个演员都公开了一个 `context.actorOf` 方法，该方法接受一个
    `Props` 实例作为参数。上下文仅可以从演员内部访问：
- en: '[PRE387]'
  id: totrans-1522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE387]'
- en: 'The difference between an actor created from the actor system and an actor
    created from another actor''s context lies in the actor hierarchy: each actor
    has a parent. Any actor created within another actor''s context will have that
    actor as its parent. An actor created by the actor system has a predefined actor,
    called the *user guardian*, as its parent. We will understand the importance of
    the actor hierarchy when we study the actor lifecycle at the end of this chapter.'
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 从演员系统创建的演员与从另一个演员的上下文创建的演员之间的区别在于演员层次结构：每个演员都有一个父演员。在另一个演员的上下文中创建的任何演员都将具有该演员作为其父演员。由演员系统创建的演员有一个预定义的演员，称为
    *用户守护者*，作为其父演员。当我们在本章末尾研究演员生命周期时，我们将了解演员层次结构的重要性。
- en: 'A very common idiom is to define a `props` method in an actor''s companion
    object that acts as a factory method for `Props` instances for that actor. Let''s
    amend the `EchoActor` companion object:'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的习惯用法是在演员的伴生对象中定义一个 `props` 方法，它作为该演员 `Props` 实例的工厂方法。让我们修改 `EchoActor`
    伴生对象：
- en: '[PRE388]'
  id: totrans-1525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE388]'
- en: 'We can then instantiate the actor as follows:'
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按照以下方式实例化演员：
- en: '[PRE389]'
  id: totrans-1527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE389]'
- en: Anatomy of an actor
  id: totrans-1528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员的解剖结构
- en: 'Before diving into a full-blown application, let''s look at the different components
    of the actor framework and how they fit together:'
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入一个完整的应用程序之前，让我们看看演员框架的不同组件以及它们是如何协同工作的：
- en: '**Mailbox**: A mailbox is basically a queue. Each actor has its own mailbox.
    When you send a message to an actor, the message lands in its mailbox and does
    nothing until the actor takes it off the queue and passes it through its `receive`
    method.'
  id: totrans-1530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**邮箱**: 邮箱基本上是一个队列。每个演员都有自己的邮箱。当你向一个演员发送消息时，消息会落在它的邮箱中，直到演员从队列中取出并通过其 `receive`
    方法处理它。'
- en: '**Messages**: Messages make synchronization between actors possible. A message
    can have any type with the sole requirement that it should be immutable. In general,
    it is better to use case classes or case objects to gain the compiler''s help
    in checking message types.'
  id: totrans-1531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息**: 消息使得演员之间的同步成为可能。消息可以具有任何类型，唯一的要求是它应该是不可变的。通常，最好使用案例类或案例对象来获得编译器的帮助，以检查消息类型。'
- en: '**Actor reference**: When we create an actor using `val echo1 = system.actorOf(Props[EchoActor])`,
    `echo1` has type `ActorRef`. An `ActorRef` is a proxy for an actor and is what
    the rest of the world interacts with: when you send a message, you send it to
    the `ActorRef`, not to the actor directly. In fact, you can never obtain a handle
    to an actor directly in Akka. An actor can obtain an `ActorRef` for itself using
    the `.self` method.'
  id: totrans-1532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员引用**: 当我们使用 `val echo1 = system.actorOf(Props[EchoActor])` 创建一个演员时，`echo1`
    具有类型 `ActorRef`。`ActorRef` 是一个代理，用于表示演员，并且是其他世界与之交互的方式：当你发送一个消息时，你是将它发送到 `ActorRef`，而不是直接发送给演员。实际上，在
    Akka 中，你永远无法直接获取到演员的句柄。演员可以使用 `.self` 方法为自己获取一个 `ActorRef`。'
- en: '**Actor context**: Each actor has a `context` attribute through which you can
    access methods to create or access other actors and find information about the
    outside world. We have already seen how to create new actors with `context.actorOf(props)`.
    We can also obtain a reference to an actor''s parent through `context.parent`.
    An actor can also stop another actor with `context.stop(actorRef)`, where `actorRef`
    is a reference to the actor that we want to stop.'
  id: totrans-1533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员上下文**: 每个演员都有一个 `context` 属性，通过它可以访问创建或访问其他演员的方法，以及获取有关外部世界的信息。我们已经看到了如何使用
    `context.actorOf(props)` 创建新的演员。我们还可以通过 `context.parent` 获取演员的父引用。演员还可以使用 `context.stop(actorRef)`
    停止另一个演员，其中 `actorRef` 是我们想要停止的演员的引用。'
- en: '**Dispatcher**: The dispatcher is the machine that actually executes the code
    in an actor. The default dispatcher uses a fork/join thread pool. Akka lets us
    use different dispatchers for different actors. Tweaking the dispatcher can be
    useful to optimize the performance and give priority to certain actors. The dispatcher
    that an actor runs on is accessible through `context.dispatcher`. Dispatchers
    implement the `ExecutionContext` interface so they can be used to run futures.'
  id: totrans-1534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**: 调度器是实际执行演员中代码的机器。默认调度器使用 fork/join 线程池。Akka 允许我们为不同的演员使用不同的调度器。调整调度器可能有助于优化性能并给某些演员赋予优先权。演员运行的调度器可以通过
    `context.dispatcher` 访问。调度器实现了 `ExecutionContext` 接口，因此它们可以用来运行未来。'
- en: Follower network crawler
  id: totrans-1535
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 追随者网络爬虫
- en: The end game for this chapter is to build a crawler to explore GitHub's follower
    graph. We have already outlined how we can do this in a single-threaded manner
    earlier in this chapter. Let's design an actor system to do this concurrently.
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最终目标是构建一个爬虫来探索GitHub的粉丝图谱。我们已经在本章前面概述了如何以单线程方式完成这项工作。现在让我们设计一个演员系统来并发地完成这项任务。
- en: The moving parts in the code are the data structures managing which users have
    been fetched or are being fetched. These need to be encapsulated in an actor to
    avoid race conditions arising from multiple actors trying to change them concurrently.
    We will therefore create a *fetcher manager* actor whose job is to keep track
    of which users have been fetched and which users we are going to fetch next.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的动态部分是管理哪些用户已被获取或正在被获取的数据结构。这些需要封装在一个演员中，以避免多个演员尝试同时更改它们时产生的竞争条件。因此，我们将创建一个*fetcher管理器*演员，其任务是跟踪哪些用户已被获取，以及我们接下来要获取哪些用户。
- en: The part of the code that is likely to be a bottleneck is querying the GitHub
    API. We therefore want to be able to scale the number of workers doing this concurrently.
    We will create a pool of *fetchers*, actors responsible for querying the API for
    the followers of a particular user. Finally, we will create an actor whose responsibility
    is to interpret the API's response. This actor will forward its interpretation
    of the response to another actor who will extract the followers and give them
    to the fetcher manager.
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中可能成为瓶颈的部分是查询GitHub API。因此，我们希望能够扩展同时执行此操作的工作者数量。我们将创建一个*fetcher*池，这些演员负责查询特定用户的API以获取粉丝。最后，我们将创建一个演员，其责任是解释API的响应。这个演员将把其对响应的解释转发给另一个演员，该演员将提取粉丝并将他们交给fetcher管理器。
- en: 'This is what the architecture of the program will look like:'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是程序架构的样貌：
- en: '![Follower network crawler](img/image01195.jpeg)'
  id: totrans-1540
  prefs: []
  type: TYPE_IMG
  zh: '![粉丝网络爬虫](img/image01195.jpeg)'
- en: Actor system for our GitHub API crawler
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub API爬虫的演员系统
- en: 'Each actor in our program performs a single task: fetchers just query the GitHub
    API and the queue manager just distributes work to the fetchers. Akka best practice
    dictates giving actors as narrow an area of responsibility as possible. This enables
    better granularity when scaling out (for instance, by adding more fetcher actors,
    we just parallelize the bottleneck) and better resilience: if an actor fails,
    it will only affect his area of responsibility. We will explore actor failure
    later on in this chapter.'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序中的每个演员都执行单一的任务：fetcher只查询GitHub API，而队列管理器只将工作分配给fetcher。Akka的最佳实践是尽可能给演员分配狭窄的责任范围。这有助于在扩展时获得更好的粒度（例如，通过添加更多的fetcher演员，我们只是并行化瓶颈）和更好的弹性：如果一个演员失败，它只会影响其责任范围。我们将在本章后面探讨演员的失败。
- en: 'We will build the app in several steps, exploring the Akka toolkit as we write
    the program. Let''s start with the `build.sbt` file. Besides Akka, we will mark
    `scalaj-http` and `json4s` as dependencies:'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分几个步骤构建应用程序，在编写程序的同时探索Akka工具包。让我们从`build.sbt`文件开始。除了Akka，我们还将`scalaj-http`和`json4s`标记为依赖项：
- en: '[PRE390]'
  id: totrans-1544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE390]'
- en: Fetcher actors
  id: totrans-1545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fetcher演员
- en: The workhorse of our application is the fetcher, the actor responsible for fetching
    the follower details from GitHub. In the first instance, our actor will accept
    a single message, `Fetch(user)`. It will fetch the followers corresponding to
    `user` and log the response to screen. We will use the recipes developed in [Chapter
    7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*, to query the
    GitHub API with an OAuth token. We will inject the token through the actor constructor.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用程序的核心是fetcher演员，它负责从GitHub获取粉丝详情。最初，我们的演员将接受一个单一的消息，`Fetch(user)`。它将获取与`user`对应的粉丝，并将响应记录到屏幕上。我们将使用在第7章中开发的配方，即*Web
    APIs*，使用OAuth令牌查询GitHub API。我们将通过演员构造函数注入令牌。
- en: 'Let''s start with the companion object. This will contain the definition of
    the `Fetch(user)` message and two factory methods to create the `Props` instances.
    You can find the code examples for this section in the `chap09/fetchers_alone`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)):'
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从伴随对象开始。这将包含`Fetch(user)`消息的定义和两个工厂方法来创建`Props`实例。您可以在本书提供的示例代码中的`chap09/fetchers_alone`目录中找到本节的代码示例（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）：
- en: '[PRE391]'
  id: totrans-1548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE391]'
- en: 'Let''s now define the fetcher itself. We will wrap the call to the GitHub API
    in a future. This avoids a single slow request blocking the actor. When our actor
    receives a `Fetch` request, it wraps this request into a future, sends it off,
    and can then process the next message. Let''s go ahead and implement our actor:'
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来定义 fetcher 本身。我们将把对 GitHub API 的调用封装在一个 future 中。这避免了单个缓慢的请求阻塞演员。当我们的演员收到一个
    `Fetch` 请求时，它会将这个请求封装在一个 future 中，发送出去，然后可以处理下一个消息。让我们继续实现我们的演员：
- en: '[PRE392]'
  id: totrans-1550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE392]'
- en: 'Let''s instantiate an actor system and four fetchers to check whether our actor
    is working as expected. We will read the GitHub token from the environment, as
    described in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web
    APIs*, then create four actors and ask each one to fetch the followers of a particular
    GitHub user. We wait five seconds for the requests to get completed, and then
    shut the system down:'
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化一个演员系统并创建四个 fetcher 来检查我们的演员是否按预期工作。我们将从环境变量中读取 GitHub 令牌，如第 7 章[Web APIs](part0059.xhtml#aid-1O8H61
    "第 7 章。Web APIs")中所述，然后创建四个演员并要求每个演员获取特定 GitHub 用户的关注者。我们等待五秒钟以完成请求，然后关闭系统：
- en: '[PRE393]'
  id: totrans-1552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE393]'
- en: 'Let''s run the code through SBT:'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 SBT 运行代码：
- en: '[PRE394]'
  id: totrans-1554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE394]'
- en: Notice how we explicitly need to shut the actor system down using `system.shutdown`.
    The program hangs until the system is shut down. However, shutting down the system
    will stop all the actors, so we need to make sure that they have finished working.
    We do this by inserting a call to `Thread.sleep`.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何明确地需要使用 `system.shutdown` 来关闭演员系统。程序会挂起，直到系统关闭。然而，关闭系统将停止所有演员，因此我们需要确保他们已经完成工作。我们通过插入对
    `Thread.sleep` 的调用来实现这一点。
- en: Using `Thread.sleep` to wait until the API calls have finished to shut down
    the actor system is a little crude. A better approach could be to let the actors
    signal back to the system that they have completed their task. We will see examples
    of this pattern later when we implement the *fetcher manager* actor.
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Thread.sleep` 等待直到 API 调用完成以关闭演员系统是一种比较粗糙的方法。更好的方法可能是让演员向系统发出信号，表明他们已经完成了任务。当我们实现
    *fetcher manager* 演员时，我们将看到这种模式的示例。
- en: 'Akka includes a feature-rich *scheduler* to schedule events. We can use the
    scheduler to replace the call to `Thread.sleep` by scheduling a system shutdown
    five seconds in the future. This is preferable as the scheduler does not block
    the calling thread, unlike `Thread.sleep`. To use the scheduler, we need to import
    a global execution context and the duration module:'
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: Akka 包含一个功能丰富的 *scheduler* 来安排事件。我们可以使用调度器来替换对 `Thread.sleep` 的调用，并安排在五秒后关闭系统。这比
    `Thread.sleep` 更好，因为调度器不会阻塞调用线程。要使用调度器，我们需要导入全局执行上下文和持续时间模块：
- en: '[PRE395]'
  id: totrans-1558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE395]'
- en: 'We can then schedule a system shutdown by replacing our call to `Thread.sleep`
    with the following:'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过用以下代码替换对 `Thread.sleep` 的调用来安排系统关闭：
- en: '[PRE396]'
  id: totrans-1560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE396]'
- en: Besides `scheduleOnce`, the scheduler also exposes a `schedule` method that
    lets you schedule events to happen regularly (every two seconds, for instance).
    This is useful for heartbeat checks or monitoring systems. For more information,
    read the API documentation on the scheduler available at [http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html](http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html).
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `scheduleOnce`，调度器还公开了一个 `schedule` 方法，允许您定期安排事件的发生（例如，每两秒一次）。这对于心跳检查或监控系统非常有用。有关更多信息，请参阅位于
    [http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html](http://doc.akka.io/docs/akka/snapshot/scala/scheduler.html)
    的调度器 API 文档。
- en: Note that we are actually cheating a little bit here by not fetching every follower.
    The response to the follower's query is actually paginated, so we would need to
    fetch several pages to fetch all the followers. Adding logic to the actor to do
    this is not terribly complicated. We will ignore this for now and assume that
    users are capped at 100 followers each.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里实际上有点作弊，因为我们没有获取每个关注者的信息。关注者查询的响应实际上是分页的，因此我们需要获取几页才能获取所有关注者。在演员中添加逻辑来完成这项工作并不复杂。我们目前将忽略这一点，并假设每个用户的关注者数量上限为
    100。
- en: Routing
  id: totrans-1563
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路由
- en: In the previous example, we created four fetchers and dispatched messages to
    them, one after the other. We have a pool of identical actors among which we distribute
    tasks. Manually routing the messages to the right actor to maximize the utilization
    of our pool is painful and error-prone. Fortunately, Akka provides us with several
    routing strategies that we can use to distribute work among our pool of actors.
    Let's rewrite the previous example with automatic routing. You can find the code
    examples for this section in the `chap09/fetchers_routing` directory in the sample
    code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    We will reuse the same definition of `Fetchers` and its companion object as we
    did in the previous section.
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们创建了四个fetchers并将消息依次派发给了它们。我们在其中分配任务的一组相同actor中有一个池。手动将消息路由到正确的actor以最大化我们池的利用率是痛苦且容易出错的。幸运的是，Akka为我们提供了几个路由策略，我们可以使用这些策略在我们的actor池中分配工作。让我们用自动路由重写之前的示例。您可以在本书提供的示例代码中的`chap09/fetchers_routing`目录下找到本节的代码示例（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。我们将重用与之前章节相同的`Fetchers`及其伴随对象的定义。
- en: 'Let''s start by importing the routing package:'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入路由包：
- en: '[PRE397]'
  id: totrans-1566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE397]'
- en: 'A *router* is an actor that forwards the messages that it receives to its children.
    The easiest way to define a pool of actors is to tell Akka to create a router
    and pass it a `Props` object for its children. The router will then manage the
    creation of the workers directly. In our example (we will only comment on the
    parts that differ from the previous example in the text, but you can find the
    full code in the `fetchers_routing` directory with the examples for this chapter),
    we replace the custom `Fetcher` creation code with the following:'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*router*是一个actor，它将其接收到的消息转发给其子actor。定义actor池的最简单方法就是告诉Akka创建一个router并传递一个为其子actor的`Props`对象。然后，router将直接管理工作者的创建。在我们的示例中（我们将在文本中仅注释与上一个示例不同的部分，但您可以在本章的示例代码的`fetchers_routing`目录中找到完整的代码），我们用以下代码替换了自定义的`Fetcher`创建代码：
- en: '[PRE398]'
  id: totrans-1568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE398]'
- en: 'We can then send the fetch messages directly to the router. The router will
    route the messages to the children in a round-robin manner:'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以直接将fetch消息发送到路由器。路由器将以轮询的方式将消息路由到其子actor：
- en: '[PRE399]'
  id: totrans-1570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE399]'
- en: We used a round-robin router in this example. Akka offers many different types
    of routers, including routers with dynamic pool size, to cater to different types
    of load balancing. Head over to the Akka documentation for a list of all the available
    routers, at [http://doc.akka.io/docs/akka/snapshot/scala/routing.html](http://doc.akka.io/docs/akka/snapshot/scala/routing.html).
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了轮询路由器。Akka提供了许多不同类型的路由器，包括具有动态池大小的路由器，以满足不同类型的负载均衡。前往Akka文档查看所有可用的路由器列表，请参阅[http://doc.akka.io/docs/akka/snapshot/scala/routing.html](http://doc.akka.io/docs/akka/snapshot/scala/routing.html)。
- en: Message passing between actors
  id: totrans-1572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Actor之间的消息传递
- en: 'Merely logging the API response is not very useful. To traverse the follower
    graph, we must perform the following:'
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 仅记录API响应并不很有用。为了遍历跟随者图，我们必须执行以下操作：
- en: Check the return code of the response to make sure that the GitHub API was happy
    with our request
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查响应的返回码以确保GitHub API对我们的请求满意
- en: Parse the response as JSON
  id: totrans-1575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将响应解析为JSON
- en: Extract the login names of the followers and, if we have not fetched them already,
    push them into the queue
  id: totrans-1576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取跟随者的登录名，如果我们还没有获取它们，将它们推入队列
- en: You learned how to do all these things in [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, but not in the context of actors.
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: 您在[第7章](part0059.xhtml#aid-1O8H61 "第7章。Web APIs")中学习了如何做所有这些事情，*Web APIs*，但不是在actor的上下文中。
- en: 'We could just add the additional processing steps to the `receive` method of
    our `Fetcher` actor: we could add further transformations to the API response
    by future composition. However, having actors do several different things, and
    possibly failing in several different ways, is an anti-pattern: when we learn
    about managing the actor life cycle, we will see that it becomes much more difficult
    to reason about our actor systems if the actors contain several bits of logic.'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将额外的处理步骤添加到我们的`Fetcher` actor的`receive`方法中：我们可以通过future composition添加进一步的转换到API响应。然而，让actor执行多个不同的操作，并且可能以多种方式失败，是一种反模式：当我们学习管理actor生命周期时，我们会看到如果actor包含多个逻辑片段，我们的actor系统就变得难以推理。
- en: 'We will therefore use a pipeline of three different actors:'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将使用三个不同actor的管道：
- en: The fetchers, which we have already encountered, are responsible just for fetching
    a URL from GitHub. They will fail if the URL is badly formatted or they cannot
    access the GitHub API.
  id: totrans-1580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经遇到的fetchers仅负责从GitHub获取URL。如果URL格式不正确或无法访问GitHub API，它们将失败。
- en: The response interpreter is responsible for taking the response from the GitHub
    API and parsing it to JSON. If it fails at any step, it will just log the error
    (in a real application, we might take different corrective actions depending on
    the type of failure). If it manages to extract JSON successfully, it will pass
    the JSON array to the follower extractor.
  id: totrans-1581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应解释器负责从GitHub API获取响应并将其解析为JSON。如果在任何步骤中失败，它将仅记录错误（在实际应用中，我们可能会根据失败类型采取不同的纠正措施）。如果它成功提取JSON，它将把JSON数组传递给跟随者提取器。
- en: The follower extractor will extract the followers from the JSON array and pass
    them on to the queue of users whose followers we need to fetch.
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟随者提取器将从JSON数组中提取跟随者，并将它们传递给需要获取其跟随者的用户队列。
- en: We have already built the fetchers, though we will need to modify them to forward
    the API response to the response interpreter rather than just logging it.
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了fetchers，尽管我们需要修改它们，以便将API响应转发给响应解释器，而不仅仅是记录日志。
- en: 'You can find the code examples for this section in the `chap09/all_workers`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).The
    first step is to modify the fetchers so that, instead of logging the response,
    they forward the response to the response interpreter. To be able to forward the
    response to the response interpreter, the fetchers will need a reference to this
    actor. We will just pass the reference to the response interpreter through the
    fetcher constructor, which is now:'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书提供的示例代码中的`chap09/all_workers`目录中找到本节的代码示例（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。第一步是修改fetchers，使其在记录响应而不是转发响应。为了能够将响应转发给响应解释器，fetchers将需要一个对这个演员的引用。我们将通过fetcher构造函数传递这个引用，现在的构造函数是：
- en: '[PRE400]'
  id: totrans-1585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE400]'
- en: 'We must also modify the `Props` factory method in the companion object:'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须修改伴随对象中的`Props`工厂方法：
- en: '[PRE401]'
  id: totrans-1587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE401]'
- en: 'We must also modify the `receive` method to forward the HTTP response to the
    interpreter rather than just logging it:'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须修改`receive`方法，将HTTP响应转发给解释器，而不仅仅是记录日志：
- en: '[PRE402]'
  id: totrans-1589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE402]'
- en: The *response interpreter* takes the response, decides if it is valid, parses
    it to JSON, and forwards it to a follower extractor. The response interpreter
    will need a reference to the follower extractor, which we will pass in the constructor.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: '*响应解释器*获取响应，判断其是否有效，将其解析为JSON，并将其转发给跟随者提取器。响应解释器将需要一个指向跟随者提取器的引用，我们将通过构造函数传递这个引用。'
- en: 'Let''s start by defining the `ResponseInterpreter` companion. It will just
    contain the definition of the messages that the response interpreter can receive
    and a factory to create a `Props` object to help with instantiation:'
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义`ResponseInterpreter`伴随对象开始。它将只包含响应解释器可以接收的消息定义以及一个用于创建`Props`对象的工厂，以帮助进行实例化：
- en: '[PRE403]'
  id: totrans-1592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE403]'
- en: 'The body of `ResponseInterpreter` should feel familiar: when the actor receives
    a message giving it a response to interpret, it parses it to JSON using the techniques
    that you learned in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"),
    *Web APIs*. If we parse the response successfully, we forward the parsed JSON
    to the follower extractor. If we fail to parse the response (possibly because
    it was badly formatted), we just log the error. We could recover from this in
    other ways, for instance, by re-adding this login to the queue manager to be fetched
    again:'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResponseInterpreter`的主体应该感觉熟悉：当演员收到一个提供要解释的响应的消息时，它使用你在[第7章](part0059.xhtml#aid-1O8H61
    "第7章。Web APIs")，“Web APIs”中学到的技术将其解析为JSON。如果我们成功解析响应，我们将把解析后的JSON转发给跟随者提取器。如果我们无法解析响应（可能是由于格式不正确），我们只需记录错误。我们可以以其他方式从中恢复，例如，通过将此登录重新添加到队列管理器中以便再次获取：'
- en: '[PRE404]'
  id: totrans-1594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE404]'
- en: 'We now have two-thirds of our worker actors. The last link is the follower
    extractor. This actor''s job is simple: it takes the `JArray` passed to it by
    the response interpreter and converts it to a list of followers. For now, we will
    just log this list, but when we build our fetcher manager, the follower extractor
    will send messages asking the manager to add the followers to its queue of logins
    to fetch.'
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了三分之二的工人演员。最后一个链接是跟随者提取器。这个演员的工作很简单：它接受响应解释器传递给它的`JArray`，并将其转换为跟随者列表。目前，我们只是记录这个列表，但当我们构建获取器管理者时，跟随者提取器将发送消息要求管理者将其添加到要获取的登录队列中。
- en: 'As before, the companion just defines the messages that this actor can receive
    and a Props factory method:'
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，伴随者仅定义了此演员可以接收的消息以及一个Props工厂方法：
- en: '[PRE405]'
  id: totrans-1597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE405]'
- en: 'The `FollowerExtractor` class receives `Extract` messages containing a `JArray`
    of information representing a follower. It extracts the `login` field and logs
    it:'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: '`FollowerExtractor`类接收包含表示跟随者的`JArray`信息的`Extract`消息。它提取`login`字段并记录它：'
- en: '[PRE406]'
  id: totrans-1599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE406]'
- en: 'Let''s write a new `main` method to exercise all our actors:'
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个新的`main`方法来测试所有我们的演员：
- en: '[PRE407]'
  id: totrans-1601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE407]'
- en: 'Let''s run this through SBT:'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过SBT运行这个例子：
- en: '[PRE408]'
  id: totrans-1603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE408]'
- en: Queue control and the pull pattern
  id: totrans-1604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 队列控制和拉取模式
- en: We have now defined the three worker actors in our crawler application. The
    next step is to define the manager. The *fetcher manager* is responsible for keeping
    a queue of logins to fetch as well as a set of login names that we have already
    seen in order to avoid fetching the same logins more than once.
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在我们的爬虫应用程序中定义了三个工作演员。下一步是定义管理者。*获取器管理者*负责维护一个要获取的登录队列以及一组我们已经看到的登录名称，以避免多次获取相同的登录。
- en: 'A first attempt might involve building an actor that keeps a set of users that
    we have already seen and just dispatches it to a round-robin router for fetchers
    when it is given a new user to fetch. The problem with this approach is that the
    number of messages in the fetchers'' mailboxes would accumulate quickly: for each
    API query, we are likely to get tens of followers, each of which is likely to
    make it back to a fetcher''s inbox. This gives us very little control over the
    amount of work piling up.'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个尝试可能涉及构建一个演员，它维护一组我们已经看到的用户，并在给定一个新用户获取时将其调度到轮询路由器。这种方法的问题在于获取器邮箱中的消息数量会迅速积累：对于每个API查询，我们可能会得到数十个跟随者，每个跟随者都可能回到获取器的收件箱。这使我们很难控制堆积的工作量。
- en: 'The first problem that this is likely to cause involves the GitHub API rate
    limit: even with authentication, we are limited to 5,000 requests per hour. It
    would be useful to stop queries as soon as we hit this threshold. We cannot be
    responsive if each fetcher has a backlog of hundreds of users that they need to
    fetch.'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能导致的第一个问题是GitHub API速率限制：即使有认证，我们每小时也限制在5,000个请求。当我们达到这个阈值时，停止查询会有用。如果每个获取器都有数百个需要获取的用户积压，我们就无法做出响应。
- en: 'A better alternative is to use a *pull* system: the fetchers request work from
    a central queue when they find themselves idle. Pull systems are common in Akka
    when we have a producer that produces work faster than consumers can process it
    (refer to [http://www.michaelpollmeier.com/akka-work-pulling-pattern/](http://www.michaelpollmeier.com/akka-work-pulling-pattern/)).'
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的替代方案是使用*拉取*系统：当获取器发现自己空闲时，它们会从中央队列请求工作。在Akka中，当我们有一个生产者比消费者处理得更快时，拉取系统很常见（参考[http://www.michaelpollmeier.com/akka-work-pulling-pattern/](http://www.michaelpollmeier.com/akka-work-pulling-pattern/)）。
- en: 'Conversations between the manager and fetchers will proceed as follows:'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 管理者和获取器之间的对话将如下进行：
- en: If the manager goes from a state of having no work to having work, it sends
    a `WorkAvailable` message to all the fetchers.
  id: totrans-1610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果管理者从没有工作状态变为有工作状态，它会向所有获取器发送`WorkAvailable`消息。
- en: Whenever a fetcher receives a `WorkAvailable` message or when it completes an
    item of work, it sends a `GiveMeWork` message to the queue manager.
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当获取器收到`WorkAvailable`消息或完成一项工作时，它会向队列管理者发送`GiveMeWork`消息。
- en: When the queue manager receives a `GiveMeWork` message, it ignores the request
    if no work is available or it is throttled. If it has work, it sends a `Fetch(user)`
    message to the actor.
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当队列管理者收到`GiveMeWork`消息时，如果没有工作可用或它被限制，它会忽略请求。如果有工作，它向演员发送`Fetch(user)`消息。
- en: 'Let''s start by modifying our fetcher. You can find the code examples for this
    section in the `chap09/ghub_crawler` directory in the sample code provided with
    this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    We will pass a reference to the fetcher manager through the constructor. We need
    to change the companion object to add the `WorkAvailable` message and the `props`
    factory to include the reference to the manager:'
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从修改我们的fetcher开始。您可以在本书提供的示例代码中的`chap09/ghub_crawler`目录中找到本节的代码示例([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds))。我们将通过构造函数传递fetcher管理器的引用。我们需要更改伴随对象以添加`WorkAvailable`消息，并将`props`工厂包括到管理器的引用中：
- en: '[PRE409]'
  id: totrans-1614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE409]'
- en: We also need to change the `receive` method so that it queries the `FetcherManager`
    asking for more work once it's done processing a request or when it receives a
    `WorkAvailable` message.
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更改`receive`方法，以便在处理完一个请求或接收到`WorkAvailable`消息后，它能够查询`FetcherManager`以获取更多工作。
- en: 'This is the final version of the fetchers:'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 这是fetchers的最终版本：
- en: '[PRE410]'
  id: totrans-1617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE410]'
- en: Now that we have a working definition of the fetchers, let's build the `FetcherManager`.
    This is the most complex actor that we have built so far, and, before we dive
    into building it, we need to learn a bit more about the components of the Akka
    toolkit.
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个工作定义的fetchers，让我们构建`FetcherManager`。这是我们迄今为止构建的最复杂的actor，在我们深入构建它之前，我们需要更多地了解Akka工具包的组件。
- en: Accessing the sender of a message
  id: totrans-1619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问消息的发送者
- en: 'When our fetcher manager receives a `GiveMeWork` request, we will need to send
    work back to the correct fetcher. We can access the actor who sent a message using
    the `sender` method, which is a method of `Actor` that returns the `ActorRef`
    corresponding to the actor who sent the message currently being processed. The
    `case` statement corresponding to `GiveMeWork` in the fetcher manager is therefore:'
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的fetcher管理器收到`GiveMeWork`请求时，我们需要将工作发送回正确的fetcher。我们可以使用`sender`方法访问发送消息的actor，这是`Actor`的一个方法，它返回正在处理的消息对应的`ActorRef`。因此，fetcher管理器中对应于`GiveMeWork`的`case`语句是：
- en: '[PRE411]'
  id: totrans-1621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE411]'
- en: 'As `sender` is a *method*, its return value will change for every new incoming
    message. It should therefore only be used synchronously with the `receive` method.
    In particular, using it in a future is dangerous:'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`sender`是一个*方法*，它的返回值会随着每个新传入的消息而改变。因此，它应该仅与`receive`方法同步使用。特别是，在future中使用它是危险的：
- en: '[PRE412]'
  id: totrans-1623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE412]'
- en: The problem is that when the future is completed 20 seconds after the message
    is processed, the actor will, in all likelihood, be processing a different message
    so the return value of `sender` will have changed. We will thus send the `Complete`
    message to a completely different actor.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，当future在消息处理后的20秒完成时，actor很可能会处理不同的消息，因此`sender`的返回值将改变。因此，我们将`Complete`消息发送给一个完全不同的actor。
- en: 'If you need to reply to a message outside of the `receive` method, such as
    when a future completes, you should bind the value of the current sender to a
    variable:'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在`receive`方法之外回复消息，例如当future完成时，您应该将当前发送者的值绑定到一个变量上：
- en: '[PRE413]'
  id: totrans-1626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE413]'
- en: Stateful actors
  id: totrans-1627
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态演员
- en: 'The behavior of the fetcher manager depends on whether it has work to give
    out to the fetchers:'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: fetcher管理器的行为取决于它是否有工作要分配给fetchers：
- en: If it has work to give, it needs to respond to `GiveMeWork` messages with a
    `Fetcher.Fetch` message
  id: totrans-1629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它有工作要提供，它需要用`Fetcher.Fetch`消息响应`GiveMeWork`消息
- en: If it does not have work, it must ignore the `GiveMeWork` messages and, if work
    gets added, it must send a `WorkAvailable` message to the fetchers
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有工作，它必须忽略`GiveMeWork`消息，并且如果添加了工作，它必须向fetchers发送一个`WorkAvailable`消息。
- en: 'Encoding the notion of state is straightforward in Akka. We specify different
    `receive` methods and switch from one to the other depending on the state. We
    will define the following `receive` methods for our fetcher manager, corresponding
    to each of the states:'
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 在Akka中编码状态的概念很简单。我们指定不同的`receive`方法，并根据状态从一个切换到另一个。我们将为我们的fetcher管理器定义以下`receive`方法，对应于每个状态：
- en: '[PRE414]'
  id: totrans-1632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE414]'
- en: 'Note that we must define the return type of the receive methods as `Receive`.
    To switch the actor from one method to the other, we can use `context.become(methodName)`.
    Thus, for instance, when the last login name is popped off the queue, we can transition
    to using the `receiveWhileEmpty` method with `context.become(receiveWhileEmpty)`.
    We set the initial state by assigning `receiveWhileEmpty` to the `receive` method:'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们必须将接收方法的返回类型定义为`Receive`。为了将演员从一个方法切换到另一个方法，我们可以使用`context.become(methodName)`。例如，当最后一个登录名从队列中弹出时，我们可以通过`context.become(receiveWhileEmpty)`过渡到使用`receiveWhileEmpty`方法。我们通过将`receiveWhileEmpty`分配给`receive`方法来设置初始状态：
- en: '[PRE415]'
  id: totrans-1634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE415]'
- en: Follower network crawler
  id: totrans-1635
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟随者网络爬虫
- en: 'We are now ready to code up the remaining pieces of our network crawler. The
    largest missing piece is the fetcher manager. Let''s start with the companion
    object. As with the worker actors, this just contains the definitions of the messages
    that the actor can receive and a factory to create the `Props` instance:'
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备编写网络爬虫剩余部分的代码。最大的缺失部分是获取器管理器。让我们从伴随对象开始。与工作演员类似，这仅包含演员可以接收的消息定义以及创建`Props`实例的工厂：
- en: '[PRE416]'
  id: totrans-1637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE416]'
- en: 'The manager can receive two messages: `AddToQueue`, which tells it to add a
    username to the queue of users whose followers need to be fetched, and `GiveMeWork`,
    emitted by the fetchers when they are unemployed.'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: 管理器可以接收两种消息：`AddToQueue`，它告诉管理器将用户名添加到需要获取跟随者的用户队列中，以及由获取器在失业时发出的`GiveMeWork`。
- en: 'The manager will be responsible for launching the fetchers, response interpreter,
    and follower extractor, as well as maintaining an internal queue of usernames
    and a set of usernames that we have seen:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 管理器将负责启动获取器、响应解释器和跟随者提取器，以及维护一个用户名内部队列和一组我们已看到的用户名：
- en: '[PRE417]'
  id: totrans-1640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE417]'
- en: 'We now have a fetcher manager. The rest of the code can remain the same, apart
    from the follower extractor. Instead of logging followers names, it must send
    `AddToQueue` messages to the manager. We will pass a reference to the manager
    at construction time:'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了获取器管理器。除了跟随者提取器之外，其余的代码可以保持不变。而不是记录跟随者名称，它必须向管理器发送`AddToQueue`消息。我们将在构造时传递管理器的引用：
- en: '[PRE418]'
  id: totrans-1642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE418]'
- en: 'The `main` method running all this is remarkably simple as all the code to
    instantiate actors has been moved to the `FetcherManager`. We just need to instantiate
    the manager and give it the first node in the network, and it will do the rest:'
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: 运行所有这些的`main`方法非常简单，因为所有实例化演员的代码都已移动到`FetcherManager`。我们只需要实例化管理器，并给它网络中的第一个节点，然后它会完成其余的工作：
- en: '[PRE419]'
  id: totrans-1644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE419]'
- en: 'Notice how we do not attempt to shut down the actor system anymore. We will
    just let it run, crawling the network, until we stop it or hit the authentication
    limit. Let''s run this through SBT:'
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们不再尝试关闭演员系统了。我们将让它运行，爬取网络，直到我们停止它或达到认证限制。让我们通过SBT运行这个程序：
- en: '[PRE420]'
  id: totrans-1646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE420]'
- en: Our program does not actually do anything useful with the followers that it
    retrieves besides logging them. We could replace the `log.info` call to, for instance,
    store the nodes in a database or draw the graph to screen.
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的程序实际上并没有对检索到的跟随者做任何有用的操作，除了记录它们。我们可以将`log.info`调用替换为，例如，将节点存储在数据库中或在屏幕上绘制图形。
- en: Fault tolerance
  id: totrans-1648
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错性
- en: Real programs fail, and they fail in unpredictable ways. Akka, and the Scala
    community in general, favors planning explicitly for failure rather than trying
    to write infallible applications. A *fault tolerant* system is a system that can
    continue to operate when one or more of its components fails. The failure of an
    individual subsystem does not necessarily mean the failure of the application.
    How does this apply to Akka?
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 真实程序会失败，并且以不可预测的方式失败。Akka以及整个Scala社区都倾向于明确规划失败，而不是试图编写不可失败的应用程序。一个*容错*系统是指当其一个或多个组件失败时仍能继续运行的系统。单个子系统的失败并不一定意味着应用程序的失败。这如何应用于Akka？
- en: 'The actor model provides a natural unit to encapsulate failure: the actor.
    When an actor throws an exception while processing a message, the default behavior
    is for the actor to restart, but the exception does not leak out and affect the
    rest of the system. For instance, let''s introduce an arbitrary failure in the
    response interpreter. We will modify the `receive` method to throw an exception
    when it is asked to interpret the response for `misto`, one of Martin Odersky''s
    followers:'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: 演员模型提供了一个自然的单元来封装失败：演员。当一个演员在处理消息时抛出异常，默认行为是演员重启，但异常不会泄露并影响系统的其余部分。例如，让我们在响应解释器中引入一个任意的失败。我们将修改`receive`方法，当它被要求解释`misto`（Martin
    Odersky的一个关注者）的响应时抛出异常：
- en: '[PRE421]'
  id: totrans-1651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE421]'
- en: 'If you rerun the code through SBT, you will notice that an error gets logged.
    The program does not crash, however. It just continues as normal:'
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过SBT重新运行代码，你会注意到记录了一个错误。然而，程序并没有崩溃。它只是继续正常运行：
- en: '[PRE422]'
  id: totrans-1653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE422]'
- en: 'None of the followers of `misto` will get added to the queue: he never made
    it past the `ResponseInterpreter` stage. Let''s step through what happens when
    the exception gets thrown:'
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
  zh: '`misto`的任何关注者都不会被添加到队列中：他从未通过`ResponseInterpreter`阶段。让我们逐步了解当异常被抛出时会发生什么：'
- en: The interpreter is sent the `InterpretResponse("misto", ...)` message. This
    causes it to throw an exception and it dies. None of the other actors are affected
    by the exception.
  id: totrans-1655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释器接收到`InterpretResponse("misto", ...)`消息。这导致它抛出异常并死亡。其他演员不受异常的影响。
- en: A fresh instance of the response interpreter is created with the same Props
    instance as the recently deceased actor.
  id: totrans-1656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与最近去世的演员相同的Props实例创建响应解释器的新实例。
- en: When the response interpreter has finished initializing, it gets bound to the
    same `ActorRef` as the deceased actor. This means that, as far as the rest of
    the system is concerned, nothing has changed.
  id: totrans-1657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当响应解释器完成初始化后，它被绑定到与去世演员相同的`ActorRef`。这意味着，对于系统的其余部分来说，没有任何变化。
- en: The mailbox is tied to `ActorRef` rather than the actor, so the new response
    interpreter will have the same mailbox as its predecessor, without the offending
    message.
  id: totrans-1658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮箱绑定到`ActorRef`而不是演员，因此新的响应解释器将与其前任具有相同的邮箱，而不包括有问题的消息。
- en: Thus, if, for whatever reason, our crawler crashes when fetching or parsing
    the response for a user, the application will be minimally affected—we will just
    not fetch this user's followers.
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论出于什么原因，我们的爬虫在抓取或解析用户响应时崩溃，应用程序的影响将最小——我们只是不会抓取此用户的关注者。
- en: 'Any internal state that an actor carries is lost when it restarts. Thus, if,
    for instance, the fetcher manager died, we would lose the current value of the
    queue and visited users. The risks associated with losing the internal state can
    be mitigated by the following:'
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: 当演员重启时，它携带的任何内部状态都会丢失。因此，例如，如果fetcher管理器死亡，我们将丢失队列的当前值和已访问用户。可以通过以下方式减轻丢失内部状态的风险：
- en: 'Adopting a different strategy for failure: we can, for instance, carry on processing
    messages without restarting the actor in the event of failure. Of course, this
    is of little use if the actor died because its internal state is inconsistent.
    In the next section, we will discuss how to change the failure recovery strategy.'
  id: totrans-1661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用不同的失败策略：例如，在失败的情况下，我们可以继续处理消息而不重启演员。当然，如果演员死亡是因为其内部状态不一致，这几乎没有什么用处。在下一节中，我们将讨论如何更改失败恢复策略。
- en: Backing up the internal state by writing it to disk periodically and loading
    from the backup on restart.
  id: totrans-1662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过定期将其写入磁盘并从备份中重新启动时加载来备份内部状态。
- en: 'Protecting actors that carry critical state by ensuring that all "risky" operations
    are delegated to other actors. In our crawler example, all the interactions with
    external services, such as querying the GitHub API and parsing the response, happen
    with actors that carry no internal state. As we saw in the previous example, if
    one of these actors dies, the application is minimally affected. By contrast,
    the precious fetcher manager is only allowed to interact with sanitized inputs.
    This is called the *error kernel* pattern: code likely to cause errors is delegated
    to kamikaze actors.'
  id: totrans-1663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过确保所有“风险”操作都委派给其他演员来保护携带关键状态的演员。在我们的爬虫示例中，所有与外部服务的交互，例如查询GitHub API和解析响应，都是通过不带内部状态的演员进行的。正如我们在前面的示例中看到的，如果这些演员中的任何一个死亡，应用程序的影响将最小。相比之下，宝贵的fetcher管理器只允许与经过清理的输入进行交互。这被称为*错误内核*模式：可能引起错误的代码被委派给自杀式演员。
- en: Custom supervisor strategies
  id: totrans-1664
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义监督策略
- en: The default strategy of restarting an actor on failure is not always what we
    want. In particular, for actors that carry a lot of data, we might want to resume
    processing after an exception rather than restarting the actor. Akka lets us customize
    this behavior by setting a *supervisor strategy* in the actor's supervisor.
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
  zh: 在失败时重新启动演员的默认策略并不总是我们想要的。特别是对于携带大量数据的演员，我们可能希望在异常后恢复处理而不是重新启动演员。Akka允许我们通过在演员的监督者中设置*监督策略*来自定义此行为。
- en: Recall that all actors have parents, including the top-level actors, who are
    children of a special actor called the *user guardian*. By default, an actor's
    supervisor is his parent, and it is the supervisor who decides what happens to
    the actor on failure.
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，所有演员都有父母，包括顶级演员，它们是称为*用户守护者*的特殊演员的孩子。默认情况下，演员的监督者是它的父母，监督者决定在失败时对演员做什么。
- en: 'Thus, to change how an actor reacts to failure, you must set its parent''s
    supervisor strategy. You do this by setting the `supervisorStrategy` attribute.
    The default strategy is equivalent to the following:'
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要改变演员对失败的反应方式，你必须设置其父母的监督策略。你通过设置`supervisorStrategy`属性来完成此操作。默认策略等同于以下内容：
- en: '[PRE423]'
  id: totrans-1668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE423]'
- en: 'There are two components to a supervisor strategy:'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: 一个监督策略有两个组成部分：
- en: '`OneForOneStrategy` determines that the strategy applies only to the actor
    that failed. By contrast, we can use `AllForOneStrategy`, which applies the same
    strategy to all the supervisees. If a single child fails, all the children will
    be restarted (or stopped or resumed).'
  id: totrans-1670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneForOneStrategy`确定策略仅适用于失败的演员。相比之下，我们可以使用`AllForOneStrategy`，它将相同的策略应用于所有被监督者。如果一个子进程失败，所有子进程都将被重新启动（或停止或恢复）。'
- en: A partial function mapping `Throwables` to a `Directive`, which is an instruction
    on what to do in response to a failure. The default strategy, for instance, maps
    `ActorInitializationException` (which happens if the constructor fails) to the
    `Stop` directive and (almost all) other exceptions to `Restart`.
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个部分函数映射`Throwables`到`Directive`，这是一个关于在失败时如何操作的指令。例如，默认策略将`ActorInitializationException`（如果构造函数失败会发生）映射到`Stop`指令，以及（几乎所有）其他异常映射到`Restart`。
- en: 'There are four directives:'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个指令：
- en: '`Restart`: This destroys the faulty actor and restarts it, binding the newborn
    actor to the old `ActorRef`. This clears the internal state of the actor, which
    may be a good thing (the actor might have failed because of some internal inconsistency).'
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Restart`：这会销毁有缺陷的演员并重新启动它，将新生成的演员绑定到旧的`ActorRef`。这将清除演员的内部状态，这可能是一件好事（演员可能因为某些内部不一致性而失败）。'
- en: '`Resume`: The actor just moves on to processing the next message in its inbox.'
  id: totrans-1674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Resume`：演员只需继续处理其收件箱中的下一个消息。'
- en: '`Stop`: The actor stops and is not restarted. This is useful in throwaway actors
    that you use to complete a single operation: if this operation fails, the actor
    is not needed any more.'
  id: totrans-1675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stop`：演员停止并不会被重新启动。这在用于完成单个操作的丢弃演员中很有用：如果这个操作失败，演员就不再需要了。'
- en: '`Escalate`: The supervisor itself rethrows the exception, hoping that its supervisor
    will know what to do with it.'
  id: totrans-1676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Escalate`：监督者本身重新抛出异常，希望它的监督者知道如何处理它。'
- en: A supervisor does not have access to which of its children failed. Thus, if
    an actor has children that might require different recovery strategies, it is
    best to create a set of intermediate supervisor actors to supervise the different
    groups of children.
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 监督者无法访问其子进程失败的情况。因此，如果一个演员的子进程可能需要不同的恢复策略，最好创建一组中间监督演员来监督不同的子进程组。
- en: 'As an example of setting the supervisor strategy, let''s tweak the `FetcherManager`
    supervisor strategy to adopt an all-for-one strategy and stop its children when
    one of them fails. We start with the relevant imports:'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: 作为设置监督策略的示例，让我们调整`FetcherManager`的监督策略以采用全为一策略，并在其中一个子进程失败时停止其子进程。我们首先进行相关导入：
- en: '[PRE424]'
  id: totrans-1679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE424]'
- en: 'Then, we just need to set the `supervisorStrategy` attribute in the `FetcherManager`
    definition:'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需在`FetcherManager`定义中设置`supervisorStrategy`属性：
- en: '[PRE425]'
  id: totrans-1681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE425]'
- en: If you run this through SBT, you will notice that when the code comes across
    the custom exception thrown by the response interpreter, the system halts. This
    is because all the actors apart from the fetcher manager are now defunct.
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过SBT运行它，你会注意到当代码遇到响应解释器抛出的自定义异常时，系统会停止。这是因为除了检索管理器之外的所有演员现在都已失效。
- en: Life-cycle hooks
  id: totrans-1683
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生命周期钩子
- en: 'Akka lets us specify code that runs in response to specific events in an actor''s
    life, through *life-cycle hooks*. Akka defines the following hooks:'
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: Akka允许我们通过*生命周期钩子*指定在演员的生命周期中响应特定事件的代码。Akka定义了以下钩子：
- en: '`preStart()`: This runs after the actor''s constructor has finished but before
    it starts processing messages. This is useful to run initialization code that
    depends on the actor being fully constructed.'
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preStart()`：在演员构造函数完成后但开始处理消息之前运行。这对于运行依赖于演员完全构建的初始化代码很有用。'
- en: '`postStop()`: This runs when the actor dies after it has stopped processing
    messages. This is useful to run cleanup code before terminating the actor.'
  id: totrans-1686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`postStop()`：当演员停止处理消息后死亡时运行。在终止演员之前运行清理代码很有用。'
- en: '`preRestart(reason: Throwable, message: Option[Any])`: This is called just
    after an actor receives an order to restart. The `preRestart` method has access
    to the exception that was thrown and to the offending message, allowing for corrective
    action. The default behavior of `preRestart` is to stop each child and then call
    `postStop`.'
  id: totrans-1687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preRestart(reason: Throwable, message: Option[Any])`：在演员收到重启命令后立即调用。`preRestart`方法可以访问抛出的异常和有问题的消息，允许进行纠正操作。`preRestart`的默认行为是停止每个子演员，然后调用`postStop`。'
- en: '`postRestart(reason:Throwable)`: This is called after an actor has restarted.
    The default behavior is to call `preStart()`.'
  id: totrans-1688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`postRestart(reason:Throwable)`：在演员重启后调用。默认行为是调用`preStart()`。'
- en: 'Let''s use system hooks to persist the state of `FetcherManager` between runs
    of the programs. You can find the code examples for this section in the `chap09/ghub_crawler_fault_tolerant`
    directory in the sample code provided with this book ([https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)).
    This will make the fetcher manager fault-tolerant. We will use `postStop` to write
    the current queue and set of visited users to text files and `preStart` to read
    these text files from the disk. Let''s start by importing the libraries necessary
    to read and write files:'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用系统钩子来在程序运行之间持久化`FetcherManager`的状态。您可以在本书提供的示例代码中的`chap09/ghub_crawler_fault_tolerant`目录中找到本节的代码示例（[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)）。这将使fetcher管理器具有容错性。我们将使用`postStop`将当前队列和已访问用户集合写入文本文件，并使用`preStart`从磁盘读取这些文本文件。让我们首先导入读取和写入文件所需的库：
- en: '[PRE426]'
  id: totrans-1690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE426]'
- en: 'We will store the names of the two text files in which we persist the state
    in the `FetcherManager` companion object (a better approach would be to store
    them in a configuration file):'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`FetcherManager`伴随对象（更好的方法是将它们存储在配置文件中）中存储两个文本文件的名称：
- en: '[PRE427]'
  id: totrans-1692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE427]'
- en: 'In the `preStart` method, we load both the set of fetched users and the backlog
    of users to fetch from the text files, and in the `postStop` method, we overwrite
    these files with the new values of these data structures:'
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
  zh: 在`preStart`方法中，我们从文本文件中加载已获取用户集合和要获取用户的后备队列，并在`postStop`方法中，我们用这些数据结构的新值覆盖这些文件：
- en: '[PRE428]'
  id: totrans-1694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE428]'
- en: Now that we save the state of the crawler when it shuts down, we can put a better
    termination condition for the program than simply interrupting the program once
    we get bored. In production, we might halt the crawler when we have enough names
    in a database, for instance. In this example, we will simply let the crawler run
    for 30 seconds and then shut it down.
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们保存了爬虫在关闭时的状态，我们可以为程序设置比仅仅在无聊时中断程序更好的终止条件。在生产中，例如，当我们在数据库中有足够的名字时，我们可能会停止爬虫。在这个例子中，我们将简单地让爬虫运行30秒然后关闭它。
- en: 'Let''s modify the `main` method:'
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改`main`方法：
- en: '[PRE429]'
  id: totrans-1697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE429]'
- en: After 30 seconds, we just call `system.shutdown`, which stops all the actors
    recursively. This will stop the fetcher manager, calling the `postStop` life cycle
    hook. After one run of the program, I have 2,164 names in the `fetched-users.txt`
    file. Running it again increases this number to 3,728 users.
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: 30秒后，我们只需调用`system.shutdown`，这将递归地停止所有演员。这将停止fetcher管理器，并调用`postStop`生命周期钩子。程序运行一次后，我在`fetched-users.txt`文件中有2,164个名字。再次运行它将用户数量增加到3,728。
- en: We could improve fault tolerance further by making the fetcher manager dump
    the data structures at regular intervals while the code runs. As writing to the
    disk (or to a database) carries a certain element of risk (What if the database
    server goes down or the disk is full?) it would be better to delegate writing
    the data structures to a custom actor rather than endangering the manager.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在代码运行时让fetcher manager定期转储数据结构来进一步提高容错性。由于写入磁盘（或数据库）存在一定的风险（如果数据库服务器宕机或磁盘空间不足怎么办？），将数据结构的写入委托给一个自定义演员而不是危及管理者会更好。
- en: 'Our crawler has one minor problem: when the fetcher manager stops, it stops
    the fetcher actors, response interpreter, and follower extractor. However, none
    of the users currently going through these actors are stored. This also results
    in a small number of undelivered messages at the end of the code: if the response
    interpreter stops before a fetcher, the fetcher will try to deliver to a non-existent
    actor. This only accounts for a small number of users. To recover these login
    names, we can create a reaper actor whose job is to coordinate the killing of
    all the worker actors in the correct order and harvest their internal state. This
    pattern is documented in a blog post by *Derek Wyatt* ([http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2](http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2)).'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的爬虫有一个小问题：当fetcher manager停止时，它会停止fetcher actors、响应解释器和follower extractor。然而，目前通过这些演员的用户信息都没有被存储。这也导致代码末尾有少量未投递的消息：如果响应解释器在fetcher之前停止，fetcher将尝试向一个不存在的演员投递。这仅涉及少数用户。为了恢复这些登录名，我们可以创建一个收割者actor，其任务是协调正确顺序地杀死所有工作actor并收集它们的内部状态。这种模式已在Derek
    Wyatt的博客文章中记录（[http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2](http://letitcrash.com/post/30165507578/shutdown-patterns-in-akka-2)）。
- en: What we have not talked about
  id: totrans-1701
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们还没有讨论的内容
- en: 'Akka is a very rich ecosystem, far too rich to do it justice in a single chapter.
    There are some important parts of the toolkit that you will need, but we have
    not covered them here. We will give brief descriptions, but you can refer to the
    Akka documentation for more details:'
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: Akka是一个非常丰富的生态系统，远远超出了单章所能公正地描述的范围。有一些重要的工具包部分是你需要的，但我们在这里没有涵盖。我们将给出简要描述，但你可以在Akka文档中查找更多详细信息：
- en: The ask operator, `?`, offers an alternative to the tell operator, `!`, that
    we have used to send messages to actors. Unlike "tell", which just fires a message
    to an actor, the ask operator expects a response. This is useful when we need
    to ask actors questions rather than just telling them what to do. The ask pattern
    is documented at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future).
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 询问操作符 `?` 提供了 `!` 操作符的替代方案，我们曾用它向演员发送消息。与“tell”不同，后者只是向演员发送消息，询问操作符期望得到响应。当我们需要向演员提问而不是仅仅告诉他们做什么时，这很有用。询问模式已在[http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Ask__Send-And-Receive-Future)中记录。
- en: Deathwatch allows actors to watch another actor and receive a message when it
    dies. This is useful for actors that might depend on another actor but not be
    its direct supervisor. This is documented at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch).
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deathwatch 允许演员观察另一个演员，并在它死亡时接收消息。这对于可能依赖于另一个演员但不是其直接管理者的演员很有用。这已在[http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Lifecycle_Monitoring_aka_DeathWatch)中记录。
- en: In our crawler, we passed references to actors explicitly through the constructor.
    We can also look up actors using the actor hierarchy with a syntax reminiscent
    of files in a filesystem at [http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection).
  id: totrans-1705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的爬虫中，我们通过构造函数显式传递了演员的引用。我们也可以使用演员层次结构通过类似文件系统中的文件语法查找演员。[http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection](http://doc.akka.io/docs/akka/snapshot/scala/actors.html#Identifying_Actors_via_Actor_Selection)。
- en: 'We briefly explored how to implement stateful actors with different receive
    methods and using `context.become` to switch between them. Akka offers a more
    powerful alternative, based on finite state machines, to encode a more complex
    set of states and transitions: [http://doc.akka.io/docs/akka/snapshot/scala/fsm.html](http://doc.akka.io/docs/akka/snapshot/scala/fsm.html).'
  id: totrans-1706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们简要探讨了如何使用不同的接收方法实现有状态的actor，并使用`context.become`在它们之间切换。Akka提供了一个更强大的替代方案，基于有限状态机，以编码更复杂的状态集和转换：[http://doc.akka.io/docs/akka/snapshot/scala/fsm.html](http://doc.akka.io/docs/akka/snapshot/scala/fsm.html)。
- en: 'We have not discussed distributing actor systems across several nodes in this
    chapter. The message passing architecture works well with distributed setups:
    [http://doc.akka.io/docs/akka/2.4.0/common/cluster.html](http://doc.akka.io/docs/akka/2.4.0/common/cluster.html).'
  id: totrans-1707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们没有讨论如何在多个节点上分发actor系统。消息传递架构与分布式设置配合良好：[http://doc.akka.io/docs/akka/2.4.0/common/cluster.html](http://doc.akka.io/docs/akka/2.4.0/common/cluster.html)。
- en: Summary
  id: totrans-1708
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to weave actors together to tackle a difficult
    concurrent problem. More importantly, we saw how Akka's actor framework encourages
    us to think about concurrent problems in terms of many separate chunks of encapsulated
    mutable data, synchronized through message passing. Akka makes concurrent programming
    easier to reason about and more fun.
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何将actor编织在一起以解决一个困难的并发问题。更重要的是，我们看到了Akka的actor框架如何鼓励我们以许多独立的封装可变数据块的形式来思考并发问题，这些数据块通过消息传递进行同步。Akka使得并发编程更容易推理，并且更有趣。
- en: References
  id: totrans-1710
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Derek Wyatt''s* book, *Akka Concurrency*, is a fantastic introduction to Akka.
    It should definitely be the first stop for anyone wanting to do serious Akka programming.'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
  zh: '*德里克·惠特尼*的书籍《Akka并发》是Akka的绝佳入门指南。对于想要进行严肃Akka编程的人来说，这绝对是一个必看的起点。'
- en: The **LET IT CRASH** blog ([http://letitcrash.com](http://letitcrash.com)) is
    the official Akka blog, and contains many examples of idioms and patterns to solve
    common issues.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: '**“让它崩溃”**博客([http://letitcrash.com](http://letitcrash.com))是官方Akka博客，其中包含了许多用于解决常见问题的惯用语句和模式的示例。'
- en: Chapter 10. Distributed Batch Processing with Spark
  id: totrans-1713
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 使用Spark进行分布式批处理
- en: 'In [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and
    Futures"), *Parallel Collections and Futures*, we discovered how to use parallel
    collections for "embarrassingly" parallel problems: problems that can be broken
    down into a series of tasks that require no (or very little) communication between
    the tasks.'
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](part0036.xhtml#aid-12AK82 "第4章. 并行集合与未来")《并行集合与未来》中，我们发现了如何使用并行集合来解决“令人尴尬”的并行问题：这些问题可以被分解成一系列不需要（或非常少）任务间通信的任务。
- en: Apache Spark provides behavior similar to Scala parallel collections (and much
    more), but, instead of distributing tasks across different CPUs on the same computer,
    it allows the tasks to be distributed across a computer cluster. This provides
    arbitrary horizontal scalability, since we can simply add more computers to the
    cluster.
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了类似于Scala并行集合（以及更多）的行为，但它不是在相同计算机的不同CPU上分发任务，而是允许任务在计算机集群中分发。这提供了任意水平的横向可伸缩性，因为我们只需简单地添加更多计算机到集群中。
- en: In this chapter, we will learn the basics of Apache Spark and use it to explore
    a set of emails, extracting features with the view of building a spam filter.
    We will explore several ways of actually building a spam filter in [Chapter 12](part0117.xhtml#aid-3FIHQ2
    "Chapter 12. Distributed Machine Learning with MLlib"), *Distributed Machine Learning
    with MLlib*.
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习Apache Spark的基础知识，并使用它来探索一组电子邮件，提取特征，以构建垃圾邮件过滤器为目标。我们将在[第12章](part0117.xhtml#aid-3FIHQ2
    "第12章. 使用MLlib进行分布式机器学习")《使用MLlib进行分布式机器学习》中探索实际构建垃圾邮件过滤器的几种方法。
- en: Installing Spark
  id: totrans-1717
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Spark
- en: In previous chapters, we included dependencies by specifying them in a `build.sbt`
    file, and relying on SBT to fetch them from the Maven Central repositories. For
    Apache Spark, downloading the source code or pre-built binaries explicitly is
    more common, since Spark ships with many command line scripts that greatly facilitate
    launching jobs and interacting with a cluster.
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们通过在`build.sbt`文件中指定依赖项，并依赖SBT从Maven Central仓库中获取它们来包含依赖项。对于Apache
    Spark，显式下载源代码或预构建的二进制文件更为常见，因为Spark附带了许多命令行脚本，这些脚本极大地简化了作业的启动和与集群的交互。
- en: Head over to [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    and download Spark 1.5.2, choosing the "pre-built for Hadoop 2.6 or later" package.
    You can also build Spark from source if you need customizations, but we will stick
    to the pre-built version since it requires no configuration.
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载Spark
    1.5.2版本，选择“为Hadoop 2.6或更高版本预构建”的包。如果您需要定制，也可以从源代码构建Spark，但我们将坚持使用预构建版本，因为它不需要配置。
- en: 'Clicking **Download** will download a tarball, which you can unpack with the
    following command:'
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**下载**将下载一个tar包，您可以使用以下命令解包：
- en: '[PRE430]'
  id: totrans-1721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE430]'
- en: This will create a `spark-1.5.2-bin-hadoop2.6` directory. To verify that Spark
    works correctly, navigate to `spark-1.5.2-bin-hadoop2.6/bin` and launch the Spark
    shell using `./spark-shell`. This is just a Scala shell with the Spark libraries
    loaded.
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个`spark-1.5.2-bin-hadoop2.6`目录。要验证Spark是否正确工作，请导航到`spark-1.5.2-bin-hadoop2.6/bin`并使用`./spark-shell`启动Spark
    shell。这只是一个加载了Spark库的Scala shell。
- en: 'You may want to add the `bin/` directory to your system path. This will let
    you call the scripts in that directory from anywhere on your system, without having
    to reference the full path. On Linux or Mac OS, you can add variables to the system
    path by entering the following line in your shell configuration file (`.bash_profile`
    on Mac OS, and `.bashrc` or `.bash_profile` on Linux):'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望将`bin/`目录添加到系统路径中。这样，您就可以从系统中的任何位置调用该目录中的脚本，而无需引用完整路径。在Linux或Mac OS上，您可以通过在shell配置文件（Mac
    OS上的`.bash_profile`，Linux上的`.bashrc`或`.bash_profile`）中输入以下行来将变量添加到系统路径：
- en: '[PRE431]'
  id: totrans-1724
  prefs: []
  type: TYPE_PRE
  zh: '[PRE431]'
- en: 'The changes will take effect in new shell sessions. On Windows (if you use
    PowerShell), you need to enter this line in the `profile.ps1` file in the `WindowsPowerShell`
    folder in `Documents`:'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更改将在新的shell会话中生效。在Windows（如果您使用PowerShell）上，您需要在`Documents`文件夹中`WindowsPowerShell`目录下的`profile.ps1`文件中输入此行：
- en: '[PRE432]'
  id: totrans-1726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE432]'
- en: If this worked correctly, you should be able to open a Spark shell in any directory
    on your system by just typing `spark-shell` in a terminal.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作正确，您应该能够在系统中的任何目录下通过在终端中输入`spark-shell`来打开Spark shell。
- en: Acquiring the example data
  id: totrans-1728
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取示例数据
- en: In this chapter, we will explore the Ling-Spam email dataset (The original dataset
    is described at [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html)).
    Download the dataset from [http://data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)
    (or [ling-spam.zip](http://ling-spam.zip), depending on your preferred mode of
    compression), and unpack the contents to the directory containing the code examples
    for this chapter. The archive contains two directories, `spam/` and `ham/`, containing
    the spam and legitimate emails, respectively.
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索Ling-Spam电子邮件数据集（原始数据集的描述见[http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html)）。从[http://data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)（或[ling-spam.zip](http://ling-spam.zip)，取决于您首选的压缩方式）下载数据集，并将内容解压缩到包含本章代码示例的目录中。该存档包含两个目录，`spam/`和`ham/`，分别包含垃圾邮件和合法邮件。
- en: Resilient distributed datasets
  id: totrans-1730
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: 'Spark expresses all computations as a sequence of transformations and actions
    on distributed collections, called **Resilient Distributed Datasets** (**RDD**).
    Let''s explore how RDDs work with the Spark shell. Navigate to the examples directory
    and open a Spark shell as follows:'
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: Spark将所有计算表达为对分布式集合的转换和操作的序列，称为**弹性分布式数据集**（**RDD**）。让我们通过Spark shell来探索RDD是如何工作的。导航到示例目录，并按照以下方式打开Spark
    shell：
- en: '[PRE433]'
  id: totrans-1732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE433]'
- en: 'Let''s start by loading an email in an RDD:'
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载一个RDD中的电子邮件开始：
- en: '[PRE434]'
  id: totrans-1734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE434]'
- en: '`email` is an RDD, with each element corresponding to a line in the input file.
    Notice how we created the RDD by calling the `textFile` method on an object called
    `sc`:'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: '`email`是一个RDD，每个元素对应输入文件中的一行。注意我们是如何通过在名为`sc`的对象上调用`textFile`方法来创建RDD的：'
- en: '[PRE435]'
  id: totrans-1736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE435]'
- en: '`sc` is a `SparkContext` instance, an object representing the entry point to
    the Spark cluster (for now, just our local machine). When we start a Spark shell,
    a context is created and bound to the variable `sc` automatically.'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc`是一个`SparkContext`实例，代表Spark集群（目前是本地机器）的入口点（现在只需我们的本地机器）。当我们启动Spark shell时，会创建一个上下文并将其自动绑定到变量`sc`。'
- en: 'Let''s split the email into words using `flatMap`:'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`flatMap`将电子邮件拆分为单词：
- en: '[PRE436]'
  id: totrans-1739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE436]'
- en: 'This will feel natural if you are familiar with collections in Scala: the `email`
    RDD behaves just like a list of strings. Here, we split using the regular expression
    `\s`, denoting white space characters. Instead of using `flatMap` explicitly,
    we can also manipulate RDDs using Scala''s syntactic sugar:'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉 Scala 中的集合，这会感觉很自然：`email` RDD 的行为就像一个字符串列表。在这里，我们使用表示空白字符的正则表达式 `\s`
    进行拆分。我们不仅可以使用 `flatMap` 显式地操作 RDD，还可以使用 Scala 的语法糖来操作 RDD：
- en: '[PRE437]'
  id: totrans-1741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE437]'
- en: 'Let''s inspect the results. We can use `.take(n)` to extract the first *n*
    elements of an RDD:'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查结果。我们可以使用 `.take(n)` 来提取 RDD 的前 *n* 个元素：
- en: '[PRE438]'
  id: totrans-1743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE438]'
- en: 'We can also use `.count` to get the number of elements in an RDD:'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 `.count` 来获取 RDD 中的元素数量：
- en: '[PRE439]'
  id: totrans-1745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE439]'
- en: 'RDDs support many of the operations supported by collections. Let''s use `filter`
    to remove punctuation from our email. We will remove all words that contain any
    non-alphanumeric character. We can do this by filtering out elements that match
    this *regular expression* anywhere in the word: `[^a-zA-Z0-9]`.'
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 支持集合支持的大多数操作。让我们使用 `filter` 从我们的电子邮件中删除标点符号。我们将删除包含任何非字母数字字符的所有单词。我们可以通过过滤掉匹配此
    *正则表达式* 的元素来实现：`[^a-zA-Z0-9]`。
- en: '[PRE440]'
  id: totrans-1747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE440]'
- en: 'In this example, we created an RDD from a text file. We can also create RDDs
    from Scala iterables using the `sc.parallelize` method available on a Spark context:'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从一个文本文件创建了一个 RDD。我们还可以使用 Spark 上下文上可用的 `sc.parallelize` 方法从 Scala 可迭代对象创建
    RDD：
- en: '[PRE441]'
  id: totrans-1749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE441]'
- en: 'This is useful for debugging and for trialling behavior in the shell. The counterpart
    to parallelize is the `.collect` method, which converts an RDD to a Scala array:'
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于调试和在 shell 中试验行为很有用。与并行化相对应的是 `.collect` 方法，它将 RDD 转换为 Scala 数组：
- en: '[PRE442]'
  id: totrans-1751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE442]'
- en: The `.collect` method requires the entire RDD to fit in memory on the master
    node. It is thus either used for debugging with a reduced dataset, or at the end
    of a pipeline that trims down a dataset.
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: '`.collect` 方法需要整个 RDD 都能在主节点上适应内存。因此，它要么用于调试较小的数据集，要么用于管道的末尾，以缩减数据集。'
- en: As you can see, RDDs offer an API much like Scala iterables. The critical difference
    is that RDDs are *distributed* and *resilient*. Let's explore what this means
    in practice.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，RDD 提供了一个类似于 Scala 可迭代对象的 API。关键的区别是 RDD 是 *分布式* 和 *容错的*。让我们探讨这在实践中意味着什么。
- en: RDDs are immutable
  id: totrans-1754
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 是不可变的
- en: You cannot change an RDD once it is created. All operations on RDDs either create
    new RDDs or other Scala objects.
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建 RDD，就不能更改它。对 RDD 的所有操作要么创建新的 RDD，要么创建其他 Scala 对象。
- en: RDDs are lazy
  id: totrans-1756
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 是惰性的
- en: 'When you execute operations like map and filter on a Scala collection in the
    interactive shell, the REPL prints the values of the new collection to screen.
    The same isn''t true of Spark RDDs. This is because operations on RDDs are lazy:
    they are only evaluated when needed.'
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在交互式 shell 中对 Scala 集合执行 map 和 filter 等操作时，REPL 会将新集合的值打印到屏幕上。这并不适用于 Spark
    RDD。这是因为 RDD 上的操作是惰性的：只有在需要时才会进行评估。
- en: 'Thus, when we write:'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们写：
- en: '[PRE443]'
  id: totrans-1759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE443]'
- en: 'We are creating an RDD, `words` that knows how to build itself from its parent
    RDD, `email`, which, in turn, knows that it needs to read a text file and split
    it into lines. However, none of the commands actually happen until we force the
    evaluation of the RDDs by calling an *action* to return a Scala object. This is
    most evident if we try to read from a non-existent text file:'
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个 RDD，`words`，它知道如何从其父 RDD，`email`，构建自己，而 `email` 又知道它需要读取一个文本文件并将其拆分为行。然而，直到我们通过调用一个返回
    Scala 对象的 *动作* 来强制评估 RDD，实际上没有任何命令发生。如果我们尝试从一个不存在的文本文件中读取，这一点尤为明显：
- en: '[PRE444]'
  id: totrans-1761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE444]'
- en: 'We can create the RDD without a hitch. We can even define further transformations
    on the RDD. The program crashes only when these transformations are finally evaluated:'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以无障碍地创建 RDD。我们甚至可以在 RDD 上定义更多的转换。程序只有在这些转换最终评估时才会崩溃：
- en: '[PRE445]'
  id: totrans-1763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE445]'
- en: The action `.count` is expected to return the number of elements in our RDD
    as an integer. Spark has no choice but to evaluate `inp`, which results in an
    exception.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: '`.count` 动作预期返回我们 RDD 中元素的数量作为一个整数。Spark 除了评估 `inp` 没有其他选择，这会导致异常。'
- en: Thus, it is probably more appropriate to think of an RDD as a pipeline of operations,
    rather than a more traditional collection.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可能更合适的是将 RDD 视为一个操作管道，而不是一个更传统的集合。
- en: RDDs know their lineage
  id: totrans-1766
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 知道它们的血缘关系
- en: RDDs can only be constructed from stable storage (for instance, by loading data
    from a file that is present on every node in the Spark cluster), or through a
    set of transformations based on other RDDs. Since RDDs are lazy, they need to
    know how to build themselves when needed. They do this by knowing who their parent
    RDD is, and what operation they need to apply to the parent. This is a well-defined
    process since the parent RDD is immutable.
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: RDD只能从稳定存储（例如，通过从Spark集群中每个节点都存在的文件加载数据）或通过基于其他RDD的转换集构建。由于RDD是惰性的，它们需要在需要时知道如何构建自身。它们通过知道自己的父RDD是谁以及需要应用于父RDD的操作来实现这一点。由于父RDD是不可变的，这是一个定义良好的过程。
- en: 'The `toDebugString` method provides a diagram of how an RDD is constructed:'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: '`toDebugString` 方法提供了一个RDD构建过程的图示：'
- en: '[PRE446]'
  id: totrans-1769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE446]'
- en: RDDs are resilient
  id: totrans-1770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD具有弹性
- en: 'If you run an application on a single computer, you generally don''t need to
    worry about hardware failure in your application: if the computer fails, your
    application is doomed anyway.'
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个单机上运行应用程序，你通常不需要担心应用程序中的硬件故障：如果计算机失败，你的应用程序无论如何都是注定要失败的。
- en: 'Distributed architectures should, by contrast, be fault-tolerant: the failure
    of a single machine should not crash the entire application. Spark RDDs are built
    with fault tolerance in mind. Let''s imagine that one of the worker nodes fails,
    causing the destruction of some of the data associated with an RDD. Since the
    Spark RDD knows how to build itself from its parent, there is no permanent data
    loss: the elements that were lost can just be re-computed when needed on another
    computer.'
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，分布式架构应该是容错的：单个机器的故障不应该导致整个应用程序崩溃。Spark RDD是以容错性为设计理念的。让我们假设其中一个工作节点失败，导致与RDD相关的一些数据被破坏。由于Spark
    RDD知道如何从其父RDD构建自身，因此不会永久丢失数据：丢失的元素可以在需要时在另一台计算机上重新计算。
- en: RDDs are distributed
  id: totrans-1773
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD是分布式的
- en: When you construct an RDD, for instance from a text file, Spark will split the
    RDD into a number of partitions. Each partition will be entirely localized on
    a single machine (though there is, in general, more than one partition per machine).
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从一个文本文件等构建RDD时，Spark会将RDD拆分为多个分区。每个分区将完全本地化在单个机器上（尽管通常每台机器有多个分区）。
- en: 'Many transformations on RDDs can be executed on each partition independently.
    For instance, when performing a `.map` operation, a given element in the output
    RDD depends on a single element in the parent: data does not need to be moved
    between partitions. The same is true of `.flatMap` and `.filter` operations. This
    means that the partition in the RDD produced by one of these operations depends
    on a single partition in the parent RDD.'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: RDD上的许多转换可以在每个分区独立执行。例如，当执行`.map`操作时，输出RDD中的给定元素依赖于父RDD中的单个元素：数据不需要在分区之间移动。`.flatMap`和`.filter`操作也是如此。这意味着由这些操作之一产生的RDD中的分区依赖于父RDD中的单个分区。
- en: On the other hand, a `.distinct` transformation, which removes all duplicate
    elements from an RDD, requires the data in a given partition to be compared to
    the data in every other partition. This requires *shuffling* the data across the
    nodes. Shuffling, especially for large datasets, is an expensive operation and
    should be avoided if possible.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`.distinct`转换，它从RDD中删除所有重复元素，需要将给定分区中的数据与每个其他分区中的数据进行比较。这需要在节点之间进行*洗牌*。洗牌，特别是对于大型数据集，是一个昂贵的操作，如果可能的话应该避免。
- en: Transformations and actions on RDDs
  id: totrans-1777
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD上的转换和操作
- en: 'The set of operations supported by an RDD can be split into two categories:'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: RDD支持的操作集可以分为两类：
- en: '**Transformations** create a new RDD from the current one. Transformations
    are lazy: they are not evaluated immediately.'
  id: totrans-1779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**从当前RDD创建一个新的RDD。转换是惰性的：它们不会立即被评估。'
- en: '**Actions** force the evaluation of an RDD, and normally return a Scala object,
    rather than an RDD, or have some form of side-effect. Actions are evaluated immediately,
    triggering the execution of all the transformations that make up this RDD.'
  id: totrans-1780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**强制评估RDD，通常返回一个Scala对象，而不是RDD，或者有一些形式的副作用。操作会立即评估，触发构成此RDD的所有转换的执行。'
- en: In the tables below, we give some examples of useful transformations and actions.
    For a full, up-to-date list, consult the Spark documentation ([http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)).
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的表格中，我们给出了一些有用的转换和操作的示例。对于完整和最新的列表，请参阅Spark文档([http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations))。
- en: 'For the examples in these tables, we assume that you have created an RDD with:'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些表格中的示例，我们假设你已经创建了一个包含以下内容的RDD：
- en: '[PRE447]'
  id: totrans-1783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE447]'
- en: 'The following table lists common transformations on an RDD. Recall that transformations
    always generate a new RDD, and that they are lazy operations:'
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了RDD上的常见转换。请记住，转换始终生成一个新的RDD，并且它们是惰性操作：
- en: '| Transformation | Notes | **Example (assuming** `rdd` **is** `{ "quick", "brown",
    "quick", "dog" }`) |'
  id: totrans-1785
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 注意事项 | **示例（假设** `rdd` **是** `{ "quick", "brown", "quick", "dog" }`）
    |'
- en: '| `rdd.map(func)` |   | `rdd.map { _.size } // => { 5, 5, 5, 3 }` |'
  id: totrans-1786
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.map(func)` |   | `rdd.map { _.size } // => { 5, 5, 5, 3 }` |'
- en: '| `rdd.filter(pred)` |   | `rdd.filter { _.length < 4 } // => { "dog" }` |'
  id: totrans-1787
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.filter(pred)` |   | `rdd.filter { _.length < 4 } // => { "dog" }` |'
- en: '| `rdd.flatMap(func)` |   | `rdd.flatMap { _.toCharArray } // => { ''q'', ''u'',
    ''i'', ''c'', ''k'', ''b'', ''r'', ''o'' … }` |'
  id: totrans-1788
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.flatMap(func)` |   | `rdd.flatMap { _.toCharArray } // => { ''q'', ''u'',
    ''i'', ''c'', ''k'', ''b'', ''r'', ''o'' … }` |'
- en: '| `rdd.distinct()` | Remove duplicate elements in RDD. | `rdd.distinct // =>
    { "dog", "brown", "quick" }` |'
  id: totrans-1789
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.distinct()` | 从RDD中移除重复元素。 | `rdd.distinct // => { "dog", "brown", "quick"
    }` |'
- en: '| `rdd.pipe(command, [envVars])` | Pipe through an external program. RDD elements
    are written, line-by-line, to the process''s `stdin`. The output is read from
    `stdout`. | `rdd.pipe("tr a-z A-Z") // => { "QUICK", "BROWN", "QUICK", "DOG" }`
    |'
  id: totrans-1790
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.pipe(command, [envVars])` | 通过外部程序进行管道传输。RDD元素逐行写入进程的`stdin`。从`stdout`读取输出。
    | `rdd.pipe("tr a-z A-Z") // => { "QUICK", "BROWN", "QUICK", "DOG" }` |'
- en: The following table describes common actions on RDDs. Recall that actions always
    generate a Scala type or cause a side-effect, rather than creating a new RDD.
    Actions force the evaluation of the RDD, triggering the execution of the transformations
    underpinning the RDD.
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格描述了RDD上的常见动作。请记住，动作始终生成Scala类型或引起副作用，而不是创建一个新的RDD。动作强制评估RDD，触发RDD下支撑的转换的执行。
- en: '| Action | Nodes | **Example (assuming** `rdd` **is** `{ "quick", "brown",
    "quick", "dog" }`) |'
  id: totrans-1792
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 节点 | **示例（假设** `rdd` **是** `{ "quick", "brown", "quick", "dog" }`） |'
- en: '| `rdd.first` | First element in the RDD. | `rdd.first // => quick` |'
  id: totrans-1793
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.first` | RDD中的第一个元素。 | `rdd.first // => quick` |'
- en: '| `rdd.collect` | Transform the RDD to an array (the array must be able to
    fit in memory on the master node). | `rdd.collect // => Array[String]("quick",
    "brown", "quick", "dog")` |'
  id: totrans-1794
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.collect` | 将RDD转换为数组（数组必须在主节点上能够适应内存）。 | `rdd.collect // => Array[String]("quick",
    "brown", "quick", "dog")` |'
- en: '| `rdd.count` | Number of elements in the RDD. | `rdd.count // => 4` |'
  id: totrans-1795
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.count` | RDD中的元素数量。 | `rdd.count // => 4` |'
- en: '| `rdd.countByValue` | Map of element to the number of times this element occurs.
    The map must fit on the master node. | `rdd.countByValue // => Map(quick -> 2,
    brown -> 1, dog -> 1)` |'
  id: totrans-1796
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.countByValue` | 元素到该元素出现次数的映射。映射必须在主节点上适应。 | `rdd.countByValue // =>
    Map(quick -> 2, brown -> 1, dog -> 1)` |'
- en: '| `rdd.take(n)` | Return an array of the first *n* elements in the RDD. | `rdd.take(2)
    // => Array(quick, brown)` |'
  id: totrans-1797
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.take(n)` | 返回RDD中前*n*个元素的数组。 | `rdd.take(2) // => Array(quick, brown)`
    |'
- en: '| `rdd.takeOrdered(n:Int)(implicit ordering: Ordering[T])` | Top *n* elements
    in the RDD according to the element''s default ordering, or the ordering passed
    as second argument. See the Scala docs for `Ordering` for how to define custom
    comparison functions ([http://www.scala-lang.org/api/current/index.html#scala.math.Ordering](http://www.scala-lang.org/api/current/index.html#scala.math.Ordering)).
    | `rdd.takeOrdered(2) // => Array(brown, dog)``rdd.takeOrdered(2) (Ordering.by
    { _.size }) // => Array[String] = Array(dog, quick)` |'
  id: totrans-1798
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.takeOrdered(n:Int)(implicit ordering: Ordering[T])` | 根据元素的默认排序或作为第二个参数传递的排序，按顺序获取RDD中的前*n*个元素。有关如何定义自定义比较函数的说明，请参阅Scala文档中的`Ordering`（[http://www.scala-lang.org/api/current/index.html#scala.math.Ordering](http://www.scala-lang.org/api/current/index.html#scala.math.Ordering)）。
    | `rdd.takeOrdered(2) // => Array(brown, dog)``rdd.takeOrdered(2) (Ordering.by
    { _.size }) // => Array[String] = Array(dog, quick)` |'
- en: '| `rdd.reduce(func)` | Reduce the RDD according to the specified function.
    Uses the first element in the RDD as the base. `func` should be commutative and
    associative. | `rdd.map { _.size }.reduce { _ + _ } // => 18` |'
  id: totrans-1799
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.reduce(func)` | 根据指定的函数减少RDD。使用RDD中的第一个元素作为基数。`func`应该是交换律和结合律的。 | `rdd.map
    { _.size }.reduce { _ + _ } // => 18` |'
- en: '| `rdd.aggregate(zeroValue)(seqOp, combOp)` | Reduction for cases where the
    reduction function returns a value of type different to the RDD''s type. In this
    case, we need to provide a function for reducing within a single partition (`seqOp`)
    and a function for combining the value of two partitions (`combOp`). | `rdd.aggregate(0)
    ( _ + _.size, _ + _ ) // => 18` |'
  id: totrans-1800
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.aggregate(zeroValue)(seqOp, combOp)` | 用于返回类型与RDD类型不同的值的减少情况。在这种情况下，我们需要提供一个用于单个分区内减少的函数（`seqOp`）和一个用于合并两个分区值的函数（`combOp`）。
    | `rdd.aggregate(0) ( _ + _.size, _ + _ ) // => 18` |'
- en: Persisting RDDs
  id: totrans-1801
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化RDDs
- en: 'We have learned that RDDs only retain the sequence of operations needed to
    construct the elements, rather than the values themselves. This, of course, drastically
    reduces memory usage since we do not need to keep intermediate versions of our
    RDDs in memory. For instance, let''s assume we want to trawl through transaction
    logs to identify all the transactions that occurred on a particular account:'
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到RDDs只保留构建元素所需的操作序列，而不是元素本身的值。这当然大大减少了内存使用，因为我们不需要在内存中保留RDDs的中间版本。例如，假设我们想要遍历事务日志以识别特定账户上发生的所有交易：
- en: '[PRE448]'
  id: totrans-1803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE448]'
- en: The set of all transactions will be large, while the set of transactions on
    the account of interest will be much smaller. Spark's policy of remembering *how*
    to construct a dataset, rather than the dataset itself, means that we never have
    all the lines of our input file in memory at any one time.
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
  zh: 所有交易的集合将很大，而感兴趣账户上的交易集合将小得多。Spark记住如何构建数据集，而不是数据集本身的政策意味着我们任何时候都不需要在内存中保留所有输入文件的行。
- en: 'There are two situations in which we may want to avoid re-computing the elements
    of an RDD every time we use it:'
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种情况我们可能希望避免每次使用RDD时重新计算其元素：
- en: 'For interactive use: we might have detected fraudulent behavior on account
    "123456", and we want to investigate how this might have arisen. We will probably
    want to perform many different exploratory calculations on this RDD, without having
    to re-read the entire log file every time. It therefore makes sense to persist
    `interestingTransactions`.'
  id: totrans-1806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于交互式使用：我们可能已经检测到账户“123456”上的欺诈行为，并希望调查这种情况是如何发生的。我们可能需要在这个RDD上执行许多不同的探索性计算，而不必每次都重新读取整个日志文件。因此，持久化`interestingTransactions`是有意义的。
- en: When an algorithm re-uses an intermediate result, or a dataset. A canonical
    example is logistic regression. In logistic regression, we normally use an iterative
    algorithm to find the 'optimal' coefficients that minimize the loss function.
    At every step in our iterative algorithm, we must calculate the loss function
    and its gradient from the training set. We should avoid re-computing the training
    set (or re-loading it from an input file) if at all possible.
  id: totrans-1807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当算法重新使用中间结果或数据集时。一个典型的例子是逻辑回归。在逻辑回归中，我们通常使用迭代算法来找到最小化损失函数的“最优”系数。在我们迭代算法的每一步中，我们必须从训练集中计算损失函数及其梯度。如果可能的话，我们应该避免重新计算训练集（或从输入文件中重新加载它）。
- en: Spark provides a `.persist` method on RDDs to achieve this. By calling `.persist`
    on an RDD, we tell Spark to keep the dataset in memory next time it is computed.
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在RDD上提供了一个`.persist`方法来实现这一点。通过在RDD上调用`.persist`，我们告诉Spark在下次计算时将数据集保留在内存中。
- en: '[PRE449]'
  id: totrans-1809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE449]'
- en: 'Spark supports different levels of persistence, which you can tune by passing
    arguments to `.persist`:'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持不同的持久化级别，您可以通过传递参数给`.persist`来调整：
- en: '[PRE450]'
  id: totrans-1811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE450]'
- en: 'Spark provides several persistence levels, including:'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了几个持久化级别，包括：
- en: '`MEMORY_ONLY`: the default storage level. The RDD is stored in RAM. If the
    RDD is too big to fit in memory, parts of it will not persist, and will need to
    be re-computed on the fly.'
  id: totrans-1813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY`: 默认存储级别。RDD存储在RAM中。如果RDD太大而无法适应内存，则其部分将不会持久化，并且需要即时重新计算。'
- en: '`MEMORY_AND_DISK`: As much of the RDD is stored in memory as possible. If the
    RDD is too big, it will spill over to disk. This is only worthwhile if the RDD
    is expensive to compute. Otherwise, re-computing it may be faster than reading
    from the disk.'
  id: totrans-1814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK`: 尽可能多地存储RDD在内存中。如果RDD太大，它将溢出到磁盘。这只有在RDD计算成本很高的情况下才有意义。否则，重新计算它可能比从磁盘读取更快。'
- en: If you persist several RDDs and run out of memory, Spark will clear the least
    recently used out of memory (either discarding them or saving them to disk, depending
    on the chosen persistence level). RDDs also expose an `unpersist` method to explicitly
    tell Spark than an RDD is not needed any more.
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你持续持久化多个RDD并且内存不足，Spark将会清除最近最少使用的RDD（根据选择的持久化级别，可能是丢弃它们或将它们保存到磁盘）。RDD还提供了一个`unpersist`方法，可以显式地告诉Spark一个RDD不再需要。
- en: Persisting RDDs can have a drastic impact on performance. What and how to persist
    therefore becomes very important when tuning a Spark application. Finding the
    best persistence level generally requires some tinkering, benchmarking and experimentation.
    The Spark documentation provides guidelines on when to use which persistence level
    ([http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)),
    as well as general tips on tuning memory usage ([http://spark.apache.org/docs/latest/tuning.html](http://spark.apache.org/docs/latest/tuning.html)).
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化RDD可能会对性能产生重大影响。因此，在调整Spark应用程序时，什么和如何持久化变得非常重要。找到最佳持久化级别通常需要一些调整、基准测试和实验。Spark文档提供了何时使用哪种持久化级别的指南([http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence))，以及调整内存使用的通用技巧([http://spark.apache.org/docs/latest/tuning.html](http://spark.apache.org/docs/latest/tuning.html))。
- en: Importantly, the `persist` method does not force the evaluation of the RDD.
    It just notifies the Spark engine that, next time the values in this RDD are computed,
    they should be saved rather than discarded.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`persist`方法不会强制RDD进行评估。它只是通知Spark引擎，下次计算此RDD中的值时，应该保存而不是丢弃。
- en: Key-value RDDs
  id: totrans-1818
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键值RDD
- en: 'So far, we have only considered RDDs of Scala value types. RDDs of more complex
    data types support additional operations. Spark adds many operations for *key-value
    RDDs*: RDDs whose type parameter is a tuple `(K, V)`, for any type `K` and `V`.'
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了Scala值类型的RDD。支持更复杂数据类型的RDD支持额外的操作。Spark为*键值RDD*添加了许多操作：类型参数为元组`(K,
    V)`的RDD，对于任何类型`K`和`V`。
- en: 'Let''s go back to our sample email:'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的示例电子邮件：
- en: '[PRE451]'
  id: totrans-1821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE451]'
- en: 'Let''s persist the `words` RDD in memory to avoid having to re-read the `email`
    file from disk repeatedly:'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`words` RDD持久化到内存中，以避免反复从磁盘重新读取`email`文件：
- en: '[PRE452]'
  id: totrans-1823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE452]'
- en: 'To access key-value operations, we just need to apply a transformation to our
    RDD that creates key-value pairs. Let''s use the words as keys. For now, we will
    just use 1 for every value:'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问键值操作，我们只需要对我们的RDD应用一个转换，创建键值对。现在，我们将使用单词作为键。对于每个值，我们将使用1：
- en: '[PRE453]'
  id: totrans-1825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE453]'
- en: 'Key-value RDDs support several operations besides the core RDD operations.
    These are added through an implicit conversion, using the "pimp my library" pattern
    that we explored in [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and
    SQL through JDBC"), *Scala and SQL through JDBC*. These additional transformations
    fall into two broad categories: *by-key* transformations and *joins* between RDDs.'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: 键值RDD除了核心RDD操作外，还支持几个操作。这些操作通过隐式转换添加，使用我们在[第5章](part0040.xhtml#aid-164MG1 "第5章。通过JDBC使用Scala和SQL")中探讨的“pimp
    my library”模式，即*通过JDBC使用Scala和SQL*。这些额外的转换分为两大类：*按键*转换和RDD之间的*连接*。
- en: 'By-key transformations are operations that aggregate the values corresponding
    to the same key. For instance, we can count the number of times each word appears
    in our email using `reduceByKey`. This method takes all the values that belong
    to the same key and combines them using a user-supplied function:'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 按键转换是聚合相同键对应值的操作。例如，我们可以使用`reduceByKey`来计算每个单词在电子邮件中出现的次数。此方法接受属于同一键的所有值，并使用用户提供的函数将它们组合起来：
- en: '[PRE454]'
  id: totrans-1828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE454]'
- en: 'Note that `reduceByKey` requires (in general) shuffling the RDD, since not
    every occurrence of a given key will be in the same partition:'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`reduceByKey`通常需要将RDD进行洗牌，因为给定键的每个出现可能不在同一个分区：
- en: '[PRE455]'
  id: totrans-1830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE455]'
- en: 'Note that key-value RDDs are not like Scala Maps: the same key can occur multiple
    times, and they do not support *O(1)* lookup. A key-value RDD can be transformed
    to a Scala map using the `.collectAsMap` action:'
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，键值RDD与Scala Maps不同：相同的键可以出现多次，并且它们不支持*O(1)*查找。可以使用`.collectAsMap`操作将键值RDD转换成Scala
    Map：
- en: '[PRE456]'
  id: totrans-1832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE456]'
- en: This requires pulling the entire RDD onto the main Spark node. You therefore
    need to have enough memory on the main node to house the map. This is often the
    last stage in a pipeline that filters a large RDD to just the information that
    we need.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要将整个 RDD 拉到主 Spark 节点上。因此，您需要在主节点上拥有足够的内存来存放映射。这通常是管道中的最后一个阶段，用于过滤大型 RDD，只保留我们所需的信息。
- en: 'There are many by-key operations, which we describe in the table below. For
    the examples in the table, we assume that `rdd` is created as follows:'
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 下面表格中描述了许多按键操作。对于表格中的示例，我们假设 `rdd` 是以下方式创建的：
- en: '[PRE457]'
  id: totrans-1835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE457]'
- en: '| Transformation | Notes | **Example (assumes** `rdd` **is** `{ quick -> 5,
    brown -> 5, quick -> 5, dog -> 3 }`) |'
  id: totrans-1836
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 备注 | **示例（假设** `rdd` **是** `{ quick -> 5, brown -> 5, quick -> 5, dog
    -> 3 }` **）** |'
- en: '| `rdd.mapValues` | Apply an operation to the values. | `rdd.mapValues { _
    * 2 } // => { quick -> 10, brown -> 10, quick -> 10, dog ->6 }` |'
  id: totrans-1837
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.mapValues` | 对值应用一个操作。 | `rdd.mapValues { _ * 2 } // => { quick -> 10,
    brown -> 10, quick -> 10, dog ->6 }` |'
- en: '| `rdd.groupByKey` | Return a key-value RDD in which values corresponding to
    the same key are grouped into iterables. | `rdd.groupByKey // => { quick -> Iterable(5,
    5), brown -> Iterable(5), dog -> Iterable(3) }` |'
  id: totrans-1838
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.groupByKey` | 返回一个键值 RDD，其中对应相同键的值被分组到可迭代对象中。 | `rdd.groupByKey // =>
    { quick -> Iterable(5, 5), brown -> Iterable(5), dog -> Iterable(3) }` |'
- en: '| `rdd.reduceByKey(func)` | Return a key-value RDD in which values corresponding
    to the same key are combined using a user-supplied function. | `rdd.reduceByKey
    { _ + _ } // => { quick -> 10, brown -> 5, dog -> 3 }` |'
  id: totrans-1839
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.reduceByKey(func)` | 返回一个键值 RDD，其中对应相同键的值使用用户提供的函数进行组合。 | `rdd.reduceByKey
    { _ + _ } // => { quick -> 10, brown -> 5, dog -> 3 }` |'
- en: '| `rdd.keys` | Return an RDD of the keys. | `rdd.keys // => { quick, brown,
    quick, dog }` |'
  id: totrans-1840
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.keys` | 返回一个键的 RDD。 | `rdd.keys // => { quick, brown, quick, dog }`
    |'
- en: '| `rdd.values` | Return an RDD of the values. | `rdd.values // => { 5, 5, 5,
    3 }` |'
  id: totrans-1841
  prefs: []
  type: TYPE_TB
  zh: '| `rdd.values` | 返回一个值的 RDD。 | `rdd.values // => { 5, 5, 5, 3 }` |'
- en: 'The second category of operations on key-value RDDs involves joining different
    RDDs together by key. This is somewhat similar to SQL joins, where the keys are
    the column being joined on. Let''s load a spam email and apply the same transformations
    we applied to our ham email:'
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: 键值 RDD 上的第二种操作类型涉及通过键将不同的 RDD 连接在一起。这与 SQL 连接有些相似，其中键是要连接的列。让我们加载一封垃圾邮件，并应用我们对我们的正常邮件应用相同的转换：
- en: '[PRE458]'
  id: totrans-1843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE458]'
- en: 'Both `spamWordCounts` and `wordCounts` are key-value RDDs for which the keys
    correspond to unique words in the message, and the values are the number of times
    that word occurs. There will be some overlap in keys between `spamWordCounts`
    and `wordCounts`, since the emails will share many of the same words. Let''s do
    an *inner join* between those two RDDs to get the words that occur in both emails:'
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: '`spamWordCounts` 和 `wordCounts` 都是键值 RDD，其中键对应于消息中的唯一单词，值是该单词出现的次数。由于电子邮件将共享许多相同的单词，因此
    `spamWordCounts` 和 `wordCounts` 之间的键将存在一些重叠。让我们在这两个 RDD 之间进行一个 *内部连接*，以获取两个电子邮件中都出现的单词：'
- en: '[PRE459]'
  id: totrans-1845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE459]'
- en: The values in the RDD resulting from an inner join will be pairs. The first
    element in the pair is the value for that key in the first RDD, and the second
    element is the value for that key in the second RDD. Thus, the word *call* occurs
    three times in the legitimate email and once in the spam email.
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
  zh: 内部连接产生的 RDD 中的值将是成对出现的。成对中的第一个元素是第一个 RDD 中该键的值，第二个元素是第二个 RDD 中该键的值。因此，单词 *call*
    在合法电子邮件中出现了三次，在垃圾邮件中出现了两次。
- en: 'Spark supports all four join types. For instance, let''s perform a left join:'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持所有四种连接类型。例如，让我们执行一个左连接：
- en: '[PRE460]'
  id: totrans-1848
  prefs: []
  type: TYPE_PRE
  zh: '[PRE460]'
- en: 'Notice that the second element in our pair has type `Option[Int]`, to accommodate
    keys absent in `spamWordCounts`. The word *paper*, for instance, occurs twice
    in the legitimate email and never in the spam email. In this case, it is more
    useful to have zeros to indicate absence, rather than `None`. Replacing `None`
    with a default value is simple with `getOrElse`:'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们成对中的第二个元素具有 `Option[Int]` 类型，以适应 `spamWordCounts` 中缺失的键。例如，单词 *paper* 在合法电子邮件中出现了两次，在垃圾邮件中从未出现。在这种情况下，用零表示缺失比用
    `None` 更有用。使用 `getOrElse` 替换 `None` 为默认值很简单：
- en: '[PRE461]'
  id: totrans-1850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE461]'
- en: 'The table below lists the most common joins on key-value RDDs:'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了键值 RDD 上最常见的连接：
- en: '| **Transformation** | **Result (assuming** `rdd1` is `{ quick -> 1, brown
    -> 2, quick -> 3, dog -> 4 }` **and** `rdd2` **is** `{ quick -> 78, brown -> 79,
    fox -> 80 }`) |'
  id: totrans-1852
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **结果（假设** `rdd1` 是 `{ quick -> 1, brown -> 2, quick -> 3, dog ->
    4 }` **和** `rdd2` **是** `{ quick -> 78, brown -> 79, fox -> 80 }` **）** |'
- en: '| `rdd1.join(rdd2)` | `{ quick -> (1, 78), quick -> (3, 78), brown -> (2, 79)
    }` |'
  id: totrans-1853
  prefs: []
  type: TYPE_TB
  zh: '| `rdd1.join(rdd2)` | `{ quick -> (1, 78), quick -> (3, 78), brown -> (2, 79)
    }` |'
- en: '| `rdd1.leftOuterJoin(rdd2)` | `{ dog -> (4, None), quick -> (1, Some(78)),
    quick -> (3, Some(78)), brown -> (2, Some(79)) }` |'
  id: totrans-1854
  prefs: []
  type: TYPE_TB
  zh: '| `rdd1.leftOuterJoin(rdd2)` | `{ dog -> (4, None), quick -> (1, Some(78)),
    quick -> (3, Some(78)), brown -> (2, Some(79)) }` |'
- en: '| `rdd1.rightOuterJoin(rdd2)` | `{ quick -> (Some(1), 78), quick -> (Some(3),
    78), brown -> (Some(2), 79), fox -> (None, 80) }` |'
  id: totrans-1855
  prefs: []
  type: TYPE_TB
  zh: '| `rdd1.rightOuterJoin(rdd2)` | `{ quick -> (Some(1), 78), quick -> (Some(3),
    78), brown -> (Some(2), 79), fox -> (None, 80) }` |'
- en: '| `rdd1.fullOuterJoin(rdd2)` | `{ dog -> (Some(4), None), quick -> (Some(1),
    Some(78)), quick -> (Some(3), Some(78)), brown -> (Some(2), Some(79)), fox ->
    (None, Some(80)) }` |'
  id: totrans-1856
  prefs: []
  type: TYPE_TB
  zh: '| `rdd1.fullOuterJoin(rdd2)` | `{ dog -> (Some(4), None), quick -> (Some(1),
    Some(78)), quick -> (Some(3), Some(78)), brown -> (Some(2), Some(79)), fox ->
    (None, Some(80)) }` |'
- en: For a complete list of transformations, consult the API documentation for `PairRDDFunctions`,
    [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions).
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取完整的转换列表，请查阅 `PairRDDFunctions` 的 API 文档，[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)。
- en: Double RDDs
  id: totrans-1858
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双精度浮点 RDD
- en: 'In the previous section, we saw that Spark adds functionality to key-value
    RDDs through an implicit conversion. Similarly, Spark adds statistics functionality
    to RDDs of doubles. Let''s extract the word frequencies for the ham message, and
    convert the values from integers to doubles:'
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们看到了 Spark 通过隐式转换向键值 RDD 添加了功能。同样，Spark 向 doubles 的 RDD 添加了统计功能。让我们提取火腿消息的单词频率，并将值从整数转换为双精度浮点数：
- en: '[PRE462]'
  id: totrans-1860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE462]'
- en: 'We can then get summary statistics using the `.stats` action:'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `.stats` 动作来获取摘要统计信息：
- en: '[PRE463]'
  id: totrans-1862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE463]'
- en: 'Thus, the most common word appears 72 times. We can also use the `.histogram`
    action to get an idea of the distribution of values:'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最常见的单词出现了 72 次。我们还可以使用 `.histogram` 动作来了解值的分布：
- en: '[PRE464]'
  id: totrans-1864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE464]'
- en: 'The `.histogram` method returns a pair of arrays. The first array indicates
    the bounds of the histogram bins, and the second is the count of elements in that
    bin. Thus, there are `391` words that appear less than `15.2` times. The distribution
    of words is very skewed, such that a histogram with regular-sized bin is not really
    appropriate. We can, instead, pass in custom bins by passing an array of bin edges
    to the `histogram` method. For instance, we might distribute the bins logarithmically:'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: '`.histogram` 方法返回一个数组的对。第一个数组表示直方图桶的界限，第二个数组表示该桶中元素的数量。因此，有 `391` 个单词出现次数少于
    `15.2` 次。单词的分布非常偏斜，以至于使用常规大小的桶并不合适。我们可以通过传递一个桶边界的数组到 `histogram` 方法来传递自定义的桶。例如，我们可能以对数方式分布桶：'
- en: '[PRE465]'
  id: totrans-1866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE465]'
- en: Building and running standalone programs
  id: totrans-1867
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和运行独立程序
- en: So far, we have interacted exclusively with Spark through the Spark shell. In
    the section that follows, we will build a standalone application and launch a
    Spark program either locally or on an EC2 cluster.
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要通过 Spark shell 与 Spark 进行交互。在接下来的部分中，我们将构建一个独立的应用程序，并在本地或 EC2 集群上启动
    Spark 程序。
- en: Running Spark applications locally
  id: totrans-1869
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地运行 Spark 应用程序
- en: The first step is to write the `build.sbt` file, as you would if you were running
    a standard Scala script. The Spark binary that we downloaded needs to be run against
    Scala 2.10 (You need to compile Spark from source to run against Scala 2.11\.
    This is not difficult to do, just follow the instructions on [http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211](http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211)).
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是编写 `build.sbt` 文件，就像运行标准的 Scala 脚本一样。我们下载的 Spark 二进制文件需要针对 Scala 2.10 运行（你需要从源代码编译
    Spark 以运行针对 Scala 2.11 的版本。这并不困难，只需遵循 [http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211](http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211)
    上的说明即可）。
- en: '[PRE466]'
  id: totrans-1871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE466]'
- en: We then run `sbt package` to compile and build a jar of our program. The jar
    will be built in `target/scala-2.10/`, and called `spam_mi_2.10-0.1-SNAPSHOT.jar`.
    You can try this with the example code provided for this chapter.
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们运行 `sbt package` 来编译和构建程序的 jar 包。jar 包将在 `target/scala-2.10/` 目录下构建，并命名为
    `spam_mi_2.10-0.1-SNAPSHOT.jar`。你可以使用本章提供的示例代码尝试这个操作。
- en: 'We can then run the jar locally using the `spark-submit` shell script, available
    in the `bin/` folder in the Spark installation directory:'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用位于 Spark 安装目录 `bin/` 文件夹中的 `spark-submit` 脚本在本地上运行 jar 包：
- en: '[PRE467]'
  id: totrans-1874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE467]'
- en: The resources allocated to Spark can be controlled by passing arguments to `spark-submit`.
    Use `spark-submit --help` to see the full list of arguments.
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过传递参数到 `spark-submit` 来控制分配给 Spark 的资源。使用 `spark-submit --help` 来查看完整的参数列表。
- en: 'If the Spark programs has dependencies (for instance, on other Maven packages),
    it is easiest to bundle them into the application jar using the *SBT* *assembly*
    plugin. Let''s imagine that our application depends on breeze-viz. The `build.sbt`
    file now looks like:'
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Spark 程序有依赖项（例如，对其他 Maven 包的依赖），最简单的方法是使用 *SBT* *打包* 插件将它们捆绑到应用程序 jar 中。让我们假设我们的应用程序依赖于
    breeze-viz。现在的 `build.sbt` 文件如下所示：
- en: '[PRE468]'
  id: totrans-1877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE468]'
- en: 'SBT assembly is an SBT plugin that builds *fat* jars: jars that contain not
    only the program itself, but all the dependencies for the program.'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: SBT 打包是一个 SBT 插件，它构建 *胖* jar：包含程序本身以及程序的所有依赖项的 jar。
- en: 'Note that we marked Spark as "provided" in the list of dependencies, which
    means that Spark itself will not be included in the jar (it is provided by the
    Spark environment anyway). To include the SBT assembly plugin, create a file called
    `assembly.sbt` in the `project/` directory, with the following line:'
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在依赖列表中将 Spark 标记为“提供”，这意味着 Spark 本身将不会包含在 jar 文件中（无论如何它都是由 Spark 环境提供的）。要包含
    SBT 打包插件，请在 `project/` 目录下创建一个名为 `assembly.sbt` 的文件，并包含以下行：
- en: '[PRE469]'
  id: totrans-1880
  prefs: []
  type: TYPE_PRE
  zh: '[PRE469]'
- en: You will need to re-start SBT for the changes to take effect. You can then create
    the assembly jar using the `assembly` command in SBT. This will create a jar called
    `spam_mi-assembly-0.1-SNAPSHOT.jar` in the `target/scala-2.10` directory. You
    can run this jar using `spark-submit`.
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要重新启动 SBT 以使更改生效。然后您可以使用 SBT 中的 `assembly` 命令创建打包 jar。这将在 `target/scala-2.10`
    目录中创建一个名为 `spam_mi-assembly-0.1-SNAPSHOT.jar` 的 jar。您可以使用 `spark-submit` 运行此 jar。
- en: Reducing logging output and Spark configuration
  id: totrans-1882
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少日志输出和 Spark 配置
- en: 'Spark is, by default, very verbose. The default log-level is set to `INFO`.
    To avoid missing important messages, it is useful to change the log settings to
    `WARN`. To change the default log level system-wide, go into the `conf` directory
    in the directory in which you installed Spark. You should find a file called `log4j.properties.template`.
    Rename this file to `log4j.properties` and look for the following line:'
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 默认情况下非常详细。默认日志级别设置为 `INFO`。为了避免错过重要信息，将日志设置更改为 `WARN` 是有用的。要全局更改默认日志级别，请进入您安装
    Spark 的目录中的 `conf` 目录。您应该会找到一个名为 `log4j.properties.template` 的文件。将此文件重命名为 `log4j.properties`
    并查找以下行：
- en: '[PRE470]'
  id: totrans-1884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE470]'
- en: 'Change this line to:'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: 将此行更改为：
- en: '[PRE471]'
  id: totrans-1886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE471]'
- en: There are several other configuration files in that directory that you can use
    to alter Spark's default behavior. For a full list of configuration options, head
    over to [http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html).
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
  zh: 在该目录中还有其他几个配置文件，您可以使用它们来更改 Spark 的默认行为。有关配置选项的完整列表，请访问 [http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html)。
- en: Running Spark applications on EC2
  id: totrans-1888
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 EC2 上运行 Spark 应用程序
- en: Running Spark locally is useful for testing, but the whole point of using a
    distributed framework is to run programs harnessing the power of several different
    computers. We can set Spark up on any set of computers that can communicate with
    each other using HTTP. In general, we also need to set up a distributed file system
    like HDFS, so that we can share input files across the cluster. For the purpose
    of this example, we will set Spark up on an Amazon EC2 cluster.
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行 Spark 对于测试很有用，但使用分布式框架的全部意义在于运行能够利用多台不同计算机能力的程序。我们可以在任何能够通过 HTTP 互相通信的计算机集上设置
    Spark。通常，我们还需要设置一个分布式文件系统，如 HDFS，这样我们就可以在集群间共享输入文件。为了本例的目的，我们将在一个 Amazon EC2 集群上设置
    Spark。
- en: 'Spark comes with a shell script, `ec2/spark-ec2`, for setting up an EC2 cluster
    and installing Spark. It will also install HDFS. You will need an account with
    Amazon Web Services (AWS) to follow these examples ([https://aws.amazon.com](https://aws.amazon.com)).
    You will need the AWS access key and secret key, which you can access through
    the **Account** / **Security Credentials** / **Access Credentials** menu in the
    AWS web console. You need to make these available to the `spark-ec2` script through
    environment variables. Inject them into your current session as follows:'
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 附带一个名为 `ec2/spark-ec2` 的 shell 脚本，用于设置 EC2 集群并安装 Spark。它还会安装 HDFS。您需要
    Amazon Web Services (AWS) 的账户才能遵循这些示例 ([https://aws.amazon.com](https://aws.amazon.com))。您需要
    AWS 访问密钥和秘密密钥，您可以通过 AWS 网络控制台的 **账户** / **安全凭证** / **访问凭证** 菜单访问它们。您需要通过环境变量将这些密钥提供给
    `spark-ec2` 脚本。如下注入到您的当前会话中：
- en: '[PRE472]'
  id: totrans-1891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE472]'
- en: You can also write these lines into the configuration script for your shell
    (your `.bashrc` file, or equivalent), to avoid having to re-enter them every time
    you run the `setup-ec2` script. We discussed environment variables in [Chapter
    6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick – A Functional Interface for SQL"),
    *Slick – A Functional Interface for SQL*.
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以将这些行写入您的shell配置脚本（例如`.bashrc`文件或等效文件），以避免每次运行`setup-ec2`脚本时都需要重新输入。我们已在[第6章](part0051.xhtml#aid-1GKCM2
    "第6章。Slick – SQL的函数式接口")中讨论了环境变量，*Slick – SQL的函数式接口*。
- en: 'You will also need to create a key pair by clicking on **Key Pairs** in the
    EC2 web console, creating a new key pair and downloading the certificate file.
    I will assume you named the key pair `test_ec2` and the certificate file `test_ec2.pem`.
    Make sure that the key pair is created in the *N*. Virginia region (by choosing
    the correct region in the upper right corner of the EC2 Management console), to
    avoid having to specify the region explicitly in the rest of this chapter. You
    will need to set access permissions on the certificate file to user-readable only:'
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要通过在EC2网页控制台中点击**密钥对**来创建一个密钥对，创建一个新的密钥对并下载证书文件。我将假设您将密钥对命名为`test_ec2`，证书文件为`test_ec2.pem`。请确保密钥对是在*N*.
    Virginia区域创建的（通过在EC2管理控制台右上角选择正确的区域），以避免在本章的其余部分中显式指定区域。您需要将证书文件的访问权限设置为仅用户可读：
- en: '[PRE473]'
  id: totrans-1894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE473]'
- en: 'We are now ready to launch the cluster. Navigate to the `ec2` directory and
    run:'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动集群了。导航到`ec2`目录并运行：
- en: '[PRE474]'
  id: totrans-1896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE474]'
- en: This will create a cluster called `test_cluster` with a master and two slaves.
    The number of slaves is set through the `-s` command line argument. The cluster
    will take a while to start up, but you can verify that the instances are launching
    correctly by looking at the **Instances** window in the EC2 Management Console.
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`test_cluster`的集群，包含一个主节点和两个从节点。从节点的数量通过`-s`命令行参数设置。集群启动需要一段时间，但您可以通过查看EC2管理控制台中的**实例**窗口来验证实例是否正在正确启动。
- en: The setup script supports many options for customizing the type of instances,
    the number of hard drives and so on. You can explore these options by passing
    the `--help` command line option to `spark-ec2`.
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: 设置脚本支持许多选项，用于自定义实例类型、硬盘数量等。您可以通过将`--help`命令行选项传递给`spark-ec2`来探索这些选项。
- en: 'The life cycle of the cluster can be controlled by passing different commands
    to the `spark-ec2` script, such as:'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向`spark-ec2`脚本传递不同的命令，可以控制集群的生命周期，例如：
- en: '[PRE475]'
  id: totrans-1900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE475]'
- en: For more detail on using Spark on EC2, consult the official documentation at
    [http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications](http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications).
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在EC2上使用Spark的更多详细信息，请参阅官方文档：[http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications](http://spark.apache.org/docs/latest/ec2-scripts.html#running-applications)。
- en: Spam filtering
  id: totrans-1902
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤
- en: 'Let''s put all we''ve learned to good use and do some data exploration for
    our spam filter. We will use the Ling-Spam email dataset: [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html).
    The dataset contains 2412 ham emails and 481 spam emails, all of which were received
    by a mailing list on linguistics. We will extract the words that are most informative
    of whether an email is spam or ham.'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所学的一切用于实际，并为我们的垃圾邮件过滤器进行一些数据探索。我们将使用Ling-Spam电子邮件数据集：[http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html)。该数据集包含2412封正常邮件和481封垃圾邮件，所有邮件都是通过语言学邮件列表收到的。我们将提取最能说明邮件是垃圾邮件还是正常邮件的单词。
- en: 'The first steps in any natural language processing workflow are to remove stop
    words and lemmatization. Removing stop words involves filtering very common words
    such as *the*, *this* and so on. Lemmatization involves replacing different forms
    of the same word with a canonical form: both *colors* and *color* would be mapped
    to *color*, and *organize*, *organizing* and *organizes* would be mapped to *organize*.
    Removing stop words and lemmatization is very challenging, and beyond the scope
    of this book (if you do need to remove stop words and lemmatize a dataset, your
    go-to tool should be the Stanford NLP toolkit: [http://nlp.stanford.edu/software/corenlp.shtml](http://nlp.stanford.edu/software/corenlp.shtml)).
    Fortunately, the Ling-Spam e-mail dataset has been cleaned and lemmatized already
    (which is why the text in the emails looks strange).'
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
  zh: 任何自然语言处理工作流程的第一步是去除停用词和词形还原。去除停用词包括过滤掉非常常见的词，如*the*、*this*等等。词形还原涉及将同一词的不同形式替换为规范形式：*colors*和*color*都会映射到*color*，而*organize*、*organizing*和*organizes*都会映射到*organize*。去除停用词和词形还原非常具有挑战性，超出了本书的范围（如果你需要去除停用词并对数据集进行词形还原，你应该使用斯坦福NLP工具包：[http://nlp.stanford.edu/software/corenlp.shtml](http://nlp.stanford.edu/software/corenlp.shtml)）。幸运的是，Ling-Spam电子邮件数据集已经被清理和词形还原了（这就是为什么电子邮件中的文本看起来很奇怪）。
- en: 'When we do build the spam filter, we will use the presence of a particular
    word in an email as the feature for our model. We will use a *bag-of-words* approach:
    we consider which words appear in an email, but not the word order.'
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建垃圾邮件过滤器时，我们将使用电子邮件中特定词的存在作为我们模型的特征。我们将使用*词袋模型*方法：我们考虑电子邮件中哪些词出现，但不考虑词序。
- en: Intuitively, some words will be more important than others when deciding whether
    an email is spam. For instance, an email that contains *language* is likely to
    be ham, since the mailing list was for linguistics discussions, and *language*
    is a word unlikely to be used by spammers. Conversely, words which are common
    to both message types, for instance *hello*, are unlikely to be much use.
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，在决定一封电子邮件是否为垃圾邮件时，一些词会比其他词更重要。例如，包含*language*的电子邮件很可能是ham，因为邮件列表是用于语言学讨论的，而*language*是一个不太可能被垃圾邮件发送者使用的词。相反，那些两种消息类型都常见的词，例如*hello*，不太可能有很大的作用。
- en: 'One way of quantifying the importance of a word in determining whether a message
    is spam is through the **Mutual Information** (**MI**). The mutual information
    is the gain in information about whether a message is ham or spam if we know that
    it contains a particular word. For instance, the presence of *language* in a particular
    email is very informative as to whether that email is spam or ham. Similarly,
    the presence of the word *dollar* is informative since it appears often in spam
    messages and only infrequently in ham messages. By contrast, the presence of the
    word *morning* is uninformative, since it is approximately equally common in both
    spam and ham messages. The formula for the mutual information between the presence
    of a particular word in an email, and whether that email is spam or ham is:'
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
  zh: 量化一个词在确定一条消息是否为垃圾邮件中的重要性的一种方法是通过**互信息**（**MI**）。互信息是在我们知道一条消息包含特定词的情况下，关于该消息是ham还是spam的信息增益。例如，特定电子邮件中存在*language*这一事实对于判断该电子邮件是垃圾邮件还是ham非常有信息量。同样，*dollar*这个词的存在也是有信息量的，因为它经常出现在垃圾邮件中，而很少出现在ham邮件中。相比之下，*morning*这个词的存在是无信息量的，因为它在垃圾邮件和ham邮件中出现的频率大致相同。电子邮件中特定词的存在与该电子邮件是垃圾邮件还是ham之间的互信息公式是：
- en: '![Spam filtering](img/image01196.jpeg)'
  id: totrans-1908
  prefs: []
  type: TYPE_IMG
  zh: '![垃圾邮件过滤](img/image01196.jpeg)'
- en: where ![Spam filtering](img/image01197.jpeg) is the joint probability of an
    email containing a particular word and being of that class (either ham or spam),
    ![Spam filtering](img/image01198.jpeg) is the probability that a particular word
    is present in an email, and ![Spam filtering](img/image01199.jpeg) is the probability
    that any email is of that class. The MI is commonly used in decision trees.
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![垃圾邮件过滤](img/image01197.jpeg)是电子邮件包含特定词和属于该类别（ham或spam）的联合概率，![垃圾邮件过滤](img/image01198.jpeg)是特定词出现在电子邮件中的概率，![垃圾邮件过滤](img/image01199.jpeg)是任何电子邮件属于该类别的概率。互信息通常用于决策树。
- en: Note
  id: totrans-1910
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The derivation of the expression for the mutual information is beyond the scope
    of this book. The interested reader is directed to *David MacKay's* excellent
    *Information Theory, Inference, and Learning Algorithms*, especially the chapter
    *Dependent Random Variables*.
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息表达式的推导超出了本书的范围。感兴趣的读者可以参考*David MacKay*的杰出著作*信息论、推理和学习算法*，特别是*依赖随机变量*这一章。
- en: A key component of our MI calculation is evaluating the probability that a word
    occurs in spam or ham messages. The best approximation to this probability, given
    our data set, is the fraction of messages a word appears in. Thus, for instance,
    if *language* appears in 40% of messages, we will assume that the probability
    ![Spam filtering](img/image01200.jpeg) of language being present in any message
    is 0.4\. Similarly, if 40% of the messages are ham, and *language* appears in
    50% of those, we will assume that the probability of language being present in
    an email, and that email being ham is ![Spam filtering](img/image01201.jpeg).
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算互信息的关键组成部分是评估一个单词出现在垃圾邮件或垃圾邮件中的概率。给定我们的数据集，这个概率的最佳近似值是该单词出现的消息比例。因此，例如，如果
    *language* 出现在 40% 的消息中，我们将假设该语言出现在任何消息中的概率 ![垃圾邮件过滤](img/image01200.jpeg) 为 0.4。同样，如果
    40% 的消息是垃圾邮件，而 *language* 出现在这些垃圾邮件的 50% 中，我们将假设该语言出现在电子邮件中的概率，以及该电子邮件是垃圾邮件的概率
    ![垃圾邮件过滤](img/image01201.jpeg)。
- en: Let's write a `wordFractionInFiles` function to calculate the fraction of messages
    in which each word appears, for all the words in a given corpus. Our function
    will take, as argument, a path with a shell wildcard identifying a set of files,
    such as `ham/*`, and it will return a key-value RDD, where the keys are words
    and the values are the probability that that word occurs in any of those files.
    We will put the function in an object called `MutualInformation`.
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个 `wordFractionInFiles` 函数来计算给定语料库中每个单词出现的消息比例。我们的函数将接受一个参数，即一个路径，该路径使用
    shell 通配符标识一组文件，例如 `ham/*`，并且它将返回一个键值 RDD，其中键是单词，值是该单词出现在这些文件中的概率。我们将该函数放入一个名为
    `MutualInformation` 的对象中。
- en: 'We first give the entire code listing for this function. Don''t worry if this
    doesn''t all make sense straight-away: we explain the tricky parts in more detail
    just after the code. You may find it useful to type some of these commands in
    the shell, replacing `fileGlob` with, for instance `"ham/*"`:'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先给出该函数的整个代码列表。如果一开始看不懂没关系：我们将在代码之后详细解释这些难点。你可能发现将这些命令在 shell 中输入是有用的，例如将
    `fileGlob` 替换为 `"ham/*"`：
- en: '[PRE476]'
  id: totrans-1915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE476]'
- en: 'Let''s play with this function in the Spark shell. To be able to access this
    function from the shell, we need to create a jar with the `MutualInformation`
    object. Write a `build.sbt` file similar to the one presented in the previous
    section and package the code into a jar using `sbt package`. Then, open a Spark
    shell with:'
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Spark shell 中玩这个函数。为了能够从 shell 中访问此函数，我们需要创建一个包含 `MutualInformation` 对象的
    jar。编写一个类似于上一节中展示的 `build.sbt` 文件，并使用 `sbt package` 将代码打包成 jar。然后，使用以下命令打开 Spark
    shell：
- en: '[PRE477]'
  id: totrans-1917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE477]'
- en: 'This will open a Spark shell with our newly created jar on the classpath. Let''s
    run our `wordFractionInFiles` method on the `ham` emails:'
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在类路径上打开一个带有我们新创建的 jar 的 Spark shell。让我们在 `ham` 邮件上运行我们的 `wordFractionInFiles`
    方法：
- en: '[PRE478]'
  id: totrans-1919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE478]'
- en: 'Let''s get a snapshot of the `fractions` RDD:'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取 `fractions` RDD 的快照：
- en: '[PRE479]'
  id: totrans-1921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE479]'
- en: 'It would be nice to see the words that come up most often in ham messages.
    We can use the `.takeOrdered` action to take the top values of an RDD, with a
    custom ordering. `.takeOrdered` expects, as its second argument, an instance of
    the type class `Ordering[T]`, where `T` is the type parameter of our RDD: `(String,
    Double)` in this case. `Ordering[T]` is a trait with a single `compare(a:T, b:T)`
    method describing how to compare `a` and `b`. The easiest way of creating an `Ordering[T]`
    is through the companion object''s `by` method, which defines a key by which to
    compare the elements of our RDD.'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 很想看到在垃圾邮件中出现频率最高的单词。我们可以使用 `.takeOrdered` 动作来获取 RDD 的顶部值，并使用自定义排序。`.takeOrdered`
    的第二个参数期望是一个类型类 `Ordering[T]` 的实例，其中 `T` 是我们 RDD 的类型参数：在这种情况下是 `(String, Double)`。`Ordering[T]`
    是一个具有单个 `compare(a:T, b:T)` 方法的特质，它描述了如何比较 `a` 和 `b`。创建 `Ordering[T]` 的最简单方法是通过伴随对象的
    `by` 方法，该方法定义了一个用于比较 RDD 元素的关键字。
- en: 'We want to order the elements in our key-value RDD by the value and, since
    we want the most common words, rather than the least, we need to reverse that
    ordering:'
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望按值对我们的键值 RDD 进行排序，并且由于我们想要最常见的单词，而不是最不常见的，我们需要反转这种排序：
- en: '[PRE480]'
  id: totrans-1924
  prefs: []
  type: TYPE_PRE
  zh: '[PRE480]'
- en: Unsurprisingly, `language` is present in 67% of ham emails, `university` in
    60% of ham emails and so on. A similar investigation on spam messages reveals
    that the exclamation mark character *!* is present in 83% of spam emails, *our*
    is present in 61% and *free* in 57%.
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，`language` 出现在 67% 的垃圾邮件中，`university` 出现在 60% 的垃圾邮件中，等等。对垃圾邮件的类似调查显示感叹号字符
    *!* 出现在 83% 的垃圾邮件中，*our* 出现在 61%，*free* 出现在 57%。
- en: We are now in a position to start writing the body of our application to calculate
    the mutual information between each word and whether a message is spam or ham.
    We will put the body of the code in the `MutualInformation` object, which already
    contains the `wordFractionInFiles` method.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始编写应用程序的主体，以计算每个单词与消息是否为垃圾邮件或正常邮件之间的互信息。我们将代码的主体放入`MutualInformation`对象中，该对象已经包含了`wordFractionInFiles`方法。
- en: 'The first step is to create a Spark context:'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个Spark上下文：
- en: '[PRE481]'
  id: totrans-1928
  prefs: []
  type: TYPE_PRE
  zh: '[PRE481]'
- en: Note that we did not need to do this when we were using the Spark shell because
    the shell comes with a pre-built context bound to the variable `sc`.
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们使用Spark shell时，我们不需要这样做，因为shell自带一个预构建的上下文绑定到变量`sc`。
- en: 'We can now calculate the conditional probabilities of a message containing
    a particular word given that it is *spam*, ![Spam filtering](img/image01202.jpeg).
    This is just the fraction of messages containing that word in the *spam* corpus.
    This, in turn, lets us infer the joint probability of a message containing a certain
    word and being *spam* ![Spam filtering](img/image01203.jpeg). We will do this
    for all four combinations of classes: whether any given word is present or absent
    in a message, and whether that message is spam or ham:'
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算在给定消息是*垃圾邮件*的情况下，包含特定单词的条件概率![垃圾邮件过滤](img/image01202.jpeg)。这仅仅是包含该单词的*垃圾邮件*语料库中的消息比例。这反过来又让我们可以推断包含特定单词且为*垃圾邮件*的联合概率![垃圾邮件过滤](img/image01203.jpeg)。我们将对所有四个类别的组合进行此操作：任何给定单词是否存在于消息中，以及该消息是否为垃圾邮件或正常邮件：
- en: '[PRE482]'
  id: totrans-1931
  prefs: []
  type: TYPE_PRE
  zh: '[PRE482]'
- en: 'We will re-use these RDDs in several places in the calculation, so let''s tell
    Spark to keep them in memory to avoid having to re-calculate them:'
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在计算中的几个地方重用这些RDD，所以让我们告诉Spark将它们保存在内存中，以避免需要重新计算：
- en: '[PRE483]'
  id: totrans-1933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE483]'
- en: We now need to calculate the probabilities of words being present, ![Spam filtering](img/image01198.jpeg).
    This is just the sum of `pPresentAndSpam` and `pPresentAndHam`, for each word.
    The tricky part is that not all words are present in both the ham and spam messages.
    We must therefore do a full outer join of those RDDs. This will give an RDD mapping
    each word to a pair of `Option[Double]` values. For words absent in either the
    ham or spam messages, we must use a default value. A sensible default is ![Spam
    filtering](img/image01204.jpeg) for spam messages (a more rigorous approach would
    be to use *additive smoothing*). This implies that the word would appear once
    if the corpus was twice as large.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要计算单词存在的概率![垃圾邮件过滤](img/image01198.jpeg)。这仅仅是`pPresentAndSpam`和`pPresentAndHam`的和，对于每个单词。棘手的部分是并非所有单词都存在于正常邮件和垃圾邮件中。因此，我们必须对这些RDD执行全外连接。这将给出一个RDD，将每个单词映射到一个`Option[Double]`值的对。对于在正常邮件或垃圾邮件中不存在的单词，我们必须使用默认值。一个合理的默认值是![垃圾邮件过滤](img/image01204.jpeg)对于垃圾邮件（更严格的方法是使用*加性平滑*）。这意味着如果语料库是两倍大，该单词将出现一次。
- en: '[PRE484]'
  id: totrans-1935
  prefs: []
  type: TYPE_PRE
  zh: '[PRE484]'
- en: Note that we could also have chosen 0 as the default value. This complicates
    the information gain calculation somewhat, since we cannot just take the log of
    a zero value, and it seems unlikely that a particular word has exactly zero probability
    of occurring in an email.
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以选择0作为默认值。这会使信息增益的计算变得有些复杂，因为我们不能对零值取对数，并且似乎不太可能一个特定的单词在电子邮件中出现的概率恰好为零。
- en: 'We can now construct an RDD mapping words to ![Spam filtering](img/image01198.jpeg),
    the probability that a word exists in either a spam or a ham message:'
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建一个RDD，将单词映射到![垃圾邮件过滤](img/image01198.jpeg)，即单词存在于垃圾邮件或正常邮件中的概率：
- en: '[PRE485]'
  id: totrans-1938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE485]'
- en: We now have all the RDDs that we need to calculate the mutual information between
    the presence of a word in a message and whether it is ham or spam. We need to
    bring them all together using the equation for the mutual information outlined
    earlier.
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有所有需要的RDD来计算单词在消息中存在与否与消息是否为正常邮件或垃圾邮件之间的互信息。我们需要使用前面概述的互信息方程将它们全部结合起来。
- en: 'We will start by defining a helper method that, given an RDD of joint probabilities
    *P(X, Y)* and marginal probabilities *P(X)* and *P(Y)*, calculates ![Spam filtering](img/image01205.jpeg).
    Here, *P(X)* could, for instance, be the probability of a word being present in
    a message ![Spam filtering](img/image01198.jpeg) and *P(Y)* would be the probability
    that that message is *spam*, ![Spam filtering](img/image01206.jpeg):'
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义一个辅助方法，该方法给定联合概率RDD *P(X, Y)* 和边缘概率 *P(X)* 和 *P(Y)*，计算![垃圾邮件过滤](img/image01205.jpeg)。在这里，*P(X)*
    例如可以是单词存在于消息中的概率![垃圾邮件过滤](img/image01198.jpeg)，而 *P(Y)* 将会是该消息是*垃圾邮件*的概率![垃圾邮件过滤](img/image01206.jpeg)：
- en: '[PRE486]'
  id: totrans-1941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE486]'
- en: 'We can use our function to calculate the four terms in the mutual information
    sum:'
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们的函数来计算互信息总和中的四个项：
- en: '[PRE487]'
  id: totrans-1943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE487]'
- en: 'Finally, we just need to sum those four terms together:'
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需要将这些四个项相加：
- en: '[PRE488]'
  id: totrans-1945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE488]'
- en: 'The RDD `mutualInformation` is a key-value RDD mapping each word to a measure
    of how informative the presence of that word is in discerning whether a message
    is spam or ham. Let''s print out the twenty words that are most informative of
    whether a message is ham or spam:'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
  zh: RDD `mutualInformation` 是一个键值 RDD，将每个词映射到衡量该词在区分邮件是否为垃圾邮件或正常邮件时的信息量的度量。让我们打印出最能够表明邮件是否为正常邮件或垃圾邮件的二十个词：
- en: '[PRE489]'
  id: totrans-1947
  prefs: []
  type: TYPE_PRE
  zh: '[PRE489]'
- en: 'Let''s run this using `spark-submit`:'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `spark-submit` 来运行这个示例：
- en: '[PRE490]'
  id: totrans-1949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE490]'
- en: Thus, we find that the presence of words like `language` or `free` or `!` carry
    the most information, because they are almost exclusively present in either just
    spam messages or just ham messages. A very simple classification algorithm could
    just take the top 10 (by mutual information) spam words, and the top 10 ham words
    and see whether a message contains more spam words or ham words. We will explore
    machine learning algorithms for classification in more depth in [Chapter 12](part0117.xhtml#aid-3FIHQ2
    "Chapter 12. Distributed Machine Learning with MLlib"), *Distributed Machine Learning
    with MLlib*.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们发现像 `language` 或 `free` 或 `!` 这样的词包含最多的信息，因为它们几乎只存在于垃圾邮件或正常邮件中。一个非常简单的分类算法只需取前
    10 个（按互信息排序）垃圾邮件词和前 10 个正常邮件词，然后查看一条消息是否包含更多垃圾邮件词或正常邮件词。我们将在第 12 章[分布式机器学习与 MLlib](part0117.xhtml#aid-3FIHQ2
    "第 12 章。使用 MLlib 的分布式机器学习")中更深入地探讨用于分类的机器学习算法。
- en: Lifting the hood
  id: totrans-1951
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开盖子
- en: In the last section of this chapter, we will discuss, very briefly, how Spark
    works internally. For a more detailed discussion, see the *References* section
    at the end of the chapter.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将非常简要地讨论 Spark 的内部工作原理。对于更详细的讨论，请参阅本章末尾的 *参考文献* 部分。
- en: When you open a Spark context, either explicitly or by launching the Spark shell,
    Spark starts a web UI with details of how the current task and past tasks have
    executed. Let's see this in action for the example mutual information program
    we wrote in the last section. To prevent the context from shutting down when the
    program completes, you can insert a call to `readLine` as the last line of the
    `main` method (after the call to `takeOrdered`). This expects input from the user,
    and will therefore pause program execution until you press *enter*.
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开一个 Spark 上下文，无论是显式地还是通过启动 Spark shell，Spark 会启动一个包含当前任务和过去任务执行详情的 Web UI。让我们看看我们在上一节中编写的示例互信息程序的实际操作。为了防止程序完成后上下文关闭，你可以在
    `main` 方法的最后（在调用 `takeOrdered` 之后）插入一个对 `readLine` 的调用。这会期望用户输入，因此程序执行将暂停，直到你按下
    *enter* 键。
- en: To access the UI, point your browser to `127.0.0.1:4040`. If you have other
    instances of the Spark shell running, the port may be `4041`, or `4042` and so
    on.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 UI，将你的浏览器指向 `127.0.0.1:4040`。如果你有其他正在运行的 Spark shell 实例，端口可能是 `4041`、`4042`
    等等。
- en: '![Lifting the hood](img/image01207.jpeg)'
  id: totrans-1955
  prefs: []
  type: TYPE_IMG
  zh: '![揭开盖子](img/image01207.jpeg)'
- en: 'The first page of the UI tells us that our application contains three *jobs*.
    A job occurs as the result of an action. There are, indeed, three actions in our
    application: the first two are called within the `wordFractionInFiles` function:'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: UI 的第一页告诉我们，我们的应用程序包含三个 *作业*。作业是动作的结果。实际上，我们的应用程序中确实有三个动作：前两个是在 `wordFractionInFiles`
    函数中调用的：
- en: '[PRE491]'
  id: totrans-1957
  prefs: []
  type: TYPE_PRE
  zh: '[PRE491]'
- en: The last job results from the call to `takeOrdered`, which forces the execution
    of the entire pipeline of RDD transformations that calculate the mutual information.
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个作业是由对 `takeOrdered` 的调用产生的，它强制执行计算互信息的整个 RDD 转换管道。
- en: 'The web UI lets us delve deeper into each job. Click on the `takeOrdered` job
    in the job table. You will get taken to a page that describes the job in more
    detail:'
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: Web UI 允许我们深入了解每个作业。点击作业表中的 `takeOrdered` 作业。你将被带到一页，其中更详细地描述了该作业：
- en: '![Lifting the hood](img/image01208.jpeg)'
  id: totrans-1960
  prefs: []
  type: TYPE_IMG
  zh: '![揭开盖子](img/image01208.jpeg)'
- en: Of particular interest is the **DAG visualization** entry. This is a graph of
    the execution plan to fulfill the action, and provides a glimpse of the inner
    workings of Spark.
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是 **DAG 可视化** 项。这是一个执行计划的图，用于满足动作，并提供了对 Spark 内部工作原理的洞察。
- en: 'When you define a job by calling an action on an RDD, Spark looks at the RDD''s
    lineage and constructs a graph mapping the dependencies: each RDD in the lineage
    is represented by a node, with directed edges going from this RDD''s parent to
    itself. This type of graph is called a **directed** **acyclic graph** (**DAG**),
    and is a data structure useful for dependency resolution. Let''s explore the DAG
    for the `takeOrdered` job in our program using the web UI. The graph is quite
    complex, and it is therefore easy to get lost, so here is a simplified reproduction
    that only lists the RDDs bound to variable names in the program.'
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: 当你通过在RDD上调用一个动作来定义一个作业时，Spark会查看RDD的 lineage 并构建一个映射依赖关系的图： lineage 中的每个RDD都由一个节点表示，从该RDD的父节点到自身的有向边。这种图称为**有向无环图**（DAG），是一种用于依赖关系解析的有用数据结构。让我们使用web
    UI来探索我们程序中`takeOrdered`作业的DAG。这个图相当复杂，因此很容易迷路，所以这里有一个简化的复制品，它只列出了程序中绑定到变量名的RDD。
- en: '![Lifting the hood](img/image01209.jpeg)'
  id: totrans-1963
  prefs: []
  type: TYPE_IMG
  zh: '![揭开盖子](img/image01209.jpeg)'
- en: 'As you can see, at the bottom of the graph, we have the `mutualInformation`
    RDD. This is the RDD that we need to construct for our action. This RDD depends
    on the intermediate elements in the sum, `igFragment1`, `igFragment2`, and so
    on. We can work our way back through the list of dependencies until we reach the
    other end of the graph: RDDs that do not depend on other RDDs, only on external
    sources.'
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在图的底部，我们有`mutualInformation` RDD。这是我们为我们的动作需要构建的RDD。这个RDD依赖于求和中的一些中间元素，例如`igFragment1`、`igFragment2`等。我们可以通过依赖关系列表回溯，直到达到图的另一端：不依赖于其他RDD，只依赖于外部源的RDD。
- en: Once the graph is built, the Spark engines formulates a plan to execute the
    job. The plan starts with the RDDs that only have external dependencies (such
    as RDDs built by loading files from disk or fetching from a database) or RDDs
    that already have cached data. Each arrow along the graph is translated to a set
    of *tasks*, with each task applying a transformation to a partition of the data.
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了图，Spark引擎就会制定一个执行作业的计划。计划从只有外部依赖（例如从磁盘加载文件或从数据库中检索而构建的RDD）或已经缓存了数据的RDD开始。图上的每个箭头都被转换为一组*任务*，每个任务将一个转换应用于数据的一个分区。
- en: Tasks are grouped into *stages*. A stage consists of a set of tasks that can
    all be performed without needing an intermediate shuffle.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: 任务被分组到*阶段*中。一个阶段由一组可以在不需要中间洗牌的情况下执行的任务组成。
- en: Data shuffling and partitions
  id: totrans-1967
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据洗牌和分区
- en: 'To understand data shuffling in Spark, we first need to understand how data
    is partitioned in RDDs. When we create an RDD by, for instance, loading a file
    from HDFS, or reading a file in local storage, Spark has no control over what
    bits of data are distributed in which partitions. This becomes a problem for key-value
    RDDs: these often require knowing where occurrences of a particular key are, for
    instance to perform a join. If the key can occur anywhere in the RDD, we have
    to look through every partition to find the key.'
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Spark中的数据洗牌，我们首先需要了解RDD中数据是如何分区的。当我们通过例如从HDFS加载文件或读取本地存储中的文件来创建一个RDD时，Spark无法控制哪些数据位被分布在哪些分区中。这对于键值RDD来说是一个问题：这些RDD通常需要知道特定键的出现位置，例如执行连接操作。如果键可以在RDD的任何位置出现，我们必须查看每个分区以找到该键。
- en: To prevent this, Spark allows the definition of a *partitioner* on key-value
    RDDs. A partitioner is an attribute of the RDD that determines which partition
    a particular key lands in. When an RDD has a partitioner set, the location of
    a key is entirely determined by the partitioner, and not by the RDD's history,
    or the number of keys. Two different RDDs with the same partitioner will map the
    same key to the same partition.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况，Spark允许在键值RDD上定义一个*分区器*。分区器是RDD的一个属性，它决定了特定键落在哪个分区。当一个RDD设置了分区器时，键的位置完全由分区器决定，而不是由RDD的历史或键的数量决定。具有相同分区器的两个不同的RDD将把相同的键映射到相同的分区。
- en: 'Partitions impact performance through their effect on transformations. There
    are two types of transformations on key-value RDDs:'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: 分区通过它们对转换的影响来影响性能。在键值RDD上有两种类型的转换：
- en: Narrow transformations, like `mapValues`. In narrow transformations, the data
    to compute a partition in the child RDD resides on a single partition in the parent.
    The data processing for a narrow transformation can therefore be performed entirely
    locally, without needing to communicate data between nodes.
  id: totrans-1971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窄变换，例如`mapValues`。在窄变换中，用于计算子RDD中分区的数据位于父分区的一个分区。因此，窄变换的数据处理可以完全本地执行，无需在节点之间通信数据。
- en: 'Wide transformations, like `reduceByKey`. In wide transformations, the data
    to compute any single partition can reside on all the partitions in the parent.
    The RDD resulting from a wide transformation will, in general, have a partitioner
    set. For instance, the output of a `reduceByKey` transformation are hash-partitioned
    by default: the partition that a particular key ends up in is determined by `hash(key)
    % numPartitions`.'
  id: totrans-1972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广泛变换，例如`reduceByKey`。在广泛变换中，用于计算任何单个分区的数据可以位于父分区中的所有分区。一般来说，由广泛变换产生的RDD将设置一个分区器。例如，`reduceByKey`变换的输出默认是哈希分区：特定键最终所在的分区由`hash(key)
    % numPartitions`确定。
- en: Thus, in our mutual information example, the RDDs `pPresentAndSpam` and `pPresentAndHam`
    will have the same partition structure since they both have the default hash partitioner.
    All descendent RDDs retain the same keys, all the way down to `mutualInformation`.
    The word `language`, for instance, will be in the same partition for each RDD.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的互信息示例中，`pPresentAndSpam`和`pPresentAndHam`将具有相同的分区结构，因为它们都有默认的哈希分区器。所有子RDD都保留相同的键，一直到`mutualInformation`。例如，单词`language`将在每个RDD中位于相同的分区。
- en: 'Why does all this matter? If an RDD has a partitioner set, this partitioner
    is retained through all subsequent narrow transformations originating from this
    RDD. Let''s go back to our mutual information example. The RDDs `pPresentGivenHam`
    and `pPresentGivenSpam` both originate from `reduceByKey` operations, and they
    both have string keys. They will therefore both have the same hash-partitioner
    (unless we explicitly set a different partitioner). This partitioner is retained
    as we construct `pPresentAndSpam` and `pPresentAndHam`. When we construct `pPresent`,
    we perform a full outer join of `pPresentAndSpam` and `pPresentAndHam`. Since
    both these RDDs have the same partitioner, the child RDD `pPresent` has narrow
    dependencies: we can just join the first partition of `pPresentAndSpam` with the
    first partition of `pPresentAndHam`, the second partition of `pPresentAndSpam`
    with the second partition of `pPresentAndHam` and so on, since any string key
    will be hashed to the same partition in both RDDs. By contrast, without partitioner,
    we would have to join the data in each partition of `pPresentAndSpam` with every
    partition of `pPresentAndSpam`. This would require sending data across the network
    to all the nodes holding `pPresentAndSpam`, a time-consuming exercise.'
  id: totrans-1974
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这些都很重要？如果一个RDD设置了分区器，那么这个分区器将保留在所有后续的窄变换中，这些变换源自该RDD。让我们回到我们的互信息示例。`pPresentGivenHam`和`pPresentGivenSpam`这两个RDD都源自`reduceByKey`操作，并且它们都有字符串键。因此，它们都将有相同的哈希分区器（除非我们明确设置不同的分区器）。当我们构建`pPresentAndSpam`和`pPresentAndHam`时，这个分区器将被保留。当我们构建`pPresent`时，我们执行`pPresentAndSpam`和`pPresentAndHam`的完全外连接。由于这两个RDD有相同的分区器，子RDD
    `pPresent`有窄依赖：我们只需将`pPresentAndSpam`的第一个分区与`pPresentAndHam`的第一个分区连接起来，将`pPresentAndSpam`的第二个分区与`pPresentAndHam`的第二个分区连接起来，依此类推，因为任何字符串键都会在两个RDD中被哈希到相同的分区。相比之下，如果没有分区器，我们就必须将`pPresentAndSpam`的每个分区的数据与`pPresentAndSpam`的每个分区连接起来。这将需要将数据发送到所有持有`pPresentAndSpam`的节点，这是一个耗时的操作。
- en: This process of having to send the data to construct a child RDD across the
    network, as a result of wide dependencies, is called *shuffling*. Much of the
    art of optimizing a Spark program involves reducing shuffling and, when shuffling
    is necessary, reducing the amount of shuffling.
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: 由于广泛依赖关系，需要将数据发送到网络中以构建子RDD的过程称为*洗牌*。优化Spark程序的大部分艺术在于减少洗牌，并在必要时减少洗牌量。
- en: Summary
  id: totrans-1976
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the basics of Spark and learned how to construct
    and manipulate RDDs. In the next chapter, we will learn about Spark SQL and DataFrames,
    a set of implicit conversions that allow us to manipulate RDDs in a manner similar
    to pandas DataFrames, and how to interact with different data sources using Spark.
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了Spark的基础知识，并学习了如何构建和操作RDD。在下一章中，我们将学习关于Spark SQL和DataFrame的知识，这是一组隐式转换，允许我们以类似于pandas
    DataFrame的方式操作RDD，以及如何使用Spark与不同的数据源进行交互。
- en: Reference
  id: totrans-1978
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*Learning Spark*, by *Holden Karau*, *Andy Konwinski*, *Patrick Wendell*, and
    *Matei Zaharia*, *O''Reilly*, provides a much more complete introduction to Spark
    that this chapter can provide. I thoroughly recommend it.'
  id: totrans-1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《学习 Spark》*，由 *Holden Karau*，*Andy Konwinski*，*Patrick Wendell* 和 *Matei Zaharia*
    撰写，*O''Reilly* 出版，提供了比本章更全面的 Spark 介绍。我强烈推荐它。'
- en: If you are interested in learning more about information theory, I recommend
    *David MacKay's* book *Information Theory, Inference, and Learning Algorithms*.
  id: totrans-1980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对学习信息理论感兴趣，我推荐 *David MacKay* 的书 *《信息理论、推理和学习算法》*。
- en: '*Information Retrieval*, by *Manning*, *Raghavan*, and *Schütze*, describes
    how to analyze textual data (including lemmatization and stemming). An online'
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《信息检索》*，由 *Manning*，*Raghavan* 和 *Schütze* 撰写，描述了如何分析文本数据（包括词形还原和词干提取）。在线'
- en: 'On the Ling-Spam dataset, and how to analyze it: [http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf](http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf).'
  id: totrans-1982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Ling-Spam 数据集上，以及如何分析它：[http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf](http://www.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf).
- en: This blog post delves into the Spark Web UI in more detail. [https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html).
  id: totrans-1983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇博客文章更详细地探讨了 Spark Web UI。[https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html).
- en: 'This blog post, by *Sandy Ryza*, is the first in a two-part series discussing
    Spark internals, and how to leverage them to improve performance: [http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/).'
  id: totrans-1984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇由 *Sandy Ryza* 撰写的博客文章是关于 Spark 内部机制的两部分系列文章的第一部分，讨论了如何利用它们来提高性能：[http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/).
- en: Chapter 11. Spark SQL and DataFrames
  id: totrans-1985
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章。Spark SQL 和 DataFrame
- en: In the previous chapter, we learned how to build a simple distributed application
    using Spark. The data that we used took the form of a set of e-mails stored as
    text files.
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用 Spark 构建一个简单的分布式应用程序。我们使用的数据是以文本文件形式存储的一组电子邮件。
- en: 'We learned that Spark was built around the concept of **resilient distributed
    datasets** (**RDDs**). We explored several types of RDDs: simple RDDs of strings,
    key-value RDDs, and RDDs of doubles. In the case of key-value RDDs and RDDs of
    doubles, Spark added functionality beyond that of the simple RDDs through implicit
    conversions. There is one important type of RDD that we have not explored yet:
    **DataFrames** (previously called **SchemaRDD**). DataFrames allow the manipulation
    of objects significantly more complex than those we have explored to date.'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到 Spark 是围绕 **弹性分布式数据集**（**RDDs**）的概念构建的。我们探索了几种类型的 RDD：简单的字符串 RDD、键值 RDD
    和双精度浮点数 RDD。在键值 RDD 和双精度浮点数 RDD 的情况下，Spark 通过隐式转换增加了比简单 RDD 更多的功能。还有一种重要的 RDD
    类型我们尚未探索：**DataFrame**（之前称为 **SchemaRDD**）。DataFrame 允许操作比我们迄今为止探索的更复杂的对象。
- en: A DataFrame is a distributed tabular data structure, and is therefore very useful
    for representing and manipulating structured data. In this chapter, we will first
    investigate DataFrames through the Spark shell, and then use the Ling-spam e-mail
    dataset, presented in the previous chapter, to see how DataFrames can be integrated
    in a machine learning pipeline.
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是一种分布式表格数据结构，因此非常适合表示和操作结构化数据。在本章中，我们将首先通过 Spark shell 研究DataFrame，然后使用上一章中介绍的
    Ling-spam 电子邮件数据集，看看DataFrame 如何集成到机器学习管道中。
- en: DataFrames – a whirlwind introduction
  id: totrans-1989
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrames – 快速入门
- en: 'Let''s start by opening a Spark shell:'
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从打开 Spark shell 开始：
- en: '[PRE492]'
  id: totrans-1991
  prefs: []
  type: TYPE_PRE
  zh: '[PRE492]'
- en: Let's imagine that we are interested in running analytics on a set of patients
    to estimate their overall health level. We have measured, for each patient, their
    height, weight, age, and whether they smoke.
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们对在患者群体上运行分析以估计他们的整体健康状况感兴趣。我们已经为每位患者测量了他们的身高、体重、年龄以及他们是否吸烟。
- en: 'We might represent the readings for each patient as a case class (you might
    wish to write some of this in a text editor and paste it into the Scala shell
    using `:paste`):'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能将每位患者的读数表示为一个案例类（你可能希望将其中一些内容写入文本编辑器，然后使用 `:paste` 命令将其粘贴到 Scala shell 中）：
- en: '[PRE493]'
  id: totrans-1994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE493]'
- en: 'We would, typically, have many thousands of patients, possibly stored in a
    database or a CSV file. We will worry about how to interact with external sources
    later in this chapter. For now, let''s just hard-code a few readings directly
    in the shell:'
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会有成千上万的病人，可能存储在数据库或 CSV 文件中。我们将在本章后面讨论如何与外部源交互。现在，让我们直接在 shell 中硬编码一些读取值：
- en: '[PRE494]'
  id: totrans-1996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE494]'
- en: 'We can convert `readings` to an RDD by using `sc.parallelize`:'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `sc.parallelize` 将 `readings` 转换为 RDD：
- en: '[PRE495]'
  id: totrans-1998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE495]'
- en: 'Note that the type parameter of our RDD is `PatientReadings`. Let''s convert
    the RDD to a DataFrame using the `.toDF` method:'
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们 RDD 的类型参数是 `PatientReadings`。让我们使用 `.toDF` 方法将 RDD 转换为 DataFrame：
- en: '[PRE496]'
  id: totrans-2000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE496]'
- en: 'We have created a DataFrame where each row corresponds to the readings for
    a specific patient, and the columns correspond to the different features:'
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个 DataFrame，其中每一行对应于特定病人的读取值，列对应于不同的特征：
- en: '[PRE497]'
  id: totrans-2002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE497]'
- en: The easiest way to create a DataFrame is to use the `toDF` method on an RDD.
    We can convert any `RDD[T]`, where `T` is a case class or a tuple, to a DataFrame.
    Spark will map each attribute of the case class to a column of the appropriate
    type in the DataFrame. It uses reflection to discover the names and types of the
    attributes. There are several other ways of constructing DataFrames, both from
    RDDs and from external sources, which we will explore later in this chapter.
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 DataFrame 最简单的方法是使用 RDD 上的 `toDF` 方法。我们可以将任何 `RDD[T]`（其中 `T` 是一个 case class
    或一个元组）转换为 DataFrame。Spark 将将 case class 的每个属性映射到 DataFrame 中适当类型的列。它使用反射来发现属性的名字和类型。还有几种其他方法可以构建
    DataFrame，无论是从 RDD 还是外部来源，我们将在本章后面探讨。
- en: 'DataFrames support many operations for manipulating the rows and columns. For
    instance, let''s add a column for the **Body Mass Index** (**BMI**). The BMI is
    a common way of aggregating *height* and *weight* to decide if someone is overweight
    or underweight. The formula for the BMI is:'
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 支持许多操作来操作行和列。例如，让我们添加一个用于 **体质指数**（**BMI**）的列。BMI 是一种常见的将 *身高* 和
    *体重* 聚合起来以判断某人是否超重或体重不足的方法。BMI 的公式是：
- en: '![DataFrames – a whirlwind introduction](img/image01210.jpeg)'
  id: totrans-2005
  prefs: []
  type: TYPE_IMG
  zh: '![DataFrames – 快速入门](img/image01210.jpeg)'
- en: 'Let''s start by creating a column of the height in meters:'
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个以米为单位的身高列：
- en: '[PRE498]'
  id: totrans-2007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE498]'
- en: '`heightM` has data type `Column`, representing a column of data in a DataFrame.
    Columns support many arithmetic and comparison operators that apply element-wise
    across the column (similarly to Breeze vectors encountered in [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze*).
    Operations on columns are lazy: the `heightM` column is not actually computed
    when defined. Let''s now define a BMI column:'
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: '`heightM` 具有数据类型 `Column`，表示 DataFrame 中的数据列。列支持许多算术和比较运算符，这些运算符按元素方式应用于列（类似于在
    [第 2 章](part0018.xhtml#aid-H5A41 "第 2 章。使用 Breeze 操作数据")中遇到的 Breeze 向量，*使用 Breeze
    操作数据*)。列上的操作是惰性的：当定义时，`heightM` 列实际上并没有计算。现在让我们定义一个 BMI 列：'
- en: '[PRE499]'
  id: totrans-2009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE499]'
- en: 'It would be useful to add the `bmi` column to our readings DataFrame. Since
    DataFrames, like RDDs, are immutable, we must define a new DataFrame that is identical
    to `readingsDF`, but with an additional column for the BMI. We can do this using
    the `withColumn` method, which takes, as its arguments, the name of the new column
    and a `Column` instance:'
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的读取 DataFrame 中添加 `bmi` 列将很有用。由于 DataFrames，就像 RDDs 一样，是不可变的，我们必须定义一个新的 DataFrame，它与
    `readingsDF` 完全相同，但增加了一个用于 BMI 的列。我们可以使用 `withColumn` 方法来实现，该方法接受新列的名称和一个 `Column`
    实例作为参数：
- en: '[PRE500]'
  id: totrans-2011
  prefs: []
  type: TYPE_PRE
  zh: '[PRE500]'
- en: 'All the operations we have seen so far are *transformations*: they define a
    pipeline of operations that create new DataFrames. These transformations are executed
    when we call an **action**, such as `show`:'
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止看到的所有操作都是 *转换*：它们定义了一个操作管道，创建新的 DataFrames。这些转换在我们调用 **动作**（如 `show`）时执行：
- en: '[PRE501]'
  id: totrans-2013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE501]'
- en: 'Besides creating additional columns, DataFrames also support filtering rows
    that satisfy a certain predicate. For instance, we can select all smokers:'
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建额外的列，DataFrames 还支持过滤满足特定谓词的行。例如，我们可以选择所有吸烟者：
- en: '[PRE502]'
  id: totrans-2015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE502]'
- en: 'Or, to select everyone who weighs more than 70 kgs:'
  id: totrans-2016
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，为了选择体重超过 70 公斤的人：
- en: '[PRE503]'
  id: totrans-2017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE503]'
- en: 'It can become cumbersome to keep repeating the DataFrame name in an expression.
    Spark defines the operator `$` to refer to a column in the current DataFrame.
    Thus, the `filter` expression above could have been written more succinctly using:'
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: 在表达式中重复 DataFrame 名称可能会变得繁琐。Spark 定义了操作符 `$` 来引用当前 DataFrame 中的列。因此，上面的 `filter`
    表达式可以更简洁地写成：
- en: '[PRE504]'
  id: totrans-2019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE504]'
- en: 'The `.filter` method is overloaded. It accepts either a column of Boolean values,
    as above, or a string identifying a Boolean column in the current DataFrame. Thus,
    to filter our `readingsWithBmiDF` DataFrame to sub-select smokers, we could also
    have used the following:'
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: '`.filter` 方法是重载的。它接受一个布尔值列，如上所述，或者一个标识当前 DataFrame 中布尔列的字符串。因此，为了过滤我们的 `readingsWithBmiDF`
    DataFrame 以子选择吸烟者，我们也可以使用以下方法：'
- en: '[PRE505]'
  id: totrans-2021
  prefs: []
  type: TYPE_PRE
  zh: '[PRE505]'
- en: 'When comparing for equality, you must compare columns with the special *triple-equals*
    operator:'
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较相等时，你必须使用特殊的 *三重等于* 操作符来比较列：
- en: '[PRE506]'
  id: totrans-2023
  prefs: []
  type: TYPE_PRE
  zh: '[PRE506]'
- en: 'Similarly, you must use `!==` to select rows that are not equal to a value:'
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你必须使用 `!==` 来选择不等于某个值的行：
- en: '[PRE507]'
  id: totrans-2025
  prefs: []
  type: TYPE_PRE
  zh: '[PRE507]'
- en: Aggregation operations
  id: totrans-2026
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合操作
- en: 'We have seen how to apply an operation to every row in a DataFrame to create
    a new column, and we have seen how to use filters to build new DataFrames with
    a sub-set of rows from the original DataFrame. The last set of operations on DataFrames
    is grouping operations, equivalent to the `GROUP BY` statement in SQL. Let''s
    calculate the average BMI for smokers and non-smokers. We must first tell Spark
    to group the DataFrame by a column (the `isSmoker` column, in this case), and
    then apply an aggregation operation (averaging, in this case) to reduce each group:'
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何将操作应用于 DataFrame 中的每一行以创建新列，以及如何使用过滤器从原始 DataFrame 中选择子集行来构建新的 DataFrame。DataFrame
    的最后一系列操作是分组操作，相当于 SQL 中的 `GROUP BY` 语句。让我们计算吸烟者和非吸烟者的平均 BMI。我们必须首先告诉 Spark 按列（在这种情况下是
    `isSmoker` 列）对 DataFrame 进行分组，然后应用聚合操作（在这种情况下是平均）以减少每个组：
- en: '[PRE508]'
  id: totrans-2028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE508]'
- en: 'This has created a new DataFrame with two columns: the grouping column and
    the column over which we aggregated. Let''s show this DataFrame:'
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经创建了一个包含两列的新 DataFrame：分组列和我们要对其聚合的列。让我们展示这个 DataFrame：
- en: '[PRE509]'
  id: totrans-2030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE509]'
- en: 'Besides averaging, there are several operators for performing the aggregation
    across each group. We outline some of the more important ones in the table below,
    but, for a full list, consult the *Aggregate functions* section of [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions%24):'
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 除了平均之外，还有几个操作符可以用于对每个组进行聚合。以下表格中概述了一些更重要的一些，但要获取完整列表，请参阅[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions%24)的
    *聚合函数* 部分：
- en: '| Operator | Notes |'
  id: totrans-2032
  prefs: []
  type: TYPE_TB
  zh: '| 操作符 | 备注 |'
- en: '| --- | --- |'
  id: totrans-2033
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `avg(column)` | Group averages of the values in the specified column. |'
  id: totrans-2034
  prefs: []
  type: TYPE_TB
  zh: '| `avg(column)` | 指定列的组平均值。 |'
- en: '| `count(column)` | Number of elements in each group in the specified column.
    |'
  id: totrans-2035
  prefs: []
  type: TYPE_TB
  zh: '| `count(column)` | 在指定列中每个组中的元素数量。 |'
- en: '| `countDistinct(column, ... )` | Number of distinct elements in each group.
    This can also accept multiple columns to return the count of unique elements across
    several columns. |'
  id: totrans-2036
  prefs: []
  type: TYPE_TB
  zh: '| `countDistinct(column, ... )` | 每个组中不同元素的数量。这也可以接受多个列以返回跨多个列的唯一元素计数。 |'
- en: '| `first(column), last(column)` | First/last element in each group |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
  zh: '| `first(column), last(column)` | 每个组中的第一个/最后一个元素 |'
- en: '| `max(column), min(column)` | Largest/smallest element in each group |'
  id: totrans-2038
  prefs: []
  type: TYPE_TB
  zh: '| `max(column), min(column)` | 每个组中的最大/最小元素 |'
- en: '| `sum(column)` | Sum of the values in each group |'
  id: totrans-2039
  prefs: []
  type: TYPE_TB
  zh: '| `sum(column)` | 每个组中值的总和 |'
- en: 'Each aggregation operator takes either the name of a column, as a string, or
    an expression of type `Column`. The latter allows aggregation of compound expressions.
    If we wanted the average height, in meters, of the smokers and non-smokers in
    our sample, we could use:'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聚合操作符都接受列名，作为字符串，或者类型为 `Column` 的表达式。后者允许对复合表达式进行聚合。如果我们想得到样本中吸烟者和非吸烟者的平均身高（以米为单位），我们可以使用：
- en: '[PRE510]'
  id: totrans-2041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE510]'
- en: 'We can also use compound expressions to define the column on which to group.
    For instance, to count the number of patients in each `age` group, increasing
    by decade, we can use:'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用复合表达式来定义要分组的列。例如，为了计算每个 `age` 组中患者的数量，按十年递增，我们可以使用：
- en: '[PRE511]'
  id: totrans-2043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE511]'
- en: We have used the short-hand `"*"` to indicate a count over every column.
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用短横线 `"*"` 来表示对每一列的计数。
- en: Joining DataFrames together
  id: totrans-2045
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 DataFrames 合并
- en: 'So far, we have only considered operations on a single DataFrame. Spark also
    offers SQL-like joins to combine DataFrames. Let''s assume that we have another
    DataFrame mapping the patient id to a (systolic) blood pressure measurement. We
    will assume we have the data as a list of pairs mapping patient IDs to blood pressures:'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了单个 DataFrame 上的操作。Spark 还提供了类似 SQL 的连接来组合 DataFrame。假设我们还有一个将患者
    ID 映射到（收缩压）血压测量的 DataFrame。我们将假设我们有一个将患者 ID 映射到血压的列表对：
- en: '[PRE512]'
  id: totrans-2047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE512]'
- en: 'We can construct a DataFrame from this RDD of tuples. However, unlike when
    constructing DataFrames from RDDs of case classes, Spark cannot infer column names.
    We must therefore pass these explicitly to `.toDF`:'
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个元组 RDD 构建一个 DataFrame。然而，与从案例类 RDD 构建 DataFrame 不同，Spark 无法推断列名。因此，我们必须将这些列名显式传递给
    `.toDF`：
- en: '[PRE513]'
  id: totrans-2049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE513]'
- en: 'Let''s join `bloodPressureDF` with `readingsDF`, using the patient ID as the
    join key:'
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `bloodPressureDF` 与 `readingsDF` 通过患者 ID 作为连接键进行连接：
- en: '[PRE514]'
  id: totrans-2051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE514]'
- en: 'This performs an *inner join*: only patient IDs present in both DataFrames
    are included in the result. The type of join can be passed as an extra argument
    to `join`. For instance, we can perform a *left join*:'
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
  zh: 这执行了一个*内连接*：只有同时存在于两个 DataFrame 中的患者 ID 被包含在结果中。连接类型可以作为额外的参数传递给 `join`。例如，我们可以执行一个*左连接*：
- en: '[PRE515]'
  id: totrans-2053
  prefs: []
  type: TYPE_PRE
  zh: '[PRE515]'
- en: Possible join types are `inner`, `outer`, `leftouter`, `rightouter`, or `leftsemi`.
    These should all be familiar, apart from `leftsemi`, which corresponds to a *left
    semi join*.This is the same as an inner join, but only the columns on the left-hand
    side are retained after the join. It is thus a way to filter a DataFrame for rows
    which are present in another DataFrame.
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的连接类型有 `inner`、`outer`、`leftouter`、`rightouter` 或 `leftsemi`。这些都应该很熟悉，除了 `leftsemi`，它对应于*左半连接*。这与内连接相同，但在连接后只保留左侧的列。因此，这是一种过滤
    DataFrame 以找到存在于另一个 DataFrame 中的行的方法。
- en: Custom functions on DataFrames
  id: totrans-2055
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义 DataFrame 上的函数
- en: 'So far, we have only used built-in functions to operate on DataFrame columns.
    While these are often sufficient, we sometimes need greater flexibility. Spark
    lets us apply custom transformations to every row through **user-defined functions**
    (**UDFs**). Let''s assume that we want to use the equation that we derived in
    [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data with Breeze"),
    *Manipulating Data with Breeze*, for the probability of a person being male, given
    their height and weight. We calculated that the decision boundary was given by:'
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用内置函数来操作 DataFrame 的列。虽然这些通常足够用，但我们有时需要更大的灵活性。Spark 允许我们通过**用户定义函数**（**UDFs**）将自定义转换应用于每一行。假设我们想使用我们在[第
    2 章](part0018.xhtml#aid-H5A41 "第 2 章。使用 Breeze 操作数据")中推导出的方程，即*使用 Breeze 操作数据*，来计算给定身高和体重的男性概率。我们计算出决策边界如下：
- en: '![Custom functions on DataFrames](img/image01211.jpeg)'
  id: totrans-2057
  prefs: []
  type: TYPE_IMG
  zh: '![自定义 DataFrame 上的函数](img/image01211.jpeg)'
- en: 'Any person with *f > 0* is more likely to be male than female, given their
    height and weight and the training set used for [Chapter 2](part0018.xhtml#aid-H5A41
    "Chapter 2. Manipulating Data with Breeze"), *Manipulating Data with Breeze* (which
    was based on students, so is unlikely to be representative of the population as
    a whole). To convert from a height in centimeters to the normalized height, *rescaledHeight*,
    we can use this formula:'
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
  zh: 任何具有 *f > 0* 的人，在给定他们的身高、体重以及用于[第 2 章](part0018.xhtml#aid-H5A41 "第 2 章。使用 Breeze
    操作数据")，*使用 Breeze 操作数据*（该数据基于学生，因此不太可能代表整个人群）的训练集的情况下，比女性更有可能是男性。要将厘米单位的身高转换为归一化身高
    *rescaledHeight*，我们可以使用以下公式：
- en: '![Custom functions on DataFrames](img/image01212.jpeg)'
  id: totrans-2059
  prefs: []
  type: TYPE_IMG
  zh: '![自定义 DataFrame 上的函数](img/image01212.jpeg)'
- en: 'Similarly, to convert a weight (in kilograms) to the normalized weight, *rescaledWeight*,
    we can use:'
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要将体重（以千克为单位）转换为归一化体重 *rescaledWeight*，我们可以使用以下公式：
- en: '![Custom functions on DataFrames](img/image01213.jpeg)'
  id: totrans-2061
  prefs: []
  type: TYPE_IMG
  zh: '![自定义 DataFrame 上的函数](img/image01213.jpeg)'
- en: 'The average and standard deviation of the *height* and *weight* are calculated
    from the training set. Let''s write a Scala function that returns whether a person
    is more likely to be male, given their height and weight:'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练集中计算了 *height* 和 *weight* 的平均值和标准差。让我们编写一个 Scala 函数，该函数返回给定身高和体重的人更有可能是男性：
- en: '[PRE516]'
  id: totrans-2063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE516]'
- en: 'To use this function on Spark DataFrames, we need to register it as a **user-defined
    function** (**UDF**). This transforms our function, which accepts integer arguments,
    into one that accepts column arguments:'
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Spark DataFrame 上使用此函数，我们需要将其注册为**用户定义函数**（**UDF**）。这将我们的函数，该函数接受整数参数，转换为接受列参数的函数：
- en: '[PRE517]'
  id: totrans-2065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE517]'
- en: To register a UDF, we must have access to a `sqlContext` instance. The SQL context
    provides the entry point for DataFrame operations. The Spark shell creates a SQL
    context at startup, bound to the variable `sqlContext`, and destroys it when the
    shell session is closed.
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册一个 UDF，我们必须能够访问一个 `sqlContext` 实例。SQL 上下文提供了 DataFrame 操作的入口点。Spark shell
    在启动时创建一个 SQL 上下文，绑定到变量 `sqlContext`，并在 shell 会话关闭时销毁它。
- en: 'The first argument passed to the `register` function is the name of the UDF
    (we will use the UDF name later when we write SQL statements on the DataFrame,
    but you can ignore it for now). We can then use the UDF just like the built-in
    transformations included in Spark:'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `register` 函数的第一个参数是 UDF 的名称（我们将在以后编写 DataFrame 上的 SQL 语句时使用 UDF 名称，但现在你可以忽略它）。然后我们可以像使用
    Spark 中包含的内置转换一样使用 UDF：
- en: '[PRE518]'
  id: totrans-2068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE518]'
- en: 'As you can see, Spark applies the function underlying the UDF to every row
    in the DataFrame. We are not limited to using UDFs to create new columns. We can
    also use them in `filter` expressions. For instance, to select rows likely to
    correspond to women:'
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，Spark 将 UDF 的底层函数应用于 DataFrame 中的每一行。我们不仅限于使用 UDF 创建新列。我们还可以在 `filter`
    表达式中使用它们。例如，为了选择可能对应女性的行：
- en: '[PRE519]'
  id: totrans-2070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE519]'
- en: Using UDFs lets us define arbitrary Scala functions to transform rows, giving
    tremendous additional power for data manipulation.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 UDF 允许我们定义任意的 Scala 函数来转换行，为数据处理提供了巨大的额外功能。
- en: DataFrame immutability and persistence
  id: totrans-2072
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame 的不可变性和持久性
- en: DataFrames, like RDDs, are immutable. When you define a transformation on a
    DataFrame, this always creates a new DataFrame. The original DataFrame cannot
    be modified in place (this is notably different to pandas DataFrames, for instance).
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RDD 一样，DataFrame 是不可变的。当你对一个 DataFrame 定义一个转换时，这总是创建一个新的 DataFrame。原始 DataFrame
    不能就地修改（这与 pandas DataFrame 明显不同，例如）。
- en: 'Operations on DataFrames can be grouped into two: *transformations*, which
    result in the creation of a new DataFrame, and *actions*, which usually return
    a Scala type or have a side-effect. Methods like `filter` or `withColumn` are
    transformations, while methods like `show` or `head` are actions.'
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 上的操作可以分为两类：*转换*，它导致创建一个新的 DataFrame，和*动作*，它通常返回一个 Scala 类型或有一个副作用。例如
    `filter` 或 `withColumn` 是转换，而 `show` 或 `head` 是动作。
- en: Transformations are lazy, much like transformations on RDDs. When you generate
    a new DataFrame by transforming an existing DataFrame, this results in the elaboration
    of an execution plan for creating the new DataFrame, but the data itself is not
    transformed immediately. You can access the execution plan with the `queryExecution`
    method.
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是惰性的，就像 RDD 上的转换一样。当你通过转换现有的 DataFrame 生成一个新的 DataFrame 时，这会导致创建新 DataFrame
    的执行计划的详细阐述，但数据本身并不会立即被转换。你可以使用 `queryExecution` 方法访问执行计划。
- en: 'When you call an action on a DataFrame, Spark processes the action as if it
    were a regular RDD: it implicitly builds a direct acyclic graph to resolve dependencies,
    processing the transformations needed to build the DataFrame on which the action
    was called.'
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 DataFrame 上调用一个动作时，Spark 会像处理一个常规 RDD 一样处理该动作：它隐式地构建一个无环图来解析依赖关系，处理构建被调用动作的
    DataFrame 所需要的转换。
- en: 'Much like RDDs, we can persist DataFrames in memory or on disk:'
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RDD 类似，我们可以在内存或磁盘上持久化 DataFrame：
- en: '[PRE520]'
  id: totrans-2078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE520]'
- en: 'This works in the same way as persisting RDDs: next time the RDD is calculated,
    it will be kept in memory (provided there is enough space), rather than discarded.
    The level of persistence can also be set:'
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: 这与持久化 RDD 的方式相同：下次计算 RDD 时，它将被保留在内存中（前提是有足够的空间），而不是被丢弃。持久化级别也可以设置：
- en: '[PRE521]'
  id: totrans-2080
  prefs: []
  type: TYPE_PRE
  zh: '[PRE521]'
- en: SQL statements on DataFrames
  id: totrans-2081
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame 上的 SQL 语句
- en: By now, you will have noticed that many operations on DataFrames are inspired
    by SQL operations. Additionally, Spark allows us to register DataFrames as tables
    and query them with SQL statements directly. We can therefore build a temporary
    database as part of the program flow.
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经注意到 DataFrame 上的许多操作都是受 SQL 操作启发的。此外，Spark 允许我们将 DataFrame 注册为表，并直接使用
    SQL 语句查询它们。因此，我们可以将临时数据库作为程序流程的一部分构建。
- en: 'Let''s register `readingsDF` as a temporary table:'
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `readingsDF` 注册为临时表：
- en: '[PRE522]'
  id: totrans-2084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE522]'
- en: This registers a temporary table that can be used in SQL queries. Registering
    a temporary table relies on the presence of a SQL context. The temporary tables
    are destroyed when the SQL context is destroyed (when we close the shell, for
    instance).
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: 这注册了一个临时表，该表可以在 SQL 查询中使用。注册临时表依赖于 SQL 上下文的存在。当 SQL 上下文被销毁时（例如，当我们关闭 shell 时），临时表将被销毁。
- en: 'Let''s explore what we can do with our temporary tables and the SQL context.
    We can first get a list of all the tables currently registered with the context:'
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索我们可以使用我们的临时表和 SQL 上下文做什么。我们首先可以获取上下文中当前注册的所有表的列表：
- en: '[PRE523]'
  id: totrans-2087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE523]'
- en: 'This returns a DataFrame. In general, all operations on a SQL context that
    return data return DataFrames:'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 DataFrame。一般来说，所有在 SQL 上下文中返回数据的操作都会返回 DataFrames：
- en: '[PRE524]'
  id: totrans-2089
  prefs: []
  type: TYPE_PRE
  zh: '[PRE524]'
- en: 'We can query this table by passing SQL statements to the SQL context:'
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向 SQL 上下文中传递 SQL 语句来查询这个表：
- en: '[PRE525]'
  id: totrans-2091
  prefs: []
  type: TYPE_PRE
  zh: '[PRE525]'
- en: 'Any UDFs registered with the `sqlContext` are available through the name given
    to them when they were registered. We can therefore use them in SQL queries:'
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sqlContext` 中注册的任何 UDF 都可以通过它们注册时给出的名称访问。因此，我们可以在 SQL 查询中使用它们：
- en: '[PRE526]'
  id: totrans-2093
  prefs: []
  type: TYPE_PRE
  zh: '[PRE526]'
- en: You might wonder why one would want to register DataFrames as temporary tables
    and run SQL queries on those tables, when the same functionality is available
    directly on DataFrames. The main reason is for interacting with external tools.
    Spark can run a SQL engine that exposes a JDBC interface, meaning that programs
    that know how to interact with a SQL database will be able to make use of the
    temporary tables.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么有人想要将 DataFrames 注册为临时表并在这些表上运行 SQL 查询，当同样的功能可以直接在 DataFrames 上使用时。主要原因是为了与外部工具交互。Spark
    可以运行一个 SQL 引擎，该引擎公开 JDBC 接口，这意味着知道如何与 SQL 数据库交互的程序将能够使用临时表。
- en: We don't have the space to cover how to set up a distributed SQL engine in this
    book, but you can find details in the Spark documentation ([http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine](http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)).
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有足够的空间在这本书中介绍如何设置分布式 SQL 引擎，但您可以在 Spark 文档中找到详细信息（[http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine](http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)）。
- en: Complex data types – arrays, maps, and structs
  id: totrans-2096
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂数据类型 - 数组、映射和 struct
- en: 'So far, all the elements in our DataFrames were simple types. DataFrames support
    three additional collection types: arrays, maps, and structs.'
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们 DataFrame 中的所有元素都是简单类型。DataFrames 支持三种额外的集合类型：数组、映射和 struct。
- en: Structs
  id: totrans-2098
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Structs
- en: 'The first compound type that we will look at is the **struct**. A struct is
    similar to a case class: it stores a set of key-value pairs, with a fixed set
    of keys. If we convert an RDD of a case class containing nested case classes to
    a DataFrame, Spark will convert the nested objects to a struct.'
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第一个复合类型是 **struct**。一个 struct 类似于 case class：它存储一组键值对，具有一组固定的键。如果我们将包含嵌套
    case class 的 case class RDD 转换为 DataFrame，Spark 将将嵌套对象转换为 struct。
- en: 'Let''s imagine that we want to serialize Lords of the Ring characters. We might
    use the following object model:'
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们想要序列化《指环王》中的角色。我们可能会使用以下对象模型：
- en: '[PRE527]'
  id: totrans-2101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE527]'
- en: 'We want to create a DataFrame of `LotrCharacter` instances. Let''s create some
    dummy data:'
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要创建一个 `LotrCharacter` 实例的 DataFrame。让我们创建一些虚拟数据：
- en: '[PRE528]'
  id: totrans-2103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE528]'
- en: 'The `weapon` attribute in the case class was converted to a struct column in
    the DataFrame. To extract sub-fields from a struct, we can pass the field name
    to the column''s `.apply` method:'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 case class 中的 `weapon` 属性在 DataFrame 中被转换为 struct 列。要从 struct 中提取子字段，我们可以将字段名传递给列的
    `.apply` 方法：
- en: '[PRE529]'
  id: totrans-2105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE529]'
- en: 'We can use this derived column just as we would any other column. For instance,
    let''s filter our DataFrame to only contain characters who wield a sword:'
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个派生列就像我们使用任何其他列一样。例如，让我们过滤我们的 DataFrame，只包含挥舞着剑的角色：
- en: '[PRE530]'
  id: totrans-2107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE530]'
- en: Arrays
  id: totrans-2108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Arrays
- en: 'Let''s return to the earlier example, and assume that, besides height, weight,
    and age measurements, we also have phone numbers for our patients. Each patient
    might have zero, one, or more phone numbers. We will define a new case class and
    new dummy data:'
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到之前的例子，并假设除了身高、体重和年龄测量值之外，我们还有我们患者的电话号码。每个患者可能有零个、一个或多个电话号码。我们将定义一个新的 case
    class 和新的虚拟数据：
- en: '[PRE531]'
  id: totrans-2110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE531]'
- en: 'The `List[String]` array in our case class gets translated to an `array<string>`
    data type:'
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 case class 中，`List[String]` 数组被转换为 `array<string>` 数据类型：
- en: '[PRE532]'
  id: totrans-2112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE532]'
- en: 'As with structs, we can construct a column for a specific index the array.
    For instance, we can select the first element in each array:'
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
  zh: 与 structs 类似，我们可以为数组中的特定索引构造一个列。例如，我们可以选择每个数组中的第一个元素：
- en: '[PRE533]'
  id: totrans-2114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE533]'
- en: Maps
  id: totrans-2115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Maps
- en: The last compound data type is the map. Maps are similar to structs inasmuch
    as they store key-value pairs, but the set of keys is not fixed when the DataFrame
    is created. They can thus store arbitrary key-value pairs.
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的复合数据类型是映射。映射在存储键值对方面与 structs 类似，但 DataFrame 创建时键的集合不是固定的。因此，它们可以存储任意键值对。
- en: Scala maps will be converted to DataFrame maps when the DataFrame is constructed.
    They can then be queried in a manner similar to structs.
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建 DataFrame 时，Scala 映射将被转换为 DataFrame 映射。然后可以以类似结构体的方式查询它们。
- en: Interacting with data sources
  id: totrans-2118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与数据源交互
- en: A major challenge in data science or engineering is dealing with the wealth
    of input and output formats for persisting data. We might receive or send data
    as CSV files, JSON files, or through a SQL database, to name a few.
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学或工程中，一个主要挑战是处理用于持久化数据的丰富输入和输出格式。我们可能以 CSV 文件、JSON 文件或通过 SQL 数据库的形式接收或发送数据，仅举几例。
- en: Spark provides a unified API for serializing and de-serializing DataFrames to
    and from different data sources.
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个统一的 API，用于将 DataFrame 序列化和反序列化到不同的数据源。
- en: JSON files
  id: totrans-2121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON 文件
- en: Spark supports loading data from JSON files, provided that each line in the
    JSON file corresponds to a single JSON object. Each object will be mapped to a
    DataFrame row. JSON arrays are mapped to arrays, and embedded objects are mapped
    to structs.
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持从 JSON 文件加载数据，前提是 JSON 文件中的每一行都对应一个 JSON 对象。每个对象将被映射到 DataFrame 行。JSON
    数组被映射到数组，嵌套对象被映射到结构体。
- en: This section would be a little dry without some data, so let's generate some
    from the GitHub API. Unfortunately, the GitHub API does not return JSON formatted
    as a single object per line. The code repository for this chapter contains a script,
    `FetchData.scala` which will download and format JSON entries for Martin Odersky's
    repositories, saving the objects to a file named `odersky_repos.json` (go ahead
    and change the GitHub user in `FetchData.scala` if you want). You can also download
    a pre-constructed data file from [data.scala4datascience.com/odersky_repos.json](http://data.scala4datascience.com/odersky_repos.json).
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有一些数据，本节可能会显得有些枯燥，所以让我们从 GitHub API 生成一些数据。不幸的是，GitHub API 并不返回每行一个 JSON
    格式的对象。本章的代码库包含一个名为 `FetchData.scala` 的脚本，该脚本将下载并格式化 Martin Odersky 的存储库的 JSON
    条目，并将对象保存到名为 `odersky_repos.json` 的文件中（如果你想的话，请更改 `FetchData.scala` 中的 GitHub
    用户）。你也可以从 [data.scala4datascience.com/odersky_repos.json](http://data.scala4datascience.com/odersky_repos.json)
    下载预先构建的数据文件。
- en: 'Let''s dive into the Spark shell and load this data into a DataFrame. Reading
    from a JSON file is as simple as passing the file name to the `sqlContext.read.json`
    method:'
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入 Spark shell 并将此数据加载到 DataFrame 中。从 JSON 文件读取就像将文件名传递给 `sqlContext.read.json`
    方法一样简单：
- en: '[PRE534]'
  id: totrans-2125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE534]'
- en: 'Reading from a JSON file loads data as a DataFrame. Spark automatically infers
    the schema from the JSON documents. There are many columns in our DataFrame. Let''s
    sub-select a few to get a more manageable DataFrame:'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
  zh: 从 JSON 文件读取数据时，数据被加载为 DataFrame。Spark 会自动从 JSON 文档中推断模式。我们的 DataFrame 中有许多列。让我们子选择一些列以获得更易于管理的
    DataFrame：
- en: '[PRE535]'
  id: totrans-2127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE535]'
- en: 'Let''s save the DataFrame back to JSON:'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 DataFrame 保存回 JSON：
- en: '[PRE536]'
  id: totrans-2129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE536]'
- en: If you look at the files present in the directory in which you are running the
    Spark shell, you will notice a `repos_short.json` directory. Inside it, you will
    see files named `part-000000`, `part-000001`, and so on. When serializing JSON,
    each partition of the DataFrame is serialized independently. If you are running
    this on several machines, you will find parts of the serialized output on each
    computer.
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看运行 Spark shell 的目录中的文件，你会注意到一个 `repos_short.json` 目录。在里面，你会看到名为 `part-000000`、`part-000001`
    等的文件。当序列化 JSON 时，DataFrame 的每个分区都是独立序列化的。如果你在多台机器上运行此操作，你将在每台计算机上找到序列化输出的部分。
- en: 'You may, optionally, pass a `mode` argument to control how Spark deals with
    the case of an existing `repos_short.json` file:'
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择性地传递一个 `mode` 参数来控制 Spark 如何处理现有的 `repos_short.json` 文件：
- en: '[PRE537]'
  id: totrans-2132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE537]'
- en: Available save modes are `ErrorIfExists`, `Append` (only available for Parquet
    files), `Overwrite`, and `Ignore` (do not save if the file exists already).
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的保存模式有 `ErrorIfExists`、`Append`（仅适用于 Parquet 文件）、`Overwrite` 和 `Ignore`（如果文件已存在则不保存）。
- en: Parquet files
  id: totrans-2134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 文件
- en: Apache Parquet is a popular file format well-suited for storing tabular data.
    It is often used for serialization in the Hadoop ecosystem, since it allows for
    efficient extraction of specific columns and rows without having to read the entire
    file.
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 是一种流行的文件格式，非常适合存储表格数据。它常用于 Hadoop 生态系统中的序列化，因为它允许在不读取整个文件的情况下高效地提取特定列和行。
- en: 'Serialization and deserialization of Parquet files is identical to JSON, with
    the substitution of `json` with `parquet`:'
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件的序列化和反序列化与 JSON 相同，只需将 `json` 替换为 `parquet`：
- en: '[PRE538]'
  id: totrans-2137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE538]'
- en: In general, Parquet will be more space-efficient than JSON for storing large
    collections of objects. Parquet is also much more efficient at retrieving specific
    columns or rows, if the partition can be inferred from the row. Parquet is thus
    advantageous over JSON unless you need the output to be human-readable, or de-serializable
    by an external program.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Parquet在存储大量对象集合时比JSON更节省空间。如果可以从行中推断出分区，Parquet在检索特定列或行时也更为高效。因此，除非您需要输出可由外部程序读取或反序列化，否则Parquet相对于JSON具有优势。
- en: Standalone programs
  id: totrans-2139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立程序
- en: 'So far, we have been using Spark SQL and DataFrames through the Spark shell.
    To use it in standalone programs, you will need to create it explicitly, from
    a Spark context:'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直通过Spark shell使用Spark SQL和DataFrame。要在独立程序中使用它，您需要从Spark上下文中显式创建它：
- en: '[PRE539]'
  id: totrans-2141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE539]'
- en: 'Additionally, importing the `implicits` object nested in `sqlContext` allows
    the conversions of RDDs to DataFrames:'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，导入嵌套在`sqlContext`中的`implicits`对象允许将RDD转换为DataFrame：
- en: '[PRE540]'
  id: totrans-2143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE540]'
- en: We will use DataFrames extensively in the next chapter to manipulate data to
    get it ready for use with MLlib.
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将广泛使用DataFrame来操纵数据，使其准备好与MLlib一起使用。
- en: Summary
  id: totrans-2145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored Spark SQL and DataFrames. DataFrames add a rich
    layer of abstraction on top of Spark's core engine, greatly facilitating the manipulation
    of tabular data. Additionally, the source API allows the serialization and de-serialization
    of DataFrames from a rich variety of data files.
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了Spark SQL和DataFrame。DataFrame在Spark的核心引擎之上增加了一层丰富的抽象层，极大地简化了表格数据的操作。此外，源API允许从丰富的数据文件中序列化和反序列化DataFrame。
- en: In the next chapter, we will build on our knowledge of Spark and DataFrames
    to build a spam filter using MLlib.
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将基于我们对Spark和DataFrame的知识来构建一个使用MLlib的垃圾邮件过滤器。
- en: References
  id: totrans-2148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'DataFrames are a relatively recent addition to Spark. There is thus still a
    dearth of literature and documentation. The first port of call should be the Scala
    docs, available at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame).'
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是Spark相对较新的功能。因此，相关的文献和文档仍然很少。首先应该查阅Scala文档，可在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)。
- en: 'The Scaladocs for operations available on the DataFrame `Column` type can be
    found at: [http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column).'
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame `Column`类型上可用的操作 Scaladocs 可在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column)。
- en: 'There is also extensive documentation on the Parquet file format: [https://parquet.apache.org](https://parquet.apache.org).'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Parquet文件格式的详细文档也可在以下网址找到：[https://parquet.apache.org](https://parquet.apache.org)。
- en: Chapter 12. Distributed Machine Learning with MLlib
  id: totrans-2152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。使用MLlib进行分布式机器学习
- en: 'Machine learning describes the construction of algorithms that make predictions
    from data. It is a core component of most data science pipelines, and is often
    seen to be the component adding the most value: the accuracy of the machine learning
    algorithm determines the success of the data science endeavor. It is also, arguably,
    the section of the data science pipeline that requires the most knowledge from
    fields beyond software engineering: a machine learning expert will be familiar,
    not just with algorithms, but also with statistics and with the business domain.'
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习描述了构建从数据中进行预测的算法。它是大多数数据科学流程的核心组件，通常被认为是增加最大价值的组件：机器学习算法的准确性决定了数据科学项目的成功。它还可能是数据科学流程中需要从软件工程以外的领域获取最多知识的部分：机器学习专家不仅熟悉算法，还熟悉统计学和业务领域。
- en: Choosing and tuning a machine learning algorithm to solve a particular problem
    involves significant exploratory analysis to try and determine which features
    are relevant, how features are correlated, whether there are outliers in the dataset,
    and so on. Designing suitable machine learning pipelines is difficult. Add on
    an additional layer of complexity resulting from the size of datasets and the
    need for scalability, and you have a real challenge.
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: 选择和调整机器学习算法来解决特定问题涉及大量的探索性分析，以尝试确定哪些特征是相关的，特征之间的相关性如何，数据集中是否存在异常值等等。设计合适的机器学习管道是困难的。再加上数据集的大小和可扩展性需求带来的额外复杂性，您就面临了一个真正的挑战。
- en: '**MLlib** helps mitigate this difficulty. MLlib is a component of Spark that
    provides machine learning algorithms on top of the core Spark libraries. It offers
    a set of learning algorithms that parallelize well over distributed datasets.'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLlib** 有助于缓解这种困难。MLlib 是 Spark 的一个组件，它提供了核心 Spark 库之上的机器学习算法。它提供了一套学习算法，这些算法在分布式数据集上并行化效果很好。'
- en: 'MLlib has evolved into two separate layers. MLlib itself contains the core
    algorithms, and **ml**, also called the *pipeline API*, defines an API for gluing
    algorithms together and provides a higher level of abstraction. The two libraries
    differ in the data types on which they operate: the original MLlib predates the
    introduction of DataFrames, and acts mainly on RDDs of feature vectors. The pipeline
    API operates on DataFrames.'
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 已经发展成两个独立的层。MLlib 本身包含核心算法，而 **ml**，也称为 *pipeline API*，定义了一个用于将算法粘合在一起的
    API，并提供了一个更高层次的抽象。这两个库在它们操作的数据类型上有所不同：原始的 MLlib 在 DataFrame 引入之前就已经存在，主要作用于特征向量的
    RDD。pipeline API 在 DataFrame 上操作。
- en: In this chapter, we will study the newer pipeline API, diving into MLlib only
    when the functionality is missing from the pipeline API.
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究较新的 pipeline API，只有在管道 API 缺少功能时才会深入研究 MLlib。
- en: This chapter does not try to teach the machine learning fundamentals behind
    the algorithms that we present. We assume that the reader has a good enough grasp
    of machine learning tools and techniques to understand, at least superficially,
    what the algorithms presented here do, and we defer to better authors for in-depth
    explanations of the mechanics of statistical learning (we present several references
    at the end of the chapter).
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章并不试图教授我们所展示算法背后的机器学习基础。我们假设读者对机器学习工具和技术有足够的了解，至少能够表面上理解这里展示的算法做什么，我们将深入解释统计学习机制的部分工作留给更好的作者（我们在本章末尾提供了一些参考文献）。
- en: MLlib is a rich library that is evolving rapidly. This chapter does not aim
    to give a complete overview of the library. We will work through the construction
    of a machine learning pipeline to train a spam filter, learning about the parts
    of MLlib that we need along the way. Having read this chapter, you will have an
    understanding of how the different parts of the library fit together, and can
    use the online documentation, or a more specialized book (see references at the
    end of this chapter) to learn about the parts of MLlib not covered here.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是一个快速发展的丰富库。本章的目标不是提供一个完整的库概述。我们将通过构建一个用于训练垃圾邮件过滤器的机器学习管道来工作，在这个过程中了解我们需要的
    MLlib 的各个部分。阅读完本章后，您将了解库的不同部分是如何结合在一起的，并且可以使用在线文档或更专业的书籍（请参阅本章末尾的参考文献）来了解这里未涵盖的
    MLlib 部分。
- en: Introducing MLlib – Spam classification
  id: totrans-2160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 MLlib – 垃圾邮件分类
- en: Let's introduce MLlib with a concrete example. We will look at spam classification
    using the Ling-Spam dataset that we used in the [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*. We will create a spam filter that uses logistic regression to estimate
    the probability that a given message is spam.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个具体的例子来介绍 MLlib。我们将查看使用我们在第 10 章[分布式批量处理与 Spark](part0097.xhtml#aid-2SG6I1
    "第 10 章。使用 Spark 进行分布式批量处理")中使用的 Ling-Spam 数据集进行的垃圾邮件分类，我们将创建一个使用逻辑回归来估计给定消息是否为垃圾邮件的概率的垃圾邮件过滤器。
- en: We will run through examples using the Spark shell, but you will find an analogous
    program in `LogisticRegressionDemo.scala` among the examples for this chapter.
    If you have not installed Spark, refer to [Chapter 10](part0097.xhtml#aid-2SG6I1
    "Chapter 10. Distributed Batch Processing with Spark"), *Distributed Batch Processing
    with Spark*, for installation instructions.
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过Spark shell运行示例，但你将在本章的示例中找到类似程序，在`LogisticRegressionDemo.scala`中。如果你还没有安装Spark，请参考第10章[分布式批量处理Spark](part0097.xhtml#aid-2SG6I1
    "第10章。使用Spark的分布式批量处理")，获取安装说明。
- en: Let's start by loading the e-mails in the Ling-Spam dataset. If you have not
    done this for [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark*, download
    the data from [data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)
    or [data.scala4datascience.com/ling-spam.zip](http://data.scala4datascience.com/ling-spam.zip),
    depending on whether you want a `tar.gz` file or a `zip` file, and unpack the
    archive. This will create a `spam` directory and a `ham` directory containing
    spam and ham messages, respectively.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载Ling-Spam数据集中的电子邮件开始。如果你在第10章[分布式批量处理Spark](part0097.xhtml#aid-2SG6I1
    "第10章。使用Spark的分布式批量处理")中没有这样做，请从[data.scala4datascience.com/ling-spam.tar.gz](http://data.scala4datascience.com/ling-spam.tar.gz)或[data.scala4datascience.com/ling-spam.zip](http://data.scala4datascience.com/ling-spam.zip)下载数据，根据你想要`tar.gz`文件还是`zip`文件来选择，然后解压存档。这将创建一个`spam`目录和一个`ham`目录，分别包含垃圾邮件和正常邮件。
- en: 'Let''s use the `wholeTextFiles` method to load spam and ham e-mails:'
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`wholeTextFiles`方法来加载垃圾邮件和正常邮件：
- en: '[PRE541]'
  id: totrans-2165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE541]'
- en: 'The `wholeTextFiles` method creates a key-value RDD where the keys are the
    file names and the values are the contents of the files:'
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: '`wholeTextFiles`方法创建一个键值RDD，其中键是文件名，值是文件内容：'
- en: '[PRE542]'
  id: totrans-2167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE542]'
- en: 'The algorithms in the pipeline API work on DataFrames. We must therefore convert
    our key-value RDDs to DataFrames. We define a new case class, `LabelledDocument`,
    which contains a message text and a category label identifying whether a message
    is `spam` or `ham`:'
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: 管道API中的算法在DataFrame上工作。因此，我们必须将我们的键值RDD转换为DataFrame。我们定义一个新的case class，`LabelledDocument`，它包含一个消息文本和一个类别标签，用于标识消息是`spam`还是`ham`：
- en: '[PRE543]'
  id: totrans-2169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE543]'
- en: 'To create models, we will need all the documents in a single DataFrame. Let''s
    therefore take the union of our two `LabelledDocument` RDDs, and transform that
    to a DataFrame. The `union` method concatenates RDDs together:'
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建模型，我们需要将所有文档放入一个DataFrame中。因此，我们将两个`LabelledDocument` RDD合并，并将其转换为DataFrame。`union`方法将RDD连接起来：
- en: '[PRE544]'
  id: totrans-2171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE544]'
- en: Let's do some basic checks to verify that we have loaded all the documents.
    We start by persisting the DataFrame in memory to avoid having to re-create it
    from the raw text files.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一些基本的检查来验证我们已经加载了所有文档。我们首先将DataFrame保存在内存中，以避免需要从原始文本文件中重新创建它。
- en: '[PRE545]'
  id: totrans-2173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE545]'
- en: Let's now split the DataFrame into a training set and a test set. We will use
    the test set to validate the model that we build. For now, we will just use a
    single split, training the model on 70% of the data and testing it on the remaining
    30%. In the next section, we will look at cross-validation, which provides more
    rigorous way to check the accuracy of our models.
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将DataFrame分割成训练集和测试集。我们将使用测试集来验证我们构建的模型。现在，我们将只使用一个分割，用70%的数据训练模型，用剩余的30%进行测试。在下一节中，我们将探讨交叉验证，它提供了一种更严格的方式来检查我们模型的准确性。
- en: 'We can achieve this 70-30 split using the DataFrame''s `.randomSplit` method:'
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用DataFrame的`.randomSplit`方法实现70-30的分割：
- en: '[PRE546]'
  id: totrans-2176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE546]'
- en: 'The `.randomSplit` method takes an array of weights and returns an array of
    DataFrames, of approximately the size specified by the weights. For instance,
    we passed weights `0.7` and `0.3`, indicating that any given row has a 70% chance
    of ending up in `trainDF`, and a 30% chance of ending up in `testDF`. Note that
    this means the split DataFrames are not of fixed size: `trainDF` is approximately,
    but not exactly, 70% the size of `documentsDF`:'
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: '`.randomSplit`方法接受一个权重数组，并返回一个DataFrame数组，其大小大约由权重指定。例如，我们传递了权重`0.7`和`0.3`，表示任何给定行有70%的机会最终进入`trainDF`，有30%的机会进入`testDF`。请注意，这意味着分割的DataFrame大小不是固定的：`trainDF`大约是`documentsDF`的70%，但不是正好70%：'
- en: '[PRE547]'
  id: totrans-2178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE547]'
- en: If you need a fixed size sample, use the DataFrame's `.sample` method to obtain
    `trainDF` and filter `documentDF` for rows not in `trainDF`.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个固定大小的样本，请使用DataFrame的`.sample`方法来获取`trainDF`，并过滤`documentDF`以排除`trainDF`中的行。
- en: 'We are now in a position to start using MLlib. Our attempt at classification
    will involve performing logistic regression on *term-frequency vectors*: we will
    count how often each word appears in each message, and use the frequency of occurrence
    as a feature. Before jumping into the code, let''s take a step back and discuss
    the structure of machine learning pipelines.'
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始使用MLlib了。我们的分类尝试将涉及在*词频向量*上执行逻辑回归：我们将计算每个单词在每个消息中出现的频率，并使用发生频率作为特征。在深入代码之前，让我们退一步来讨论机器学习管道的结构。
- en: Pipeline components
  id: totrans-2181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道组件
- en: 'Pipelines consist of a set of components joined together such that the DataFrame
    produced by one component is used as input for the next component. The components
    available are split into two classes: *transformers* and *estimators*.'
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
  zh: 管道由一系列组件组成，这些组件连接在一起，使得一个组件生成的DataFrame被用作下一个组件的输入。可用的组件分为两类：*转换器*和*估计器*。
- en: Transformers
  id: totrans-2183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器
- en: '**Transformers** transform one DataFrame into another, normally by appending
    one or more columns.'
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换器**将一个DataFrame转换成另一个，通常是通过添加一个或多个列。'
- en: 'The first step in our spam classification algorithm is to split each message
    into an array of words. This is called **tokenization**. We can use the `Tokenizer`
    transformer, provided by MLlib:'
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们垃圾邮件分类算法的第一步是将每个消息分割成一个单词数组。这被称为**分词**。我们可以使用MLlib提供的`Tokenizer`转换器：
- en: '[PRE548]'
  id: totrans-2186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE548]'
- en: 'The behavior of transformers can be customized through getters and setters.
    The easiest way of obtaining a list of the parameters available is to call the
    `.explainParams` method:'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过获取器和设置器来定制转换器的行为。获取可用参数列表的最简单方法是通过调用`.explainParams`方法：
- en: '[PRE549]'
  id: totrans-2188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE549]'
- en: 'We see that the behavior of a `Tokenizer` instance can be customized using
    two parameters: `inputCol` and `outputCol`, describing the header of the column
    containing the input (the string to be tokenized) and the output (the array of
    words), respectively. We can set these parameters using the `setInputCol` and
    `setOutputCol` methods.'
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，可以使用两个参数来定制`Tokenizer`实例的行为：`inputCol`和`outputCol`，分别描述包含输入（要分词的字符串）和输出（单词数组）的列的标题。我们可以使用`setInputCol`和`setOutputCol`方法设置这些参数。
- en: 'We set `inputCol` to `"text"`, since that is what the column is called in our
    training and test DataFrames. We will set `outputCol` to `"words"`:'
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`inputCol`设置为`"text"`，因为在我们的训练和测试DataFrame中，该列被命名为`text`。我们将`outputCol`设置为`"words"`：
- en: '[PRE550]'
  id: totrans-2191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE550]'
- en: In due course, we will integrate `tokenizer` into a pipeline, but, for now,
    let's just use it to transform the training DataFrame, to verify that it works
    correctly.
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当的时候，我们将`tokenizer`集成到管道中，但现在，让我们只使用它来转换训练DataFrame，以验证其是否正确工作。
- en: '[PRE551]'
  id: totrans-2193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE551]'
- en: The `tokenizer` transformer produces a new DataFrame with an additional column,
    `words`, containing an array of the words in the `text` column.
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenizer`转换器生成一个新的DataFrame，其中包含一个额外的列`words`，包含`text`列中的单词数组。'
- en: Clearly, we can use our `tokenizer` to transform any DataFrame with the correct
    schema. We could, for instance, use it on the test set. Much of machine learning
    involves calling the same (or a very similar) pipeline on different data sets.
    By providing the pipeline abstraction, MLlib facilitates reasoning about complex
    machine learning algorithms consisting of many cleaning, transformation, and modeling
    components.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们可以使用我们的`tokenizer`来转换具有正确模式的任何DataFrame。例如，我们可以将其用于测试集。机器学习的许多方面都涉及到在不同的数据集上调用相同的（或非常相似的）管道。通过提供管道抽象，MLlib简化了由许多清理、转换和建模组件组成的复杂机器学习算法的推理。
- en: The next step in our pipeline is to calculate the frequency of occurrence of
    each word in each message. We will eventually use these frequencies as features
    in our algorithm. We will use the `HashingTF` transformer to transform from arrays
    of words to word frequency vectors for each message.
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道中的下一步是计算每个消息中每个单词的出现频率。我们最终将使用这些频率作为算法中的特征。我们将使用`HashingTF`转换器将单词数组转换为每个消息的词频向量。
- en: The `HashingTF` transformer constructs a sparse vector of word frequencies from
    input iterables. Each element in the word array gets transformed to a hash code.
    This hash code is truncated to a value between *0* and a large number *n*, the
    total number of elements in the output vector. The term frequency vector is just
    the number of occurrences of the truncated hash.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingTF`转换器从输入可迭代对象构建一个词频的稀疏向量。单词数组中的每个元素都被转换为一个哈希码。这个哈希码被截断为一个介于*0*和输出向量中元素总数*大数n*之间的值。词频向量仅仅是截断哈希的出现次数。'
- en: 'Let''s run through an example manually to understand how this works. We will
    calculate the term frequency vector for `Array("the", "dog", "jumped", "over",
    "the")`. Let''s set *n*, the number of elements in the sparse output vector, to
    16 for this example. The first step is to calculate the hash code for each element
    in our array. We can use the built-in `##` method, which calculates a hash code
    for any object:'
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动运行一个示例来理解它是如何工作的。我们将计算`Array("the", "dog", "jumped", "over", "the")`的词频向量。在这个例子中，我们将稀疏输出向量中的元素数量*n*设为16。第一步是计算数组中每个元素的哈希码。我们可以使用内置的`##`方法，该方法为任何对象计算哈希码：
- en: '[PRE552]'
  id: totrans-2199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE552]'
- en: 'To transform the hash codes into valid vector indices, we take the modulo of
    each hash by the size of the vector (`16`, in this case):'
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将哈希码转换为有效的向量索引，我们将每个哈希码对向量的大小（在这种情况下为`16`）取模：
- en: '[PRE553]'
  id: totrans-2201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE553]'
- en: 'We can then create a mapping from indices to the number of times that index
    appears:'
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个从索引到该索引出现次数的映射：
- en: '[PRE554]'
  id: totrans-2203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE554]'
- en: 'Finally, we can convert this map to a sparse vector, where the value at each
    element in the vector is the frequency with which this particular index occurs:'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将此映射转换为稀疏向量，其中向量的每个元素的值是此特定索引出现的频率：
- en: '[PRE555]'
  id: totrans-2205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE555]'
- en: 'Note that the `.toString` output for a sparse vector consists of three elements:
    the total size of the vector, followed by two lists: the first is a series of
    indices, and the second is a series of values at those indices.'
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，稀疏向量的`.toString`输出由三个元素组成：向量的总大小，后跟两个列表：第一个是索引系列，第二个是这些索引处的值系列。
- en: 'Using a sparse vector provides a compact and efficient way of representing
    the frequency of occurrence of words in the message, and is exactly how `HashingTF`
    works under the hood. The disadvantage is that the mapping from words to indices
    is not necessarily unique: truncating hash codes by the length of the vector will
    map different strings to the same index. This is known as a *collision*. The solution
    is to make *n* large enough that the frequency of collisions is minimized.'
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稀疏向量提供了一种紧凑且高效的方式来表示消息中单词出现的频率，这正是`HashingTF`在底层的工作方式。缺点是，从单词到索引的映射不一定唯一：通过向量的长度截断哈希码将不同的字符串映射到相同的索引。这被称为*碰撞*。解决方案是使*n*足够大，以最小化碰撞的频率。
- en: Tip
  id: totrans-2208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '`HashingTF` is similar to building a hash table (for example, a Scala map)
    whose keys are words and whose values are the number of times that word occurs
    in the message, with one important difference: it does not attempt to deal with
    hash collisions. Thus, if two words map to the same hash, they will have the wrong
    frequency. There are two advantages to using this algorithm over just constructing
    a hash table:'
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingTF`类似于构建一个哈希表（例如，Scala映射），其键是单词，值是单词在消息中出现的次数，但有一个重要区别：它不试图处理哈希冲突。因此，如果两个单词映射到相同的哈希，它们将具有错误的频率。使用此算法而不是仅构建哈希表有两个优点：'
- en: We do not have to maintain a list of distinct words in memory.
  id: totrans-2210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要在内存中维护一个不同单词的列表。
- en: 'Each e-mail can be transformed to a vector independently of all others: we
    do not have to reduce over different partitions to get the set of keys in the
    map. This greatly eases applying this algorithm to each e-mail in a distributed
    manner, since we can apply the `HashingTF` transformation on each partition independently.'
  id: totrans-2211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每封电子邮件都可以独立于其他所有电子邮件转换为向量：我们不需要在不同的分区上执行降维操作来获取映射中的键集。这极大地简化了将此算法应用于分布式环境中的每封电子邮件，因为我们可以在每个分区上独立应用`HashingTF`转换。
- en: The main disadvantage is that we must use machine learning algorithms that can
    take advantage of the sparse representation efficiently. This is the case with
    logistic regression, which we will use here.
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: 主要缺点是我们必须使用能够高效利用稀疏表示的机器学习算法。这种情况适用于逻辑回归，我们将在下面使用。
- en: As you might expect, the `HashingTF` transformer takes, as parameters, the input
    and output columns. It also takes a parameter defining the number of distinct
    hash buckets in the vector. Increasing the number of buckets decreases the number
    of collisions. In practice, a value between ![Transformers](img/image01214.jpeg)
    and ![Transformers](img/image01215.jpeg) is recommended.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期的那样，`HashingTF`转换器接受输入和输出列作为参数。它还接受一个参数，定义向量中不同的哈希桶的数量。增加桶的数量会减少冲突的数量。在实践中，建议的值在![Transformers](img/image01214.jpeg)和![Transformers](img/image01215.jpeg)之间。
- en: '[PRE556]'
  id: totrans-2214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE556]'
- en: 'Each element in the `features` column is a sparse vector:'
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: '`features`列中的每个元素都是一个稀疏向量：'
- en: '[PRE557]'
  id: totrans-2216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE557]'
- en: 'We can thus interpret our vector as: the word that hashes to element `33` occurs
    three times, the word that hashes to element `36` occurs four times etc.'
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将我们的向量解释为：哈希到元素`33`的单词出现三次，哈希到元素`36`的单词出现四次等等。
- en: Estimators
  id: totrans-2218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计器
- en: 'We now have the features ready for logistic regression. The last step prior
    to running logistic regression is to create the target variable. We will transform
    the `category` column in our DataFrame to a binary 0/1 target column. Spark provides
    a `StringIndexer` class that replaces a set of strings in a column with doubles.
    A `StringIndexer` is not a transformer: it must first be ''fitted'' to a set of
    categories to calculate the mapping from string to numeric value. This introduces
    the second class of components in the pipeline API: *estimators*.'
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为逻辑回归准备好了特征。在运行逻辑回归之前，最后一步是创建目标变量。我们将DataFrame中的`category`列转换为二进制0/1目标列。Spark提供了一个`StringIndexer`类，该类将列中的字符串集替换为双精度浮点数。`StringIndexer`不是一个转换器：它必须首先与一组类别拟合以计算从字符串到数值值的映射。这引入了管道API中的第二类组件：*估计器*。
- en: Unlike a transformer, which works "out of the box", an estimator must be fitted
    to a DataFrame. For our string indexer, the fitting process involves obtaining
    the list of unique strings (`"spam"` and `"ham"`) and mapping each of these to
    a double. The fitting process outputs a transformer which can be used on subsequent
    DataFrames.
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
  zh: 与“开箱即用”的转换器不同，估计器必须与DataFrame拟合。对于我们的字符串索引器，拟合过程包括获取唯一字符串列表（`"spam"`和`"ham"`）并将每个这些映射到双精度浮点数。拟合过程输出一个转换器，该转换器可以用于后续的DataFrames。
- en: '[PRE558]'
  id: totrans-2221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE558]'
- en: 'The transformer produced by the fitting process has a `labels` attribute describing
    the mapping it applies:'
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合过程产生的转换器有一个`labels`属性，描述了它应用的映射：
- en: '[PRE559]'
  id: totrans-2223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE559]'
- en: 'Each label will get mapped to its index in the array: thus, our transformer
    maps `ham` to `0` and `spam` to `1`:'
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标签都将映射到数组中的索引：因此，我们的转换器将`ham`映射到`0`，将`spam`映射到`1`：
- en: '[PRE560]'
  id: totrans-2225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE560]'
- en: 'We now have the feature vectors and classification labels in the correct format
    for logistic regression. The component for performing logistic regression is an
    estimator: it is fitted to a training DataFrame to create a trained model. The
    model can then be used to transform test DataFrames.'
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了适合逻辑回归的正确格式的特征向量和分类标签。执行逻辑回归的组件是一个估计器：它被拟合到一个训练DataFrame中，以创建一个训练好的模型。然后，可以使用该模型来转换测试DataFrame。
- en: '[PRE561]'
  id: totrans-2227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE561]'
- en: 'The `LogisticRegression` estimator expects the feature column to be named `"features"`
    and the label column (the target) to be named `"label"`, by default. There is
    no need to set these explicitly, since they match the column names set by `hashingTF`
    and `indexer`. There are several parameters that can be set to control how logistic
    regression works:'
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression`估计器默认期望特征列命名为`"features"`，标签列（目标）命名为`"label"`。没有必要明确设置这些，因为它们与`hashingTF`和`indexer`设置的列名匹配。有几个参数可以设置以控制逻辑回归的工作方式：'
- en: '[PRE562]'
  id: totrans-2229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE562]'
- en: 'For now, we just set the `maxIter` parameter. We will look at the effect of
    other parameters, such as regularization, later on. Let''s now fit the classifier
    to `labelledDF`:'
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只设置`maxIter`参数。稍后我们将研究其他参数的影响，例如正则化。现在，让我们将分类器拟合到`labelledDF`：
- en: '[PRE563]'
  id: totrans-2231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE563]'
- en: 'This produces a transformer that we can use on a DataFrame with a `features`
    column. The transformer appends a `prediction` column and a `probability` column.
    We can, for instance use `trainedClassifier` to transform `labelledDF`, the training
    set itself:'
  id: totrans-2232
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了一个转换器，我们可以将其应用于具有`features`列的DataFrame。转换器附加了一个`prediction`列和一个`probability`列。例如，我们可以使用`trainedClassifier`将`labelledDF`（训练集本身）转换：
- en: '[PRE564]'
  id: totrans-2233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE564]'
- en: 'A quick way of checking the performance of our model is to just count the number
    of misclassified messages:'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们模型性能的一个快速方法是仅计算误分类消息的数量：
- en: '[PRE565]'
  id: totrans-2235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE565]'
- en: In this case, logistic regression managed to correctly classify every message
    but one in the training set. This is perhaps unsurprising, given the large number
    of features and the relatively clear demarcation between the words used in spam
    and legitimate e-mails.
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，逻辑回归成功地将训练集中除了一条消息外的所有消息正确分类。考虑到特征数量众多，以及垃圾邮件和合法电子邮件中使用的单词之间的相对清晰界限，这也许并不令人惊讶。
- en: 'Of course, the real test of a model is not how well it performs on the training
    set, but how well it performs on a test set. To test this, we could just push
    the test DataFrame through the same stages that we used to train the model, replacing
    estimators with the fitted transformer that they produced. MLlib provides the
    *pipeline* abstraction to facilitate this: we wrap an ordered list of transformers
    and estimators in a pipeline. This pipeline is then fitted to a DataFrame corresponding
    to the training set. The fitting produces a `PipelineModel` instance, equivalent
    to the pipeline but with estimators replaced by transformers, as shown in this
    diagram:'
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对模型的真实测试不是它在训练集上的表现，而是在测试集上的表现。为了测试这一点，我们可以将测试DataFrame通过与我们用于训练模型的相同阶段，用它们产生的拟合转换器替换估计器。MLlib提供了*管道*抽象来简化这个过程：我们将有序列表的转换器和估计器包装在管道中。然后，这个管道被拟合到一个对应于训练集的DataFrame中。拟合产生一个`PipelineModel`实例，相当于管道，但估计器被转换器替换，如图所示：
- en: '![Estimators](img/image01216.jpeg)'
  id: totrans-2238
  prefs: []
  type: TYPE_IMG
  zh: '![估计器](img/image01216.jpeg)'
- en: 'Let''s construct the pipeline for our logistic regression spam filter:'
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的逻辑回归垃圾邮件过滤器的管道：
- en: '[PRE566]'
  id: totrans-2240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE566]'
- en: 'Once the pipeline is defined, we fit it to the DataFrame holding the training
    set:'
  id: totrans-2241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了管道，我们就将其拟合到包含训练集的DataFrame中：
- en: '[PRE567]'
  id: totrans-2242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE567]'
- en: 'When fitting a pipeline to a DataFrame, estimators and transformers are treated
    differently:'
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
  zh: 当将管道拟合到DataFrame时，估计器和转换器被处理得不同：
- en: Transformers are applied to the DataFrame and copied, as is, into the pipeline
    model.
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器被应用到DataFrame中，并直接复制到管道模型中。
- en: Estimators are fitted to the DataFrame, producing a transformer. The transformer
    is then applied to the DataFrame, and appended to the pipeline model.
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器被拟合到DataFrame中，生成一个转换器。然后，转换器被应用到DataFrame上，并附加到管道模型中。
- en: 'We can now apply the pipeline model to the test set:'
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将管道模型应用到测试集上：
- en: '[PRE568]'
  id: totrans-2247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE568]'
- en: 'This has added a `prediction` column to the DataFrame with the predictions
    of our logistic regression model. To measure the performance of our algorithm,
    we calculate the classification error on the test set:'
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
  zh: 这在DataFrame中添加了一个`prediction`列，其中包含我们的逻辑回归模型的预测结果。为了衡量我们算法的性能，我们在测试集上计算分类错误：
- en: '[PRE569]'
  id: totrans-2249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE569]'
- en: Thus, our naive logistic regression algorithm, with no model selection, or regularization,
    mis-classifies 2.3% of e-mails. You may, of course, get slightly different results,
    since the train-test split was random.
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的朴素逻辑回归算法，没有模型选择或正则化，将2.3%的电子邮件误分类。当然，由于训练集和测试集的划分是随机的，你可能会得到略微不同的结果。
- en: 'Let''s save the training and test DataFrames, with predictions, as `parquet`
    files:'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将包含预测的培训和测试DataFrame保存为`parquet`文件：
- en: '[PRE570]'
  id: totrans-2252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE570]'
- en: Tip
  id: totrans-2253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In spam classification, a false positive is considerably worse than a false
    negative: it is much worse to classify a legitimate message as spam, than it is
    to let a spam message through. To account for this, we could increase the threshold
    for classification: only messages that score, for instance, 0.7 or above would
    get classified as spam. This raises the obvious question of choosing the right
    threshold. One way to do this would be to investigate the false positive rate
    incurred in the test set for different thresholds, and choosing the lowest threshold
    to give us an acceptable false positive rate. A good way of visualizing this is
    to use ROC curves, which we will investigate in the next section.'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: 在垃圾邮件分类中，误报比漏报要严重得多：将合法邮件误判为垃圾邮件，比让垃圾邮件通过要糟糕得多。为了解决这个问题，我们可以提高分类的阈值：只有得分达到0.7或以上的邮件才会被分类为垃圾邮件。这引发了选择正确阈值的问题。一种方法是对不同阈值在测试集上产生的误报率进行调查，并选择最低的阈值以获得可接受的误报率。可视化这一点的良好方法是使用ROC曲线，我们将在下一节中探讨。
- en: Evaluation
  id: totrans-2255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Unfortunately, the functionality for evaluating model quality in the pipeline
    API remains limited, as of version 1.5.2\. Logistic regression does output a summary
    containing several evaluation metrics (available through the `summary` attribute
    on the trained model), but these are calculated on the training set. In general,
    we want to evaluate the performance of the model both on the training set and
    on a separate test set. We will therefore dive down to the underlying MLlib layer
    to access evaluation metrics.
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，截至版本1.5.2，管道API中评估模型质量的功能仍然有限。逻辑回归确实输出一个包含多个评估指标（通过训练模型的`summary`属性可用）的摘要，但这些是在训练集上计算的。通常，我们希望在训练集和单独的测试集上评估模型性能。因此，我们将深入到底层的MLlib层以访问评估指标。
- en: MLlib provides a module, `org.apache.spark.mllib.evaluation`, with a set of
    classes for assessing the quality of a model. We will use the `BinaryClassificationMetrics`
    class here, since spam classification is a binary classification problem. Other
    evaluation classes provide metrics for multi-class models, regression models and
    ranking models.
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib提供了一个模块，`org.apache.spark.mllib.evaluation`，其中包含一系列用于评估模型质量的类。在这里，我们将使用`BinaryClassificationMetrics`类，因为垃圾邮件分类是一个二分类问题。其他评估类为多分类模型、回归模型和排序模型提供指标。
- en: 'As in the previous section, we will illustrate the concepts in the shell, but
    you will find analogous code in the `ROC.scala` script in the code examples for
    this chapter. We will use *breeze-viz* to plot curves, so, when starting the shell,
    we must ensure that the relevant libraries are on the classpath. We will use SBT
    assembly, as described in [Chapter 10](part0097.xhtml#aid-2SG6I1 "Chapter 10. Distributed
    Batch Processing with Spark"), *Distributed Batch Processing with Spark* (specifically,
    the *Building and running standalone programs* section), to create a JAR with
    the required dependencies. We will then pass this JAR to the Spark shell, allowing
    us to import breeze-viz. Let''s write a `build.sbt` file that declares a dependency
    on breeze-viz:'
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将在shell中阐述这些概念，但您将在本章代码示例中的`ROC.scala`脚本中找到类似代码。我们将使用*breeze-viz*来绘制曲线，因此，在启动shell时，我们必须确保相关的库在类路径上。我们将使用SBT
    assembly，如第10章[分布式批处理与Spark](part0097.xhtml#aid-2SG6I1 "第10章。使用Spark进行分布式批处理")中所述，*分布式批处理与Spark*（特别是*构建和运行独立程序*部分），来创建一个包含所需依赖项的JAR文件。然后我们将这个JAR文件传递给Spark
    shell，这样我们就可以导入breeze-viz。让我们编写一个`build.sbt`文件，声明对breeze-viz的依赖：
- en: '[PRE571]'
  id: totrans-2259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE571]'
- en: 'Package the dependencies into a jar with:'
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令将依赖项打包到jar中：
- en: '[PRE572]'
  id: totrans-2261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE572]'
- en: 'This will create a jar called `spam_filter-assembly-0.1-SNAPSHOT.jar` in the
    `target/scala-2.10`/ directory. To include this jar in the Spark shell, re-start
    the shell with the `--jars` command line argument:'
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`target/scala-2.10`目录下创建一个名为`spam_filter-assembly-0.1-SNAPSHOT.jar`的JAR文件。要将此JAR文件包含在Spark
    shell中，请使用`--jars`命令行参数重新启动shell：
- en: '[PRE573]'
  id: totrans-2263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE573]'
- en: 'To verify that the packaging worked correctly, try to import `breeze.plot`:'
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证打包是否成功，尝试导入`breeze.plot`：
- en: '[PRE574]'
  id: totrans-2265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE574]'
- en: 'Let''s load the test set, with predictions, which we created in the previous
    section and saved as a `parquet` file:'
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载测试集，包括预测，我们在上一节中创建并保存为`parquet`文件：
- en: '[PRE575]'
  id: totrans-2267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE575]'
- en: 'The `BinaryClassificationMetrics` object expects an `RDD[(Double, Double)]`
    object of pairs of scores (the probability assigned by the classifier that a particular
    e-mail is spam) and labels (whether an e-mail is actually spam). We can extract
    this RDD from our DataFrame:'
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: '`BinaryClassificationMetrics`对象期望一个`RDD[(Double, Double)]`对象，其中包含一对分数（分类器分配给特定电子邮件是垃圾邮件的概率）和标签（电子邮件是否实际上是垃圾邮件）。我们可以从我们的DataFrame中提取这个RDD：'
- en: '[PRE576]'
  id: totrans-2269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE576]'
- en: 'We can now construct the `BinaryClassificationMetrics` instance:'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建`BinaryClassificationMetrics`实例：
- en: '[PRE577]'
  id: totrans-2271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE577]'
- en: The `BinaryClassificationMetrics` objects contain many useful metrics for evaluating
    the performance of a classification model. We will look at the **receiver operating**
    **characteristic** (**ROC**) curve.
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
  zh: '`BinaryClassificationMetrics` 对象包含许多用于评估分类模型性能的有用指标。我们将探讨**接收者操作****特征**（**ROC**）曲线。'
- en: Tip
  id: totrans-2273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**ROC Curves**'
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC曲线**'
- en: 'Imagine gradually decreasing, from 1.0, the probability threshold at which
    we assume a particular e-mail is spam. Clearly, when the threshold is set to 1.0,
    no e-mails will get classified as spam. This means that there will be no **false
    positives** (ham messages which we incorrectly classify as spam), but it also
    means that there will be no **true positives** (spam messages that we correctly
    identify as spam): all spam e-mails will be incorrectly identified as ham.'
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
  zh: 想象逐渐降低，从1.0开始，我们假设特定电子邮件是垃圾邮件的概率阈值。显然，当阈值设置为1.0时，没有电子邮件会被分类为垃圾邮件。这意味着不会有**假阳性**（我们错误地将正常邮件分类为垃圾邮件），但也意味着不会有**真阳性**（我们正确地将垃圾邮件识别为垃圾邮件）：所有垃圾邮件都会被错误地识别为正常邮件。
- en: As we gradually lower the probability threshold at which we assume a particular
    e-mail is spam, our spam filter will, hopefully, start identifying a large fraction
    of e-mails as spam. The vast majority of these will, if our algorithm is well-designed,
    be real spam. Thus, our rate of true positives increases. As we gradually lower
    the threshold, we start classifying messages about which we are less sure of as
    spam. This will increase the number of messages correctly identified as spam,
    but it will also increase the number of false positives.
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们逐渐降低我们假设特定电子邮件是垃圾邮件的概率阈值，我们的垃圾邮件过滤器，希望如此，将开始识别大量电子邮件为垃圾邮件。其中绝大多数，如果我们的算法设计得很好，将是真正的垃圾邮件。因此，我们的真正例率增加。随着我们逐渐降低阈值，我们开始将我们不太确定的邮件分类为垃圾邮件。这将增加正确识别为垃圾邮件的邮件数量，但也会增加误报的数量。
- en: 'The ROC curve plots, for each threshold value, the fraction of true positives
    against the fraction of false positives. In the best case, the curve is always
    1: this happens when all spam messages are given a score of 1.0, and all ham messages
    are given a score of 0.0\. By contrast, the worst case happens when the curve
    is a diagonal *P(true positive) = P(false positive)*, which occurs when our algorithm
    does no better than random. In general, ROC curves fall somewhere in between,
    forming a convex shell above the diagonal. The deeper this shell, the better our
    algorithm.'
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线图对于每个阈值值，绘制真正例率与假正例率的比率。在最佳情况下，曲线始终为 1：这发生在所有垃圾邮件消息都被赋予 1.0 分，而所有正常邮件都被赋予
    0.0 分时。相比之下，最坏的情况发生在曲线为对角线 *P(真正例) = P(假正例)* 时，这发生在我们的算法不如随机时。通常，ROC 曲线位于两者之间，形成一个位于对角线之上的凸壳。这个壳越深，我们的算法就越好。
- en: '![Evaluation](img/image01217.jpeg)'
  id: totrans-2278
  prefs: []
  type: TYPE_IMG
  zh: '![评估](img/image01217.jpeg)'
- en: '(left) ROC curve for a model performing much better than random: the curve
    reaches very high true positive rates for a low false positive rate.'
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
  zh: （左）对于一个明显优于随机性能的模型的 ROC 曲线：曲线在低误报率下达到非常高的真正例率。
- en: (middle) ROC curve for a model performing significantly better than random.
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
  zh: （中间）对于一个显著优于随机性能的模型的 ROC 曲线。
- en: '(right) ROC curve for a model performing only marginally better than random:
    the true positive rate is only marginally larger than the rate of false positives,
    for any given threshold, meaning that nearly half the examples are misclassified.'
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
  zh: （右）对于一个仅略优于随机性能的模型：对于任何给定的阈值，真正例率仅略高于假正例率，这意味着近一半的示例被错误分类。
- en: 'We can calculate an array of points on the ROC curve using the `.roc` method
    on our `BinaryClassificationMetrics` instance. This returns an `RDD[(Double, Double)]`
    of (*false positive*, *true positive*) fractions for each threshold value. We
    can collect this as an array:'
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `BinaryClassificationMetrics` 实例上的 `.roc` 方法计算 ROC 曲线上的点数组。这返回一个 `RDD[(Double,
    Double)]`，包含每个阈值值的 (*假正例*，*真正例*) 比率。我们可以将其收集为数组：
- en: '[PRE578]'
  id: totrans-2283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE578]'
- en: 'Of course, an array of numbers is not very enlightening, so let''s plot the
    ROC curve with breeze-viz. We start by transforming our array of pairs into two
    arrays, one of false positives and one of true positives:'
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一个数字数组并不很有启发性，所以让我们用 breeze-viz 绘制 ROC 曲线。我们首先将我们的配对数组转换为两个数组，一个为假正例，一个为真正例：
- en: '[PRE579]'
  id: totrans-2285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE579]'
- en: 'Let''s plot these two arrays:'
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这两个数组：
- en: '[PRE580]'
  id: totrans-2287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE580]'
- en: 'The ROC curve hits *1.0* for a small value of x: that is, we retrieve all true
    positives at the cost of relatively few false positives. To visualize the curve
    more accurately, it is instructive to limit the range on the *x*-axis from *0*
    to *0.1*.'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线在 x 的一个较小值时达到 *1.0*：也就是说，我们以相对较少的误报为代价检索到所有真正例。为了更准确地可视化曲线，限制 *x* 轴的范围从
    *0* 到 *0.1* 是有益的。
- en: '[PRE581]'
  id: totrans-2289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE581]'
- en: 'We also need to tell breeze-viz to use appropriate tick spacing, which requires
    going down to the JFreeChart layer underlying breeze-viz:'
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要告诉 breeze-viz 使用适当的刻度间隔，这需要深入到 breeze-viz 之下的 JFreeChart 层：
- en: '[PRE582]'
  id: totrans-2291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE582]'
- en: 'We can now save the graph:'
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以保存这个图表：
- en: '[PRE583]'
  id: totrans-2293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE583]'
- en: 'This produces the following graph, stored in `roc.png`:'
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表，存储在 `roc.png` 中：
- en: '![Evaluation](img/image01218.jpeg)'
  id: totrans-2295
  prefs: []
  type: TYPE_IMG
  zh: '![评估](img/image01218.jpeg)'
- en: ROC curve for spam classification with logistic regression. Note that we have
    limited the false positive axis at 0.1
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行垃圾邮件分类的 ROC 曲线。注意，我们已经将假正例轴限制在 0.1
- en: By looking at the graph, we see that we can filter out 85% of spam without a
    single **false positive**. Of course, we would need a larger test set to really
    validate this assumption.
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察图表，我们看到我们可以过滤掉 85% 的垃圾邮件而没有单个 **误报**。当然，我们需要一个更大的测试集来真正验证这个假设。
- en: 'A graph is useful to really understand the behavior of a model. Sometimes,
    however, we just want to have a single measure of the quality of a model. The
    area under the ROC curve can be a good such metric:'
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
  zh: 图形有助于真正理解模型的行为。有时，我们只是想有一个衡量模型质量的单一指标。ROC曲线下的面积可以是一个很好的这样的指标：
- en: '[PRE584]'
  id: totrans-2299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE584]'
- en: 'This can be interpreted as follows: given any two messages randomly drawn from
    the test set, one of which is ham, and one of which is spam, there is a 99.8%
    probability that the model assigned a greater likelihood of spam to the spam message
    than to the ham message.'
  id: totrans-2300
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以解释如下：给定从测试集中随机抽取的两个消息，其中一个为垃圾邮件，另一个为正常邮件，模型将垃圾邮件分配给垃圾邮件消息的似然性大于正常邮件消息的似然性的概率为99.8%。
- en: 'Other useful measures of model quality are the precision and recall for particular
    thresholds, or the F1 score. All of these are provided by the `BinaryClassificationMetrics`
    instance. The API documentation lists the methods available: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).'
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
  zh: 其他衡量模型质量的指标包括特定阈值下的精确度和召回率，或者F1分数。所有这些都可以通过`BinaryClassificationMetrics`实例提供。API文档列出了可用的方法：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics)。
- en: Regularization in logistic regression
  id: totrans-2302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归中的正则化
- en: 'One of the dangers of machine learning is over-fitting: the algorithm captures
    not only the signal in the training set, but also the statistical noise that results
    from the finite size of the training set.'
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个危险是过拟合：算法不仅捕获了训练集中的信号，而且还捕获了由训练集有限大小产生的统计噪声。
- en: 'A way to mitigate over-fitting in logistic regression is to use regularization:
    we impose a penalty for large values of the parameters when optimizing. We can
    do this by adding a penalty to the cost function that is proportional to the magnitude
    of the parameters. Formally, we re-write the logistic regression cost function
    (described in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data
    with Breeze"), *Manipulating Data with Breeze*) as:'
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中减轻过拟合的一种方法是使用正则化：我们在优化时对参数的大值施加惩罚。我们可以通过向代价函数添加与参数幅度成比例的惩罚来实现这一点。形式上，我们将逻辑回归代价函数（在[第2章](part0018.xhtml#aid-H5A41
    "第2章. 使用Breeze操作数据")，*使用Breeze操作数据*中描述）重新写为：
- en: '![Regularization in logistic regression](img/image01219.jpeg)'
  id: totrans-2305
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归中的正则化](img/image01219.jpeg)'
- en: 'where ![Regularization in logistic regression](img/image01220.jpeg) is the
    normal logistic regression cost function:'
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![逻辑回归中的正则化](img/image01220.jpeg) 是标准的逻辑回归代价函数：
- en: '![Regularization in logistic regression](img/image01221.jpeg)'
  id: totrans-2307
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归中的正则化](img/image01221.jpeg)'
- en: 'Here, *params* is the vector of parameters, ![Regularization in logistic regression](img/image01222.jpeg)
    is the vector of features for the *i^(th)* training example, and ![Regularization
    in logistic regression](img/image01223.jpeg) is *1* if the *i* *th* training example
    is spam, and *0* otherwise. This is identical to the logistic regression cost-function
    introduced in [Chapter 2](part0018.xhtml#aid-H5A41 "Chapter 2. Manipulating Data
    with Breeze"), *Manipulating data with Breeze*, apart from the addition of the
    regularization term ![Regularization in logistic regression](img/image01224.jpeg),
    the ![Regularization in logistic regression](img/image01225.jpeg) norm of the
    parameter vector. The most common value of *n* is 2, in which case ![Regularization
    in logistic regression](img/image01226.jpeg) is just the magnitude of the parameter
    vector:'
  id: totrans-2308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*params* 是参数向量，![逻辑回归中的正则化](img/image01222.jpeg) 是第 *i* 个训练示例的特征向量，而 ![逻辑回归中的正则化](img/image01223.jpeg)
    是当第 *i* 个训练示例是垃圾邮件时为 *1*，否则为 *0*。这与[第2章](part0018.xhtml#aid-H5A41 "第2章. 使用Breeze操作数据")，*使用Breeze操作数据*中引入的逻辑回归代价函数相同，除了添加了正则化项
    ![逻辑回归中的正则化](img/image01224.jpeg) 和参数向量的 ![逻辑回归中的正则化](img/image01225.jpeg) 范数。*n*
    的最常见值是2，在这种情况下 ![逻辑回归中的正则化](img/image01226.jpeg) 只是参数向量的幅度：
- en: '![Regularization in logistic regression](img/image01227.jpeg)'
  id: totrans-2309
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归中的正则化](img/image01227.jpeg)'
- en: The additional regularization term drives the algorithm to reduce the magnitude
    of the parameter vector. When using regularization, features must all have comparable
    magnitude. This is commonly achieved by normalizing the features. The logistic
    regression estimator provided by MLlib normalizes all features by default. This
    can be turned off with the `setStandardization` parameter.
  id: totrans-2310
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的正则化项驱动算法减小参数向量的幅度。在使用正则化时，特征必须具有可比的幅度。这通常通过归一化特征来实现。MLlib提供的逻辑回归估计器默认情况下归一化所有特征。这可以通过`setStandardization`参数关闭。
- en: 'Spark has two hyperparameters that can be tweaked to control regularization:'
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有两个可以调整的超参数来控制正则化：
- en: The type of regularization, set with the `elasticNetParam` parameter. A value
    of 0 indicates ![Regularization in logistic regression](img/image01228.jpeg) regularization.
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化的类型，通过`elasticNetParam`参数设置。0值表示![逻辑回归中的正则化](img/image01228.jpeg)正则化。
- en: The degree of regularization (![Regularization in logistic regression](img/image01229.jpeg)
    in the cost function), set with the `regParam` parameter. A high value of the
    regularization parameter indicates a strong regularization. In general, the greater
    the danger of over-fitting, the larger the regularization parameter ought to be.
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化的程度（成本函数中的![逻辑回归中的正则化](img/image01229.jpeg)），通过`regParam`参数设置。正则化参数的高值表示强烈的正则化。一般来说，过拟合的危险越大，正则化参数应该越大。
- en: 'Let''s create a new logistic regression instance that uses regularization:'
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的逻辑回归实例，该实例使用正则化：
- en: '[PRE585]'
  id: totrans-2315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE585]'
- en: To choose the appropriate value of ![Regularization in logistic regression](img/image01229.jpeg),
    we fit the pipeline to the training set and calculate the classification error
    on the test set for several values of ![Regularization in logistic regression](img/image01229.jpeg).
    Further on in the chapter, we will learn about cross-validation in MLlib, which
    provides a much more rigorous way of choosing hyper-parameters.
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择![逻辑回归中的正则化](img/image01229.jpeg)的适当值，我们将管道拟合到训练集，并计算测试集上![逻辑回归中的正则化](img/image01229.jpeg)的几个值的分类误差。在章节的后面，我们将学习MLlib中的交叉验证，它提供了一种更严格的方法来选择超参数。
- en: '[PRE586]'
  id: totrans-2317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE586]'
- en: For our example, we see that any attempt to add L[2] regularization leads to
    a decrease in classification accuracy.
  id: totrans-2318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，我们看到任何尝试添加L[2]正则化的尝试都会导致分类精度的下降。
- en: Cross-validation and model selection
  id: totrans-2319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证和模型选择
- en: 'In the previous example, we validated our approach by withholding 30% of the
    data when training, and testing on this subset. This approach is not particularly
    rigorous: the exact result changes depending on the random train-test split. Furthermore,
    if we wanted to test several different hyperparameters (or different models) to
    choose the best one, we would, unwittingly, choose the model that best reflects
    the specific rows in our test set, rather than the population as a whole.'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们通过在训练时保留30%的数据，并在该子集上进行测试来验证我们的方法。这种方法并不特别严格：确切的结果取决于随机的训练-测试分割。此外，如果我们想测试几个不同的超参数（或不同的模型）以选择最佳模型，我们可能会无意中选择的模型最能反映测试集中特定行，而不是整体人群。
- en: This can be overcome with *cross-validation*. We have already encountered cross-validation
    in [Chapter 4](part0036.xhtml#aid-12AK82 "Chapter 4. Parallel Collections and
    Futures"), *Parallel Collections and Futures*. In that chapter, we used random
    subsample cross-validation, where we created the train-test split randomly.
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过*交叉验证*来克服。我们已经在[第4章](part0036.xhtml#aid-12AK82 "第4章。并行集合和未来")中遇到了交叉验证，*并行集合和未来*。在那个章节中，我们使用了随机子样本交叉验证，其中我们随机创建训练-测试分割。
- en: 'In this chapter, we will use **k-fold cross-validation**: we split the training
    set into *k* parts (where, typically, *k* is *10* or *3*) and use *k-1* parts
    as the training set and the last as the test set. The train/test cycle is repeated
    *k* times, keeping a different part as test set each time.'
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**k折交叉验证**：我们将训练集分成*k*部分（其中，通常*k*是*10*或*3*），使用*k-1*部分作为训练集，最后的部分作为测试集。重复*k*次训练/测试周期，每次保持不同的部分作为测试集。
- en: Cross-validation is commonly used to choose the best set of hyperparameters
    for a model. To illustrate choosing suitable hyperparameters, we will go back
    to our regularized logistic regression example. Instead of intuiting the hyper-parameters
    ourselves, we will choose the hyper-parameters that give us the best cross-validation
    score.
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证通常用于选择模型的最佳超参数集。为了说明选择合适的超参数，我们将回到我们的正则化逻辑回归示例。我们不会自己直觉超参数，而是选择给我们最佳交叉验证分数的超参数。
- en: 'We will explore setting both the regularization type (through `elasticNetParam`)
    and the degree of regularization (through `regParam`). A crude, but effective
    way to find good values of the parameters is to perform a grid search: we calculate
    the cross-validation score for every pair of values of the regularization parameters
    of interest.'
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨设置正则化类型（通过`elasticNetParam`）和正则化程度（通过`regParam`）。找到一个好的参数值的一个粗略但有效的方法是执行网格搜索：我们计算正则化参数感兴趣值对的交叉验证分数。
- en: We can build a grid of parameters using MLlib's `ParamGridBuilder`.
  id: totrans-2325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MLlib的`ParamGridBuilder`构建参数网格。
- en: '[PRE587]'
  id: totrans-2326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE587]'
- en: 'To add hyper-parameters over which to optimize to the grid, we use the `addGrid`
    method:'
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
  zh: 要将优化超参数添加到网格中，我们使用`addGrid`方法：
- en: '[PRE588]'
  id: totrans-2328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE588]'
- en: 'Once all the dimensions are added, we can just call the `build` method on the
    builder to build the grid:'
  id: totrans-2329
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了所有维度，我们只需在构建器上调用`build`方法来构建网格：
- en: '[PRE589]'
  id: totrans-2330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE589]'
- en: As we can see, the grid is just a one-dimensional array of sets of parameters
    to pass to the logistic regression model prior to fitting.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，网格只是一个参数集的一维数组，在拟合逻辑回归模型之前传递给模型。
- en: 'The next step in setting up the cross-validation pipeline is to define a metric
    for comparing model performance. Earlier in the chapter, we saw how to use `BinaryClassificationMetrics`
    to estimate the quality of a model. Unfortunately, the `BinaryClassificationMetrics`
    class is part of the core MLLib API, rather than the new pipeline API, and is
    thus not (easily) compatible. The pipeline API offers a `BinaryClassificationEvaluator`
    class instead. This class works directly on DataFrames, and thus fits perfectly
    into the pipeline API flow:'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: 设置交叉验证管道的下一步是定义一个用于比较模型性能的指标。在本章的早期，我们看到了如何使用`BinaryClassificationMetrics`来估计模型的质量。不幸的是，`BinaryClassificationMetrics`类是核心MLLib
    API的一部分，而不是新的管道API，因此它（不容易）兼容。管道API提供了一个`BinaryClassificationEvaluator`类。这个类直接在DataFrame上工作，因此非常适合管道API流程：
- en: '[PRE590]'
  id: totrans-2333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE590]'
- en: 'From the parameter list, we see that the `BinaryClassificationEvaluator` class
    supports two metrics: the area under the ROC curve, and the area under the precision-recall
    curve. It expects, as input, a DataFrame containing a `label` column (the model
    truth) and a `rawPrediction` column (the column containing the probability that
    an e-mail is spam or ham).'
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
  zh: 从参数列表中，我们看到`BinaryClassificationEvaluator`类支持两个指标：ROC曲线下的面积和精确率-召回率曲线下的面积。它期望输入一个包含`label`列（模型真实值）和`rawPrediction`列（包含电子邮件是垃圾邮件或正常邮件的概率的列）的DataFrame。
- en: 'We now have all the parameters we need to run cross-validation. We first build
    the pipeline, and then pass the pipeline, the evaluator and the array of parameters
    over which to run the cross-validation to an instance of `CrossValidator`:'
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了运行交叉验证所需的所有参数。我们首先构建管道，然后将管道、评估器和要运行交叉验证的参数数组传递给`CrossValidator`的一个实例：
- en: '[PRE591]'
  id: totrans-2336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE591]'
- en: 'We will now fit `crossval` to `trainDF`:'
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将`crossval`拟合到`trainDF`：
- en: '[PRE592]'
  id: totrans-2338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE592]'
- en: 'This step can take a fairly long time (over an hour on a single machine). This
    creates a transformer, `cvModel`, corresponding to the logistic regression object
    with the parameters that best represent `trainDF`. We can use it to predict the
    classification error on the test DataFrame:'
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可能需要相当长的时间（在单台机器上可能超过一小时）。这创建了一个对应于具有最佳参数表示`trainDF`的逻辑回归对象的transformer，`cvModel`。我们可以用它来预测测试DataFrame上的分类错误：
- en: '[PRE593]'
  id: totrans-2340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE593]'
- en: 'Cross-validation has therefore resulted in a model that performs identically
    to the original, naive logistic regression model with no hyper-parameters. `cvModel`
    also contains a list of the evaluation score for each set of parameter in the
    parameter grid:'
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，交叉验证产生了一个与原始、无超参数的朴素逻辑回归模型表现相同的模型。`cvModel`还包含参数网格中每组的评估分数列表：
- en: '[PRE594]'
  id: totrans-2342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE594]'
- en: 'The easiest way to relate this to the hyper-parameters is to zip it with `cvModel.getEstimatorParamMaps`.
    This gives us a list of (*hyperparameter values*, *cross-validation score*) pairs:'
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与超参数相关联的最简单方法是将它与`cvModel.getEstimatorParamMaps`一起压缩。这给我们一个（超参数值，交叉验证分数）对的列表：
- en: '[PRE595]'
  id: totrans-2344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE595]'
- en: The best set of hyper-parameters correspond to L[2] regularization with a regularization
    parameter of `1E-10`, though this only corresponds to a tiny improvement in AUC.
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的超参数集对应于L[2]正则化，正则化参数为`1E-10`，尽管这仅对应于AUC的微小提升。
- en: 'This completes our spam filter example. We have successfully trained a spam
    filter for this particular Ling-Spam dataset. To obtain better results, one could
    experiment with better feature extraction: we could remove stop words or use TF-IDF
    vectors, rather than just term frequency vectors as features, and we could add
    additional features like the length of messages, or even *n-grams*. We could also
    experiment with non-linear algorithms, such as random forest. All of these steps
    would be straightforward to add to the pipeline.'
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的垃圾邮件过滤器示例。我们已经成功地为这个特定的Ling-Spam数据集训练了一个垃圾邮件过滤器。为了获得更好的结果，可以尝试更好的特征提取：我们可以移除停用词或使用TF-IDF向量，而不是仅使用词频向量作为特征，我们还可以添加额外的特征，如消息长度，甚至*n-grams*。我们还可以尝试非线性算法，如随机森林。所有这些步骤都很容易添加到管道中。
- en: Beyond logistic regression
  id: totrans-2347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归之外
- en: We have concentrated on logistic regression in this chapter, but MLlib offers
    many alternative algorithms that will capture non-linearity in the data more effectively.
    The consistency of the pipeline API makes it easy to try out different algorithms
    and see how they perform. The pipeline API offers decision trees, random forest
    and gradient boosted trees for classification, as well as a simple feed-forward
    neural network, which is still experimental. It offers lasso and ridge regression
    and decision trees for regression, as well as PCA for dimensionality reduction.
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中专注于逻辑回归，但MLlib提供了许多其他算法，这些算法可以更有效地捕捉数据中的非线性。管道API的一致性使得尝试不同的算法并查看它们的性能变得容易。管道API提供了用于分类的决策树、随机森林和梯度提升树，以及一个简单的前馈神经网络，这仍然是实验性的。它还提供了Lasso和岭回归以及用于回归的决策树，以及用于降维的PCA。
- en: The lower level MLlib API also offers principal component analysis for dimensionality
    reduction, several clustering methods including *k*-means and latent Dirichlet
    allocation and recommender systems using alternating least squares.
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
  zh: 更低级别的MLlib API还提供了降维的主成分分析，包括*k*-means和潜在狄利克雷分配在内的几种聚类方法，以及使用交替最小二乘法的推荐系统。
- en: Summary
  id: totrans-2350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: MLlib tackles the challenge of devising scalable machine learning algorithms
    head-on. In this chapter, we used it to train a simple scalable spam filter. MLlib
    is a vast, rapidly evolving library. The best way to learn more about what it
    can offer is to try and port code that you might have written using another library
    (such as scikit-learn).
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib直面设计可扩展机器学习算法的挑战。在本章中，我们用它来训练一个简单的可扩展垃圾邮件过滤器。MLlib是一个庞大且快速发展的库。了解它能提供什么最好的方式是尝试将你使用其他库（如scikit-learn）编写的代码移植过来。
- en: In the next chapter, we will look at how to build web APIs and interactive visualizations
    to share our results with the rest of the world.
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何构建Web API和交互式可视化，以便与世界分享我们的结果。
- en: References
  id: totrans-2353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The best reference is the online documentation, including:'
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的参考资料是在线文档，包括：
- en: 'The pipeline API: [http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)'
  id: totrans-2355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '管道API: [http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)'
- en: 'A full list of transformers: [http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines](http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines)'
  id: totrans-2356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '完整的转换器列表: [http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines](http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines)'
- en: '*Advanced Analytics with Spark*, by *Sandy Ryza*, *Uri Laserson*, *Sean Owen*
    and *Josh Wills* provides a detailed and up-to-date introduction to machine learning
    with Spark.'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
  zh: '*《Spark高级分析》*，由*Sandy Ryza*、*Uri Laserson*、*Sean Owen*和*Josh Wills*所著，提供了Spark机器学习的详细和最新介绍。'
- en: There are several books that introduce machine learning in more detail than
    we can here. We have mentioned *The Elements of Statistical Learning*, by *Friedman*,
    *Tibshirani* and *Hastie* several times in this book. It is one of the most complete
    introductions to the mathematical underpinnings of machine learning currently
    available.
  id: totrans-2358
  prefs: []
  type: TYPE_NORMAL
  zh: 有几本书比我们在这里介绍的更详细地介绍了机器学习。我们在这本书中多次提到了*《统计学习的要素》*，由*Friedman*、*Tibshirani*和*Hastie*所著。这是目前可用的最完整的机器学习数学基础介绍之一。
- en: Andrew Ng's Machine Learning course on [https://www.coursera.org/](https://www.coursera.org/)
    provides a good introduction to machine learning. It uses Octave/MATLAB as the
    programming language, but should be straightforward to adapt to Breeze and Scala.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
  zh: 安德鲁·纳格（Andrew Ng）的机器学习课程在[https://www.coursera.org/](https://www.coursera.org/)提供了机器学习的良好介绍。它使用Octave/MATLAB作为编程语言，但应该可以轻松地适应Breeze和Scala。
- en: Chapter 13. Web APIs with Play
  id: totrans-2360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。使用Play的网络API
- en: 'In the first 12 chapters of this book, we introduced basic tools and libraries
    for anyone wanting to build data science applications: we learned how to interact
    with SQL and MongoDB databases, how to build fast batch processing applications
    using Spark, how to apply state-of-the-art machine learning algorithms using MLlib,
    and how to build modular concurrent applications in Akka.'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前12章中，我们介绍了任何想要构建数据科学应用程序的人的基本工具和库：我们学习了如何与SQL和MongoDB数据库交互，如何使用Spark构建快速批处理应用程序，如何使用MLlib应用最先进的机器学习算法，以及如何在Akka中构建模块化并发应用程序。
- en: 'In the last chapters of this book, we will branch out to look at a web framework:
    *Play*. You might wonder why a web framework would feature in a data science book;
    surely such topics are best left to software engineers or web developers. Data
    scientists, however, rarely exist in a vacuum. They often need to communicate
    results or insights to stakeholders. As compelling as an ROC curve may be to someone
    well versed in statistics, it may not carry as much weight with less technical
    people. Indeed, it can be much easier to sell insights when they are accompanied
    by an engaging visualization.'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后几章中，我们将扩展讨论，探讨一个网络框架：*Play*。你可能会 wonder为什么网络框架会出现在一本数据科学书中；当然，这样的主题最好留给软件工程师或网络开发者。然而，数据科学家很少存在于真空之中。他们经常需要将结果或见解传达给利益相关者。对于一个精通统计学的某人来说，ROC曲线可能很有说服力，但对于技术不那么精通的人来说，它可能没有那么大的分量。事实上，当见解伴随着引人入胜的视觉呈现时，销售见解可能会更容易。
- en: Many modern interactive data visualization applications are web applications
    running in a web browser. Often, these involve **D3.js**, a JavaScript library
    for building data-driven web pages. In this chapter and the next, we will look
    at integrating D3 with Scala.
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代交互式数据可视化应用程序都是运行在网页浏览器中的网络应用程序。通常，这些应用程序涉及**D3.js**，这是一个用于构建数据驱动网页的JavaScript库。在本章和下一章中，我们将探讨如何将D3与Scala集成。
- en: 'Writing a web application is a complex endeavor. We will split this task over
    this chapter and the next. In this chapter, we will learn how to write a REST
    API that we can use as backend for our application, or query in its own right.
    In the next chapter, we will look at integrating front-end code with Play to query
    the API exposed by the backend and display it using D3\. We assume at least a
    basic familiarity with HTTP in this chapter: you should have read [Chapter 7](part0059.xhtml#aid-1O8H61
    "Chapter 7. Web APIs"), *Web APIs*, at least.'
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个网络应用程序是一项复杂的任务。我们将把这个任务分成本章和下一章来讨论。在本章中，我们将学习如何编写一个REST API，我们可以将其用作应用程序的后端，或者直接查询。在下一章中，我们将探讨如何将前端代码与Play集成，以查询后端暴露的API并使用D3进行展示。在本章中，我们假设你对HTTP有至少基本的了解：你应该至少阅读过[第7章](part0059.xhtml#aid-1O8H61
    "第7章。网络API")，*网络API*。
- en: Many data scientists or aspiring data scientists are unlikely to be familiar
    with the inner workings of web technologies. Learning how to build complex websites
    or web APIs can be daunting. This chapter therefore starts with a general discussion
    of dynamic websites and the architecture of web applications. If you are already
    familiar with server-side programming and with web frameworks, you can easily
    skip over the first few sections.
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家或希望成为数据科学家的人可能不太熟悉网络技术的内部工作原理。学习如何构建复杂的网站或网络API可能会令人望而却步。因此，本章从对动态网站和网络应用程序架构的一般讨论开始。如果你已经熟悉服务器端编程和网络框架，你可以轻松地跳过前几节。
- en: Client-server applications
  id: totrans-2366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端-服务器应用程序
- en: 'A website works through the interaction between two computers: the client and
    the server. If you enter the URL [www.github.com/pbugnion/s4ds/graphs](http://www.github.com/pbugnion/s4ds/graphs)
    in a web browser, your browser queries one of the GitHub servers. The server will
    look though its database for information concerning the repository that you are
    interested in. It will serve this information as HTML, CSS, and JavaScript to
    your computer. Your browser is then responsible for interpreting this response
    in the correct way.'
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
  zh: 网站通过两台计算机之间的交互来工作：客户端和服务器。如果你在网页浏览器中输入[www.github.com/pbugnion/s4ds/graphs](http://www.github.com/pbugnion/s4ds/graphs)，你的浏览器会查询GitHub服务器之一。服务器会在其数据库中查找有关你感兴趣的仓库的信息。它将以HTML、CSS和JavaScript的形式将此信息提供给你的电脑。然后，你的浏览器负责以正确的方式解释这个响应。
- en: If you look at the URL in question, you will notice that there are several graphs
    on that page. Unplug your internet connection and you can still interact with
    the graphs. All the information necessary for interacting with the graphs was
    transferred, as JavaScript, when you loaded that webpage. When you play with the
    graphs, the CPU cycles necessary to make those changes happen are spent on *your*
    computer, not a GitHub server. The code is executed *client-side*. Conversely,
    when you request information about a new repository, that request is handled by
    a GitHub server. It is said to be handled *server-side*.
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看相关的URL，你会注意到该页面上有几个图表。即使断开互联网连接，你仍然可以与这些图表进行交互。所有与图表交互所需的信息，在加载该网页时，都已经以JavaScript的形式传输。当你与图表互动时，使这些变化发生的CPU周期是在你的电脑上消耗的，而不是GitHub服务器。代码是在客户端执行的。相反，当你请求有关新仓库的信息时，该请求由GitHub服务器处理。这被称为服务器端处理。
- en: 'A web framework like Play can be used on the server. For client-side code,
    we can only use a language that the client browser will understand: HTML for the
    layout, CSS for the styling and JavaScript, or languages that can compile to JavaScript,
    for the logic.'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在服务器上使用像Play这样的网络框架。对于客户端代码，我们只能使用客户端浏览器能理解的编程语言：HTML用于布局，CSS用于样式，JavaScript用于逻辑，或者可以编译成JavaScript的语言。
- en: Introduction to web frameworks
  id: totrans-2370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络框架简介
- en: This section is a brief introduction to how modern web applications are designed.
    Go ahead and skip it if you already feel comfortable writing backend code.
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了现代网络应用的设计方式。如果你已经熟悉编写后端代码，可以跳过这一部分。
- en: Loosely, a web framework is a set of tools and code libraries for building web
    applications. To understand what a web framework provides, let's take a step back
    and think about what you would need to do if you did not have one.
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
  zh: 大体上，网络框架是一套用于构建网络应用的工具和代码库。为了理解网络框架提供的内容，让我们退一步思考，如果你没有网络框架，你需要做什么。
- en: 'You want to write a program that listens on port 80 and sends HTML (or JSON
    or XML) back to clients that request it. This is simple if you are serving the
    same file back to every client: just load the HTML from file when you start the
    server, and send it to clients who request it.'
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要编写一个程序，监听80端口，并将HTML（或JSON或XML）发送回请求它的客户端。如果你要向每个客户端发送相同的文件，这很简单：只需在启动服务器时从文件中加载HTML，并将其发送给请求它的客户端。
- en: So far, so good. But what if you now want to customize the HTML based on the
    client request? You might choose to respond differently based on part of the URL
    that the client put in his browser, or based on specific elements in the HTTP
    request. For instance, the product page on [amazon.com](http://amazon.com) is
    different to the payment page. You need to write code to parse the URL and the
    request, and then route the request to the relevant handler.
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。但如果你现在想根据客户端请求自定义HTML，你会怎么做？你可能选择根据客户端在其浏览器中输入的URL的一部分，或者根据HTTP请求中的特定元素来做出不同的响应。例如，[amazon.com](http://amazon.com)上的产品页面与支付页面不同。你需要编写代码来解析URL和请求，然后将请求路由到相应的处理器。
- en: You might now want to customize the HTML returned dynamically, based on specific
    elements of the request. The page for every product on [amazon.com](http://amazon.com)
    follows the same outline, but specific elements are different. It would be wasteful
    to store the entire HTML content for every product. A better way is to store the
    details for each product in a database and inject them into an HTML template when
    a client requests information on that product. You can do this with a *template
    processor*. Of course, writing a good template processor is difficult.
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能希望根据请求的具体元素动态定制返回的HTML。每个产品在[amazon.com](http://amazon.com)的页面都有相同的轮廓，但具体元素是不同的。为每个产品存储整个HTML内容将是浪费的。更好的方法是存储每个产品的详细信息到数据库中，并在客户端请求该产品的信息时将其注入到HTML模板中。您可以使用*模板处理器*来完成这项工作。当然，编写一个好的模板处理器是困难的。
- en: You might deploy your web framework and realize that it cannot handle the traffic
    directed to it. You decide that handlers responding to client requests should
    run asynchronously. You now have to deal with concurrency.
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能部署了您的Web框架，并意识到它无法处理指向它的流量。您决定响应客户端请求的处理程序应该异步运行。现在您必须处理并发。
- en: A web framework essentially provides the wires to bind everything together.
    Besides bundling an HTTP server, most frameworks will have a router that automatically
    routes a request, based on the URL, to the correct handler. In most cases, the
    handler will run asynchronously, giving you much better scalability. Many frameworks
    have a template processor that lets you write HTML (or sometimes JSON or XML)
    templates intuitively. Some web frameworks also provide functionality for accessing
    a database, for parsing JSON or XML, for formulating HTTP requests and for localization
    and internationalization.
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
  zh: Web框架本质上提供了将一切连接在一起的“电线”。除了捆绑HTTP服务器外，大多数框架还会有一个路由器，它会根据URL自动将请求路由到正确的处理器。在大多数情况下，处理器将异步运行，这为您提供了更好的可扩展性。许多框架都有一个模板处理器，允许您直观地编写HTML（有时是JSON或XML）模板。一些Web框架还提供访问数据库、解析JSON或XML、制定HTTP请求以及本地化和国际化的功能。
- en: Model-View-Controller architecture
  id: totrans-2378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型-视图-控制器架构
- en: 'Many web frameworks impose program architectures: it is difficult to provide
    wires to bind disparate components together without making some assumptions about
    what those components are. The **Model-View-Controller** (**MVC**) architecture
    is particularly popular on the Web, and it is the architecture the Play framework
    assumes. Let''s look at each component in turn:'
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Web框架强加了程序架构：在不做出一些关于这些组件是什么的假设的情况下，很难提供将不同组件连接在一起的“电线”。**模型-视图-控制器**（**MVC**）架构在Web上特别受欢迎，并且是Play框架所假设的架构。让我们依次看看每个组件：
- en: The model is the data underlying the application. For example, I expect the
    application underlying GitHub has models for users, repositories, organizations,
    pull requests and so on. In the Play framework, a model is often an instance of
    a case class. The core responsibility of the model is to remember the current
    state of the application.
  id: totrans-2380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是应用程序背后的数据。例如，我期望GitHub背后的应用程序有用户、存储库、组织、拉取请求等模型。在Play框架中，模型通常是案例类的实例。模型的核心责任是记住应用程序的当前状态。
- en: Views are representations of a model or a set of models on the screen.
  id: totrans-2381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视图是模型或一组模型在屏幕上的表示。
- en: 'The controller handles client interactions, possibly changing the model. For
    instance, if you *star* a project on GitHub, the controller will update the relevant
    models. Controllers normally carry very little application state: remembering
    things is the job of the models.![Model-View-Controller architecture](img/image01230.jpeg)'
  id: totrans-2382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器处理客户端交互，可能会更改模型。例如，如果您在GitHub上为项目添加星标，控制器将更新相关的模型。控制器通常携带很少的应用程序状态：记住事情是模型的工作。![模型-视图-控制器架构](img/image01230.jpeg)
- en: 'MVC architecture: the state of the application is provided by the model. The
    view provides a visual representation of the model to the user, and the controller
    handles logic: what to do when the user presses a button or submits a form.'
  id: totrans-2383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MVC架构：应用程序的状态由模型提供。视图向用户提供模型的视觉表示，控制器处理逻辑：当用户按下按钮或提交表单时应该做什么。
- en: 'The MVC framework works well because it decouples the user interface from the
    underlying data and structures the flow of actions: a controller can update the
    model state or the view, a model can send signals to the view to tell it to update,
    and the view merely displays that information. The model carries no information
    related to the user interface. This separation of concerns results in an easier
    mental model of information flow, better encapsulation and greater testability.'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
  zh: MVC框架工作得很好，因为它将用户界面与底层数据和结构分离，并结构化了动作的流程：控制器可以更新模型状态或视图，模型可以向视图发送信号，告诉它更新，而视图只是显示这些信息。模型不携带与用户界面相关的任何信息。这种关注点的分离导致了对信息流的更容易的心理模型、更好的封装和更高的可测试性。
- en: Single page applications
  id: totrans-2385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单页应用程序
- en: The client-server duality adds a degree of complication to the elegant MVC architecture.
    Where should the model reside? What about the controller? Traditionally, the model
    and the controller ran almost entirely on the server, which just pushed the relevant
    HTML view to the client.
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端-服务器二元性给优雅的MVC架构增加了一层复杂性。模型应该放在哪里？控制器呢？传统上，模型和控制器几乎完全运行在服务器上，服务器只是将相关的HTML视图推送到客户端。
- en: 'The growth in client-side JavaScript frameworks, such AngularJS, has resulted
    in a gradual shift to putting more code in the client. Both the controller and
    a temporary version of the model typically run client-side. The server just functions
    as a web API: if, for instance, the user updates the model, the controller will
    send an HTTP request to the server informing it of the change.'
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端JavaScript框架的增长，如AngularJS，导致了将更多代码放入客户端的逐渐转变。控制器和模型的临时版本通常都在客户端运行。服务器仅作为Web
    API运行：例如，如果用户更新了模型，控制器将向服务器发送一个HTTP请求，通知它变化。
- en: 'It then makes sense to think of the program running server-side and the one
    running client-side as two separate applications: the server persists data in
    databases, for instance, and provides a programmatic interface to this data, usually
    as a web service returning JSON or XML data. The client-side program maintains
    its own model and controller, and polls the server whenever it needs a new model,
    or whenever it needs to inform the server that the persistent view of the model
    should be changed.'
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将运行在服务器端和客户端的程序视为两个独立的应用程序是有意义的：服务器在数据库中持久化数据，例如，并提供一个程序接口来访问这些数据，通常是通过返回JSON或XML数据的Web服务。客户端程序维护自己的模型和控制器，并在需要新的模型或需要通知服务器模型持久视图应该更改时轮询服务器。
- en: Taken to the extreme, this results in **Single-Page Applications**. In a single-page
    application, the first time the client requests a page from the server, he receives
    the HTML and the JavaScript necessary to build the framework for the entire application.
    If the client needs further data from the server, he will poll the server's API.
    This data is returned as JSON or XML.
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
  zh: 极端情况下，这导致了**单页应用程序**。在单页应用程序中，客户端第一次从服务器请求页面时，他会收到构建整个应用程序框架所需的HTML和JavaScript。如果客户端需要从服务器获取更多数据，他将轮询服务器的API。这些数据以JSON或XML的形式返回。
- en: 'This might seem a little complicated in the abstract, so let''s think how the
    Amazon website might be structured as a single-page application. We will just
    concern ourselves with the products page here, since that''s complicated enough.
    Let''s imagine that you are on the home page, and you hit a link for a particular
    product. The application running on your computer knows how to display products,
    for instance through an HTML template. The JavaScript also has a prototype for
    the model, such as:'
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
  zh: 这在抽象上可能看起来有些复杂，所以让我们思考一下亚马逊网站可能作为单页应用程序的结构。我们在这里只关注产品页面，因为那已经足够复杂了。让我们想象一下，你正在主页上，点击了一个特定产品的链接。运行在你电脑上的应用程序知道如何显示产品，例如通过HTML模板。JavaScript也有一个模型的原型，例如：
- en: '[PRE596]'
  id: totrans-2391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE596]'
- en: 'What it''s currently missing is knowledge of what data to put in those fields
    for the product you have just selected: there is no way that information could
    have been sent to your computer when the website loaded, since there was no way
    to know what product you might click on (and sending information about every product
    would be prohibitively costly). So the Amazon client sends a request to the server
    for information on that product. The Amazon server replies with a JSON object
    (or maybe XML). The client then updates its model with that information. When
    the update is complete, an event is fired to update the view:'
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
  zh: 目前缺少的是关于你刚刚选择的产品应该在这些字段中放入哪些数据的知识：当网站加载时，信息不可能发送到你的电脑，因为没有方法知道你可能点击的产品（发送关于每个产品的信息将成本过高）。因此，Amazon
    客户端向服务器发送关于该产品的信息请求。Amazon 服务器以 JSON 对象（或可能是 XML）的形式回复。然后客户端使用该信息更新其模型。当更新完成后，将触发一个事件来更新视图：
- en: '![Single page applications](img/image01231.jpeg)'
  id: totrans-2393
  prefs: []
  type: TYPE_IMG
  zh: '![单页应用程序](img/image01231.jpeg)'
- en: 'Client-server communications in a single-page application: when the client
    first accesses the website, it receives HTML, CSS and JavaScript files that contain
    the entire logic for the application. From then on, the client only uses the server
    as an API when it requests additional data. The application running in the user''s
    web browser and the one running on the server are nearly independent. The only
    coupling is through the structure of the API exposed by the server.'
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
  zh: 单页应用程序中的客户端-服务器通信：当客户端首次访问网站时，它会接收到包含应用程序全部逻辑的 HTML、CSS 和 JavaScript 文件。从那时起，客户端只有在请求额外数据时才将服务器用作
    API。运行在用户浏览器中的应用程序和运行在服务器上的应用程序几乎是独立的。唯一的耦合是通过服务器暴露的 API 结构。
- en: Building an application
  id: totrans-2395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建应用程序
- en: 'In this chapter and the next, we will build a single-page application that
    relies on an API written in Play. We will build a webpage that looks like this:'
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将构建一个依赖于用 Play 编写的 API 的单页应用程序。我们将构建一个看起来像这样的网页：
- en: '![Building an application](img/image01232.jpeg)'
  id: totrans-2397
  prefs: []
  type: TYPE_IMG
  zh: '![构建应用程序](img/image01232.jpeg)'
- en: The user enters the name of someone on GitHub and can view a list of their repositories
    and a chart summarizing what language they use. You can find the application deployed
    at `app.scala4datascience.com`. Go ahead and give it a whirl.
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入 GitHub 上某人的名字，可以查看他们的仓库列表以及一个总结他们使用语言的图表。您可以在 `app.scala4datascience.com`
    上找到已部署的应用程序。不妨试一试。
- en: 'To get a glimpse of the innards, type `app.scala4datascience.com/api/repos/odersky`.
    This returns a JSON object like:'
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
  zh: 要一窥其内部结构，请输入 `app.scala4datascience.com/api/repos/odersky`。这将返回一个类似以下的 JSON
    对象：
- en: '[PRE597]'
  id: totrans-2400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE597]'
- en: We will build the API in this chapter, and write the front-end code in the next
    chapter.
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章构建 API，并在下一章编写前端代码。
- en: The Play framework
  id: totrans-2402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Play 框架
- en: The Play framework is a web framework built on top of Akka. It has a proven
    track record in industry, and is thus a reliable choice for building scalable
    web applications.
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
  zh: Play 框架是在 Akka 之上构建的 Web 框架。它在业界有着可靠的记录，因此是构建可扩展 Web 应用程序的一个可靠选择。
- en: 'Play is an *opinionated* web framework: it expects you to follow the MVC architecture,
    and it has a strong opinion about the tools you should be using. It comes bundled
    with its own JSON and XML parsers, with its own tools for accessing external APIs,
    and with recommendations for how to access databases.'
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
  zh: Play 是一个有**明确立场**的 Web 框架：它期望你遵循 MVC 架构，并且对应该使用哪些工具有着强烈的看法。它自带 JSON 和 XML 解析器，自带访问外部
    API 的工具，以及关于如何访问数据库的建议。
- en: 'Web applications are much more complex than the command line scripts we have
    been developing in this book, because there are many more components: the backend
    code, routing information, HTML templates, JavaScript files, images, and so on.
    The Play framework makes strong assumptions about the directory structure for
    your project. Building that structure from scratch is both mind-numbingly boring
    and easy to get wrong. Fortunately, we can use **Typesafe activators** to bootstrap
    the project (you can also download the code from the Git repository in [https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)
    but I encourage you to start the project from a basic activator structure and
    code along instead, using the finished version as an example).'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在本书中开发的命令行脚本相比，Web应用程序要复杂得多，因为它们包含更多的组件：后端代码、路由信息、HTML模板、JavaScript文件、图片等等。Play框架对你的项目目录结构有很强的假设。从头开始构建这个结构既无聊又容易出错。幸运的是，我们可以使用**Typesafe
    activators**来启动项目（你也可以从[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)的Git仓库下载代码，但我鼓励你从基本的activator结构开始项目，并使用完成的版本作为示例，边学边做）。
- en: 'Typesafe activator is a custom version of SBT that includes templates to get
    Scala programmers up and running quickly. To install activator, you can either
    download a JAR from [https://www.typesafe.com/activator/download](https://www.typesafe.com/activator/download),
    or, on Mac OS, via homebrew:'
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
  zh: Typesafe activator是SBT的一个定制版本，它包含模板，可以帮助Scala程序员快速启动。要安装activator，你可以从[https://www.typesafe.com/activator/download](https://www.typesafe.com/activator/download)下载一个JAR文件，或者在Mac
    OS上通过homebrew：
- en: '[PRE598]'
  id: totrans-2407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE598]'
- en: 'You can then launch the activator console from the terminal. If you downloaded
    activator:'
  id: totrans-2408
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从终端启动activator控制台。如果你下载了activator：
- en: '[PRE599]'
  id: totrans-2409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE599]'
- en: 'Or, if you installed via Homebrew:'
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你是通过Homebrew安装的：
- en: '[PRE600]'
  id: totrans-2411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE600]'
- en: This starts a new project in the current directory. It starts by asking what
    template you want to start with. Choose `play-scala`. It then asks for a name
    for your application. I chose `ghub-display`, but go ahead and be creative!
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前目录中启动一个新的项目。它首先会询问你想要从哪个模板开始。选择 `play-scala`。然后它会询问你的应用程序的名称。我选择了 `ghub-display`，但你可以发挥创意！
- en: 'Let''s explore the newly created project structure (I have only retained the
    most important files):'
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索新创建的项目结构（我只保留了最重要的文件）：
- en: '[PRE601]'
  id: totrans-2414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE601]'
- en: 'Let''s run the app:'
  id: totrans-2415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行应用程序：
- en: '[PRE602]'
  id: totrans-2416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE602]'
- en: Head over to your browser and navigate to the URL `127.0.0.1:9000/`. The page
    may take a few seconds to load. Once it is loaded, you should see a default page
    that says **Your application is ready**.
  id: totrans-2417
  prefs: []
  type: TYPE_NORMAL
  zh: 打开浏览器并导航到URL `127.0.0.1:9000/`。页面可能需要几秒钟才能加载。一旦加载完成，你应该看到一个默认页面，上面写着**您的应用程序已准备就绪**。
- en: 'Before we modify anything, let''s walk through how this happens. When you ask
    your browser to take you to `127.0.0.1:9000/`, your browser sends an HTTP request
    to the server listening at that address (in this case, the Netty server bundled
    with Play). The request is a GET request for the route `/`. The Play framework
    looks in `conf/routes` to see if it has a route satisfying `/`:'
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们修改任何内容之前，让我们了解一下这个过程。当你要求浏览器带你到 `127.0.0.1:9000/` 时，你的浏览器会向监听该地址的服务器发送一个HTTP请求（在这个例子中，是Play框架捆绑的Netty服务器）。请求是一个针对路由
    `/` 的GET请求。Play框架会在 `conf/routes` 中查找是否有满足 `/` 的路由：
- en: '[PRE603]'
  id: totrans-2419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE603]'
- en: 'We see that the `conf/routes` file does contain the route `/` for GET requests.
    The second part of that line, `controllers.Application.index`, is the name of
    a Scala function to handle that route (more on that in a moment). Let''s experiment.
    Change the route end-point to `/hello`. Refresh your browser without changing
    the URL. This will trigger recompilation of the application. You should now see
    an error page:'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`conf/routes` 文件确实包含了针对GET请求的路由 `/`。该行的第二部分，`controllers.Application.index`，是处理该路由的Scala函数的名称（稍后我们会详细讨论）。让我们进行实验。将路由端点更改为
    `/hello`。刷新浏览器而不更改URL。这将触发应用程序的重新编译。你现在应该看到一个错误页面：
- en: '![The Play framework](img/image01233.jpeg)'
  id: totrans-2421
  prefs: []
  type: TYPE_IMG
  zh: '![Play框架](img/image01233.jpeg)'
- en: The error page tells you that the app does not have an action for the route
    `/` any more. If you navigate to `127.0.0.1:9000/hello`, you should see the landing
    page again.
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
  zh: 错误页面告诉你，应用程序不再有针对路由 `/` 的操作。如果你导航到 `127.0.0.1:9000/hello`，你应该再次看到着陆页面。
- en: 'Besides learning a little of how routing works, we have also learned two things
    about developing Play applications:'
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习一点路由的工作原理外，我们还了解了关于开发Play应用程序的两大要点：
- en: In development mode, code gets recompiled when you refresh your browser and
    there have been code changes
  id: totrans-2424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发模式下，当你刷新浏览器并且有代码变更时，代码会被重新编译
- en: Compilation and runtime errors get propagated to the web page
  id: totrans-2425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译时和运行时错误会传播到网页
- en: Let's change the route back to `/`. There is a lot more to say on routing, but
    it can wait till we start building our application.
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将路由改回 `/`。关于路由还有很多要说的，但我们可以等到我们开始构建应用程序时再说。
- en: 'The `conf/routes` file tells the Play framework to use the method `controllers.Application.index`
    to handle requests to `/`. Let''s look at the `Application.scala` file in `app/controllers`,
    where the `index` method is defined:'
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
  zh: '`conf/routes` 文件告诉 Play 框架使用 `controllers.Application.index` 方法来处理对 `/` 的请求。让我们看看
    `app/controllers` 中的 `Application.scala` 文件，其中定义了 `index` 方法：'
- en: '[PRE604]'
  id: totrans-2428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE604]'
- en: 'We see that `controllers.Application.index` refers to the method `index` in
    the class `Application`. This method has return type `Action`. An `Action` is
    just a function that maps HTTP requests to responses. Before explaining this in
    more detail, let''s change the action to:'
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `controllers.Application.index` 指的是 `Application` 类中的 `index` 方法。这个方法返回类型为
    `Action`。`Action` 只是一个将 HTTP 请求映射到响应的函数。在详细解释之前，让我们将操作更改为：
- en: '[PRE605]'
  id: totrans-2430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE605]'
- en: Refresh your browser and you should see the landing page replaced with `"hello
    world"`. By having our action return `Ok("hello, world")`, we are asking Play
    to return an HTTP response with status code 200 (indicating that the request was
    successful) and the body `"hello world"`.
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新浏览器，你应该会看到登录页面被替换为 `"hello world"`。通过让我们的操作返回 `Ok("hello, world")`，我们是在请求
    Play 返回一个状态码为 200 的 HTTP 响应（表示请求成功）和正文 `"hello world"`。
- en: 'Let''s go back to the original content of `index`:'
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 `index` 的原始内容：
- en: '[PRE606]'
  id: totrans-2433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE606]'
- en: 'We can see that this calls the method `views.html.index`. This might appear
    strange, because there is no `views` package anywhere. However, if you look at
    the `app/views` directory, you will notice two files: `index.scala.html` and `main.scala.html`.
    These are templates, which, at compile time, get transformed into Scala functions.
    Let''s have a look at `main.scala.html`:'
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这调用了 `views.html.index` 方法。这看起来可能有些奇怪，因为任何地方都没有 `views` 包。但是，如果你查看 `app/views`
    目录，你会注意到两个文件：`index.scala.html` 和 `main.scala.html`。这些是模板，在编译时，它们被转换成 Scala 函数。让我们看看
    `main.scala.html`：
- en: '[PRE607]'
  id: totrans-2435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE607]'
- en: 'At compile time, this template is compiled to a function `main(title:String)(content:Html)`
    in the package `views.html`. Notice that the function package and name comes from
    the template file name, and the function arguments come from the first line of
    the template. The template contains embedded `@title` and `@content` values, which
    get filled in by the arguments to the function. Let''s experiment with this in
    a Scala console:'
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
  zh: '在编译时，这个模板被编译为 `views.html` 包中的 `main(title: String)(content: Html)` 函数。请注意，函数的包和名称来自模板文件名，而函数参数来自模板的第一行。模板包含嵌入的
    `@title` 和 `@content` 值，这些值由传递给函数的参数填充。让我们在 Scala 控制台中实验一下：'
- en: '[PRE608]'
  id: totrans-2437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE608]'
- en: We can call `views.html.main`, just like we would call a normal Scala function.
    The arguments we pass in get embedded in the correct place, as defined by the
    template in `views/main.scala.html`.
  id: totrans-2438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用 `views.html.main`，就像我们调用一个普通的 Scala 函数一样。我们传递的参数被嵌入到由 `views/main.scala.html`
    中的模板定义的正确位置。
- en: 'This concludes our introductory tour of Play. Let''s briefly go over what we
    have learnt: when a request reaches the Play server, the server reads the URL
    and the HTTP verb and checks that these exist in its `conf/routes` file. It will
    then pass the request to the `Action` defined by the controller for that route.
    This `Action` returns an HTTP response that gets fed back to the browser. In constructing
    the response, the `Action` may make use of a template, which, as far as it is
    concerned is just a function `(arguments list) => String` or `(arguments list)
    => HTML`.'
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 Play 的入门之旅。让我们简要回顾一下我们学到了什么：当一个请求到达 Play 服务器时，服务器读取 URL 和 HTTP 动词，并检查这些是否存在于其
    `conf/routes` 文件中。然后，它将请求传递给为该路由定义的控制器中的 `Action`。这个 `Action` 返回一个 HTTP 响应，该响应被反馈到浏览器。在构建响应时，`Action`
    可能会使用模板，对于它来说，模板只是一个 `(arguments list) => String` 或 `(arguments list) => HTML`
    的函数。
- en: Dynamic routing
  id: totrans-2440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态路由
- en: 'Routing, as we saw, is the mapping of HTTP requests to Scala handlers. Routes
    are stored in `conf/routes`. A route is defined by an HTTP verb, followed by the
    end-point, followed by a Scala function:'
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，路由是将 HTTP 请求映射到 Scala 处理器。路由存储在 `conf/routes` 中。一个路由由一个 HTTP 动词、端点和一个
    Scala 函数定义：
- en: '[PRE609]'
  id: totrans-2442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE609]'
- en: 'We learnt to add new routes by just adding lines to the `routes` file. We are
    not limited to static routes, however. The Play framework lets us include wild
    cards in routes. The value of the wild card can be passed as an argument to the
    controller. To see how this works, let''s create a controller that takes the name
    of a person as argument. In the `Application` object in `app.controllers`, add:'
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学会了通过向`routes`文件中添加行来添加新路由。然而，我们并不局限于静态路由。Play框架允许我们在路由中包含通配符。通配符的值可以作为参数传递给控制器。为了了解这是如何工作的，让我们创建一个以人的名字作为参数的控制器。在`app.controllers`中的`Application`对象中添加：
- en: '[PRE610]'
  id: totrans-2444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE610]'
- en: 'We can now define a route handled by this controller:'
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义由该控制器处理的路由：
- en: '[PRE611]'
  id: totrans-2446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE611]'
- en: If you now point your browser to `127.0.0.1:9000/hello/Jim`, you will see **hello,
    Jim** appear on the screen.
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在将浏览器指向`127.0.0.1:9000/hello/Jim`，你将在屏幕上看到**hello, Jim**。
- en: 'Any string between `:` and the following `/` is treated as a wild card: it
    will match any combination of characters. The value of the wild card can be passed
    to the controller. Note that the wild card can appear anywhere in the URL, and
    there can be more than one wild card. The following are all valid route definitions,
    for instance:'
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在`:`和随后的`/`之间的字符串都被视为通配符：它将匹配任何字符组合。通配符的值可以传递给控制器。请注意，通配符可以出现在URL的任何位置，并且可以有多个通配符。以下都是有效的路由定义，例如：
- en: '[PRE612]'
  id: totrans-2449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE612]'
- en: 'There are many other options for selecting routes and passing arguments to
    the controller. Consult the documentation for the Play framework for a full discussion
    on the routing possibilities: [https://www.playframework.com/documentation/2.4.x/ScalaRouting](https://www.playframework.com/documentation/2.4.x/ScalaRouting).'
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
  zh: 选择路由并将参数传递给控制器有许多其他选项。请参阅Play框架的文档，以全面讨论路由的可能性：[https://www.playframework.com/documentation/2.4.x/ScalaRouting](https://www.playframework.com/documentation/2.4.x/ScalaRouting)。
- en: Tip
  id: totrans-2451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**URL design**'
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
  zh: '**URL设计**'
- en: 'It is generally considered best practice to leave the URL as simple as possible.
    The URL should reflect the hierarchical structure of the information of the website,
    rather than the underlying implementation. GitHub is a very good example of this:
    its URLs make intuitive sense. For instance, the URL for the repository for this
    book is:'
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为，将URL尽可能简化是最佳实践。URL应反映网站信息的层次结构，而不是底层实现。GitHub就是很好的例子：它的URL直观易懂。例如，这本书的仓库URL是：
- en: '[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)'
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/pbugnion/s4ds](https://github.com/pbugnion/s4ds)'
- en: To access the issues page for that repository, add `/issues` to the route. To
    access the first issue, add `/1` to that route. These are called **semantic URLs**
    ([https://en.wikipedia.org/wiki/Semantic_URL](https://en.wikipedia.org/wiki/Semantic_URL)).
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该仓库的问题页面，请在路由中添加`/issues`。要访问第一个问题，请在该路由中添加`/1`。这些被称为**语义URL**([https://en.wikipedia.org/wiki/Semantic_URL](https://en.wikipedia.org/wiki/Semantic_URL))。
- en: Actions
  id: totrans-2456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作
- en: We have talked about routes, and how to pass parameters to controllers. Let's
    now talk about what we can do with the controller.
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了路由以及如何向控制器传递参数。现在让我们谈谈我们可以用控制器做什么。
- en: The method defined in the route must return a `play.api.mvc.Action` instance.
    The `Action` type is a thin wrapper around the type `Request[A] => Result`, where
    `Request[A]` identifies an HTTP request and `Result` is an HTTP response.
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
  zh: 路由中定义的方法必须返回一个`play.api.mvc.Action`实例。`Action`类型是`Request[A] => Result`类型的薄包装，其中`Request[A]`标识一个HTTP请求，`Result`是HTTP响应。
- en: Composing the response
  id: totrans-2459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合响应
- en: 'An HTTP response, as we saw in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web
    APIs"), *Web APIs*, is composed of:'
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第7章](part0059.xhtml#aid-1O8H61 "第7章。Web APIs")中看到的，HTTP响应由以下组成：
- en: the status code (such as 200 for a successful response, or 404 for a missing
    page)
  id: totrans-2461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态码（例如，成功响应的200或缺失页面的404）
- en: the response headers, a key-value list indicating metadata related to the response
  id: totrans-2462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应头，一个表示与响应相关的元数据的键值列表
- en: The response body. This can be HTML for web pages, or JSON, XML or plain text
    (or many other formats). This is generally the bit that we are really interested
    in.
  id: totrans-2463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应正文。这可以是网页的HTML，或JSON、XML或纯文本（或许多其他格式）。这通常是真正感兴趣的部分。
- en: The Play framework defines a `play.api.mvc.Result` object that symbolizes a
    response. The object contains a `header` attribute with the status code and the
    headers, and a `body` attribute containing the body.
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
  zh: Play框架定义了一个`play.api.mvc.Result`对象，它表示一个响应。该对象包含一个`header`属性，包含状态码和头信息，以及一个包含正文的`body`属性。
- en: 'The simplest way to generate a `Result` is to use one of the factory methods
    in `play.api.mvc.Results`. We have already seen the `Ok` method, which generates
    a response with status code 200:'
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 `Result` 的最简单方法就是使用 `play.api.mvc.Results` 中的工厂方法之一。我们已经看到了 `Ok` 方法，它生成状态码为
    200 的响应：
- en: '[PRE613]'
  id: totrans-2466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE613]'
- en: 'Let''s take a step back and open a Scala console so we can understand how this
    works:'
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步，打开一个 Scala 控制台，以便我们理解它是如何工作的：
- en: '[PRE614]'
  id: totrans-2468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE614]'
- en: 'We can see how the `Results.Ok(...)` creates a `Result` object with status
    `200` and (in this case), a single header denoting the content type. The body
    is a bit more complicated: it is an enumerator that can be pushed onto the output
    stream when needed. The enumerator contains the argument passed to `Ok`: `"hello,
    world"`, in this case.'
  id: totrans-2469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `Results.Ok(...)` 是如何创建一个状态为 `200` 的 `Result` 对象，在这个例子中，它包含一个表示内容类型的单个头信息。体部分稍微复杂一些：它是一个枚举器，当需要时可以推送到输出流。枚举器包含传递给
    `Ok` 的参数：在这个例子中是 `"hello, world"`。
- en: 'There are many factory methods in `Results` for returning different status
    codes. Some of the more relevant ones are:'
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
  zh: '`Results` 中有许多用于返回不同状态码的工厂方法。其中一些更相关的如下：'
- en: '`Action { Results.NotFound }`'
  id: totrans-2471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Action { Results.NotFound }`'
- en: '`Action { Results.BadRequest("bad request") }`'
  id: totrans-2472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Action { Results.BadRequest("bad request") }`'
- en: '`Action { Results.InternalServerError("error") }`'
  id: totrans-2473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Action { Results.InternalServerError("error") }`'
- en: '`Action { Results.Forbidden }`'
  id: totrans-2474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Action { Results.Forbidden }`'
- en: '`Action { Results.Redirect("/home") }`'
  id: totrans-2475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Action { Results.Redirect("/home") }`'
- en: For a full list of `Result` factories, consult the API documentation for Results
    ([https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results](https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results)).
  id: totrans-2476
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 `Result` 工厂方法的完整列表，请参阅 Results 的 API 文档 ([https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results](https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.mvc.Results))。
- en: 'We have, so far, been limiting ourselves to passing strings as the content
    of the `Ok` result: `Ok("hello, world")`. We are not, however, limited to passing
    strings. We can pass a JSON object:'
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直限制自己只将字符串作为 `Ok` 结果的内容传递：`Ok("hello, world")`。然而，我们并不局限于传递字符串。我们可以传递一个
    JSON 对象：
- en: '[PRE615]'
  id: totrans-2478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE615]'
- en: 'We will cover interacting with JSON in more detail when we start building the
    API. We can also pass HTML as the content. This is most commonly the case when
    returning a view:'
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始构建 API 时，我们将更详细地介绍与 JSON 的交互。我们也可以传递 HTML 作为内容。这通常是在返回视图时的情况。
- en: '[PRE616]'
  id: totrans-2480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE616]'
- en: Note how the `Content-Type` header is set based on the type of content passed
    to `Ok`. The `Ok` factory uses the `Writeable` type class to convert its argument
    to the body of the response. Thus, any content type for which a `Writeable` type
    class exists can be used as argument to `Ok`. If you are unfamiliar with type
    classes, you might want to read the *Looser coupling with type classes* section
    in [Chapter 5](part0040.xhtml#aid-164MG1 "Chapter 5. Scala and SQL through JDBC"),
    *Scala and SQL through JDBC*.
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `Content-Type` 头是如何根据传递给 `Ok` 的内容类型设置的。`Ok` 工厂使用 `Writeable` 类型类将它的参数转换为响应体。因此，对于任何存在
    `Writeable` 类型类的类型，都可以用作 `Ok` 的参数。如果你对类型类不熟悉，你可能想阅读第 5 章 *使用类型类进行松耦合* 的部分，*Scala
    和 SQL 通过 JDBC*。
- en: Understanding and parsing the request
  id: totrans-2482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和解析请求
- en: 'We now know how to formulate (basic) responses. The other half of the equation
    is the HTTP request. Recall that an `Action` is just a function mapping `Request
    => Result`. We can access the request using:'
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道了如何制定（基本的）响应。等式的另一半是 HTTP 请求。回想一下，`Action` 只是一个将 `Request => Result` 映射的函数。我们可以使用以下方式访问请求：
- en: '[PRE617]'
  id: totrans-2484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE617]'
- en: 'One of the reasons for needing a reference to the request is to access parameters
    in the query string. Let''s modify the `Hello, <name>` example that we wrote earlier
    to, optionally, include a title in the query string. Thus, a URL could be formatted
    as `/hello/Jim?title=Dr`. The `request` instance exposes the `getQueryString`
    method for accessing specific keys in the query string. This method returns `Some[String]`
    if the key is present in the query, or `None` otherwise. We can re-write our `hello`
    controller as:'
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
  zh: 需要引用请求的一个原因是访问查询字符串中的参数。让我们修改我们之前写的 `Hello, <name>` 示例，使其可选地包含查询字符串中的标题。因此，一个
    URL 可以格式化为 `/hello/Jim?title=Dr`。`request` 实例公开了 `getQueryString` 方法，用于访问查询字符串中的特定键。如果键存在于查询中，该方法返回
    `Some[String]`，否则返回 `None`。我们可以将我们的 `hello` 控制器重写为：
- en: '[PRE618]'
  id: totrans-2486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE618]'
- en: Try this out by accessing the URL `127.0.0.1:9000/hello/Odersky?title=Dr` in
    your browser. The browser should display `Hello, Dr Odersky`.
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在浏览器中访问 URL `127.0.0.1:9000/hello/Odersky?title=Dr` 来尝试这个示例。浏览器应该显示 `Hello,
    Dr Odersky`。
- en: 'We have, so far, been concentrating on GET requests. These do not have a body.
    Other types of HTTP request, most commonly POST requests, do contain a body. Play
    lets the user pass *body parsers* when defining the action. The request body will
    be passed through the body parser, which will convert it from a byte stream to
    a Scala type. As a very simple example, let''s define a new route that accepts
    POST requests:'
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于 GET 请求。这些请求没有正文。其他类型的 HTTP 请求，最常见的是 POST 请求，确实包含正文。Play 允许用户在定义操作时传递
    *正文解析器*。请求正文将通过正文解析器传递，它将将其从字节流转换为 Scala 类型。作为一个非常简单的例子，让我们定义一个新的路由，它接受 POST 请求：
- en: '[PRE619]'
  id: totrans-2489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE619]'
- en: 'We will apply the predefined `parse.text` body parser to the incoming request
    body. This converts the body of the request to a string. The `helloPost` controller
    looks like:'
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预定义的 `parse.text` 正文解析器应用于传入的请求正文。这会将请求正文转换为字符串。`helloPost` 控制器看起来像：
- en: '[PRE620]'
  id: totrans-2491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE620]'
- en: Tip
  id: totrans-2492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You cannot test POST requests easily in the browser. You can use cURL instead.
    cURL is a command line utility for dispatching HTTP requests. It is installed
    by default on Mac OS and should be available via the package manager on Linux
    distributions. The following will send a POST request with `"I think that Scala
    is great"` in the body:'
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中轻松测试 POST 请求是不可能的。您可以使用 cURL。cURL 是一个用于发送 HTTP 请求的命令行实用程序。它在 Mac OS 上默认安装，并且应该可以通过
    Linux 发行版的包管理器获得。以下示例将发送一个正文为 `"I think that Scala is great"` 的 POST 请求：
- en: '[PRE621]'
  id: totrans-2494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE621]'
- en: 'This prints the following line to the terminal:'
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在终端打印以下行：
- en: '`Hello. You told me: I think that Scala is great`'
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hello. You told me: I think that Scala is great`'
- en: 'There are several types of built-in body parsers:'
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种内置的正文解析器：
- en: '`parse.file(new File("filename.txt"))` will save the body to a file.'
  id: totrans-2498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse.file(new File("filename.txt"))` 将正文保存到文件中。'
- en: '`parse.json` will parse the body as JSON (we will learn more about interacting
    with JSON in the next section).'
  id: totrans-2499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse.json` 将正文解析为 JSON（我们将在下一节中了解更多关于与 JSON 交互的内容）。'
- en: '`parse.xml` will parse the body as XML.'
  id: totrans-2500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse.xml` 将正文解析为 XML。'
- en: '`parse.urlFormEncoded` will parse the body as returned by submitting an HTML
    form. The `request.body` attribute is a Scala map from `String` to `Seq[String]`,
    mapping each form element to its value(s).'
  id: totrans-2501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse.urlFormEncoded` 将解析由提交 HTML 表单返回的正文。`request.body` 属性是一个从 `String` 到
    `Seq[String]` 的 Scala 映射，将每个表单元素映射到其值（们）。'
- en: 'For a full list of body parsers, the best source is the Scala API documentation
    for `play.api.mvc.BodyParsers.parse` available at: [https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers$parse$](https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers%24parse%24).'
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取正文解析器的完整列表，最佳来源是 `play.api.mvc.BodyParsers.parse` 的 Scala API 文档，可在以下网址找到：[https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers$parse$](https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.mvc.BodyParsers%24parse%24)。
- en: Interacting with JSON
  id: totrans-2503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 JSON 交互
- en: JSON, as we discovered in previous chapters, is becoming the de-facto language
    for communicating structured data over HTTP. If you develop a web application
    or a web API, it is likely that you will have to consume or emit JSON, or both.
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的章节中所发现的，JSON 正在成为通过 HTTP 通信结构化数据的默认语言。如果您开发一个 Web 应用程序或 Web API，您可能需要消费或发射
    JSON，或者两者都要。
- en: In [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*,
    we learned how to parse JSON through `json4s`. The Play framework includes its
    own JSON parser and emitter. Fortunately, it behaves in much the same way as `json4s`.
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](part0059.xhtml#aid-1O8H61 "第 7 章。Web APIs")，*Web APIs* 中，我们学习了如何通过
    `json4s` 解析 JSON。Play 框架包括它自己的 JSON 解析器和发射器。幸运的是，它的行为与 `json4s` 非常相似。
- en: Let's imagine that we are building an API that summarizes information about
    GitHub repositories. Our API will emit a JSON array listing a user's repositories
    when queried about a specific user (much like the GitHub API, but with just a
    subset of fields).
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们正在构建一个总结 GitHub 仓库信息的 API。当查询特定用户时，我们的 API 将输出一个 JSON 数组，列出该用户的仓库（类似于
    GitHub API，但只包含部分字段）。
- en: 'Let''s start by defining a model for the repository. In Play applications,
    models are normally stored in the folder `app/models`, in the `models` package:'
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义一个仓库的模型。在 Play 应用程序中，模型通常存储在 `app/models` 文件夹中的 `models` 包下：
- en: '[PRE622]'
  id: totrans-2508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE622]'
- en: 'Let''s add a route to our application that serves arrays of repos for a particular
    user. In `conf/routes`, add the following line:'
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在应用程序中添加一个路由，为特定用户提供仓库数组。在 `conf/routes` 中添加以下行：
- en: '[PRE623]'
  id: totrans-2510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE623]'
- en: 'Let''s now implement the framework for the controller. We will create a new
    controller for our API, imaginatively called `Api`. For now, we will just have
    the controller return dummy data. This is what the code looks like (we will explain
    the details shortly):'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现控制器的框架。我们将为我们的 API 创建一个新的控制器，暂时命名为 `Api`。目前，我们只是让控制器返回一些示例数据。代码如下（我们将在稍后解释细节）：
- en: '[PRE624]'
  id: totrans-2512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE624]'
- en: 'If you point your web browser to `127.0.0.1:9000/api/repos/odersky`, you should
    now see the following JSON object:'
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将你的网络浏览器指向 `127.0.0.1:9000/api/repos/odersky`，你现在应该看到以下 JSON 对象：
- en: '[PRE625]'
  id: totrans-2514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE625]'
- en: The only tricky part of this code is the conversion from `Repo` to JSON. We
    call `Json.toJson` on `data`, an instance of type `List[Repo]`. The `toJson` method
    relies on the existence of a type class `Writes[T]` for the type `T` passed to
    it.
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中唯一棘手的部分是将 `Repo` 转换为 JSON。我们在 `data` 上调用 `Json.toJson`，它是 `List[Repo]`
    类型的实例。`toJson` 方法依赖于传递给它的类型 `T` 存在的类型类 `Writes[T]`。
- en: 'The Play framework makes extensive use of type classes to define how to convert
    models to specific formats. Recall that we learnt how to write type classes in
    the context of SQL and MongoDB. The Play framework''s expectations are very similar:
    for the `Json.toJson` method to work on an instance of type `Repo`, there must
    be a `Writes[Repo]` implementation available that specifies how to transform `Repo`
    objects to JSON.'
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
  zh: Play 框架广泛使用类型类来定义如何将模型转换为特定格式。回想一下，我们学习了如何在 SQL 和 MongoDB 的上下文中编写类型类。Play 框架的期望非常相似：为了使
    `Json.toJson` 方法能够在 `Repo` 类型的实例上工作，必须有 `Writes[Repo]` 的实现可用，该实现指定了如何将 `Repo`
    对象转换为 JSON。
- en: 'In the Play framework, the `Writes[T]` type class defines a single method:'
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Play 框架中，`Writes[T]` 类型类定义了一个单一的方法：
- en: '[PRE626]'
  id: totrans-2518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE626]'
- en: '`Writes` methods for built-in simple types and for collections are already
    built into the Play framework, so we do not need to worry about defining `Writes[Boolean]`,
    for instance.'
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
  zh: '`Writes` 方法为内置简单类型和集合已经内置到 Play 框架中，因此我们不需要担心定义 `Writes[Boolean]` 等。'
- en: The `Writes[Repo]` instance is commonly defined either directly in the controller,
    if it is just used for that controller, or in the `Repo` companion object, where
    it can be used across several controllers. For simplicity, we just embedded it
    in the controller.
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
  zh: '`Writes[Repo]` 实例通常直接在控制器中定义，如果它仅用于该控制器，或者在 `Repo` 伴生对象中定义，这样它就可以在多个控制器中使用。为了简单起见，我们只是将其嵌入到控制器中。'
- en: Note how type-classes allow for separation of concerns. The model just defines
    the `Repo` type, without attaching any behavior. The `Writes[Repo]` type class
    just knows how to convert from a `Repo` instance to JSON, but knows nothing of
    the context in which it is used. Finally, the controller just knows how to create
    a JSON HTTP response.
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
  zh: 注意类型类如何实现关注点的分离。模型仅定义了 `Repo` 类型，而没有附加任何行为。`Writes[Repo]` 类型类只知道如何将 `Repo` 实例转换为
    JSON，但不知道它在什么上下文中被使用。最后，控制器只知道如何创建 JSON HTTP 响应。
- en: Congratulations, you have just defined a web API that returns JSON! In the next
    section, we will learn how to fetch data from the GitHub web API to avoid constantly
    returning the same array.
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你刚刚定义了一个返回 JSON 的 Web API！在下一节中，我们将学习如何从 GitHub Web API 获取数据，以避免不断返回相同的数组。
- en: Querying external APIs and consuming JSON
  id: totrans-2523
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询外部 API 和消费 JSON
- en: So far, we have learnt how to provide the user with a dummy JSON array of repositories
    in response to a request to `/api/repos/:username`. In this section, we will replace
    the dummy data with the user's actual repositories, dowloaded from GitHub.
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何响应 `/api/repos/:username` 的请求，向用户提供一个示例 JSON 数组形式的仓库。在本节中，我们将用从
    GitHub 下载的实际仓库数据替换示例数据。
- en: In [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"), *Web APIs*,
    we learned how to query the GitHub API using Scala's `Source.fromURL` method and
    `scalaj-http`. It should come as no surprise that the Play framework implements
    its own library for interacting with external web services.
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](part0059.xhtml#aid-1O8H61 "第 7 章。Web APIs")，*Web APIs* 中，我们学习了如何使用
    Scala 的 `Source.fromURL` 方法以及 `scalaj-http` 查询 GitHub API。Play 框架实现自己的库以与外部 Web
    服务交互应该不会让人感到惊讶。
- en: 'Let''s edit the `Api` controller to fetch information about a user''s repositories
    from GitHub, rather than using dummy data. When called with a username as argument,
    the controller will:'
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编辑 `Api` 控制器以从 GitHub 获取有关用户仓库的信息，而不是使用示例数据。当以用户名作为参数调用时，控制器将：
- en: Send a GET request to the GitHub API for that user's repositories.
  id: totrans-2527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 GitHub API 发送 GET 请求以获取该用户的仓库信息。
- en: Interpret the response, converting the body from a JSON object to a `List[Repo]`.
  id: totrans-2528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释响应，将体从 JSON 对象转换为 `List[Repo]`。
- en: Convert from the `List[Repo]` to a JSON array, forming the response.
  id: totrans-2529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `List[Repo]` 转换为 JSON 数组，形成响应。
- en: 'We start by giving the full code listing before explaining the thornier parts
    in detail:'
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先给出完整的代码列表，然后再详细解释更复杂的部分：
- en: '[PRE627]'
  id: totrans-2531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE627]'
- en: 'If you have written all this, point your browser to, for instance, `127.0.0.1:9000/api/repos/odersky`
    to see the list of repositories owned by Martin Odersky:'
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经编写了所有这些，请将浏览器指向例如 `127.0.0.1:9000/api/repos/odersky` 来查看 Martin Odersky
    拥有的仓库列表：
- en: '[PRE628]'
  id: totrans-2533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE628]'
- en: This code sample is a lot to take in, so let's break it down.
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例内容很多，所以让我们将其分解。
- en: Calling external web services
  id: totrans-2535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调用外部网络服务
- en: The first step in querying external APIs is to import the `WS` object, which
    defines factory methods for creating HTTP requests. These factory methods rely
    on a reference to an implicit Play application in the namespace. The easiest way
    to ensure this is the case is to import `play.api.Play.current`, a reference to
    the current application.
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
  zh: 查询外部 API 的第一步是导入 `WS` 对象，它定义了创建 HTTP 请求的工厂方法。这些工厂方法依赖于命名空间中隐含的 Play 应用程序的引用。确保这种情况的最简单方法是导入
    `play.api.Play.current`，这是对当前应用程序的引用。
- en: 'Let''s ignore the `readsRepoFromGithub` type class for now and jump straight
    to the controller body. The URL that we want to hit with a GET request is `"https://api.github.com/users/$username/repos"`,
    with the appropriate value for `$username`. We create a GET request with `WS.url(url).get()`.
    We can also add headers to an existing request. For instance, to specify the content
    type, we could have written:'
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们忽略 `readsRepoFromGithub` 类型类，直接跳到控制器主体。我们想要通过 GET 请求访问的 URL 是 `"https://api.github.com/users/$username/repos"`，其中
    `$username` 是适当的值。我们使用 `WS.url(url).get()` 创建一个 GET 请求。我们还可以向现有请求添加头信息。例如，为了指定内容类型，我们可以这样写：
- en: '[PRE629]'
  id: totrans-2538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE629]'
- en: 'We can use headers to pass a GitHub OAuth token using:'
  id: totrans-2539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用头信息通过以下方式传递 GitHub OAuth 令牌：
- en: '[PRE630]'
  id: totrans-2540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE630]'
- en: To formulate a POST request, rather than a GET request, replace the final `.get()`
    with `.post(data)`. Here, `data` can be JSON, XML or a string.
  id: totrans-2541
  prefs: []
  type: TYPE_NORMAL
  zh: 要形成 POST 请求而不是 GET 请求，将最后的 `.get()` 替换为 `.post(data)`。在这里，`data` 可以是 JSON、XML
    或字符串。
- en: Adding `.get` or `.post` fires the request, returning a `Future[WSResponse]`.
    You should, by now, be familiar with futures. By writing `response.map { r =>
    ... }`, we specify a transformation to be executed on the future result, when
    it returns. The transformation verifies the response's status, returning `NotFound`
    if the status code of the response is anything but 200.
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 `.get` 或 `.post` 触发请求，返回一个 `Future[WSResponse]`。到现在为止，您应该熟悉 futures。通过编写
    `response.map { r => ... }`，我们指定在 future 结果返回时要执行的可转换操作，该操作验证响应的状态，如果响应的状态码不是
    200，则返回 `NotFound`。
- en: Parsing JSON
  id: totrans-2543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析 JSON
- en: 'If the status code is 200, the callback parses the response body to JSON and
    converts the parsed JSON to a `List[Repo]` instance. We already know how to convert
    from a `Repo` object to JSON using the `Writes[Repo]` type class. The converse,
    going from JSON to a `Repo` object, is a little more challenging, because we have
    to account for incorrectly formatted JSON. To this effect, the Play framework
    provides the `.validate[T]` method on JSON objects. This method tries to convert
    the JSON to an instance of type `T`, returning `JsSuccess` if the JSON is well-formatted,
    or `JsError` otherwise (similar to Scala''s `Try` object). The `.validate` method
    relies on the existence of a type class `Reads[Repo]`. Let''s experiment with
    a Scala console:'
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态码是 200，回调将解析响应体为 JSON，并将解析后的 JSON 转换为 `List[Repo]` 实例。我们已经知道如何使用 `Writes[Repo]`
    类型类将 `Repo` 对象转换为 JSON。反过来，从 JSON 到 `Repo` 对象的转换要复杂一些，因为我们必须考虑格式不正确的 JSON。为此，Play
    框架在 JSON 对象上提供了 `.validate[T]` 方法。此方法尝试将 JSON 转换为类型 `T` 的实例，如果 JSON 格式良好，则返回 `JsSuccess`，否则返回
    `JsError`（类似于 Scala 的 `Try` 对象）。`.validate` 方法依赖于类型类 `Reads[Repo]` 的存在。让我们在 Scala
    控制台中实验一下：
- en: '[PRE631]'
  id: totrans-2545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE631]'
- en: 'Using `Json.parse` converts a string to an instance of `JsValue`, the super-type
    for JSON instances. We can access specific fields in `parsedJson` using XPath-like
    syntax (if you are not familiar with XPath-like syntax, you might want to read
    [Chapter 6](part0051.xhtml#aid-1GKCM2 "Chapter 6. Slick – A Functional Interface
    for SQL"), *Slick – A Functional Interface for SQL*):'
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Json.parse` 将字符串转换为 `JsValue` 实例，它是 JSON 实例的超类型。我们可以使用类似 XPath 的语法访问 `parsedJson`
    中的特定字段（如果您不熟悉类似 XPath 的语法，您可能想阅读[第 6 章](part0051.xhtml#aid-1GKCM2 "第 6 章。Slick
    – 一个 SQL 的函数式接口")，*Slick – 一个 SQL 的函数式接口*）：
- en: '[PRE632]'
  id: totrans-2547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE632]'
- en: 'XPath-like lookups return an instance with type `JsLookupResult`. This takes
    two values: either `JsDefined`, if the path is valid, or `JsUndefined` if it is
    not:'
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
  zh: XPath 类似的查找返回一个类型为 `JsLookupResult` 的实例。它包含两个值：如果路径有效，则为 `JsDefined`，如果无效，则为
    `JsUndefined`：
- en: '[PRE633]'
  id: totrans-2549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE633]'
- en: 'To go from a `JsLookupResult` instance to a String in a type-safe way, we can
    use the `.validate[String]` method:'
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
  zh: 要以类型安全的方式将 `JsLookupResult` 实例转换为 String，我们可以使用 `.validate[String]` 方法：
- en: '[PRE634]'
  id: totrans-2551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE634]'
- en: 'The `.validate[T]` method returns either `JsSuccess` if the `JsDefined` instance
    could be successfully cast to `T`, or `JsError` otherwise. To illustrate the latter,
    let''s try validating this as an `Int`:'
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
  zh: '`.validate[T]` 方法返回 `JsSuccess`，如果 `JsDefined` 实例可以被成功转换为 `T`，否则返回 `JsError`。为了说明后者，让我们尝试将其验证为
    `Int`：'
- en: '[PRE635]'
  id: totrans-2553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE635]'
- en: 'Calling `.validate` on an instance of type `JsUndefined` also returns in a
    `JsError`:'
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `JsUndefined` 实例上调用 `.validate` 也会返回 `JsError`：
- en: '[PRE636]'
  id: totrans-2555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE636]'
- en: 'To convert from an instance of `JsResult[T]` to an instance of type `T`, we
    can use pattern matching:'
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `JsResult[T]` 实例转换为类型 `T` 的实例，我们可以使用模式匹配：
- en: '[PRE637]'
  id: totrans-2557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE637]'
- en: We can now use `.validate` to cast JSON to simple types in a type-safe manner.
    But, in the code example, we used `.validate[Repo]`. This works provided a `Reads[Repo]`
    type class is implicitly available in the namespace.
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `.validate` 以类型安全的方式将 JSON 转换为简单类型。但在代码示例中，我们使用了 `.validate[Repo]`。这只有在命名空间中隐式可用
    `Reads[Repo]` 类型类时才有效。
- en: 'The most common way of defining `Reads[T]` type classes is through a DSL provided
    in `import play.api.libs.functional.syntax._`. The DSL works by chaining operations
    returning either `JsSuccess` or `JsError` together. Discussing exactly how this
    DSL works is outside the scope of this chapter (see, for instance, the Play framework
    documentation page on JSON combinators: [https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators](https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators)).
    We will stick to discussing the syntax.'
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 `Reads[T]` 类型类最常见的方式是通过在 `import play.api.libs.functional.syntax._` 中提供的
    DSL（领域特定语言）。该 DSL 通过链式操作返回 `JsSuccess` 或 `JsError` 来工作。具体讨论这个 DSL 的工作原理超出了本章的范围（例如，可以参考
    Play 框架关于 JSON 组合器的文档页面：[https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators](https://www.playframework.com/documentation/2.4.x/ScalaJsonCombinators)）。我们将坚持讨论语法。
- en: '[PRE638]'
  id: totrans-2560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE638]'
- en: 'The `Reads` type class is defined in two stages. The first chains together
    `read[T]` methods with `and`, combining successes and errors. The second uses
    the apply method of the companion object of a case class (or `Tuple` instance)
    to construct the object, provided the first stage completed successfully. Now
    that we have defined the type class, we can call `validate[Repo]` on a `JsValue`
    object:'
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
  zh: '`Reads` 类型类分为两个阶段定义。第一阶段通过 `read[T]` 方法与 `and` 链接起来，结合成功和错误。第二阶段使用案例类（或 `Tuple`
    实例）的伴生对象的 `apply` 方法来构建对象，前提是第一阶段成功完成。现在我们已经定义了类型类，我们可以在 `JsValue` 对象上调用 `validate[Repo]`：'
- en: '[PRE639]'
  id: totrans-2562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE639]'
- en: 'We can then use pattern matching to extract the `Repo` object from the `JsSuccess`
    instance:'
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用模式匹配从 `JsSuccess` 实例中提取 `Repo` 对象：
- en: '[PRE640]'
  id: totrans-2564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE640]'
- en: We have, so far, only talked about validating single repos. The Play framework
    defines type classes for collection types, so, provided `Reads[Repo]` is defined,
    `Reads[List[Repo]]` will also be defined.
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了验证单个仓库。Play 框架为集合类型定义了类型类，因此，如果 `Reads[Repo]` 被定义，`Reads[List[Repo]]`
    也会被定义。
- en: Now that we understand how to extract Scala objects from JSON, let's get back
    to the code. If we manage to successfully convert the repositories to a `List[Repo]`,
    we emit it again as JSON. Of course, converting from GitHub's JSON representation
    of a repository to a Scala object, and from that Scala object directly to our
    JSON representation of the object, might seem convoluted. However, if this were
    a real application, we would have additional logic. We could, for instance, store
    repos in a cache, and try and fetch from that cache instead of querying the GitHub
    API. Converting from JSON to Scala objects as early as possible decouples the
    code that we write from the way GitHub returns repositories.
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何从 JSON 中提取 Scala 对象，让我们回到代码。如果我们能够成功将仓库转换为 `List[Repo]`，我们再次将其作为
    JSON 发射。当然，将 GitHub 的仓库 JSON 表示转换为 Scala 对象，然后直接转换为我们的对象 JSON 表示可能看起来很复杂。然而，如果这是一个真实的应用程序，我们会有额外的逻辑。例如，我们可以将仓库存储在缓存中，并尝试从缓存中获取而不是查询
    GitHub API。尽早将 JSON 转换为 Scala 对象可以解耦我们编写的代码与 GitHub 返回仓库的方式。
- en: Asynchronous actions
  id: totrans-2567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步操作
- en: The last bit of the code sample that is new is the call to `Action.async`, rather
    than just `Action`. Recall that an `Action` instance is a thin wrapper around
    a `Request => Result` method. Our code, however, returns a `Future[Result]`, rather
    than a `Result`. When that is the case, use the `Action.async` to construct the
    action, rather than `Action` directly. Using `Action.async` tells the Play framework
    that the code creating the `Action` is asynchronous.
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
  zh: 代码示例中新增的最后部分是调用 `Action.async`，而不是仅仅 `Action`。回想一下，`Action` 实例是 `Request =>
    Result` 方法的薄包装。然而，我们的代码返回一个 `Future[Result]`，而不是 `Result`。在这种情况下，使用 `Action.async`
    来构建动作，而不是直接使用 `Action`。使用 `Action.async` 告诉 Play 框架，创建 `Action` 的代码是异步的。
- en: 'Creating APIs with Play: a summary'
  id: totrans-2569
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Play 创建 API：总结
- en: 'In the last section, we deployed an API that responds to GET requests. Since
    this is a lot to take in, let''s summarize how to go about API creation:'
  id: totrans-2570
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们部署了一个响应 GET 请求的 API。由于这需要很多理解，让我们总结一下如何进行 API 创建：
- en: Define appropriate routes in `/conf/routes`, using wildcards in the URL as needed.
  id: totrans-2571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `/conf/routes` 中定义适当的路由，根据需要使用 URL 中的通配符。
- en: Create Scala case classes in `/app/models` to represent the models used by the
    API.
  id: totrans-2572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `/app/models` 中创建 Scala 的案例类来表示 API 使用的模型。
- en: Create `Write[T]` methods to write models to JSON or XML so that they can be
    returned by the API.
  id: totrans-2573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `Write[T]` 方法，以便将模型写入 JSON 或 XML，这样它们就可以通过 API 返回。
- en: Bind the routes to controllers. If the controllers need to do more than a trivial
    amount a work, wrap the work in a future to avoid blocking the server.
  id: totrans-2574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将路由绑定到控制器。如果控制器需要做更多的工作，将工作包装在 future 中以避免阻塞服务器。
- en: There are many more useful components of the Play framework that you are likely
    to need, such as, for instance, how to use Slick to access SQL databases. We do
    not, unfortunately, have time to cover these in this introduction. The Play framework
    has extensive, well-written documentation that will fill the gaping holes in this
    tutorial.
  id: totrans-2575
  prefs: []
  type: TYPE_NORMAL
  zh: Play 框架中还有许多你可能需要的更有用的组件，例如，例如如何使用 Slick 访问 SQL 数据库。不幸的是，我们没有时间在这篇介绍中涵盖这些内容。Play
    框架有详尽、写得很好的文档，将填补这篇教程中的空白。
- en: 'Rest APIs: best practice'
  id: totrans-2576
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rest API：最佳实践
- en: 'As the Internet matures, REST (representational state transfer) APIs are emerging
    as the most reliable design pattern for web APIs. An API is described as *RESTful*
    if it follows these guiding principles:'
  id: totrans-2577
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网的成熟，REST（表示状态转移）API 正在成为网络 API 最可靠的设计模式。如果一个 API 遵循以下指导原则，则称为 *RESTful*：
- en: The API is designed as a set of resources. For instance, the GitHub API provides
    information about users, repositories, followers, etc. Each user, or repository,
    is a specific resource. Each resource can be addressed through a different HTTP
    end-point.
  id: totrans-2578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 被设计为一组资源。例如，GitHub API 提供了关于用户、仓库、关注者等信息。每个用户或仓库都是一个特定的资源。每个资源都可以通过不同的 HTTP
    端点进行访问。
- en: The URLs should be simple and should identify the resource clearly. For instance,
    `api.github.com/users/odersky` is simple and tells us clearly that we should expect
    information about the user Martin Odersky.
  id: totrans-2579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网址应当简洁明了，并能清楚地标识资源。例如，`api.github.com/users/odersky` 就很简单，清楚地告诉我们应该期待关于用户马丁·奥德斯基的信息。
- en: There is no *world resource* that contains all the information about the system.
    Instead, top-level resources contain links to more specialized resources. For
    instance, the user resource in the GitHub API contains links to that user's repositories
    and that user's followers, rather than having all that information embedded in
    the user resource directly.
  id: totrans-2580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有一个包含系统所有信息的 *全局资源*。相反，顶级资源包含指向更专业资源的链接。例如，GitHub API 中的用户资源包含指向该用户仓库和该用户关注者的链接，而不是直接在用户资源中嵌入所有这些信息。
- en: The API should be discoverable. The response to a request for a specific resource
    should contain URLs for related resources. When you query the user resource on
    GitHub, the response contains the URL for accessing that user's followers, repositories
    etc. The client should use the URLs provided by the API, rather than attempting
    to construct them client-side. This makes the client less brittle to changes in
    the API.
  id: totrans-2581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 应该是可发现的。对特定资源的请求的响应应包含相关资源的 URL。当你查询 GitHub 上的用户资源时，响应包含访问该用户关注者、仓库等的 URL。客户端应使用
    API 提供的 URL，而不是尝试在客户端构建它们。这使客户端对 API 的变化更加稳健。
- en: 'There should be as little state maintained on the server as possible. For instance,
    when querying the GitHub API, we must pass the authentication token with every
    request, rather than expecting our authentication status to be *remembered* on
    the server. Having each interaction be independent of the history provides much
    better scalability: if any interaction can be handled by any server, load balancing
    is much easier.'
  id: totrans-2582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应尽可能在服务器上保持最少的状态。例如，在查询GitHub API时，我们必须在每次请求中传递认证令牌，而不是期望我们的认证状态在服务器上被*记住*。使每次交互独立于历史记录提供了更好的可扩展性：如果任何交互可以由任何服务器处理，负载均衡就更容易实现。
- en: Summary
  id: totrans-2583
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the Play framework as a tool for building web
    APIs. We built an API that returns a JSON array of a user's GitHub repositories.
    In the next chapter, we will build on this API and construct a single-page application
    to represent this data graphically.
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Play框架作为构建Web API的工具。我们构建了一个返回用户GitHub仓库JSON数组的API。在下一章中，我们将在此基础上构建API，并构建一个单页应用程序来图形化表示这些数据。
- en: References
  id: totrans-2585
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'This Wikipedia page gives information on semantic URLs: [https://en.wikipedia.org/wiki/Semantic_URL](https://en.wikipedia.org/wiki/Semantic_URL)
    and [http://apiux.com/2013/04/03/url-design-restful-web-services/](http://apiux.com/2013/04/03/url-design-restful-web-services/).'
  id: totrans-2586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下维基百科页面提供了关于语义URL的信息：[https://zh.wikipedia.org/wiki/%E8%AF%AD%E4%B9%89_URL](https://zh.wikipedia.org/wiki/%E8%AF%AD%E4%B9%89_URL)
    和 [http://apiux.com/2013/04/03/url-design-restful-web-services/](http://apiux.com/2013/04/03/url-design-restful-web-services/)。
- en: For a much more in depth discussion of the Play framework, I suggest *Play Framework
    Essentials* by *Julien Richard-Foy*.
  id: totrans-2587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Play框架的更深入讨论，我建议阅读*Julien Richard-Foy*的《Play框架基础》。
- en: '*REST in Practice: Hypermedia and Systems Architecture*, by *Jim Webber*, *Savas
    Parastatidis* and *Ian Robinson* describes how to architect REST APIs.'
  id: totrans-2588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《实践中的REST：超媒体和系统架构》，作者*Jim Webber*、*Savas Parastatidis*和*Ian Robinson*，描述了如何设计REST
    API。
- en: Chapter 14. Visualization with D3 and the Play Framework
  id: totrans-2589
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章. 使用D3和Play框架进行可视化
- en: In the previous chapter, we learned about the Play framework, a web framework
    for Scala. We built an API that returns a JSON array describing a user's GitHub
    repositories.
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了Play框架，一个Scala的Web框架。我们构建了一个返回描述用户GitHub仓库的JSON数组的API。
- en: In this chapter, we will construct a fully-fledged web application that displays
    a table and a chart describing a user's repositories. We will learn to integrate
    **D3.js**, a JavaScript library for building data-driven web pages, with the Play
    framework. This will set you on the path to building compelling interactive visualizations
    that showcase results obtained with machine learning.
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个完整的Web应用程序，显示一个表格和一个图表，描述用户的仓库。我们将学习如何将**D3.js**，一个用于构建数据驱动Web页面的JavaScript库，与Play框架集成。这将使你走上构建引人注目的交互式可视化之路，展示使用机器学习获得的结果。
- en: This chapter assumes that you are familiar with HTML, CSS, and JavaScript. We
    present references at the end of the chapter. You should also have read the previous
    chapter.
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设你已经熟悉HTML、CSS和JavaScript。我们将在本章末尾提供参考资料。你还应该阅读上一章。
- en: GitHub user data
  id: totrans-2593
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GitHub用户数据
- en: 'We will build a single-page application that uses, as its backend, the API
    developed in the previous chapter. The application contains a form where the user
    enters the login name for a GitHub account. The application queries the API to
    get a list of repositories for that user and displays them on the screen as both
    a table and a pie chart summarizing programming language use for that user:'
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个单页应用程序，其后端使用上一章开发的API。该应用程序包含一个表单，用户可以输入GitHub账户的登录名。应用程序查询API以获取该用户的仓库列表，并在屏幕上以表格和饼图的形式显示，总结该用户使用的编程语言：
- en: '![GitHub user data](img/image01234.jpeg)'
  id: totrans-2595
  prefs: []
  type: TYPE_IMG
  zh: '![GitHub用户数据](img/image01234.jpeg)'
- en: To see a live version of the application, head over to [http://app.scala4datascience.com](http://app.scala4datascience.com).
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看应用程序的实时版本，请访问 [http://app.scala4datascience.com](http://app.scala4datascience.com)。
- en: Do I need a backend?
  id: totrans-2597
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我需要后端吗？
- en: 'In the previous chapter, we learned about the client-server model that underpins
    how the internet works: when you enter a website URL in your browser, the server
    serves HTML, CSS, and JavaScript to your browser, which then renders it in the
    appropriate manner.'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了支撑互联网工作方式的客户端-服务器模型：当你在你浏览器中输入一个网站URL时，服务器会向你的浏览器提供HTML、CSS和JavaScript，然后浏览器以适当的方式渲染它们。
- en: What does this all mean for you? Arguably the second question that you should
    be asking yourself when building a web application is whether you need to do any
    server-side processing (right after "is this really going to be worth the effort?").
    Could you just create an HTML web-page with some JavaScript?
  id: totrans-2599
  prefs: []
  type: TYPE_NORMAL
  zh: '这对你意味着什么？当构建Web应用时，你应考虑的第二个问题可能是你是否需要进行任何服务器端处理（在“这真的值得付出努力吗？”之后）。你能否仅仅创建一个带有一些JavaScript的HTML网页？ '
- en: 'You can get away without a backend if the data needed to build the whole application
    is small enough: typically a few megabytes. If your application is larger, you
    will need a backend to transfer just the data the client currently needs. Surprisingly,
    you can often build visualizations without a backend: while data science is accustomed
    to dealing with terabytes of data, the goal of the data science process is often
    condensing these huge data sets to a few meaningful numbers.'
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
  zh: 如果构建整个应用所需的数据足够小，你就可以不使用后端：通常只有几兆字节。如果你的应用更大，你需要一个后端来传输客户端当前需要的仅有的数据。令人惊讶的是，你通常可以在没有后端的情况下构建可视化：虽然数据科学通常习惯于处理TB级别的数据，但数据科学流程的目标通常是将这些庞大的数据集压缩成几个有意义的数字。
- en: 'Having a backend also lets you include logic invisible to the client. If you
    need to validate a password, you clearly cannot send the code to do that to the
    client computer: it needs to happen out of sight, on the server.'
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有后端还可以让你包含对客户端不可见的逻辑。如果你需要验证密码，显然你不能将执行该操作的代码发送到客户端计算机：它需要在看不见的地方，在服务器上完成。
- en: If your application is small enough and you do not need to do any server-side
    processing, stop reading this chapter, brush up on your JavaScript if you have
    to, and forget about Scala for now. Not having to worry about building a backend
    will make your life easier.
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用足够小，且不需要进行任何服务器端处理，就停止阅读这一章，如果你需要的话，复习一下JavaScript，现在先别考虑Scala。不必担心构建后端会使你的生活更轻松。
- en: 'Clearly, however, we do not have that freedom for the application that we want
    to build: the user could enter the name of anyone on GitHub. Finding information
    about that user requires a backend with access to tremendous storage and querying
    capacity (which we simulate by just forwarding the request to the GitHub API and
    re-interpreting the response).'
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显然我们并没有为我们要构建的应用拥有这样的自由度：用户可以输入GitHub上任何人的名字。查找关于该用户的信息需要后端访问巨大的存储和查询能力（我们通过仅将请求转发到GitHub
    API并重新解释响应来模拟）。
- en: JavaScript dependencies through web-jars
  id: totrans-2604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过web-jars使用JavaScript依赖项
- en: 'One of the challenges of developing web applications is that we are writing
    two quasi-separate programs: the server-side program and the client-side program.
    These generally require different technologies. In particular, for any but the
    most trivial application, we must keep track of JavaScript libraries, and integrate
    processing the JavaScript code (for instance, for minification) in the build process.'
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
  zh: 开发Web应用的一个挑战是，我们需要编写两个近乎独立的程序：服务器端程序和客户端程序。这些程序通常需要不同的技术。特别是，对于任何非最简单应用，我们必须跟踪JavaScript库，并在构建过程中集成处理JavaScript代码（例如，进行压缩）。
- en: 'The Play framework manages JavaScript dependencies through *web-jars*. These
    are just JavaScript libraries packaged as jars. They are deployed on Maven Central,
    which means that we can just add them as dependencies to our `build.sbt` file.
    For this application, we will need the following JavaScript libraries:'
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
  zh: Play框架通过*web-jars*管理JavaScript依赖项。这些只是打包成jar文件的JavaScript库。它们部署在Maven Central上，这意味着我们只需将它们添加到我们的`build.sbt`文件中作为依赖项。对于这个应用，我们需要以下JavaScript库：
- en: Require.js, a library for writing modular JavaScript
  id: totrans-2607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Require.js，一个用于编写模块化JavaScript的库
- en: JQuery
  id: totrans-2608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JQuery
- en: Bootstrap
  id: totrans-2609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bootstrap
- en: Underscore.js, a library that adds many functional constructs and client-side
    templating.
  id: totrans-2610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Underscore.js，一个添加了许多功能构造和客户端模板的库。
- en: D3, the graph plotting library
  id: totrans-2611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D3，一个图形绘图库
- en: NVD3, a graph library built on top of D3
  id: totrans-2612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVD3，一个基于D3构建的图形库
- en: 'If you are planning on coding up the examples provided in this chapter, the
    easiest will be for you to start from the code for the previous chapter (You can
    download the code for [Chapter 13](part0125.xhtml#aid-3N6MA1 "Chapter 13. Web
    APIs with Play"), *Web APIs with Play*, from GitHub: [https://github.com/pbugnion/s4ds/tree/master/chap13](https://github.com/pbugnion/s4ds/tree/master/chap13)).
    We will assume this as a starting point here onwards.'
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划编写本章提供的示例，最简单的方法是从上一章的代码开始（您可以从GitHub下载[第13章](part0125.xhtml#aid-3N6MA1
    "第13章。使用Play的Web API")的代码，*使用Play的Web API*：[https://github.com/pbugnion/s4ds/tree/master/chap13](https://github.com/pbugnion/s4ds/tree/master/chap13)）。从现在起，我们将假设这是一个起点。
- en: 'Let''s include the dependencies on the web-jars in the `build.sbt` file:'
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`build.sbt`文件中包含对web-jars的依赖项：
- en: '[PRE641]'
  id: totrans-2615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE641]'
- en: Fetch the modules by running `activator` `update`. Once you have done this,
    you will notice the JavaScript libraries in `target/web/public/main/lib`.
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行`activator update`来获取模块。一旦完成此操作，您将注意到`target/web/public/main/lib`中的JavaScript库。
- en: 'Towards a web application: HTML templates'
  id: totrans-2617
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迈向网络应用：HTML模板
- en: In the previous chapter, we briefly saw how to construct HTML templates by interleaving
    Scala snippets in an HTML file. We saw that templates are compiled to Scala functions,
    and we learned how to call these functions from the controllers.
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们简要介绍了如何在HTML文件中交错Scala片段来构建HTML模板的方法。我们了解到模板被编译为Scala函数，并学习了如何从控制器中调用这些函数。
- en: In single-page applications, the majority of the logic governing what is actually
    displayed in the browser resides in the client-side JavaScript, not in the server.
    The pages served by the server contain the bare-bones HTML framework.
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
  zh: 在单页应用中，控制浏览器实际显示的大多数逻辑位于客户端JavaScript中，而不是服务器端。服务器提供的页面包含基本的HTML框架。
- en: Let's create the HTML layout for our application. We will save this in `views/index.scala.html`.
    The template will just contain the layout for the application, but will not contain
    any information about any user's repositories. To fetch that information, the
    application will have to query the API developed in the previous chapter. The
    template does not take any parameters, since all the dynamic HTML generation will
    happen client-side.
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的应用程序创建HTML布局。我们将将其保存在`views/index.scala.html`中。该模板将仅包含应用程序的布局，但不会包含任何关于任何用户仓库的信息。要获取这些信息，应用程序必须查询上一章开发的API。该模板不接收任何参数，因为所有动态HTML生成都将发生在客户端。
- en: We use the Bootstrap grid layout to control the HTML layout. If you are not
    familiar with Bootstrap layouts, consult the documentation at [http://getbootstrap.com/css/#grid-example-basic](http://getbootstrap.com/css/#grid-example-basic).
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Bootstrap网格布局来控制HTML布局。如果您不熟悉Bootstrap布局，请查阅[http://getbootstrap.com/css/#grid-example-basic](http://getbootstrap.com/css/#grid-example-basic)的文档。
- en: '[PRE642]'
  id: totrans-2622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE642]'
- en: In the HTML head, we link the CSS stylesheets that we need for the application.
    Instead of specifying the path explicitly, we use the `@routes.Assets.versioned(...)`
    function. This resolves to a URI corresponding to the location where the assets
    are stored post-compilation. The argument passed to the function should be the
    path from `target/web/public/main` to the asset you need.
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML头部，我们链接应用程序所需的CSS样式表。我们不是明确指定路径，而是使用`@routes.Assets.versioned(...)`函数。这解析为编译后资产存储位置的URI。传递给函数的参数应该是从`target/web/public/main`到所需资产的路径。
- en: 'We want to serve the compiled version of this view when the user accesses the
    route `/` on our server. We therefore need to add this route to `conf/routes`:'
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户访问我们的服务器上的路由`/`时，我们希望提供此视图的编译版本。因此，我们需要将此路由添加到`conf/routes`中：
- en: '[PRE643]'
  id: totrans-2625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE643]'
- en: 'The route is served by the `index` function in the `Application` controller.
    All this controller needs to do is serve the `index` view:'
  id: totrans-2626
  prefs: []
  type: TYPE_NORMAL
  zh: 路由由`Application`控制器中的`index`函数提供。此控制器需要做的只是提供`index`视图：
- en: '[PRE644]'
  id: totrans-2627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE644]'
- en: Start the Play framework by running `activator run` in the root directory of
    the application and point your web browser to `127.0.0.1:9000/`. You should see
    the framework for our web application. Of course, the application does not do
    anything yet, since we have not written any of the JavaScript logic yet.
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在应用程序根目录中运行`activator run`来启动Play框架，并将您的网络浏览器指向`127.0.0.1:9000/`。您应该看到我们的网络应用程序框架。当然，应用程序目前还没有做任何事情，因为我们还没有编写任何JavaScript逻辑。
- en: '![Towards a web application: HTML templates](img/image01235.jpeg)'
  id: totrans-2629
  prefs: []
  type: TYPE_IMG
  zh: '![迈向网络应用：HTML模板](img/image01235.jpeg)'
- en: Modular JavaScript through RequireJS
  id: totrans-2630
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过RequireJS模块化JavaScript
- en: 'The simplest way of injecting JavaScript libraries into the namespace is to
    add them to the HTML framework via `<script>...</script>` tags in the HTML header.
    For instance, to add JQuery, we would add the following line to the head of the
    document:'
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
  zh: 将JavaScript库注入命名空间的最简单方法是将它们添加到HTML框架中，通过HTML头部的`<script>...</script>`标签。例如，要添加JQuery，我们会在文档的头部添加以下行：
- en: '[PRE645]'
  id: totrans-2632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE645]'
- en: While this works, it does not scale well to large applications, since every
    library gets imported into the global namespace. Modern client-side JavaScript
    frameworks such as AngularJS provide an alternative way of defining and loading
    modules that preserve encapsulation.
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可行，但它不适合大型应用程序的扩展，因为每个库都被导入到全局命名空间中。现代客户端JavaScript框架，如AngularJS，提供了一种定义和加载模块的替代方法，这有助于保持封装性。
- en: 'We will use RequireJS. In a nutshell, RequireJS lets us encapsulate JavaScript
    modules through functions. For instance, if we wanted to write a module `example`
    that contains a function for hiding a `div`, we would define the module as follows:'
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用RequireJS。简而言之，RequireJS让我们可以通过函数封装JavaScript模块。例如，如果我们想编写一个包含用于隐藏`div`的函数的模块`example`，我们会按照以下方式定义该模块：
- en: '[PRE646]'
  id: totrans-2635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE646]'
- en: 'We encapsulate our module as a callback in a function called `define`. The
    `define` function takes two arguments: a list of dependencies, and a function
    definition. The `define` function binds the dependencies to the arguments list
    of the callback: in this case, functions in JQuery will be bound to `$` and functions
    in Underscore will be bound to `_`. This creates a module which exposes whatever
    the callback function returns. In this case, we export the `hide` function, binding
    it to the name `"hide"`. Our example module thus exposes the `hide` function.'
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模块封装在一个名为`define`的函数的回调中。`define`函数接受两个参数：一个依赖项列表和一个函数定义。`define`函数将依赖项绑定到回调的参数列表：在这种情况下，JQuery中的函数将被绑定到`$`，而Underscore中的函数将被绑定到`_`。这创建了一个模块，它暴露了回调函数返回的任何内容。在这种情况下，我们导出`hide`函数，并将其绑定到名称`"hide"`。因此，我们的示例模块暴露了`hide`函数。
- en: 'To load this module, we pass it as a dependency to the module in which we want
    to use it:'
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载此模块，我们将它作为依赖项传递给我们要在其中使用它的模块：
- en: '[PRE647]'
  id: totrans-2638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE647]'
- en: Notice how the functions in `example` are encapsulated, rather than existing
    in the global namespace. We call them through `example.<function-name>`. Furthermore,
    any functions or variables defined internally to the `example` module remain private.
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`example`中的函数是如何被封装的，而不是存在于全局命名空间中。我们通过`example.<function-name>`来调用它们。此外，在`example`模块内部定义的任何函数或变量都保持私有。
- en: 'Sometimes, we want JavaScript code to exist outside of modules. This is often
    the case for the script that bootstraps the application. For these, replace `define`
    with `require`:'
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望JavaScript代码存在于模块之外。对于启动应用程序的脚本来说，这通常是情况。对于这些脚本，将`define`替换为`require`：
- en: '[PRE648]'
  id: totrans-2641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE648]'
- en: 'Now that we have an overview of RequireJS, how do we use it in the Play framework?
    The first step is to add the dependency on the RequireJS web jar, which we have
    done. The Play framework also adds a RequireJS SBT plugin ([https://github.com/sbt/sbt-rjs](https://github.com/sbt/sbt-rjs)),
    which should be installed by default if you used the `play-scala` activator. If
    this is missing, it can be added with the following line in `plugins.sbt`:'
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对RequireJS有了概述，那么我们如何在Play框架中使用它？第一步是添加对RequireJS web jar的依赖，我们已经这样做了。Play框架还添加了一个RequireJS
    SBT插件([https://github.com/sbt/sbt-rjs](https://github.com/sbt/sbt-rjs))，如果您使用了`play-scala`激活器，则默认应该已安装。如果缺少此插件，可以在`plugins.sbt`文件中添加以下行：
- en: '[PRE649]'
  id: totrans-2643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE649]'
- en: 'We also need to add the plugin to the list of stages. This allows the plugin
    to manipulate the JavaScript assets when packaging the application as a jar. Add
    the following line to `build.sbt`:'
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将插件添加到阶段列表中。这允许插件在打包应用程序为jar时操作JavaScript资源。将以下行添加到`build.sbt`文件中：
- en: '[PRE650]'
  id: totrans-2645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE650]'
- en: You will need to restart the activator for the changes to take effect.
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要重新启动激活器以使更改生效。
- en: 'We are now ready to use RequireJS in our application. We can use it by adding
    the following line in the head section of our view:'
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好在我们的应用程序中使用RequireJS。我们可以在视图的头部部分添加以下行来使用它：
- en: '[PRE651]'
  id: totrans-2648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE651]'
- en: 'When the view is compiled, this is resolved to tags like:'
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
  zh: 当视图被编译时，这会被解析为类似以下标签：
- en: '[PRE652]'
  id: totrans-2650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE652]'
- en: The argument passed to `data-main` is the entry point for our application. When
    RequireJS loads, it will execute `main.js`. That script must therefore bootstrap
    our application. In particular, it should contain a configuration object for RequireJS,
    to make it aware of where all the libraries are.
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `data-main` 的参数是应用程序的入口点。当 RequireJS 加载时，它将执行 `main.js`。因此，该脚本必须引导我们的应用程序。特别是，它应该包含一个用于
    RequireJS 的配置对象，使其知道所有库的位置。
- en: Bootstrapping the applications
  id: totrans-2652
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引导应用程序
- en: 'When we linked `require.js` to our application, we told it to use `main.js`
    as our entry point. To test that this works, let''s start by entering a dummy
    `main.js`. JavaScript files in Play applications go in `/public/javascripts`:'
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 `require.js` 链接到我们的应用程序时，我们告诉它使用 `main.js` 作为我们的入口点。为了测试这是否工作，让我们首先输入一个虚拟的
    `main.js`。Play 应用程序中的 JavaScript 文件位于 `/public/javascripts`：
- en: '[PRE653]'
  id: totrans-2654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE653]'
- en: To verify that this worked, head to `127.0.0.1:9000` and open the browser console.
    You should see `"hello, JavaScript"` in the console.
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这已经工作，转到 `127.0.0.1:9000` 并打开浏览器控制台。你应该在控制台中看到 `"hello, JavaScript"`。
- en: 'Let''s now write a more useful `main.js`. We will start by configuring RequireJS,
    giving it the location of modules we will use in our application. Unfortunately,
    NVD3, the graph library that we use, does not play very well with RequireJS so
    we have to use an ugly hack to make it work. This complicates our `main.js` file
    somewhat:'
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来编写一个更有用的 `main.js`。我们首先配置 RequireJS，给它提供我们将在应用程序中使用的模块的位置。不幸的是，我们使用的图形库
    NVD3 与 RequireJS 不是很兼容，所以我们不得不使用一个丑陋的技巧来使其工作。这使我们的 `main.js` 文件变得有些复杂：
- en: '[PRE654]'
  id: totrans-2657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE654]'
- en: Now that we have the configuration in place, we can dig into the JavaScript
    part of the application.
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了配置，我们可以深入到应用程序的 JavaScript 部分。
- en: Client-side program architecture
  id: totrans-2659
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端程序架构
- en: 'The basic idea is simple: the user searches for the name of someone on GitHub
    in the input box. When he enters a name, we fire a request to the API designed
    earlier in this chapter. When the response from the API returns, the program binds
    that response to a model and emits an event notifying that the model has been
    changed. The views listen for this event and refresh from the model in response.'
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想很简单：用户在输入框中搜索 GitHub 上某人的名字。当他输入名字时，我们向本章 earlier 设计的 API 发送请求。当 API 的响应返回时，程序将响应绑定到模型，并发出一个事件通知模型已更改。视图监听此事件并从模型中刷新。
- en: Designing the model
  id: totrans-2661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计模型
- en: Let's start by defining the client-side model. The model holds information regarding
    the repos of the user currently displayed. It gets filled in after the first search.
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义客户端模型。模型保存有关当前显示的用户仓库的信息。它会在第一次搜索后填充。
- en: '[PRE655]'
  id: totrans-2663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE655]'
- en: 'To see a populated value of the model, head to the complete application example
    on `app.scala4datascience.com`, open a JavaScript console in your browser, search
    for a user (for example, `odersky`) in the application and type the following
    in the console:'
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看模型的填充值，请转到 `app.scala4datascience.com` 上的完整应用程序示例，在浏览器中打开一个 JavaScript 控制台，搜索应用程序中的用户（例如，`odersky`），然后在控制台中输入以下内容：
- en: '[PRE656]'
  id: totrans-2665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE656]'
- en: These import the `"model"` module, bind it to the variable `model`, and then
    print information to the console.
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入 `"model"` 模块，将其绑定到变量 `model`，然后打印信息到控制台。
- en: The event bus
  id: totrans-2667
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件总线
- en: We need a mechanism for informing the views when the model is updated, since
    the views need to refresh from the new model. This is commonly handled through
    *events* in web applications. JQuery lets us bind callbacks to specific events.
    The callback is executed when that event occurs.
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个机制来通知视图当模型更新时，因为视图需要从新的模型中刷新。这通常通过 Web 应用程序中的 *事件* 来处理。jQuery 允许我们将回调绑定到特定事件。当该事件发生时，回调将被执行。
- en: 'For instance, to bind a callback to the event `"custom-event"`, enter the following
    in a JavaScript console:'
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要将回调绑定到事件 `"custom-event"`，在 JavaScript 控制台中输入以下内容：
- en: '[PRE657]'
  id: totrans-2670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE657]'
- en: 'We can fire the event using:'
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方式触发事件：
- en: '[PRE658]'
  id: totrans-2672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE658]'
- en: 'Events in JQuery require an *event* bus, a DOM element on which the event is
    registered. In this case, we used the `window` DOM element as our event bus, but
    any JQuery element would have served. Centralizing event definitions to a single
    module is helpful. We will, therefore, create an `events` module containing two
    functions: `trigger`, which triggers an event (specified by a string) and `on`,
    which binds a callback to a specific event:'
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
  zh: 在 jQuery 中，事件需要注册在 *事件总线* 上的 DOM 元素。在这个例子中，我们使用了 `window` DOM 元素作为我们的事件总线，但任何
    jQuery 元素都可以。将事件定义集中到单个模块中是有帮助的。因此，我们将创建一个包含两个函数的 `events` 模块：`trigger`，用于触发一个事件（由一个字符串指定），和
    `on`，用于将回调绑定到特定事件：
- en: '[PRE659]'
  id: totrans-2674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE659]'
- en: 'We can now emit and receive events using the `events` module. You can test
    this out in a JavaScript console on the live version of the application (at `app.scala4datascience.com`).
    Let''s start by registering a listener:'
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `events` 模块发出和接收事件。你可以在应用程序的实时版本（在 `app.scala4datascience.com`）的 JavaScript
    控制台中测试这一点。让我们首先注册一个监听器：
- en: '[PRE660]'
  id: totrans-2676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE660]'
- en: 'If we now trigger the event `"hello_event"`, the listener prints `"Received
    event"`:'
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在触发事件 `"hello_event"`，监听器将打印 `"Received event"`：
- en: '[PRE661]'
  id: totrans-2678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE661]'
- en: Using events allows us to decouple the controller from the views. The controller
    does not need to know anything about the views, and vice-versa. The controller
    just needs to emit a `"model_updated"` event when the model is updated, and the
    views need to refresh from the model when they receive that event.
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
  zh: 使用事件使我们能够将控制器与视图解耦。控制器不需要了解任何关于视图的信息，反之亦然。控制器只需要在模型更新时发出一个 `"model_updated"`
    事件，而视图在接收到该事件时需要从模型刷新。
- en: AJAX calls through JQuery
  id: totrans-2680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 jQuery 进行 AJAX 调用
- en: We can now write the controller for our application. When the user enters a
    name in the text input, we query the API, update the model and trigger a `model_updated`
    event.
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写我们应用程序的控制器。当用户在文本输入框中输入名称时，我们查询 API，更新模型并触发一个 `model_updated` 事件。
- en: 'We use JQuery''s `$.getJSON` function to query our API. This function takes
    a URL as its first argument, and a callback as its second argument. The API call
    is asynchronous: `$.getJSON` returns immediately after execution. All request
    processing must, therefore, be done in the callback. The callback is called if
    the request is successful, but we can define additional handlers that are always
    called, or called on failure. Let''s try this out in the browser console (either
    your own, if you are running the API developed in the previous chapter, or on
    `app.scala4datascience.com`). Recall that the API is listening to the end-point
    `/api/repos/:user`:'
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 jQuery 的 `$.getJSON` 函数来查询我们的 API。这个函数将其第一个参数作为 URL，第二个参数作为回调函数。API 调用是异步的：`$.getJSON`
    在执行后立即返回。因此，所有请求处理都必须在回调中进行。如果请求成功，将调用回调，但我们可以定义始终调用或失败时调用的额外处理程序。让我们在浏览器控制台（无论是你自己的，如果你正在运行上一章开发的
    API，还是 `app.scala4datascience.com` 上的）中尝试一下。回想一下，API 正在监听 `/api/repos/:user` 的端点：
- en: '[PRE662]'
  id: totrans-2683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE662]'
- en: '`getJSON` returns immediately. A few tenths of a second later, the API responds,
    at which point the response gets fed through the callback.'
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
  zh: '`getJSON` 立即返回。几秒钟后，API 响应，此时响应将通过回调传递。'
- en: 'The callback only gets executed on success. It takes, as its argument, the
    JSON object returned by the API. To bind a callback that is executed when the
    API request fails, call the `.fail` method on the return value of `getJSON`:'
  id: totrans-2685
  prefs: []
  type: TYPE_NORMAL
  zh: 回调仅在成功时执行。它接受 API 返回的 JSON 对象作为其参数。要绑定在 API 请求失败时执行的回调，请调用 `getJSON` 返回值的 `.fail`
    方法：
- en: '[PRE663]'
  id: totrans-2686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE663]'
- en: We can also use the `.always` method on the return value of `getJSON` to specify
    a callback that is executed, whether the API query was successful or not.
  id: totrans-2687
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `getJSON` 返回值的 `.always` 方法来指定无论 API 查询是否成功都执行的回调。
- en: 'Now that we know how to use `$.getJSON` to query our API, we can write the
    controller. The controller listens for changes to the `#user-selection` input
    field. When a change occurs, it fires an AJAX request to the API for information
    on that user. It binds a callback which updates the model when the API replies
    with a list of repositories. We will define a `controller` module that exports
    a single function, `initialize`, that creates the event listeners:'
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用 `$.getJSON` 查询我们的 API，我们可以编写控制器。控制器监听 `#user-selection` 输入字段的更改。当发生更改时，它向
    API 发送 AJAX 请求以获取该用户的信息。它绑定一个回调，当 API 回复包含存储库列表时更新模型。我们将定义一个 `controller` 模块，该模块导出一个名为
    `initialize` 的单个函数，该函数创建事件监听器：
- en: '[PRE664]'
  id: totrans-2689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE664]'
- en: 'Our controller module just exposes the `initialize` method. Once the initialization
    is performed, the controller interacts with the rest of the application through
    event listeners. We will call the controller''s `initialize` method in `main.js`.
    Currently, the last lines of that file are just an empty `require` block. Let''s
    import our controller and initialize it:'
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的控制模块仅公开 `initialize` 方法。一旦初始化完成，控制器将通过事件监听器与应用程序的其余部分交互。我们将在 `main.js` 中调用控制器的
    `initialize` 方法。目前，该文件的最后一行只是一个空的 `require` 块。让我们导入我们的控制器并初始化它：
- en: '[PRE665]'
  id: totrans-2691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE665]'
- en: 'To test that this works, we can bind a dummy listener to the `"model_updated"`
    event. For instance, we could log the current model to the browser JavaScript
    console with the following snippet (which you can write directly in the JavaScript
    console):'
  id: totrans-2692
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们可以将一个虚拟监听器绑定到 `"model_updated"` 事件。例如，我们可以使用以下片段将当前模型记录到浏览器 JavaScript
    控制台中（您可以直接在 JavaScript 控制台中编写此片段）：
- en: '[PRE666]'
  id: totrans-2693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE666]'
- en: If you then search for a user, the model will be printed to the console. We
    now have the controller in place. The last step is writing the views.
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索一个用户，模型将被打印到控制台。我们现在有了控制器。最后一步是编写视图。
- en: Response views
  id: totrans-2695
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应视图
- en: 'If the request fails, we just display **Not found** in the response div. This
    part is the easiest to code up, so let''s do that first. We define an `initialize`
    method that generates the view. The view then listens for the `"model_updated"`
    event, which is fired by the controller after it updates the model. Once the initialization
    is complete, the only way to interact with the response view is through `"model_updated"`
    events:'
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
  zh: 如果请求失败，我们只需在响应 div 中显示 **未找到**。这部分代码编写起来最简单，所以让我们先做这个。我们定义一个 `initialize` 方法来生成视图。然后视图监听
    `"model_updated"` 事件，该事件在控制器更新模型后触发。一旦初始化完成，与响应视图交互的唯一方式是通过 `"model_updated"`
    事件：
- en: '[PRE667]'
  id: totrans-2697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE667]'
- en: 'To bootstrap the view, we must call the initialize function from `main.js`.
    Just add a dependency on `responseView` in the require block, and call `responseView.initialize()`.
    With these modifications, the final `require` block in `main.js` is:'
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动视图，我们必须从 `main.js` 中调用 `initialize` 函数。只需在 `require` 块中添加对 `responseView`
    的依赖，并调用 `responseView.initialize()`。经过这些修改，`main.js` 中的最终 `require` 块如下：
- en: '[PRE668]'
  id: totrans-2699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE668]'
- en: You can check that this all works by entering junk in the user input to deliberately
    cause the API request to fail.
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在用户输入中输入垃圾信息来故意导致 API 请求失败，以检查这一切是否正常工作。
- en: When the user enters a valid GitHub login name and the API returns a list of
    repos, we must display those on the screen. We display a table and a pie chart
    that aggregates the repository sizes by language. We will define the pie chart
    and the table in two separate modules, called `repoGraph.js` and `repoTable.js`.
    Let's assume those exist for now and that they expose a `build` method that accepts
    a `model` and the name of a `div` in which to appear.
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户输入有效的 GitHub 登录名并且 API 返回一个仓库列表时，我们必须在屏幕上显示这些信息。我们显示一个表格和一个饼图，该饼图按语言聚合仓库大小。我们将定义饼图和表格在两个单独的模块中，分别称为
    `repoGraph.js` 和 `repoTable.js`。现在让我们假设这些模块存在，并且它们公开了一个接受 `model` 和要在其中出现的 `div`
    名称的 `build` 方法。
- en: 'Let''s update the code for `responseView` to accommodate the user entering
    a valid GitHub user name:'
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新 `responseView` 的代码，以适应用户输入有效的 GitHub 用户名：
- en: '[PRE669]'
  id: totrans-2703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE669]'
- en: 'Let''s walk through what happens in the event of a successful API call. We
    inject the following bit of HTML in the `#response` div:'
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在 API 调用成功时会发生什么。我们在 `#response` div 中注入以下 HTML 片段：
- en: '[PRE670]'
  id: totrans-2705
  prefs: []
  type: TYPE_PRE
  zh: '[PRE670]'
- en: This adds two HTML divs, one for the table of repositories, and the other for
    the graph. We use Bootstrap classes to split the response div vertically.
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
  zh: 这添加了两个 HTML div，一个用于仓库表，另一个用于图表。我们使用 Bootstrap 类来垂直分割响应 div。
- en: 'Let''s now turn our attention to the table view, which needs to expose a single
    `build` method, as described in the previous section. We will just display the
    repositories in an HTML table. We will use *Underscore templates* to build the
    table dynamically. Underscore templates work much like string interpolation in
    Scala: we define a template with placeholders. Let''s try this in a browser console:'
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将注意力转向表格视图，它需要暴露一个单独的 `build` 方法，正如前一小节所描述的。我们只需在 HTML 表格中显示仓库。我们将使用 *Underscore
    模板* 来动态构建表格。Underscore 模板的工作方式与 Scala 中的字符串插值类似：我们定义一个带有占位符的模板。让我们在浏览器控制台中试一试：
- en: '[PRE671]'
  id: totrans-2708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE671]'
- en: 'This creates a `myTemplate` function which accepts an object with attributes
    `title` and `name`:'
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个 `myTemplate` 函数，该函数接受具有 `title` 和 `name` 属性的对象：
- en: '[PRE672]'
  id: totrans-2710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE672]'
- en: 'Underscore templates thus provide a convenient mechanism for formatting an
    object as a string. We will create a template for each row in our table, and pass
    the model for each repository to the template:'
  id: totrans-2711
  prefs: []
  type: TYPE_NORMAL
  zh: Underscore 模板因此提供了一种方便的机制，可以将对象格式化为字符串。我们将为表格中的每一行创建一个模板，并将每个仓库的模型传递给模板：
- en: '[PRE673]'
  id: totrans-2712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE673]'
- en: Drawing plots with NVD3
  id: totrans-2713
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NVD3 绘制图表
- en: D3 is a library that offers low-level components for building interactive visualizations
    in JavaScript. By offering the low-level components, it gives a huge degree of
    flexibility to the developer. The learning curve can, however, be quite steep.
    In this example, we will use NVD3, a library which provides pre-made graphs for
    D3\. This can greatly speed up initial development. We will place the code in
    the file `repoGraph.js` and expose a single method, `build`, which takes, as arguments,
    a model and a div and draws a pie chart in that div. The pie chart will aggregate
    language use across all the user's repositories.
  id: totrans-2714
  prefs: []
  type: TYPE_NORMAL
  zh: D3 是一个提供用于在 JavaScript 中构建交互式可视化的低级组件的库。通过提供低级组件，它为开发者提供了极大的灵活性。然而，学习曲线可能相当陡峭。在这个例子中，我们将使用
    NVD3，这是一个为 D3 提供预制图表的库。这可以大大加快初始开发速度。我们将代码放置在文件 `repoGraph.js` 中，并公开一个名为 `build`
    的单一方法，该方法接受一个模型和一个 div 作为参数，并在该 div 中绘制饼图。饼图将汇总所有用户仓库中的语言使用情况。
- en: 'The code for generating a pie chart is nearly identical to the example given
    in the NVD3 documentation, available at [http://nvd3.org/examples/pie.html](http://nvd3.org/examples/pie.html).
    The data passed to the graph must be available as an array of objects. Each object
    must contain a `label` field and a `size` field. The `label` field identifies
    the language, and the `size` field is the total size of all the repositories for
    that user written in that language. The following would be a valid data array:'
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
  zh: 生成饼图的代码几乎与 NVD3 文档中给出的示例相同，该文档可在 [http://nvd3.org/examples/pie.html](http://nvd3.org/examples/pie.html)
    找到。传递给图表的数据必须作为一个对象数组可用。每个对象必须包含一个 `label` 字段和一个 `size` 字段。`label` 字段标识语言，而 `size`
    字段是该用户用该语言编写的所有仓库的总大小。以下是一个有效的数据数组：
- en: '[PRE674]'
  id: totrans-2716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE674]'
- en: 'To get the data in this format, we must aggregate sizes across the repositories
    written in a particular language in our model. We write the `generateDataFromModel`
    function to transform the `repos` array in the model to an array suitable for
    NVD3\. The crux of the aggregation is performed by a call to Underscore''s `groupBy`
    method, to group repositories by language. This method works exactly like Scala''s
    `groupBy` method. With this in mind, the `generateDataFromModel` function is:'
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
  zh: 要以这种格式获取数据，我们必须在我们的模型中聚合特定语言编写的仓库的大小。我们编写了 `generateDataFromModel` 函数来将模型中的
    `repos` 数组转换为适合 NVD3 的数组。聚合的核心操作是通过调用 Underscore 的 `groupBy` 方法来按语言分组仓库。此方法与 Scala
    的 `groupBy` 方法完全相同。考虑到这一点，`generateDataFromModel` 函数如下：
- en: '[PRE675]'
  id: totrans-2718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE675]'
- en: 'We can now build the pie chart, using NVD3''s `addGraph` method:'
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 NVD3 的 `addGraph` 方法来构建饼图：
- en: '[PRE676]'
  id: totrans-2720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE676]'
- en: This was the last component of our application. Point your browser to `127.0.0.1:9000`
    and you should see the application running.
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们应用程序的最后一个组件。将您的浏览器指向 `127.0.0.1:9000`，您应该会看到应用程序正在运行。
- en: Congratulations! We have built a fully-functioning single-page web application.
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经构建了一个功能齐全的单页 Web 应用程序。
- en: Summary
  id: totrans-2723
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to write a fully-featured web application with
    the Play framework. Congratulations on making it this far. Building web applications
    are likely to push many data scientists beyond their comfort zone, but knowing
    enough about the web to build basic applications will allow you to share your
    results in a compelling, engaging manner, as well as facilitate communications
    with software engineers and web developers.
  id: totrans-2724
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用 Play 框架编写一个功能齐全的 Web 应用程序。恭喜你走到了这一步。构建 Web 应用程序可能会让许多数据科学家超出他们的舒适区，但了解足够的
    Web 知识来构建基本的应用程序将允许你以引人入胜、吸引人的方式分享你的结果，同时也有助于与软件工程师和 Web 开发者进行沟通。
- en: This concludes our whistle stop tour of Scala libraries. Over the course of
    this book, we have learned how to tackle linear algebra and optimization problems
    efficiently using Breeze, how to insert and query data in SQL databases in a functional
    manner, and both how to interact with web APIs and how to create them. We have
    reviewed some of tools available to the data scientist for writing concurrent
    or parallel applications, from parallel collections and futures to Spark via Akka.
    We have seen how pervasive these constructs are in Scala libraries, from futures
    in the Play framework to Akka as the backbone of Spark. If you have read this
    far, pat yourself on the back.
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Scala库的快速浏览。在这本书的过程中，我们学习了如何使用Breeze高效地解决线性代数和优化问题，如何以函数式的方式在SQL数据库中插入和查询数据，以及如何与Web
    API交互以及如何创建它们。我们回顾了一些数据科学家用于编写并发或并行应用程序的工具，从并行集合和未来到Spark通过Akka。我们看到了这些结构在Scala库中的普遍性，从Play框架中的未来到Spark的骨干Akka。如果你已经读到这儿，给自己点个赞。
- en: This books gives you the briefest of introduction to the libraries it covers,
    hopefully just enough to give you a taste of what each tool is good for, what
    you could accomplish with it, and how it fits in the wider Scala ecosystem. If
    you decide to use any of these in your data science pipeline, you will need to
    read the documentation in more detail, or a more complete reference book. The
    references listed at the end of each chapter should provide a good starting point.
  id: totrans-2726
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书对其涵盖的库进行了最简短的介绍，希望这足以让你尝到每个工具的用途，你可以用它完成什么，以及它在更广泛的Scala生态系统中的位置。如果你决定在你的数据科学流程中使用这些工具，你需要更详细地阅读文档，或者一本更完整的参考书。每章末尾列出的参考应该是一个良好的起点。
- en: 'Both Scala and data science are evolving rapidly. Do not stay wedded to a particular
    toolkit or concept. Remain on top of current developments and, above all, remain
    pragmatic: find the right tool for the right job. Scala and the libraries discussed
    here will often be that tool, but not always: sometimes, a shell command or a
    short Python script will be more effective. Remember also that programming skills
    are but one aspect of the data scientist''s body of knowledge. Even if you want
    to specialize in the engineering side of data science, learn about the problem
    domain and the mathematical underpinnings of machine learning.'
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
  zh: Scala和数据科学都在快速发展。不要对某个特定的工具包或概念过于执着。保持对当前发展的关注，最重要的是，保持务实：找到适合工作的正确工具。Scala和这里讨论的库通常会是那个工具，但并不总是：有时，一个shell命令或简短的Python脚本会更有效。记住，编程技能只是数据科学家知识体系的一个方面。即使你想在数据科学的工程方面专长，也要了解问题领域和机器学习数学基础。
- en: Most importantly, if you have taken the time to read this book, it is likely
    that you view programming and data science as more than a day job. Coding in Scala
    can be satisfying and rewarding, so have fun and be awesome!
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，如果你花时间阅读这本书，你很可能认为编程和数据科学不仅仅是工作。在Scala中编码可以令人满意和有成就感，所以享受乐趣，做得很棒！
- en: References
  id: totrans-2729
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: There are thousands of HTML and CSS tutorials dotted around the web. A simple
    Google search will give you a much better idea of the resources available than
    any list of references I can provide.
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
  zh: 网上到处都是HTML和CSS教程。简单的Google搜索就能给你一个比我提供的任何参考列表都要好的资源了解。
- en: 'Mike Bostock''s website has a wealth of beautiful D3 visualizations: [http://bost.ocks.org/mike/.](http://bost.ocks.org/mike/.)
    To understand a bit more about D3, I recommend *Scott Murray''s Interactive Data
    Visualization for the Web*.'
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
  zh: Mike Bostock的网站上有丰富的美丽D3可视化：[http://bost.ocks.org/mike/](http://bost.ocks.org/mike/)。为了更好地了解D3，我推荐*Scott
    Murray的《Web交互数据可视化》*。
- en: You may also wish to consult the references given in the previous chapter for
    reference books on the Play framework and designing REST APIs.
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以参考前一章中给出的关于Play框架和设计REST API的参考书籍。
- en: Appendix A. Pattern Matching and Extractors
  id: totrans-2733
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. 模式匹配和提取器
- en: Pattern matching is a powerful tool for control flow in Scala. It is often underused
    and under-estimated by people coming to Scala from imperative languages.
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配是Scala中控制流的一个强大工具。对于从命令式语言转向Scala的人来说，它往往被低估和未充分利用。
- en: 'Let''s start with a few examples of pattern matching before diving into the
    theory. We start by defining a tuple:'
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入理论之前，我们先来看几个模式匹配的例子。我们首先定义一个元组：
- en: '[PRE677]'
  id: totrans-2736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE677]'
- en: 'We can use pattern matching to extract the elements of this tuple and bind
    them to variables:'
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用模式匹配来提取这个元组的元素并将它们绑定到变量上：
- en: '[PRE678]'
  id: totrans-2738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE678]'
- en: 'We just extracted the two elements of the `names` tuple, binding them to the
    variables `firstName` and `lastName`. Notice how the left-hand side defines a
    pattern that the right-hand side must match: we are declaring that the variable
    `names` must be a two-element tuple. To make the pattern more specific, we could
    also have specified the expected types of the elements in the tuple:'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提取了`names`元组的两个元素，并将它们绑定到变量`firstName`和`lastName`。注意左侧定义了一个模式，右侧必须匹配：我们声明变量`names`必须是一个包含两个元素的元组。为了使模式更具体，我们还可以指定元组中元素的预期类型：
- en: '[PRE679]'
  id: totrans-2740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE679]'
- en: What happens if the pattern on the left-hand side does not match the right-hand
    side?
  id: totrans-2741
  prefs: []
  type: TYPE_NORMAL
  zh: 如果左侧的模式与右侧不匹配会发生什么？
- en: '[PRE680]'
  id: totrans-2742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE680]'
- en: This results in a compile error. Other types of pattern matching failures result
    in runtime errors.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致编译错误。其他类型的模式匹配失败会导致运行时错误。
- en: 'Pattern matching is very expressive. To achieve the same behavior without pattern
    matching, you would have to do the following explicitly:'
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配非常具有表现力。要实现没有模式匹配的相同行为，你必须明确地做以下操作：
- en: Verify that the variable `names` is a two-element tuple
  id: totrans-2745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证变量`names`是一个包含两个元素的元组
- en: Extract the first element and bind it to `firstName`
  id: totrans-2746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取第一个元素并将其绑定到`firstName`
- en: Extract the second element and bind it to `lastName`
  id: totrans-2747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取第二个元素并将其绑定到`lastName`
- en: 'If we expect certain elements in the tuple to have specific values, we can
    verify this as part of the pattern match. For instance, we can verify that the
    first element of the `names` tuple matches `"Pascal"`:'
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们期望元组中的某些元素具有特定的值，我们可以在模式匹配过程中验证这一点。例如，我们可以验证`names`元组的第一个元素匹配`"Pascal"`：
- en: '[PRE681]'
  id: totrans-2749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE681]'
- en: 'Besides tuples, we can also match on Scala collections:'
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
  zh: 除了元组之外，我们还可以在Scala集合上进行匹配：
- en: '[PRE682]'
  id: totrans-2751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE682]'
- en: 'Notice the similarity between this pattern matching and array construction:'
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这种模式匹配与数组构造的相似性：
- en: '[PRE683]'
  id: totrans-2753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE683]'
- en: Syntactically, Scala expresses pattern matching as the reverse process to instance
    construction. We can think of pattern matching as the deconstruction of an object,
    binding the object's constituent parts to variables.
  id: totrans-2754
  prefs: []
  type: TYPE_NORMAL
  zh: 语法上，Scala将模式匹配表达为实例构造过程的逆过程。我们可以将模式匹配视为对象的解构，将对象的组成部分绑定到变量。
- en: 'When matching against collections, one is sometimes only interested in matching
    the first element, or the first few elements, and discarding the rest of the collection,
    whatever its length. The operator `_*` will match against any number of elements:'
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
  zh: 当匹配集合时，有时人们只对匹配第一个元素或前几个元素感兴趣，并丢弃集合的其余部分，无论其长度如何。运算符`_*`将匹配任意数量的元素：
- en: '[PRE684]'
  id: totrans-2756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE684]'
- en: 'By default, the part of the pattern matched by the `_*` operator is not bound
    to a variable. We can capture it as follows:'
  id: totrans-2757
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，由`_*`运算符匹配的模式部分不会被绑定到变量。我们可以如下捕获它：
- en: '[PRE685]'
  id: totrans-2758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE685]'
- en: 'Besides tuples and collections, we can also match against case classes. Let''s
    start by defining a case representing a name:'
  id: totrans-2759
  prefs: []
  type: TYPE_NORMAL
  zh: 除了元组和集合之外，我们还可以匹配case类。让我们首先定义一个表示名称的case：
- en: '[PRE686]'
  id: totrans-2760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE686]'
- en: 'We can match against instances of `Name` in much the same way we matched against
    tuples:'
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以与匹配元组相同的方式匹配`Name`的实例：
- en: '[PRE687]'
  id: totrans-2762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE687]'
- en: 'All these patterns can also be used in `match` statements:'
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模式也可以用在`match`语句中：
- en: '[PRE688]'
  id: totrans-2764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE688]'
- en: Pattern matching in for comprehensions
  id: totrans-2765
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在for推导式中的模式匹配
- en: 'Pattern matching is useful in *for* comprehensions for extracting items from
    a collection that match a specific pattern. Let''s build a collection of `Name`
    instances:'
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配在*for*推导式中非常有用，可以提取与特定模式匹配的集合中的项目。让我们构建一个`Name`实例的集合：
- en: '[PRE689]'
  id: totrans-2767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE689]'
- en: 'We can use pattern matching to extract the internals of the class in a for-comprehension:'
  id: totrans-2768
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在for推导式中使用模式匹配来提取类的内部结构：
- en: '[PRE690]'
  id: totrans-2769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE690]'
- en: So far, nothing terribly ground-breaking. But what if we wanted to extract the
    surname of everyone whose first name is `"Martin"`?
  id: totrans-2770
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，没有什么特别突破性的。但如果我们想提取所有名字为`"Martin"`的人的姓氏呢？
- en: '[PRE691]'
  id: totrans-2771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE691]'
- en: Writing `Name("Martin", last) <- names` extracts the elements of names that
    match the pattern. You might think that this is a contrived example, and it is,
    but the examples in [Chapter 7](part0059.xhtml#aid-1O8H61 "Chapter 7. Web APIs"),
    *Web APIs* demonstrate the usefulness and versatility of this language pattern,
    for instance, for extracting specific fields from JSON objects.
  id: totrans-2772
  prefs: []
  type: TYPE_NORMAL
  zh: 写作`Name("Martin", last) <- names`提取与模式匹配的`names`元素。你可能认为这是一个人为的例子，确实如此，但[第7章](part0059.xhtml#aid-1O8H61
    "第7章。Web APIs")，*Web APIs*中的例子展示了这种语言模式的有用性和多功能性，例如，用于从JSON对象中提取特定字段。
- en: Pattern matching internals
  id: totrans-2773
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式匹配内部机制
- en: If you define a case class, as we saw with `Name`, you get pattern matching
    against the constructor *for free*. You should be using case classes to represent
    your data as much as possible, thus reducing the need to implement your own pattern
    matching. It is nevertheless useful to understand how pattern matching works.
  id: totrans-2774
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你定义了一个案例类，就像我们用 `Name` 看到的那样，你将免费获得对构造函数的模式匹配。你应该尽可能使用案例类来表示你的数据，从而减少实现自己的模式匹配的需求。尽管如此，了解模式匹配的工作原理仍然是有用的。
- en: 'When you create a case class, Scala automatically builds a companion object:'
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个案例类时，Scala 会自动构建一个伴随对象：
- en: '[PRE692]'
  id: totrans-2776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE692]'
- en: The method used (internally) for pattern matching is `unapply`. This method
    takes, as argument, an object and returns `Option[T],` where `T` is a tuple of
    the values of the case class.
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
  zh: 用于（内部）模式匹配的方法是 `unapply`。此方法接受一个对象作为参数，并返回 `Option[T]，其中 T 是案例类的值元组的元组。
- en: '[PRE693]'
  id: totrans-2778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE693]'
- en: 'The `unapply` method is an *extractor*. It plays the opposite role of the constructor:
    it takes an object and extracts the list of parameters needed to construct that
    object. When you write `val Name(firstName, lastName)`, or when you use `Name`
    as a case in a match statement, Scala calls `Name.unapply` on what you are matching
    against. A value of `Some[(String, String)]` implies a pattern match, while a
    value of `None` implies that the pattern fails.'
  id: totrans-2779
  prefs: []
  type: TYPE_NORMAL
  zh: '`unapply` 方法是一个 *提取器*。它扮演着与构造函数相反的角色：它接受一个对象，并提取构建该对象所需的参数列表。当你写 `val Name(firstName,
    lastName)`，或者当你使用 `Name` 作为匹配语句中的案例时，Scala 会调用 `Name.unapply` 来匹配你正在匹配的对象。`Some[(String,
    String)]` 的值表示模式匹配，而 `None` 的值表示模式失败。'
- en: 'To write custom extractors, you just need an object with an `unapply` method.
    While `unapply` normally resides in the companion object of a class that you are
    deconstructing, this need not be the case. In fact, it does not need to correspond
    to an existing class at all. For instance, let''s define a `NonZeroDouble` extractor
    that matches any non-zero double:'
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写自定义提取器，你只需要一个具有 `unapply` 方法的对象。虽然 `unapply` 通常位于你正在解构的类的伴随对象中，但这并不一定是这种情况。实际上，它根本不需要对应于现有的类。例如，让我们定义一个匹配任何非零双精度浮点数的
    `NonZeroDouble` 提取器：
- en: '[PRE694]'
  id: totrans-2781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE694]'
- en: We defined an extractor for `NonZeroDouble`, despite the absence of a corresponding
    `NonZeroDouble` class.
  id: totrans-2782
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有相应的 `NonZeroDouble` 类，我们仍然定义了一个 `NonZeroDouble` 提取器。
- en: 'This `NonZeroDouble` extractor would be useful in a match object. For instance,
    let''s define a `safeDivision` function that returns a default value when the
    denominator is zero:'
  id: totrans-2783
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `NonZeroDouble` 提取器在匹配对象中很有用。例如，让我们定义一个 `safeDivision` 函数，当分母为零时返回默认值：
- en: '[PRE695]'
  id: totrans-2784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE695]'
- en: 'This is a trivial example because the `NonZeroDouble.unapply` method is so
    simple, but you can hopefully see the usefulness and expressiveness, if we were
    to define a more complex test. Defining custom extractors lets you define powerful
    control flow constructs to leverage `match` statements. More importantly, they
    enable the client using the extractors to think about control flow declaratively:
    the client can declare that they need a `NonZeroDouble`, rather than instructing
    the compiler to check whether the value is zero.'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子，因为 `NonZeroDouble.unapply` 方法非常简单，但如果你定义一个更复杂的测试，你可能会看到它的有用性和表达性。定义自定义提取器让你能够定义强大的控制流结构来利用
    `match` 语句。更重要的是，它们使使用提取器的客户端能够以声明性的方式考虑控制流：客户端可以声明他们需要一个 `NonZeroDouble`，而不是指示编译器检查值是否为零。
- en: Extracting sequences
  id: totrans-2786
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取序列
- en: 'The previous section explains extraction from case classes, and how to write
    custom extractors, but it does not explain how extraction works on sequences:'
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节解释了从案例类中提取数据以及如何编写自定义提取器，但没有解释如何在序列上提取：
- en: '[PRE696]'
  id: totrans-2788
  prefs: []
  type: TYPE_PRE
  zh: '[PRE696]'
- en: 'Rather than relying on an `unapply` method, sequences rely on an `unapplySeq`
    method defined in the companion object. This is expected to return an `Option[Seq[A]]`:'
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖 `unapply` 方法不同，序列依赖于在伴随对象中定义的 `unapplySeq` 方法。这应该返回 `Option[Seq[A]]`：
- en: '[PRE697]'
  id: totrans-2790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE697]'
- en: 'Let''s write an example. We will write an extractor for Breeze vectors (which
    do not currently support pattern matching). To avoid clashing with the `DenseVector`
    companion object, we will write our `unapplySeq` in a separate object, called
    `DV`. All our `unapplySeq` method needs to do is convert its argument to a Scala
    `Vector` instance. To avoid muddying the concepts with generics, we will write
    this implementation for `[Double]` vectors only:'
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写一个例子。我们将编写一个用于Breeze向量的提取器（目前Breeze向量不支持模式匹配）。为了避免与`DenseVector`伴生对象冲突，我们将我们的`unapplySeq`写在单独的对象中，称为`DV`。我们的`unapplySeq`方法需要做的只是将其参数转换为Scala
    `Vector`实例。为了避免泛型混淆概念，我们将仅为此`[Double]`向量编写此实现：
- en: '[PRE698]'
  id: totrans-2792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE698]'
- en: 'Let''s try our new extractor implementation:'
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试我们的新提取器实现：
- en: '[PRE699]'
  id: totrans-2794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE699]'
- en: Summary
  id: totrans-2795
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Pattern matching is a powerful tool for control flow. It encourages the programmer
    to think declaratively: declare that you expect a variable to match a certain
    pattern, rather than explicitly tell the computer how to check that it matches
    this pattern. This can save many lines of code and enhance clarity.'
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配是控制流的一个强大工具。它鼓励程序员以声明式的方式思考：声明你期望一个变量匹配某个模式，而不是明确告诉计算机如何检查它是否匹配这个模式。这可以节省许多代码行并提高清晰度。
- en: Reference
  id: totrans-2797
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: 'For an overview of pattern matching in Scala, there is no better reference
    than *Programming in Scala*, by *Martin Odersky*, *Bill Venners*, and *Lex Spoon*.
    An online version of the first edition is available at: [https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html](https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html).'
  id: totrans-2798
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Scala中模式匹配的概述，没有比*Programming in Scala*更好的参考书籍了，作者是*Martin Odersky*、*Bill
    Venners*和*Lex Spoon*。第一版的在线版本可在以下网址找到：[https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html](https://www.artima.com/pins1ed/case-classes-and-pattern-matching.html)。
- en: '*Daniel Westheide''s* blog covers slightly more advanced Scala constructs,
    and is a very useful read: [http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html](http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html).'
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
  zh: '*Daniel Westheide*的博客涵盖了稍微高级一点的Scala结构，并且是非常有用的阅读材料：[http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html](http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html)。'
