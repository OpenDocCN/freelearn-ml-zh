- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Techniques for Programmatic Labeling in Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的程序化标记技术
- en: 'In machine learning, the accurate labeling of data is crucial for training
    effective models. Data labeling involves assigning meaningful categories or classes
    to data instances, and while traditionally a human-driven process, there are various
    programmatic approaches to dataset labeling. This chapter delves into the following
    methods of programmatic data labeling in machine learning:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，数据的准确标记对于训练有效的模型至关重要。数据标记涉及将有意义的类别或类分配给数据实例，虽然传统上是一个由人类驱动的流程，但存在各种程序化方法来标记数据集。本章深入探讨了机器学习中以下程序化数据标记方法：
- en: Pattern matching
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式匹配
- en: '**Database** (**DB**) lookup'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库**（**DB**）查找'
- en: Boolean flags
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔标志
- en: Weak supervision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱监督
- en: Semi-weak supervision
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半弱监督
- en: Slicing functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切片函数
- en: Active learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动学习
- en: Transfer learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Semi-supervised learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To execute the code examples provided in this chapter on programmatic labeling
    techniques, ensure that you have the following technical prerequisites installed
    in your Python environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行本章提供的程序化标记技术示例，请确保您在 Python 环境中安装了以下技术先决条件：
- en: Python version
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 版本
- en: 'The examples in this chapter require Python version 3.7 or higher. You can
    check your Python version by running the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例需要 Python 版本 3.7 或更高。您可以通过运行以下命令来检查您的 Python 版本：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We recommend using the Jupyter Notebook **integrated development environment**
    (**IDE**) for an interactive and organized coding experience. If you don’t have
    it installed, you can install it using this line:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用 Jupyter Notebook **集成开发环境**（**IDE**）以获得交互式和有组织的编码体验。如果您尚未安装它，可以使用以下命令进行安装：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Launch Jupyter Notebook with the following command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启动 Jupyter Notebook：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Library requirements
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库要求
- en: 'Ensure that the following Python packages are installed in your environment.
    You can install them using the following commands:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在您的环境中安装以下 Python 包。您可以使用以下命令进行安装：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Additionally, for the TensorFlow and Keras components, you may need GPU support
    for optimal performance. Refer to the TensorFlow documentation for GPU installation
    instructions if you have a compatible GPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于 TensorFlow 和 Keras 组件，您可能需要 GPU 支持以获得最佳性能。如果您有兼容的 GPU，请参阅 TensorFlow 文档中的
    GPU 安装说明。
- en: Pattern matching
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式匹配
- en: In machine learning, one of the most important tasks is to label or classify
    data based on some criteria or patterns. However, labeling data manually can be
    time consuming and costly, especially when dealing with a large amount of data.
    By leveraging predefined patterns, this labeling approach enables the automatic
    assignment of meaningful categories or classes to data instances.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最重要的任务之一是根据某些标准或模式对数据进行标记或分类。然而，手动标记数据可能既耗时又昂贵，尤其是在处理大量数据时。通过利用预定义的模式，这种标记方法能够自动将有意义的类别或类分配给数据实例。
- en: '**Pattern matching** involves the identification of specific patterns or sequences
    within data that can be used as indicators for assigning labels. These patterns
    can be defined using regular expressions, rule-based systems, or other pattern
    recognition algorithms. The objective is to capture relevant information and characteristics
    from the data that can be matched against predefined patterns to infer labels
    accurately.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式匹配**涉及在数据中识别特定的模式或序列，这些模式或序列可以用作分配标签的指示器。这些模式可以使用正则表达式、基于规则的系统或其他模式识别算法来定义。目标是捕获数据中的相关信息和特征，以便与预定义的模式匹配，以准确推断标签。'
- en: 'Pattern matching can be applied to various domains and scenarios in machine
    learning. Some common applications include the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配可以应用于机器学习中的各种领域和场景。以下是一些常见应用：
- en: '**Text classification**: In natural language processing, pattern matching can
    be utilized to label text data based on specific keywords, phrases, or syntactic
    patterns. This enables tasks such as sentiment analysis, spam detection, and topic
    categorization.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：在自然语言处理中，模式匹配可以用来根据特定的关键词、短语或句法模式对文本数据进行标记。这使任务如情感分析、垃圾邮件检测和主题分类成为可能。'
- en: '**Image recognition**: Pattern matching can aid in labeling images by identifying
    distinctive visual patterns or features that correspond to specific classes. This
    technique can be valuable in tasks such as object recognition, facial detection,
    and image segmentation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：模式匹配可以通过识别与特定类别相对应的独特视觉模式或特征来帮助标记图像。这项技术在对象识别、人脸检测和图像分割等任务中非常有价值。'
- en: '**Time series analysis**: When dealing with time-dependent data, pattern matching
    can be employed to label sequences of events or patterns that occur over time.
    This is particularly useful in financial analysis, anomaly detection, and predicting
    stock market trends.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列分析**：在处理时间依赖性数据时，模式匹配可以用于标记事件序列或随时间发生的模式。这在金融分析、异常检测和预测股市趋势方面特别有用。'
- en: '**Fraud detection**: Pattern matching can play a crucial role in identifying
    fraudulent activities by matching suspicious patterns or anomalies against known
    fraud patterns. This technique can help in credit card fraud detection, network
    intrusion detection, and cybersecurity.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**：模式匹配可以通过将可疑模式或异常与已知的欺诈模式进行匹配，在识别欺诈活动中发挥关键作用。这项技术有助于信用卡欺诈检测、网络入侵检测和网络安全。'
- en: 'Pattern matching offers several advantages as a labeling technique in machine
    learning:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配作为机器学习中的标签技术提供了以下优势：
- en: '**Automation and efficiency**: By automating the labeling process, pattern
    matching reduces the reliance on manual labeling, saving time and effort. It allows
    for large-scale dataset labeling with increased efficiency.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化和效率**：通过自动化标签过程，模式匹配减少了对手动标签的依赖，节省了时间和精力。它允许以更高的效率进行大规模数据集的标签。'
- en: '**Flexibility and adaptability**: Patterns can be easily modified or extended
    to accommodate new data or evolving requirements. This provides flexibility in
    adapting to changing labeling criteria and ensures scalability.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性和适应性**：模式可以很容易地修改或扩展以适应新的数据或不断变化的需求。这提供了适应不断变化的标签标准的灵活性，并确保了可扩展性。'
- en: '**Interpretability**: Pattern matching provides a transparent and interpretable
    approach to labeling, as the rules and patterns can be examined and understood.
    This aids in the transparency and explainability of the labeling process.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：模式匹配提供了一种透明且可解释的标签方法，因为规则和模式可以被检查和理解。这有助于标签过程的透明性和可解释性。'
- en: '**Complementing other techniques**: Pattern matching can be used in conjunction
    with other labeling techniques, such as weak supervision or transfer learning,
    to enhance the overall labeling accuracy and robustness of machine learning models.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他技术的结合**：模式匹配可以与其他标签技术结合使用，例如弱监督或迁移学习，以提高机器学习模型的总体标签准确性和鲁棒性。'
- en: 'While pattern matching is a valuable labeling technique, it also presents certain
    challenges and considerations:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模式匹配是一种有价值的标签技术，但它也带来了一些挑战和考虑因素：
- en: '**Noise and ambiguity**: Data instances that do not perfectly match predefined
    patterns may introduce noise or ambiguity in the labeling process. Handling such
    cases requires careful design and consideration of pattern definitions.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声和歧义**：不完美匹配预定义模式的数据实例可能在标签过程中引入噪声或歧义。处理此类情况需要仔细设计和考虑模式定义。'
- en: '**Scalability**: As datasets grow larger, the scalability of pattern matching
    becomes crucial. Efficient algorithms and techniques must be employed to handle
    the increasing computational demands.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：随着数据集的增大，模式匹配的可扩展性变得至关重要。必须采用高效算法和技术来处理不断增加的计算需求。'
- en: '**Overfitting**: Overfitting can occur if patterns are too specific and fail
    to generalize well to unseen data instances. Regularization techniques and cross-validation
    can be used to mitigate this risk.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：如果模式过于特定并且无法很好地推广到未见过的数据实例，则可能发生过拟合。可以使用正则化技术和交叉验证来减轻这种风险。'
- en: In this section of the chapter, we will explore how to create pattern-matching
    labeling functions using Python and apply them to the `credit-g` dataset. The
    `credit-g` dataset, also known as the German Credit dataset, is a collection of
    data points used for risk analysis in the field of finance and machine learning.
    It’s used to classify people as good or bad credit risks based on a set of attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一节中，我们将探讨如何使用Python创建模式匹配标签函数，并将它们应用于`credit-g`数据集。`credit-g`数据集，也称为德国信用数据集，是一组用于金融和机器学习领域风险分析的数据点。它用于根据一组属性将人们分类为良好或不良信用风险。
- en: The dataset consists of 20 variables, including both numerical and categorical
    data. These variables provide information about each individual, such as their
    checking account status, credit history, purpose of the loan, credit amount, savings
    account/bonds, employment, installment rate in percentage of disposable income,
    personal status and gender, and other attributes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 20 个变量，包括数值和分类数据。这些变量提供了有关每个个人的信息，例如他们的支票账户状态、信用历史、贷款目的、信用额度、储蓄账户/债券、就业、可支配收入的百分比分期付款率、个人状况和性别，以及其他属性。
- en: Each entry in the dataset represents an individual who has applied for a loan.
    The target variable indicates whether the individual is classified as a ‘good’
    or ‘bad’ credit risk. This makes the dataset particularly useful for supervised
    machine learning tasks, especially binary classification problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每一项代表一个申请贷款的个人。目标变量表示该个人是否被分类为“良好”或“不良”信用风险。这使得数据集特别适用于监督机器学习任务，尤其是二元分类问题。
- en: The `credit-g` dataset is widely used in academia and industry for developing
    and testing machine learning models for credit risk assessment. It is available
    on several platforms, such as DataHub, Kaggle, OpenML, and UCI Machine Learning
    Repository.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`credit-g` 数据集在学术界和工业界广泛用于开发和测试用于信用风险评估的机器学习模型。它可在多个平台上获得，如 DataHub、Kaggle、OpenML
    和 UCI 机器学习仓库。'
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that the specifics of the variables might differ slightly depending
    on the source of the dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，变量的具体细节可能因数据集的来源而略有不同。
- en: 'We can start by loading the `credit-g` dataset into Python. The dataset contains
    information about loan applicants, including their demographic information, financial
    information, and loan approval status. We can use the `pandas` library to load
    the dataset and explore its structure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从将 `credit-g` 数据集加载到 Python 开始。该数据集包含有关贷款申请人的信息，包括他们的人口统计信息、财务信息和贷款批准状态。我们可以使用
    `pandas` 库来加载数据集并探索其结构：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the first five rows of the dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集的前五行：
- en: '![Figure 6.1 – The features (first column) and first five rows of the credit-g
    dataset](img/B19297_06_1.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 信用-g 数据集的特征（第一列）和前五行](img/B19297_06_1.jpg)'
- en: Figure 6.1 – The features (first column) and first five rows of the credit-g
    dataset
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 信用-g 数据集的特征（第一列）和前五行
- en: Now that we have loaded the dataset, we can create pattern-matching labeling
    functions. In this example, we will create two labeling functions that assign
    labels to loan applicants based on their income and credit history. `income_labeling_function`
    assigns a label of `1` to loan applicants with an income greater than 5,000 and
    a label of `0` to all others. `credit_history_labeling_function` assigns a label
    of `1` to loan applicants with a credit history of 1, and a label of `0` to all
    others.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据集，我们可以创建模式匹配的标签函数。在这个例子中，我们将创建两个标签函数，根据申请人的收入和信用历史来分配标签。`income_labeling_function`
    将标签 `1` 分配给收入超过 5,000 的贷款申请人，将标签 `0` 分配给所有其他人。`credit_history_labeling_function`
    将标签 `1` 分配给信用历史为 1 的贷款申请人，将标签 `0` 分配给所有其他人。
- en: 'Given the features in the `credit-g` dataset, we can create two labeling functions
    based on `credit_amount` and `age`. `credit_amount_labeling_function` assigns
    a label of `1` to loan applicants with a credit amount greater than 5,000 and
    a label of `0` to all others. `age_labeling_function` assigns a label of `1` to
    loan applicants older than 30 and a label of `0` to all others:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 `credit-g` 数据集中的特征，我们可以根据 `credit_amount` 和 `age` 创建两个标签函数。`credit_amount_labeling_function`
    将标签 `1` 分配给信用额度超过 5,000 的贷款申请人，将标签 `0` 分配给所有其他人。`age_labeling_function` 将标签 `1`
    分配给年龄超过 30 的贷款申请人，将标签 `0` 分配给所有其他人：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After creating the labeling functions, we can apply them to the `credit-g`
    dataset. We can use the `apply` function in pandas to apply the labeling functions
    to each row of the dataset. The `apply` function applies the labeling functions
    to each row of the dataset and assigns the labels to new columns in the dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建标签函数之后，我们可以将它们应用到 `credit-g` 数据集上。我们可以使用 pandas 中的 `apply` 函数将标签函数应用到数据集的每一行。`apply`
    函数将标签函数应用到数据集的每一行，并将标签分配到数据集的新列中：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output DataFrame using these functions. The DataFrame now has two
    additional columns with newly created labels:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用这些函数的输出 DataFrame。DataFrame 现在有两个额外的列，包含新创建的标签：
- en: '![Figure 6.2 – The updated credit-g dataset with two new features, credit_amount_label
    and age_label](img/B19297_06_2.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 更新后的credit-g数据集，新增两个特征：credit_amount_label和age_label](img/B19297_06_2.jpg)'
- en: Figure 6.2 – The updated credit-g dataset with two new features, credit_amount_label
    and age_label
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 更新后的credit-g数据集，新增两个特征：credit_amount_label和age_label
- en: Having explored pattern-matching functions, we now shift our focus to the simplicity
    and effectiveness of database lookup techniques. In this next section, we’ll harness
    structured databases to enhance labeling accuracy, making our approach even more
    robust.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了模式匹配函数之后，我们现在将关注数据库查找技术的简单性和有效性。在下一节中，我们将利用结构化数据库来提高标记准确性，使我们的方法更加稳健。
- en: Database lookup
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库查找
- en: The **database lookup** (**DB lookup**) labeling technique provides a powerful
    means of assigning labels to data instances by leveraging information stored in
    databases. By querying relevant databases and retrieving labeled information,
    this approach enables automated and accurate labeling. This technique involves
    searching and retrieving labels from databases based on specific attributes or
    key-value pairs associated with data instances. It relies on the premise that
    databases contain valuable labeled information that can be utilized for data labeling
    purposes. By performing queries against databases, relevant labels are fetched
    and assigned to the corresponding data instances.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据库查找**（**DB查找**）标签技术通过利用数据库中存储的信息，为数据实例分配标签提供了一种强大的手段。通过查询相关数据库并检索标记信息，这种方法可以实现自动化和准确的标记。该技术涉及根据与数据实例相关的特定属性或键值对从数据库中搜索和检索标签。它基于数据库包含可用于数据标记目的的有价值标记信息的假设。通过针对数据库执行查询，可以检索相关标签并将其分配给相应的数据实例。'
- en: 'The DB lookup technique finds application in various domains and scenarios
    within machine learning. Some common applications include the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库查找技术在机器学习的各个领域和场景中都有应用。以下是一些常见应用：
- en: '**Entity recognition**: In natural language processing tasks, such as named
    entity recognition or entity classification, DB lookup can be used to retrieve
    labels for entities based on their attributes stored in databases. This aids in
    the accurate identification and categorization of entities in text data.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体识别**：在自然语言处理任务中，如命名实体识别或实体分类，可以使用数据库查找根据存储在数据库中的属性检索实体的标签。这有助于在文本数据中准确识别和分类实体。'
- en: '**Product categorization**: E-commerce platforms often maintain databases containing
    product information, including categories and attributes. DB lookup can be employed
    to fetch product labels based on their features, allowing for automated categorization
    and organization of products.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品分类**：电子商务平台通常维护包含产品信息（包括类别和属性）的数据库。可以使用数据库查找根据其特征检索产品标签，从而实现产品的自动化分类和组织。'
- en: '**Geospatial analysis**: Databases containing geographical information, such
    as maps or geotagged data, can be queried using DB lookup to assign labels based
    on spatial attributes. This technique facilitates tasks such as location-based
    recommendations, geospatial clustering, and boundary identification.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地理空间分析**：包含地理信息的数据库，如地图或地理标记数据，可以通过数据库查找来查询，根据空间属性分配标签。这项技术有助于实现基于位置推荐、地理空间聚类和边界识别等任务。'
- en: '**Medical diagnosis**: Medical databases store extensive information about
    diseases, symptoms, and patient records. DB lookup can be utilized to retrieve
    relevant labels for patient symptoms, aiding in automated medical diagnosis and
    decision support systems.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗诊断**：医疗数据库存储有关疾病、症状和患者记录的大量信息。可以使用数据库查找来检索与患者症状相关的相关标签，从而帮助自动化医疗诊断和决策支持系统。'
- en: Now, let’s talk about Boolean flag labeling. It’s a simple yet powerful method
    that helps us improve and automate labeling by using clear and logical conditions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈布尔标志标记。这是一种简单而强大的方法，通过使用清晰和逻辑的条件来帮助我们改进和自动化标记。
- en: Boolean flags
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 布尔标志
- en: The `true`/`false` or `1`/`0`), are associated with specific characteristics
    or properties that help identify the desired label. By examining the presence
    or absence of these flags, data instances can be automatically labeled.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (`true`/`false` 或 `1`/`0`) 与特定的特征或属性相关联，有助于识别所需的标签。通过检查这些标志的存在与否，数据实例可以自动标记。
- en: 'The Boolean flags labeling technique finds applications across various domains
    in machine learning. Some common applications include the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔标志标记技术在机器学习的各个领域都有应用。以下是一些常见应用：
- en: '**Data filtering**: Boolean flags can be used to filter and label data instances
    based on specific criteria. For example, in sentiment analysis, a positive sentiment
    flag can be assigned to text instances that contain positive language or keywords,
    while a negative sentiment flag can be assigned to instances with negative language.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据过滤**：布尔标志可用于根据特定标准过滤和标记数据实例。例如，在情感分析中，可以将积极情感标志分配给包含积极语言或关键词的文本实例，而将消极情感标志分配给包含消极语言的实例。'
- en: '**Event detection**: Boolean flags can aid in labeling instances to detect
    specific events or conditions. For instance, in cybersecurity, a flag can be set
    to indicate instances with suspicious network activity, enabling the identification
    of potential security threats.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件检测**：布尔标志可以帮助标记实例以检测特定事件或条件。例如，在网络安全领域，可以设置一个标志来指示具有可疑网络活动的实例，从而识别潜在的安全威胁。'
- en: '**Anomaly detection**: Boolean flags can be used to label instances as normal
    or anomalous. By defining flags that capture typical patterns or behaviors, instances
    that deviate from these patterns can be flagged as anomalies, facilitating anomaly
    detection tasks.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：布尔标志可用于将实例标记为正常或异常。通过定义捕获典型模式或行为的标志，偏离这些模式的实例可以标记为异常，从而促进异常检测任务。'
- en: '**Quality control**: Boolean flags can assist in labeling instances for quality
    control purposes. For example, in manufacturing, flags can be set to label instances
    as defective or non-defective based on predefined quality criteria.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：布尔标志可以帮助标记实例进行质量控制。例如，在制造业中，可以根据预定义的质量标准设置标志来标记实例为不合格或合格。'
- en: 'The Boolean flags labeling technique offers several advantages in machine learning
    applications:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔标志标记技术在机器学习应用中提供了几个优势：
- en: '**Simplicity and efficiency**: Boolean flags provide a straightforward and
    efficient labeling mechanism. The labeling process involves checking the presence
    or absence of flags, which can be implemented using simple conditional statements
    or logical operations.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁性与效率**：布尔标志提供了一种简单且高效的标记机制。标记过程涉及检查标志的存在或不存在，这可以通过简单的条件语句或逻辑运算来实现。'
- en: '**Flexibility and customization**: Boolean flags allow for customization and
    adaptability to different labeling scenarios. Flags can be defined based on specific
    criteria or requirements, providing flexibility in assigning labels according
    to the desired characteristics.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性与定制**：布尔标志允许定制和适应不同的标记场景。标志可以根据特定标准或要求定义，提供根据所需特征分配标签的灵活性。'
- en: '**Interpretability**: The Boolean flags labeling technique offers interpretability,
    as the presence or absence of flags directly corresponds to the assigned labels.
    This transparency allows for better understanding and validation of the labeling
    process.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：布尔标志标记技术提供了可解释性，因为标志的存在或不存在直接对应于分配的标签。这种透明度有助于更好地理解和验证标记过程。'
- en: '**Scalability**: Boolean flags can be easily scaled to handle large datasets.
    Since the labeling decision is based on binary indicators, the computational overhead
    remains low, making it suitable for processing massive amounts of data.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：布尔标志可以轻松扩展以处理大量数据集。由于标记决策基于二进制指示器，计算开销保持较低，使其适合处理大量数据。'
- en: 'While the Boolean flags labeling technique provides simplicity and efficiency,
    certain challenges and considerations should be taken into account:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然布尔标志标记技术提供了简洁性和效率，但应考虑以下某些挑战和注意事项：
- en: '**Feature engineering**: Designing effective Boolean flags requires careful
    feature engineering. The flags should be informative and relevant to the desired
    labels, necessitating a deep understanding of the problem domain and data characteristics.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：设计有效的布尔标志需要仔细的特征工程。标志应该是信息丰富且与所需标签相关的，这需要深入理解问题域和数据特征。'
- en: '**Data imbalance**: In scenarios where the data is imbalanced, meaning one
    label dominates over others, the Boolean flags technique may face challenges.
    Proper handling techniques, such as oversampling or under-sampling, may be required
    to address the imbalance issue.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不平衡**：在数据不平衡的情况下，即一个标签支配其他标签，布尔标志技术可能会面临挑战。可能需要适当的处理技术，如过采样或欠采样，来解决不平衡问题。'
- en: '**Generalization**: Boolean flags may not capture the full complexity of the
    underlying data distribution, potentially leading to overfitting or limited generalization.
    It is important to consider complementary techniques, such as feature extraction
    or more advanced machine learning algorithms, to enhance the performance and generalization
    capabilities.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化**：布尔标志可能无法捕捉到潜在数据分布的全部复杂性，可能导致过拟合或泛化能力有限。考虑互补技术，如特征提取或更高级的机器学习算法，以增强性能和泛化能力是很重要的。'
- en: '**Flag interpretation**: While Boolean flags provide interpretability, it is
    crucial to carefully interpret the flags’ meanings in relation to the assigned
    labels. In some cases, the flags may capture correlations rather than causal relationships,
    requiring further investigation for a more accurate interpretation.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标志解释**：虽然布尔标志提供了可解释性，但仔细解释标志的含义与分配的标签之间的关系是至关重要的。在某些情况下，标志可能捕捉到相关性而不是因果关系，需要进一步调查以获得更准确的解释。'
- en: You may have already noticed some similarities between Boolean flags and one-hot
    encoding (covered in [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070)*, Techniques
    for Data Cleaning*). Therefore, it’s important to understand when these techniques
    are appropriate.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了布尔标志和独热编码（在第[*第5章*](B19297_05.xhtml#_idTextAnchor070)*，数据清洗技术*中介绍）之间的相似之处。因此，了解何时使用这些技术是很重要的。
- en: When choosing between Boolean flags and one-hot encoding, the specific use case
    is a crucial factor. If you’re working with a categorical variable that can be
    naturally divided into two categories or states (such as yes/no, true/false),
    using a Boolean flag might be the best option. It’s simpler, more memory-efficient,
    and can make the model easier to interpret.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择布尔标志和独热编码之间，具体的用例是一个关键因素。如果你正在处理一个可以自然地分为两个类别或状态（例如是/否、真/假）的分类变量，使用布尔标志可能是最佳选择。它更简单，更节省内存，并且可以使模型更容易解释。
- en: For example, if you’re predicting whether an email is spam or not, a Boolean
    flag such as `contains_link` (`1` if the email contains a link, `0` otherwise)
    could be a very effective feature. This simplicity can lead to more interpretable
    models, as each feature directly corresponds to a condition or state.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在预测一封电子邮件是否为垃圾邮件，一个布尔标志如`contains_link`（如果电子邮件包含链接则为`1`，否则为`0`）可能是一个非常有效的特征。这种简单性可以导致更可解释的模型，因为每个特征直接对应一个条件或状态。
- en: On the other hand, one-hot encoding is more suitable for categorical variables
    with multiple categories where no natural binary division exists. For instance,
    if you’re working with a feature such as `color` with values such as `red`, `blue`,
    `green`, etc., one-hot encoding would be a better choice. That’s because the numbers
    assigned to each category shouldn’t imply a mathematical relationship between
    the categories unless one exists. For example, encoding red as `1` and blue as
    `2` doesn’t mean blue is twice red.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，独热编码更适合于具有多个类别且不存在自然二分法的分类变量。例如，如果你正在处理一个如`color`这样的特征，其值有`red`、`blue`、`green`等，独热编码将是一个更好的选择。这是因为分配给每个类别的数字不应该暗示类别之间存在数学关系，除非确实存在。例如，将红色编码为`1`，蓝色编码为`2`并不意味着蓝色是红色的两倍。
- en: To avoid implying such unintended relationships, creating a separate feature
    for each possible color is preferred. This approach captures more information
    about the color feature and doesn’t impose an arbitrary order or importance on
    the different colors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免暗示这种未预期的关系，为每种可能的颜色创建一个单独的特征是首选的。这种方法可以捕捉更多关于颜色特征的信息，并且不对不同颜色施加任意顺序或重要性。
- en: Furthermore, the type of machine learning model being used also influences the
    choice. Some models, such as decision trees and random forests, can handle categorical
    variables quite well, so one-hot encoding (which increases the dimensionality
    of the dataset) might not be necessary. However, others, such as linear regression,
    logistic regression, and support vector machines, require numerical input, necessitating
    some form of encoding for categorical variables.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所使用的机器学习模型的类型也会影响选择。一些模型，如决策树和随机森林，可以很好地处理分类变量，因此可能不需要一热编码（这会增加数据集的维度）。然而，其他模型，如线性回归、逻辑回归和支持向量机，需要数值输入，因此需要对分类变量进行某种形式的编码。
- en: Lastly, it’s worth noting that these aren’t the only methods for handling categorical
    data. There are other techniques, such as ordinal encoding, target encoding, and
    bin counting, each with its own strengths and weaknesses. The key is to understand
    the nature of your data and the requirements of your specific use case to choose
    the most appropriate method.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，这些并不是处理分类数据的唯一方法。还有其他技术，如有序编码、目标编码和二进制计数，每种技术都有其自身的优缺点。关键是理解你数据的本质和特定用例的要求，以选择最合适的方法。
- en: Let’s explore how to utilize Boolean flags in Python with the `credit-g` dataset.
    Imagine we want to create a function that applies Boolean flags to label data
    points according to basic rules or heuristics. For instance, we can write a function
    that evaluates whether a credit applicant’s credit amount is above a specific
    threshold, subsequently assigning a Boolean flag to the data point based on this
    assessment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何使用Python中的布尔标志在`credit-g`数据集中。想象一下，我们想要创建一个函数，该函数根据基本规则或启发式方法应用布尔标志来标记数据点。例如，我们可以编写一个函数，评估信用申请人的信用额度是否高于特定阈值，然后根据这种评估为数据点分配布尔标志。
- en: 'The following functions will check if the credit amount is below or above the
    median credit amount:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将检查信用额度是否低于或高于中值信用额度：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have defined our function, we can apply it to our `df` DataFrame
    to label the data points with Boolean flags:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的函数，我们可以将其应用于我们的`df` DataFrame，用布尔标志标记数据点：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Figure 6**.3* is the output DataFrame after we have applied these functions.
    Notice that we have now created a new column giving us additional information
    on the applicant’s credit amount, which can be used as a feature in machine learning
    models.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3* 是应用这些函数后的输出DataFrame。注意，我们现在创建了一个新列，提供了关于申请人信用额度的额外信息，这可以用作机器学习模型中的特征。'
- en: '![Figure 6.3 – The credit-g dataset with the new Boolean flag LF_CreditAmountAboveMedian
    added](img/B19297_06_3.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 添加了新布尔标志LF_CreditAmountAboveMedian的credit-g数据集](img/B19297_06_3.jpg)'
- en: Figure 6.3 – The credit-g dataset with the new Boolean flag LF_CreditAmountAboveMedian
    added
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 添加了新布尔标志LF_CreditAmountAboveMedian的credit-g数据集
- en: In the next section, let’s explore weak supervision—a sophisticated labeling
    technique that adeptly integrates information from various sources, navigating
    the intricacies of real-world data to enhance precision and adaptability.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们探讨弱监督——这是一种复杂的标记技术，它巧妙地整合了来自各种来源的信息，在现实世界数据的复杂性中导航，以增强精确性和适应性。
- en: Weak supervision
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱监督
- en: Weak supervision is a labeling technique in machine learning that leverages
    imperfect or noisy sources of supervision to assign labels to data instances.
    Unlike traditional labeling methods that rely on manually annotated data, weak
    supervision allows for a more scalable and automated approach to labeling. It
    refers to the use of heuristics, rules, or probabilistic methods to generate approximate
    labels for data instances.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督是机器学习中的一种标记技术，它利用不完美或噪声的监督源为数据实例分配标签。与依赖于手动标注数据的传统标记方法不同，弱监督允许更可扩展和自动化的标记方法。它指的是使用启发式方法、规则或概率方法为数据实例生成近似标签。
- en: Rather than relying on a single authoritative source of supervision, weak supervision
    harnesses multiple sources that may introduce noise or inconsistency. The objective
    is to generate labels that are “weakly” indicative of the true underlying labels,
    enabling model training in scenarios where obtaining fully labeled data is challenging
    or expensive.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督不是依赖于单一的权威监督源，而是利用多个可能引入噪声或不一致性的来源。目标是生成“弱”指示真实潜在标签的标签，从而在获取完全标注数据具有挑战性或成本高昂的情况下进行模型训练。
- en: For instance, consider a task where we want to build a machine learning model
    to identify whether an email is spam or not. Ideally, we would have a large dataset
    of emails that are accurately labeled as “spam” or “not spam.” However, obtaining
    such a dataset could be challenging, time-consuming, and expensive.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个任务，我们想要构建一个机器学习模型来识别一封电子邮件是否为垃圾邮件。理想情况下，我们会有一大批被准确标记为“垃圾邮件”或“非垃圾邮件”的电子邮件数据集。然而，获取这样的数据集可能具有挑战性，耗时且成本高昂。
- en: 'With weak supervision, we can use alternative, less perfect ways to label our
    data. For instance, we could create some rules or heuristics based on common patterns
    in spam emails. Here are a few examples of such rules:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用弱监督，我们可以使用替代的、不那么完美的方法来标记我们的数据。例如，我们可以根据垃圾邮件中的常见模式创建一些规则或启发式方法。以下是一些此类规则的例子：
- en: If the email contains words such as “lottery”, “win”, or “prize”, it might be
    spam
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果电子邮件包含诸如“彩票”、“赢”或“奖品”等词语，它可能是垃圾邮件
- en: If the email is from an unknown sender and contains many links, it might be
    spam
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果电子邮件来自未知发件人且包含许多链接，它可能是垃圾邮件
- en: If the email contains phrases such as “urgent action required”, it might be
    spam
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果电子邮件包含诸如“紧急行动要求”之类的短语，它可能是垃圾邮件
- en: Using these rules, we can automatically label our email dataset. These labels
    won’t be perfect— there will be false positives (non-spam emails incorrectly labeled
    as spam) and false negatives (spam emails incorrectly labeled as non-spam). But
    they give us a starting point for training our machine learning model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些规则，我们可以自动标记我们的电子邮件数据集。这些标签不会完美——会有误报（非垃圾邮件被错误地标记为垃圾邮件）和漏报（垃圾邮件被错误地标记为非垃圾邮件）。但它们为我们训练机器学习模型提供了一个起点。
- en: The model can then learn from these “weak” labels and, with a sufficiently large
    and diverse dataset, should still be able to generalize well to new, unseen emails.
    This makes weak supervision a scalable and efficient approach to labeling, particularly
    useful when perfect labels are hard to come by.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以从这些“弱”标签中学习，并且，在有足够大和多样化的数据集的情况下，应该仍然能够很好地泛化到新的、未见过的电子邮件。这使得弱监督成为一种可扩展且高效的标注方法，特别适用于难以获得完美标签的情况。
- en: 'Weak supervision can be derived from various sources, including the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督可以来自各种来源，包括以下：
- en: '**Rule-based systems**: Domain experts or heuristics-based approaches can define
    rules or guidelines for labeling data based on specific patterns, features, or
    conditions. These rules may be derived from knowledge bases, existing models,
    or expert opinions.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的系统**：领域专家或基于启发式的方法可以定义基于特定模式、特征或条件的规则或指南来标记数据。这些规则可能来自知识库、现有模型或专家意见。'
- en: '**Crowdsourcing**: Leveraging the power of human annotators through crowdsourcing
    platforms, weak supervision can be obtained by aggregating the annotations from
    multiple individuals. This approach introduces noise but can be cost-effective
    and scalable.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**众包**：通过众包平台利用人类标注者的力量，可以通过汇总多个个体的标注来获得弱监督。这种方法引入了噪声，但可能具有成本效益和可扩展性。'
- en: '**Distant supervision**: Distant supervision involves using existing labeled
    data that may not perfectly align with the target task but can serve as a proxy.
    An example is using existing data with auxiliary labels to train a model for a
    related but different task.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程监督**：远程监督涉及使用可能与目标任务不完全一致但可以作为代理的现有标记数据。一个例子是使用具有辅助标签的现有数据来训练一个相关但不同的任务的模型。'
- en: '**Data augmentation**: Weak supervision can be obtained through data augmentation
    techniques such as data synthesis, transformation, or perturbation. By generating
    new labeled instances based on existing labeled data, weak supervision can be
    expanded.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：通过数据增强技术，如数据合成、转换或扰动，可以获得弱监督。通过根据现有标记数据生成新的标记实例，弱监督可以扩展。'
- en: 'Weak supervision offers several advantages in machine learning applications:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督在机器学习应用中提供了几个优势：
- en: '**Scalability**: Weak supervision allows for large-scale data labeling by leveraging
    automated or semi-automated techniques. It reduces the manual effort required
    for manual annotation, enabling the utilization of larger datasets.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：弱监督通过利用自动化或半自动化技术，允许进行大规模数据标记。它减少了手动标注所需的劳动强度，使得可以利用更大的数据集。'
- en: '**Cost-effectiveness**: By leveraging weakly supervised sources, the cost of
    obtaining labeled data can be significantly reduced compared to fully supervised
    approaches. This is particularly beneficial in scenarios where manual labeling
    is expensive or impractical.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：通过利用弱监督来源，与全监督方法相比，获取标记数据的成本可以显著降低。这在手动标记昂贵或不切实际的情况下尤其有益。'
- en: '**Flexibility and adaptability**: Weak supervision techniques can be easily
    adapted and modified to incorporate new sources of supervision or update existing
    rules. This flexibility allows for iterative improvement and refinement of the
    labeling process.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性和适应性**：弱监督技术可以轻松地适应和修改，以纳入新的监督来源或更新现有规则。这种灵活性允许迭代改进和细化标记过程。'
- en: '**Handling noisy labels**: Weak supervision techniques can handle noisy or
    inconsistent labels by aggregating multiple weak signals. This robustness to noise
    reduces the impact of individual labeling errors on the overall training process.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理噪声标签**：弱监督技术可以通过聚合多个弱信号来处理噪声或不一致的标签。这种对噪声的鲁棒性减少了单个标记错误对整体训练过程的影响。'
- en: 'There are, however, certain challenges and considerations to be aware of:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些挑战和注意事项需要注意：
- en: '**Noise and label quality**: Weakly supervised labels may contain noise or
    errors due to the imperfect nature of the supervision sources. Careful evaluation
    and validation are necessary to ensure label quality and minimize the propagation
    of noisy labels during model training.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声与标签质量**：由于监督来源的不完美性质，弱监督标签可能包含噪声或错误。仔细评估和验证是必要的，以确保标签质量并最小化在模型训练过程中噪声标签的传播。'
- en: '**Trade-off between precision and recall**: Weak supervision techniques often
    prioritize scalability and coverage over precision. Balancing the trade-off between
    recall (coverage) and precision (accuracy) is essential in obtaining reliable
    weakly labeled data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度与召回率的权衡**：弱监督技术通常优先考虑可扩展性和覆盖范围，而不是精确度。在获取可靠的弱标签数据时，平衡召回率（覆盖范围）和精确度（准确性）之间的权衡至关重要。'
- en: '**Labeling confidence and model training**: Handling the uncertainty associated
    with weakly supervised labels is crucial. Techniques such as label calibration,
    data augmentation, or active learning can be employed to mitigate the impact of
    label uncertainty during model training.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记置信度与模型训练**：处理与弱监督标签相关的不确定性至关重要。可以采用标签校准、数据增强或主动学习等技术来减轻在模型训练期间标签不确定性的影响。'
- en: '**Generalization and model performance**: Weakly supervised models may struggle
    with generalizing to unseen or challenging instances due to the inherent noise
    in the labels. Strategies such as regularization, ensemble methods, or transfer
    learning can be employed to enhance model performance.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化与模型性能**：由于标签中的固有噪声，弱监督模型可能难以泛化到未见或具有挑战性的实例。可以采用正则化、集成方法或迁移学习等策略来提高模型性能。'
- en: 'In this section, we will explore how to use labeling functions in Python to
    train a machine learning model on the *Loan Prediction* dataset we introduced
    in [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070)*, Techniques for Data Cleaning*.
    First, we need to prepare the data by importing the necessary libraries and loading
    the dataset, and we need to preprocess the data by handling missing values and
    encoding categorical variables:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何使用Python中的标记函数在[*第5章*](B19297_05.xhtml#_idTextAnchor070)*数据清洗技术*中介绍的*贷款预测*数据集上训练机器学习模型。首先，我们需要通过导入必要的库和加载数据集来准备数据，并且需要通过处理缺失值和编码分类变量来预处理数据：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will use the `LabelEncoder` function from scikit-learn’s `preprocessing`
    class to encode categorical columns:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn的`preprocessing`类中的`LabelEncoder`函数来编码分类列：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can define our labeling functions. In this example, we will define
    three labeling functions based on some simple heuristics. These labeling functions
    take in a row of the dataset as input and return a label. The label is `1` if
    the row is likely to be in the positive class, `0` if it is likely to be in the
    negative class, and `-1` if it is uncertain. Functions such as these are commonly
    used in weak supervision approaches where you have a large amount of unlabeled
    data and you want to generate noisy labels for them:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们的标签函数。在这个例子中，我们将基于一些简单的启发式方法定义三个标签函数。这些标签函数接受数据集的一行作为输入，并返回一个标签。如果该行可能属于正类，则标签为
    `1`；如果可能属于负类，则标签为 `0`；如果不确定，则标签为 `-1`。这类函数在弱监督方法中常用，其中你有一大量未标记数据，并希望为它们生成噪声标签：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can apply the labeling functions to the dataset using the Snorkel library.
    Here, we create a list of the three labeling functions and use the `PandasLFApplier`
    to apply them to the dataset. The output is a `L_train` matrix where each row
    corresponds to a data point and each column corresponds to a labeling function:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Snorkel 库将标签函数应用于数据集。在这里，我们创建了一个包含三个标签函数的列表，并使用 `PandasLFApplier` 将它们应用于数据集。输出是一个
    `L_train` 矩阵，其中每一行对应一个数据点，每一列对应一个标签函数：
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You’ll see the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '![Figure 6.4 – Progress bar showing the training progress](img/B19297_06_4.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 显示训练进度的进度条](img/B19297_06_4.jpg)'
- en: Figure 6.4 – Progress bar showing the training progress
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 显示训练进度的进度条
- en: 'To improve the output of the labeling functions, we need to combine them to
    obtain a more accurate label for each data point. We can do this using the `LabelModel`
    class in the Snorkel library. The `LabelModel` class is a probabilistic model
    used for combining the outputs of multiple labeling functions to generate more
    accurate and reliable labeling for each data point. It plays a crucial role in
    addressing the noise and inaccuracies that may arise from individual labeling
    functions. We create a `LabelModel` object as `label_model` and fit it to the
    output of the labeling functions. The cardinality parameter specifies the number
    of classes, which is `2` in this case. We also specify the number of epochs to
    train for and a random seed for reproducibility:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高标签函数的输出，我们需要将它们结合起来，为每个数据点获得更准确的标签。我们可以使用 Snorkel 库中的 `LabelModel` 类来完成此操作。`LabelModel`
    类是一个概率模型，用于结合多个标签函数的输出，为每个数据点生成更准确和可靠的标签。它在解决可能由单个标签函数引起的噪声和不准确性方面发挥着关键作用。我们创建一个
    `LabelModel` 对象作为 `label_model`，并将其拟合到标签函数的输出。基数参数指定了类的数量，在本例中为 `2`。我们还指定了训练的轮数和一个随机种子以确保可重复性：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After executing the preceding code snippet utilizing Snorkel’s labeling model,
    a progress bar will display the incremental application of the labeling:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码片段，使用 Snorkel 的标签模型后，将显示标签的增量应用进度条：
- en: '![Figure 6.5 – The Snorkel progress bar showing the incremental progress of
    the labeling process](img/B19297_06_5.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 显示标签过程增量进度的 Snorkel 进度条](img/B19297_06_5.jpg)'
- en: Figure 6.5 – The Snorkel progress bar showing the incremental progress of the
    labeling process
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 显示标签过程增量进度的 Snorkel 进度条
- en: 'We can now use the `LabelModel` class to generate labels for the training data
    and evaluate its performance:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `LabelModel` 类为训练数据生成标签并评估其性能：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, let’s learn how to use semi-weak supervision for labeling. It’s a smart
    technique that combines weak supervision with a bit of manual labeling to make
    our labels more accurate.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何使用半弱监督进行标签化。这是一种智能技术，它结合了弱监督和一些手动标签，使我们的标签更加准确。
- en: Semi-weak supervision
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半弱监督
- en: Semi-weak supervision is a technique used in machine learning to improve the
    accuracy of a model by combining a small set of labeled data with a larger set
    of weakly labeled data. In this approach, the labeled data is used to guide the
    learning process, while the weakly labeled data provides additional information
    to improve the accuracy of the model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 半弱监督是一种在机器学习中使用的技巧，通过结合一小部分标记数据和大量弱标记数据来提高模型的准确性。在这种方法中，标记数据用于指导学习过程，而弱标记数据提供额外的信息以提高模型的准确性。
- en: Semi-weak supervision is particularly useful when labeled data is limited or
    expensive to obtain and can be applied to a wide range of machine learning tasks,
    such as text classification, image recognition, and object detection.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 半弱监督在标记数据有限或难以获取时特别有用，并且可以应用于广泛的机器学习任务，如文本分类、图像识别和目标检测。
- en: In the loan prediction dataset, we have a set of data points representing loan
    applications, each with a set of features such as income, credit history, and
    loan amount, and a label indicating whether the loan was approved or not. However,
    this labeled data may be incomplete or inaccurate, which can lead to poor model
    performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在贷款预测数据集中，我们有一组代表贷款申请的数据点，每个数据点都包含一系列特征，如收入、信用历史和贷款金额，以及一个表示贷款是否批准的标签。然而，这些标记数据可能是不完整或不准确的，这可能导致模型性能不佳。
- en: To address this issue, we can use semi-weak supervision to generate additional
    labels for the loan prediction dataset. One approach is to use weak supervision
    techniques to generate labels automatically based on heuristics or rules. For
    example, we can use regular expressions to identify patterns in the loan application
    text data that are indicative of a high-risk loan. We can also use external data
    sources, such as credit reports or social media data, to generate additional weakly
    labeled data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用半弱监督为贷款预测数据集生成额外的标签。一种方法是通过启发式或规则自动生成标签的弱监督技术。例如，我们可以使用正则表达式来识别贷款申请文本数据中的模式，这些模式表明高风险贷款。我们还可以使用外部数据源，如信用报告或社交媒体数据，来生成额外的弱标记数据。
- en: Once we have a set of weakly labeled data, we can use it to train a model along
    with the small set of labeled data. The labeled data is used to guide the learning
    process, while the weakly labeled data provides additional information to improve
    the accuracy of the model. By using semi-weak supervision, we can effectively
    use all available data to improve model performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一组弱标记数据，我们就可以使用它来训练一个模型，同时结合少量标记数据。标记数据用于指导学习过程，而弱标记数据提供额外的信息以提高模型的准确性。通过使用半弱监督，我们可以有效地利用所有可用数据来提高模型性能。
- en: 'Here is an example of how to implement semi-weak supervision for the loan prediction
    dataset using Snorkel and Python. We first import the necessary libraries and
    functions from those libraries. We then load the dataset using pandas:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用Snorkel和Python实现贷款预测数据集半弱监督的示例。我们首先从这些库中导入必要的库和函数。然后使用pandas加载数据集：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s define a function to preprocess the dataset. We will use similar preprocessing
    methods to the ones discussed earlier in this chapter:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来预处理数据集。我们将使用本章前面讨论过的类似预处理方法：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we create three labeling functions for `ApplicantIncome` and `LoanAmount`.
    The `lf1(x)` function takes a data instance `x` as input and performs a labeling
    operation based on the value of the `ApplicantIncome` feature. If the `ApplicantIncome`
    is less than 5,000, the function returns a label of `0`. Otherwise, if the `ApplicantIncome`
    is greater than or equal to 5,000, the function returns a label of `1`. Essentially,
    this function assigns a label of `0` to instances with low applicant income and
    a label of `1` to instances with higher applicant income.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为`ApplicantIncome`和`LoanAmount`创建了三个标记函数。`lf1(x)`函数接收一个数据实例`x`作为输入，并根据`ApplicantIncome`特征值执行标记操作。如果`ApplicantIncome`小于5,000，则函数返回标签`0`。否则，如果`ApplicantIncome`大于或等于5,000，则函数返回标签`1`。本质上，这个函数将标签`0`分配给申请收入较低的数据实例，将标签`1`分配给申请收入较高的数据实例。
- en: The `lf2(x)` function also takes a data instance `x` as input and assigns a
    label based on the value of the `LoanAmount` feature. If the `LoanAmount` is greater
    than 200, the function returns a label of `0`. Conversely, if the `LoanAmount`
    is less than or equal to 200, the function returns a label of `1`. This function
    categorizes instances with large loan amounts as label `0` and instances with
    smaller loan amounts as label `1`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`lf2(x)`函数同样接收一个数据实例`x`作为输入，并根据`LoanAmount`特征值分配标签。如果`LoanAmount`大于200，则函数返回标签`0`。相反，如果`LoanAmount`小于或等于200，则函数返回标签`1`。这个函数将贷款金额较大的数据实例分类为标签`0`，将贷款金额较小的数据实例分类为标签`1`。'
- en: 'Utilizing the `lf3(x)` function, we compute the ratio between the loan amount
    and the applicant’s income. This ratio serves as a crucial metric in determining
    the feasibility of the loan. Based on this calculated ratio, we categorize the
    data points into different labels. If the loan-to-income ratio falls below or
    equals 0.3, we assign a label of `1`, indicating approval of the loan request.
    In cases where the ratio exceeds 0.3 but remains less than or equal to 0.5, we
    designate the data point with a label of `0`, signifying uncertainty regarding
    loan approval. Conversely, if the ratio surpasses 0.5, we assign the label `-1`,
    indicating denial of the loan application. This approach enables us to incorporate
    the affordability aspect into our labeling process, enhancing the granularity
    of our weak supervision approach for loan approval prediction:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 利用`lf3(x)`函数，我们计算贷款金额与申请人收入的比率。这个比率是确定贷款可行性的关键指标。根据这个计算出的比率，我们将数据点分类到不同的标签。如果贷款与收入比率低于或等于0.3，我们分配标签`1`，表示批准贷款申请。在比率超过0.3但不超过0.5的情况下，我们指定数据点的标签为`0`，表示对贷款批准的不确定性。相反，如果比率超过0.5，我们分配标签`-1`，表示拒绝贷款申请。这种方法使我们能够将可负担性方面纳入我们的标签过程，增强我们贷款批准预测的弱监督方法的粒度：
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then apply the preprocessing techniques to the input data (`df`). The `preprocess_data`
    function is used to perform the necessary preprocessing steps. The resulting preprocessed
    data is stored in the variable `X`. Additionally, the target variable, `Loan_Status`,
    is transformed from categorical values (`N` and `Y`) to numerical values (`0`
    and `1`) and stored in the variable `y`. This step ensures that the data is ready
    for training and evaluation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将预处理技术应用于输入数据（`df`）。使用`preprocess_data`函数执行必要的预处理步骤。处理后的数据存储在变量`X`中。此外，目标变量`Loan_Status`从分类值（`N`和`Y`）转换为数值（`0`和`1`），并存储在变量`y`中。这一步骤确保数据已准备好进行训练和评估：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next step involves splitting the preprocessed data into training and testing
    sets. The `train_test_split` function from the scikit-learn library is used for
    this purpose. The data is divided into `X_train` and `X_test` for the features
    and `y_train` and `y_test` for the corresponding labels. This separation allows
    for training the model on the training set and evaluating its performance on the
    test set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将预处理数据分为训练集和测试集。使用scikit-learn库中的`train_test_split`函数来完成此操作。数据被分为特征`X_train`和`X_test`以及相应的标签`y_train`和`y_test`。这种分离允许在训练集上训练模型并在测试集上评估其性能：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we apply the two labeling functions, `lf1` and `lf2`, to the training set
    (`X_train`) using the `PandasLFApplier` class. The resulting weakly labeled data
    is stored in `L_train_weak`. The LFs analyze the features of each instance and
    assign labels based on predefined rules or conditions:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`PandasLFApplier`类将两个标签函数`lf1`和`lf2`应用于训练集（`X_train`）。生成的弱标签数据存储在`L_train_weak`中。LF分析每个实例的特征并根据预定义的规则或条件分配标签：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The label model is instantiated using the `LabelModel` class. It is configured
    with a cardinality of `2` (indicating binary classification) and set to run in
    verbose mode for progress updates. The label model is then trained on the training
    data (`L_train_weak`) using the `fit` method:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`LabelModel`类实例化标签模型。它配置为`2`的基数（表示二元分类）并设置为以详细模式运行以更新进度。然后使用`fit`方法在训练数据（`L_train_weak`）上训练标签模型：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the label model is trained, it is evaluated on the test set (`X_test`)
    to assess its performance. The applier object is used again to apply the labeling
    functions to the test set, resulting in `L_test`, which contains the weakly labeled
    instances. The score method of the label model is then used to calculate the accuracy
    of the label predictions compared to the ground truth labels (`y_test`):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦标签模型训练完成，它将在测试集（`X_test`）上评估其性能。再次使用applier对象将标签函数应用于测试集，结果生成`L_test`，其中包含弱标签实例。然后使用标签模型的score方法计算标签预测的准确度与真实标签（`y_test`）相比：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the upcoming section, we explore slicing functions for labeling—an advanced
    technique that allows us to finely segment our data. These functions provide a
    tailored approach, enabling us to apply specific labeling strategies to distinct
    subsets of our dataset.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨用于标签的切片函数——这是一种高级技术，允许我们精细分割我们的数据。这些函数提供了一种定制的方法，使我们能够将特定的标签策略应用于数据集的不同子集：
- en: Slicing functions
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切片函数
- en: Slicing functions are functions that operate on data instances and produce binary
    labels based on specific conditions. Unlike traditional labeling functions that
    provide labels for the entire dataset, slicing functions are designed to focus
    on specific subsets of the data. These subsets, or slices, can be defined based
    on various features, patterns, or characteristics of the data. Slicing functions
    offer a fine-grained approach to labeling, enabling more targeted and precise
    labeling of data instances.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 切片函数是作用于数据实例并基于特定条件产生二元标签的函数。与为整个数据集提供标签的传统标记函数不同，切片函数旨在关注数据的具体子集。这些子集或切片可以根据数据的各种特征、模式或特征来定义。切片函数提供了一种细粒度的标记方法，使数据实例的标记更加有针对性和精确。
- en: Slicing functions play a crucial role in weak supervision approaches, where
    multiple labeling sources are leveraged to assign approximate labels. Slicing
    functions complement other labeling techniques, such as rule-based systems or
    crowdsourcing, by capturing specific patterns or subsets of the data that may
    be challenging to label accurately using other methods. By applying slicing functions
    to the data, practitioners can exploit domain knowledge or specific data characteristics
    to improve the labeling process.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 切片函数在弱监督方法中起着至关重要的作用，其中利用多个标记源来分配近似标签。切片函数通过捕获可能难以使用其他方法准确标记的数据的特定模式或子集，补充了其他标记技术，如基于规则的系统或众包。通过应用切片函数到数据中，从业者可以利用领域知识或特定的数据特征来改进标记过程。
- en: To fully understand the concept of slicing functions, let’s use an example of
    a dataset containing reviews for a range of products from an e-commerce website.
    Our goal is to label these reviews as either positive or negative.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解切片函数的概念，让我们以一个包含来自电子商务网站一系列产品评论的数据集为例。我们的目标是将这些评论标记为正面或负面。
- en: 'For simplicity, let’s consider two slicing functions:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们考虑两个切片函数：
- en: '**Slicing Function 1** (**SF1**): This function targets reviews that contain
    the word “refund”. It labels a review as negative if it includes the word “refund”
    and leaves it unlabeled otherwise. The intuition behind this slicing function
    is that customers asking for a refund are likely dissatisfied with their purchase,
    hence the negative sentiment.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切片函数 1**（**SF1**）：此函数针对包含单词“退款”的评论。如果评论包含“退款”一词，则将其标记为负面，否则不标记。这个切片函数背后的直觉是，要求退款的客户可能对其购买不满意，因此具有负面情绪。'
- en: '**Slicing Function 2** (**SF2**): This function targets reviews from customers
    who purchased electronics. It labels a review as positive if it includes words
    such as “great”, “excellent”, or “love” and labels it as negative if it includes
    words such as “broken”, “defective”, or “useless”. It leaves the review unlabeled
    if it doesn’t meet any of these conditions.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切片函数 2**（**SF2**）：此函数针对购买电子产品的客户评论。如果评论包含“出色”、“优秀”或“喜爱”等词语，则将其标记为正面；如果包含“损坏”、“有缺陷”或“无用”等词语，则将其标记为负面。如果评论不符合任何这些条件，则将其保留为未标记状态。'
- en: 'You will notice that these slicing functions operate on specific subsets of
    the data and enable us to incorporate domain knowledge into the labeling process.
    Therefore, designing effective slicing functions requires a combination of domain
    knowledge, feature engineering, and experimentation. Here are some key considerations
    for designing and implementing slicing functions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这些切片函数作用于数据的具体子集，使我们能够将领域知识纳入标记过程。因此，设计有效的切片函数需要结合领域知识、特征工程和实验。以下是设计和实现切片函数的一些关键考虑因素：
- en: '**Identify relevant slices**: Determine the specific subsets or slices of the
    data that are relevant to the labeling task. This involves understanding the problem
    domain, analyzing the data, and identifying distinct patterns or characteristics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别相关切片**：确定与标记任务相关的特定数据子集或切片。这涉及到理解问题领域、分析数据以及识别独特的模式或特征。'
- en: '**Define slicing conditions**: Specify the conditions or rules that capture
    the desired subsets of the data. These conditions can be based on feature thresholds,
    pattern matching, statistical properties, or any other relevant criteria.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义切片条件**：指定捕获所需数据子集的条件或规则。这些条件可以基于特征阈值、模式匹配、统计属性或任何其他相关标准。'
- en: '**Evaluate and iterate**: Assess the performance of the slicing functions by
    comparing the assigned labels to ground truth labels or existing labeling sources.
    Iterate on the design of the slicing functions, refining the conditions and rules
    to improve the quality of the assigned labels.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估和迭代**：通过将分配的标签与地面真实标签或现有的标签源进行比较来评估切片函数的性能。对切片函数的设计进行迭代，细化条件和规则，以提高分配标签的质量。'
- en: 'Slicing functions offer several benefits in the labeling process:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 切片函数在标签过程中提供了几个好处：
- en: '**Fine-grained labeling**: Slicing functions allow for targeted labeling of
    specific subsets of the data, providing more detailed and granular labels that
    capture distinct patterns or characteristics.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细粒度标签化**：切片函数允许对数据的特定子集进行有针对性的标签化，提供更详细和粒度化的标签，以捕捉独特的模式或特征。'
- en: '**Domain knowledge incorporation**: Slicing functions enable the incorporation
    of domain expertise and specific domain knowledge into the labeling process. This
    allows for more informed and context-aware labeling decisions.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域知识整合**：切片函数允许将领域专业知识和特定领域知识整合到标签过程中。这允许做出更明智和情境感知的标签决策。'
- en: '**Complementarity with other techniques**: Slicing functions complement other
    labeling techniques by capturing slices of the data that may be challenging to
    label using traditional methods. They provide an additional source of weak supervision
    that enhances the overall labeling process.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他技术的互补性**：切片函数通过捕捉可能难以使用传统方法进行标签化的数据切片来补充其他标签技术。它们提供了额外的弱监督来源，从而增强了整体标签过程。'
- en: '**Scalability and efficiency**: Slicing functions can be automated and applied
    programmatically, allowing for scalable and efficient labeling of large datasets.
    This reduces the dependency on manual annotation and enables the labeling of data
    at a larger scale.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和效率**：切片函数可以自动化并程序化应用，允许对大型数据集进行可扩展和高效的标签化。这减少了对手动标注的依赖，并允许在大规模上对数据进行标签化。'
- en: 'Let’s understand how we can implement slicing functions in Python using the
    loan prediction dataset. We first import the required libraries and load the dataset
    into a pandas DataFrame. We will use the same preprocessing step discussed in
    the previous sections:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解如何使用贷款预测数据集在Python中实现切片函数。我们首先导入所需的库并将数据集加载到pandas DataFrame中。我们将使用之前章节中讨论的相同预处理步骤：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To create slicing functions, we utilize the `@labeling_function` decorator
    provided by Snorkel. These functions encapsulate the labeling logic based on specific
    conditions or rules. For example, we can define slicing functions based on the
    `ApplicantIncome`, `LoanAmount`, or `Self_Employed` features:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建切片函数，我们利用Snorkel提供的`@labeling_function`装饰器。这些函数封装了基于特定条件或规则的标签逻辑。例如，我们可以根据`ApplicantIncome`、`LoanAmount`或`Self_Employed`特征定义切片函数：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To apply the slicing functions to the training data, we use the `PandasLFApplier`
    class provided by Snorkel. This class takes the slicing functions as input and
    applies them to the training dataset, generating weak labels. The resulting weak
    labels will be used to train the label model later:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要将切片函数应用于训练数据，我们使用Snorkel提供的`PandasLFApplier`类。这个类接受切片函数作为输入，并将它们应用于训练数据集，生成弱标签。生成的弱标签将用于稍后训练标签模型：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once we have the weak labels from the slicing functions, we can train a label
    model using the `LabelModel` class from Snorkel. The label model learns the correlation
    between the weak labels and the true labels and estimates the posterior probabilities
    for each data instance. In this step, we create a `LabelModel` object, specify
    the cardinality of the labels (e.g., binary classification), and fit it to the
    weakly labeled training data:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从切片函数中获得弱标签，我们就可以使用Snorkel的`LabelModel`类来训练标签模型。标签模型学习弱标签与真实标签之间的相关性，并估计每个数据实例的后验概率。在这个步骤中，我们创建一个`LabelModel`对象，指定标签的基数（例如，二元分类），并将其拟合到弱标签的训练数据：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After training the label model, we want to evaluate its performance on the
    test data. We use the `PandasLFApplier` to apply the slicing functions to the
    test dataset, obtaining the weak labels. Then, we calculate the accuracy of the
    label model’s predictions compared to the true labels of the test set:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练标签模型后，我们希望评估其在测试数据上的性能。我们使用`PandasLFApplier`将切片函数应用于测试数据集，获取弱标签。然后，我们计算标签模型预测的准确率，与测试集的真实标签进行比较：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Snorkel provides the `LFAnalysis` module, which allows us to analyze the performance
    and characteristics of the labeling functions. We can compute various metrics
    such as coverage, conflicts, and accuracy for each labeling function to gain insights
    into their effectiveness and potential issues:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 提供了 `LFAnalysis` 模块，该模块允许我们分析标记函数的性能和特征。我们可以计算每个标记函数的覆盖率、冲突和准确度等指标，以深入了解其有效性和潜在问题：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will generate the following summary table:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下汇总表：
- en: '![Figure 6.6 – Summary table showing statistics for each labeling function
    (LF)](img/B19297_06_6.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 显示每个标记函数（LF）统计信息的汇总表](img/B19297_06_6.jpg)'
- en: Figure 6.6 – Summary table showing statistics for each labeling function (LF)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 显示每个标记函数（LF）统计信息的汇总表
- en: In the next section, we’ll explore active learning for labeling—a clever strategy
    that involves picking the right data to label, making our model smarter with each
    iteration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨用于标记的主动学习——一种涉及选择正确数据进行标记的巧妙策略，通过每次迭代使我们的模型更智能。
- en: Active learning
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主动学习
- en: In this section, we will explore the concept of active learning and its application
    in data labeling. Active learning is a powerful technique that allows us to label
    data more efficiently by actively selecting the most informative samples for annotation.
    By strategically choosing which samples to label, we can achieve higher accuracy
    with a smaller dataset, all else being equal. On the following pages, we will
    discuss various active learning strategies and implement them using Python code
    examples.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨主动学习的概念及其在数据标记中的应用。主动学习是一种强大的技术，它允许我们通过积极选择最具信息量的样本进行标注来更有效地标记数据。通过战略性地选择哪些样本进行标注，我们可以在数据集较小的情况下实现更高的准确度，其他条件保持不变。在接下来的几页中，我们将讨论各种主动学习策略，并使用
    Python 代码示例进行实现。
- en: Active learning is a semi-supervised learning approach that involves iteratively
    selecting a subset of data points for manual annotation based on their informativeness.
    The key idea is to actively query the labels of the most uncertain or informative
    instances to improve the learning process. This iterative process of selecting
    and labeling samples can significantly reduce the amount of labeled data required
    to achieve the desired level of performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是一种半监督学习方法，它涉及根据数据点的信息量迭代选择数据子集进行手动标注。关键思想是积极查询最不确定或最具信息量的实例的标签，以改进学习过程。通过选择和标注样本的这种迭代过程，可以显著减少实现所需性能水平所需标记数据的数量。
- en: Let’s start with a simple example of active learning to help you get the basic
    idea before going into detail on specific active learning strategies.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主动学习的一个简单例子开始，帮助你了解基本概念，然后再详细讨论具体的主动学习策略。
- en: Suppose we are building a machine learning model to classify emails into spam
    and not spam. We have a large dataset of unlabeled emails, but manually labeling
    all of them would be very time-consuming.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在构建一个机器学习模型，用于将电子邮件分类为垃圾邮件和非垃圾邮件。我们有一个大量未标记的电子邮件数据集，但手动标记所有这些邮件将非常耗时。
- en: 'Here is where active learning comes in:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是主动学习发挥作用的地方：
- en: '**Initial training**: We start by randomly selecting a small subset of emails
    and manually labeling them as spam or not spam. We then train our model on this
    small labeled dataset.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始训练**：我们首先随机选择一小部分电子邮件，并将它们手动标记为垃圾邮件或非垃圾邮件。然后，我们在这个小型标记数据集上训练我们的模型。'
- en: '**Uncertainty sampling**: After training, we use the model to make predictions
    on the rest of the unlabeled emails. However, instead of labeling all the emails,
    we choose the ones where the model is most uncertain about its predictions. For
    example, if our model outputs a probability close to 0.5 (i.e., it’s unsure whether
    the email is spam or not), these emails are considered ‘informative’ or ‘uncertain’.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不确定性采样**：训练完成后，我们使用模型对剩余的未标记电子邮件进行预测。然而，我们不会标记所有电子邮件，而是选择模型对其预测最不确定的电子邮件。例如，如果我们的模型输出一个接近
    0.5 的概率（即它不确定电子邮件是否为垃圾邮件），这些电子邮件被认为是“信息丰富”或“不确定”的。'
- en: '**Label query**: We then manually label these uncertain emails, adding them
    to our training set.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标签查询**：然后，我们手动标记这些不确定的电子邮件，并将它们添加到我们的训练集中。'
- en: '**Iterative learning**: *Step 2* and *step 3* are repeated in several iterations—
    retraining the model with the newly labeled data, using it to predict labels for
    the remaining unlabeled data, and choosing the most uncertain instances to label
    next.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代学习**：*步骤2*和*步骤3*在多次迭代中重复——用新标记的数据重新训练模型，使用它来预测剩余未标记数据的标签，并选择下一个最不确定的实例进行标记。'
- en: This way, active learning allows us to strategically select the most informative
    examples to label, thereby potentially improving the model’s performance with
    fewer labeled instances.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，主动学习使我们能够有策略地选择最有信息量的示例进行标记，从而有可能在更少的标记实例的情况下提高模型的表现。
- en: There are several active learning strategies that can be employed based on different
    criteria for selecting informative samples. Let’s discuss a few commonly used
    strategies and their Python implementations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 根据选择信息样本的不同标准，可以采用几种主动学习策略。让我们讨论一些常用的策略及其Python实现。
- en: Uncertainty sampling
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性采样
- en: Uncertainty sampling is based on the assumption that instances on which a model
    is uncertain are more informative and beneficial to label. The idea is to select
    instances that are close to the decision boundary or have conflicting predictions.
    By actively acquiring labels for these challenging instances, the model can refine
    its understanding of the data and improve its performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样基于这样的假设：模型不确定的实例更有信息量，对标记有益。想法是选择接近决策边界或具有冲突预测的实例。通过积极获取这些具有挑战性的实例的标签，模型可以细化其对数据的理解并提高其性能。
- en: 'There are several common approaches to uncertainty sampling in active learning:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习中，不确定性采样有几种常见的方法：
- en: '**Least confidence**: This method selects instances for which the model has
    the lowest confidence in its predictions. It focuses on instances where the predicted
    class probability is closest to 0.5, indicating uncertainty. For instance, in
    our email example, if the model predicts a 0.52 probability of a particular email
    being spam and a 0.48 probability of it not being spam, this indicates that the
    model is uncertain about its prediction. This email would be a prime candidate
    for labeling under the least confidence method.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最低信心度**：此方法选择模型对其预测最没有信心的实例。它关注预测类别概率最接近0.5的实例，这表明存在不确定性。例如，在我们的电子邮件示例中，如果模型预测某个电子邮件是垃圾邮件的概率为0.52，不是垃圾邮件的概率为0.48，这表明模型对其预测不确定。这封电子邮件将是最低信心度方法下标记的理想候选。'
- en: '**Margin sampling**: Margin sampling aims to find instances where the model’s
    top two predicted class probabilities are close. It selects instances with the
    smallest difference between the highest and second-highest probabilities, as these
    are likely to be near the decision boundary. Let’s say we have a model that classifies
    images of animals. If it predicts an image with probabilities of 0.4 for cat,
    0.38 for dog, and 0.22 for bird, the small difference (0.02) between the top two
    probabilities suggests uncertainty. This image would be chosen for labeling in
    margin sampling.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘采样**：边缘采样旨在找到模型的前两个预测类别概率接近的实例。它选择最高和第二高概率之间差异最小的实例，因为这些实例可能接近决策边界。假设我们有一个对动物图像进行分类的模型。如果它预测一个图像的概率为猫0.4，狗0.38，鸟0.22，最高和第二高概率之间的小差异（0.02）表明存在不确定性。这个图像将在边缘采样中被选中进行标记。'
- en: '**Entropy**: Entropy-based sampling considers the entropy of the predicted
    class probabilities. It selects instances with high entropy, indicating a high
    level of uncertainty in the model’s predictions. Using the same animal classification
    model, if an image gets equal 0.33 probabilities for each class (cat, dog, bird),
    it shows high uncertainty. This image would be selected for labeling by the entropy
    method.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熵**：基于熵的采样考虑预测类别概率的熵。它选择具有高熵的实例，这表明模型预测的不确定性很高。使用相同的动物分类模型，如果一个图像每个类别（猫、狗、鸟）的概率都为0.33，这表明存在高度不确定性。这个图像将通过熵方法被选中进行标记。'
- en: 'Let’s implement uncertainty sampling in Python. For this example, we go back
    to the `credit-g` dataset introduced at the beginning of this chapter. Let’s have
    a look at the features in this dataset to refresh your memory:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中实现不确定性采样。为此示例，我们回到本章开头介绍的`credit-g`数据集。让我们看看这个数据集中的特征，以刷新你的记忆：
- en: '![Figure 6.7 – The features of the credit-g dataset](img/B19297_06_7.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – credit-g数据集的特征](img/B19297_06_7.jpg)'
- en: Figure 6.7 – The features of the credit-g dataset
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – credit-g数据集的特征
- en: 'Under the assumption that the dataset has already been loaded into a `df` DataFrame,
    we start by preprocessing the dataset by standardizing the numerical features
    and one-hot encoding categorical features:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设数据集已经加载到 `df` DataFrame 的情况下，我们首先通过标准化数值特征和独热编码分类特征来预处理数据集：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With our new `df_preprocessed` DataFrame in hand, we can perform uncertainty
    sampling. We start by importing the necessary libraries and modules, including
    pandas, NumPy and scikit-learn, for data manipulation and machine learning operations:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手头有了新的 `df_preprocessed` DataFrame，我们可以执行不确定性采样。我们首先导入必要的库和模块，包括 pandas、NumPy
    和 scikit-learn，用于数据处理和机器学习操作：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We split the `df_preprocessed` dataset into a small labeled dataset and the
    remaining unlabeled data. In this example, we randomly select a small portion
    of 10% as labeled data and leave the rest as unlabeled data:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `df_preprocessed` 数据集分为一个小型标记数据集和剩余的未标记数据。在这个例子中，我们随机选择10%的小部分作为标记数据，其余的作为未标记数据：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We define the uncertainty sampling functions—`least_confidence`, `margin_sampling`,
    and `entropy_sampling`—as discussed earlier.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了不确定性采样函数——`least_confidence`、`margin_sampling` 和 `entropy_sampling`，如前所述。
- en: 'Here is an explanation of each of these functions:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是每个这些函数的解释：
- en: '`least_confidence`: This function takes in a 2D array of probabilities with
    each row representing an instance and each column representing a class. For each
    instance, it calculates the confidence as 1 – `max_probability`, where `max_probability`
    is the largest predicted probability across all classes. It then sorts the instances
    by confidence in ascending order. The idea is that instances with lower confidence
    (i.e., higher uncertainty) are more informative and should be labeled first:'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`least_confidence`：此函数接受一个概率的二维数组，其中每一行代表一个实例，每一列代表一个类别。对于每个实例，它计算置信度为 1 –
    `max_probability`，其中 `max_probability` 是所有类别中最大的预测概率。然后按置信度的升序对实例进行排序。其思路是，具有较低置信度（即更高不确定性）的实例更有信息量，应该首先进行标记：'
- en: '[PRE32]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`margin_sampling`: This function also takes in a 2D array of probabilities.
    For each instance, it calculates the margin as the difference between the highest
    and second-highest predicted probabilities. It then sorts the instances by margin
    in ascending order. The idea is that instances with smaller margins (i.e., closer
    top-two class probabilities) are more informative and should be labeled first:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`margin_sampling`：此函数也接受一个概率的二维数组。对于每个实例，它计算边缘作为最高和第二高预测概率之间的差异。然后按边缘的升序对实例进行排序。其思路是，具有较小边缘（即接近前两个类别的概率）的实例更有信息量，应该首先进行标记：'
- en: '[PRE33]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`entropy_sampling`: This function calculates the entropy of the predicted probabilities
    for each instance. Entropy is a measure of uncertainty or disorder, with higher
    values indicating greater uncertainty. It then sorts the instances by entropy
    in ascending order. The idea is that instances with higher entropy (i.e., more
    uncertainty in the class probabilities) are more informative and should be labeled
    first:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entropy_sampling`：此函数计算每个实例预测概率的熵。熵是衡量不确定性或无序的指标，值越高表示不确定性越大。然后按熵的升序对实例进行排序。其思路是，具有更高熵（即类别概率中的更大不确定性）的实例更有信息量，应该首先进行标记：'
- en: '[PRE34]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We enter the active learning loop, where we iteratively train a model, select
    instances for labeling using uncertainty sampling, obtain labels for those instances,
    and update the labeled dataset.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入主动学习循环，在这个循环中，我们迭代地训练模型，使用不确定性采样选择标记实例，为这些实例获取标签，并更新标记数据集。
- en: Firstly, a list named `accuracies` is used to keep track of the accuracy of
    the model on the labeled data at each iteration.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用名为 `accuracies` 的列表来跟踪模型在每个迭代中对标记数据的准确率。
- en: 'The active learning loop is then implemented over a specified number of iterations.
    In each iteration, a logistic regression model is trained on the labeled data,
    and its accuracy is calculated and stored. The model then makes predictions on
    the unlabeled data, and the instances about which it is least confident (as determined
    by the `least_confidence` function) are added to the labeled dataset. These instances
    are removed from the unlabeled dataset:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在指定的迭代次数上实现主动学习循环。在每个迭代中，使用标记数据训练一个逻辑回归模型，并计算其准确率并存储。然后模型对未标记数据进行预测，并将它最不自信的实例（由
    `least_confidence` 函数确定）添加到标记数据集中。这些实例从未标记数据集中移除：
- en: '[PRE35]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The final output is an updated labeled dataset containing additional instances
    labeled during each iteration of active learning. The process aims to improve
    model performance by iteratively selecting and labeling the most informative instances
    from the unlabeled data. In the preceding code, the success and effectiveness
    of active learning depend on the `least_confidence` custom function and the characteristics
    of the dataset. The `least_confidence` function is assumed to return indices corresponding
    to the least confident predictions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是一个包含在主动学习每次迭代中标记的额外实例的更新标记数据集。该过程旨在通过迭代选择和标记未标记数据中最具信息量的实例来提高模型性能。在前面的代码中，主动学习的成功和有效性取决于`least_confidence`自定义函数和数据集的特征。假设`least_confidence`函数返回对应于最不自信预测的索引。
- en: Note that this code uses the `least_confidence` function to perform active learning.
    To perform the same process using `margin_sampling` or `entropy_sampling` instead
    of `least_confidence`, you could replace `least_confidence(probabilities)[:batch_size]`
    with `margin_sampling(probabilities)[:batch_size]` or `entropy_sampling(probabilities)[:batch_size]`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此代码使用`least_confidence`函数进行主动学习。要使用`margin_sampling`或`entropy_sampling`而不是`least_confidence`执行相同的过程，可以将`least_confidence(probabilities)[:batch_size]`替换为`margin_sampling(probabilities)[:batch_size]`或`entropy_sampling(probabilities)[:batch_size]`。
- en: 'Let’s compare the performance of each of the three active learning functions
    on the same sample from `credit-g`. We use matplotlib to produce visual representations
    of the accuracy for each of the three active learning functions. To replicate
    the output, apply the following code to the outputs of each function:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较三个主动学习函数在`credit-g`相同样本上的性能。我们使用matplotlib来生成三个主动学习函数准确率的视觉表示。要复制输出，将以下代码应用于每个函数的输出：
- en: '[PRE36]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The *least confidence* method achieved a model accuracy of 0.878 after five
    iterations, with the best performance observed after the first iteration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*最不自信*方法在五次迭代后达到了0.878的模型准确率，最佳性能出现在第一次迭代后：'
- en: '![Figure 6.8 – Accuracy of the least_confidence active learning function over
    five iterations when predicting the target variable on the credit-g dataset](img/B19297_06_8.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 在信用-g数据集上预测目标变量时，最不自信主动学习函数在五次迭代中的准确率](img/B19297_06_8.jpg)'
- en: Figure 6.8 – Accuracy of the least_confidence active learning function over
    five iterations when predicting the target variable on the credit-g dataset
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 在信用-g数据集上预测目标变量时，最不自信主动学习函数在五次迭代中的准确率
- en: 'Margin sampling achieved a slightly higher accuracy of 0.9 after two and three
    iterations:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘采样在经过两次和三次迭代后达到了略高的准确率，为0.9：
- en: '![Figure 6.9 – Accuracy of the margin_sampling active learning function over
    five iterations when predicting the target variable on the credit-g dataset](img/B19297_06_9.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 在信用-g数据集上预测目标变量时，边缘采样主动学习函数在五次迭代中的准确率](img/B19297_06_9.jpg)'
- en: Figure 6.9 – Accuracy of the margin_sampling active learning function over five
    iterations when predicting the target variable on the credit-g dataset
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 在信用-g数据集上预测目标变量时，边缘采样主动学习函数在五次迭代中的准确率
- en: Lastly, entropy sampling and least confidence achieved identical results. How
    come?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，熵采样和最不自信采样得到了相同的结果。这是为什么？
- en: '![Figure 6.10 – Accuracy of the entropy_sampling active learning function over
    five iterations when predicting the target variable on the credit-g dataset](img/B19297_06_10.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 在信用-g数据集上预测目标变量时，熵采样主动学习函数在五次迭代中的准确率](img/B19297_06_10.jpg)'
- en: Figure 6.10 – Accuracy of the entropy_sampling active learning function over
    five iterations when predicting the target variable on the credit-g dataset
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 在信用-g数据集上预测目标变量时，熵采样主动学习函数在五次迭代中的准确率
- en: These two methods may yield the same results in certain scenarios, especially
    when working with binary classification problems. Here’s why.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法在某些场景下可能会得到相同的结果，尤其是在处理二元分类问题时。原因如下。
- en: The least confidence method considers the class with the highest predicted probability.
    If the model is very confident about a particular class (i.e., the probability
    is close to 1), then it’s less likely that this instance will be selected for
    labeling.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最不自信方法考虑预测概率最高的类别。如果模型对某个特定类别非常自信（即，概率接近1），那么这个实例被选为标记的可能性就较小。
- en: Entropy sampling considers the entropy or “disorder” of the predicted probabilities.
    For binary classification problems, entropy is maximized when the probabilities
    are both equal (i.e., the model is completely unsure which class to predict).
    This could coincide with low-confidence predictions.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 熵采样考虑预测概率的熵或“无序”。对于二元分类问题，当概率都相等时（即模型完全不确定要预测哪个类别），熵达到最大。这可能与低置信度预测相一致。
- en: As a result, both methods will often select the same instances for labeling
    in the context of binary classification. However, this might not always be the
    case, especially for multi-class problems.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在二元分类的背景下，这两种方法通常会选择相同的实例进行标签。然而，这并不总是如此，特别是对于多类问题。
- en: On the other hand, margin sampling focuses on the difference between the highest
    and second-highest predicted probabilities. Even slight differences in these probabilities
    can lead to different instances being selected compared to the other methods.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，边缘采样关注最高和第二高预测概率之间的差异。这些概率之间的微小差异可能导致与其他方法相比选择不同的实例。
- en: In the next section, we’ll explore **Query by Committee** (**QBC**) for labeling—a
    method that brings together a group of models to help decide which data points
    are most important for labeling.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨**委员会查询**（**QBC**）用于标签——一种将一组模型汇集起来以帮助决定哪些数据点对标签最重要的方法。
- en: Query by Committee (QBC)
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 委员会查询（QBC）
- en: QBC is based on the idea that instances for which the committee of models disagrees
    or exhibits high uncertainty are the most informative and should be prioritized
    for labeling. Instead of relying on a single model’s prediction, QBC takes advantage
    of the diversity and collective decision-making of the committee to make informed
    labeling decisions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: QBC基于这样一个观点：模型委员会不一致或表现出高度不确定性的实例最有信息量，应该优先进行标签。QBC不是依赖于单个模型的预测，而是利用委员会的多样性和集体决策来做出明智的标签决策。
- en: 'The QBC process typically involves the following steps:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: QBC过程通常包括以下步骤：
- en: '**Committee creation**: Create an initial committee of multiple models trained
    on the available labeled data. Models in a committee can be diverse in terms of
    their architectures, initializations, or training methodologies.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**委员会创建**：创建一个由多个模型组成的初始委员会，这些模型在可用的标记数据上进行了训练。委员会中的模型在架构、初始化或训练方法方面可以多样化。'
- en: '**Instance selection**: Apply the committee of models to the unlabeled instances
    and obtain predictions. Choose the instances that elicit the most disagreement
    or uncertainty among the committee members for labeling.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实例选择**：将模型委员会应用于未标记的实例并获取预测。选择在委员会成员中引起最多不一致或不确定性的实例进行标签。'
- en: '**Committee update**: Label the selected instances and add them to the labeled
    dataset. Re-train or update the committee of models using the expanded labeled
    dataset.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**委员会更新**：对选定的实例进行标签并添加到标记数据集中。使用扩展的标记数据集重新训练或更新模型委员会。'
- en: '**Repeat**: Iterate the process by returning to *step 2* until a desired performance
    level is achieved or labeling resources are exhausted.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复**：通过返回到*步骤2*来迭代过程，直到达到期望的性能水平或标签资源耗尽。'
- en: 'QBC offers several advantages in active learning for labeling:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: QBC在主动学习标签方面提供了几个优势：
- en: '**Model diversity**: QBC utilizes a committee of models, allowing for diverse
    perspectives and capturing different aspects of the data distribution. This diversity
    helps identify instances that are challenging or ambiguous, leading to improved
    labeling decisions.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型多样性**：QBC利用一个模型委员会，允许有不同观点并捕捉数据分布的不同方面。这种多样性有助于识别具有挑战性或模糊的实例，从而提高标签决策。'
- en: '**Model confidence estimation**: By observing the disagreement or uncertainty
    among the committee members, QBC provides an estimate of the models’ confidence
    in their predictions. Instances that lead to disagreement or uncertainty can be
    considered more informative and valuable for labeling.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型置信度估计**：通过观察委员会成员之间的不一致或不确定性，QBC提供了模型对其预测置信度的估计。导致不一致或不确定性的实例可以被认为是更有信息和价值的标签。'
- en: '**Labeling efficiency**: QBC aims to prioritize instances that have the greatest
    impact on the committee’s decision. This approach can save labeling efforts by
    focusing on instances that provide the most relevant information to improve the
    model’s performance.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签效率**：QBC旨在优先考虑对委员会决策影响最大的实例。这种方法可以通过关注提供最多相关信息以改进模型性能的实例来节省标签工作。'
- en: Let’s implement this approach using the `credit-g` dataset in Python. First,
    we define functions for creating the committee, obtaining committee predictions,
    and measuring disagreement or uncertainty among the committee members.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python 中的 `credit-g` 数据集来实现这种方法。首先，我们定义创建委员会、获取委员会预测和测量委员会成员之间分歧或不确定性的函数。
- en: The `create_committee(num_models)` function creates a committee of logistic
    regression models. The number of models in the committee is specified by `num_models`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_committee(num_models)` 函数创建一个逻辑回归模型的委员会。委员会中的模型数量由 `num_models` 指定。'
- en: The `get_committee_predictions(committee, data)` function gets predictions from
    each model in the committee for the provided data. It returns an array of prediction
    probabilities.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_committee_predictions(committee, data)` 函数从委员会中的每个模型获取提供数据的预测。它返回一个预测概率数组。'
- en: 'The `measure_disagreement(predictions)` function measures the disagreement
    among the committee’s predictions. It calculates the variance of the predictions
    and returns the mean disagreement:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`measure_disagreement(predictions)` 函数测量委员会预测之间的分歧。它计算预测的方差并返回平均分歧：'
- en: '[PRE37]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We enter the active learning loop, where we iteratively train the committee,
    measure disagreement or uncertainty, select instances for labeling, obtain labels
    for those instances, and update the labeled dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入主动学习循环，其中迭代训练委员会，测量分歧或不确定性，选择标记实例，为这些实例获取标签，并更新标记数据集：
- en: '[PRE38]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here’s an explanation of the code. `labeled_dataset = labeled_data.copy()` creates
    a copy of the initial labeled dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对代码的解释。`labeled_dataset = labeled_data.copy()` 创建了初始标记数据集的副本。
- en: 'The loop over `num_iterations` represents the number of rounds of semi-supervised
    learning. In each round, the following steps occur:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_iterations` 循环表示半监督学习的轮数。在每一轮中，发生以下步骤：'
- en: Each model in the committee is trained on the current labeled dataset.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 委员会中的每个模型都是在当前标记的数据集上训练的。
- en: The committee makes predictions on the unlabeled data.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 委员会对未标记的数据进行预测。
- en: The disagreement among the committee’s predictions is calculated.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算委员会预测之间的分歧。
- en: The indices of the instances with the highest disagreement are identified. The
    size of this batch is specified by `batch_size`.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定分歧最大的实例的索引。这个批次的规模由 `batch_size` 指定。
- en: These instances are added to the labeled dataset.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些实例被添加到标记数据集中。
- en: Finally, these instances are removed from the unlabeled data.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这些实例将从未标记的数据中移除。
- en: The idea behind this approach is that the instances the models disagree about
    the most are the ones where the models are most uncertain. By adding these instances
    to the labeled dataset, the models can learn more from them in the next round
    of training. This process continues for a specified number of iterations.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的思想是，模型分歧最大的实例是模型最不确定的实例。通过将这些实例添加到标记数据集中，模型可以在下一轮训练中从它们中学到更多。这个过程会持续进行指定次数的迭代。
- en: Now let’s discuss diversity sampling in labeling—a smart technique that focuses
    on selecting a varied set of data points to ensure a well-rounded and representative
    labeled dataset.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论在标记过程中的多样性采样——一种专注于选择多样化的数据点集，以确保标记数据集全面且具有代表性的智能技术。
- en: Diversity sampling
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性采样
- en: Diversity sampling is based on the principle that selecting instances that cover
    diverse patterns or regions in the dataset can provide a more comprehensive understanding
    of the underlying data distribution. By actively seeking diverse instances for
    labeling, diversity sampling aims to improve model generalization and robustness.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性采样基于这样一个原则：选择覆盖数据集中不同模式或区域的实例可以提供对潜在数据分布的更全面的理解。通过积极寻求多样化的实例进行标记，多样性采样旨在提高模型泛化能力和鲁棒性。
- en: 'The diversity sampling process typically involves the following steps:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性采样过程通常涉及以下步骤：
- en: '**Initial model training**: Train an initial machine learning model using a
    small, labeled dataset. This model will be used to guide the selection of diverse
    instances for labeling.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始模型训练**：使用一个小型标记数据集训练一个初始机器学习模型。此模型将用于指导选择多样化的实例进行标记。'
- en: '**Instance selection**: Apply the trained model to the unlabeled instances
    and obtain predictions. Calculate a diversity metric to measure the dissimilarity
    or coverage of each instance with respect to the already labeled instances. Select
    instances with the highest diversity metric for labeling.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实例选择**：将训练好的模型应用于未标记的实例并获取预测。计算多样性度量来衡量每个实例与已标记实例之间的相似度或覆盖率。选择具有最高多样性度量的实例进行标记。'
- en: '**Labeling and model update**: Label the selected instances and add them to
    the labeled dataset. Retrain or update the machine learning model using the expanded
    labeled dataset.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记和模型更新**：标记所选实例并将它们添加到标记数据集中。使用扩展的标记数据集重新训练或更新机器学习模型。'
- en: '**Repeat**: Iterate the process by returning to *step 2* until a desired performance
    level is achieved or labeling resources are exhausted.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复**：通过返回到*步骤2*来迭代这个过程，直到达到所需的性能水平或标记资源耗尽。'
- en: 'Diversity sampling offers several advantages in active learning for labeling:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习标记中，多样性采样提供了几个优势：
- en: '**Comprehensive data coverage**: By selecting diverse instances, diversity
    sampling ensures that the labeled dataset covers a wide range of patterns or regions
    in the data. This approach helps the model generalize better to unseen instances
    and improves its ability to handle different scenarios.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全面数据覆盖**：通过选择具有多样性的实例，多样性采样确保标记数据集覆盖数据中的广泛模式或区域。这种方法有助于模型更好地泛化到未见过的实例，并提高其处理不同场景的能力。'
- en: '**Exploration of data distribution**: Diversity sampling encourages the exploration
    of the underlying data distribution by actively seeking instances from different
    parts of the feature space. This exploration can reveal important insights about
    the data and improve the model’s understanding of complex relationships.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索数据分布**：多样性采样通过积极从特征空间的不同部分寻找实例来鼓励探索底层数据分布。这种探索可以揭示关于数据的重要见解，并提高模型对复杂关系的理解。'
- en: '**Mitigation of bias and overfitting**: Diversity sampling can help mitigate
    biases and overfitting that may arise from selecting only easy or similar instances
    for labeling. By diversifying the labeled dataset, diversity sampling reduces
    the risk of model overconfidence and enhances its robustness.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓解偏差和过拟合**：多样性采样可以帮助缓解由于仅选择易于或相似的实例进行标记而可能出现的偏差和过拟合。通过多样化标记数据集，多样性采样降低了模型过度自信的风险，并增强了其鲁棒性。'
- en: Let’s explore this approach on our preprocessed `credit-g` dataset, using pairwise
    distances from the `sklearn` library in Python. The `pairwise_distances` function
    from `sklearn` calculates the distance between each pair of instances in a dataset.
    In the context of diversity sampling, this function is used to find instances
    that are most different from each other.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在预处理后的`credit-g`数据集上探索这种方法，使用Python中的`sklearn`库中的成对距离。`sklearn`库中的`pairwise_distances`函数计算数据集中每对实例之间的距离。在多样性采样的上下文中，此函数用于找到彼此差异最大的实例。
- en: 'Here’s the process:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这个过程：
- en: Compute the pairwise distances between all pairs of instances in the unlabeled
    dataset.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算未标记数据集中所有实例对之间的成对距离。
- en: Identify the instances that have the greatest distances between them. These
    are the most diverse instances according to the distance metric used.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别彼此之间距离最大的实例。这些是根据距离度量指标确定的最具多样性的实例。
- en: Select these diverse instances for labeling and add them to the labeled dataset.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择这些具有多样性的实例进行标记并将它们添加到标记数据集中。
- en: The idea is that by actively seeking out diverse instances (those that are farthest
    apart in terms of the chosen distance metric), you can cover a wider range of
    patterns in the underlying data distribution. This helps to improve the model’s
    ability to generalize to new data and enhances its robustness.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是通过积极寻找具有最高多样性的实例（在所选距离度量下最远的实例），可以覆盖底层数据分布中的更广泛模式。这有助于提高模型对新数据的泛化能力，并增强其鲁棒性。
- en: 'First, we import the `pairwise_distances` function:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入`pairwise_distances`函数：
- en: '[PRE39]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We define functions for calculating diversity and selecting instances with
    the highest diversity for labeling. We will use pairwise Euclidean distance as
    the diversity metric:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了用于计算多样性和选择具有最高多样性的实例进行标记的函数。我们将使用成对欧几里得距离作为多样性度量指标：
- en: '[PRE40]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We enter the active learning loop, where we iteratively calculate diversity,
    select diverse instances for labeling, obtain labels for those instances, and
    update the labeled dataset:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入活跃学习循环，其中我们迭代地计算多样性，选择用于标注的多样性实例，为这些实例获取标签，并更新标注数据集：
- en: '[PRE41]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the next section, we’ll explore transfer learning in labeling—an advanced
    method that leverages knowledge gained from one task to improve performance on
    a different but related task.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨迁移学习在标注中的应用——这是一种利用一个任务获得的知识来提高另一个不同但相关任务性能的高级方法。
- en: Transfer learning
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'Transfer learning involves using knowledge gained from a source task or domain
    to aid learning. Instead of starting from scratch, transfer learning leverages
    pre-existing information, such as labeled data or pre-trained models, to bootstrap
    the learning process and improve the performance of the target task. Transfer
    learning offers several advantages in the labeling process of machine learning:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习涉及使用从源任务或领域获得的知识来辅助学习。而不是从头开始，迁移学习利用现有的信息，如标注数据或预训练模型，来启动学习过程并提高目标任务的表现。迁移学习在机器学习的标注过程中提供了几个优势：
- en: '**Reduced labeling effort**: By leveraging pre-existing labeled data, transfer
    learning reduces the need for the manual labeling of a large amount of data for
    the target task. It enables the reuse of knowledge from related tasks, domains,
    or datasets, saving time and effort in acquiring new labels.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少标注工作量**：通过利用现有的标注数据，迁移学习减少了为目标任务手动标注大量数据的需求。它允许重用相关任务、领域或数据集的知识，从而在获取新标签时节省时间和精力。'
- en: '**Improved model performance**: Transfer learning allows the target model to
    benefit from the knowledge learned by a source model. The source model might have
    been trained on a large, labeled dataset or a different but related task, providing
    valuable insights and patterns that can enhance the target model’s performance.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高模型性能**：迁移学习允许目标模型从源模型学习到的知识中受益。源模型可能是在大型标注数据集或不同但相关的任务上训练的，提供了有价值的见解和模式，可以增强目标模型的表现。'
- en: '**Adaptability to limited labeled data**: Transfer learning is particularly
    useful when the target task has limited labeled data. By leveraging labeled data
    from a source task or domain, transfer learning can help generalize the target
    model better and mitigate the risk of overfitting on a small, labeled dataset.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应有限标注数据**：当目标任务有限标注数据时，迁移学习特别有用。通过利用源任务或领域的标注数据，迁移学习可以帮助更好地泛化目标模型，并减轻在小型标注数据集上过拟合的风险。'
- en: 'Transfer learning can be applied in various ways for labeling in machine learning:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习可以以多种方式应用于机器学习的标注：
- en: '**Feature extraction**: Utilize pre-trained models as feature extractors. Extract
    high-level features from pre-trained models and feed them as inputs to a new model
    that is trained on the target labeled dataset.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：利用预训练模型作为特征提取器。从预训练模型中提取高级特征，并将它们作为输入提供给在目标标注数据集上训练的新模型。'
- en: '**Fine-tuning pre-trained models**: Use pre-trained models that have been trained
    on large, labeled datasets, such as models from popular deep learning architectures
    such as VGG, ResNet, or BERT. Fine-tune these pre-trained models on a smaller
    labeled dataset specific to the target task.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调预训练模型**：使用在大型标注数据集上训练的预训练模型，如VGG、ResNet或BERT等流行的深度学习架构中的模型。在针对目标任务的较小标注数据集上对这些预训练模型进行微调。'
- en: Let’s discuss these in more detail.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些内容。
- en: Feature extraction
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: Feature extraction involves using the representations learned by a pre-trained
    model as input features for a new model. This approach is particularly useful
    when the pre-trained model has been trained on a large, general-purpose dataset
    such as ImageNet. Here’s an example of using transfer learning for image labeling
    using the VGG16 model.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取涉及使用预训练模型学习到的表示作为新模型的输入特征。这种方法特别适用于预训练模型已经在大型通用数据集（如ImageNet）上训练的情况。以下是一个使用VGG16模型进行图像标注的迁移学习示例。
- en: Note
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The data used in this example is available from [https://github.com/odegeasslbc/FastGAN-pytorch](https://github.com/odegeasslbc/FastGAN-pytorch).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中使用的数据可以从[https://github.com/odegeasslbc/FastGAN-pytorch](https://github.com/odegeasslbc/FastGAN-pytorch)获取。
- en: 'We first import the necessary libraries:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库：
- en: '[PRE42]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will use an image of a Golden Retriever for labeling. We can view this image
    using the `PIL` library:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用金毛猎犬的图片进行标注。我们可以使用`PIL`库查看这张图片：
- en: '[PRE43]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will display the following image:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下图像：
- en: '![Figure 6.11 – The Golden Retriever: man’s best friend and our sample image](img/B19297_06_11.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 金毛寻回犬：人类的最佳伙伴以及我们的样本图像](img/B19297_06_11.jpg)'
- en: 'Figure 6.11 – The Golden Retriever: man’s best friend and our sample image'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 金毛寻回犬：人类的最佳伙伴以及我们的样本图像
- en: 'Let’s load the pre-trained VGG16 model from the TensorFlow library and predict
    the label for this sample image:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载TensorFlow库中的预训练VGG16模型，并预测这个样本图像的标签：
- en: '[PRE44]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will generate the following output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 6.12 – The VGG16 model’s prediction with confidence levels; the model
    has labeled the image as golden_retriever with a high level of confidence](img/B19297_06_12.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – VGG16模型的预测结果及置信度；模型以高置信度将图像标记为golden_retriever](img/B19297_06_12.jpg)'
- en: Figure 6.12 – The VGG16 model’s prediction with confidence levels; the model
    has labeled the image as golden_retriever with a high level of confidence
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – VGG16模型的预测结果及置信度；模型以高置信度将图像标记为golden_retriever
- en: The model has correctly predicted the image as `golden_retriever` with `0.92119014`
    confidence. We now understand how a pre-trained model can be used on a new dataset.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已正确预测图像为`golden_retriever`，置信度为`0.92119014`。我们现在理解了如何在一个新的数据集上使用预训练模型。
- en: Fine-tuning pre-trained models
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调预训练模型
- en: Fine-tuning in transfer learning refers to the process of adapting or updating
    the pre-trained model’s parameters to better fit a specific task or dataset of
    interest. When using transfer learning, the pre-trained model is initially trained
    on a large-scale dataset, typically from a different but related task or domain.
    Fine-tuning allows us to take advantage of the knowledge learned by the pre-trained
    model and customize it for a specific task or dataset.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，微调指的是调整或更新预训练模型的参数，以更好地适应特定任务或感兴趣的数据集。在使用迁移学习时，预训练模型最初是在一个大规模数据集上训练的，通常来自不同但相关的任务或领域。微调使我们能够利用预训练模型学到的知识，并针对特定任务或数据集进行定制。
- en: 'The fine-tuning process typically involves the following steps:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程通常包括以下步骤：
- en: '**Pre-trained model initialization**: The pre-trained model, which has already
    learned useful representations from a source task or dataset, is loaded. The model’s
    parameters are frozen initially, meaning they are not updated during the initial
    training.'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练模型初始化**：加载已经从源任务或数据集中学习到有用表示的预训练模型。模型的参数最初被冻结，这意味着在初始训练期间它们不会被更新。'
- en: '**Modification of the model**: Depending on the specific task or dataset, the
    last few layers or specific parts of the pre-trained model may be modified or
    replaced. The architecture of the model can be adjusted to match the desired output
    or accommodate the characteristics of the target task.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型的修改**：根据具体任务或数据集，预训练模型的最后几层或特定部分可能需要修改或替换。模型的架构可以根据所需的输出或适应目标任务的特性进行调整。'
- en: '**Unfreezing and training**: After modifying the model, the previously frozen
    parameters are unfrozen, allowing them to be updated during training. The model
    is then trained on the target task-specific dataset, often referred to as the
    fine-tuning dataset. The weights of the model are updated using backpropagation
    and gradient-based optimization algorithms to minimize the task-specific loss
    function.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解冻和训练**：修改模型后，之前冻结的参数被解冻，允许它们在训练过程中更新。然后，模型在目标任务特定的数据集上训练，通常称为微调数据集。模型的权重通过反向传播和基于梯度的优化算法更新，以最小化任务特定的损失函数。'
- en: '**Training with a lower learning rate**: During fine-tuning, a smaller learning
    rate is typically used compared to the initial training of the pre-trained model.
    This smaller learning rate helps to ensure that the previously learned representations
    are preserved to some extent while allowing the model to adapt to the target task
    or dataset.'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用较低的学习率进行训练**：在微调过程中，通常使用比预训练模型初始训练时更小的学习率。这个较小的学习率有助于在一定程度上保留之前学到的表示，同时允许模型适应目标任务或数据集。'
- en: The process of fine-tuning strikes a balance between utilizing the knowledge
    captured by the pre-trained model and tailoring it to the specifics of the target
    task. By fine-tuning, the model can learn task-specific patterns and optimize
    its performance for the new task or dataset. The amount of fine-tuning required
    may vary depending on the similarity between the source and target tasks or domains.
    In some cases, only a few training iterations may be sufficient, while in others,
    more extensive training may be necessary.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的过程在利用预训练模型捕获的知识和针对目标任务的特定细节进行定制之间找到了平衡。通过微调，模型可以学习特定于任务的模式并优化其在新任务或数据集上的性能。所需的微调量可能因源任务和目标任务或领域之间的相似性而异。在某些情况下，只需要几次训练迭代就足够了，而在其他情况下，可能需要更广泛的训练。
- en: 'Fine-tuning is a crucial step in transfer learning, as it enables the transfer
    of knowledge from a source task or dataset to a target task, resulting in improved
    performance and faster convergence on the target task. Here’s an example of using
    transfer learning for image labeling using the VGG16 model. We first import the
    necessary libraries:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是迁移学习中的一个关键步骤，因为它使得从源任务或数据集向目标任务的知识迁移成为可能，从而提高了目标任务的表现并加快了收敛速度。以下是一个使用VGG16模型进行图像标注的迁移学习示例。我们首先导入必要的库：
- en: '[PRE45]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We have two classes of images – that of dogs and cats – and therefore we set
    the number of classes variable to `2`. We will also load a pre-trained VGG16 model
    without the top layers. The image sizes we have here are 256 x 256 x 3:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种图像类别——狗和猫，因此我们将类别数量变量设置为`2`。我们还将加载一个不带顶层预训练的VGG16模型。我们这里的图像大小是256 x 256
    x 3：
- en: '[PRE46]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We now freeze the pre-trained layers and create a new model for fine-tuning.
    We then compile the model using `''adam''` as the optimizer:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在冻结预训练层并创建一个新的模型用于微调。然后我们使用`'adam'`作为优化器来编译模型：
- en: '[PRE47]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To prepare for training, we are configuring data generators for both training
    and validation datasets. This crucial step involves rescaling pixel values to
    a range between 0 and 1 using `ImageDataGenerator`. By doing so, we ensure consistent
    and efficient processing of image data, enhancing the model’s ability to learn
    patterns and features during training:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练，我们正在配置训练集和验证集的数据生成器。这一关键步骤涉及使用`ImageDataGenerator`将像素值缩放到0到1之间，这样做可以确保图像数据的一致和高效处理，增强模型在训练期间学习模式和特征的能力：
- en: '[PRE48]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can now fine-tune the model and save it:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以微调模型并保存它：
- en: '[PRE49]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: With this saved model, we can deploy it for various applications, such as making
    predictions on new data, integrating it into larger systems, or further fine-tuning
    similar tasks. The saved model file encapsulates the learned patterns and features
    from the training process, providing a valuable resource for future use and analysis.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个保存的模型，我们可以将其部署到各种应用中，例如对新数据进行预测、将其集成到更大的系统中，或进一步微调类似任务。保存的模型文件封装了训练过程中学习到的模式和特征，为未来的使用和分析提供了宝贵的资源。
- en: In the next section, we’ll delve into the concept of semi-supervised learning
    in labeling—a sophisticated yet approachable technique that combines the strengths
    of both labeled and unlabeled data.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨标签中的半监督学习概念——这是一种既复杂又易于接近的技术，它结合了标记和无标记数据的优点。
- en: Semi-supervised learning
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Traditional supervised learning relies on a fully labeled dataset, which can
    be time-consuming and costly to obtain. Semi-supervised learning, on the other
    hand, allows us to leverage both labeled and unlabeled data to train models and
    make predictions. This approach offers a more efficient way to label data and
    improve model performance.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的监督学习依赖于一个完全标记的数据集，这可能会消耗大量时间和成本。另一方面，半监督学习允许我们利用标记和无标记数据来训练模型并做出预测。这种方法提供了一种更有效的方式来标记数据并提高模型性能。
- en: 'Semi-supervised learning is particularly useful when labeled data is scarce
    or expensive to obtain. It allows us to make use of the vast amounts of readily
    available unlabeled data, which is often abundant in real-world scenarios. By
    leveraging unlabeled data, semi-supervised learning offers several benefits:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习在标记数据稀缺或昂贵时特别有用。它允许我们利用大量现成的无标记数据，这在现实世界场景中通常很丰富。通过利用无标记数据，半监督学习提供了几个好处：
- en: '**Cost-effectiveness**: Semi-supervised learning reduces the reliance on expensive
    manual labeling efforts. By using unlabeled data, which can be collected at a
    lower cost, we can significantly reduce the expenses associated with acquiring
    labeled data.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：半监督学习减少了对外昂贵的手动标记工作的依赖。通过使用成本较低的未标记数据，我们可以显著减少获取标记数据相关的费用。'
- en: '**Utilization of large unlabeled datasets**: Unlabeled data is often abundant
    and easily accessible. Semi-supervised learning enables us to tap into this vast
    resource, allowing us to train models on much larger datasets compared to fully
    supervised learning. This can lead to better model generalization and performance.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用大型未标记数据集**：未标记数据通常很丰富且易于获取。半监督学习使我们能够利用这一巨大资源，使我们能够在比全监督学习更大的数据集上训练模型。这可能导致更好的模型泛化能力和性能。'
- en: '**Improved model performance**: By incorporating unlabeled data during training,
    semi-supervised learning can improve model performance. The unlabeled data provides
    additional information and helps the model capture the underlying data distribution
    more accurately. This can lead to better generalization and increased accuracy
    on unseen data.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进模型性能**：通过在训练过程中结合未标记数据，半监督学习可以提高模型性能。未标记数据提供了额外的信息，并帮助模型更准确地捕捉潜在的数据分布。这可能导致更好的泛化能力和在未见数据上的更高准确性。'
- en: 'There are different approaches within semi-supervised learning that leverage
    the unlabeled data in different ways. Some common methods include the following:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习中，有不同方法可以利用未标记数据以不同的方式。以下是一些常见方法：
- en: '**Self-training**: Self-training involves training a model initially on the
    limited labeled data. Then, the model is used to make predictions on the unlabeled
    data, and the confident predictions are considered as pseudo-labels for the unlabeled
    instances. These pseudo-labeled instances are then combined with the labeled data
    to retrain the model iteratively.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自训练**：自训练涉及首先在有限的标记数据上训练一个模型。然后，使用该模型对未标记数据进行预测，并将自信的预测视为未标记实例的伪标签。这些伪标签实例随后与标记数据结合，以迭代方式重新训练模型。'
- en: '**Co-training**: Co-training involves training multiple models on different
    subsets or views of the data. Each model learns from the labeled data and then
    predicts labels for the unlabeled data. The agreement or disagreement between
    the models’ predictions on the unlabeled data is used to select the most confident
    instances, which are then labeled and added to the training set for further iterations.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同训练**：协同训练涉及在数据的不同子集或视图中训练多个模型。每个模型从标记数据中学习，然后为未标记数据预测标签。模型对未标记数据预测的一致性或差异性被用来选择最自信的实例，这些实例随后被标记并添加到训练集中进行进一步的迭代。'
- en: '**Generative models**: Generative models, such as **variational autoencoders**
    (**VAEs**) or **generative adversarial networks** (**GANs**), can be used in semi-supervised
    learning. These models learn the underlying data distribution and generate plausible
    instances. By incorporating the generated instances into the training process,
    the model can capture more diverse representations and improve its generalization
    performance.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**：在半监督学习中，可以使用生成模型，如**变分自编码器**（**VAEs**）或**生成对抗网络**（**GANs**）。这些模型学习潜在的数据分布并生成合理的实例。通过将生成的实例纳入训练过程，模型可以捕捉更多样化的表示并提高其泛化性能。'
- en: 'Let’s see a simple implementation in Python of this labeling approach. First,
    we import the necessary libraries:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在Python中实现这种标记方法的一个简单示例。首先，我们导入必要的库：
- en: '[PRE50]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We utilize the preprocessed `credit-g` dataset from previous examples and split
    it into labeled and unlabeled subsets. This example assumes that you are using
    the `df_preprocessed` DataFrame we created in the *Uncertainty* *sampling* section:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用之前示例中预处理过的`credit-g`数据集，并将其分为标记和未标记的子集。本例假设您正在使用我们在*不确定性* *采样*部分创建的`df_preprocessed`
    DataFrame：
- en: '[PRE51]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then we train a supervised machine learning model using the labeled data. In
    this example, we will use logistic regression as the supervised model:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用标记数据训练一个监督机器学习模型。在本例中，我们将使用逻辑回归作为监督模型：
- en: '[PRE52]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We then apply the trained supervised model to predict labels for the unlabeled
    data. The predicted labels are considered as pseudo-labels for the unlabeled instances:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着应用训练好的监督模型来预测未标记数据的标签。预测的标签被视为未标记实例的伪标签：
- en: '[PRE53]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now concatenate the labeled data (`X_labeled`) with the pseudo-labeled data
    (`X_unlabeled`) to create the combined feature dataset (`X_combined`). Concatenate
    the corresponding labels (`y_labeled` and `pseudo_labels`) to create the combined
    label dataset (`y_combined`):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将标记数据（`X_labeled`）与伪标记数据（`X_unlabeled`）连接起来，创建结合的特征数据集（`X_combined`）。将相应的标签（`y_labeled`和`pseudo_labels`）连接起来，创建结合的标签数据集（`y_combined`）：
- en: '[PRE54]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, train a semi-supervised machine learning model using the combined feature
    dataset (`X_combined`) and label dataset (`y_combined`). In this example, we will
    use `LabelPropagation` as the semi-supervised model:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用结合的特征数据集（`X_combined`）和标签数据集（`y_combined`）训练一个半监督机器学习模型。在这个例子中，我们将使用`LabelPropagation`作为半监督模型：
- en: '[PRE55]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Use the trained semi-supervised model to make predictions on the test set and
    calculate the accuracy:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的半监督模型对测试集进行预测并计算准确率：
- en: '[PRE56]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The print statement outputs the resulting accuracy score, which, in this case,
    is 0.635.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句输出了最终的准确率分数，在这个例子中，准确率为0.635。
- en: After training our `semi_supervised_model` using `LabelPropagation`, the resulting
    model has effectively learned from both labeled and unlabeled data. The predictions
    on the test set (`y_pred`) showcase the model’s ability to generalize and infer
    labels for previously unseen instances. This output serves as a valuable demonstration
    of how semi-supervised learning techniques, leveraging both labeled and unlabeled
    data, can contribute to robust and accurate predictions in real-world scenarios.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`LabelPropagation`训练我们的`semi_supervised_model`之后，得到的模型有效地从标记和无标记数据中学习。测试集（`y_pred`）上的预测展示了模型泛化和推断先前未见实例标签的能力。这个输出作为半监督学习技术的一个宝贵示例，这些技术利用了标记和无标记数据，可以在现实世界场景中做出稳健和准确的预测。
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various programmatic labeling techniques in machine
    learning. Labeling data is essential for training effective models, and manual
    labeling can be time-consuming and expensive. Programmatic labeling offers automated
    ways to assign meaningful categories or classes to instances of data. We discussed
    a range of techniques, including pattern matching, DB lookup, Boolean flags, weak
    supervision, semi-weak supervision, slicing functions, active learning, transfer
    learning, and semi-supervised learning.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了机器学习中各种程序化标记技术。标记数据对于训练有效的模型至关重要，而手动标记可能既耗时又昂贵。程序化标记提供了自动化的方法来为数据的实例分配有意义的类别或类别。我们讨论了一系列技术，包括模式匹配、数据库查找、布尔标志、弱监督、半弱监督、切片函数、主动学习、迁移学习和半监督学习。
- en: Each technique offers unique benefits and considerations based on the nature
    of the data and the specific labeling requirements. By leveraging these techniques,
    practitioners can streamline the labeling process, reduce manual effort, and train
    effective models using large amounts of labeled or weakly labeled data. Understanding
    and utilizing programmatic labeling techniques are crucial for building robust
    and scalable machine learning systems.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 每种技术都根据数据的性质和特定的标记要求提供独特的优势和考虑。通过利用这些技术，从业者可以简化标记过程，减少人工工作量，并使用大量标记或弱标记的数据训练有效的模型。理解和利用程序化标记技术对于构建稳健和可扩展的机器学习系统至关重要。
- en: In the next chapter, we’ll explore the role of synthetic data in data-centric
    machine learning.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨合成数据在以数据为中心的机器学习中的作用。
