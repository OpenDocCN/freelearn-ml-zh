<html><head></head><body>
<div id="_idContainer024">
<h1 class="chapter-number" id="_idParaDest-45"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-46"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.2.1">Machine Learning Life Cycle</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Machine learning </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.4.1">modeling in practice, either at the industrial level or in academic research, is beyond writing a couple of lines of Python code to train and evaluate a model on a public dataset. </span><span class="koboSpan" id="kobo.4.2">Learning to write a piece of Python program to train a machine learning model using Python and </span><strong class="source-inline"><span class="koboSpan" id="kobo.5.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.6.1"> or a deep learning model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.7.1">PyTorch</span></strong><span class="koboSpan" id="kobo.8.1"> is a starting point for becoming a machine learning developer and specialist. </span><span class="koboSpan" id="kobo.8.2">In this chapter, you will learn about the components of the machine learning life cycle and how, while considering this life cycle when planning for machine learning modeling, it helps you in designing a valuable and </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">scalable model.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">Here are the topics, including the main components of the machine learning life cycle, that will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Before we </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">start modeling</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.14.1">Data collection</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">Data selection</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Data exploration</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.17.1">Data wrangling</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Modeling </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">data preparation</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Model training </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">and evaluation</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Testing the code and </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">the model</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Model deployment </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">and monitoring</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.26.1">By the end of this chapter, you will have learned how to design a machine learning life cycle for your projects and why modularizing your projects into the components of a life cycle helps you in your collaborative model developments. </span><span class="koboSpan" id="kobo.26.2">You will have also learned about some of the techniques and their Python implementations for different components of a machine learning life cycle, such as data wrangling and model training </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">and evaluation.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.28.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.29.1">The following requirements should be considered for this chapter as they will help you better understand the concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.31.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">sklearn</span></strong><span class="koboSpan" id="kobo.34.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">numpy</span></strong><span class="koboSpan" id="kobo.37.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">1.22.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">pandas</span></strong><span class="koboSpan" id="kobo.40.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">1.4.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.42.1">matplotlib</span></strong><span class="koboSpan" id="kobo.43.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">3.5.3</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.45.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02"><span class="No-Break"><span class="koboSpan" id="kobo.47.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.48.1">.</span></span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.49.1">Before we start modeling</span></h1>
<p><span class="koboSpan" id="kobo.50.1">Before collecting data</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.51.1"> as the starting point of a machine learning life cycle, you need to know your objectives. </span><span class="koboSpan" id="kobo.51.2">You need to know what problems you want to solve and then define smaller subproblems that would be machine learning solvable. </span><span class="koboSpan" id="kobo.51.3">For example, in the case of a problem such as, “</span><em class="italic"><span class="koboSpan" id="kobo.52.1">How could we reduce the number of fragile products returned to a manufacturing facility?,</span></em><span class="koboSpan" id="kobo.53.1">” the subproblems could be </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">as follows:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.55.1">How could we detect the cracks </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">before packaging?</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.57.1">How could we design better packaging to protect the products and reduce </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.58.1">transportation-caused cracks?</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.59.1">Could we use better materials to reduce the risk </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">of cracking?</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.61.1">Could we apply small design changes to our product that do not change its functionality but reduce the risk </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">of cracking?</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.63.1">Once you </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.64.1">have identified your subproblems, you can find out how you can use machine learning for each and go through a machine learning life cycle for the defined subproblems. </span><span class="koboSpan" id="kobo.64.2">Each of the subproblems may need specific data processing and machine learning modeling, and some of them could be easier to solve compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">the rest.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.66.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.67.1">.1</span></em><span class="koboSpan" id="kobo.68.1"> shows the major steps in machine learning life cycles. </span><span class="koboSpan" id="kobo.68.2">Some of these names are not universally defined. </span><span class="koboSpan" id="kobo.68.3">For example, data exploration sometimes gets included in data wrangling. </span><span class="koboSpan" id="kobo.68.4">But all these steps are required, even if they are named differently in </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">different resources:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.70.1"><img alt="Figure 2.1 – Machine learning life cycle" src="image/B16369_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.71.1">Figure 2.1 – Machine learning life cycle</span></p>
<p><span class="koboSpan" id="kobo.72.1">When you rely </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.73.1">on a dataset that’s already available in Python, through </span><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.75.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">PyTorch</span></strong><span class="koboSpan" id="kobo.77.1">, for example, or a dataset that is ready for modeling in public repositories, you don’t need to worry about the early steps, such as data collection, selection, and wrangling. </span><span class="koboSpan" id="kobo.77.2">These steps have already been taken care of for you. </span><span class="koboSpan" id="kobo.77.3">Or if you are just doing modeling for practice and don’t want to provide your model in a production system, you don’t need to worry about model deployment and monitoring. </span><span class="koboSpan" id="kobo.77.4">But understanding the meaning, importance, and benefits of all these steps helps you develop or design a functional technology with continuous improvement to be provided for users. </span><span class="koboSpan" id="kobo.77.5">It also helps you better understand your role as a machine learning developer or find your first job or a better job in </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">this fiel</span><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.79.1">d.</span></span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.80.1">Data collection</span></h1>
<p><span class="koboSpan" id="kobo.81.1">The first step in</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.82.1"> the machine learning life cycle is </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.83.1">data collection. </span><span class="koboSpan" id="kobo.83.2">It could be about collecting data from different public or commercial databases, storing user data back into your database or any data storage system you have, or even using commercial entities that take care of data collection and annotation for you. </span><span class="koboSpan" id="kobo.83.3">If you are relying on free resources, the main consideration for you could be the space the data will get in your local or cloud-based storage system and the time you need to spend to collect the data and analyze it in future steps. </span><span class="koboSpan" id="kobo.83.4">But for paid data, either provided in commercial resources or generated by data collection, generation, and annotation companies, you need to assess the value of the data for modeling before you decide to pay </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">for</span><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.85.1"> it.</span></span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.86.1">Data selection</span></h1>
<p><span class="koboSpan" id="kobo.87.1">Depending </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.88.1">on the objectives of the corresponding projects, we need to select the required data for model training and testing. </span><span class="koboSpan" id="kobo.88.2">For example, you might have access to information about cancer patients in one or multiple hospitals, such as their age, gender, whether they smoke or not, their genetic information if </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.89.1">available, their MRI or CT scans if available, history of their medication, their response to cancer drugs, whether they had surgery or not, their prescriptions, either handwritten or in PDF format, and much more. </span><span class="koboSpan" id="kobo.89.2">When you want to build a machine learning model to predict the response of patients to therapy using their CT scans, you need to select different data for each patient compared to when you want to build a model using their information, such as age, gender, and smoking status. </span><span class="koboSpan" id="kobo.89.3">You also need to select patients from whom you have the input and output data available if you are building a supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">learning model.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.91.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.92.1">It is possible to combine data points with and without outputs in a semi-supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">learning model.</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">Selecting relevant data for your models is not an easy task as the information that separates data as </span><em class="italic"><span class="koboSpan" id="kobo.95.1">relevant</span></em><span class="koboSpan" id="kobo.96.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.97.1">irrelevant</span></em><span class="koboSpan" id="kobo.98.1"> from your model objective is not necessarily available in this binary way. </span><span class="koboSpan" id="kobo.98.2">Imagine you need to extract data from a chemical, biological, or physical database, which could be a collection of data from different smaller datasets, supplementary materials of papers, or even data coming from within scientific articles. </span><span class="koboSpan" id="kobo.98.3">Or perhaps you want to extract information from the medical records of patients or even from written answers to an economical or sociological survey. </span><span class="koboSpan" id="kobo.98.4">In all such examples, separation of data for your model, or querying relevant data from relevant </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.99.1">databases, is </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.100.1">not as simple as searching for one keyword. </span><span class="koboSpan" id="kobo.100.2">Each keyword could have synonyms, either in plain English or in technical terms, could be written in different ways, or even sometimes the relevant information could exist in different columns of a data file or a relational database. </span><span class="koboSpan" id="kobo.100.3">Proper data selection and query systems provide you with a huge opportunity to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">your models.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">You can benefit from a literature review and asking experts, if needed, to extend the keywords you are using. </span><span class="koboSpan" id="kobo.102.2">You can benefit from known data selection methods for specific tasks you have or even license tools or pay for services to help you in extracting more relevant data for your objectives. </span><span class="koboSpan" id="kobo.102.3">There are also advanced natural language processing techniques to help you in your query system from text. </span><span class="koboSpan" id="kobo.102.4">We will discuss these in </span><a href="B16369_13.xhtml#_idTextAnchor342"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.103.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.104.1">, </span><em class="italic"><span class="koboSpan" id="kobo.105.1">Advanced Deep Learning Techniques</span></em><span class="koboSpan" id="kobo.106.1">, and </span><a href="B16369_14.xhtml#_idTextAnchor379"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.107.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.108.1">, </span><em class="italic"><span class="koboSpan" id="kobo.109.1">Introduction to Recent Advancements in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Machine L</span><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.111.1">earning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">.</span></span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.113.1">Data exploration</span></h1>
<p><span class="koboSpan" id="kobo.114.1">In this</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.115.1"> stage, your </span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.116.1">data is selected and you can explore the quantity, quality, sparsity, and format of the data. </span><span class="koboSpan" id="kobo.116.2">You can find the number of data points in each class if you have categorical output in supervised learning, distribution of features, confidence in output variables, if available, and other characteristics of the data you get out of the </span><em class="italic"><span class="koboSpan" id="kobo.117.1">data selection</span></em><span class="koboSpan" id="kobo.118.1"> stage. </span><span class="koboSpan" id="kobo.118.2">This process helps you identify issues with your data that need to be fixed in </span><em class="italic"><span class="koboSpan" id="kobo.119.1">data wrangling</span></em><span class="koboSpan" id="kobo.120.1">, which is the next step in the life cycle, or opportunities for improving your data by revising your </span><em class="italic"><span class="koboSpan" id="kobo.121.1">data </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.122.1">selectio</span><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.123.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.124.1"> process.</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.125.1">Data wrangling</span></h1>
<p><span class="koboSpan" id="kobo.126.1">Your data</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.127.1"> needs to go through structuring and </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.128.1">enriching processes and be transformed and cleaned up, if necessary. </span><span class="koboSpan" id="kobo.128.2">All these aspects are part of </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">dat</span><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.130.1">a wrangling.</span></span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.131.1">Structuring</span></h2>
<p><span class="koboSpan" id="kobo.132.1">The raw</span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.133.1"> data might come in different formats and sizes. </span><span class="koboSpan" id="kobo.133.2">You might have access to handwritten notes, Excel sheets, or even images of tables that contain information that needs to be extracted and put in the right format for further analysis and used for modeling. </span><span class="koboSpan" id="kobo.133.3">This process is not about transforming all data into a table-like format. </span><span class="koboSpan" id="kobo.133.4">In the process of data structuring, you need to be careful regarding information loss. </span><span class="koboSpan" id="kobo.133.5">For example, you could have features that are in a specific order, such as based on time, date, or the sequence of information coming thro</span><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.134.1">ugh </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">a device.</span></span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.136.1">Enriching</span></h2>
<p><span class="koboSpan" id="kobo.137.1">After</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.138.1"> structuring and formatting your data, you need to assess whether you have the right data to build a machine learning model of that cycle. </span><span class="koboSpan" id="kobo.138.2">You might identify opportunities to add or generate new data before continuing the wrangling process. </span><span class="koboSpan" id="kobo.138.3">For example, you might find out that in the data for identifying cracks in images of products in a manufacturing pipeline, you only have 50 out of 10,000 images that are labeled as images of cracked products. </span><span class="koboSpan" id="kobo.138.4">You might be able to find other images of cracked products or you could generate new images using a process</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.139.1"> called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.140.1">data augmentation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.142.1">Data augmentation</span></p>
<p class="callout"><span class="koboSpan" id="kobo.143.1">Data augmentation is a series of techniques for generating new data points, computationally, using the original dataset we have at hand. </span><span class="koboSpan" id="kobo.143.2">For example, if you rotate your portrait, or change the quality of an image by adding Gaussian noise to it, the new image will still show your face. </span><span class="koboSpan" id="kobo.143.3">But it could help your model to be more generalizable. </span><span class="koboSpan" id="kobo.143.4">We will talk about different data augmentation techniques in </span><a href="B16369_05.xhtml#_idTextAnchor183"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.144.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.145.1">, </span><em class="italic"><span class="koboSpan" id="kobo.146.1">Improving the Performance of Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.147.1">L</span><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.148.1">earning Models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">.</span></span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.150.1">Data transformation</span></h2>
<p><span class="koboSpan" id="kobo.151.1">The</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.152.1"> features and the output of datasets could be different types of variables, including </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.154.1">Quantitative </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.155.1">or numerical</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.157.1">Discrete</span></strong><span class="koboSpan" id="kobo.158.1">: For </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.159.1">example, the </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.160.1">number of houses in </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">a neighborhood</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.162.1">Continuous</span></strong><span class="koboSpan" id="kobo.163.1">: For </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.164.1">example, the age or weight </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">of patients</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.166.1">Qualitative </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.167.1">or categorical</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Nominal (no order)</span></strong><span class="koboSpan" id="kobo.170.1">: For</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.171.1"> example, different</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.172.1"> colors </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">of cars</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.174.1">Ordinal (qualitative variable with order)</span></strong><span class="koboSpan" id="kobo.175.1">: For</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.176.1"> example, grades of students, such as A, B, C, </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">or D</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.178.1">When we train a machine learning model, the model needs to use numerical values to calculate the loss function in each iteration of the optimization process. </span><span class="koboSpan" id="kobo.178.2">Hence, we need to transform categorical variables into numerical alternatives. </span><span class="koboSpan" id="kobo.178.3">There are multiple feature encoding techniques, three of which are one-hot encoding, target encoding (Micci-Barreca, 2001), and label encoding. </span><span class="koboSpan" id="kobo.178.4">A one-hot, label, and target encoding calculation for an example matrix of four columns, including age, gender, group, and target, and seven rows, as seven example data points, is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.179.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.182.1"><img alt="Figure 2.2 – Manual calculations for one-hot, target, and ﻿label encoding using a simple example dataset with four features and seven data points" src="image/B16369_02_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.183.1">Figure 2.2 – Manual calculations for one-hot, target, and label encoding using a simple example dataset with four features and seven data points</span></p>
<p><span class="koboSpan" id="kobo.184.1">This is an</span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.185.1"> imaginary dataset for predicting the response of patients to a drug, with the target column as the output. </span><span class="koboSpan" id="kobo.185.2">Variable categories are abbreviated as F: Female, M: Male, H1: Hospital 1, H2: Hospital 2, and H3: Hospital 3. </span><span class="koboSpan" id="kobo.185.3">In reality, many more variables need to be considered and more data points are necessary to have a reliable model for drug response prediction and assess whether there are biases in the response of patients to drugs between male and female groups or in </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">different hospitals.</span></span></p>
<p><span class="koboSpan" id="kobo.187.1">Each of these techniques has its benefits and caveats. </span><span class="koboSpan" id="kobo.187.2">For example, one-hot encoding increases the number of features (that is, the dimensionality of the dataset) and increases the chance of overfitting. </span><span class="koboSpan" id="kobo.187.3">Label encoding assigns integer values to each category, which </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.188.1">do not necessarily have a meaning. </span><span class="koboSpan" id="kobo.188.2">For example, considering male as 1 and female as 0 is arbitrary and doesn’t have any real meaning. </span><span class="koboSpan" id="kobo.188.3">Target encoding is an alternative approach that considers the probabilities of each category concerning the target. </span><span class="koboSpan" id="kobo.188.4">You can read the mathematical details of this process in Micci-Barreca, 2001. </span><span class="koboSpan" id="kobo.188.5">Python’s implementation of these approaches is provided in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">code snippets.</span></span></p>
<p><span class="koboSpan" id="kobo.190.1">Let’s define a synthetic DataFrame to use for </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">feature encoding:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.192.1">
import pandas as pdorig_df = pd.DataFrame({
    'age': [45, 43, 54, 56, 54, 52, 41],
    'gender': ['M', 'F', 'F', 'M', 'M', 'F', 'M'],
    'group': ['H1', 'H1', 'H2', 'H3', 'H2', 'H1', 'H3'],
    'target': [0, 0, 1, 0, 1, 1, 0]})</span></pre>
<p><span class="koboSpan" id="kobo.193.1">First, we will use label encoding to encode the categorical features in the </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">defined DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.195.1">
# encoding using label encodingfrom sklearn.preprocessing import LabelEncoder
# initializing LabelEncoder
le = LabelEncoder()
# encoding gender and group columns
label_encoded_df = orig_df.copy()
label_encoded_df['gender'] = le.fit_transform(
    label_encoded_df.gender)
label_encoded_df['group'] = le.fit_transform(
    label_encoded_df.group)</span></pre>
<p><span class="koboSpan" id="kobo.196.1">Then, we will try to perform one-hot encoding for categorical </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">feature transformation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.198.1">
# encoding using one hot encodingfrom sklearn.preprocessing import OneHotEncoder
# initializing OneHotEncoder
ohe = OneHotEncoder(categories = 'auto')
# encoding gender column
gender_ohe = ohe.fit_transform(
    orig_df['gender'].values.reshape(-1,1)).toarray()
gender_ohe_df = pd.DataFrame(gender_ohe)
# encoding group column
group_ohe = ohe.fit_transform(
    orig_df['group'].values.reshape(-1,1)).toarray()
group_ohe_df = pd.DataFrame(group_ohe)
# generating the new dataframe with one hot encoded features
onehot_encoded_df = pd.concat(
    [orig_df, gender_ohe_df, group_ohe_df], axis =1)
onehot_encoded_df = onehot_encoded_df.drop(
    ['gender', 'group'], axis=1)
onehot_encoded_df.columns = [
    'age','target','M', 'F','H1','H2', 'H3']</span></pre>
<p><span class="koboSpan" id="kobo.199.1">Now, we will </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.200.1">implement target encoding in Python, after installing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">category_encoders</span></strong><span class="koboSpan" id="kobo.202.1"> library, as the third encoding approach, </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.204.1">
# encoding using target encodingfrom category_encoders import TargetEncoder
# initializing LabelEncoder
te = TargetEncoder()
# encoding gender and group columns
target_encoded_df = orig_df.copy()
target_encoded_df['gender'] = te.fit_transform(
    orig_df['gender'], orig_df['target'])
target_encoded_df['group'] = te.fit_transform(
    orig_df['group'], orig_df['target'])</span></pre>
<p><span class="koboSpan" id="kobo.205.1">Ordinal variables </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.206.1">can also be transformed using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">OrdinalEncoder</span></strong><span class="koboSpan" id="kobo.208.1"> class as part of </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">sklearn.preprocessing</span></strong><span class="koboSpan" id="kobo.210.1">. </span><span class="koboSpan" id="kobo.210.2">The difference between ordinal and nominal transformation is the meaning behind the order of categories in ordinal variables. </span><span class="koboSpan" id="kobo.210.3">For example, if we are encoding grades of students, A, B, C, and D could be transformed into 1, 2, 3, and 4, or 4, 3, 2, and 1, but transforming them into 1, 3, 4, and 2 will not be acceptable as it is changing the meaning behind the order of </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">the grades.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">Output variables can also be categorical. </span><span class="koboSpan" id="kobo.212.2">You can use label encoding to transform a nominal output into a numerical variable f</span><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.213.1">or </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">classification models.</span></span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.215.1">Cleaning</span></h2>
<p><span class="koboSpan" id="kobo.216.1">After</span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.217.1"> structuring the data, it </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.218.1">needs to be cleaned. </span><span class="koboSpan" id="kobo.218.2">Cleaning data helps increase the quality of your data and makes it closer to being ready for modeling. </span><span class="koboSpan" id="kobo.218.3">An example of a cleaning process is filling in missing values in your data. </span><span class="koboSpan" id="kobo.218.4">For example, if you want to use patients’ living habits to predict their risk of getting diabetes using their responses to a survey, you might find out some of the participants didn’t respond to the questions </span><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.219.1">about their </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">smoking habits.</span></span></p>
<h3><span class="koboSpan" id="kobo.221.1">Feature imputation for filling in missing values</span></h3>
<p><span class="koboSpan" id="kobo.222.1">The features</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.223.1"> of a dataset we have at hand could contain missing values. </span><span class="koboSpan" id="kobo.223.2">The majority of machine learning models and their corresponding Python implementations cannot handle missing values. </span><span class="koboSpan" id="kobo.223.3">In these cases, we need to either remove data points with missing feature values or somehow fill in those missing values. </span><span class="koboSpan" id="kobo.223.4">There are feature imputation techniques we can use to calculate the values of features that are missing in our dataset. </span><span class="koboSpan" id="kobo.223.5">Examples of such methods</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.224.1"> are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.225.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.226.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<span class="koboSpan" id="kobo.228.1"><img alt="Figure 2.3 – Feature imputation techniques for calculating missing feature values" src="image/B16369_02_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.229.1">Figure 2.3 – Feature imputation techniques for calculating missing feature values</span></p>
<p><span class="koboSpan" id="kobo.230.1">As you can see, either we can use other values of the same features and replace the missing values with the mean or median of the available values, or we can use other features with low or no missing values that have a high correlation with the feature with missing values. </span><span class="koboSpan" id="kobo.230.2">In the second case, we can use the feature with the highest correlation, with the target feature with missing values, to build a linear model. </span><span class="koboSpan" id="kobo.230.3">The linear model considers the correlated feature as input and the feature with missing values as output and then uses the predictions of the linear model to calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">missing values.</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">When we use a </span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.233.1">statistical summary of the values of the same feature, such as the mean or median, we are reducing the variance of the feature values as those summary values will be used for all the missing values of the same feature (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.234.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.235.1">.3</span></em><span class="koboSpan" id="kobo.236.1">). </span><span class="koboSpan" id="kobo.236.2">On the other hand, when we use a linear model between the feature with missing values and a highly correlated feature with low or no missing values, we are assuming a linear relationship between them. </span><span class="koboSpan" id="kobo.236.3">Alternatively, we can build more complex models between features for missing value calculation. </span><span class="koboSpan" id="kobo.236.4">All these approaches have their benefits and limitations, and you need to choose the one that works best for your dataset, depending on the distribution of feature values, the fraction of data points with missing features values, the correlation range between features, the existence of features with low or no missing value, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">relevant factors.</span></span></p>
<p><span class="koboSpan" id="kobo.238.1">We used a very simple case of four features and five data points in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.239.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.240.1">.3</span></em><span class="koboSpan" id="kobo.241.1"> to showcase the discussed feature imputation techniques. </span><span class="koboSpan" id="kobo.241.2">But in reality, we need to build models with more than four features. </span><span class="koboSpan" id="kobo.241.3">We can use Python libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.243.1"> for feature imputation by using the mean of the same feature values, as follows. </span><span class="koboSpan" id="kobo.243.2">First, we will import the </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">required libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.245.1">
import numpy as npfrom sklearn.impute import SimpleImputer</span></pre>
<p><span class="koboSpan" id="kobo.246.1">Then, we must define the two-dimensional input list, where each internal list shows the feature values of a </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">data point:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.248.1">
X = [[5, 1, 2, 8],    [2, 3, np.nan, 4],
    [5, 4, 4, 6],
    [8, 5, np.nan, 7],
    [7, 8, 8, 3]]</span></pre>
<p><span class="koboSpan" id="kobo.249.1">Now, we are ready to fit a </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">SimpleImputer</span></strong><span class="koboSpan" id="kobo.251.1"> function by specifying what needs to be considered as a missing value and what strategy to be used </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">for imputation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.253.1">
# strategy options: mean, median, most_frequent, constantimp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(X)
# calculate missing values of the input
X_no_missing = imp.transform(X)</span></pre>
<p><span class="koboSpan" id="kobo.254.1">We can</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.255.1"> also use </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.257.1"> to make a linear regression model that calculates missing </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">feature values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
import numpy as npfrom sklearn.linear_model import LinearRegression as LR
# defining input variables for feature 2 and 3
f2 = np.array([1, 4, 8]).reshape((-1, 1))
f3 = np.array([2, 4, 8])
# initializing a linear regression model with sklearn LinearRegression
model = LR()
# fitting the linear regression model using f2 and f3 as input and output variables, respectively
model.fit(f2, f3)
# predicting missing values of feature 3
model.predict(np.a</span><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.260.1">rray([3, 5]).reshape((-1, 1)))</span></pre>
<h3><span class="koboSpan" id="kobo.261.1">Outlier removal</span></h3>
<p><span class="koboSpan" id="kobo.262.1">Numerical variables</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.263.1"> in our datasets could have values that are far away from the rest of the data. </span><span class="koboSpan" id="kobo.263.2">They could be real values that are dissimilar to the rest of the data points or caused by errors in data generation, such as in experimental measurement processes. </span><span class="koboSpan" id="kobo.263.3">You can visually see and detect them using a boxplot (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.265.1">.4</span></em><span class="koboSpan" id="kobo.266.1">). </span><span class="koboSpan" id="kobo.266.2">The circles of the plot are the outliers that get automatically detected by the plotting functions in Python, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">matplotlib.pyplot.boxplot</span></strong><span class="koboSpan" id="kobo.268.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.269.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.270.1">.4</span></em><span class="koboSpan" id="kobo.271.1">). </span><span class="koboSpan" id="kobo.271.2">Although visualization is a good way of exploring our data and understanding the distribution of numerical variables, we need to have a quantitative way of detecting outliers without the need to plot the values of all the variables in </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">our datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.273.1">The simplest way of detecting outliers is by using quantiles of the distribution of variable values. </span><span class="koboSpan" id="kobo.273.2">Data points that are beyond the upper and lower bounds are considered outliers (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.275.1">.4</span></em><span class="koboSpan" id="kobo.276.1">). </span><span class="koboSpan" id="kobo.276.2">Lower and upper bounds can be calculated as </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.277.1">Q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.278.1">1</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.279.1"> - a.IQR</span></span><span class="koboSpan" id="kobo.280.1"> and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">Q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">3</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.283.1"> - a.IQR</span></span><span class="koboSpan" id="kobo.284.1">, where  can be a real value between 1.5 and 3. </span><span class="koboSpan" id="kobo.284.2">The common value of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.285.1">a</span></span><span class="koboSpan" id="kobo.286.1">, which is also used by default in drawing boxplots, is 1.5, but having higher values makes the process of outlier identification less stringent and lets fewer data points be detected as outliers. </span><span class="koboSpan" id="kobo.286.2">For example, by changing the stringency of outlier detection from the default (that is, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.287.1">a </span></span><span class="koboSpan" id="kobo.288.1">= 1.5) to </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.289.1">a </span></span><span class="koboSpan" id="kobo.290.1">= 3, none of the data points in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.291.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.292.1">.4</span></em><span class="koboSpan" id="kobo.293.1"> would be detected as outliers. </span><span class="koboSpan" id="kobo.293.2">This approach for outlier identification is non-parametric, meaning it doesn’t have any assumptions regarding the distribution of data points. </span><span class="koboSpan" id="kobo.293.3">Hence, it can be applied to non-normal distributions, such as the data shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.294.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.295.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<span class="koboSpan" id="kobo.297.1"><img alt="Figure 2.4 – Outliers in histograms and boxplots" src="image/B16369_02_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.298.1">Figure 2.4 – Outliers in histograms and boxplots</span></p>
<p><span class="koboSpan" id="kobo.299.1">In the preceding figure, the plots were generated using the values of features in the diabetes dataset of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.301.1"> package, which was loaded </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">via </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">skl</span><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.304.1">earn.datasets.load_diabetes()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.306.1">Data scaling</span></h3>
<p><span class="koboSpan" id="kobo.307.1">The values of features, either originally numerical or after transformation, could have different ranges. </span><span class="koboSpan" id="kobo.307.2">Many machine learning models perform better, or at least their optimization processes converge faster, if their feature values get scaled and normalized properly. </span><span class="koboSpan" id="kobo.307.3">For example, if you have a feature ranging from 0.001 to 0.05 and another one from 1,000 to 5,000, bringing both of them to a reasonable range such as [0, 1] or [-1, 1] could help improve the speed of convergence or the performance of your model. </span><span class="koboSpan" id="kobo.307.4">You need to make sure the scaling and normalizations you implement don’t cause ties in your feature values, meaning data points don’t lose their difference based on features that went </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">under transformation.</span></span></p>
<p><span class="koboSpan" id="kobo.309.1">The objective of scaling is to change the range of values of a variable. </span><span class="koboSpan" id="kobo.309.2">In normalization, the shape of the distribution of values could also change. </span><span class="koboSpan" id="kobo.309.3">You can use examples of these methods and the corresponding classes available in </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.311.1"> in your projects to improve the scale and distribution of your features (</span><em class="italic"><span class="koboSpan" id="kobo.312.1">Table 2.1</span></em><span class="koboSpan" id="kobo.313.1">). </span><span class="koboSpan" id="kobo.313.2">The resulting scaled variables after using each of these classes have specific characteristics. </span><span class="koboSpan" id="kobo.313.3">For example, the values of a variable after using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">StandardScalar</span></strong><span class="koboSpan" id="kobo.315.1"> class of </span><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.317.1"> will be centered around zero with a standard deviation </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">of one.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">Some of these techniques, such as robust scaling, which can be done using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">RobustScaler</span></strong><span class="koboSpan" id="kobo.321.1"> class of </span><strong class="source-inline"><span class="koboSpan" id="kobo.322.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.323.1">, are less likely to be affected by outliers (</span><em class="italic"><span class="koboSpan" id="kobo.324.1">Table 2.1</span></em><span class="koboSpan" id="kobo.325.1">). </span><span class="koboSpan" id="kobo.325.2">In robust scaling, outliers, based on the definition we provided, don’t affect how the median and </span><em class="italic"><span class="koboSpan" id="kobo.326.1">IQR</span></em><span class="koboSpan" id="kobo.327.1"> are calculated and, therefore, do not affect the scaling process. </span><span class="koboSpan" id="kobo.327.2">Outliers themselves then be scaled using the calculated median and </span><em class="italic"><span class="koboSpan" id="kobo.328.1">IQR</span></em><span class="koboSpan" id="kobo.329.1">. </span><span class="koboSpan" id="kobo.329.2">Outliers can be either kept or removed before or after scaling, depending on the machine learning method used and the task at hand. </span><span class="koboSpan" id="kobo.329.3">But the important point is to detect them and be aware of them when you’re trying to prepare data for modeling and, if required, scale or </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">remove them:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.331.1">Python Class</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.332.1">Mathematical Definition</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.333.1">Value Limits</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.334.1">sklearn.preprocessing.StandardScaler()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.335.1">Z = (X - u) / s</span></p>
<p><span class="No-Break"><span class="koboSpan" id="kobo.336.1">u: Mean</span></span></p>
<p><span class="koboSpan" id="kobo.337.1">s: </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">Standard deviation</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.339.1">No limit</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">&gt;99% of data between -3 </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">and 3</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">sklearn.preprocessing.MinMaxScaler()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.343.1">X_scaled = (</span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">X-X</span></span><span class="No-Break"><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.345.1">min</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">)/(X</span></span><span class="No-Break"><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.347.1">max</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">-X</span></span><span class="No-Break"><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.349.1">min</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.351.1">[</span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">0,1]</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">sklearn.preprocessing.MaxAbsScaler()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.354.1">X_scaled = </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">X/|X|</span></span><span class="No-Break"><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.356.1">max</span></span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.357.1">[-</span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">1,1]</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">sklearn.preprocessing.RobustScaler()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.360.1">Z</span><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.361.1">robust</span></span><span class="koboSpan" id="kobo.362.1"> = (X - Q</span><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.363.1">2</span></span><span class="koboSpan" id="kobo.364.1">) / </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">IQR</span></span></p>
<p><span class="No-Break"><span class="koboSpan" id="kobo.366.1">Q</span></span><span class="No-Break"><span class="subscript" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.367.1">2</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">: Median</span></span></p>
<p><span class="koboSpan" id="kobo.369.1">IQR: </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">Interquartile range</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.371.1">No limit</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">Majority of data between -3 </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">and 3</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.374.1">Table 2.1 – Example of Python classes for scaling and normalizing feature values</span></p>
<p><span class="koboSpan" id="kobo.375.1">Other forms of </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.376.1">exploratory data analysis are conducted after data wrangling before machine learning modeling is started. </span><span class="koboSpan" id="kobo.376.2">Domain expertise could also help in identifying patterns whose interpretations need to be better understood regarding the subject domain for which the problem has been defined. </span><span class="koboSpan" id="kobo.376.3">To increase the likelihood of success for machine learning modeling, you may need feature engineering to build new features or learn new features through representation learning. </span><span class="koboSpan" id="kobo.376.4">These new features could be as simple as body mass index, defined as the ratio of someone’s weight in kilograms to the square of their height in meters. </span><span class="koboSpan" id="kobo.376.5">Or they could be new features and representations that are learned through complicated processes or extra machine learning modeling. </span><span class="koboSpan" id="kobo.376.6">We will talk about this later in </span><a href="B16369_14.xhtml#_idTextAnchor379"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.378.1">, </span><em class="italic"><span class="koboSpan" id="kobo.379.1">Introduction to Recent Advancements in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.380.1">Machine Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">.</span></span><a id="_idTextAnchor099"/></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.382.1">Modeling data preparation</span></h1>
<p><span class="koboSpan" id="kobo.383.1">In this </span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.384.1">stage of a machine learning life cycle, we need to finalize the features and data points we want to use for modeling, as well as our model evaluation and </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">testing strategies</span><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.386.1">.</span></span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.387.1">Feature selection and extraction</span></h2>
<p><span class="koboSpan" id="kobo.388.1">The original </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.389.1">features that were normalized and scaled in previous steps can be now processed further to increase the likelihood of having a high-performance model. </span><span class="koboSpan" id="kobo.389.2">In general, features can either be sub-selected, meaning some of the features get thrown out, using a </span><em class="italic"><span class="koboSpan" id="kobo.390.1">feature selection</span></em><span class="koboSpan" id="kobo.391.1"> method, or be used to generate new features, which is traditionally called </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.392.1">feature extractio</span><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.393.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.395.1">Feature selection</span></h3>
<p><span class="koboSpan" id="kobo.396.1">The goal of feature selection is to reduce the number of features, or the dimensionality of your data, and keep features that are information-rich. </span><span class="koboSpan" id="kobo.396.2">For example, if we have 20,000 features and 500 data points, there is a high chance that most of the original 20,000 features are not informative when used to build a supervised learning model. </span><span class="koboSpan" id="kobo.396.3">The following list explains some simple techniques for </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">feature selection:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.398.1">Keeping features with a high variance or MAD across the </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">data points</span></span></li>
<li><span class="koboSpan" id="kobo.400.1">Keeping features with the highest number of unique values across the </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">data points</span></span></li>
<li><span class="koboSpan" id="kobo.402.1">Keeping representative features from groups of highly </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">correlated features</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.404.1">These processes can be conducted using all the data points or just training data to avoid potential information leakage between the training and </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">test dat</span><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.406.1">a.</span></span></p>
<h3><span class="koboSpan" id="kobo.407.1">Feature extraction</span></h3>
<p><span class="koboSpan" id="kobo.408.1">Combining</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.409.1"> original features linearly or nonlinearly could result in more informative features for building a predictive model. </span><span class="koboSpan" id="kobo.409.2">This process is called feature extraction and could be conducted based on domain knowledge or through different statistical or machine learning models. </span><span class="koboSpan" id="kobo.409.3">For example, you can use principal component analysis or isometric mapping to reduce the dimensionality of your data in a linear or non-linear way, respectively. </span><span class="koboSpan" id="kobo.409.4">Then, you can use these new features in your training and testing process. </span><span class="koboSpan" id="kobo.409.5">The</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.410.1"> Python implementation of these two approaches is provided in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">code snippets.</span></span></p>
<p><span class="koboSpan" id="kobo.412.1">First, let’s import the required libraries and load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">scikit-learn</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.414.1">digit dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.415.1">
import numpy as npimport matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import Isomap
from sklearn.datasets import load_digits
# loading digit dataset from sklearn
X, _ = load_digits(return_X_y=True)
print('Number of features: {}'.format(X.shape[1]))</span></pre>
<p><span class="koboSpan" id="kobo.416.1">Now, let’s use </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">isomap</span></strong><span class="koboSpan" id="kobo.418.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">pca</span></strong><span class="koboSpan" id="kobo.420.1">, both of which are available </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.424.1">
# fitting isomap and build new dataframe of feature with 5 componentsembedding = Isomap(n_components=5)
X_transformed_isomap = embedding.fit_transform(X)
print('Number of features: {}'.format(
    X_transformed_isomap.shape[1]))
# fitting pca and build new dataframe of feature with 5 components
pca = PCA(n_components=5)
X_transformed_pca = pca.fit_transform(X)
print('Number of features: {}'.format(
    X_transformed_pca.shape[1]))
# plotting ratio of variance explained by the first n, being between 1 and 5, components
plt.bar(x = np.arange(0, len(
    pca.explained_variance_ratio_)),
    height = np.cumsum(pca.explained_variance_ratio_))
plt.ylabel('Explained variance ratio')
plt.xlabel('Number of components')
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.425.1">The number</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.426.1"> of components you can select from each such method can be determined through different techniques. </span><span class="koboSpan" id="kobo.426.2">For example, the explained variance ratio is a commonly used approach to select the number of principal components. </span><span class="koboSpan" id="kobo.426.3">These are identified through principal component analysis and collectively explain more than a specific percentage, such as 70% of the total variance in </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">a dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.428.1">There are also more advanced techniques that are part of self-supervised pre-training and representation learning for identifying new features. </span><span class="koboSpan" id="kobo.428.2">In these techniques, large amounts of data are used to calculate new features, representations, or embeddings. </span><span class="koboSpan" id="kobo.428.3">For example, the English version of Wikipedia can be used to come up with better representations of English words rather than performing one-hot encoding for each word. </span><span class="koboSpan" id="kobo.428.4">We will talk about self-supervised learning models in  </span><a href="B16369_14.xhtml#_idTextAnchor379"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.429.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.430.1">, </span><em class="italic"><span class="koboSpan" id="kobo.431.1">Introduction to Recent Advancements in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.432.1">Machine Lear</span><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.433.1">ning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">.</span></span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.435.1">Designing an evaluation and testing strategy</span></h2>
<p><span class="koboSpan" id="kobo.436.1">We need to</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.437.1"> specify our testing strategy before we train our model to identify its parameters or optimal hyperparameters. </span><span class="koboSpan" id="kobo.437.2">Model testing could be done by another team on separate </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.438.1">datasets if you are working in a big organization. </span><span class="koboSpan" id="kobo.438.2">Alternatively, you can dedicate one or multiple datasets, separate from your training set, or separate part of your data so that you can test it separately from the training set. </span><span class="koboSpan" id="kobo.438.3">You also need to list the ways you want to assess the performance of your model in the testing stage. </span><span class="koboSpan" id="kobo.438.4">For example, you may need to specify the performance plots or measures you want to use, such as </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.439.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.440.1">receiver operating curve</span></strong><span class="koboSpan" id="kobo.441.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.442.1">ROC</span></strong><span class="koboSpan" id="kobo.443.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.444.1">precision-recall</span></strong><span class="koboSpan" id="kobo.445.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.446.1">PR</span></strong><span class="koboSpan" id="kobo.447.1">) curve, or </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.448.1">other criteria, to select a new </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">classification model.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">Once your testing strategy has been defined, you can use the rest of the data to specify training and validation sets. </span><span class="koboSpan" id="kobo.450.2">Validation and training sets don’t need to be one series of fixed data points. </span><span class="koboSpan" id="kobo.450.3">We can </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.451.1">use </span><em class="italic"><span class="koboSpan" id="kobo.452.1">k</span></em><span class="koboSpan" id="kobo.453.1">-fold </span><strong class="bold"><span class="koboSpan" id="kobo.454.1">cross-validation</span></strong><span class="koboSpan" id="kobo.455.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.456.1">CV</span></strong><span class="koboSpan" id="kobo.457.1">) to split a dataset into </span><em class="italic"><span class="koboSpan" id="kobo.458.1">k</span></em><span class="koboSpan" id="kobo.459.1"> chunks and use one chunk at a time as a validation set and the rest as the training set. </span><span class="koboSpan" id="kobo.459.2">Then, the average of the performance across all </span><em class="italic"><span class="koboSpan" id="kobo.460.1">k</span></em><span class="koboSpan" id="kobo.461.1"> chunks can be used as a validation set to calculate the validation’s performance. </span><span class="koboSpan" id="kobo.461.2">Training performance is important for finding optimal values </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.462.1">for model parameters based on the objective of the model. </span><span class="koboSpan" id="kobo.462.2">You can also use validation performance to identify optimal hyperparameter values. </span><span class="koboSpan" id="kobo.462.3">If you specify one validation set or use </span><em class="italic"><span class="koboSpan" id="kobo.463.1">k</span></em><span class="koboSpan" id="kobo.464.1">-fold CV, you can use the validation performance of different hyperparameter combinations to identify the best one. </span><span class="koboSpan" id="kobo.464.2">Then, the best hyperparameter set can be used to train the model on all data, excluding test data, so that you can come up with the final model to be tested in the </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">testing stage.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">There are </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.467.1">some common practices for each application regarding the number of folds (that is, </span><em class="italic"><span class="koboSpan" id="kobo.468.1">k</span></em><span class="koboSpan" id="kobo.469.1">) or fraction of data points to be separated as validation and test sets. </span><span class="koboSpan" id="kobo.469.2">For small datasets, 60%, 30%, and 10% are commonly used to specify the training, validation, and testing fraction of data points, respectively. </span><span class="koboSpan" id="kobo.469.3">But both the number of data points and their diversity are important factors in deciding on the number of data points within validation and test sets or specifying </span><em class="italic"><span class="koboSpan" id="kobo.470.1">k</span></em><span class="koboSpan" id="kobo.471.1"> in CV. </span><span class="koboSpan" id="kobo.471.2">You can also use available Python classes that perform training and validation using </span><em class="italic"><span class="koboSpan" id="kobo.472.1">k</span></em><span class="koboSpan" id="kobo.473.1">-fold CV with your choice of </span><em class="italic"><span class="koboSpan" id="kobo.474.1">k</span></em><span class="koboSpan" id="kobo.475.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.477.1">
from sklearn.model_selection import cross_val_score,KFoldfrom sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
# loading breast cancer dataset
X, y = load_breast_cancer(return_X_y=True)
# defining the k-fold CV
k_CV = KFold(n_splits=5)
# initializing a k nearest neighbor model
knn = KNeighborsClassifier()
# outputting validation performances using average precision across different folds of the designed CV
scores = cross_val_score(
    estimator = knn, X = X, y = y, cv = k_CV,
    scoring = 'average_precision')
print("Average cross validation score: {}".format(
    round(scores.mean(),4)))</span></pre>
<p><span class="koboSpan" id="kobo.478.1">This </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.479.1">returns</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.480.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.482.1">
Average Cross Validation score: 0.9496</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.483.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.484.1">Preferably, the data you prepared in each of these stages shouldn’t just get dumped in the cloud or a hard drive, or get added to a database after each of the previous steps in a life cycle. </span><span class="koboSpan" id="kobo.484.2">It is beneficial to have a report attached to the data to track historical efforts in each step and provide that information for other individuals or teams within your team or organization. </span><span class="koboSpan" id="kobo.484.3">Proper reporting, such as on data wrangling, could provide feedback-seeking opportunities to help you improve data provided for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">lear</span><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.486.1">ning modeling.</span></span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.487.1">Model training and evaluation</span></h1>
<p><span class="koboSpan" id="kobo.488.1">The process </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.489.1">of training and validating or testing a model consists of the following three major steps if you use </span><strong class="source-inline"><span class="koboSpan" id="kobo.490.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.491.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">PyTorch</span></strong><span class="koboSpan" id="kobo.493.1"> and TensorFlow for neural </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">network modeling:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Initializing the model</span></strong><span class="koboSpan" id="kobo.496.1">: Initializing a model is about specifying the method, its hyperparameters, and the random state to be used </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">for modeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.498.1">Training the model</span></strong><span class="koboSpan" id="kobo.499.1">: In model training, the initialized model in </span><em class="italic"><span class="koboSpan" id="kobo.500.1">Step 1</span></em><span class="koboSpan" id="kobo.501.1"> gets used on the training data to train a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">learning model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.503.1">Inference, assignment, and performance assessment</span></strong><span class="koboSpan" id="kobo.504.1">: In this step, the trained model can be used for inference (for example, predicting outputs) in supervised learning or, for example, assigning new data points to identified clusters in unsupervised learning. </span><span class="koboSpan" id="kobo.504.2">In supervised learning, you can use these predictions for model </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">performance assessment.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.506.1">These steps are similar for both supervised learning and unsupervised learning models. </span><span class="koboSpan" id="kobo.506.2">In </span><em class="italic"><span class="koboSpan" id="kobo.507.1">Steps 1</span></em><span class="koboSpan" id="kobo.508.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.509.1">2</span></em><span class="koboSpan" id="kobo.510.1">, both types of models can be trained. </span><span class="koboSpan" id="kobo.510.2">Python’s implementation of these three steps using </span><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.512.1"> is provided in the following code snippets for the random forest classifier and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.513.1">k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">-means clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">First, let’s import the required libraries and load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.517.1"> breast </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">cancer dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.519.1">
from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split
from sklearn import metrics
# loading breast cancer dataset
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.30, random_state=5)</span></pre>
<p><span class="koboSpan" id="kobo.520.1">Now, we can use a random </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.521.1">forest to train and test a supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">learning model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.523.1">
from sklearn.ensemble import RandomForestClassifier# initializing a random forest model
rf_model = RandomForestClassifier(n_estimators=10,
    max_features=10, max_depth=4)
# training the random forest model using training set
rf_model.fit(X_train, y_train)
# predicting values of test set using the trained random forest model
y_pred_rf = rf_model.predict(X_test)
# assessing performance of the model on test setprint("Balanced accuracy of the predictions:",
    metrics.balanced_accuracy_score(y_test, y_pred_rf))</span></pre>
<p><span class="koboSpan" id="kobo.524.1">This code prints out the following performance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">test set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.526.1">
Balanced accuracy of the predictions: 0.9572</span></pre> <p><span class="koboSpan" id="kobo.527.1">We can also build a </span><em class="italic"><span class="koboSpan" id="kobo.528.1">k</span></em><span class="koboSpan" id="kobo.529.1">-means clustering model, </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.531.1">
from sklearn import cluster# initializing a random forest model
kmeans_model = cluster.KMeans(n_clusters=2, n_init = 10)
# training the kmeans clustering model using training set
kmeans_model.fit(X_train)
# assigning new observations, that are test set datapoints here, to the identified clusters
y_pred_kmeans = kmeans_model.predict(X_test)</span></pre>
<p><span class="koboSpan" id="kobo.532.1">If you don’t have </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.533.1">enough experience in machine learning modeling, the methodologies and corresponding Python classes provided in </span><em class="italic"><span class="koboSpan" id="kobo.534.1">Table 2.2</span></em><span class="koboSpan" id="kobo.535.1"> could be a good </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">starting point:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.537.1">Type</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.538.1">Method</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.539.1">Python Class</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="6">
<p><span class="No-Break"><span class="koboSpan" id="kobo.540.1">Classification</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.541.1">Logistic regression</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">sklearn.linear_model.LogisticRegression()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.543.1">K-nearest neighbors</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">sklearn.neighbors.KNeighborsClassifier()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.545.1">Support vector </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">machine classifier</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.547.1">sklearn.svm.SVC()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.548.1">Random </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">forest classifier</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.550.1">sklearn.ensemble.RandomForestClassifier()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.551.1">XGBoost classifier</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.552.1">xgboost.XGBClassifier()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.553.1">LightGBM classifier</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.554.1">Lightgbm.LGBMClassifier()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="5">
<p><span class="No-Break"><span class="koboSpan" id="kobo.555.1">Regression</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.556.1">Linear regression</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">sklearn.linear_model.LinearRegression()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.558.1">Support vector </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">machine regressor</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">sklearn.svm.SVR()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.561.1">Random </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">forest regressor</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">sklearn.ensemble.RandomForestRegressor()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.564.1">XGBoost regressor</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">xgboost.XGBRegressor()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.566.1">LightGBM regressor</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">Lightgbm.LGBMRegressor()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="4">
<p><span class="No-Break"><span class="koboSpan" id="kobo.568.1">Clustering</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.569.1">K-means clustering</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">sklearn.cluster.KMeans()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.571.1">Agglomerative clustering</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">sklearn.cluster.AgglomerativeClustering()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.573.1">DBSCAN clustering</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">sklearn.cluster.DBSCAN()</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.575.1">UMAP</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">umap.UMAP()</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.577.1">Table 2.2 – Starting methods and their Python classes for your supervised learning or clustering problems with tabular data</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.578.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.579.1">UMAP is a dimensionality reduction approach that provides lower dimensional visualization, such as a 2D plot of a series of data points. </span><span class="koboSpan" id="kobo.579.2">The resulting groups of data points in the lower dimensional space can also be used as </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">reliable clusters.</span></span></p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.581.1">Testing the code and the model</span></h1>
<p><span class="koboSpan" id="kobo.582.1">Although the </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.583.1">performance of a machine learning model that is selected and brought to this stage of the life cycle can be further tested using </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.584.1">one or multiple datasets, there are a series of tests that need to be done in this stage to make sure </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">of this:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.586.1">Ensuring the process of deployment and bringing the model into production </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">goes smoothly</span></span></li>
<li><span class="koboSpan" id="kobo.588.1">Ensuring the model will work as expected from a performance and computational </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">cost perspective</span></span></li>
<li><span class="koboSpan" id="kobo.590.1">Ensuring that using the model in production will not have legal and </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">financial implications</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.592.1">Here are some such tests that can be used in </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">this stage:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.594.1">Unit tests</span></strong><span class="koboSpan" id="kobo.595.1">: These </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.596.1">are fast tests that make sure our code runs correctly. </span><span class="koboSpan" id="kobo.596.2">These</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.597.1"> tests are not specific to machine learning modeling and not even to this stage. </span><span class="koboSpan" id="kobo.597.2">Throughout the life cycle, you need to design unit tests to make sure your data processing and modeling code runs </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">as expected.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.599.1">A/B testing</span></strong><span class="koboSpan" id="kobo.600.1">: This </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.601.1">type of testing helps you, your team, and your</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.602.1"> organization in deciding whether to select a model or reject it. </span><span class="koboSpan" id="kobo.602.2">The idea of this test is to assess two possible scenarios, such as two models, or two different designs of the frontend, and check which one is more favorable. </span><span class="koboSpan" id="kobo.602.3">But you need to quantitatively assess the result by deciding </span><em class="italic"><span class="koboSpan" id="kobo.603.1">what needs to be measured</span></em><span class="koboSpan" id="kobo.604.1"> and your </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.605.1">selection criteria</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.607.1">Regression tests</span></strong><span class="koboSpan" id="kobo.608.1">: This</span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.609.1"> type of test assesses whether your code</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.610.1"> and model perform as expected after a change in dependencies and environment variables. </span><span class="koboSpan" id="kobo.610.2">For example, if your version of Python, </span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.612.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">PyTorch</span></strong><span class="koboSpan" id="kobo.614.1">, or TensorFlow changes, this test makes sure your code runs and checks the effects of those changes on model performance </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">and predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.616.1">Security tests</span></strong><span class="koboSpan" id="kobo.617.1">: Security testing</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.618.1"> is an important part of programming </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.619.1">and modeling at an industrial level. </span><span class="koboSpan" id="kobo.619.2">You need to make sure your code and dependencies are not vulnerable. </span><span class="koboSpan" id="kobo.619.3">However, you need to design a test for advanced adversarial attacks. </span><span class="koboSpan" id="kobo.619.4">We will discuss this in </span><a href="B16369_03.xhtml#_idTextAnchor119"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.620.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.621.1">, </span><em class="italic"><span class="koboSpan" id="kobo.622.1">Debugging toward </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.623.1">Responsible AI</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Responsible AI test</span></strong><span class="koboSpan" id="kobo.626.1">: We </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.627.1">need to design tests to assess </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.628.1">the important factors of responsible AI, such as transparency, privacy, and fairness. </span><span class="koboSpan" id="kobo.628.2">We will go through some important aspects of responsible AI in the </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">next chapter.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.630.1">Although</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.631.1"> these </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.632.1">kinds of tests need to be designed for this stage, similar ones could be integrated as part of previous steps of the life cycle. </span><span class="koboSpan" id="kobo.632.2">For example, you can have security testing in all steps of the life cycle, especially if you are using different tools or code bases. </span><span class="koboSpan" id="kobo.632.3">There could be other tests such as checking the memory size and prediction runtime of a model or whether the format and structure of data in production and what is expected in the deployed model </span><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.633.1">are </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">the same.</span></span></p>
<h1 id="_idParaDest-62"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.635.1">Model deployment and monitoring</span></h1>
<p><span class="koboSpan" id="kobo.636.1">If you are new to </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.637.1">deployment, you might think of it as how to develop a frontend, mobile application, or API for end users of your models. </span><span class="koboSpan" id="kobo.637.2">But that is not what we want to talk about in this book. </span><span class="koboSpan" id="kobo.637.3">There are two important aspects of deployment that we want to cover here and in future chapters: the actions needed to provide a model in production and integrating a model into a process that is supposed to benefit </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">the users.</span></span></p>
<p><span class="koboSpan" id="kobo.639.1">When you deploy your model, your code should run properly in the designated environment and have access to the required hardware, such as the GPU, and users’ data needs to be accessible in the right format for your model to work. </span><span class="koboSpan" id="kobo.639.2">Some of the tests that we talked about in the </span><em class="italic"><span class="koboSpan" id="kobo.640.1">testing</span></em><span class="koboSpan" id="kobo.641.1"> stage of the life cycle make sure that your model runs as expected in the </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">production environment.</span></span></p>
<p><span class="koboSpan" id="kobo.643.1">When we talk about providing a model in a production environment, it either gets used behind the scenes for the benefit of the user, such as when Netflix and Amazon Prime suggest movies to you using their machine learning models, or gets used directly by the user as a standalone process or as part of a bigger system, such as when machine learning models get used in hospitals to help clinicians in disease diagnosis. </span><span class="koboSpan" id="kobo.643.2">The considerations for these two different use cases are not the same. </span><span class="koboSpan" id="kobo.643.3">If you want to deploy a model in hospitals to be used directly by clinicians, you need to consider all the difficulties and planning needed to set up the proper production environment and all the software dependencies. </span><span class="koboSpan" id="kobo.643.4">You also need to make sure their local system has the necessary hardware requirements. </span><span class="koboSpan" id="kobo.643.5">Alternatively, you can provide your model through web applications. </span><span class="koboSpan" id="kobo.643.6">In this case, you need to ensure the security and privacy of the data that gets uploaded into </span><a id="_idTextAnchor112"/><span class="No-Break"><span class="koboSpan" id="kobo.644.1">your database.</span></span></p>
<p><span class="koboSpan" id="kobo.645.1">Model mentoring is a critical part </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.646.1">of the machine learning life cycle when it comes to collecting the necessary information and feedback. </span><span class="koboSpan" id="kobo.646.2">This feedback can then be used to improve or correct the data that’s used for modeling or improve the model’s training and testing. </span><span class="koboSpan" id="kobo.646.3">Monitoring machine learning models helps us ensure that the models in production provide predictions according to expectations. </span><span class="koboSpan" id="kobo.646.4">Three of the issues that could cause unreliable predictions by a machine learning model are data variance, data drift, and concept drift. </span><span class="koboSpan" id="kobo.646.5">Data drift and concept drift are considered two different types of model drift. </span><span class="koboSpan" id="kobo.646.6">Model drift is about different kinds of changes in the data, either features or output variables, that make predictions of a model irrelevant or ineffective on the</span><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.647.1"> new </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">user data.</span></span></p>
<p><span class="koboSpan" id="kobo.649.1">We will talk more about model deployment and monitoring and the engineering aspects of the machine learning life cycles in future chapters of this book, such as </span><a href="B16369_10.xhtml#_idTextAnchor286"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.650.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.651.1">, </span><em class="italic"><span class="koboSpan" id="kobo.652.1">Versioning and Reproducible Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.653.1">Learning Modeling</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">.</span></span></p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.655.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.656.1">In this chapter, we talked about different components of a machine learning life cycle, from data collection and selection to model training and evaluation and, finally, model deployment and monitoring. </span><span class="koboSpan" id="kobo.656.2">We also showed how modularizing the data processing, modeling, and deployment aspects of the machine learning life cycle helps in identifying opportunities for improving machine </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">In the next chapter, you will learn about concepts beyond improving the performance of machine learning models, such as impartial modeling and fairness, accountability, and transparency toward achieving responsi</span><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.659.1">ble </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">AI systems.</span></span></p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.661.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.662.1">Can you provide two examples of data </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">cleaning processes?</span></span></li>
<li><span class="koboSpan" id="kobo.664.1">Can you explain the difference between the one-hot and label </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">encoding methods?</span></span></li>
<li><span class="koboSpan" id="kobo.666.1">How can you use quantiles of a distribution to detect </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">its outliers?</span></span></li>
<li><span class="koboSpan" id="kobo.668.1">What comes to your mind regarding the differences between the considerations of deploying a model locally for doctors versus deploying models behind chatbots in a </span><a id="_idTextAnchor117"/><span class="No-Break"><span class="koboSpan" id="kobo.669.1">banking system?</span></span></li>
</ol>
<h1 id="_idParaDest-65"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.670.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.671.1">Micci-Barreca, Daniele. </span><em class="italic"><span class="koboSpan" id="kobo.672.1">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</span></em><span class="koboSpan" id="kobo.673.1">. </span><span class="koboSpan" id="kobo.673.2">ACM SIGKDD Explorations Newsletter 3.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">2001): 27-32.</span></span></li>
<li><span class="koboSpan" id="kobo.675.1">Basu, Anirban, </span><em class="italic"><span class="koboSpan" id="kobo.676.1">Software Quality Assurance, Testing and Metrics</span></em><span class="koboSpan" id="kobo.677.1">, PRENTICE HALL, January </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">1, 2015.</span></span></li>
</ul>
</div>
</body></html>