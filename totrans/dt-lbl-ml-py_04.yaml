- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Image Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to explore image data using various packages
    and libraries in Python. We will also see how to visualize images using Matplotlib
    and analyze image properties using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Image data is widely used in machine learning, computer vision, and object detection
    across various real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter is divided into three key sections covering visualizing image data,
    analyzing image size and aspect ratios, and performing transformations on images.
    Each section focuses on a specific aspect of image data analysis, providing practical
    insights and techniques to extract valuable information.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, *Visualizing image data*, we will utilize the Matplotlib,
    Seaborn, **Python Imaging Library** (**PIL**), and NumPy libraries and explore
    techniques such as plotting histograms of pixel values for grayscale images, visualizing
    color channels in RGB images, adding annotations to enhance image interpretation,
    and performing image segmentation. Additionally, we will dive into feature extraction
    using the **Histogram of Oriented Gradients** (**HOG**). Through practical examples
    and hands-on exercises, this section equips you with essential skills for visually
    analyzing and interpreting image data using Python libraries. Whether you’re a
    beginner or seeking to deepen your image processing expertise, this section provides
    valuable insights and practical knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to the second *Analyzing image size and aspect ratio* section, we
    delve into the importance of understanding the dimensions and proportions of images.
    We demonstrate how Python libraries such as **Python Imaging Library** (**PIL**)
    and OpenCV can be utilized to extract and analyze image size and aspect ratios.
    By studying these attributes, we can derive meaningful insights about the composition
    and structure of images, which can inform data-labeling decisions and contribute
    to accurate classification or object detection tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The final *Performing transformations on images* section explores the concept
    of data augmentation through transformations. We delve into how various image
    transformations, such as rotations, translations, and shearing, can be applied
    using libraries such as OpenCV and scikit-image. These transformations not only
    enhance the diversity and size of the dataset but also enable the creation of
    augmented images that capture different orientations, perspectives, or variations.
    We discuss how these transformed images can be leveraged for data labeling and
    improving model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we emphasize the practical implementation of these techniques
    using Python. By leveraging the rich ecosystem of image processing libraries and
    visualization tools, we empower readers to perform exploratory data analysis specifically
    tailored for image datasets. The insights gained from visualizing image data,
    analyzing size and aspect ratios, and performing transformations lay a strong
    foundation for effective data labeling and building robust machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are an aspiring data scientist, an image processing enthusiast,
    or a professional looking to enhance your data labeling skills, this chapter provides
    valuable guidance and hands-on examples to explore, analyze, and label image data
    effectively using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we will have covered the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing image size and aspect ratios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing transformations on images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll need VS Code, Keras, CV2, and OpenCV. A Python notebook
    with the example code used in this chapter can be downloaded from [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code//Ch04](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code//Ch04).
  prefs: []
  type: TYPE_NORMAL
- en: You will find the results of all code blocks in the notebook in this GitHub
    repository. As well as this, you will need the environment setup outlined in the
    *Preface* of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing image data using Matplotlib in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore the power of visualization tools and techniques
    to gain meaningful insights into the characteristics and patterns of image data.
    Using Python libraries such as Matplotlib and Seaborn, we learn how to create
    visualizations that showcase image distributions, class imbalances, color distributions,
    and other essential features. By visualizing the image data, we can uncover hidden
    patterns, detect anomalies, and make informed decisions for data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis** (**EDA**) is an important step in the process
    of building computer vision models. In EDA, we analyze the image data to understand
    its characteristics and identify patterns and relationships that can inform our
    modeling decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some real-world examples of image data analysis and AI applications are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autonomous vehicles**: Image data plays a crucial role in enabling autonomous
    vehicles to perceive their surroundings. Cameras mounted on vehicles capture images
    of the road and surroundings, and machine learning algorithms analyze these images
    to detect and recognize objects such as pedestrians, vehicles, and traffic signs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical image analysis**: In the field of medical imaging, machine learning
    is used for tasks such as tumor detection, organ segmentation, and disease diagnosis.
    Radiological images, such as X-rays, MRIs, and CT scans, are analyzed to identify
    anomalies and assist healthcare professionals in making informed decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retail and e-commerce**: Object detection is employed in retail for inventory
    management and customer experience improvement. For example, automated checkout
    systems use computer vision to recognize and tally products in a shopping cart,
    enhancing the efficiency of the checkout process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and surveillance**: Image data is utilized in security systems for
    surveillance and threat detection. Machine learning models can analyze video feeds
    to identify and alert authorities about suspicious activities, intruders, or unusual
    behavior in public spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial recognition**: Facial recognition technology relies on image data
    to identify and verify individuals. This is used in various applications, including
    smartphone authentication, access control systems, and law enforcement for criminal
    identification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented Reality (AR)**: AR applications overlay digital information onto
    the real world. Image data is essential for tracking and recognizing objects and
    surfaces, enabling realistic and interactive AR experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control in manufacturing**: Computer vision is employed in manufacturing
    to inspect products for defects and ensure quality. Automated systems analyze
    images of products on assembly lines, identifying any deviations from the desired
    specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Satellite image analysis**: Satellite imagery is used for various purposes,
    including land cover classification, environmental monitoring, and disaster response.
    Machine learning algorithms can analyze satellite images to identify changes in
    landscapes, detect deforestation, or assess the impact of natural disasters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These examples illustrate the diverse applications of image data in machine
    learning, computer vision, and object detection, showcasing its significance in
    solving real-world problems across different domains.
  prefs: []
  type: TYPE_NORMAL
- en: The following are some steps to follow when conducting EDA for image data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in any EDA process is to load the image data into your **Integrated
    Development Environment** (**IDE**) workspace, such as VS Code, Jupyter Notebook,
    or any other Python editor. Depending on the format of the data, you may need
    to use a library such as OpenCV or PIL to read in the images.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to check the dimensions of the images. Image dimensions can
    affect the performance of your model, as larger images require more memory and
    computation. You should also check that all the images have the same dimensions,
    as this is a requirement for most computer vision models. If the images are not
    of the same size, then preprocessing is required to convert them to the same size.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization is a powerful tool for understanding image data. You can use the
    Matplotlib or Seaborn libraries to visualize the data in various ways. You can
    plot histograms of pixel values to see their distributions or use scatter plots
    to visualize the relationship between pixel values. We will cover this later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers can have a significant impact on your model’s performance. You should
    check for outliers in your image data by plotting boxplots and examining the distribution
    of pixel values. In the context of image data, outliers are data points (in this
    case, images) that significantly deviate from the expected or normal distribution
    of the dataset. Outliers in image data are images that have distinct characteristics
    or patterns that are different from the majority of images in the dataset. Images
    with pixel values that are much higher or lower than the typical range for the
    dataset can be considered outliers. These extreme values might be due to sensor
    malfunctions, data corruption, or other anomalies. Images with color distributions
    that significantly differ from the expected color distributions of the dataset
    can be considered outliers. These might be images with unusual color casts, saturation,
    or intensity.
  prefs: []
  type: TYPE_NORMAL
- en: Performing data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Preprocessing is an important step in EDA, as it can help to reduce noise and
    improve the quality of the images. Common preprocessing techniques include resizing,
    normalization, data augmentation, image segmentation, and feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: In image data, preprocessing involves several steps.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Image resizing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in preprocessing image data is resizing the images. Image resizing
    is essential because we need all the images to be of the same size. If we do not
    make sure to resize the images, we may end up with images of different sizes,
    which can lead to issues during training.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Image normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step in preprocessing image data is normalization. Normalization is
    essential because it helps to reduce the effect of lighting and color variations
    on the images. Normalization involves scaling the pixel values of the images to
    a specific range. The most common method of normalization is to scale the pixel
    values to the range [0,1]. Scaling pixel values to the range [0, 1] during image
    dataset normalization has several significant advantages and implications that
    make it a common and effective practice in various image processing and machine
    learning tasks. Here’s why this range is significant. Normalizing images to a
    common range ensures that all pixel values across different images have the same
    scale. This makes it easier for algorithms to compare and process images, as they
    don’t need to deal with varying pixel value ranges. The range [0, 1] is well suited
    for numerical stability in computations. Many machine learning algorithms and
    image processing techniques work best when dealing with values that are not too
    large or too small. Scaling to [0, 1] helps prevent numerical instability and
    issues such as exploding gradients during training.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Image augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image augmentation is a technique used to increase the size of the training
    dataset by creating additional images. Image augmentation involves applying various
    transformations to the original images, such as rotation, flipping, zooming, and
    shearing. It is used in image classification and object detection tasks. Image
    augmentation is essential because it helps to reduce overfitting and improves
    the generalization of the model. Overfitting is a common problem in machine learning
    and deep learning where a model learns the training data so well that it starts
    capturing noise and random fluctuations in the data instead of the underlying
    patterns. It helps produce robust models. Excessive augmentation can lead to unrealistic
    models or overfitting, which can result in reduced generalization ability, limiting
    the model’s usefulness in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding more training data is one way to help reduce overfitting. However, in
    many situations, collecting a large amount of new, diverse data can be impractical
    or expensive. This is where data augmentation comes in. Data augmentation involves
    applying various transformations to the existing training data to artificially
    increase its size and diversity. Here’s how data augmentation helps reduce overfitting,
    particularly in the context of image datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved generalization**: Augmentation helps the model generalize better
    to unseen data by exposing it to a diverse range of transformations. This can
    enhance the model’s ability to handle variations in object appearance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness to variations**: Models trained with augmented data are often
    more robust to changes in lighting, orientation, and other factors that may be
    present in real-world scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data efficiency**: Augmentation allows for the creation of a larger effective
    training dataset without collecting additional labeled samples. This can be particularly
    beneficial when the available labeled data is limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mitigating overfitting**: Augmentation introduces variability, helping to
    prevent overfitting. Models trained on augmented data are less likely to memorize
    specific training examples and are more likely to learn generalizable features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Considerations**: While augmentation is generally beneficial, it’s essential
    to apply transformations that make sense for the specific task. For example, randomly
    flipping images horizontally makes sense for many tasks, but randomly rotating
    images might not be suitable for tasks with strict orientation requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Image segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image segmentation is the process of dividing an image into multiple meaningful
    segments or regions. Image segmentation is essential in medical image analysis,
    where we need to identify the different organs or tissues in the image. Image
    segmentation is also used in object detection, where we need to identify the different
    objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature extraction is the process of extracting relevant features or information
    from the image data. Feature extraction is essential because it helps to reduce
    the dimensionality of the image data, which can improve the performance of machine
    learning algorithms. Feature extraction involves applying various filters to the
    images, such as edge detection, texture analysis, and color segmentation. Examples
    of color features are color histograms that represent the distribution of color
    intensities in an image. Similarly, shape features include the Hough transform
    that detects and represents shapes such as lines and circles.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, data exploration and preprocessing are essential steps in the
    machine learning pipeline. In image data, we need to resize the images, normalize
    the pixel values, apply image augmentation, perform image segmentation, and extract
    relevant features from the images. By following these preprocessing steps, we
    can improve the performance of the machine learning algorithm and achieve better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for class imbalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many image classification problems, the classes may not be evenly represented
    in the dataset. You should check for class imbalance by counting the number of
    images in each class and visualizing the distribution of classes. If there is
    an imbalance, we augment the minority class data by applying transformations such
    as rotations, flips, crops, and color variations. This increases the diversity
    of the minority class without needing to generate entirely new samples.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying patterns and relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of EDA is to identify patterns and relationships in the data that can
    inform your modeling decisions. You can use techniques such as clustering to identify
    patterns in the data or examine the relationship between different features using
    scatter plots or correlation matrices. Clustering, in the context of image dataset
    analysis, is a technique used to group similar images together based on their
    inherent patterns and characteristics. It’s a data exploration method that aids
    in understanding the structure of image data by identifying groups or clusters
    of images that share similar visual traits. Clustering algorithms analyze the
    visual properties of images, such as pixel values or extracted features, to group
    images that are visually similar into clusters. Images that share common visual
    traits are grouped together, forming distinct clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the impact of preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, you should evaluate the impact of preprocessing on your image data.
    You can compare the performance of your model on preprocessed and unprocessed
    data to determine the effectiveness of your preprocessing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, EDA is an important step in the process of building computer vision
    models. By visualizing the data, checking for outliers and class imbalance, identifying
    patterns and relationships, and evaluating the impact of preprocessing, you can
    gain a better understanding of your image data and make informed decisions about
    your modeling approach.
  prefs: []
  type: TYPE_NORMAL
- en: Practice example of visualizing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see an example of visualizing image data using Matplotlib. In the following
    code, we first load the image using the PIL library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we convert it to a NumPy array using the `np.array` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, plot the result with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.1\uFEFF – Visualizing image data](img/B18944_04_1.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Visualizing image data
  prefs: []
  type: TYPE_NORMAL
- en: We then use the `imshow` function from Matplotlib to plot the image. Converting
    images to NumPy arrays during EDA offers several benefits that make data manipulation,
    analysis, and visualization more convenient and efficient. NumPy is a powerful
    numerical computing library in Python that provides support for multi-dimensional
    arrays and a wide range of mathematical operations. Converting images to NumPy
    arrays is common during EDA as NumPy arrays provide direct access to individual
    pixels in an image, making it easier to analyze pixel values and perform pixel-level
    operations. Many data analysis and visualization libraries in Python, including
    Matplotlib and scikit-learn, work seamlessly with NumPy arrays. This allows you
    to take advantage of a rich ecosystem of tools and techniques for image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to visualize image data using Matplotlib. We’ll
    now review a few commonly encountered examples.
  prefs: []
  type: TYPE_NORMAL
- en: '`cmap` parameter of the `imshow` function to `''gray''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure is the result of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.2\uFEFF – Gr\uFEFFayscale image](img/B18944_04_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Grayscale image
  prefs: []
  type: TYPE_NORMAL
- en: '**Histogram of pixel values**: We can use a histogram to visualize the distribution
    of pixel values in an image. This can help us understand the overall brightness
    and contrast of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.3\uFEFF – Histogram of pixel values](img/B18944_04_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Histogram of pixel values
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple images side by side**: We can use subplots to display multiple images
    side by side for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the stunning result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.4\uFEFF – Multiple images side by side](img/B18944_04_4.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Multiple images side by side
  prefs: []
  type: TYPE_NORMAL
- en: '**Color channel visualization**: For color images, we can plot each color channel
    separately to see how they contribute to the overall image. In an image dataset,
    a color channel refers to a single component of color information in each pixel
    of an image. Color images are composed of multiple color channels, where each
    channel represents a specific color aspect or color space. The combination of
    these color channels creates the full-color representation of an image. Common
    color spaces include **Red, Green, Blue** (**RGB**), **Hue, Saturation, Value**
    (**HSV**), and **Cyan, Magenta, Yellow,** **Key/Black** (**CMYK**).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, RGB color channels are visualized using the appropriate colormap
    to represent their respective colors. When visualizing individual color channels
    (red, green, and blue) separately, it’s common to use colormaps that highlight
    the specific color information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are typical colormaps used for visualizing individual RGB channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''Reds''` colormap is often used to visualize the red channel. It ranges from
    dark to light red, with the darker values representing lower intensity and the
    lighter values representing higher intensity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''Greens''` colormap is commonly used to visualize the green channel. Similar
    to `''Reds''`, it ranges from dark to light green.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''Blues''` colormap is used for visualizing the blue channel. It ranges from
    dark to light blue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of how you might visualize individual RGB channels using
    these colormaps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we see the following channels:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.5\uFEFF – Color channel visualization](img/B18944_04_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Color channel visualization
  prefs: []
  type: TYPE_NORMAL
- en: Practice example for adding annotations to an image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can add annotations to an image to highlight specific regions of interest,
    such as marking key features within an image, perhaps facial landmarks on a person’s
    face (eyes, nose, mouth), to emphasize important attributes for analysis or recognition.
    Annotations can also be used to highlight regions that exhibit anomalies, defects,
    or irregularities in industrial inspection images, medical images, and quality
    control processes, along with identifying and marking specific points of interest,
    such as landmarks on a map. Let’s see annotations at work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result as output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.6\uFEFF – Image annotation](img/B18944_04_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Image annotation
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few examples of the many ways that we can use Matplotlib to
    visualize image data. With some creativity and experimentation, we can create
    a wide variety of visualizations to help us understand our image data better.
  prefs: []
  type: TYPE_NORMAL
- en: Practice example of image segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following simple code snippet demonstrates how to perform basic image segmentation
    using the CIFAR-10 dataset and a simple thresholding technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.7\uFEFF – Image segmentation](img/B18944_04_7.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Image segmentation
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a basic thresholding technique to segment the image based
    on pixel intensity values.
  prefs: []
  type: TYPE_NORMAL
- en: Practice example for feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature extraction from an image dataset such as CIFAR-10 involves transforming
    raw image data into a set of relevant features that can be used as input for machine
    learning models. Here’s a simple example using the **Histogram of Oriented Gradients**
    (**HOG**) feature extraction technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.8\uFEFF – HOG feature extraction](img/B18944_04_8.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – HOG feature extraction
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a picture, and you want to understand what’s in the picture
    by looking at the patterns of lines and edges. HOG is a way to do that by focusing
    on the directions of lines and edges in an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code block, the `hog` function internally performs the following
    four steps to generate the HOG image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Divide the image into small cells**: First, the function takes the image
    and divide it into small boxes called cells. Think of these like little squares
    placed over the image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculate gradients**: Inside each cell, we look at how the colors change.
    If the colors change significantly, it means there’s probably an edge or a line.
    We figure out the direction of this color change, and this is called a gradient.
    Imagine drawing little arrows to show the directions of these color changes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Group arrows into directions**: Now, we group these little arrows with similar
    directions together. This is like saying, “Hey, there are a lot of edges going
    this way, and a lot of edges going that way.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Make a histogram of the directions**: A histogram is like a chart that shows
    how many times something happens. Here, we make a histogram that shows how many
    arrows are pointing in each direction. This tells us which directions of edges
    and lines are more common in that cell.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we have seen how to visualize the image data and plot various
    features including color pixel histograms, grayscale images, RGB color channels,
    image segmentation, and annotations on images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will examine the importance of image size and aspect
    ratio distribution in image data models.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing image size and aspect ratio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is very important to understand the distribution of image sizes and aspect
    ratios in image data models.
  prefs: []
  type: TYPE_NORMAL
- en: Aspect ratio, in the context of image dataset EDA, refers to the proportional
    relationship between the width and height of an image. It’s a numerical representation
    that helps describe the shape of an image. Aspect ratio is especially important
    when working with images, as it provides insights into how elongated or compressed
    an image appears visually. Mathematically, the aspect ratio is calculated by dividing
    the width of the image by its height. It’s typically expressed as a ratio or a
    decimal value. A square image has an aspect ratio of 1:1, while a rectangular
    image would have an aspect ratio different from 1:1.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of aspect ratios on model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s understand the impact of aspect ratios on the model performance using
    the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object recognition**: In object recognition tasks, maintaining the correct
    aspect ratio is essential for accurate detection. If the aspect ratio is distorted
    during preprocessing or augmentation, it may lead to misinterpretation of object
    shapes by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training stability**: Ensuring consistent aspect ratios across the training
    dataset can contribute to training stability. Models may struggle if they encounter
    variations in aspect ratios that were not present in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bounding-box accuracy**: In object detection, bounding boxes are often defined
    by aspect ratios. Deviations from the expected aspect ratios can impact the accuracy
    of bounding box predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider a scenario where we have an image represented by a matrix with
    dimensions *M×N*, where *M* is the number of rows (height) and *N* is the number
    of columns (width). The image size, aspect ratio, and pixel aspect ratio can be
    calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`300×200`, the image size would be `300×200=60,000` pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`300×200`, the aspect ratio would be 200/300, which simplifies to 2/3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pixel Aspect Ratio (PAR)**: It is the ratio of the width of a pixel to its
    height. This is especially relevant when dealing with non-square pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PAR = *Height of pixel*/*Width* *of pixel*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example: If the pixel aspect ratio is 3/4, it means that the width of a pixel
    is three-quarters of its height.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These mathematical examples provide a basic understanding of how image size,
    aspect ratio, and pixel aspect ratio can be calculated using simple formulas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s delve into the concepts of padding, cropping, and aspect ratio evaluation
    metrics in the context of image data analysis in machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '`200×200` pixels, and you want to apply a `3×3` convolutional filter. Without
    padding, the output size would be `198×198`. To maintain the spatial size, you
    can add a border of one pixel around the image, resulting in a `202×202` image
    after padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`300×300` pixels and you decide to crop the central region, you might end up
    with a `200×200` pixel image by removing `50` pixels from each side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aspect ratio evaluation metrics are measures used to assess the similarity between
    the aspect ratio of predicted bounding boxes and the ground truth bounding boxes
    in object detection tasks. Common metrics include **Intersection over Union**
    (**IoU**) and F1 score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In image classification, aspect ratio evaluation metrics play a crucial role
    in gauging the accuracy of predicted bounding boxes compared to the ground truth
    bounding boxes in object detection tasks. One widely employed metric is IoU, calculated
    by dividing the area of overlap between the predicted and ground truth bounding
    boxes by the total area covered by both. The resulting IoU score ranges from `0`
    to `1`, where a score of `0` indicates no overlap, and a score of `1` signifies
    perfect alignment. Additionally, the F1 score, another common metric, combines
    precision and recall, providing a balanced assessment of the model’s performance
    in maintaining accurate aspect ratios across predicted and true bounding boxes.
    These metrics collectively offer valuable insights into the effectiveness of object
    detection models in preserving the spatial relationships of objects within an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Let’s say that in an object detection task, you have a ground-truth
    bounding box with an aspect ratio of `2:1` for a specific object. If your model
    predicts a bounding box with an aspect ratio of `1.5:1`, you can use IoU to measure
    how well the predicted box aligns with the ground truth. If the IoU metric is
    high, it indicates good alignment; if it’s low, there may be a mismatch in aspect
    ratios.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and effectively applying padding, cropping, and aspect ratio evaluation
    metrics are crucial aspects of preprocessing and evaluating image data in machine
    learning models, particularly in tasks such as object detection where accurate
    bounding box predictions are essential.
  prefs: []
  type: TYPE_NORMAL
- en: Image resizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image resizing is the process of changing the dimensions of an image while preserving
    its aspect ratio. It is a common preprocessing step in computer vision applications,
    including object detection, image classification, and image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary reasons for resizing images are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To fit the image into a specific display size or aspect ratio, such as for web
    pages or mobile applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reduce the computational complexity of processing the image, such as for
    real-time computer vision applications or when the image size is too large to
    fit into memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When resizing an image, we need to decide on a new size for the image. The new
    size can be specified in terms of pixels or as a scaling factor. In the latter
    case, we multiply the original image dimensions by a scaling factor to obtain
    the new dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two primary methods for resizing an image: interpolation and resampling:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation is a technique for estimating the pixel values in the resized
    image. It involves computing a weighted average of the pixel values in the original
    image surrounding the target pixel location. There are several interpolation methods
    available, including nearest neighbor, bilinear, bicubic, and Lanczos resampling.
  prefs: []
  type: TYPE_NORMAL
- en: Lanczos resampling is a method used in digital image processing for resizing
    or resampling images. It is a type of interpolation algorithm that aims to produce
    high-quality results, particularly when downscaling images. The Lanczos algorithm
    is named after Cornelius Lanczos, a Hungarian mathematician and physicist. The
    Lanczos resampling algorithm involves applying a sinc function (a type of mathematical
    function) to the pixel values in the original image to calculate the values of
    pixels in the resized image. This process is more complex than simple interpolation
    methods such as bilinear or bicubic, but it tends to produce better results, especially
    when reducing the size of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple example in Python using the Pillow library (a fork
    of PIL) to demonstrate nearest neighbor, bilinear, bicubic, and Lanczos resampling
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.9\uFEFF – \uFEFFThe results of each interpolation method](img/B18944_04_X_Merged_images.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The results of each interpolation method
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into the details of each interpolation method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Image.NEAREST`): This method chooses the nearest pixel value to the interpolated
    point:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample=Image.NEAREST`): Simple and fast. Often used for upscaling pixel
    art images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual effect**: Results in blocky or pixelated images, especially noticeable
    during upscaling.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Image.BILINEAR`): Uses a linear interpolation between the four nearest pixels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample=Image.BILINEAR`): Commonly used for general image resizing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual effect**: Smoother than nearest neighbor but may result in some loss
    of sharpness'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Image.BICUBIC`): Employs a cubic polynomial for interpolation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample=Image.BICUBIC`): Typically used for high-quality downsampling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual effect**: Smoother than bilinear; often used for photographic images,
    but can introduce slight blurring'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Image.LANCZOS`): Applies a `sinc` function as the interpolation kernel:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample=Image.LANCZOS`): Preferred for downscaling images and maintaining
    quality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual effect**: Generally produces the highest quality, especially noticeable
    in downscaling scenarios. May take longer to compute.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the** **right method**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality versus speed**: Nearest neighbor is the fastest but may result in
    visible artifacts. Bicubic and Lanczos are often preferred for quality, sacrificing
    a bit of speed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Downscaling versus upscaling**: Bicubic and Lanczos are commonly used for
    downscaling, while bilinear might be sufficient for upscaling.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the images do not show noticeable differences, it could be due to factors
    such as the original image’s characteristics, the magnitude of resizing, or the
    viewer’s display capabilities. Generally, for high-quality resizing, especially
    downscaling, Lanczos interpolation tends to provide superior results. If the images
    are small or the differences subtle, the choice of method may have less impact.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Resampling**: Resampling is the process of selecting a subset of the pixels
    from the original image to create the resized image. This method can result in
    loss of information or artifacts in the image due to the removal of pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Python, we can use the Pillow library for image resizing. Here is some example
    code for resizing an image using the Pillow library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.\uFEFF10 – Resized image (200*200)](img/B18944_04_9.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Resized image (200*200)
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we first open an image using the `Image.open()` function
    from the Pillow library. We then define the new size of the image as a tuple `(500,
    500)`. Finally, we call the `resize()` method on the image object with the new
    size tuple as an argument, which returns a new resized image object. We then save
    the resized image using the `save()` method with the new filename.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see one more example of resizing images using Python. We first import
    the necessary libraries: `os` for file and directory operations and `cv2` for
    image loading and manipulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the path to the image directory and get a list of all image filenames
    in the directory using a list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the new size of the images using a tuple `(224, 224)` in this example.
    You can change the tuple to any other size you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then resize the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output of the resized images in the relevant directory:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.1\uFEFF1 – Resized images in the directory](img/B18944_04_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Resized images in the directory
  prefs: []
  type: TYPE_NORMAL
- en: We loop through all the image files using a `for` loop. For each image file,
    we load the image using OpenCV (`cv2.imread()`), resize the image using `cv2.resize()`,
    and save the resized image with the same filename using `cv2.imwrite()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cv2.resize()` function takes three parameters: the image to resize, the
    new size of the image as a tuple `(width, height)`, and an interpolation method.
    The default interpolation method is `cv2.INTER_LINEAR`, which produces good results
    in most cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Resizing an image is a common preprocessing step in image classification and
    object detection tasks. It is often necessary to resize images to a fixed size
    to ensure that all images have the same size and aspect ratio, which makes it
    easier to train machine learning models on the images. Resizing can also help
    to reduce the computational cost of processing images, as smaller images require
    less memory and computing resources than larger images.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, image resizing is the process of changing the dimensions of an image
    while preserving its aspect ratio. It is a common preprocessing step in computer
    vision applications and can be performed using interpolation or resampling techniques.
    In Python, we can use the Pillow library for image resizing.
  prefs: []
  type: TYPE_NORMAL
- en: Image normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image normalization is a preprocessing technique that is commonly used in computer
    vision applications. The goal of image normalization is to transform the pixel
    values of an image that are within a certain range or have certain statistical
    properties. Normalization is used to reduce the impact of variations in lighting
    conditions or to standardize the color or brightness of images.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization techniques typically involve scaling the pixel values of an image
    to fall within a certain range or modifying the distribution of pixel values to
    have certain statistical properties. There are many different techniques for image
    normalization, and the choice of technique depends on the specific application
    and the characteristics of the image data.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some common techniques for image normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 1]` or `[-1, 1]`. This can be done using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, `min_value` and `max_value` are the minimum and maximum pixel values in
    the image, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Z-score normalization**: This technique modifies the distribution of pixel
    values in an image to have a mean of 0 and a standard deviation of 1\. This can
    be done using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, `mean_value` and `std_value` are the mean and standard deviation of the
    pixel values in the image, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Histogram equalization**: This technique modifies the distribution of pixel
    values in an image to be more uniform. This can be done by computing the **cumulative
    distribution function** (**CDF**) of the pixel values and mapping the pixel values
    to new values based on the CDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first load an image using the OpenCV library. We then
    apply histogram equalization using the `equalizeHist()` function, which returns
    a new image with a more uniform distribution of pixel values. OpenCV is a powerful
    and widely used open source library that plays a crucial role in image recognition
    and computer vision tasks. Its importance stems from its comprehensive collection
    of tools, functions, and algorithms designed to handle various aspects of image
    processing, analysis, and recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of image normalization using Python. We first import the
    necessary libraries: `os` for file and directory operations, `cv2` for image loading
    and manipulation, and `numpy` for mathematical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the path to the image directory and get a list of all image filenames
    in the directory using a list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop through all the image files using a `for` loop. For each image file,
    we load the image using OpenCV (`cv2.imread()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the image to `float32` data type using `astype(np.float32)`. This
    is necessary for the next step of normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We normalize the image pixels to have zero mean and unit variance using the
    following formula: `img -= np.mean(img); img /= np.std(img)`. This is also known
    as standardization or z-score normalization. This step is important for machine
    learning models that are sensitive to the scale of input features, as it ensures
    that the pixel values have a similar scale across all images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save the normalized image with the same filename using `cv2.imwrite()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Image normalization is a critical step in many computer vision applications,
    as it can help to reduce the impact of variations in lighting conditions and standardize
    the color and brightness of images. By transforming the pixel values of an image,
    we can make it easier for machine learning algorithms to learn from the image
    data and improve the accuracy of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Performing transformations on images – image augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of image processing and deep learning, the ability to effectively
    work with image data is paramount. However, acquiring a diverse and extensive
    dataset can be a challenge. This is where the concept of image augmentation comes
    into play. Image augmentation is a transformative technique that holds the power
    to enhance the richness of a dataset without the need to amass additional images
    manually. This section delves into the intricacies of image augmentation – an
    indispensable tool for improving model performance, enhancing generalization capabilities,
    and mitigating overfitting concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Image augmentation is a technique for artificially increasing the size of a
    dataset by generating new training examples from existing ones. It is commonly
    used in deep learning applications to prevent overfitting and improve generalization
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind image augmentation is to apply a variety of transformations
    to existing images to create new, slightly modified versions of the original images.
    By doing so, we can effectively increase the size of our dataset without having
    to collect and label new images manually. For example, in medical image analysis,
    acquiring a large number of high-quality medical images with accurate annotations
    is often difficult due to patient privacy concerns and the expertise required
    for labeling. Image augmentation techniques can help to generate diverse training
    examples to train accurate diagnostic models. Another scenario is when dealing
    with rare events or anomalies, such as defects in manufacturing or diseases in
    agriculture, where collecting a sufficient number of real-world instances can
    be challenging. Image augmentation allows the generation of various scenarios
    of these rare events, improving the model’s ability to detect them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of image augmentation techniques that can be used.
    The most commonly used techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation**: Rotating the image by a specified angle in degrees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flipping**: Flipping the image horizontally or vertically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zooming**: Zooming in or out on the image by a specified factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shearing**: Shearing the image in the *x* or *y* direction by a specified
    factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shifting**: Shifting the image horizontally or vertically by a specified
    number of pixels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques can be applied in various combinations to generate a large
    number of new images from a small set of original images. For example, we can
    rotate an image by 45 degrees, flip it horizontally, and shift it vertically,
    resulting in a new image that is quite different from the original but still retains
    some of its features.
  prefs: []
  type: TYPE_NORMAL
- en: One important consideration when using image augmentation is to ensure that
    the generated images are still representative of the underlying dataset. For example,
    if we are training a model to recognize handwritten digits, we should ensure that
    the generated images are still recognizable as digits and not some random patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, image augmentation is a powerful technique that can be used to increase
    the size of a dataset and improve the performance of deep learning models. The
    Keras library provides a convenient way to apply various image augmentation techniques
    to a dataset, as we will see in the following code example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see some example Python code for image augmentation. We first import
    the necessary libraries: `keras.preprocessing.image` for image augmentation and
    `os` for file and directory operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Import the necessary libraries for* *image augmentation*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to import the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the path to the image directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 2: Create an instance* *of ImageDataGenerator*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create an instance of the `ImageDataGenerator` class, which allows us to
    define various types of image augmentation techniques. In this example, we use
    rotation, horizontal and vertical shifts, shear, zoom, and horizontal flipping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 3: Load each image from the directory and convert the image to* *an array*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We get a list of all image filenames in the directory using a list comprehension.
    We then loop through all the image files using a `for` loop. For each image file,
    we load the image using Keras’ `load_img` function and convert it to an array
    using Keras’ `img_to_array` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We reshape the array to have a batch dimension of `1`, which is required by
    the `flow` method of the `ImageDataGenerator` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 4: Regenerate five augmented images for each* *input image*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then regenerate our augmented images as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see five augmented images generated for the flow in the GitHub repository
    in the directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.1\uFEFF2 – Augmented images](img/B18944_04_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Augmented images
  prefs: []
  type: TYPE_NORMAL
- en: We use the `flow` method to generate five augmented images for each input image.
    The `flow` method takes the array of input images, a batch size of 1, and various
    parameters defined in *step 3*. It returns a generator that generates augmented
    images on the fly. We save each augmented image with a filename prefix of `aug_`
    using the `save_to_dir`, `save_prefix`, and `save_format` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to transform a dataset using data augmentation
    and saw some commonly used data augmentation techniques for generating additional
    data for training.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to review images after loading an image dataset
    and explore them using a tool called Matplotlib in Python. We also found out how
    to change the size of pictures using two handy tools called PIL and OpenCV. And
    just when things were getting interesting, we discovered a cool trick called data
    augmentation that helps us make our dataset bigger and teaches our computer how
    to understand different versions of the same picture.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there’s more to come! In the next chapter, we are going to see how
    to label our image data using Snorkel based on rules and heuristics. Get ready
    for some more fun as we dive into the world of labeling images!
  prefs: []
  type: TYPE_NORMAL
