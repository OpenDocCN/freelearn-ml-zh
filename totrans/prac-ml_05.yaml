- en: Chapter 5. Decision Tree based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting this chapter, we will take a deep dive into each of the Machine learning
    algorithms. We begin with a non-parametric supervised learning method, Decision
    trees, and advanced techniques, used for classification and regression. We will
    outline a business problem that can be addressed by building a Decision tree-based
    model and learn how it can be implemented in Apache Mahout, R, Julia, Apache Spark,
    and Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees: definition, terminology, the need, advantages, and limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of constructing and understanding Decision trees and some key aspects
    such as Information gain and Entropy. You will also learn to build regression,
    the classification of trees and measuring errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding some common problems with Decision trees, need for pruning Decision
    trees, and techniques for pruning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn Decision tree algorithms such as CART, C4.5, C5.0 and so on;
    and specialized trees such as Random forests, Oblique trees, Evolutionary and
    Hellinger trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a business use case for classification and regression trees, and
    an implementation of the same using Apache Mahout, R, Apache Spark, and Julia
    and Python (scikit-learn) libraries and modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are known to be one of the most powerful and widely used modeling
    techniques in the field of Machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees naturally induce rules that can be used in data classification
    and prediction. Following is an example of a rule definition derived from building
    a Decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: If (laptop model is *x*) and (manufactured by *y*) and (is *z* years old) and
    (with some owners being *k*) then (the battery life is *n* hours).
  prefs: []
  type: TYPE_NORMAL
- en: When closely observed, these rules are expressed in simple, human readable,
    and comprehensible formats. Additionally, these rules can be stored for later
    reference in a data store. The following concept map depicts various characteristics
    and attributes of Decision trees that will be covered in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/B03980_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees classify instances by representing in a tree structure starting
    from the root to a leaf. Most importantly, at a high level, there are two representations
    of a Decision tree—a node and an arc that connects nodes. To make a decision,
    the flow starts at the root nodes, navigates to the arcs until it has reached
    a leaf node, and then makes a decision. Each node of the tree denotes testing
    of an attribute, and the branches denote the possible values that the attribute
    can take.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some of the characteristics of a Decision tree representation:'
  prefs: []
  type: TYPE_NORMAL
- en: Every non-leaf node (for example, a decision node) denotes a representation
    of the attribute value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every branch denotes the rest of the value representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every leaf (or terminal) node represents the value of the target attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The starting node is called the root node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure is a representation of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Terminology](img/B03980_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Purpose and uses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Decision trees are used for classification and regression. Two types of trees
    are used in this context:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification trees are used to classify the given data set into categories.
    To use classification trees, the response of the target variable needs to be a
    categorical value such as yes/no, true/false. On the other hand, regression trees
    are used to address prediction requirements and are always used when the target
    or response variable is a numeric or discrete value such as stock value, commodity
    price, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure depicts the purpose of the Decision tree and relevant tree
    category as the classification or regression tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Purpose and uses](img/B03980_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Constructing a Decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees can be learned best by taking a simple example and constructing
    a Decision tree by hand. In this section, let's look at a simple example; the
    following table shows the dataset on hand. Our target is to predict whether a
    customer will accept a loan or not, given their demographics. Clearly, it will
    be most useful for the business user if we can come out with a rule as a model
    for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a Decision tree](img/B03980_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the previous table, since age and experience are highly correlated, we
    can choose to ignore one of the attributes. This aids the feature selection implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: Let''s start building the Decision tree. To start with, we will choose
    to split by CCAvg (the average credit card balance).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a Decision tree](img/B03980_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this Decision tree, we now have two very explicit rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If CCAvg is medium then loan = accept* or *if CCAvg is high then loan = accept*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more clarity in the rules, let''s add the income attribute. We have two
    more rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If CCAvg is low and income is low, then loan is not accept*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If CCAvg is low and income is high, then loan is accept*'
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining the second rule here and the first two rules, we can derive the
    following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If (CCAvg is medium) or (CCAvg is high) or (CCAvg is low, and income is high)
    then loan = accept*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: Let''s start building the Decision tree using Family:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a Decision tree](img/B03980_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, there is just one rule that it is not giving an accurate result
    as it has only two data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, choosing a valid attribute to start the tree makes a difference to the
    accuracy of the model. From the previous example, let''s list out some core rules
    for building Decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: We usually start building Decision trees with one attribute, split the data
    based on the attribute, and continue with the same process for other attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be many Decision trees for the given problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The depth of the tree is directly proportional to the number of attributes chosen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There needs to be a Termination Criteria that will determine when to stop further
    building the tree. In the case of no termination criteria, the model will result
    in the over-fitting of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the output is always in the form of simple rule(s) that can be stored
    and applied to different datasets for classification and/or prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the reasons why Decision trees are preferred in the field of Machine
    learning is because of their robustness to errors; they can be used when there
    are some unknown values in the training datasets too (for example, the data for
    income is not available for all the records).
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the interesting ways of assigning values to some unknowns is to see that
    the most common value in terms of occurrence is assigned and in some cases they
    can belong to the same class, if possible we should bring it closer to accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another probabilistic way of doing this where the prediction is distributed
    proportionately:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign a probability *pi* for every value *vi* of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, assign the fraction *pi* of *x* to each of the descendants. These probabilities
    can be estimated again based on the observed frequencies of the various values
    for A, among the examples at node *n*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's consider a Boolean attribute *A*. Let there be 10 values
    for *A* out of which three have a value of True and the rest 7 have a value of
    False. So, the probability of *A(x) = True* is 0.3, and the probability that *A(x)
    = False* is 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: A fractional 0.3 of this is distributed down the branch for *A = True*, and
    a fractional 0.7 is distributed down the other. These probability values are used
    for computing the information gain, and can be used if a second missing attribute
    value needs to be tested. The same methodology can be applied in the case of learning
    when we need to fill any unknowns for the new branches. The C4.5 algorithm uses
    this mechanism for filling the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for constructing Decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key to constructing Decision trees is knowing where to split them. To do
    this, we need to be clear on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which attribute to start and which attribute to apply subsequently?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do we stop building the Decision tree (that is avoid over-fitting)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the appropriate attribute(s)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three different ways to identify the best-suited attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain and Entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gini index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information gain and Entropy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This entity is used in an algorithm known as C4.5\. Entropy is a measure of
    uncertainty in the data. Let us take an intuitive approach to understand the concepts
    of Information gain and Entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a coin is being tossed, and there are five coins with
    a probability for heads as 0, 0.25, 0.5, 0.75, and 1 respectively. So, if we think
    which one has the highest and which one has the lowest uncertainty, then the case
    of 0 or 1 will be the lowest certain one and highest would be when it is 0.5\.
    The following figure depicts the representation of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A mathematical representation is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: H = -∑p[i]log2p[i]
  prefs: []
  type: TYPE_NORMAL
- en: Here, p[i] is the probability of a specific state.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a system has four events with probabilities 1/2, 1/4, 1/5, and 1/8 indicate
    the total Entropy of the system as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: H = -1/2 log2(1/2)-1/4log2(1/4)-1/5log2(1/5)-1/8log2(1/8)
  prefs: []
  type: TYPE_NORMAL
- en: In the original version of the C5.0 and C4.5 algorithms (ID3), a root node was
    chosen on the basis of how much of the total Entropy was reduced if this node
    was chosen. This is called information gain.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain = Entropy of the system before split - Entropy of the system
    after split
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy in the system before split is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Entropy after using *A* to split *D* into *v* partitions to classify *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Information gained by branching on an attribute is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now compute the information gained from our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Class P accepts the loan = yes/ 1\. Class N accepts the loan = no / 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy before split is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is obvious and expected as we have almost a fifty-fifty split of the data.
    Let's now see which attribute gives the best information gain.
  prefs: []
  type: TYPE_NORMAL
- en: In case the split is based on CCAvg and Family, the Entropy computations can
    be shown as follows. The total Entropy is weighted as the sum of the Entropies
    of each of the nodes that were created.
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Entropy after its split is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The information gain is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information gain and Entropy](img/B03980_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This methodology is applied to compute the information gain for all other attributes.
    It chooses the one with the highest information gain. This is tested at each node
    to select the best node.
  prefs: []
  type: TYPE_NORMAL
- en: Gini index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Gini index is a general splitting criterion. It is named after an Italian statistician
    and economist—Corrado Gini. Gini Index is used to measure the probability of two
    random items belonging to the same class. In the case of a real dataset, this
    probability value is 1\. The Gini measure of a node is the sum of the squares
    of the proportions of the classes. A node with two classes each has a score of
    *0.52 + 0.52 = 0.5*. This is because the probability of picking the same class
    at random is 1 out of 2\. Now, if we apply Gini index for the data set we get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The original Gini Index = ![Gini index](img/B03980_05_15.jpg) = 0.502959
  prefs: []
  type: TYPE_NORMAL
- en: 'When split with CCAvg and Family, the Gini Index changes to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gini index](img/B03980_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gain ratio
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another improvement in C4.5 compared to ID3 is that the factor that decides
    the attribute is the gain ratio. The gain ratio is the ratio of information gain
    and information content. The attribute that gives the maximum amount of gain ratio
    is the attribute that is used to split it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do some calculations with an extremely simple example to highlight why
    the gain ratio is a better attribute than the information gain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gain ratio](img/B03980_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dependent variable is whether they are married under a specific circumstance.
    Let's assume that in this case, no man is married. Whereas all women, except the
    last one (60 women), are married.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, intuitively the rule has to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If it is a man, then he is unmarried
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is a woman then she is married (the only isolated case where she is not
    married must be noise).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's systematically solve this problem to gain insights into various parameters.
    First let's split the data into two halves as training and testing data. So, our
    training set consists of the last 20 males (all insensible and aged between 21-40),
    and the last 30 females (all married and aged between 71-99, except the last one).
    Testing contains the other half where all the women are married.
  prefs: []
  type: TYPE_NORMAL
- en: The gain ratio requires measure for **Information content**.
  prefs: []
  type: TYPE_NORMAL
- en: Information content is defined as *-f[i] log[2] f[i]*. Note that here, we do
    not take the value of the dependent variable into account. We only want to know
    the fraction of the members in a state divided by the total members.
  prefs: []
  type: TYPE_NORMAL
- en: The information content of gender is that it has only two states; males are
    20 and females are 30\. So, the information content for the gender is *2/5*LOG(2/5,2)-3/5*LOG(3/5,2)=0.9709*.
  prefs: []
  type: TYPE_NORMAL
- en: The information content of age is that there is a total of 49 states for the
    age. For the states that have only one data point, the information content is
    *-(1/50)*log(1/50,2) = 0.1129*.
  prefs: []
  type: TYPE_NORMAL
- en: There are 48 such states with a single data point. So, their information content
    is (0.1129*48), 5.4192\. In the last state, there are two data points. So, its
    information content is *-(2/50 * LOG(2/50,2)) = 0.1857*. The total information
    content for the age is 5.6039.
  prefs: []
  type: TYPE_NORMAL
- en: The gain ratio for the gender = Information gain for gender / Information content
    for gender = 0.8549/0.9709 = 0.8805.
  prefs: []
  type: TYPE_NORMAL
- en: The gain ratio for the age = 0.1680
  prefs: []
  type: TYPE_NORMAL
- en: So, if we consider the gain ratio, we get that the gender is a more suitable
    measure. This aligns with the intuition. Let's now say that we used the gain ratio
    and built the tree. Our rule is if the gender is male, the person is unmarried
    and if it is female, the person is married.
  prefs: []
  type: TYPE_NORMAL
- en: Termination Criteria / Pruning Decision trees
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each branch is grown deeply enough to classify the training examples perfectly
    by the Decision tree algorithm. This can turn out to be an acceptable approach
    and most of the times results in problems when there is some noise in the data.
    In case the training dataset is too small and cannot represent the true picture
    of the actual data set the Decision tree might end up over-fitting the training
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways of avoiding over-fitting in Decision tree learning. Following
    are the two different cases:'
  prefs: []
  type: TYPE_NORMAL
- en: One case where the Decision tree is terminated for growth way before a perfect
    classification of the training data is done
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another case where the over-fitting of data is done and then the tree is pruned
    to recover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though the first case might seem to be more direct, the second case of post-pruning
    the over-fitting trees is more successful in reality. The reason is the difficulty
    to know when to stop growing the tree. Irrespective of the approach taken, it
    is more important to identify the criterion to determine the final, appropriate
    tree size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are a couple of approaches to find the correct tree size:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify a separate and different dataset to that of the target training data
    set to be used, and evaluate the correctness of post-pruning nodes in the tree.
    This is a common approach and is called training and validation set approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of having a subset of data in the training set, use up all the data
    in the training set, and apply probabilistic methods to check if pruning a particular
    node has any likelihood to produce any improvement over and above the training
    dataset. Use all the available data for training. For example, the chi-square
    test can be used to check this probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduced-Error-Pruning (D): We prune at a node by removing the subtree that
    is rooted at the node. We make that node a leaf (with the majority label of associated
    examples); algorithm is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Termination Criteria / Pruning Decision trees](img/B03980_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Rule post-pruning is a more commonly used method and is a highly accurate hypotheses
    technique. A variation of this pruning method is used in C4.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the steps of the Rule Post-Pruning process:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct a Decision tree from the training set by growing it until there is
    an obvious over-fitting seen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate rules from the constructed Decision tree with every path, starting
    from the root node to a particular leaf node mapping to a rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply pruning to each rule for removing identified preconditions and help improve
    the probabilistic accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, use the pruned rules in the order of their increased accuracy on the subsequent
    instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following are the advantages of rule-based pruning and its need for converting
    into rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the readability of the rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A consistent testing can be done at both the root and leaf level nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a clear decision that can be made of either removing the decision node
    or retaining it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees in a graphical representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until now, we have seen how Decision trees are described by dividing the data
    at the node and comparing the value with a constant. Another way of representing
    Decision trees is to visualize and have graphical representation. For example,
    we can choose two input attributes in two dimensions, then compare the value of
    one attribute with constant and show the split on the data to a parallel axis.
    We can also compare two attributes with one another along with a linear combination
    of attributes, instead of a hyperplane that is not parallel to an axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees in a graphical representation](img/B03980_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Constructing multiple Decision trees for the given data is possible. The process
    of identifying the smallest and a perfect tree is called a minimum consistent
    hypothesis. Let''s use two arguments to see why this is the best Decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: Occam's Razor is simple; when there are two ways to solve a problem and both
    give the same result, the simplest of them prevails.
  prefs: []
  type: TYPE_NORMAL
- en: In data mining analysis, one is likely to fall into the trap of complex methods
    and large computations. So, it is essential to internalize the line of reasoning
    adopted by Occam. Always choose a Decision tree that has an optimum combination
    of size and errors.
  prefs: []
  type: TYPE_NORMAL
- en: Inducing Decision trees – Decision tree algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many Decision tree inducing methods. Among all the methods, C4.5 and
    CART are the most adopted or popular ones. In this section, we will cover these
    methods in depth and list a brief on other methods.
  prefs: []
  type: TYPE_NORMAL
- en: CART
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CART stands for Classification and Regression Trees (Breiman et al., 1984).
    CART creates binary trees. This means there are always two branches that can emerge
    from a given node. The philosophy of the CART algorithm is to follow a *goodness*
    criterion, which is all about choosing the best possible partition. Moreover,
    as the tree grows, a cost-complexity pruning mechanism is adopted. CART uses the
    Gini index to select appropriate attributes or the splitting criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Using CART, the prior probability distribution can be provided. We can generate
    Regression trees using CART that in turn help in predicting real numbers against
    a class. The prediction is done by applying the weighted mean for the node. CART
    identifies splits that minimize the prediction squared error (that is, the least-squared
    deviation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The depiction in the following figure of CART is for the same example referred
    in the previous section, where Decision tree construction is demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART](img/B03980_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: C4.5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to CART, C4.5 is a Decision tree algorithm with a primary difference
    that it can generate more than binary trees, which means support for multiway
    splits. For attribute selection, C4.5 uses the information gain measure. As explained
    in the previous section, an attribute with the largest information gain (or the
    lowest Entropy reduction) value helps to achieve closer to accurate classification
    with the least quantity of data. One of the key drawbacks of C4.5 is the need
    for large memory and CPU capacity for generating rules. The C5.0 algorithm is
    a commercial version of C4.5 that was presented in 1997.
  prefs: []
  type: TYPE_NORMAL
- en: C4.5 is an evolution of the ID3 algorithm. The gain ratio measure is used for
    identifying the splitting criteria. The splitting process stops when the number
    of splits reaches a boundary condition definition that acts as a threshold. Post
    this growing phase of the tree, pruning is done, and an error-based pruning method
    is followed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of the C4.5 way of constructing the Decision tree
    for the same example used in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C4.5](img/B03980_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '| Tree Induction method | How does it work? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ID3 | The ID3 (**Iterative Dichotomiser 3**) algorithm is considered the
    simplest among the Decision tree algorithms. The information gain method is used
    as splitting criteria; the splitting is done until the best information gain is
    not greater than zero. There is no specific pruning done with ID3\. It cannot
    handle numeric attributes and missing values. |'
  prefs: []
  type: TYPE_TB
- en: '| CHAID | **CHAID** (**Chi-squared Automatic Interaction Detection**) was built
    to support only nominal attributes. For every attribute, a value is chosen in
    such a way that it is the closest to the target attribute. There is an additional
    statistical measure, depending on the type of the target attribute that differentiates
    this algorithm.F test for a continuous target attribute, Pearson chi-squared test
    for nominal target attribute, and likelihood–ratio test for an ordinal target
    attribute is used. CHAID checks a condition to merge that can have a threshold
    and moves for a next check for merging. This process is repeated until no matching
    pairs are found.CHAID addresses missing values in a simple way, and it operates
    on the assumption that all values belong to a single valid category. No pruning
    is done in this process. |'
  prefs: []
  type: TYPE_TB
- en: '| QUEST | The acronym QUEST stands for Quick, Unbiased, Efficient, and Statistical
    Tree.This algorithm supports univariate and linear combination splits. ANOVA F-test
    or Pearson''s chi-square or two-means clustering methods are used to compute the
    relationship between each input attribute and the target attribute, depending
    on the type of the attribute. Splitting is applied on attributes that have stronger
    association with the target attribute. To ensure that there is an optimal splitting
    point achieved, **Quadratic Discriminant Analysis** (**QDA**) is applied. Again,
    QUEST achieves binary trees and for pruning 10-fold cross-validation is used.
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAL5 | This works with numerical attributes. |'
  prefs: []
  type: TYPE_TB
- en: '| FACT | This algorithm is an earlier version of QUEST that uses statistical
    methods followed by discriminant analysis for attribute selection. |'
  prefs: []
  type: TYPE_TB
- en: '| LMDT | This uses a multivariate testing mechanism to build Decision trees.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MARS | A multiple regression function is approximated using linear splines
    and their tensor products. |'
  prefs: []
  type: TYPE_TB
- en: Greedy Decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A vital characteristic of Decision trees is that they are *Greedy!* A greedy
    algorithm targets achieving optimal solutions globally by achieving local optimums
    at every stage. Though the global optimum is not always guaranteed, the local
    optimums help in achieving global optimum to a maximum extent.
  prefs: []
  type: TYPE_NORMAL
- en: Every node is greedily searched to reach the local optimum, and the possibility
    of getting stuck at achieving local optima is high. Most of the time, targeting
    local optima might help in providing a good enough solution.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the advantages of using Decision trees are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are fast and easy to build and require little experimentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are robust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are easy to understand and interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees do not require complex data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can handle both categorical and numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are supported using statistical models for validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can handle highly dimensional data and also operate large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore some important special situations we face and
    special types of Decision trees. These become handy while solving special kinds
    of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Oblique trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Oblique trees are used in cases where the data is extremely complex. If the
    attributes are *x1, x2, AND x3…xn*, then the C4.5 and CART tests the criteria
    as *x1>some value* or *x2< some other value*, and so on. The goal in such cases
    is to find an attribute to test at each node. These are graphically parallel axis
    splits as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Oblique trees](img/B03980_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, we need to construct enormous trees. At this point, let's learn a data
    mining jargon called hyperplanes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a **1 D** problem, a point classifies the space. In **2 D**, a line (straight
    or curved) classifies the space. In a **3 D** problem, a plane (linear or curved)
    classifies the space. In higher dimensional space, we imagine a plane like a thing
    splitting and classifying the space, calling it **hyperplane**. This is shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Oblique trees](img/B03980_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the traditional Decision tree algorithms produce axis parallel hyperplanes
    that split the data. These can be cumbersome if the data is complex. If we can
    construct oblique planes, the explicability may come down, but we might reduce
    the tree size substantially. So, the idea is to change the testing conditions
    from the following:'
  prefs: []
  type: TYPE_NORMAL
- en: xi > K or < K to a1x1+ a2x2+ … + c > K or < K
  prefs: []
  type: TYPE_NORMAL
- en: 'These oblique hyperplanes can at times drastically reduce the length of the
    tree. The same data shown in figure 2 is classified using oblique planes in the
    figure here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Oblique trees](img/B03980_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These specialized trees are used when there are too many dimensions. We have
    learned about curse of dimensionality in the Machine learning introduction chapter.
    The basic premise of the curse of dimensionality is that high dimensional data
    brings in complexity. With more dimensions and features, the possibility of errors
    is also high. Before we take a deep dive into Random forests, let's understand
    the concept of Boosting. More details on boosting methods are covered as a part
    of [Chapter 13](ch13.html "Chapter 13. Ensemble learning"), *Ensemble learning*.
    In the case of Random forests, the application of boosting is about how single
    tree methods are brought together to see a boost in the result regarding accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A Random forest extends Decision trees by including more number of Decision
    trees. These Decision trees are built by a combination of random selection of
    data (samples) and a random selection of a subset of attributes. The following
    diagram depicts the random selection of datasets to build each of the Decision
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another variable input required for the making of multiple Decision trees are
    random subsets of the attributes, which is represented in the diagram here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since each tree is built using random dataset and random variable set, these
    trees are called Random trees. Moreover, many such Random trees define a Random
    forest.
  prefs: []
  type: TYPE_NORMAL
- en: The result of a Random tree is based on two radical beliefs. One is that each
    of the trees make an accurate prediction for maximum part of the data. Second,
    mistakes are encountered at different places. So, on an average, a poll of results
    is taken across the Decision trees to conclude a result.
  prefs: []
  type: TYPE_NORMAL
- en: There are not enough observations to get good estimates, which leads to sparsity
    issues. There are two important causes for exponential increase on spatial density,
    one, is increase in dimensionality and the other is increase in the equidistant
    points in data. Most of the data is in the tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the density of a given accuracy, the following table shows how
    the sample size increases with dimensionality. The subsequent computations table
    shows how the mean square error of an estimate of multivariate normal distribution
    increases with an increase in dimensionality (as demonstrated by Silverman and
    computed by the formula given here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_05_31.jpg)![Random forests](img/B03980_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random forests are a vital extension of the Decision trees that are very simple
    to understand and are extremely efficient, particularly when one is dealing with
    high dimensional spaces. When the original data has many dimensions, we randomly
    pick a small subset of the dimensions (columns) and construct a tree. We let it
    grow all the way without pruning. Now, we iterate this process and construct hundreds
    of trees with a different set of attributes each time.
  prefs: []
  type: TYPE_NORMAL
- en: For prediction, a new sample is pushed down the tree. A new label of the training
    sample is assigned to the terminal node, where it ends up. This procedure is iterated
    over all the trees in the group, and the average vote of all trees is reported
    as the Random forest prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When achieving the global optima seems almost impossible, Evolutionary trees
    are used. As you learned, Decision trees are greedy. So sometimes, we may be constructing
    much bigger trees just because we are stuck in local optima. So, if your tree
    length is just too much, try oblique trees or evolutionary trees.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of evolutionary trees is originated from a very exciting concept
    called genetic algorithms. You will learn about it in detail in a different course.
    Let us only look at the essence.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of mathematically computing the best attribute at every node, an Evolutionary
    tree randomly picks a node at each point and creates a tree. It then iterates
    and creates a collection of trees (forest). Now, it identifies the best trees
    in the forest for the data. It then creates the next generation of the forest
    by combining these trees randomly.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary trees, on the other hand, choose a radically different top node
    and produce a much shorter tree, which has the same efficiency. Evolutionary algorithms
    take more time to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Hellinger trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There have been attempts to identify impurity measures that are less sensitive
    to the distribution of dependent variable values than Entropy or Gini index. A
    very recent paper suggested Hellinger distance as a measure of impurity that does
    not depend on the distribution of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hellinger trees](img/B03980_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, *P(Y+|X)* is the probability of finding *Y+* for each attribute
    and similarly, *P(Y-|X)* for each attribute is computed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hellinger trees](img/B03980_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the previous image, for a **High** value of the first attribute, only a
    **High** value of the second attribute results in a probability value of 1\. This
    brings the total distance value to *sqrt(2)*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing Decision
    Trees and Random Forests (source code path `.../chapter5/...` under each of the
    folder for the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter5/decisiontreeexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder`.../mahout/chapter5/randomforestexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter5/decisiontreeexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter5/randomforestexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter5/decisiontreeexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter5/randomforestexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (scikit-learn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python scikit-learn/chapter5/decisiontreeexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../python scikit-learn/chapter5/randomforestexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter5/decisiontreeexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter5/randomforestexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned a supervised learning technique with Decision trees
    to solve classification and regression problems. We also covered methods to select
    attributes, split the tree, and prune the tree. Among all other Decision tree
    algorithms, we have explored the CART and C4.5 algorithms. For a special requirement
    or a problem, you have also learned how to implement Decision tree-based models
    using MLib of Spark, R, and Julia. In the next chapter, we will cover **Nearest
    Neighbour** and **SVM** (**Support Vector Machines**) to solve supervised and
    unsupervised learning problems.
  prefs: []
  type: TYPE_NORMAL
