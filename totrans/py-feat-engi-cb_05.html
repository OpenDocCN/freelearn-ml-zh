<html><head></head><body>
		<div id="_idContainer093">
			<h1 id="_idParaDest-144" class="chapter-number"><a id="_idTextAnchor662"/><st c="0">5</st></h1>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor663"/><a id="_idTextAnchor664"/><st c="2">Working with Outliers</st></h1>
			<p><st c="23">An outlier</st><a id="_idIndexMarker360"/><st c="34"> is a data point that diverges notably from other values within a variable. </st><st c="110">Outliers may stem from the inherent variability of the feature itself, manifesting as extreme values that occur infrequently within the distribution (typically found in the tails). </st><st c="291">They can be the result of experimental errors or inaccuracies in data collection processes, or they can signal important events. </st><st c="420">For instance, an unusually high expense in a card transaction may indicate fraudulent activity, warranting flagging and potentially blocking the card to safeguard customers. </st><st c="594">Similarly, unusually distinct tumor morphologies can suggest malignancy, prompting </st><span class="No-Break"><st c="677">further examination.</st></span></p>
			<p><st c="697">Outliers can exert a disproportionately large impact on a statistical analysis. </st><st c="778">For example, a small number of outliers can reverse the statistical significance of a test in either direction (think A/B testing) or directly influence the estimation of the parameters of the statistical model (think coefficients). </st><st c="1011">Some machine learning models are well known for being susceptible to outliers, such as linear regression. </st><st c="1117">Other models are known for being robust to outliers, such as decision-tree-based models. </st><st c="1206">AdaBoost is said to be sensitive to outliers in the target variable, and in principle, distance-based models, such as PCA and KNN, could also be affected by the presence </st><span class="No-Break"><st c="1376">of outliers.</st></span></p>
			<p><st c="1388">There isn’t a strict mathematical definition for what qualifies as an outlier, and there is also no consensus on how to handle outliers in statistical or machine learning models. </st><st c="1568">If outliers stem from flawed data collection, discarding them seems like a safe option. </st><st c="1656">However, in many datasets, pinpointing the exact nature of outliers is challenging. </st><st c="1740">Ultimately, detecting and handling outliers remains a subjective exercise, reliant on domain knowledge and an understanding of their potential impact </st><span class="No-Break"><st c="1890">on models.</st></span></p>
			<p><st c="1900">In this chapter, we will begin by discussing methods to identify potential outliers, or more precisely, observations that significantly deviate from the rest. </st><st c="2060">Then, we’ll proceed under the assumption that these observations are not relevant for the analysis, and show how to either remove them or reduce their impact on models </st><span class="No-Break"><st c="2228">through truncation.</st></span></p>
			<p><st c="2247">This chapter contains the </st><span class="No-Break"><st c="2274">following recipes:</st></span></p>
			<ul>
				<li><st c="2292">Visualizing outliers with boxplots and the inter-quartile </st><span class="No-Break"><st c="2351">proximity rule</st></span></li>
				<li><st c="2365">Finding outliers using the mean and </st><span class="No-Break"><st c="2402">standard deviation</st></span></li>
				<li><st c="2420">Using the median absolute deviation to </st><span class="No-Break"><st c="2460">find outliers</st></span></li>
				<li><span class="No-Break"><st c="2473">Removing outliers</st></span></li>
				<li><st c="2491">Bringing outliers back within </st><span class="No-Break"><st c="2522">acceptable limits</st></span></li>
				<li><span class="No-Break"><st c="2539">Applying winsorization</st></span><a id="_idTextAnchor665"/><a id="_idTextAnchor666"/></li>
			</ul>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor667"/><st c="2562">Technical requirements</st></h1>
			<p><st c="2585">In this chapter, we will use the Python </st><strong class="source-inline"><st c="2626">numpy</st></strong><st c="2631">, </st><strong class="source-inline"><st c="2633">pandas</st></strong><st c="2639">, </st><strong class="source-inline"><st c="2641">matplotlib</st></strong><st c="2651">, </st><strong class="source-inline"><st c="2653">seaborn</st></strong><st c="2660">, and </st><span class="No-Break"><strong class="source-inline"><st c="2666">feature-engine</st></strong></span><span class="No-Break"><st c="2680"> libraries.</st></span><a id="_idTextAnchor668"/><a id="_idTextAnchor669"/></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor670"/><st c="2691">Visualizing outliers with boxplots and the inter-quartile proximity rule</st></h1>
			<p><st c="2764">A </st><a id="_idIndexMarker361"/><st c="2767">common way to visualize outliers is by usin</st><a id="_idTextAnchor671"/><st c="2810">g boxplots. </st><st c="2823">Boxplots </st><a id="_idIndexMarker362"/><st c="2832">provide a standardized display of the variable’s distribution based on quartiles. </st><st c="2914">The box contains the observations within the first and third quartiles, known as the </st><strong class="bold"><st c="2999">Inter-Quartile Range</st></strong><strong class="bold"> </strong><st c="3019">(</st><strong class="bold"><st c="3021">IQR</st></strong><st c="3024">). </st><st c="3028">The</st><a id="_idIndexMarker363"/><st c="3031"> first quartile is the value below which 25% of the observations lie (equivalent to the 25th percentile), while the third quartile is the value below which 75% of the observations lie (equivalent to the 75th percentile). </st><st c="3252">The IQR is calculated </st><span class="No-Break"><st c="3274">as follows:</st></span></p>
			<p><img src="image/21.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mo&gt;−&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.963em;width:13.330em"/><st c="3285"/></p>
			<p><st c="3320">Boxplots also </st><a id="_idIndexMarker364"/><st c="3334">display whiskers, which are lines that protrude from each end of the box toward the minimum and maximum values and up to a limit. </st><st c="3464">These limits are given by the minimum or maximum value of the distribution or, in the presence of extreme values, by the </st><span class="No-Break"><st c="3585">following equations:</st></span></p>
			<p><img src="image/22.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;1.5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.252em;height:0.963em;width:15.427em"/><st c="3605"/></p>
			<p><img src="image/23.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;1.5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.252em;height:0.963em;width:15.092em"/><st c="3645"/></p>
			<p><st c="3684">According to the </st><strong class="bold"><st c="3701">IQR proximity rule</st></strong><st c="3719">, we</st><a id="_idIndexMarker365"/><st c="3723"> can consider a value an</st><a id="_idIndexMarker366"/><st c="3747"> outlier if it falls beyond the whisker limits determined by the previous equations. </st><st c="3832">In boxplots, outliers are indicated </st><span class="No-Break"><st c="3868">as dots.</st></span></p>
			<p class="callout-heading"><st c="3876">Note</st></p>
			<p class="callout"><st c="3881">If the variable has a normal distribution, about 99% of the observations will be located within the interval delimited by the whiskers. </st><st c="4018">Hence, we can treat values beyond the whiskers as outliers. </st><st c="4078">Boxplots are, however, non-parametric, which is why we also use them to visualize outliers in </st><span class="No-Break"><st c="4172">skewed variables.</st></span></p>
			<p><st c="4189">In this </st><a id="_idIndexMarker367"/><st c="4198">recipe, we’ll begin by visualizing the variable distribution with boxplots, and then we’ll calculate the whisker’s limits manually to identify the points beyond which we could consider a value as </st><span class="No-Break"><st c="4394">an o</st><a id="_idTextAnchor672"/><a id="_idTextAnchor673"/><st c="4398">utlier.</st></span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor674"/><st c="4406">How to do it...</st></h2>
			<p><a id="_idTextAnchor675"/><st c="4422">We will create boxplots utilizing the </st><strong class="source-inline"><st c="4461">seaborn</st></strong><st c="4468"> library</st><a id="_idTextAnchor676"/><st c="4476">. Let’s begin by importing the Python libraries and loading </st><span class="No-Break"><st c="4536">the dataset:</st></span></p>
			<ol>
				<li><st c="4548">Let’s import the Python libraries and </st><span class="No-Break"><st c="4587">the dataset:</st></span><pre class="source-code"><st c="4599">
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing</st></pre></li>				<li><st c="4707">Modify the default background from </st><strong class="source-inline"><st c="4743">seaborn</st></strong><st c="4750"> (it makes prettier plots, but that’s subjective, </st><span class="No-Break"><st c="4800">of course):</st></span><pre class="source-code"><st c="4811">
sns.set(style="darkgrid")</st></pre></li>				<li><st c="4837">Load the California house prices dataset </st><span class="No-Break"><st c="4879">from scikit-learn:</st></span><pre class="source-code"><st c="4897">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="4962">Make a boxplot of the </st><strong class="source-inline"><st c="4985">MedInc</st></strong><st c="4991"> variable to visualize </st><span class="No-Break"><st c="5014">its distribution:</st></span><pre class="source-code"><st c="5031">
plt.figure(figsize=(8, 3))
sns.boxplot(data=X["MedInc"], orient="y")
plt.title("Boxplot")
plt.show()</st></pre><p class="list-inset"><st c="5132">In the following boxplot, we identify the box containing the observations within the IQR, that is, the observations between the first and third quartiles. </st><st c="5288">We also see the whiskers. </st><st c="5314">On</st><a id="_idIndexMarker368"/><st c="5316"> the left, the whisker extends to the minimum value of </st><strong class="source-inline"><st c="5371">MedInc</st></strong><st c="5377">; on the right, the whisker goes up to the third quartile plus 1.5 times the IQR. </st><st c="5460">Values beyond the right whisker are represented as dots and could </st><span class="No-Break"><st c="5526">constitute out</st><a id="_idTextAnchor677"/><st c="5540">liers:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B22396_05_1.jpg" alt="Figure 5.1 – Boxplot of the MedInc variable highlighting potential outliers on the right tail of the distribution"/><st c="5547"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5569">Figure 5.1 – Boxplot of the MedInc variable highlighting potential outliers on the right tail of the distribution</st></p>
			<p class="callout-heading"><st c="5682">Note</st></p>
			<p class="callout"><st c="5687">As shown in </st><span class="No-Break"><em class="italic"><st c="5700">Figure 5</st></em></span><em class="italic"><st c="5708">.1</st></em><st c="5710">, the boxplot returns asymmetric boundaries denoted by the varying lengths of the left and right whiskers. </st><st c="5817">This makes boxplots a suitable method for identifying outliers in highly skewed distributions. </st><st c="5912">As we’ll see in the coming recipes, alternative methods to identify outliers create symmetric boundaries around the center of the distribution, which may not be the best option for </st><span class="No-Break"><st c="6093">asymmetric distributions</st><a id="_idTextAnchor678"/><st c="6117">.</st></span></p>
			<ol>
				<li value="5"><st c="6118">Let’s now create a function to</st><a id="_idTextAnchor679"/><st c="6149"> plot a boxplot next to </st><span class="No-Break"><st c="6173">a histogram:</st></span><pre class="source-code"><st c="6185">
def plot_boxplot_and_hist(data, variable):
    f, (ax_box, ax_hist) = plt.subplots(
        2, sharex=True,
        gridspec_kw={"height_ratios": (0.50, 0.85)})
    sns.boxplot(x=data[variable], ax=ax_box)
    sns.histplot(data=data, x=variable, ax=ax_hist)
    plt.show(</st><a id="_idTextAnchor680"/><st c="6425">)</st></pre></li>				<li><st c="6427">Let’s use </st><a id="_idIndexMarker369"/><st c="6437">the previous </st><a id="_idTextAnchor681"/><st c="6450">function to create the plots for the </st><span class="No-Break"><strong class="source-inline"><st c="6487">MedInc</st></strong></span><span class="No-Break"><st c="6493"> variable:</st></span><pre class="source-code"><st c="6503">
plot_boxplot_and_hist(X, "MedInc")</st></pre><p class="list-inset"><st c="6538">In the following figure, we can see the relationship between the boxplot and the variable’s distribution shown in the histogram. </st><st c="6668">Note how most of </st><strong class="source-inline"><st c="6685">MedInc</st></strong><st c="6691">’s observations are located within the IQR box. </st><strong class="source-inline"><st c="6740">MedInc</st></strong><st c="6746">’s potential outliers lie on the right tail, corresponding to people with unusually </st><span class="No-Break"><st c="6831">high-income sa</st><a id="_idTextAnchor682"/><st c="6845">laries:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B22396_05_2.jpg" alt="Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s distribution"/><st c="6853"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6920">Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s distribution</st></p>
			<p class="list-inset"><st c="7005">Now that </st><a id="_idIndexMarker370"/><st c="7015">we’ve seen how we can visualize outliers, let’s see how to calculate the limits beyond which we find outliers at each side of </st><span class="No-Break"><st c="7141">the distribution.</st></span></p>
			<ol>
				<li value="7"><st c="7158">Let’s create a function that returns the limits based on the IQR </st><span class="No-Break"><st c="7224">proximity rule:</st></span><pre class="source-code"><st c="7239">
def find_limits(df, variable, fold):
    q1 = df[variable].quantile(0.25)
    q3 = df[variable].quantile(0.75)
    IQR = q3 - q1
    lower_limit = q1 - (IQR * fold)
    upper_limit = q3 + (IQR * fold)
    return lower_limit, upper_limit</st></pre></li>			</ol>
			<p class="callout-heading"><st c="7452">Note</st></p>
			<p class="callout"><st c="7457">Remember that the first and third quartiles are equivalent to the 25th and 75th percentiles. </st><st c="7551">That’s why we use pandas’ </st><strong class="source-inline"><st c="7577">quantile</st></strong><st c="7585"> to determine </st><span class="No-Break"><st c="7599">those values.</st></span></p>
			<ol>
				<li value="8"><st c="7612">With the function from </st><em class="italic"><st c="7636">step 7</st></em><st c="7642">, we’ll calculate the extreme limits </st><span class="No-Break"><st c="7679">for </st></span><span class="No-Break"><strong class="source-inline"><st c="7683">MedInc</st></strong></span><span class="No-Break"><st c="7689">:</st></span><pre class="source-code"><st c="7691">
lower_limit, upper_limit = find_limits(
    X, "MedInc", 1.5)</st></pre><p class="list-inset"><st c="7749">If we now execute </st><strong class="source-inline"><st c="7768">lower_limit</st></strong><st c="7779"> and </st><strong class="source-inline"><st c="7784">upper_limit</st></strong><st c="7795">, we will see the values </st><strong class="source-inline"><st c="7820">-0.7063</st></strong><st c="7827"> and </st><strong class="source-inline"><st c="7832">8.013</st></strong><st c="7837">. The lower limit is beyond </st><strong class="source-inline"><st c="7865">MedInc</st></strong><st c="7871">’s minimum value, hence in </st><a id="_idIndexMarker371"/><st c="7899">the boxplot, the whisker only goes up to the minimum value. </st><st c="7959">The upper limit, on the other hand, coincides with the right </st><span class="No-Break"><st c="8020">whisker’s limit.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="8036">Note</st></p>
			<p class="callout"><st c="8041">Common values to multiply the IQR are </st><strong class="source-inline"><st c="8080">1.5</st></strong><st c="8083">, which is the default value in boxplots, or </st><strong class="source-inline"><st c="8128">3 </st></strong><st c="8130">if we want to be </st><span class="No-Break"><st c="8147">more conservative.</st></span></p>
			<ol>
				<li value="9"><st c="8165">Let’s display the box plot and histogram for the </st><span class="No-Break"><strong class="source-inline"><st c="8215">HouseAge</st></strong></span><span class="No-Break"><st c="8223"> variable:</st></span><pre class="source-code"><st c="8233">
plot_boxplot_and_hist(X, "HouseAge")</st></pre><p class="list-inset"><st c="8270">We can see that this variable does not seem to contain outliers, and hence the whiskers in the box plot extend to the minimum and </st><span class="No-Break"><st c="8401">maximum values:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B22396_05_3.jpg" alt="Figure 5.3 – Boxplot and histogram of the HouseAge variable"/><st c="8416"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8438">Figure 5.3 – Boxplot and histogram of the HouseAge variable</st></p>
			<ol>
				<li value="10"><st c="8497">Let’s</st><a id="_idIndexMarker372"/><st c="8503"> find the variable’s limits according to the IQR </st><span class="No-Break"><st c="8552">proximity rule:</st></span><pre class="source-code"><st c="8567">
lower_limit, upper_limit = find_limits(
    X, "HouseAge", 1.5)</st></pre></li>			</ol>
			<p><st c="8627">If we execute </st><strong class="source-inline"><st c="8642">lower_limit</st></strong><st c="8653"> and </st><strong class="source-inline"><st c="8658">upper_limit</st></strong><st c="8669">, we will see the values </st><strong class="source-inline"><st c="8694">-10.5</st></strong><st c="8699"> and </st><strong class="source-inline"><st c="8704">65.5</st></strong><st c="8708">, which are beyond the edges of the plots, and hence we don’t see </st><span class="No-Break"><st c="8774">any </st><a id="_idTextAnchor683"/><a id="_idTextAnchor684"/><st c="8778">outliers.</st></span></p>
			<h2 id="_idParaDest-149"><st c="8787">How it works</st><a id="_idTextAnchor685"/><st c="8800">…</st></h2>
			<p><st c="8801">In this recipe, we used the </st><strong class="source-inline"><st c="8829">boxplot</st></strong><st c="8836"> method from Seaborn to create the boxplots and then we calculated the limits beyond which a value could be considered an outlier based on the IQR </st><span class="No-Break"><st c="8983">proximity rule.</st></span></p>
			<p><st c="8998">In </st><span class="No-Break"><em class="italic"><st c="9002">Figure 5</st></em></span><em class="italic"><st c="9010">.2</st></em><st c="9012">, we saw that the box in the boxplot for </st><strong class="source-inline"><st c="9053">MedInc</st></strong><st c="9059"> extended from approximately 2 to 5, corresponding to the first and third quantiles (you can determine these values precisely by executing </st><strong class="source-inline"><st c="9198">X[</st></strong><st c="9200">“</st><strong class="source-inline"><st c="9202">MedInc</st></strong><st c="9208">”</st><strong class="source-inline"><st c="9210">].quantile(0.25)</st></strong><st c="9226"> and </st><strong class="source-inline"><st c="9231">X[</st></strong><st c="9233">“</st><strong class="source-inline"><st c="9235">MedInc</st></strong><st c="9241">”</st><strong class="source-inline"><st c="9243">].quantile(0.75)</st></strong><st c="9259">). </st><st c="9263">We also saw that the whiskers start at </st><strong class="source-inline"><st c="9302">MedInc</st></strong><st c="9308">’s minimum on the left and extend up to </st><strong class="source-inline"><st c="9349">8.013</st></strong><st c="9354"> on the right (we know this value exactly because we calculated it in </st><em class="italic"><st c="9424">step 8</st></em><st c="9430">). </st><strong class="source-inline"><st c="9434">MedInc</st></strong><st c="9440"> showed values greater than </st><strong class="source-inline"><st c="9468">8.013</st></strong><st c="9473">, which were displayed</st><a id="_idIndexMarker373"/><st c="9495"> in the boxplot as dots. </st><st c="9520">Those are the values that could be </st><span class="No-Break"><st c="9555">considered outliers.</st></span></p>
			<p><st c="9575">In </st><span class="No-Break"><em class="italic"><st c="9579">Figure 5</st></em></span><em class="italic"><st c="9587">.3</st></em><st c="9589">, we displayed the boxplot for the </st><strong class="source-inline"><st c="9624">HouseAge</st></strong><st c="9632"> variable. </st><st c="9643">The box included values ranging from approximately 18 to 35 (you can determine the precise values by executing </st><strong class="source-inline"><st c="9754">X[</st></strong><st c="9756">“</st><strong class="source-inline"><st c="9758">HouseAge</st></strong><st c="9766">”</st><strong class="source-inline"><st c="9768">].quantile(0.25)</st></strong><st c="9784"> and </st><strong class="source-inline"><st c="9789">X[</st></strong><st c="9791">“</st><strong class="source-inline"><st c="9793">HouseAge</st></strong><st c="9801">”</st><strong class="source-inline"><st c="9803">].quantile(0.75)</st></strong><st c="9819">). </st><st c="9823">The whiskers extended to the minimum and maximum values of the distribution. </st><st c="9900">The limits of the whiskers in the plot did not coincide with those based on the IQR proximity rule (which we calculated in </st><em class="italic"><st c="10023">step 10</st></em><st c="10030">) because these limits were far beyond the value range observed for </st><span class="No-Break"><st c="10099">this</st><a id="_idTextAnchor686"/><a id="_idTextAnchor687"/><st c="10103"> variable.</st></span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor688"/><st c="10113">Finding outliers using the mean and standard deviation</st></h1>
			<p><st c="10168">I</st><a id="_idTextAnchor689"/><st c="10170">n </st><a id="_idIndexMarker374"/><st c="10172">normally distributed variables, around 99.8% of the observations lie within the interval comprising the mean plus and minus </st><a id="_idIndexMarker375"/><st c="10296">thr</st><a id="_idTextAnchor690"/><st c="10299">ee times the standard devi</st><a id="_idTextAnchor691"/><st c="10326">ation. </st><st c="10334">Thus, values beyond those limits can be considered outliers; they </st><span class="No-Break"><st c="10400">are rare.</st></span></p>
			<p class="callout-heading"><st c="10409">Note</st></p>
			<p class="callout"><st c="10414">Using the mean and standard deviation to detect outliers has some drawbacks. </st><st c="10492">Firstly, it assumes a normal distribution, including outliers. </st><st c="10555">Secondly, outliers strongly influence the mean and standard deviation. </st><st c="10626">Therefore, a recommended alternative is the </st><strong class="bold"><st c="10670">Median Absolute Deviation</st></strong><st c="10695"> (</st><strong class="bold"><st c="10697">MAD</st></strong><st c="10700">), which</st><a id="_idIndexMarker376"/><st c="10709"> we’ll discuss in the </st><span class="No-Break"><st c="10731">next recipe.</st></span></p>
			<p><st c="10743">In this recipe, we will identify outliers as those observations that lie outside the interval delimited by the mean plus and minus three times the </st><span class="No-Break"><st c="10891">standa</st><a id="_idTextAnchor692"/><a id="_idTextAnchor693"/><st c="10897">rd deviation.</st></span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor694"/><st c="10911">How to do it...</st></h2>
			<p><st c="10927">Let’s begin the</st><a id="_idIndexMarker377"/><st c="10943"> recipe by importing the Python libraries and loading </st><span class="No-Break"><st c="10997">the dataset:</st></span></p>
			<ol>
				<li><st c="11009">Let’s import the Python libraries </st><span class="No-Break"><st c="11044">and dataset:</st></span><pre class="source-code"><st c="11056">
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer</st></pre></li>				<li><st c="11177">Load the breast cancer dataset </st><span class="No-Break"><st c="11209">from scikit-learn:</st></span><pre class="source-code"><st c="11227">
X, y = load_breast_cancer(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="11286">Create </st><a id="_idIndexMarker378"/><st c="11294">a function to plot a </st><a id="_idIndexMarker379"/><st c="11315">boxplot next to </st><span class="No-Break"><st c="11331">a histogram:</st></span><pre class="source-code"><st c="11343">
def plot_boxplot_and_hist(data, variable):
    f, (ax_box, ax_hist) = plt.subplots(
        2, sharex=True,
        gridspec_kw={"height_ratios": (0.50, 0.85)})
    sns.boxplot(x=data[variable], ax=ax_box)
    sns.histplot(data=data, x=variable, ax=ax_hist)
    plt.show()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="11584">Note</st></p>
			<p class="callout"><st c="11589">We discussed the function from </st><em class="italic"><st c="11621">step 3</st></em><st c="11627"> in the previous recipe, </st><em class="italic"><st c="11652">Visualizing outliers with boxplots and the inter-quartile </st></em><span class="No-Break"><em class="italic"><st c="11710">proximity rule</st></em></span><span class="No-Break"><st c="11724">.</st></span></p>
			<ol>
				<li value="4"><st c="11725">Let’s plot the distribution of the </st><strong class="source-inline"><st c="11761">mean </st></strong><span class="No-Break"><strong class="source-inline"><st c="11766">smoothness</st></strong></span><span class="No-Break"><st c="11776"> variable:</st></span><pre class="source-code"><st c="11786">
plot_boxplot_and_hist(X, "mean smoothness")</st></pre><p class="list-inset"><st c="11830">In the</st><a id="_idIndexMarker380"/><st c="11837"> following boxplot, we see that the variable’s values show a distribution like the</st><a id="_idIndexMarker381"/><st c="11919"> normal distribution, and it has six outliers – one on the left and five on the </st><span class="No-Break"><st c="11999">right tail:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B22396_05_4.jpg" alt="Figure 5.4 – Boxplot and histogram of the variable mean smoothness"/><st c="12010"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12063">Figure 5.4 – Boxplot and histogram of the variable mean smoothness</st></p>
			<ol>
				<li value="5"><st c="12129">Create a function that returns the mean plus and minus </st><strong class="source-inline"><st c="12185">fold</st></strong><st c="12189"> times the standard deviation, where </st><strong class="source-inline"><st c="12226">fold</st></strong><st c="12230"> is a parameter to </st><span class="No-Break"><st c="12249">the function:</st></span><pre class="source-code"><st c="12262">
def find_limits(df, variable, fold):
    var_mean = df[variable].mean()
    var_std = df[variable].std()
    lower_limit = var_mean - fold * var_std
    upper_limit = var_mean + fold * var_std
    return lower_limit, upper_limit</st></pre></li>				<li><st c="12471">Use the</st><a id="_idIndexMarker382"/><st c="12479"> function to identify the extreme limits of the </st><strong class="source-inline"><st c="12527">mean </st></strong><span class="No-Break"><strong class="source-inline"><st c="12532">smoothness</st></strong></span><span class="No-Break"><st c="12542"> variable:</st></span><pre class="source-code"><st c="12552">
lower_limit, upper_limit = find_limits(
    X, "mean smo</st><a id="_idTextAnchor695"/><st c="12605">othness", 3)</st></pre><p class="list-inset"><st c="12618">If we now execute </st><strong class="source-inline"><st c="12637">lower_limit</st></strong> <a id="_idTextAnchor696"/><st c="12648">or </st><strong class="source-inline"><st c="12652">upper_limit</st></strong><st c="12663">, we will see the values </st><strong class="source-inline"><st c="12688">0.0541</st></strong><st c="12694"> and </st><strong class="source-inline"><st c="12699">0.13</st><a id="_idTextAnchor697"/><st c="12703">855</st></strong><st c="12707">, correspon</st><a id="_idTextAnchor698"/><st c="12718">ding to the limits beyond which we can consider a value </st><span class="No-Break"><st c="12775">an outlier.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="12786">Note</st></p>
			<p class="callout"><st c="12791">The interval between the mean plus and minus three times the standard deviation encloses 99.87% of the observations if the variable is normally distributed. </st><st c="12949">For less conservative limits, we could multiply the standard deviation by 2 or 2.5, which would produce intervals that enclose 95.4% and 97.6% of the </st><span class="No-Break"><st c="13099">observations, respectively.</st></span></p>
			<ol>
				<li value="7"><st c="13126">Create </st><a id="_idIndexMarker383"/><st c="13134">a Boolean vector that flags observations with values beyond the limits determined in </st><span class="No-Break"><em class="italic"><st c="13219">step 6</st></em></span><span class="No-Break"><st c="13225">:</st></span><pre class="source-code"><st c="13227">
outliers = np.where(
    (X[«mean smoothness»] &gt; upper_limit) |
    (X[«mean smoothness»] &lt; lower_limit),
    True,
    False
)</st></pre><p class="list-inset"><st c="13339">If we now execute </st><strong class="source-inline"><st c="13358">outliers.sum()</st></strong><st c="13372">, we will see the value </st><strong class="source-inline"><st c="13396">5</st></strong><st c="13397">, indicating that there are five outliers or observations that are smaller or greater than the extreme values found with the mean and the standard deviation. </st><st c="13555">According to these limits, we’d identify one outlier less compared to the </st><span class="No-Break"><st c="13629">IQR rule.</st></span></p></li>				<li><st c="13638">Let’s add red vertical lines to the histogram from </st><em class="italic"><st c="13690">step 3</st></em><st c="13696"> to highlight the limits determined</st><a id="_idIndexMarker384"/><st c="13731"> by using the mean and the </st><span class="No-Break"><st c="13758">standard deviation:</st></span><pre class="source-code"><st c="13777">
def plot_boxplot_and_hist(data, variable):
    f, (ax_box, ax_hist) = plt.subplots(
        2, sharex=True,
        gridspec_kw={"height_ratios": (0.50, 0.85)})
    sns.boxplot(x=data[variable], ax=ax_box)
    sns.histplot(data=data, x=variable, ax=ax_hist)
    plt.vlines(
        x=lower_limit, ymin=0, ymax=80, color='r')
    plt.vlines(
        x=upper_limit, ymin=0, ymax=80, color='r')
     plt.show()</st></pre></li>				<li><st c="14128">And </st><a id="_idIndexMarker385"/><st c="14133">now let’s make </st><span class="No-Break"><st c="14148">the plots:</st></span><pre class="source-code"><st c="14158">
plot_boxplot_and_hist(X, "mean smoothness")</st></pre><p class="list-inset"><st c="14202">In the following plot, we see that the limits observed by the IQR proximity rule in the box plot are less conservative than those identified by the mean and the standard deviation. </st><st c="14384">Hence why we observe six potential outliers in the boxplot, but only five based on the mean and standard </st><span class="No-Break"><st c="14489">deviation calculations:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B22396_05_5.jpg" alt="Figure 5.5 – Comparison of the limits between the whiskers in the boxplot and those determined by using the mean and the standard deviation (vertical lines in the histogram)"/><st c="14512"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14560">Figure 5.5 – Comparison of the limits between the whiskers in the boxplot and those determined by using the mean and the standard deviation (vertical lines in the histogram)</st></p>
			<p class="list-inset"><st c="14733">The </st><a id="_idIndexMarker386"/><st c="14738">boundaries derived from the </st><a id="_idIndexMarker387"/><st c="14766">mean and standard deviation are symmetric. </st><st c="14809">They extend equidistantly from the center of the distribution toward both tails. </st><st c="14890">As previously mentioned, these boundaries are only suitable for normally</st><a id="_idTextAnchor699"/><a id="_idTextAnchor700"/> <span class="No-Break"><st c="14962">distributed variables.</st></span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor701"/><st c="14985">How it works…</st></h2>
			<p><st c="14999">With pandas’ </st><strong class="source-inline"><st c="15013">mean()</st></strong><st c="15019"> and </st><strong class="source-inline"><st c="15024">std()</st></strong><st c="15029">, we captured the mean and standard deviation of the variable. </st><st c="15092">We determined the limits as the mean plus and minus three times the standard deviation. </st><st c="15180">To highlight the outliers, we used NumPy’s </st><strong class="source-inline"><st c="15223">where()</st></strong><st c="15230">. The </st><strong class="source-inline"><st c="15236">where()</st></strong><st c="15243"> function scanned the rows of the vari</st><a id="_idTextAnchor702"/><st c="15281">able, and if the value was </st><a id="_idTextAnchor703"/><st c="15309">greater than the upper limit or smaller than the lower limit, it was assigned </st><strong class="source-inline"><st c="15387">True</st></strong><st c="15391">, and alternatively </st><strong class="source-inline"><st c="15411">False</st></strong><st c="15416">. Finally, we used pandas’ </st><strong class="source-inline"><st c="15443">sum()</st></strong><st c="15448"> over this Boolean vector to calculate the total number </st><span class="No-Break"><st c="15504">of outliers.</st></span></p>
			<p><st c="15516">Finally, we compared the boundaries to determine outliers returned by the IQR proximity rule, which we discussed in the previous recipe, </st><em class="italic"><st c="15654">Visualizing outliers with boxplots and the inter-quartile proximity rule</st></em><st c="15726">, and the mean and the standard deviation. </st><st c="15769">We observed that the limits of the IQR rule are less conservative. </st><st c="15836">That means that with the IQR rule, we’d flag more outliers in this </st><span class="No-Break"><st c="15903">particular variable.</st></span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor704"/><st c="15923">Using the median absolute deviation to find outliers</st></h1>
			<p><st c="15976">The mean </st><a id="_idIndexMarker388"/><st c="15986">and the standard deviation are heavily impacted by outliers. </st><st c="16047">Hence, using these parameters to identify outliers can defeat the purpose. </st><st c="16122">A better way to identify outliers is </st><a id="_idIndexMarker389"/><st c="16159">by using MAD. </st><st c="16173">MAD is the median of the absolute deviation between each observation and the median value of </st><span class="No-Break"><st c="16266">the variable:</st></span></p>
			<p><img src="image/24.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfenced open=&quot;|&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.254em;height:1.015em;width:15.728em"/><st c="16279"/></p>
			<p><st c="16314">In the previous equation, </st><strong class="source-inline"><st c="16340">xi</st></strong><st c="16342"> is each observation in the </st><strong class="source-inline"><st c="16370">X</st></strong><st c="16371"> variable. </st><st c="16382">The beauty of MAD is that it uses the median instead of the mean, which is robust to outliers. </st><st c="16477">The </st><strong class="source-inline"><st c="16481">b</st></strong><st c="16482"> constant is used to estimate the standard deviation from MAD, and if we assume normality, then </st><strong class="source-inline"><st c="16578">b = </st></strong><span class="No-Break"><strong class="source-inline"><st c="16582">1.4826</st></strong></span><span class="No-Break"><st c="16588">.</st></span></p>
			<p class="callout-heading"><st c="16589">Note</st></p>
			<p class="callout"><st c="16594">If the variable is assumed to have a different distribution, </st><strong class="source-inline"><st c="16656">b</st></strong><st c="16657"> is then calculated as 1 divided by the 75th percentile. </st><st c="16714">In the case of normality, 1/75th percentile = </st><span class="No-Break"><st c="16760">1.4826.</st></span></p>
			<p><st c="16767">After computing MAD, we use the median and MAD to establish distribution limits, designating values beyond these limits as outliers. </st><st c="16901">The limits are set as the median plus and minus a multiple of MAD, typically ranging from 2 to 3.5. </st><st c="17001">The multiplication factor we choose reflects how stringent we want to be (the higher, the more conservative). </st><st c="17111">In this recipe, we will identify outliers </st><span class="No-Break"><st c="17153">using MAD.</st></span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor705"/><st c="17163">How to do it...</st></h2>
			<p><st c="17179">Let’s begin the recipe by importing the Python libraries and loading </st><span class="No-Break"><st c="17249">the dataset:</st></span></p>
			<ol>
				<li><st c="17261">Let’s import the Python libraries </st><span class="No-Break"><st c="17296">and dataset:</st></span><pre class="source-code"><st c="17308">
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer</st></pre></li>				<li><st c="17429">Load the </st><a id="_idIndexMarker390"/><st c="17439">breast cancer dataset </st><span class="No-Break"><st c="17461">from scikit-learn:</st></span><pre class="source-code"><st c="17479">
X, y = load_breast_cancer(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="17538">Create a </st><a id="_idIndexMarker391"/><st c="17548">function that returns the limits based </st><span class="No-Break"><st c="17587">on MAD:</st></span><pre class="source-code"><st c="17594">
def find_limits(df, variable, fold):
    median = df[variable].median()
    center = df[variable] - median
    MAD = center.abs().median() * 1.4826
    lower_limit = median - fold * MAD
    upper_limit = median + fold * MAD
    return lower_limit, upper_limit</st></pre></li>				<li><st c="17830">Let’s use the function to capture the extreme limits of the </st><strong class="source-inline"><st c="17891">mean </st></strong><span class="No-Break"><strong class="source-inline"><st c="17896">smoothness</st></strong></span><span class="No-Break"><st c="17906"> variable:</st></span><pre class="source-code"><st c="17916">
lower_limit, upper_limit = find_limits(
    X, "mean smoothness", 3)</st></pre><p class="list-inset"><st c="17981">If we execute </st><strong class="source-inline"><st c="17996">lower_limit</st></strong><st c="18007"> or </st><strong class="source-inline"><st c="18011">upper_limit</st></strong><st c="18022">, we will see the values </st><strong class="source-inline"><st c="18047">0.0536</st></strong><st c="18053"> and </st><strong class="source-inline"><st c="18058">0.13812</st></strong><st c="18065">, corresponding to the limits beyond which we can consider a value </st><span class="No-Break"><st c="18132">an outlier.</st></span></p></li>				<li><st c="18143">Let’s create a Boolean vector that flags observations with values beyond </st><span class="No-Break"><st c="18217">the limits:</st></span><pre class="source-code"><st c="18228">
outliers = np.where(
    (X[«mean smoothness»] &gt; upper_limit) |
    (X[«mean smoothness»] &lt; lower_limit),
    True,
    False
)</st></pre><p class="list-inset"><st c="18340">If we </st><a id="_idIndexMarker392"/><st c="18347">now execute </st><strong class="source-inline"><st c="18359">outliers.sum()</st></strong><st c="18373">, we will see the value </st><strong class="source-inline"><st c="18397">5</st></strong><st c="18398">, indicating that there are five outliers or observations that are smaller or greater than the extreme values found </st><span class="No-Break"><st c="18514">with MAD.</st></span></p></li>				<li><st c="18523">Let’s </st><a id="_idIndexMarker393"/><st c="18530">make a function to plot a boxplot next to a histogram of a variable, highlighting in the histogram the limits calculated in </st><span class="No-Break"><em class="italic"><st c="18654">step 4</st></em></span><span class="No-Break"><st c="18660">:</st></span><pre class="source-code"><st c="18662">
def plot_boxplot_and_hist(data, variable):
    f, (ax_box, ax_hist) = plt.subplots(
        2, sharex=True,
        gridspec_kw={"height_ratios": (0.50, 0.85)})
    sns.boxplot(x=data[variable], ax=ax_box)
    sns.histplot(data=data, x=variable, ax=ax_hist)
    plt.vlines(
        x=lower_limit, ymin=0, ymax=80, color='r')
    plt.vlines(
        x=upper_limit, ymin=0, ymax=80, color='r')
    plt.show()</st></pre></li>				<li><st c="19013">And now let’s make </st><span class="No-Break"><st c="19033">the plots:</st></span><pre class="source-code"><st c="19043">
plot_boxplot_and_hist(X, "mean smoothness")</st></pre><p class="list-inset"><st c="19087">In the following plot, we see that the limits observed by the IQR proximity rule in the box plot are less conservative than those identified by using MAD. </st><st c="19243">MAD returns </st><a id="_idIndexMarker394"/><st c="19255">symmetric boundaries, while the boxplot generates asymmetric boundaries, which are </st><a id="_idIndexMarker395"/><st c="19338">more suitable for highly </st><span class="No-Break"><st c="19363">skewed distributions:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B22396_05_6.jpg" alt="Figure 5.6 – Comparison of the limits between the whiskers in the boxplot and those determined by using MAD"/><st c="19384"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="19416">Figure 5.6 – Comparison of the limits between the whiskers in the boxplot and those determined by using MAD</st></p>
			<p class="callout-heading"><st c="19523">Note</st></p>
			<p class="callout"><st c="19528">Detecting outliers with MAD requires that the variable has certain variability. </st><st c="19609">If more than 50% of the values in a variable are identical, the median coincides with the most frequent value, and MAD=0. </st><st c="19731">This means that all values different from the median will be flagged as outliers. </st><st c="19813">This constitutes another limitation of using MAD in </st><span class="No-Break"><st c="19865">outlier detection.</st></span></p>
			<p><st c="19883">That’s it! </st><st c="19895">You now know how to identify outliers using the median </st><span class="No-Break"><st c="19950">and MAD.</st></span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor706"/><st c="19958">How it works…</st></h2>
			<p><st c="19972">We </st><a id="_idIndexMarker396"/><st c="19976">determined the median with pandas’ </st><strong class="source-inline"><st c="20011">median()</st></strong><st c="20019"> and the absolute difference with pandas’ </st><strong class="source-inline"><st c="20061">abs()</st></strong><st c="20066">. Next, we used the NumPy </st><strong class="source-inline"><st c="20092">where()</st></strong><st c="20099"> function to create a Boolean vector with </st><strong class="source-inline"><st c="20141">True</st></strong><st c="20145"> if a value was greater than the upper limit or smaller than the lower limit, otherwise </st><strong class="source-inline"><st c="20233">False</st></strong><st c="20238">. Finally, we used pandas’ </st><strong class="source-inline"><st c="20265">sum()</st></strong><st c="20270"> over this Boolean vector to calculate the total number </st><span class="No-Break"><st c="20326">of outliers.</st></span></p>
			<p><st c="20338">Finally, we </st><a id="_idIndexMarker397"/><st c="20351">compared the boundaries to determine outliers returned by the IQR proximity rule, which we discussed in the </st><em class="italic"><st c="20459">Visualizing outliers with boxplots and the inter-quartile range proximity rule</st></em><st c="20537"> recipe, and those returned by using MAD. </st><st c="20579">The limits returned by the IQR rule were less conservative. </st><st c="20639">This behavior can be changed by multiplying the IQR by 3 instead of 1.5, which is the default value in boxplots. </st><st c="20752">In addition, we noted that MAD returns symmetric boundaries, whereas the boxplot provided asymmetric limits, which could be better suited for </st><span class="No-Break"><st c="20894">asymmetric distributions.</st></span></p>
			<h3><st c="20919">See also</st></h3>
			<p><st c="20928">For a thorough discussion of the advantages and limitations of the different methods to detect outliers, check out the </st><span class="No-Break"><st c="21048">following resources:</st></span></p>
			<ul>
				<li><st c="21068">Rousseeuw PJ, Croux C. </st><em class="italic"><st c="21092">Alternatives to the Median Absolute deviation</st></em><st c="21137">. Journal of the American Statistical Association, </st><span class="No-Break"><st c="21188">1993. </st></span><a href="https://www.jstor.org/stable/2291267"><span class="No-Break"><st c="21194">http://www.jstor.org/stable/2291267</st></span></a><span class="No-Break"><st c="21229">.</st></span></li>
				<li><st c="21230">Leys C, et. </st><st c="21243">al. </st><em class="italic"><st c="21247">Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</st></em><st c="21354">. Journal of Experimental Social Psychology, </st><span class="No-Break"><st c="21399">2013. </st></span><span class="No-Break"><st c="21405">http://dx.doi.org/10.1016/j.jesp.2013.03.013</st></span><span class="No-Break"><st c="21449">.</st></span></li>
				<li><st c="21450">Thériault R, et. </st><st c="21468">al. </st><em class="italic"><st c="21472">Check your outliers</st></em><em class="italic"><st c="21491">! An introduction to identifying statistical outliers in R with easystats</st></em><st c="21564">. Behavior Research Methods, </st><span class="No-Break"><st c="21593">2024. </st></span><a href="https://link.springer.com/article/10.3758/s13428-024-02356-w"><span class="No-Break"><st c="21599">https://doi.</st><span id="_idTextAnchor707"/><span id="_idTextAnchor708"/><span id="_idTextAnchor709"/><span id="_idTextAnchor710"/><span id="_idTextAnchor711"/><st c="21611">org/10.3758/s13428-024-02356-w</st></span></a><span class="No-Break"><st c="21642">.</st></span></li>
			</ul>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor712"/><st c="21643">Removing outliers</st></h1>
			<p><st c="21661">Recent </st><a id="_idIndexMarker398"/><st c="21669">studies distinguish three types of outliers: error outliers, interesting outliers, and random outliers. </st><st c="21773">Error outliers are likely due to human or methodological errors and should be either corrected or removed from the data analysis. </st><st c="21903">In this recipe, we’ll assume outliers are errors (you don’t want to remove interesting or random outliers) a</st><a id="_idTextAnchor713"/><a id="_idTextAnchor714"/><st c="22011">nd remove them from </st><span class="No-Break"><st c="22032">the dataset.</st></span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor715"/><st c="22044">How to do it...</st></h2>
			<p><st c="22060">We’ll use the IQR proximity rule to find the outliers and then remove them from the data using pandas </st><span class="No-Break"><st c="22163">and Feature-engine:</st></span></p>
			<ol>
				<li><st c="22182">Let’s import the Python libraries, functions, </st><span class="No-Break"><st c="22229">and classes:</st></span><pre class="source-code"><st c="22241">
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from feature_engine.out</st><a id="_idTextAnchor716"/><st c="22426">liers import OutlierTrimmer</st></pre></li>				<li><st c="22454">Load the California housing dataset from scikit-learn and separate it into train and </st><span class="No-Break"><st c="22540">test sets:</st></span><pre class="source-code"><st c="22550">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="22705">Let’s create a function to find the limits beyond which we’ll consider a data point an outlier using the IQR </st><span class="No-Break"><st c="22815">proximity rule:</st></span><pre class="source-code"><st c="22830">
def find_limits(df, variable, fold):
    q1 = df[variable].quantile(0.25)
    q3 = df[variable].quantile(0.75)
    IQR = q3 - q1
    lower_limit = q1 - (IQR * fold)
    upper_limit = q3 + (IQR * fold)
    return lower_limit, upper_limit</st></pre></li>			</ol>
			<p class="callout-heading"><st c="23043">Note</st></p>
			<p class="callout"><st c="23048">In </st><em class="italic"><st c="23052">step 3</st></em><st c="23058">, we use the IQR proximity rule to find the limits beyond which data points will be considered outliers, which we discussed in the </st><em class="italic"><st c="23189">Visualizing outliers with boxplots and the inter-quartile proximity rule</st></em><st c="23261"> recipe. </st><st c="23270">Alternatively, you can identify outliers with the mean and the standard deviation or MAD, as we covered in the </st><em class="italic"><st c="23381">Finding outliers using the mean and standard deviation</st></em><st c="23435"> and </st><em class="italic"><st c="23440">Using the median absolute deviation to find </st></em><span class="No-Break"><em class="italic"><st c="23484">outliers</st></em></span><span class="No-Break"><st c="23492"> recipes.</st></span></p>
			<ol>
				<li value="4"><st c="23501">Using the</st><a id="_idIndexMarker399"/><st c="23511"> function from </st><em class="italic"><st c="23526">step 3</st></em><st c="23532"> , let’s determine the limits of the </st><span class="No-Break"><strong class="source-inline"><st c="23568">MedInc</st></strong></span><span class="No-Break"><st c="23574"> variable:</st></span><pre class="source-code"><st c="23584">
lower, upper = find_limits(X_train, "MedInc", 3)</st></pre><p class="list-inset"><st c="23633">If you execute </st><strong class="source-inline"><st c="23649">print(lower_limit, upper_limit)</st></strong><st c="23680">, you’ll see the result of the previous command: </st><strong class="source-inline"><st c="23729">(-</st></strong><span class="No-Break"><strong class="source-inline"><st c="23731">3.925900000000002, 11.232600000000001)</st></strong></span><span class="No-Break"><st c="23770">.</st></span></p></li>				<li><st c="23771">Let’s retain the observations in the train and test sets whose values are greater than or equal to (</st><strong class="source-inline"><st c="23872">ge</st></strong><st c="23875">) the </st><span class="No-Break"><st c="23882">lower limit:</st></span><pre class="source-code"><st c="23894">
inliers = X_train["MedInc"].ge(lower)
train_t = X_train.loc[inliers]
inliers = X_test["MedInc"].ge(lower)
test_t = X_test.loc[inliers]</st></pre></li>				<li><st c="24029">Let’s retain the observations whose values are lower than or equal to (</st><strong class="source-inline"><st c="24101">le</st></strong><st c="24104">) the </st><span class="No-Break"><st c="24111">upper limit:</st></span><pre class="source-code"><st c="24123">
inliers = X_train["MedInc"].le(upper)
train_t = X_train.loc[inliers]
inliers = X_test["MedInc"].le(upper)
test_t = X_test.loc[inliers]</st></pre><p class="list-inset"><st c="24258">Go </st><a id="_idIndexMarker400"/><st c="24262">ahead and execute </st><strong class="source-inline"><st c="24280">X_train.shape</st></strong><st c="24293"> followed by </st><strong class="source-inline"><st c="24306">train_t.shape</st></strong><st c="24319"> to corroborate that the transformed DataFrame contains fewer observations than the original one after removing </st><span class="No-Break"><st c="24431">the outliers.</st></span></p><p class="list-inset"><st c="24444">We can remove outliers across multiple variables simultaneously </st><span class="No-Break"><st c="24509">with </st></span><span class="No-Break"><strong class="source-inline"><st c="24514">feature-engine</st></strong></span><span class="No-Break"><st c="24528">.</st></span></p></li>				<li><st c="24529">Set up a transformer to identify outliers in three variables by using the </st><span class="No-Break"><st c="24604">IQR rule:</st></span><pre class="source-code"><st c="24613">
trimmer = OutlierTrimmer(
    variables = [«MedInc", "HouseAge", "Population"],
    capping_method="iqr",
    tail="both",
    fold=1.5,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="24736">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="24741">OutlierTrimmer</st></strong><st c="24756"> can identify boundaries using the IQR, as we show in this recipe, as well as by using the mean and standard deviation, or MAD. </st><st c="24884">You need to change </st><strong class="source-inline"><st c="24903">capping_method</st></strong><st c="24917"> to </st><strong class="source-inline"><st c="24921">gaussian</st></strong><st c="24929"> or </st><span class="No-Break"><strong class="source-inline"><st c="24933">mad</st></strong></span><span class="No-Break"><st c="24936">, respectively.</st></span></p>
			<ol>
				<li value="8"><st c="24951">Fit the transformer to the training set so that it learns </st><span class="No-Break"><st c="25010">those limits:</st></span><pre class="source-code"><st c="25023">
trimmer.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="25044">Note</st></p>
			<p class="callout"><st c="25049">By executing </st><strong class="source-inline"><st c="25063">trimmer.left_tail_caps_</st></strong><st c="25086">, we can visualize the lower limits for the three variables: </st><strong class="source-inline"><st c="25147">{'MedInc': -0.6776500000000012, 'HouseAge': -10.5, 'Population': -626.0}</st></strong><st c="25219">. By executing </st><strong class="source-inline"><st c="25234">trimmer.right_tail_caps_</st></strong><st c="25258">, we can see the variables’ upper limits: </st><strong class="source-inline"><st c="25300">{'MedInc': 7.984350000000001, 'HouseAge': 65.5, '</st></strong><span class="No-Break"><strong class="source-inline"><st c="25349">Population': 3134.0}</st></strong></span><span class="No-Break"><st c="25370">.</st></span></p>
			<ol>
				<li value="9"><st c="25371">Finally, let’s remove outliers from the train and </st><span class="No-Break"><st c="25422">test sets:</st></span><pre class="source-code"><st c="25432">
X_train_t = trimmer.transform(X_train)
X_test_t = t</st><a id="_idTextAnchor717"/><st c="25484">rimmer.transform(X_test)</st></pre><p class="list-inset"><st c="25509">To finish with the recipe, let’s compare the distribution of a variable before and after </st><span class="No-Break"><st c="25599">removing outliers.</st></span></p></li>				<li><st c="25617">Let’s create</st><a id="_idIndexMarker401"/><st c="25630"> a function to display a boxplot on top of </st><span class="No-Break"><st c="25673">a histogram:</st></span><pre class="source-code"><st c="25685">
def plot_boxplot_and_hist(data, variable):
    f, (ax_box, ax_hist) = plt.subplots(
        2, sharex=True,
        gridspec_kw={"height_ratios": (0.50, 0.85)}
    )
    sns.boxplot(x=data[variable], ax=ax_box)
    sns.histplot(data=data, x=variable, ax=ax_hist)
    plt.show()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="25927">Note</st></p>
			<p class="callout"><st c="25932">We discussed the code in </st><em class="italic"><st c="25958">step 10</st></em><st c="25965"> in the </st><em class="italic"><st c="25973">Visualizing outliers with boxplots</st></em><st c="26007"> recipe earlier in </st><span class="No-Break"><st c="26026">this chapter.</st></span></p>
			<ol>
				<li value="11"><st c="26039">Let’s plot the distribution of </st><strong class="source-inline"><st c="26071">MedInc</st></strong><st c="26077"> before removing </st><span class="No-Break"><st c="26094">the outliers:</st></span><pre class="source-code"><st c="26107">
plot_boxplot_and_hist(X_train, "MedInc")</st></pre><p class="list-inset"><st c="26148">In the following plot, we see that </st><strong class="source-inline"><st c="26184">MedInc</st></strong><st c="26190"> is skewed and observations grea</st><a id="_idTextAnchor718"/><st c="26222">ter than 8 are marked </st><span class="No-Break"><st c="26245">as outliers:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B22396_05_7.jpg" alt="Figure 5.7– Boxplot and the histogram of MedInc before removing outliers."/><st c="26257"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26277">Figure 5.7– Boxplot and the histogram of MedInc before removing outliers.</st></p>
			<ol>
				<li value="12"><st c="26350">Finally, let’s plot </st><a id="_idIndexMarker402"/><st c="26371">the distribution of </st><strong class="source-inline"><st c="26391">MedInc</st></strong><st c="26397"> after removing </st><span class="No-Break"><st c="26413">the outliers:</st></span><pre class="source-code"><st c="26426">
plot_boxplot_and_hist(train_t, "MedInc")</st></pre><p class="list-inset"><st c="26467">After removing outliers, </st><strong class="source-inline"><st c="26493">MedInc</st></strong><st c="26499"> seems less skewed and i</st><a id="_idTextAnchor719"/><st c="26523">ts values more </st><span class="No-Break"><st c="26539">evenly distributed:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B22396_05_8.jpg" alt="Figure 5.8 – Boxplot and the histogram of MedInc after removing outliers"/><st c="26558"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26577">Figure 5.8 – Boxplot and the histogram of MedInc after removing outliers</st></p>
			<p class="callout-heading"><st c="26649">Note</st></p>
			<p class="callout"><st c="26654">Using the IQR rule over the transformed variable reveals new outliers. </st><st c="26726">This is not surprising; removing observations at the extremes of the distribution alters parameters such as the median and quartile values, which in turn determine the length of the whiskers, potentially identifying additional observations as outliers. </st><st c="26979">The tools that we use to identify outliers are just that: tools. </st><st c="27044">To unequivocally identify outliers, we need to support these tools with additional </st><span class="No-Break"><st c="27127">data analysis.</st></span></p>
			<p class="list-inset"><st c="27141">If thinking</st><a id="_idIndexMarker403"/><st c="27153"> of removing error outliers from the dataset, make sure to compare and report the results with and without outliers, to see the ext</st><a id="_idTextAnchor720"/><a id="_idTextAnchor721"/><st c="27284">ent of their impact on </st><span class="No-Break"><st c="27308">the models.</st></span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor722"/><st c="27319">How it works...</st></h2>
			<p><st c="27335">The </st><strong class="source-inline"><st c="27340">ge()</st></strong><st c="27344"> and </st><strong class="source-inline"><st c="27349">le()</st></strong><st c="27353"> methods </st><a id="_idIndexMarker404"/><st c="27362">from pandas created Boolean vectors identifying observations exceeding or falling below thresholds set by the IQR proximity rule. </st><st c="27492">We used these vectors with pandas </st><strong class="source-inline"><st c="27526">loc</st></strong><st c="27529"> to retain observations within the interval defined by </st><span class="No-Break"><st c="27584">the IQR.</st></span></p>
			<p><st c="27592">The </st><strong class="source-inline"><st c="27597">feature-engine</st></strong><st c="27611"> library’s </st><strong class="source-inline"><st c="27622">OutlierTrimmer()</st></strong><st c="27638"> automates the procedure of removing outliers for multiple variables. </st><strong class="source-inline"><st c="27708">OutlierTrimmer()</st></strong><st c="27724"> can identify outliers based on the mean and standard deviation, IQR proximity rule, MAD, or quantiles. </st><st c="27828">We can modify this behavior through the </st><span class="No-Break"><strong class="source-inline"><st c="27868">capping_method</st></strong></span><span class="No-Break"><st c="27882"> parameter.</st></span></p>
			<p><st c="27893">The methods to identify outliers can be made more or less conservative by changing the factor by which we multiply the IQR, the standard deviation, or MAD. </st><st c="28050">With </st><strong class="source-inline"><st c="28055">OutlierTrimmer()</st></strong><st c="28071">, we can control the strength of the methods through the </st><span class="No-Break"><strong class="source-inline"><st c="28128">fold</st></strong></span><span class="No-Break"><st c="28132"> parameter.</st></span></p>
			<p><st c="28143">With </st><strong class="source-inline"><st c="28149">tails</st></strong><st c="28154"> set to </st><strong class="source-inline"><st c="28162">"both"</st></strong><st c="28168">, </st><strong class="source-inline"><st c="28170">OutlierTrimmer()</st></strong><st c="28186"> found and removed outliers at both ends of the variables’ distribution. </st><st c="28259">To remove outliers just on one of the tails, we can pass </st><strong class="source-inline"><st c="28316">"left"</st></strong><st c="28322"> or </st><strong class="source-inline"><st c="28326">"right"</st></strong><st c="28333"> to the </st><span class="No-Break"><strong class="source-inline"><st c="28341">tails</st></strong></span><span class="No-Break"><st c="28346"> parameter.</st></span></p>
			<p><strong class="source-inline"><st c="28357">OutlierTrimmer()</st></strong><st c="28374"> adopts the scikit-learn functionality with the </st><strong class="source-inline"><st c="28422">fit()</st></strong><st c="28427"> method, to learn parameters, and </st><strong class="source-inline"><st c="28461">transform()</st></strong><st c="28472"> to modify the dataset. </st><st c="28496">With </st><strong class="source-inline"><st c="28501">fit()</st></strong><st c="28506">, the transformer learned and stored the limits for each variable. </st><st c="28573">With </st><strong class="source-inline"><st c="28578">transform()</st></strong><st c="28589">, it removed the outliers from the data, returning </st><span class="No-Break"><strong class="source-inline"><st c="28640">pandas</st></strong></span><span class="No-Break"><st c="28646"> DataFrames.</st></span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor723"/><st c="28658">See also</st></h2>
			<p><st c="28667">This is the study that I mentioned earlier that classifies outliers into errors; it is interesting and random: Leys C, et.al. </st><st c="28794">2019. </st><em class="italic"><st c="28800">How to Classify, Detect, and Manage Univariate and Multivariate Outliers, with Emphasis on Pre-Registration</st></em><st c="28907">. International Review of Social </st><span class="No-Break"><st c="28940">Psycholo</st><a id="_idTextAnchor724"/><a id="_idTextAnchor725"/><st c="28948">gy. </st></span><a href="https://rips-irsp.com/articles/10.5334/irsp.289"><span class="No-Break"><st c="28953">https://doi.org/10.5334/irsp.289</st></span></a><span class="No-Break"><st c="28985">.</st></span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor726"/><st c="28986">Bringing outliers back within acceptable limits</st></h1>
			<p><st c="29034">Removing </st><a id="_idIndexMarker405"/><st c="29044">error outliers can be a valid strategy. </st><st c="29084">However, this approach can reduce statistical power, in particular when there are outliers across many variables, because we end up removing big parts of the dataset. </st><st c="29251">An alternative way to handle error outliers is by bringing outliers back within acceptable limits. </st><st c="29350">In practice, what this means is replacing the value of the outliers with some thresholds identified with the IQR proximity rule, the mean and standard deviation, or MAD. </st><st c="29520">In this recipe, we’ll replace outlier va</st><a id="_idTextAnchor727"/><a id="_idTextAnchor728"/><st c="29560">lues using </st><strong class="source-inline"><st c="29572">pandas</st></strong> <span class="No-Break"><st c="29578">and </st></span><span class="No-Break"><strong class="source-inline"><st c="29583">feature-engine</st></strong></span><span class="No-Break"><st c="29597">.</st></span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor729"/><st c="29598">How to do it...</st></h2>
			<p><st c="29614">We’ll use </st><a id="_idIndexMarker406"/><st c="29625">the mean and standard deviation to find outliers and then replace their values u</st><a id="_idTextAnchor730"/><st c="29705">sing </st><strong class="source-inline"><st c="29711">pandas</st></strong> <span class="No-Break"><st c="29717">and </st></span><span class="No-Break"><strong class="source-inline"><st c="29722">feature-engine</st></strong></span><span class="No-Break"><st c="29736">:</st></span></p>
			<ol>
				<li><st c="29738">Let’s import the required Python libraries </st><span class="No-Break"><st c="29781">and functions:</st></span><pre class="source-code"><st c="29795">
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from feature_engine.outliers import Winsorizer</st></pre></li>				<li><st c="29943">Load the breast cancer dataset from scikit-learn and separate it into train and </st><span class="No-Break"><st c="30024">test sets:</st></span><pre class="source-code"><st c="30034">
X, y = load_breast_cancer(
    return_X_y=True, as_frame=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="30183">Let’s create a function to find outliers using the mean and </st><span class="No-Break"><st c="30244">standard deviation:</st></span><pre class="source-code"><st c="30263">
def find_limits(df, variable, fold):
    var_mean = df[variable].mean()
    var_std = df[variable].std()
    lower_limit = var_mean - fold * var_std
    upper_limit = var_mean + fold * var_std
    return lower_limit, upper_limit</st></pre></li>			</ol>
			<p class="callout-heading"><st c="30472">Note</st></p>
			<p class="callout"><st c="30477">In </st><em class="italic"><st c="30481">step 3</st></em><st c="30487">, we use the mean and standard deviation to find the limits beyond which data points will be considered outliers, as discussed in the </st><em class="italic"><st c="30621">Finding outliers using the mean and standard deviation</st></em><st c="30675"> recipe. </st><st c="30684">Alternatively, you can identify outliers with the IQR rule or MAD, as we covered in the </st><em class="italic"><st c="30772">Visualizing outliers with boxplots and the inter-quartile proximity rule</st></em><st c="30844"> and </st><em class="italic"><st c="30849">Using the median absolute deviation to find </st></em><span class="No-Break"><em class="italic"><st c="30893">outliers </st></em></span><span class="No-Break"><st c="30902">recipes.</st></span></p>
			<ol>
				<li value="4"><st c="30910">Using</st><a id="_idIndexMarker407"/><st c="30916"> the function from </st><em class="italic"><st c="30935">step 3</st></em><st c="30941">, let’s determine the limits of the </st><strong class="source-inline"><st c="30977">mean smoothness</st></strong><st c="30992"> variable, which follows approximately a </st><span class="No-Break"><st c="31033">Gaussian distribution:</st></span><pre class="source-code"><st c="31055">
var = "worst smoothness"
lower_limit, upper_limit = find_limits(
    X_train, var, 3)</st></pre></li>				<li><st c="31137">Let’s make a copy of the </st><span class="No-Break"><st c="31163">original datasets:</st></span><pre class="source-code"><st c="31181">
train_t = X_train.copy()
test_t = X_test.copy()</st></pre></li>				<li><st c="31229">Now, replace outliers with the lower or upper limits from </st><em class="italic"><st c="31288">step 4</st></em><st c="31294"> in the </st><span class="No-Break"><st c="31302">new DataFrames:</st></span><pre class="source-code"><st c="31317">
train_t[var] = train_t[var].clip(
    lower=lower_limit, upper=upper_limit)
test_t[var] = test_t[var].clip(
    lower=lower_limit, upper=upper_limit)</st></pre><p class="list-inset"><st c="31459">To corroborate that the outliers were replaced with the values determined in </st><em class="italic"><st c="31537">step 4</st></em><st c="31543">, execute </st><strong class="source-inline"><st c="31553">train_t["worst smoothness"].agg(["min", "max"])</st></strong><st c="31600"> to obtain the new maximum and minimum values. </st><st c="31647">They should coincide with the minimum and maximum values of the variable, or the limits returned in </st><span class="No-Break"><em class="italic"><st c="31747">step 4</st></em></span><span class="No-Break"><st c="31753">.</st></span></p><p class="list-inset"><st c="31754">We can replace outliers in multiple variables simultaneously by </st><span class="No-Break"><st c="31819">utilizing </st></span><span class="No-Break"><strong class="source-inline"><st c="31829">feature-engine</st></strong></span><span class="No-Break"><st c="31843">.</st></span></p></li>				<li><st c="31844">Let’s set up a transformer to replace outliers in two variables, using limits determined </st><a id="_idIndexMarker408"/><st c="31934">with the mean and </st><span class="No-Break"><st c="31952">standard deviation:</st></span><pre class="source-code"><st c="31971">
capper = Winsorizer(
    variables=[«worst smoothness», «worst texture»],
    capping_method="gaussian",
    tail="both",
    fold=3,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="32091">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="32096">Winsorizer</st></strong><st c="32107"> can</st><a id="_idIndexMarker409"/><st c="32111"> identify boundaries using the mean and standard deviation, as we show in this recipe, as well as the IQR proximity rule and MAD. </st><st c="32241">You need to change </st><strong class="source-inline"><st c="32260">capping_meth</st><a id="_idTextAnchor731"/><st c="32272">od</st></strong><st c="32275"> to </st><strong class="source-inline"><st c="32279">iqr</st></strong><st c="32282"> or </st><span class="No-Break"><strong class="source-inline"><st c="32286">mad</st></strong></span><span class="No-Break"><st c="32289">, respectively.</st></span></p>
			<ol>
				<li value="8"><st c="32304">Let’s fit the transformer to the data so that it learns </st><span class="No-Break"><st c="32361">those limits:</st></span><pre class="source-code"><st c="32374">
capper.fit(X_train)</st></pre><p class="list-inset"><st c="32394">By executing </st><strong class="source-inline"><st c="32408">capper.left_tail</st><a id="_idTextAnchor732"/><st c="32424">_caps_</st></strong><st c="32431">, we can visualize the lower limits for the two variables: </st><strong class="source-inline"><st c="32490">{'worst smoothness': 0.06364743973736293, 'worst texture': 7.115307053129349}</st></strong><st c="32567">. By executing </st><strong class="source-inline"><st c="32582">capper.right_tail_caps_</st></strong><st c="32605">, we can see the variables’ upper limits: </st><strong class="source-inline"><st c="32647">{'worst smoothness': 0.20149734880520967, 'worst </st></strong><span class="No-Break"><strong class="source-inline"><st c="32696">texture': 43.97692158753917}</st></strong></span><span class="No-Break"><st c="32724">.</st></span></p></li>				<li><st c="32725">Finally, let’s replace the outliers with the limits from </st><span class="No-Break"><em class="italic"><st c="32783">step 8</st></em></span><span class="No-Break"><st c="32789">:</st></span><pre class="source-code"><st c="32791">
X_train = capper.transform(X_train)
X_test = capper.transform(X_test)</st></pre><p class="list-inset"><st c="32861">If we now execute </st><strong class="source-inline"><st c="32880">train_t[capper.variables_].agg(["min", "max"])</st></strong><st c="32926">, we’ll see that the maximum and minimum values of the transformed DataFrame coincide with either the maximum and minimum values of the variables or the identified limits, whatever </st><span class="No-Break"><st c="33107">comes first:</st></span></p><pre class="source-code"><st c="33119">      worst smoothness  worst texture
min              0.071170        12.020000
max              0.201411        43.953738</st></pre><p class="list-inset"><st c="33196">If you are</st><a id="_idIndexMarker410"/><st c="33207"> planning to cap variables, make sure you compare the performance of your models or the results of your ana</st><a id="_idTextAnchor733"/><a id="_idTextAnchor734"/><st c="33314">lysis before and after </st><span class="No-Break"><st c="33338">replacing outliers.</st></span></p></li>			</ol>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor735"/><st c="33357">How it works...</st></h2>
			<p><st c="33373">The </st><strong class="source-inline"><st c="33378">clip()</st></strong><st c="33384"> function from</st><a id="_idIndexMarker411"/><st c="33398"> pandas is used to cap values at lower or upper specified limits. </st><st c="33464">In this recipe, we found those limits using the mean and standard deviation, and then clipped the variable so that all observations took values within those limits. </st><st c="33629">The minimum value of the </st><strong class="source-inline"><st c="33654">worst smoothness</st></strong><st c="33670"> variable was actually greater than the lower limit we found in </st><em class="italic"><st c="33734">step 4</st></em><st c="33740">, so no values were replaced at the left of its distribution. </st><st c="33802">However, there were values greater than the upper limit from </st><em class="italic"><st c="33863">step 4</st></em><st c="33869">, and those were replaced with the limit. </st><st c="33911">This means that the minimum value of the transformed variable and the original variable coincide, but the maximum values </st><span class="No-Break"><st c="34032">do not.</st></span></p>
			<p><st c="34039">We used </st><strong class="source-inline"><st c="34048">feature-engine</st></strong><st c="34062"> to replace outliers in multiple variables simultaneously. </st><strong class="source-inline"><st c="34121">Winsorizer()</st></strong><st c="34133"> can identify outliers based on the mean and standard deviation, the IQR proximity rule, MAD, or by using percentiles. </st><st c="34252">We can modify this behavior through the </st><span class="No-Break"><strong class="source-inline"><st c="34292">capping_method</st></strong></span><span class="No-Break"><st c="34306"> parameter.</st></span></p>
			<p><st c="34317">The methods to identify outliers can be made more or less conservative by changing the factor by which we multiply the IQR, the standard deviation, or MAD. </st><st c="34474">With </st><strong class="source-inline"><st c="34479">Winsorizer()</st></strong><st c="34491">, we can control the strength of the methods through the </st><span class="No-Break"><strong class="source-inline"><st c="34548">fold</st></strong></span><span class="No-Break"><st c="34552"> parameter.</st></span></p>
			<p><st c="34563">With </st><strong class="source-inline"><st c="34569">tails</st></strong><st c="34574"> set to </st><strong class="source-inline"><st c="34582">"both"</st></strong><st c="34588">, </st><strong class="source-inline"><st c="34590">Winsorizer()</st></strong><st c="34602"> found and replaced outliers at both ends of the variables’ distribution. </st><st c="34676">To replace outliers at either end, we can pass </st><strong class="source-inline"><st c="34723">"left"</st></strong><st c="34729"> or </st><strong class="source-inline"><st c="34733">"right"</st></strong><st c="34740"> to the </st><span class="No-Break"><strong class="source-inline"><st c="34748">tails</st></strong></span><span class="No-Break"><st c="34753"> parameter.</st></span></p>
			<p><strong class="source-inline"><st c="34764">Winsorizer()</st></strong><st c="34777"> adopts the scikit-learn functionality with the </st><strong class="source-inline"><st c="34825">fit()</st></strong><st c="34830"> method, to learn parameters, and </st><strong class="source-inline"><st c="34864">transform()</st></strong><st c="34875"> to modify the dataset. </st><st c="34899">With </st><strong class="source-inline"><st c="34904">fit()</st></strong><st c="34909">, the transformer learned and </st><a id="_idIndexMarker412"/><st c="34939">stored the limits for each variable. </st><st c="34976">With </st><strong class="source-inline"><st c="34981">transform()</st></strong><st c="34992">, it replaced the values of the ou</st><a id="_idTextAnchor736"/><a id="_idTextAnchor737"/><st c="35026">tliers, returning </st><span class="No-Break"><st c="35045">pandas DataFrames.</st></span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor738"/><st c="35063">See also</st></h2>
			<p><strong class="source-inline"><st c="35072">feature-engine</st></strong><st c="35087"> has </st><strong class="source-inline"><st c="35092">ArbitraryOutlierCapper()</st></strong><st c="35116">, which caps variables at arbitrary minimum and maximum </st><span class="No-Break"><st c="35172">values: </st></span><a href="https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html"><span class="No-Break"><st c="35180">https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html</st></span></a><span class="No-Break"><st c="35272">.</st></span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor739"/><st c="35273">Applying winsorization</st></h1>
			<p><st c="35296">Winsorizing, or winsorization, consists</st><a id="_idIndexMarker413"/><st c="35336"> of replacing extreme, poorly known </st><a id="_idIndexMarker414"/><st c="35372">observations, that is, outliers, with the magnitude of the next largest (or smallest) observation. </st><st c="35471">It’s similar to the procedure described in the previous recipe, </st><em class="italic"><st c="35535">Bringing outliers back within acceptable limits</st></em><st c="35582">, but not exactly the same. </st><st c="35610">Winsorization involves replacing the </st><em class="italic"><st c="35647">same number of outliers</st></em><st c="35670"> at both ends of the distribution, which makes Winsorization a symmetric process. </st><st c="35752">This guarantees that the </st><strong class="bold"><st c="35777">Winsorized mean</st></strong><st c="35792">, that is, the</st><a id="_idIndexMarker415"/><st c="35806"> mean estimated after replacing outliers, remains a robust estimator of the central tendency of </st><span class="No-Break"><st c="35902">the variable.</st></span></p>
			<p><st c="35915">In practice, to</st><a id="_idIndexMarker416"/><st c="35931"> remove a similar number of observations at both tails, we’d use percentiles. </st><st c="36009">For example, the 5th percentile is the value below which 5% of the observations lie and the 95th percentile is the value beyond which 5% of the observations lie. </st><st c="36171">Using these values as replacements might result in replacing a similar number of observations on both tails, but it’s not guaranteed. </st><st c="36305">If the dataset contains repeated values, obtaining reliable percentiles is challenging and can lead to an uneven replacement of values at each tail. </st><st c="36454">If this happens, then the winsorized mean is not a good estimator of the central tenden</st><a id="_idTextAnchor740"/><a id="_idTextAnchor741"/><st c="36541">cy. </st><st c="36546">In this recipe, we will </st><span class="No-Break"><st c="36570">apply winsorization.</st></span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor742"/><st c="36590">How to do it...</st></h2>
			<p><st c="36606">We will cap all </st><a id="_idIndexMarker417"/><st c="36623">variables of the breast cancer dataset at their 5th and </st><span class="No-Break"><st c="36679">95th percentiles:</st></span></p>
			<ol>
				<li><st c="36696">Let’s import the required Python libraries </st><span class="No-Break"><st c="36740">and functions:</st></span><pre class="source-code"><st c="36754">
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="36909">Load the breast cancer dataset </st><span class="No-Break"><st c="36941">from scikit-learn:</st></span><pre class="source-code"><st c="36959">
X, y = load_breast_cancer(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="37018">Separate</st><a id="_idIndexMarker418"/><st c="37027"> the data into a train and </st><span class="No-Break"><st c="37054">test sets:</st></span><pre class="source-code"><st c="37064">
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="37156">Capture the 5th and 95th percentiles of each variable </st><span class="No-Break"><st c="37211">in dictionaries:</st></span><pre class="source-code"><st c="37227">
q05 = X_train.quantile(0.05).to_dict()
q95 = X_train.quantile(0.95).to_dict()</st></pre></li>				<li><st c="37305">Let’s now replace values beyond those percentiles with the respective percentiles for all variables </st><span class="No-Break"><st c="37406">at once:</st></span><pre class="source-code"><st c="37414">
train_t = X_train.clip(lower=q05, upper=q95)
test_t = X_test.clip(lower=q05, upper=q95)</st></pre></li>				<li><st c="37502">Let’s display the minimum, maximum, and mean values of one variable </st><span class="No-Break"><st c="37571">before winsorization:</st></span><pre class="source-code"><st c="37592">
var = 'worst smoothness'
X_train[var].agg(["min", "max", "mean"])</st></pre><p class="list-inset"><st c="37658">We can see the values in the </st><span class="No-Break"><st c="37688">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="37705">min      0.071170</st></strong>
<strong class="bold"><st c="37718">max      0.222600</st></strong>
<strong class="bold"><st c="37731">mean     0.132529</st></strong>
<strong class="bold"><st c="37745">Name: worst smoothness, dtype: float64</st></strong></pre></li>				<li><st c="37784">Display the</st><a id="_idIndexMarker419"/><st c="37796"> minimum, maximum, and mean values </st><a id="_idIndexMarker420"/><st c="37831">of the same variable </st><span class="No-Break"><st c="37852">after winsorization:</st></span><pre class="source-code"><st c="37872">
train_t[var].agg([„min", „max"])</st></pre><p class="list-inset"><st c="37905">In the following output, we can see that the minimum and maximum values correspond to the percentiles. </st><st c="38009">However, the mean is quite similar to the original mean of </st><span class="No-Break"><st c="38068">the variable:</st></span></p><pre class="source-code"><strong class="bold"><st c="38081">min      0.096053</st></strong>
<strong class="bold"><st c="38094">max      0.173215</st></strong>
<strong class="bold"><st c="38107">mean     0.132063</st></strong>
<strong class="bold"><st c="38121">Name: worst smoothness, dtype: float64</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="38160">Note</st></p>
			<p class="callout"><st c="38165">If you want to use winsorization as part of a scikit-learn pipeline, you can use the </st><strong class="source-inline"><st c="38251">feature-engine</st></strong><st c="38265"> library’s </st><strong class="source-inline"><st c="38276">Winsorizer()</st></strong><st c="38288">, by setting it up </st><span class="No-Break"><st c="38307">as follows:</st></span></p>
			<p class="callout"><strong class="source-inline"><st c="38318">capper = </st></strong><span class="No-Break"><strong class="source-inline"><st c="38328">Winsorizer(</st></strong></span></p>
			<p class="callout"><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="38339">capping_method="quantiles",</st></strong></span></p>
			<p class="callout"><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="38367">tail="both",</st></strong></span></p>
			<p class="callout"><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="38380">fold=0.05,</st></strong></span></p>
			<p class="callout"><strong class="source-inline"><st c="38391">)</st></strong></p>
			<p class="callout"><st c="38393">After this, proceed with the </st><strong class="source-inline"><st c="38422">fit()</st></strong><st c="38427"> and </st><strong class="source-inline"><st c="38432">transform()</st></strong><st c="38443"> methods as described in the </st><em class="italic"><st c="38472">Bringing outliers back within acceptable </st></em><span class="No-Break"><em class="italic"><st c="38513">limits</st></em></span><span class="No-Break"><st c="38519"> recipe.</st></span></p>
			<p class="list-inset"><st c="38527">It’s </st><a id="_idIndexMarker421"/><st c="38533">worth </st><a id="_idIndexMarker422"/><st c="38539">noting that despite employing percentiles, the procedure didn’t precisely replace the same number of observations on both sides of the distribution. </st><st c="38688">If you intend to winsorize your variables, compare the out</st><a id="_idTextAnchor743"/><a id="_idTextAnchor744"/><st c="38746">comes of your analyses before and </st><span class="No-Break"><st c="38781">after winsorization.</st></span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor745"/><st c="38801">How it works...</st></h2>
			<p><st c="38817">We used pandas’ </st><strong class="source-inline"><st c="38834">quantiles()</st></strong><st c="38845"> to obtain the 5th and 95th percentiles of all the variables in the dataset, and combined it with </st><strong class="source-inline"><st c="38943">to_dict()</st></strong><st c="38952"> to retain those percentiles in dictionaries, where the keys were the variables and the values were the percentiles. </st><st c="39069">We then passed these dictionaries to pandas’ </st><strong class="source-inline"><st c="39114">clip()</st></strong><st c="39120"> to replace values smaller or larger than those percentiles by the percentiles. </st><st c="39200">By using dictionaries, we capped multiple variables </st><span class="No-Break"><st c="39252">at once.</st></span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor746"/><st c="39260">See also</st></h2>
			<p><st c="39269">For more details about how winsorization affects the mean and standard deviation in symmetric and asymmetric replacements, check out the </st><span class="No-Break"><st c="39407">original article:</st></span></p>
			<p><st c="39424">Dixon W. </st><em class="italic"><st c="39434">Simplified Estimation from Censored Normal Samples. </st><st c="39486">The Annals of Mathematica</st><a id="_idTextAnchor747"/><st c="39511">l Statistics</st></em><st c="39524">, </st><span class="No-Break"><st c="39526">1960. </st></span><a href="https://www.jstor.org/stable/2237953"><span class="No-Break"><st c="39532">http://www.jstor.org/stable/2237953</st></span></a></p>
		</div>
	<div id="charCountTotal" value="39567"/></body></html>