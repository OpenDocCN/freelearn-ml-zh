["```py\n# import out grid search module\nfrom sklearn.model_selection import GridSearchCV\n\ndef get_best_model_and_accuracy(model, params, X, y):\n    grid = GridSearchCV(model, # the model to grid search\n                        params, # the parameter set to try \n                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0\n    grid.fit(X, y) # fit the model and parameters\n    # our classical metric for performance\n    print \"Best Accuracy: {}\".format(grid.best_score_)\n    # the best parameters that caused the best accuracy\n    print \"Best Parameters: {}\".format(grid.best_params_)\n    # the average time it took a model to fit to the data (in seconds)\n    print \"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3))\n    # the average time it took a model to predict out of sample data (in seconds)\n    # this metric gives us insight into how this model will perform in real-time analysis\n    print \"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3))\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# we will set a random seed to ensure that whenever we use random numbers \n# which is a good amount, we will achieve the same random numbers\nnp.random.seed(123)\n```", "```py\n# archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n# import the newest csv\ncredit_card_default = pd.read_csv('../data/credit_card_default.csv')\n```", "```py\n# 30,000 rows and 24 columns\ncredit_card_default.shape \n```", "```py\n# Some descriptive statistics\n# We invoke the .T to transpose the matrix for better viewing\ncredit_card_default.describe().T\n```", "```py\n# check for missing values, none in this dataset\ncredit_card_default.isnull().sum()\nLIMIT_BAL                     0\nSEX                           0\nEDUCATION                     0\nMARRIAGE                      0\nAGE                           0\nPAY_0                         0\nPAY_2                         0\nPAY_3                         0\nPAY_4                         0\nPAY_5                         0\nPAY_6                         0\nBILL_AMT1                     0\nBILL_AMT2                     0\nBILL_AMT3                     0\nBILL_AMT4                     0\nBILL_AMT5                     0\nBILL_AMT6                     0\nPAY_AMT1                      0\nPAY_AMT2                      0\nPAY_AMT3                      0\nPAY_AMT4                      0\nPAY_AMT5                      0\nPAY_AMT6                      0\ndefault payment next month    0\ndtype: int64\n```", "```py\n# Create our feature matrix\nX = credit_card_default.drop('default payment next month', axis=1)\n\n# create our response variable\ny = credit_card_default['default payment next month']\n```", "```py\n# get our null accuracy rate\ny.value_counts(normalize=True)\n\n0    0.7788\n1    0.2212\n```", "```py\n# Import four machine learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n```", "```py\n# Set up some parameters for our grid search\n# We will start with four different machine learning model parameters\n\n# Logistic Regression\nlr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}\n\n# KNN\nknn_params = {'n_neighbors': [1, 3, 5, 7]}\n\n# Decision Tree\ntree_params = {'max_depth':[None, 1, 3, 5, 7]}\n\n# Random Forest\nforest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}\n```", "```py\n# instantiate the four machine learning models\nlr = LogisticRegression()\nknn = KNeighborsClassifier()\nd_tree = DecisionTreeClassifier()\nforest = RandomForestClassifier()\n```", "```py\nget_best_model_and_accuracy(lr, lr_params, X, y)\n\nBest Accuracy: 0.809566666667\nBest Parameters: {'penalty': 'l1', 'C': 0.1}\nAverage Time to Fit (s): 0.602\nAverage Time to Score (s): 0.002\n```", "```py\nget_best_model_and_accuracy(knn, knn_params, X, y)\n\nBest Accuracy: 0.760233333333\nBest Parameters: {'n_neighbors': 7}\nAverage Time to Fit (s): 0.035\nAverage Time to Score (s): 0.88\n```", "```py\n# bring in some familiar modules for dealing with this sort of thing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# construct pipeline parameters based on the parameters\n# for KNN on its own\nknn_pipe_params = {'classifier__{}'.format(k): v for k, v in knn_params.iteritems()}\n\n# KNN requires a standard scalar due to using Euclidean distance # as the main equation for predicting observations\nknn_pipe = Pipeline([('scale', StandardScaler()), ('classifier', knn)])\n\n# quick to fit, very slow to predict\nget_best_model_and_accuracy(knn_pipe, knn_pipe_params, X, y)\n\nprint knn_pipe_params  # {'classifier__n_neighbors': [1, 3, 5, 7]} \n\nBest Accuracy: 0.8008 \nBest Parameters: {'classifier__n_neighbors': 7} \nAverage Time to Fit (s): 0.035 \nAverage Time to Score (s): 6.723\n```", "```py\nget_best_model_and_accuracy(d_tree, tree_params, X, y)\n\nBest Accuracy: 0.820266666667\nBest Parameters: {'max_depth': 3}\nAverage Time to Fit (s): 0.158\nAverage Time to Score (s): 0.002\n```", "```py\nget_best_model_and_accuracy(forest, forest_params, X, y)\n\nBest Accuracy: 0.819566666667\nBest Parameters: {'n_estimators': 50, 'max_depth': 7}\nAverage Time to Fit (s): 1.107\nAverage Time to Score (s): 0.044\n```", "```py\ncredit_card_default.corr()\n```", "```py\n# using seaborn to generate heatmaps\nimport seaborn as sns\nimport matplotlib.style as style\n# Use a clean stylizatino for our charts and graphs\nstyle.use('fivethirtyeight')\n\nsns.heatmap(credit_card_default.corr())\n```", "```py\n# just correlations between every feature and the response\ncredit_card_default.corr()['default payment next month'] \n\nLIMIT_BAL                    -0.153520\nSEX                          -0.039961\nEDUCATION                     0.028006\nMARRIAGE                     -0.024339\nAGE                           0.013890\nPAY_0                         0.324794\nPAY_2                         0.263551\nPAY_3                         0.235253\nPAY_4                         0.216614\nPAY_5                         0.204149\nPAY_6                         0.186866\nBILL_AMT1                    -0.019644\nBILL_AMT2                    -0.014193\nBILL_AMT3                    -0.014076\nBILL_AMT4                    -0.010156\nBILL_AMT5                    -0.006760\nBILL_AMT6                    -0.005372\nPAY_AMT1                     -0.072929\nPAY_AMT2                     -0.058579\nPAY_AMT3                     -0.056250\nPAY_AMT4                     -0.056827\nPAY_AMT5                     -0.055124\nPAY_AMT6                     -0.053183\ndefault payment next month    1.000000\n```", "```py\n# filter only correlations stronger than .2 in either direction (positive or negative)\n\ncredit_card_default.corr()['default payment next month'].abs() > .2\n\nLIMIT_BAL                     False\nSEX                           False\nEDUCATION                     False\nMARRIAGE                      False\nAGE                           False\nPAY_0                          True\nPAY_2                          True\nPAY_3                          True\nPAY_4                          True\nPAY_5                          True\nPAY_6                         False\nBILL_AMT1                     False\nBILL_AMT2                     False\nBILL_AMT3                     False\nBILL_AMT4                     False\nBILL_AMT5                     False\nBILL_AMT6                     False\nPAY_AMT1                      False\nPAY_AMT2                      False\nPAY_AMT3                      False\nPAY_AMT4                      False\nPAY_AMT5                      False\nPAY_AMT6                      False\ndefault payment next month     True\n```", "```py\n# store the features\nhighly_correlated_features = credit_card_default.columns[credit_card_default.corr()['default payment next month'].abs() > .2]\n\nhighly_correlated_features\n\nIndex([u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5',\n       u'default payment next month'],\n      dtype='object')\n```", "```py\n# drop the response variable\nhighly_correlated_features = highly_correlated_features.drop('default payment next month')\n\nhighly_correlated_features\n\nIndex([u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5'], dtype='object')\n```", "```py\n# only include the five highly correlated features\nX_subsetted = X[highly_correlated_features]\n\nget_best_model_and_accuracy(d_tree, tree_params, X_subsetted, y) \n\n# barely worse, but about 20x faster to fit the model\nBest Accuracy: 0.819666666667 \nBest Parameters: {'max_depth': 3} \nAverage Time to Fit (s): 0.01 \nAverage Time to Score (s): 0.002\n```", "```py\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nclass CustomCorrelationChooser(TransformerMixin, BaseEstimator):\n    def __init__(self, response, cols_to_keep=[], threshold=None):\n        # store the response series\n        self.response = response\n        # store the threshold that we wish to keep\n        self.threshold = threshold\n        # initialize a variable that will eventually\n        # hold the names of the features that we wish to keep\n        self.cols_to_keep = cols_to_keep\n\n    def transform(self, X):\n        # the transform method simply selects the appropiate\n        # columns from the original dataset\n        return X[self.cols_to_keep]\n\n    def fit(self, X, *_):\n        # create a new dataframe that holds both features and response\n        df = pd.concat([X, self.response], axis=1)\n        # store names of columns that meet correlation threshold\n        self.cols_to_keep = df.columns[df.corr()[df.columns[-1]].abs() > self.threshold]\n        # only keep columns in X, for example, will remove response variable\n        self.cols_to_keep = [c for c in self.cols_to_keep if c in X.columns]\n        return self\n```", "```py\n# instantiate our new feature selector\nccc = CustomCorrelationChooser(threshold=.2, response=y)\nccc.fit(X)\n\nccc.cols_to_keep\n\n['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5']\n```", "```py\nccc.transform(X).head()\n```", "```py\n# instantiate our feature selector with the response variable set\nccc = CustomCorrelationChooser(response=y)\n\n# make our new pipeline, including the selector\nccc_pipe = Pipeline([('correlation_select', ccc), \n ('classifier', d_tree)])\n\n# make a copy of the decisino tree pipeline parameters\nccc_pipe_params = deepcopy(tree_pipe_params)\n\n# update that dictionary with feature selector specific parameters\nccc_pipe_params.update({\n 'correlation_select__threshold':[0, .1, .2, .3]})\n\nprint ccc_pipe_params  #{'correlation_select__threshold': [0, 0.1, 0.2, 0.3], 'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}\n\n# better than original (by a little, and a bit faster on \n# average overall\nget_best_model_and_accuracy(ccc_pipe, ccc_pipe_params, X, y) \n\nBest Accuracy: 0.8206\nBest Parameters: {'correlation_select__threshold': 0.1, 'classifier__max_depth': 5}\nAverage Time to Fit (s): 0.105\nAverage Time to Score (s): 0.003\n```", "```py\n# check the threshold of .1\nccc = CustomCorrelationChooser(threshold=0.1, response=y)\nccc.fit(X)\n\n# check which columns were kept\nccc.cols_to_keep\n['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n```", "```py\n# SelectKBest selects features according to the k highest scores of a given scoring function\nfrom sklearn.feature_selection import SelectKBest\n\n# This models a statistical test known as ANOVA\nfrom sklearn.feature_selection import f_classif\n\n# f_classif allows for negative values, not all do\n# chi2 is a very common classification criteria but only allows for positive values\n# regression has its own statistical tests\n```", "```py\n# keep only the best five features according to p-values of ANOVA test\nk_best = SelectKBest(f_classif, k=5)\n```", "```py\n# matrix after selecting the top 5 features\nk_best.fit_transform(X, y)\n\n# 30,000 rows x 5 columns\narray([[ 2,  2, -1, -1, -2],\n       [-1,  2,  0,  0,  0],\n       [ 0,  0,  0,  0,  0],\n       ..., \n       [ 4,  3,  2, -1,  0],\n       [ 1, -1,  0,  0,  0],\n       [ 0,  0,  0,  0,  0]])\n```", "```py\n# get the p values of columns\nk_best.pvalues_\n\n# make a dataframe of features and p-values\n# sort that dataframe by p-value\np_values = pd.DataFrame({'column': X.columns, 'p_value': k_best.pvalues_}).sort_values('p_value')\n\n# show the top 5 features\np_values.head()\n```", "```py\n# features with a low p value\np_values[p_values['p_value'] < .05]\n```", "```py\n# features with a high p value\np_values[p_values['p_value'] >= .05]\n```", "```py\nk_best = SelectKBest(f_classif)\n\n# Make a new pipeline with SelectKBest\nselect_k_pipe = Pipeline([('k_best', k_best), \n ('classifier', d_tree)])\n\nselect_k_best_pipe_params = deepcopy(tree_pipe_params)\n# the 'all' literally does nothing to subset\nselect_k_best_pipe_params.update({'k_best__k':range(1,23) + ['all']})\n\nprint select_k_best_pipe_params # {'k_best__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 'all'], 'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}\n\n# comparable to our results with correlationchooser\nget_best_model_and_accuracy(select_k_pipe, select_k_best_pipe_params, X, y)\n\nBest Accuracy: 0.8206\nBest Parameters: {'k_best__k': 7, 'classifier__max_depth': 5}\nAverage Time to Fit (s): 0.102\nAverage Time to Score (s): 0.002\n```", "```py\nk_best = SelectKBest(f_classif, k=7)\n\n# lowest 7 p values match what our custom correlationchooser chose before\n# ['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n\np_values.head(7)\n```", "```py\n# sanity check\n# If we only the worst columns\nthe_worst_of_X = X[X.columns.drop(['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])]\n\n# goes to show, that selecting the wrong features will \n# hurt us in predictive performance\nget_best_model_and_accuracy(d_tree, tree_params, the_worst_of_X, y)\n\nBest Accuracy: 0.783966666667\nBest Parameters: {'max_depth': 5}\nAverage Time to Fit (s): 0.21\nAverage Time to Score (s): 0.002\n```", "```py\n# bring in the tweet dataset\ntweets = pd.read_csv('../data/twitter_sentiment.csv', \n encoding='latin1')\n```", "```py\ntweets.head()\n```", "```py\ntweets_X, tweets_y = tweets['SentimentText'], tweets['Sentiment']\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n# import a naive bayes to help predict and fit a bit faster \nfrom sklearn.naive_bayes import MultinomialNB\n\nfeaturizer = CountVectorizer()\n\ntext_pipe = Pipeline([('featurizer', featurizer), \n                 ('classify', MultinomialNB())])\n\ntext_pipe_params = {'featurizer__ngram_range':[(1, 2)], \n               'featurizer__max_features': [5000, 10000],\n               'featurizer__min_df': [0., .1, .2, .3], \n               'featurizer__max_df': [.7, .8, .9, 1.]}\n\nget_best_model_and_accuracy(text_pipe, text_pipe_params, \n                            tweets_X, tweets_y)\n\nBest Accuracy: 0.755753132845\nBest Parameters: {'featurizer__min_df': 0.0, 'featurizer__ngram_range': (1, 2), 'featurizer__max_df': 0.7, 'featurizer__max_features': 10000}\nAverage Time to Fit (s): 5.808\nAverage Time to Score (s): 0.957\n```", "```py\n# Let's try a more basic pipeline, but one that relies on SelectKBest as well\nfeaturizer = CountVectorizer(ngram_range=(1, 2))\n\nselect_k_text_pipe = Pipeline([('featurizer', featurizer), \n                      ('select_k', SelectKBest()),\n                      ('classify', MultinomialNB())])\n\nselect_k_text_pipe_params = {'select_k__k': [1000, 5000]}\n\nget_best_model_and_accuracy(select_k_text_pipe, \n                            select_k_text_pipe_params, \n                            tweets_X, tweets_y)\n\nBest Accuracy: 0.755703127344\nBest Parameters: {'select_k__k': 10000}\nAverage Time to Fit (s): 6.927\nAverage Time to Score (s): 1.448\n```", "```py\n# create a brand new decision tree classifier\ntree = DecisionTreeClassifier()\n\ntree.fit(X, y)\n```", "```py\n# note that we have some other features in play besides what our last two selectors decided for us\n\nimportances = pd.DataFrame({'importance': tree.feature_importances_, 'feature':X.columns}).sort_values('importance', ascending=False)\n\nimportances.head()\n```", "```py\n# similar to SelectKBest, but not with statistical tests\nfrom sklearn.feature_selection import SelectFromModel\n```", "```py\n# instantiate a class that choses features based\n# on feature importances according to the fitting phase\n# of a separate decision tree classifier\nselect_from_model = SelectFromModel(DecisionTreeClassifier(), \n threshold=.05)\n```", "```py\nselected_X = select_from_model.fit_transform(X, y)\nselected_X.shape\n\n(30000, 9)\n```", "```py\n# to speed things up a bit in the future\ntree_pipe_params = {'classifier__max_depth': [1, 3, 5, 7]}\n\nfrom sklearn.pipeline import Pipeline\n\n# create a SelectFromModel that is tuned by a DecisionTreeClassifier\nselect = SelectFromModel(DecisionTreeClassifier())\n\nselect_from_pipe = Pipeline([('select', select),\n                             ('classifier', d_tree)])\n\nselect_from_pipe_params = deepcopy(tree_pipe_params)\n\nselect_from_pipe_params.update({\n 'select__threshold': [.01, .05, .1, .2, .25, .3, .4, .5, .6, \"mean\", \"median\", \"2.*mean\"],\n 'select__estimator__max_depth': [None, 1, 3, 5, 7]\n })\n\nprint select_from_pipe_params  # {'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__max_depth': [None, 1, 3, 5, 7], 'classifier__max_depth': [1, 3, 5, 7]}\n\nget_best_model_and_accuracy(select_from_pipe, \n select_from_pipe_params, \n X, y)\n\n# not better than original\nBest Accuracy: 0.820266666667\nBest Parameters: {'select__threshold': 0.01, 'select__estimator__max_depth': None, 'classifier__max_depth': 3} \nAverage Time to Fit (s): 0.192 \nAverage Time to Score (s): 0.002\n```", "```py\n# set the optimal params to the pipeline\nselect_from_pipe.set_params(**{'select__threshold': 0.01, \n 'select__estimator__max_depth': None, \n 'classifier__max_depth': 3})\n\n# fit our pipeline to our data\nselect_from_pipe.steps[0][1].fit(X, y)\n\n# list the columns that the SVC selected by calling the get_support() method from SelectFromModel\nX.columns[select_from_pipe.steps[0][1].get_support()]\n\n[u'LIMIT_BAL', u'SEX', u'EDUCATION', u'MARRIAGE', u'AGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_6', u'BILL_AMT1', u'BILL_AMT2', u'BILL_AMT3', u'BILL_AMT4', u'BILL_AMT5', u'BILL_AMT6', u'PAY_AMT1', u'PAY_AMT2', u'PAY_AMT3', u'PAY_AMT4', u'PAY_AMT5', u'PAY_AMT6']\n```", "```py\n# a new selector that uses the coefficients from a regularized logistic regression as feature importances\nlogistic_selector = SelectFromModel(LogisticRegression())\n\n# make a new pipeline that uses coefficients from LogistisRegression as a feature ranker\nregularization_pipe = Pipeline([('select', logistic_selector), \n ('classifier', tree)])\n\nregularization_pipe_params = deepcopy(tree_pipe_params)\n\n# try l1 regularization and l2 regularization\nregularization_pipe_params.update({\n 'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],\n 'select__estimator__penalty': ['l1', 'l2'],\n })\n\nprint regularization_pipe_params  # {'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__penalty': ['l1', 'l2']}\n\nget_best_model_and_accuracy(regularization_pipe, \n regularization_pipe_params, \n X, y)\n\n# better than original, in fact the best so far, and much faster on the scoring side\nBest Accuracy: 0.821166666667 Best Parameters: {'select__threshold': 0.01, 'classifier__max_depth': 5, 'select__estimator__penalty': 'l1'} \nAverage Time to Fit (s): 0.51 \nAverage Time to Score (s): 0.001\n```", "```py\n# set the optimal params to the pipeline\nregularization_pipe.set_params(**{'select__threshold': 0.01, \n 'classifier__max_depth': 5, \n 'select__estimator__penalty': 'l1'})\n\n# fit our pipeline to our data\nregularization_pipe.steps[0][1].fit(X, y)\n\n# list the columns that the Logisti Regression selected by calling the get_support() method from SelectFromModel\nX.columns[regularization_pipe.steps[0][1].get_support()]\n\n[u'SEX', u'EDUCATION', u'MARRIAGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5']\n```", "```py\n# SVC is a linear model that uses linear supports to \n# seperate classes in euclidean space\n# This model can only work for binary classification tasks\nfrom sklearn.svm import LinearSVC\n\n# Using a support vector classifier to get coefficients\nsvc_selector = SelectFromModel(LinearSVC())\n\nsvc_pipe = Pipeline([('select', svc_selector), \n ('classifier', tree)])\n\nsvc_pipe_params = deepcopy(tree_pipe_params)\n\nsvc_pipe_params.update({\n 'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],\n 'select__estimator__penalty': ['l1', 'l2'],\n 'select__estimator__loss': ['squared_hinge', 'hinge'],\n 'select__estimator__dual': [True, False]\n })\n\nprint svc_pipe_params  # 'select__estimator__loss': ['squared_hinge', 'hinge'], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__penalty': ['l1', 'l2'], 'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__dual': [True, False]}\n\nget_best_model_and_accuracy(svc_pipe, \n svc_pipe_params, \n X, y) \n\n# better than original, in fact the best so far, and much faster on the scoring side\nBest Accuracy: 0.821233333333\nBest Parameters: {'select__estimator__loss': 'squared_hinge', 'select__threshold': 0.01, 'select__estimator__penalty': 'l1', 'classifier__max_depth': 5, 'select__estimator__dual': False}\nAverage Time to Fit (s): 0.989\nAverage Time to Score (s): 0.001\n```", "```py\n# set the optimal params to the pipeline\nsvc_pipe.set_params(**{'select__estimator__loss': 'squared_hinge', \n 'select__threshold': 0.01, \n 'select__estimator__penalty': 'l1', \n 'classifier__max_depth': 5, \n 'select__estimator__dual': False})\n\n# fit our pipeline to our data\nsvc_pipe.steps[0][1].fit(X, y)\n\n# list the columns that the SVC selected by calling the get_support() method from SelectFromModel\nX.columns[svc_pipe.steps[0][1].get_support()]\n\n[u'SEX', u'EDUCATION', u'MARRIAGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_5']\n```"]