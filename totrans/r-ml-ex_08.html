<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Social Media Analysis &#x2013; Analyzing Twitter Data" id="1O8H61-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Social Media Analysis – Analyzing Twitter Data</h1></div></div></div><p class="calibre8">Connected is the word that describes life in the 21<sup class="calibre15">st</sup> century. Though various factors contribute to the term connected, there's one aspect which has played a pivotal role. It's called the <a id="id506" class="calibre1"/>Web. The Web, which has made distance an irrelevant metric and blurred socio-economic boundaries, is a world in itself and we all are a part of it. The Web or Internet in particular has been a central entity in this data-driven revolution. As we have seen in our previous chapters, for most modern day problems, it is the Web/Internet (henceforth used interchangeably) that acts as a source of data. Be it e-commerce platforms or financial domain, the Internet provides us with huge amounts of data every second. There's another ocean of data within this virtual world which touches our lives at a very personal level. Social networks, or social media, is a behemoth of information and the topic for this chapter.</p><p class="calibre8">In the previous chapter, we covered the financial domain, where we analyzed and predicted credit risk for customers of a certain bank. We now shift gears and move into the realm of social media and see how machine learning and R empower us to uncover insights from this ocean of data.</p><p class="calibre8">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Data mining specifics for social networks</li><li class="listitem">The importance and use of different data visualizations</li><li class="listitem">An overview of how to connect and collect Twitter data</li><li class="listitem">Utilizing Twitter data to uncover amazing insights</li><li class="listitem">Seeing how social networks pose new challenges to the data mining process</li></ul></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Social Media Analysis &#x2013; Analyzing Twitter Data" id="1O8H61-973e731d75c2419489ee73e3a0cf4be8">
<div class="book" title="Social networks (Twitter)"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec53" class="calibre1"/>Social networks (Twitter)</h1></div></div></div><p class="calibre8">We all use social <a id="id507" class="calibre1"/>networks day in and day out. There are numerous social networks catering to all sorts of ideologies and philosophies, but Facebook and Twitter (barring a couple more) have become synonymous with the term social network itself. These two social networks enjoy popularity not only because of their uniqueness and the quality of service but because of the way they enable us to interact in a very intuitive way. As we saw with recommendation engines used in e-commerce websites (see <a class="calibre1" title="Chapter 4. Building a Product Recommendation System" href="part0032_split_000.html#UGI01-973e731d75c2419489ee73e3a0cf4be8">Chapter 4</a>, <span class="strong"><em class="calibre10">Building a Product Recommendation System</em></span>), social networks have existed long before Facebook, Twitter, or even the Internet.</p><p class="calibre8">Social networks have <a id="id508" class="calibre1"/>interested scientists and mathematicians alike. It is an interdisciplinary topic which spans but is not limited to sociology, psychology, biology, economics, communication studies, and information science. Various theories have been developed to analyze social networks and their impact on human lives in the form of factors influencing economics, demographics, health, language, literacy, crime, and more.</p><p class="calibre8">Studies done as early as the late 1800s form the basis of what we today refer to as social networks. A social network, as the word itself says, is a sort of connection/network between nodes or entities represented by humans and elements affecting social life. More formally, it is a network depicting relationships and interactions. Hence, it is not surprising to see various graph theories and algorithms being employed to understand social networks. Where the 19<sup class="calibre15">th</sup> and 20<sup class="calibre15">th</sup> centuries were limited to theoretical models and painstaking social experiments, the 21<sup class="calibre15">st</sup> century's technology has opened the doors for these theories to be tested, fine tuned, and modeled to help understand the dynamics of social interactions. Though testing these theories by some social networks (called social experiments) have been caught in controversies, such topics are beyond the scope of this book. We shall limit ourselves to the algorithmic/data science space and leave the controversies for the experts to discuss.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note18" class="calibre1"/>Note</h3><p class="calibre8">The Milgram Experiment, or the small world experiment, was conducted in the late 1960s to examine the average path length for people in United States. As part of this experiment, random people were selected as starting points of a mail chain. These random people were tasked to send the mail to the next person so that the mail gets one step closer to its destination (somewhere in Boston) and so on. An average of six hops to the destination is the documented result of this famous experiment. Urban folklore suggests the phrase <span class="strong"><em class="calibre10">6 degrees of separation</em></span> originated from this experiment, even though Dr. Milgram never used the term himself! He conducted many more experiments; search and be amazed.</p><p class="calibre8">Source:</p><p class="calibre8"><a class="calibre1" href="http://www.simplypsychology.org/milgram.html">http://www.simplypsychology.org/milgram.html</a></p></div><p class="calibre8">Before we jump into the specifics, let us try and understand the reason behind choosing Twitter as our point of analysis for this and the upcoming chapter. Let us begin with understanding what Twitter is and why is it so popular with both end users and data scientists alike.</p><p class="calibre8">Twitter, as we all know, is a social network/micro-blogging service that enables its users to send and receive tweets of a maximum of 140 characters. But what makes Twitter so popular is the way it caters to the basic human instincts. We, humans, are curious creatures with an <a id="id509" class="calibre1"/>incessant need to be heard. It is important for us to have someone or some place to voice our opinions. We love to share our experiences, feats, failures, and ideas. At some level or other, we also want to know what our peers are up to, what's keeping celebrities busy, or simply what's on the news. Twitter addresses just that.</p><p class="calibre8">With multiple social networks existing long before Twitter came into existence, it wasn't some other service which <a id="id510" class="calibre1"/>Twitter replaced. In our view, it was the way Twitter organized the information and its users that clicked. Its unique <span class="strong"><em class="calibre10">Follow</em></span> model of relationship caters to our hunger for curiosity, while its short, free, and high-speed communication platform enables the users to speak out and be heard globally. By allowing users to follow a person or an entity of interest, it enables us to keep up with their latest happenings without the other user following us back. The <span class="strong"><em class="calibre10">Follow</em></span> model tips Twitter's relationships towards more of an interest graph rather than the friendship model usually found in social networks such as Facebook.</p><p class="calibre8">Twitter is known and used across the globe for the super-fast spread of information (and rumors). It has been innovatively used in certain circumstances unimaginable before, such as finding people in times of natural calamities such as earthquakes or typhoons. It has been used to spread information so far and deep that it takes viral proportions. The asymmetric relationships and high speed information exchange aid in making Twitter such a dynamic entity. If we closely analyze and study the data and dynamics of this social network we can uncover many insights. Hence, it is the topic for this chapter.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Interesting links</strong></span>:</p><p class="calibre8"><a class="calibre1" href="https://www.technologyreview.com/s/419368/how-twitter-helps-in-a-disaster/">https://www.technologyreview.com/s/419368/how-twitter-helps-in-a-disaster/</a></p><p class="calibre8"><a class="calibre1" href="http://www.citylab.com/tech/2015/04/how-twitter-maps-can-be-useful-during-disasters/391436/">http://www.citylab.com/tech/2015/04/how-twitter-maps-can-be-useful-during-disasters/391436/</a></p><p class="calibre8"><a class="calibre1" href="https://www.researchgate.net/publication/282150020_A_Systematic_Literature_Review_of_Twitter_Research_from_a_Socio-Political_Revolution_Perspective?channel=doi&amp;linkId=56050b3f08ae5e8e3f3125cb&amp;showFulltext=true">https://www.researchgate.net/publication/282150020_A_Systematic_Literature_Review_of_Twitter_Research_from_a_Socio-Political_Revolution_Perspective?channel=doi&amp;linkId=56050b3f08ae5e8e3f3125cb&amp;showFulltext=true</a></p><p class="calibre8"><a class="calibre1" href="http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.696123">http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.696123</a></p><p class="calibre8"><a class="calibre1" href="http://www.psmag.com/nature-and-technology/how-to-use-social-media-usefully">http://www.psmag.com/nature-and-technology/how-to-use-social-media-usefully</a></p></div><p class="calibre8">Let's apply some <a id="id511" class="calibre1"/>data science to tweets using #RMachineLearningByExample!</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Data mining @social networks" id="1P71O1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec54" class="calibre1"/>Data mining @social networks</h1></div></div></div><p class="calibre8">We have traveled <a id="id512" class="calibre1"/>quite a distance so far through the chapters of this book, understanding various concepts and learning some amazing algorithms. We have even worked on projects that have applications in our daily lives. In short, we have done data mining without using the term explicitly. Let us now take this opportunity to formally define data mining.</p><p class="calibre8">Mining, in the classical sense of the word, refers to the extraction of useful minerals from the Earth (such as coal mining). Put in the context of the information age, mining refers to the extraction of useful information from large pools of data. Thus, if we look carefully, <span class="strong"><strong class="calibre9">Knowledge </strong></span><a id="id513" class="calibre1"/>
<span class="strong"><strong class="calibre9">Mining</strong></span> or <span class="strong"><strong class="calibre9">Knowledge Discovery from </strong></span><a id="id514" class="calibre1"/>
<span class="strong"><strong class="calibre9">Data</strong></span> (<span class="strong"><strong class="calibre9">KDD</strong></span>) seems to be a better representation than the term data mining. As is the case with many keywords, short and sweet catches the attention. Thus, you may find in many places the terms Knowledge Discovery from Data and data mining being used interchangeably, which is rightly so. The process of data mining, analogous to the mining of minerals, involves the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Data cleansing to remove noise and unwanted data</li><li class="listitem" value="2">Data transformation to transform the data into relevant form for analysis</li><li class="listitem" value="3">Data/pattern evaluation to uncover interesting insights</li><li class="listitem" value="4">Data presentation to visualize knowledge in a useful form</li></ol><div class="calibre14"/></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre8">Data mining isn't about using a search engine to get information, say regarding snakes. Rather it is about uncovering hidden insights like snakes are the only creatures found on every continent except Antarctica!</p></div><p class="calibre8">If we take a minute to understand the preceding steps, we can see that we used exactly the same process across our projects. Please keep in mind that we have simply formalized and presented the process we have been following across chapters and not missed or modified any step done in previous chapters.</p></div>

<div class="book" title="Data mining @social networks" id="1P71O1-973e731d75c2419489ee73e3a0cf4be8">
<div class="book" title="Mining social network data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec67" class="calibre1"/>Mining social network data</h2></div></div></div><p class="calibre8">Now that we have <a id="id515" class="calibre1"/>formally defined data mining and seen the steps involved in transforming data to knowledge, let us focus on data from social networks. While data mining methodology is independent of the source of data, there are certain things to be kept in mind which could lead to better processing and improved results.</p><p class="calibre8">Like the mining of any other type of data, domain knowledge is definitely a plus for mining social network data. Even though social network analysis is an interdisciplinary subject (as discussed in the previous section), it primarily involves the analysis of data pertaining to users or entities and their interactions.</p><p class="calibre8">In previous chapters, we have seen all sorts of data from e-commerce platforms to banks to data related to the characteristics of flowers. The data we have seen has had different attributes and characteristics. But if we look carefully, the data was a result of some sort of measurement or event capture.</p><p class="calibre8">Coming onto the social network's domain, the playground is a little, if not completely different. Unlike what we have seen so far, data from social media platforms is extremely dynamic. When we say dynamic, we refer to the actual content on a data point and not its structure. The data point itself may (or may not) be structured, but the content itself is not.</p><p class="calibre8">Let us be specific and talk about data contained in a tweet. A sample tweet looks something like this:</p><div class="mediaobject"><img src="../images/00214.jpeg" alt="Mining social network data" class="calibre11"/><div class="caption"><p class="calibre18">Image source: <a class="calibre1" href="https://twitter.com/POTUS/status/680464195993911296">https://twitter.com/POTUS/status/680464195993911296</a></p></div></div><p class="calibre12"> </p><p class="calibre8">A tweet, as we all know, is a 140 character message. Since the message is generated by a user (usually), the actual message may be of a different length, language, and or it may contain images, links, videos, and more. Thus, a tweet is a structured data point which contains the handle of the user (<code class="email">@POTUS</code>), the name of the user (<code class="email">President Obama</code>), the message (<code class="email">From the Obama family...</code>), along with information related to when was it tweeted (<code class="email">26 Dec 2015</code>), the number of likes, and the number of retweets. A tweet may also contain hashtags, hyperlinks, images, and videos embedded within the message. As we will see in the coming sections, a tweet contains tons of metadata (data about the data) apart from the attributes discussed preceding. Similarly, data from other social networks also contains a lot more information than what usually meets the eye.</p><p class="calibre8">This much information <a id="id516" class="calibre1"/>from a single tweet coupled with millions of users tweeting frantically every second across the globe presents a huge amount of data with interesting patterns waiting to be discovered.</p><p class="calibre8">In its true sense, Twitter's data (and of social networks in general) represents the 3 Vs (Volume, Variety, and Velocity) of big data very well.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre8">143,199 tweets per second is a record achieved during the airing of the film Castle in the Sky in Japan on August 3, 2013. The average tweets per second is usually around 5700; the record multiplied it 25 times! Read more about it on the Twitter blog: <a class="calibre1" href="https://blog.twitter.com/2013/new-tweets-per-second-record-and-how">https://blog.twitter.com/2013/new-tweets-per-second-record-and-how</a></p></div><p class="calibre8">Thus, the mining of data from a social network involves understanding the structure of the data point, the underlying philosophy or use of the social network (Twitter is used for quick exchange of information, while LinkedIn is used for professional networking), the velocity and volume of the data being generated, along with the thinking cap of a data scientist.</p><p class="calibre8">Towards the end of the chapter, we will also touch upon the challenges presented by social networks to the usual mining methodology.</p></div></div>

<div class="book" title="Data mining @social networks" id="1P71O1-973e731d75c2419489ee73e3a0cf4be8">
<div class="book" title="Data and visualization"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec68" class="calibre1"/>Data and visualization</h2></div></div></div><p class="calibre8">When the amount of <a id="id517" class="calibre1"/>data is growing exponentially every passing minute, the outcome of data mining activity must empower decision-makers to quickly identify action points. The outcome should be free of noise/excess information, yet be crisp and complete enough to be useable.</p><p class="calibre8">This unique challenge of presenting information in its most convenient and useable form for easy consumption by its intended audience (which may be nontechnical) is an important aspect of the data mining process. So far in this book, we have analyzed data and made use of line graphs, bar graphs, histograms, and scatter plots to uncover and present insights. Before we make use of these and a few more visualizations/graphs in this chapter as well, let us try and understand their importance and use them wisely.</p><p class="calibre8">While working on a data mining assignment, we usually get so engrossed in the data, its complexities, algorithms, and whatnot, that we tend to overlook the part where we have to make the outcome consumable rather than a difficult to read sheet of numbers and jargon. Apart from making sure that the final report/document contains the correct and verified figures, we also need to make sure that the figures are presented in such a manner that it is easy for the end user to make use of it. To enable easy consumption of this information/knowledge, we take the help of different visualizations.</p><p class="calibre8">Since this isn't a book <a id="id518" class="calibre1"/>on visualizations, we've taken the liberty of skipping the usual line graphs, bar graphs, pie charts, histograms, and other details. Let us understand some unconventional yet widely known/used visualizations before we use them in the coming sections.</p><div class="book" title="Word clouds"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec20" class="calibre1"/>Word clouds</h3></div></div></div><p class="calibre8">Social networks generate data <a id="id519" class="calibre1"/>in different forms and formats. The data on such platforms may be created, shared, modified, quoted, or used in various different ways. To represent complex relationships, one of the most widely used visualizations for social network data are <span class="strong"><strong class="calibre9">tag </strong></span><a id="id520" class="calibre1"/>
<span class="strong"><strong class="calibre9">clouds</strong></span> or <span class="strong"><strong class="calibre9">word clouds</strong></span>. For example, objects such as text, images, videos, and blogs on these platforms are frequently tagged. Thus, a tag cloud/word cloud represents statistics of user-generated tags. These tags may represent the relative frequency of the use of words or their presence in multiple objects. The words/tags are differentiated using different font sizes and colors to represent the statistic of choice (mostly frequency).</p><div class="mediaobject"><img src="../images/00215.jpeg" alt="Word clouds" class="calibre11"/><div class="caption"><p class="calibre18">A word cloud depicting frequently used words in a subset of tweets</p></div></div><p class="calibre12"> </p></div><div class="book" title="Treemaps"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec21" class="calibre1"/>Treemaps</h3></div></div></div><p class="calibre8">To represent data of high <a id="id521" class="calibre1"/>dimensionality, it is usually not possible to visualize all dimensions at the same time. Treemaps are one such type of visualization that partition all dimensions into subsets and present them in a hierarchical manner. Specifically, treemaps partition dimensions into a set of nested rectangles. One of the mostly widely cited examples of a treemap is the newsmap, which visualizes news aggregated by Google news and displays it in different categories shown by different colors; color gradients denote the appearance of the article (on a time scale), while the size of the rectangle denotes the popularity of the news item.</p><div class="mediaobject"><img src="../images/00216.jpeg" alt="Treemaps" class="calibre11"/><div class="caption"><p class="calibre18">Treemap showing news aggregated by Google News</p><p class="calibre18">Image source: <a class="calibre1" href="http://newsmap.jp/">http://newsmap.jp/</a></p></div></div><p class="calibre12"> </p></div><div class="book" title="Pixel-oriented maps"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec22" class="calibre1"/>Pixel-oriented maps</h3></div></div></div><p class="calibre8">Visualizations not only <a id="id522" class="calibre1"/>make outcomes easier to understand, they are very utilitarian as well. Most of the time, the outcome of an analysis process is multidimensional. To represent this data graphically on a two dimensional screen/piece of paper is a challenge. This is where pixel-oriented visualizations come into the picture. For an <span class="strong"><em class="calibre10">n-dimensional</em></span> data set, pixel-oriented visualizations map each <span class="strong"><em class="calibre10">n-dimensional</em></span> data point to a single pixel in <span class="strong"><em class="calibre10">n</em></span> different sub-windows. Thus, each data point is split across <span class="strong"><em class="calibre10">n</em></span> windows, one for each dimension. These help us map a large amount of data in single visualization. Pixel-oriented visualization look like this:</p><div class="mediaobject"><img src="../images/00217.jpeg" alt="Pixel-oriented maps" class="calibre11"/><div class="caption"><p class="calibre18">Sample pixel-oriented maps</p><p class="calibre18">Image source: <a class="calibre1" href="http://bib.dbvis.de/uploadedFiles/163.pdf">http://bib.dbvis.de/uploadedFiles/163.pdf</a></p></div></div><p class="calibre12"> </p></div><div class="book" title="Other visualizations"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec23" class="calibre1"/>Other visualizations</h3></div></div></div><p class="calibre8">Apart from the already mentioned visualizations, there are many other interesting visualizations, which come in handy for different use cases. For example, visualizations such as box plots come in handy <a id="id523" class="calibre1"/>for understanding data distribution and outlier detection. Similarly, there are visualizations such as Chernoff faces, scatter plots, network graphs, and so on which have their own merits and use cases.</p><p class="calibre8">Please do note that visualization is in itself a field of study and this section is merely trying to touch the tip of the iceberg. We urge readers to go through books/online content as shared in the <span class="strong"><em class="calibre10">References</em></span> section of the chapter to read more on this.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Getting started with Twitter APIs"><div class="book" id="1Q5IA2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec55" class="calibre1"/>Getting started with Twitter APIs</h1></div></div></div><p class="calibre8">Twitter is as much <a id="id524" class="calibre1"/>a delight for tweeple (people using Twitter to tweet) as it is for data scientists. The APIs and the documentation are well updated and easy to use. Let us get started with the APIs.</p></div>

<div class="book" title="Getting started with Twitter APIs">
<div class="book" title="Overview"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec69" class="calibre1"/>Overview</h2></div></div></div><p class="calibre8">Twitter has one of easiest <a id="id525" class="calibre1"/>yet most powerful set of APIs available of any social network out there. These APIs have been used by Twitter itself and data scientists to understand the dynamics of the Twitter world. Twitter APIs make use of four different objects, namely:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Tweets</strong></span>: A tweet is the central entity that defines Twitter itself. As discussed in the previous section, a tweet contains far more information (metadata) than just the content/message of the tweet.</li><li class="listitem"><span class="strong"><strong class="calibre9">Users</strong></span>: Anybody or anything that can tweet, follow, or perform any of Twitter's actions is a user. Twitter is unique in its definition of user, which need not necessarily be a human. <code class="email">@MarsCuriosity</code> is one such nonhuman popular Twitter handle with over 2 million followers!</li><li class="listitem"><span class="strong"><strong class="calibre9">Entities</strong></span>: These are structured pieces of information extracted from the tweet object itself. These may include information on URLs, hashtags, user mentions, and so on. These objects enable quicker processing without parsing the tweet text.</li><li class="listitem"><span class="strong"><strong class="calibre9">Places</strong></span>: A tweet may also have location attached to it. This information may be used for various purposes, such as displaying <span class="strong"><em class="calibre10">Trending Topics Near You</em></span> or targeted marketing.</li></ul></div><p class="calibre8">The preceding objects from the Twitter APIs have been explained at length on the website <a class="calibre1" href="https://dev.twitter.com/">https://dev.twitter.com/</a>. We urge readers to go through it to understand the objects and APIs even better.</p><p class="calibre8">Twitter has libraries available <a id="id526" class="calibre1"/>in all major programming languages/platforms. We will be making use of TwitteR, that is, Twitter's library for R.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip05" class="calibre1"/>Tip</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Twitter Best Practices</strong></span></p><p class="calibre8">Twitter has a set of <span class="strong"><em class="calibre10">best practices</em></span> and a list of dos and don'ts specified clearly on its developer site, <a class="calibre1" href="https://dev.twitter.com/">https://dev.twitter.com/</a>, which talks about security/authentication, privacy, and more.  Since Twitter supports a huge customer <a id="id527" class="calibre1"/>base with high availability, it tracks the usage of its APIs as well to keep its systems healthy. There is a defined rate limit on the number of times their APIs are queried. Kindly go through the best practices and be a <code class="email">#gooddeveloper</code>!</p></div></div></div>

<div class="book" title="Getting started with Twitter APIs">
<div class="book" title="Registering the application"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec70" class="calibre1"/>Registering the application</h2></div></div></div><p class="calibre8">Now that we have <a id="id528" class="calibre1"/>enough background about Twitter and its API objects, let us get our hands dirty. The first step when starting to use the APIs is to <a id="id529" class="calibre1"/>inform Twitter about your application. Twitter uses the standard <span class="strong"><strong class="calibre9">Open Authentication</strong></span> (<span class="strong"><strong class="calibre9">OAuth</strong></span>) protocol for authorizing a third party app. OAuth uses an application's consumer key, consumer secret, access token, and access token secret to allow it to use APIs and data of the connected service.</p><p class="calibre8">The following quick steps will set us up for the game:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Go to Twitter's Application Management Console at <a class="calibre1" href="https://apps.twitter.com/">https://apps.twitter.com/</a> and log in with your credentials or create an account if you don't have one.</li><li class="listitem" value="2">Click on <span class="strong"><strong class="calibre9">Create New App</strong></span> and fill in the details for the app's name, website, and so on. For our purposes, we will name our app <code class="email">TwitterAnalysis_rmre</code>. For callback URL use <code class="email">http://127.0.0.1:1410</code> to point back to your local system. You may choose any other port number as well.</li><li class="listitem" value="3">Click on <span class="strong"><strong class="calibre9">Create your Twitter Application</strong></span> to complete the process. <a id="id530" class="calibre1"/>Your Application Management Console would look like the following screenshot:<div class="mediaobject"><img src="../images/00218.jpeg" alt="Registering the application" class="calibre11"/><div class="caption"><p class="calibre18">The Twitter application page</p></div></div><p class="calibre21"> </p></li></ol><div class="calibre14"/></div><p class="calibre8">Congratulations, your app is created and registered with Twitter. But before we can use it, there's one more piece to it. We need to create access tokens, and to do that we perform the following steps.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Go to the link <span class="strong"><strong class="calibre9">Keys and Access Tokens</strong></span> on the Twitter app's details page.</li><li class="listitem" value="2">Scroll down and click on <span class="strong"><strong class="calibre9">Create My Access Token</strong></span> to generate an access token for your profile.</li><li class="listitem" value="3">The <span class="strong"><strong class="calibre9">Keys and Access Tokens</strong></span> page looks like the following screenshot after completing the preceding steps:<div class="mediaobject"><img src="../images/00219.jpeg" alt="Registering the application" class="calibre11"/><div class="caption"><p class="calibre18">Application keys and access tokens</p></div></div><p class="calibre21"> </p></li></ol><div class="calibre14"/></div><p class="calibre8">We will be using the <a id="id531" class="calibre1"/>same application for this as well as in the coming chapter. Make a note of the consumer key, consumer secret, access token and access secret; we will need these in our application.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre8">The keys and secrets generated for OAuth are sensitive pieces of information. They enable access for your app to Twitter's data. Please keep them as safe as you would keep your passwords (even safer than that). <code class="email">#SafetyFirst</code>.</p></div></div></div>

<div class="book" title="Getting started with Twitter APIs">
<div class="book" title="Connect/authenticate"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec71" class="calibre1"/>Connect/authenticate</h2></div></div></div><p class="calibre8">Now that we have <a id="id532" class="calibre1"/>everything ready at Twitter's end, let us set things up at R's end as well. Before we start playing with the data from Twitter, the first step would be to connect and authenticate ourselves through the app we just created using R.</p><p class="calibre8">We will make use of R's TwitteR library by Jeff Gentry. This library or client allows us to use Twitter's web APIs through R. We will use the method <code class="email">setup_twitter_oauth()</code> to connect to Twitter using our app's credentials (keys and access tokens). Kindly replace <code class="email">XXXX</code> in the following code with your access keys/tokens generated in the previous step:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; # load library</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(twitteR)</strong></span>
<span class="strong"><strong class="calibre9">&gt; # set credentials</strong></span>
<span class="strong"><strong class="calibre9">&gt; consumerSecret = "XXXXXXXXXXXXX"</strong></span>
<span class="strong"><strong class="calibre9">&gt; consumerKey = "XXXXXXXXXXXXXXXXXXXXXXXXXx"</strong></span>
</pre></div><p class="calibre8">Upon executing the <a id="id533" class="calibre1"/>preceding snippet of code, it will prompt you to use a local file to cache credentials or not. For now, we will say <code class="email">No</code> to it:</p><p class="calibre8"><span class="strong"><img src="../images/00220.jpeg" alt="Connect/authenticate" class="calibre16"/></span></p><p class="calibre8">This will open up your browser and ask you to log in using your Twitter credentials and authorize this app, as shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00221.jpeg" alt="Connect/authenticate" class="calibre11"/><div class="caption"><p class="calibre18">Authorize app to fetch data</p></div></div><p class="calibre12"> </p><p class="calibre8">Once authorized, the browser will be redirected to the callback URL we mentioned when we created the app on Twitter. You may use a more informative URL for the user as well.</p><div class="mediaobject"><img src="../images/00222.jpeg" alt="Connect/authenticate" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Congratulations, you <a id="id534" class="calibre1"/>are now connected to the ocean of tweets.</p></div></div>

<div class="book" title="Getting started with Twitter APIs">
<div class="book" title="Extracting sample tweets"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec72" class="calibre1"/>Extracting sample tweets</h2></div></div></div><p class="calibre8">Now that we are <a id="id535" class="calibre1"/>connected to Twitter using R, it's time to extract some latest tweets and analyze what we get. To extract tweets, we will use the handle for Twitter's account 001 (Twitter's founder and first user), Jack Dorsey, <code class="email">@jack</code>. The following snippet of code extracts the latest 300 tweets from him:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; twitterUser &lt;- getUser("jack")</strong></span>
<span class="strong"><strong class="calibre9">&gt; # extract jack's tweets</strong></span>
<span class="strong"><strong class="calibre9">&gt; tweets &lt;- userTimeline(twitterUser, n = 300)</strong></span>
<span class="strong"><strong class="calibre9">&gt; tweets</strong></span>
</pre></div><p class="calibre8">The output contains text combined with unprintable characters and URLs due to Twitter's content-rich data. We will look at the metadata of a tweet in a bit, but before that, the extracted information looks like this:</p><div class="mediaobject"><img src="../images/00223.jpeg" alt="Extracting sample tweets" class="calibre11"/><div class="caption"><p class="calibre18">Sample tweets</p></div></div><p class="calibre12"> </p><p class="calibre8">To see the attributes and functions available to analyze and manipulate each tweet, use the <code class="email">getClass</code> method as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; # get tweet attributes</strong></span>
<span class="strong"><strong class="calibre9">&gt; tweets[[1]]$getClass()</strong></span>
<span class="strong"><strong class="calibre9">&gt;</strong></span>
<span class="strong"><strong class="calibre9">&gt; # get retweets count</strong></span>
<span class="strong"><strong class="calibre9">&gt; tweets[[1]]$retweetCount</strong></span>
<span class="strong"><strong class="calibre9">&gt;</strong></span>
<span class="strong"><strong class="calibre9">&gt; # get favourite count</strong></span>
<span class="strong"><strong class="calibre9">&gt; tweets[[1]]$favoriteCount</strong></span>
</pre></div><p class="calibre8">The following output will be generated:</p><div class="mediaobject"><img src="../images/00224.jpeg" alt="Extracting sample tweets" class="calibre11"/></div><p class="calibre12"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Twitter data mining"><div class="book" id="1R42S2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec56" class="calibre1"/>Twitter data mining</h1></div></div></div><p class="calibre8">Now that we have <a id="id536" class="calibre1"/>tested our tools, libraries, and connections to Twitter APIs, the time has come to begin our search for the hidden treasures in Twitter land. Let's wear our data miner's cap and start digging!</p><p class="calibre8">In this section, we will be working on Twitter data gathered from searching keywords (or hashtags in Twitter vocabulary) and user timelines. Using this data, we will be uncovering some interesting insights while using different functions and utilities from TwitteR and other R packages.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre8">Please note that our process will implicitly follow the steps outlined for data mining. In the spirit of brevity, we might take the liberty to not mention each of the steps explicitly. We are mining for some <span class="strong"><em class="calibre10">gold-plated</em></span> insights; rest assured nothing is skipped!</p></div><p class="calibre8">Every year, we begin with a new zeal to achieve great feats and improve upon our shortcomings. Most of us make promises to ourselves in the form of New Year's resolutions. Let us explore what tweeple are doing with their resolutions in 2016!</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Note</strong></span>: Twitter data changes very rapidly and your results/plots may vary from the ones depicted in this chapter.</p></div><p class="calibre8">We will use the same app and its credentials to connect and tap into Twitter for data. The following code works in exactly the same way that we extracted sample tweets in the previous section:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">library(twitteR)</strong></span>
<span class="strong"><strong class="calibre9">library(ggplot2)</strong></span>
<span class="strong"><strong class="calibre9">library(stringr)</strong></span>
<span class="strong"><strong class="calibre9">library(tm)</strong></span>
<span class="strong"><strong class="calibre9">library(wordcloud)</strong></span>

<span class="strong"><strong class="calibre9">consumerSecret = "XXXXXXXXX"</strong></span>
<span class="strong"><strong class="calibre9">consumerKey = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"</strong></span>

<span class="strong"><strong class="calibre9">setup_twitter_oauth(consumer_key = consumerKey,consumer_secret = consumerSecret)</strong></span>
</pre></div><p class="calibre8">Apart from connecting to Twitter, we have also loaded required packages, such as <code class="email">ggplot</code>, <code class="email">stringr</code>, <code class="email">tm</code>, and <code class="email">wordcloud</code>. We will see where and how these packages are useful as we proceed.</p><p class="calibre8">Once connected to our data source, we can proceed towards collecting the required data. Since we are planning to learn about tweeple and their New Year's resolutions, we will extract data for the <a id="id537" class="calibre1"/>hashtag <code class="email">#ResolutionsFor2016</code>. We can also use any hashtag, such as <code class="email">#NewYearResolutions</code>, <code class="email">#2016Resolutions</code>, or a combination of hashtags to get relevant tweets. The following piece of code not only extracts tweets, but also converts the list of tweet/status objects into an R data frame. We also convert each of the tweets to UTF-8 to handle text from different languages.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Amazing fact</strong></span>: Twitter is available in 48 different languages and counting!</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># trending tweets</strong></span>
<span class="strong"><strong class="calibre9">trendingTweets = searchTwitter("#ResolutionsFor2016",n=1000)</strong></span>
<span class="strong"><strong class="calibre9">trendingTweets.df = twListToDF(trendingTweets)</strong></span>
<span class="strong"><strong class="calibre9">trendingTweets.df$text &lt;- sapply(trendingTweets.df$text,function(x) iconv(x,to='UTF-8'))</strong></span>
</pre></div><p class="calibre8">As we saw in the previous section, a tweet contains far more information than mere text. One of the various attributes is the status source. The status source denotes the device from where the tweet was made. It may be a mobile phone, tablet, and so on. Before we apply major transformations and clean up tweet objects, we apply a quick transformation to transform status source to meaningful form:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">trendingTweets.df$tweetSource = sapply(trendingTweets.df$statusSource,function(sourceSystem) enodeSource(sourceSystem))</strong></span>
</pre></div><p class="calibre8">The preceding code transforms <code class="email">statusSource</code> from values such as <code class="email">&lt;a href=\"http://twitter.com/download/android\" rel=\"nofollow\"&gt;Twitter for Android&lt;/a&gt;</code> to simply Android and assigns it to a new attribute named <code class="email">tweetSource</code>.</p><p class="calibre8">Once we have the data, the next set of steps in the data mining process is to clean up the data. We use the text mining package <code class="email">tm</code> to perform transformation and cleanup. The <code class="email">Corpus</code> function in particular helps us handle tweet/status objects as a collection of documents. We then use the <code class="email">tm_map</code> utility from the same package to apply/map transformations such as converting all text to lower case, removing punctuation, numbers, and stop words. Stop words is a list of the most commonly used words, such as a, an, the, and so on, which can safely <a id="id538" class="calibre1"/>be removed while analyzing text without loss of meaning.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># transformations</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- Corpus(VectorSource(trendingTweets.df$text))</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, tolower)</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, removePunctuation)</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, removeNumbers)</strong></span>

<span class="strong"><strong class="calibre9"># remove URLs</strong></span>
<span class="strong"><strong class="calibre9">removeURL &lt;- function(x) gsub("http[[:alnum:]]*", "", x)</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, removeURL) </strong></span>

<span class="strong"><strong class="calibre9"># remove stop words</strong></span>
<span class="strong"><strong class="calibre9">twtrStopWords &lt;- c(stopwords("english"),'resolution','resolutions','resolutionsfor','resolutionsfor2016','2016','new','year','years','newyearresolution')</strong></span>
<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, removeWords, twtrStopWords)</strong></span>

<span class="strong"><strong class="calibre9">tweetCorpus &lt;- tm_map(tweetCorpus, PlainTextDocument)</strong></span>
</pre></div><p class="calibre8">The final transformation before we proceed to the next step of analyzing our data for hidden patterns/insights is a term-document matrix. As the name itself says, a term-document matrix is a matrix representation in which terms act as rows while columns are represented by documents. Each entry in this matrix represents the number of occurrences of a term in a given document. More formally, a term-document matrix is a matrix representation that describes the frequency of terms in a collection of documents. This representation is extremely useful in natural language processing applications. It is an optimized data structure that enables quick searches, topical modeling, and more. The data structure can be explained using the following simple example where we have two text documents, <span class="strong"><strong class="calibre9">TD1</strong></span> and <span class="strong"><strong class="calibre9">TD2</strong></span>:</p><div class="mediaobject"><img src="../images/00225.jpeg" alt="Twitter data mining" class="calibre11"/><div class="caption"><p class="calibre18">Sample term-document matrix</p></div></div><p class="calibre12"> </p><p class="calibre8">The tm package provides <a id="id539" class="calibre1"/>us another easy-to-use utility called term-document matrix (<code class="email">TermDocumentMatrix</code> is also available), which we use to convert our <code class="email">Corpus</code> object into the required form:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># Term Document Matrix</strong></span>
<span class="strong"><strong class="calibre9">&gt; twtrTermDocMatrix &lt;- TermDocumentMatrix(tweetCorpus, control = list(minWordLength = 1))</strong></span>
</pre></div></div>

<div class="book" title="Twitter data mining">
<div class="book" title="Frequent words and associations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec73" class="calibre1"/>Frequent words and associations</h2></div></div></div><p class="calibre8">The <a id="id540" class="calibre1"/>term-document matrix thus prepared contains words from each of the tweets (post the cleanup and transformations) as rows, while columns represent the tweet themselves.</p><p class="calibre8">As a quick check, let us see which of the words are most frequently used in our dataset. Let the threshold be set to <code class="email">30</code> occurrences or more. We use the apply utility to iterate each term in our term-document matrix and sum its occurrences. The function helps us filter out the terms that have appeared 30 times or more.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># Terms occuring in more than 30 times</strong></span>
<span class="strong"><strong class="calibre9">&gt; which(apply(twtrTermDocMatrix,1,sum)&gt;=30)</strong></span>
</pre></div><p class="calibre8">The result will be as shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00226.jpeg" alt="Frequent words and associations" class="calibre11"/><div class="caption"><p class="calibre18">Terms with 30 or more occurrences across tweets</p></div></div><p class="calibre12"> </p><p class="calibre8">As the preceding screenshot shows, words such as healthy, inspire, and positivity feature in the list of words with 30 or more occurrences. We all have a lot in common when it comes to yearly goals, no?</p><p class="calibre8">The preceding manipulation was a quick check to see if we really have tweets that help us find out something interesting about New Year's resolutions. Let us now take a formal approach and identify frequent terms in our data set. We will also try and present the information <a id="id541" class="calibre1"/>in a creative yet easy-to-understand representation. To get the most frequent terms in our data set, we use the function <code class="email">findFreqTerms</code> from the <code class="email">tm</code> package again. This function provides us an abstraction over quick hacks, such as the one we previously used. <code class="email">findFreqTerms</code> also lets us set a minimum and maximum threshold for term frequencies. For our case, we will only mention the lower bound and see the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># print the frequent terms from termdocmatrix</strong></span>
<span class="strong"><strong class="calibre9">&gt; (frequentTerms&lt;-findFreqTerms(twtrTermDocMatrix,lowfreq = 10))</strong></span>
</pre></div><p class="calibre8">The results look something like the following screenshot:</p><p class="calibre8"><span class="strong"><img src="../images/00227.jpeg" alt="Frequent words and associations" class="calibre16"/></span></p><p class="calibre8">We get about 107 terms with a minimum occurrence of 10. If you look carefully, the terms we saw with frequencies of at least 30 also appear in this list, and rightly so.</p><p class="calibre8">Now that we are certain that there are terms/words with occurrences of more than 10, let us create a data frame and plot the terms versus their frequencies as we decided previously. We use the <code class="email">rowSums</code> function to calculate the total occurrence of each term/word. We <a id="id542" class="calibre1"/>then pick a subset of terms which have more than 10 occurrences and plot them using <code class="email">ggplot</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># calculate frequency of each term</strong></span>
<span class="strong"><strong class="calibre9">term.freq &lt;- rowSums(as.matrix(twtrTermDocMatrix))</strong></span>

<span class="strong"><strong class="calibre9"># picking only a subset</strong></span>
<span class="strong"><strong class="calibre9">subsetterm.freq &lt;- subset(term.freq, term.freq &gt;= 10)</strong></span>


<span class="strong"><strong class="calibre9"># create data frame from subset of terms</strong></span>
<span class="strong"><strong class="calibre9">frequentTermsSubsetDF &lt;- data.frame(term = names(subsetterm.freq), freq = subsetterm.freq)</strong></span>

<span class="strong"><strong class="calibre9"># create data frame with all terms</strong></span>
<span class="strong"><strong class="calibre9">frequentTermsDF &lt;- data.frame(term = names(term.freq), freq = term.freq)</strong></span>

<span class="strong"><strong class="calibre9"># sort by subset DataFrame frequency</strong></span>
<span class="strong"><strong class="calibre9">frequentTermsSubsetDF &lt;- frequentTermsSubsetDF[with(frequentTermsSubsetDF, order(-frequentTermsSubsetDF$freq)), ]</strong></span>

<span class="strong"><strong class="calibre9"># sort by complete DataFrame frequency</strong></span>
<span class="strong"><strong class="calibre9">frequentTermsDF &lt;- frequentTermsDF[with(frequentTermsDF, order(-frequentTermsDF$freq)), ]</strong></span>

<span class="strong"><strong class="calibre9"># words by frequency from subset data frame</strong></span>
<span class="strong"><strong class="calibre9">ggplot(frequentTermsSubsetDF, aes(x = reorder(term,freq), y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Frequency") + coord_flip()</strong></span>
</pre></div><p class="calibre8">The preceding piece of code generates the following frequency graph:</p><div class="mediaobject"><img src="../images/00228.jpeg" alt="Frequent words and associations" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Upon <a id="id543" class="calibre1"/>analyzing the preceding graph, we can quickly get some interesting points:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The words <span class="strong"><strong class="calibre9">mom</strong></span>, <span class="strong"><strong class="calibre9">elected</strong></span>, <span class="strong"><strong class="calibre9">president</strong></span>, and <span class="strong"><strong class="calibre9">trillionaire</strong></span> feature in the top 10. Strange set, yet interesting. More on this in a bit.</li><li class="listitem">Health features high in the list, but doesn't make it to the top 10. So, it seems like health is on the cards but not very high. This is the same for <span class="strong"><strong class="calibre9">fitness</strong></span> and <span class="strong"><strong class="calibre9">diet</strong></span>.</li><li class="listitem">Most of the words in this list are positive in essence. Words such as <span class="strong"><strong class="calibre9">happy</strong></span>, <span class="strong"><strong class="calibre9">hope</strong></span>, <span class="strong"><strong class="calibre9">positivity</strong></span>, <span class="strong"><strong class="calibre9">change</strong></span>, and so on all point to the upbeat mood while taking up New Year's resolutions!</li></ul></div><p class="calibre8">Though the preceding graph gives us quite a lot of information regarding the words and their frequencies in a nice layout, it still doesn't show us the full picture. Remember that we crafted a subset of items from our data set before generating this graph? We did that on purpose, otherwise the graph would have become too long and words with lesser frequencies would clutter the whole thing. Another point which this graph misses out is the relative difference in the frequencies.</p><p class="calibre8">If our aim is to see the relative difference between the frequencies, we need a different visualization altogether. Here comes word cloud to the rescue. Using the <code class="email">wordcloud</code> library, we can easily generate word clouds from a dataframe using a one liner:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># wordcloud</strong></span>
<span class="strong"><strong class="calibre9">&gt; wordcloud(words=frequentTermsDF$term, freq=frequentTermsDF$freq,random.order=FALSE)</strong></span>
</pre></div><p class="calibre8">The word<a id="id544" class="calibre1"/>cloud using the complete data frame looks something like this:</p><div class="mediaobject"><img src="../images/00229.jpeg" alt="Frequent words and associations" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The preceding word cloud renders words in decreasing order of frequency. The size of each word emphasizes its frequency. You can play around with the <code class="email">wordcloud</code> function to generate some interesting visualizations or even art!</p><p class="calibre8">A lot of words appear in the preceding graphs, but isn't it rather interesting to see the word trillionaire pop up in the top 10? What could be the reason for it? Was it a spam post by a bot, or a tweet by some celebrity that went viral, or something completely different altogether? Let's check out the top tweet in this list and see if it contains the word trillionaire:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># top retweets</strong></span>
<span class="strong"><strong class="calibre9">&gt; head(subset(trendingTweets.df$text, grepl("trillionaire",trendingTweets.df$text) ),n=1)</strong></span>
</pre></div><p class="calibre8">The following screenshot is what you get:</p><p class="calibre8"><span class="strong"><img src="../images/00230.jpeg" alt="Frequent words and associations" class="calibre16"/></span></p><p class="calibre8">It turns out that our hunch was right. It was a New Year resolution tweet by a celebrity that went viral. A quick search on Twitter reveals the tweet:</p><div class="mediaobject"><img src="../images/00231.jpeg" alt="Frequent words and associations" class="calibre11"/><div class="caption"><p class="calibre18">Image source: <a class="calibre1" href="https://twitter.com/mishacollins?lang=en">https://twitter.com/mishacollins?lang=en</a></p></div></div><p class="calibre12"> </p><p class="calibre8">A bit further searching reveals Misha Collins is a famous actor from the television series Supernatural. We can also see that the above resolution was retweeted a staggering 5k times! It's interesting to note that the number of likes is 14k, outnumbering the retweets. Can we infer <a id="id545" class="calibre1"/>that tweeple prefer likes/hearts to retweets? It can also be seen that words such as mom, learn, trillionaire, elected, and President all occur as most frequent words without a doubt. Indirectly, we can also infer that Supernatural has a huge fan following on Twitter and that Castiel (Misha's role in the TV series) is a popular character from the show. A bit of a surprise is his resolution to learn to crochet, no?</p><p class="calibre8">Moving on from supernatural stuff, let us go back to the fitness debate. Fitness is important to most of us. Activities such as exercising or hitting the gym see a surge during the initial months/weeks of the year. Let's see how health-conscious our friends on Twitter are!</p><p class="calibre8">Since a lot of words such as health, diet, fitness, gym, and so on point towards a healthy lifestyle, let us try and find words associated with the word <span class="strong"><em class="calibre10">fitness</em></span> itself. <code class="email">findAssocs</code> is a handy function which helps us find words from a term-document matrix that have at least a specified level of correlation to a given word. We will use the output from this function to prepare a term-association (correlation) graph using <code class="email">ggplot</code>. The process is similar to how we prepared the preceding frequency graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># Associatons</strong></span>
<span class="strong"><strong class="calibre9">(fitness.associations &lt;- findAssocs(twtrTermDocMatrix,"fitness",0.25))</strong></span>

<span class="strong"><strong class="calibre9">fitnessTerm.freq &lt;- rowSums(as.matrix(fitness.associations$fitness))</strong></span>

<span class="strong"><strong class="calibre9">fitnessDF &lt;- data.frame(term=names(fitnessTerm.freq),freq=fitnessTerm.freq)</strong></span>

<span class="strong"><strong class="calibre9">fitnessDF &lt;- fitnessDF[with(fitnessDF, order(-fitnessDF$freq)), ]</strong></span>
<span class="strong"><strong class="calibre9">ggplot(fitnessDF,aes(x=reorder(term,freq),y=freq))</strong></span>
<span class="strong"><strong class="calibre9">+geom_bar(stat = "identity") +xlab("Terms")</strong></span>
<span class="strong"><strong class="calibre9">+ ylab("Associations")</strong></span>
<span class="strong"><strong class="calibre9">+ coord_flip()</strong></span>
</pre></div><p class="calibre8">The words most <a id="id546" class="calibre1"/>closely correlated to the word fitness are as follows:</p><p class="calibre8"><span class="strong"><img src="../images/00232.jpeg" alt="Frequent words and associations" class="calibre16"/></span></p><p class="calibre8">The same data is more readable in graphical form, as follows:</p><div class="mediaobject"><img src="../images/00233.jpeg" alt="Frequent words and associations" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">As evident from the preceding graph, terms such as <span class="strong"><strong class="calibre9">lossweight</strong></span>, <span class="strong"><strong class="calibre9">workout</strong></span>, <span class="strong"><strong class="calibre9">getfit</strong></span>, and so on. prove our point that tweeple are as concerned about health as we are. It is interesting to note the occurrence of the term <span class="strong"><em class="calibre10">yogavideos</em></span> in this list. It looks like yoga is catching up with other techniques of staying fit in 2016. There's <span class="strong"><strong class="calibre9">meditation</strong></span> on the list too.</p></div></div>

<div class="book" title="Twitter data mining">
<div class="book" title="Popular devices"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec74" class="calibre1"/>Popular devices</h2></div></div></div><p class="calibre8">So far, we have <a id="id547" class="calibre1"/>dealt with the visible components of a tweet, such as the text, retweet counts, and so on, and we were able to extract many interesting insights. Let us take out our precision tools and dig deeper into our data.</p><p class="calibre8">As mentioned a couple times in the above sections, a tweet has far more information than what meets the eye. One such piece of information is about the source of the tweet. Twitter was born of the SMS era, and many of its characteristics, such as the 140 character word limit, are reminiscent of that era. It would be interesting to see how tweeple use Twitter, that is, what devices are used to access and post on Twitter frequently. Though the world has moved a long way from the SMS era, mobile phones are ubiquitous. To get this information, we will make use of the attribute <code class="email">tweetSource</code> from our dataframe <code class="email">trendingTweets.df</code>. We created this additional attribute from the <code class="email">statusSource</code> attribute already existing in the <code class="email">tweet</code> object (see the beginning of this section for a quick recap).</p><p class="calibre8">We shall use a subset of the data frame <code class="email">trendingTweets.df</code> based upon retweet counts for the sake of clarity. We will use <code class="email">ggplot</code> again to visualize our results.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># Source by retweet count</strong></span>
<span class="strong"><strong class="calibre9">trendingTweetsSubset.df &lt;- subset(trendingTweets.df, trendingTweets.df$retweetCount &gt;= 5000 )</strong></span>

<span class="strong"><strong class="calibre9">ggplot(trendingTweetsSubset.df, aes(x =tweetSource, y =retweetCount/100)) + geom_bar(stat = "identity") +xlab("Source") + ylab("Retweet Count")</strong></span>
</pre></div><p class="calibre8">The following plot is your result:</p><div class="mediaobject"><img src="../images/00234.jpeg" alt="Popular devices" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Without a doubt, the iPhone is the most preferred device, followed by Android and the Web. It is interesting to see that people use the Web/website to retweet more than the iPad! Windows Phone clearly has some serious issues to tackle here. Can we also infer that the iPhone is the preferred device amongst tweeples? Or does the iPhone provide a better experience than any other device for Twitter? Or we could even go deeper and say that Twitter on iPhone has an easier-to-access "retweets" button than any other device. Inferences such as these and many more, require a bit more digging than this, but all of this has a lot of <a id="id548" class="calibre1"/>knowledge/potential that could be used by managements, UX teams, and so on to improve and change things around.</p></div></div>

<div class="book" title="Twitter data mining">
<div class="book" title="Hierarchical clustering"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec75" class="calibre1"/>Hierarchical clustering</h2></div></div></div><p class="calibre8">We have <a id="id549" class="calibre1"/>seen clustering and classification in previous chapters (see <a class="calibre1" title="Chapter 2. Let's Help Machines Learn" href="part0022_split_000.html#KVCC1-973e731d75c2419489ee73e3a0cf4be8">Chapter 2</a>, <span class="strong"><em class="calibre10">Let's Help Machines Learn</em></span>) and uncovered some interesting facts about the data at hand. For our current use case, even though our tweets are all related to 2016 resolutions, we can never be sure of the kinds of resolutions tweeple make. This makes it a very apt use case for hierarchical clustering. Unlike k-means or other clustering algorithms that require a preset number of clusters before computation, hierarchical clustering algorithms work independently of it.</p><p class="calibre8">Let us take this opportunity to understand hierarchical clustering before we apply it to our data. Hierarchical clustering, like any other clustering algorithm, helps us group similar items together. The exact details for this algorithm in general can be explained as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Initialize</strong></span>: This is the first step, where each element is assigned to a cluster of its own. For a dataset containing <span class="strong"><em class="calibre10">n</em></span> elements, the algorithm creates <span class="strong"><em class="calibre10">n</em></span> different clusters with one element in each of them. A distance/similarity measure is decided at this step.</li><li class="listitem"><span class="strong"><strong class="calibre9">Merge</strong></span>: During this step, depending upon the distance/similarity measure chosen, the closest pair of clusters are identified and merged into a single cluster. This step results in one fewer clusters than the total clusters so far.</li><li class="listitem"><span class="strong"><strong class="calibre9">Compute</strong></span>/<span class="strong"><strong class="calibre9">recompute</strong></span>: We compute/recompute distances/similarities between the new cluster formed in the Merge step and the existing clusters.</li></ul></div><p class="calibre8">The <span class="strong"><strong class="calibre9">merge</strong></span> and <span class="strong"><strong class="calibre9">compute</strong></span> steps are repeated until we are left with a single cluster containing all <span class="strong"><em class="calibre10">n</em></span> items. As the name suggests, this algorithm generates a hierarchical structure with the leaves denoting individual elements as clusters combined based upon similarity/distance as we go toward the root of the tree. The output tree is generally referred to as a <a id="id550" class="calibre1"/>
<span class="strong"><strong class="calibre9">dendrogram</strong></span>.</p><p class="calibre8">The merge step is where variations of this algorithm exist. There are several ways in which the closest clusters could be identified. From simple methods, such as single-link, which consider the shortest distance between any two elements of the two clusters in consideration as the distance measure, to complex ones such as Ward's method which uses variance to find the most compact clusters, there are several methods that could be employed depending upon the use case.</p><p class="calibre8">Coming back to the Twitter world, let us use hierarchical clustering to see which terms/tweets are the closest. For our current use case, we will use the single method for our merge criteria. You may try out different algorithms and observe the differences.</p><p class="calibre8">To perform <a id="id551" class="calibre1"/>hierarchical clustering, we first treat our dataset to remove sparse terms for the sake of clarity. For this, the <code class="email">removeSparseTerms</code> function helps us remove rows of data that have sparsity below a specified limit. We then use the <code class="email">hclust</code> utility to form clusters. The output of this utility is directly plottable. Let us write some code for this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># remove sparse terms</strong></span>
<span class="strong"><strong class="calibre9">twtrTermDocMatrix2 &lt;- removeSparseTerms(twtrTermDocMatrix, sparse = 0.98)</strong></span>

<span class="strong"><strong class="calibre9">tweet_matrix &lt;- as.matrix(twtrTermDocMatrix2)</strong></span>

<span class="strong"><strong class="calibre9"># cluster terms</strong></span>
<span class="strong"><strong class="calibre9">distMatrix &lt;- dist(scale(tweet_matrix))</strong></span>

<span class="strong"><strong class="calibre9">fit &lt;- hclust(distMatrix,method="single")</strong></span>
<span class="strong"><strong class="calibre9">plot(fit)</strong></span>
</pre></div><p class="calibre8">The output <span class="strong"><em class="calibre10">dendrogram</em></span> is amazingly simple to understand:</p><div class="mediaobject"><img src="../images/00235.jpeg" alt="Hierarchical clustering" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">If you observe the second cluster from right, it contains terms <span class="strong"><strong class="calibre9">trillionaire</strong></span>, <span class="strong"><strong class="calibre9">elected</strong></span>, <span class="strong"><strong class="calibre9">mom</strong></span>, <span class="strong"><strong class="calibre9">call</strong></span>, and so on. Mapping back to the top retweeted tweet from Mischa Collins, all these terms are mentioned in that single tweet and our algorithm has rightly clustered them together. Smart, isn't it? As a small exercise, observe other clusters and see how the terms occur in the tweets that contain them. One important observation to make here is that the <span class="strong"><em class="calibre10">dendrogram</em></span> correctly maps all frequent terms under a single root, which reaffirms that all these terms <a id="id552" class="calibre1"/>point to our central theme of 2016 resolutions!</p></div></div>

<div class="book" title="Twitter data mining">
<div class="book" title="Topic modeling"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec76" class="calibre1"/>Topic modeling</h2></div></div></div><p class="calibre8">So far, our <a id="id553" class="calibre1"/>analysis has been about tweets related to New Year's resolutions from users across the world. We have analyzed tweets related to a topic of our choice. Ignoring spam and other noisy tweets, more or less, our data conformed to a single topic. The topic itself constituted a group of words (such as health, trillionaire, fitness, diet, mom, and so on) which broadly describe different resolutions. To broaden our scope of analysis and to discover even more insights, let us touch upon the concept of topic modeling.</p><p class="calibre8">Topic modeling is a process of discovering patterns in a corpus of unlabeled text that represents the gist of the corpus. A topic itself may be described as a group of words that occur together to describe a large body of text.</p><p class="calibre8">Another definition tweeted during one of the conferences on topic modeling:</p><div class="mediaobject"><img src="../images/00236.jpeg" alt="Topic modeling" class="calibre11"/><div class="caption"><p class="calibre18">Image source: <a class="calibre1" href="https://twitter.com/footnotesrising/status/264823621799780353">https://twitter.com/footnotesrising/status/264823621799780353</a></p></div></div><p class="calibre12"> </p><p class="calibre8">The aim of topic modeling is to automatically identify the underlying theme of a corpus and thus be useful in applications that require information retrieval based on a theme but in absence of known keywords (the exact opposite of our current usage of search engines). For example, wouldn't it be amazing to learn about relations between two countries from a newspaper's archive by using the theme <span class="strong"><em class="calibre10">relations between country one and country two</em></span> rather than searching for a keyword and then following link after link. Please note that following links to discover information is equally powerful, but it leaves a lot to be desired.</p><p class="calibre8">One of the ways to perform topic modeling is through <span class="strong"><strong class="calibre9">Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong class="calibre9">LDA</strong></span>); it is one of the most powerful and widely used models.</p><p class="calibre8">LDA was presented by David M Blie in his paper <span class="strong"><em class="calibre10">Introduction to Probabilistic Topic Models</em></span> in 2003. LDA, as his paper says, can be defined as a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data is similar. LDA works upon the assumption that documents exhibit multiple topics.</p><p class="calibre8">LDA is a probabilistic <a id="id554" class="calibre1"/>model and the mathematics of it are fairly involved and beyond the scope of this book. In a nonmathematical way, LDA can be explained as a model/process that helps identify the topics that have resulted in the generation of a collection of documents.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre8">For further reading, refer to Blei's paper.</p><p class="calibre8"><a class="calibre1" href="https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf">https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf</a></p><p class="calibre8">A blog which explains everything in simple words:</p><p class="calibre8"><a class="calibre1" href="http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/">http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/</a></p></div><p class="calibre8">For our purpose/use case, we can assume LDA as a model/process which helps us to identify the underlying (hidden/latent) topics from a corpus of unlabeled text. Luckily, R abstracts most of the mathematical details in the form of a library called <code class="email">topicmodels</code>.</p><p class="calibre8">For the purpose of topic modeling, we shall use a new set of tweets. The <span class="strong"><strong class="calibre9">International Space </strong></span><a id="id555" class="calibre1"/>
<span class="strong"><strong class="calibre9">Station</strong></span> (<span class="strong"><strong class="calibre9">ISS</strong></span>) has multiple Twitter handles, and one of them is <code class="email">@ISS_Research</code>, which particularly caters to research related tweets from the ISS. Let us explore what <code class="email">@ISS_Research</code> is up to these days by analyzing the tweets from its timeline. We will analyze these tweets to identify the underlying topics of research at the ISS. For this purpose, we will use the same process to extract tweets and perform transformations/cleanup as we have done before. The following snippet of code does this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># set user handle</strong></span>
<span class="strong"><strong class="calibre9">atISS &lt;- getUser("ISS_Research")</strong></span>

<span class="strong"><strong class="calibre9"># extract iss_research tweets</strong></span>
<span class="strong"><strong class="calibre9">tweets &lt;- userTimeline(atISS, n = 1000)</strong></span>

<span class="strong"><strong class="calibre9">tweets.df=twListToDF(tweets)</strong></span>

<span class="strong"><strong class="calibre9">tweets.df$text &lt;- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))</strong></span>

<span class="strong"><strong class="calibre9">#Document Term Matrix</strong></span>
<span class="strong"><strong class="calibre9">twtrDTM &lt;- DocumentTermMatrix(twtrCorpus, control = list(minWordLength = 1))</strong></span>
</pre></div><p class="calibre8">Please note that the preceding snippet prepares a <span class="strong"><em class="calibre10">document-term matrix</em></span>, unlike last time where we prepared a <span class="strong"><em class="calibre10">term-document matrix</em></span>.</p><p class="calibre8">Once we have tweets in the required format, the <code class="email">LDA</code> utility from the <code class="email">topicmodels</code> package helps us uncover the hidden topics/patterns. The LDA utility requires the number of topics as input along with the document-term matrix. We will try eight topics for now. The <a id="id556" class="calibre1"/>following code uses <code class="email">LDA</code> to extract six terms for each of the eight topics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#topic modeling</strong></span>

<span class="strong"><strong class="calibre9"># find 8 topics</strong></span>
<span class="strong"><strong class="calibre9">ldaTopics &lt;- LDA(twtrDTM, k = 8) </strong></span>

<span class="strong"><strong class="calibre9">#first 6 terms of every topic</strong></span>
<span class="strong"><strong class="calibre9">ldaTerms &lt;- terms(ldaTopics, 6) </strong></span>

<span class="strong"><strong class="calibre9"># concatenate terms</strong></span>
<span class="strong"><strong class="calibre9">(ldaTerms &lt;- apply(ldaTerms, MARGIN = 2, paste, collapse = ", "))</strong></span>
</pre></div><p class="calibre8">The list of topics generated using LDA is as follows:</p><p class="calibre8"><span class="strong"><img src="../images/00237.jpeg" alt="Topic modeling" class="calibre16"/></span></p><p class="calibre8">A visual representation would be easier to understand. We can make use of <code class="email">qplot</code> to quickly plot the topics across time on an area chart, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># first topic identified for every tweet</strong></span>
<span class="strong"><strong class="calibre9">firstTopic &lt;- topics(ldaTopics, 1)</strong></span>

<span class="strong"><strong class="calibre9">topics &lt;- data.frame(date=as.Date(tweets.df$created), firstTopic)</strong></span>

<span class="strong"><strong class="calibre9">qplot(date, ..count.., data=topics, geom="density",fill=ldaTerms[firstTopic], position="stack")+scale_fill_grey()</strong></span>
</pre></div><p class="calibre8">The generated chart looks like the following screenshot:</p><div class="mediaobject"><img src="../images/00238.jpeg" alt="Topic modeling" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Let us now analyze the outputs. The list of terms per topic generated by LDA seems to give us a nice insight into the kind of work/research going on at the ISS. Terms such as mars, microgravity, flower, Cygnus, and so on tell us about the main areas of research or at least the topics <a id="id557" class="calibre1"/>about which scientists/astronauts on the ISS are talking. Terms such as stationcdrkelly and astrotimpeake look more like Twitter handles.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note27" class="calibre1"/>Note</h3><p class="calibre8">A quick exercise would be to use the current <code class="email">@ISS_Research</code> timeline data and mine for the handles, such as <code class="email">stationcdrkelly</code>, to discover more information. Who knows, it may turn out be a nice list of astronauts to follow!</p></div><p class="calibre8">The <code class="email">qplot</code> output adds the time dimension to our plain list of topics. Analyzing topics across the time dimension helps us understand when a particular research topic was discussed or when something amazing was announced. Topic two in the list, or the fourth one from the top in the graph legend comprises the word flower. Since scientists were successful in blooming some orange flowers in space recently, the above graph helps us get an idea that the news first broke on Twitter on/around 15<sup class="calibre15">th</sup> January. A quick look on Twitter/news websites confirms that the news broke by tweet on 18<sup class="calibre15">th</sup> January 2016…close enough!</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip06" class="calibre1"/>Tip</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Colorful area charts</strong></span></p><p class="calibre8">Try removing the option <code class="email">scale_fill_grey()</code> from <code class="email">qplot</code> to get some beautiful charts that are far easier to read than plain gray scale.</p></div><p class="calibre8">So, finally we learnt about topic modeling using LDA on data from the ISS and found what amazing <a id="id558" class="calibre1"/>things scientists and astronauts are doing up there in outer space.</p></div></div>
<div class="book" title="Challenges with social network data mining" id="1S2JE1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec57" class="calibre1"/>Challenges with social network data mining</h1></div></div></div><p class="calibre8">Before we close the <a id="id559" class="calibre1"/>chapter, let us look at the different challenges posed by social networks to the process of data mining. The following points present a few arguments, questions, and challenges:</p><div class="book"><ul class="itemizedlist"><li class="listitem">No doubt the data generated by social networks classifies as big data in every aspect. It has all the volume, velocity, and variety in it to overwhelm any system. Yet, interestingly, the challenge with such a huge source of data is the availability of enough granular data. If we zoom into our data sets and try to use data on a per user basis, we find that there isn't enough data to do some of the most common tasks, such as making recommendations!</li><li class="listitem">Social networks such as Twitter handle millions of users creating and sharing tons of data every second. To keep their systems up and running at all times, they put limits upon the amount of data that can be tapped using their APIs (security is also a major reason behind these limits, though). These limits put data science efforts in a quandary as it is difficult to obtain sufficient samples of data that represent the population correctly/completely. Insufficient samples may result in incorrect patterns or missing out on patterns altogether.</li><li class="listitem">Preprocessing and evaluation of results is also a challenge with social network analysis. While preprocessing data, we remove noisy content. With data coming in all shapes and sizes, determining noisy content is far more of a challenge than simply removing stopwords. Evaluation of results is another challenge, as there is no ground truth available in most cases, and due to the limitations presented here and otherwise, it is difficult to ascertain the validity of results with confidence.</li></ul></div><p class="calibre8">The arguments/challenges presented above call for innovative and creative ways to be devised by data scientists, and that is what makes their job interesting and highly rewarding.</p></div>
<div class="book" title="References" id="1T1401-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec58" class="calibre1"/>References</h1></div></div></div><p class="calibre8">Some of the well-known <a id="id560" class="calibre1"/>books on visualization are as follows :</p><div class="book"><ul class="itemizedlist"><li class="listitem"><a class="calibre1" href="http://www.amazon.in/Information-Dashboard-Design-At-Glance/dp/1938377001">http://www.amazon.in/Information-Dashboard-Design-At-Glance/dp/1938377001</a></li><li class="listitem"><a class="calibre1" href="http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X">http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X</a></li><li class="listitem"><a class="calibre1" href="http://www.amazon.com/Information-Visualization-Second-Interactive-Technologies/dp/1558608192">http://www.amazon.com/Information-Visualization-Second-Interactive-Technologies/dp/1558608192</a></li></ul></div><p class="calibre8">Some well known blogs <a id="id561" class="calibre1"/>on this are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Tableau </strong></span><a id="id562" class="calibre1"/><span class="strong"><strong class="calibre9">specific</strong></span>: <a class="calibre1" href="http://www.jewelloree.com/">http://www.jewelloree.com/</a></li><li class="listitem"><a class="calibre1" href="http://flowingdata.com/">http://flowingdata.com/</a></li><li class="listitem"><a class="calibre1" href="http://www.informationisbeautiful.net/">http://www.informationisbeautiful.net/</a></li><li class="listitem"><a class="calibre1" href="http://infosthetics.com/">http://infosthetics.com/</a></li><li class="listitem"><a class="calibre1" href="http://www.visualisingdata.com/">http://www.visualisingdata.com/</a></li><li class="listitem"><a class="calibre1" href="https://eagereyes.org/">https://eagereyes.org/</a></li><li class="listitem"><a class="calibre1" href="http://thedailyviz.com/">http://thedailyviz.com/</a></li><li class="listitem"><span class="strong"><strong class="calibre9">D3</strong></span>: <a id="id563" class="calibre1"/><a class="calibre1" href="https://d3js.org/">https://d3js.org/</a></li></ul></div></div>
<div class="book" title="Summary" id="1TVKI1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec59" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Social network analysis is one the trending topics in the world of data science. As we have seen throughout the chapter, these platforms not only provide us with ways to connect but they also present a unique opportunity to study human dynamics at a global scale. Through this chapter, we have learned some interesting techniques. We started off by understanding data mining in the social network context followed by the importance of visualizations. We focused on Twitter and understood different objects and APIs to manipulate them. We used various packages from R, such as <code class="email">TwitteR</code> and <code class="email">TM</code>, to connect, collect, and manipulate data for our analysis. We used data from Twitter to learn about frequency throughout. Finally, we presented some of the challenges posed by social networks words and associations, popular devices used by tweeple, hierarchical clustering and even touched upon topic modeling. We used <code class="email">ggplot2</code> and <code class="email">wordcloud</code> to visualize our results to the data mining process in general. While concluding this chapter, we are sure that by now you can appreciate the amazing dynamics behind these platforms and R's ability to analyze it all. We aren't done with <code class="email">@Twitter</code> yet, hold on to your <code class="email">#sentiments</code>!</p></div></body></html>