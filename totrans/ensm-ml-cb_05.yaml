- en: Bag the Models with Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discuss the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble meta-estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging regressors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The combination of classifiers can help reduce misclassification errors substantially. Many
    studies have proved such ensembling methods can significantly reduce the variance
    of the prediction model. Several techniques have been proposed to achieve a variance
    reduction. For example, in many cases, bootstrap aggregating (bagging) classification
    trees have been shown to have higher accuracy than a single classification tree. Bagging
    can be applied to tree-based algorithms to enhance the accuracy of the predictions,
    although it can be used with methods other than tree-based methods as well.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bootstrap aggregation**, also known as **bagging**, is a powerful ensemble
    method that was proposed by Leo Breiman in 1994 to prevent overfitting. The concept
    behind bagging is to combine the predictions of several base learners to create
    a more accurate output.'
  prefs: []
  type: TYPE_NORMAL
- en: Breiman showed that bagging can successfully achieve the desired result in **unstable **learning
    algorithms where small changes to the training data can lead to large variations
    in the predictions. Breiman demonstrated that algorithms such as neural networks
    and decision trees are examples of **unstable** learning algorithms. Bootstrap
    aggregation is effective on small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general procedure for bagging helps to reduce variance for those algorithms
    have high variance. Bagging also supports the classification and regression problem.
    The following diagram shows how the bootstrap aggregation flow works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa8702be-8271-4dba-9264-13e122c57947.png)'
  prefs: []
  type: TYPE_IMG
- en: Using bootstrapping with a training dataset *X***,** we generate N bootstrap
    samples *X1*, *X2,....., XN*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each bootstrap sample, we train a classifier, ![](img/0f7c18d2-774c-46a5-958b-e14bfac327eb.png).
    The combined classifier will average the outputs from all these individual classifiers
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9332286b-1e5f-4fb6-839f-8afe04b73931.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *N* represents the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a bagging classifier, voting is used to make a final prediction. The pseudo-code
    for the bagging classifier proposed by Breiman is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/542d5b93-e002-4f48-b9c6-f76f9909939b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the bagging regressor, the final prediction is the average of
    the predictions of the models that are built over each bootstrap sample. The following
    pseudo-code describes the bagging regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a400a3a-8888-45de-8a81-161d00c22819.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries and reading our file. We suppress
    any warnings using the `warnings.filterwarnings()` function from the `warnings`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now set our working folder. Download the `autompg.csv` file from the
    GitHub and copy the file into your working folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We read our data with `read_csv()` and prefix the name of the data frame with
    `df_` so that it is easier to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We check whether the dataset has any missing values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that the `horsepower` variable has six missing values. We can fill
    in the missing values using the median of the `horsepower` variable''s existing
    values with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that the `carname` variable is an identifier and is not useful in
    our model-building exercise, so we can drop it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look at the data with the `dataframe.head()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how to build a model using bootstrap samples:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating the bootstrap samples. In [Chapter 3](6a5a73fc-dba9-4903-a54a-6c79a8ee57b4.xhtml),
    *Resampling Methods*, we wrote a custom function, `create_bootstrap_oob()`, to
    create both bootstrap and **out-of-bag** (**OOB**) samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code block, we see how to create bootstrap and OOB samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We build models using the bootstrap samples and average the cost function across
    all the models. We use the `SGDRegressor()` on each bootstrap sample. In the following
    code block, we reuse our previously written custom function, `create_bootstrap_oob()`,
    to create the bootstrap and OOB error samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to plot the MSE for each model built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8316674-8694-4913-9562-18dde46e30a7.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 1*, we executed our custom function code to create the `create_bootstrap_oob()` function
    that creates the bootstrap and OOB samples for us. In *Step 2*, we executed the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We decided to make 50 iterations, so we set the `iteration` variable to `50`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `create_bootstrap_oob()` function returned two DataFrame objects, `df_bootstrap_sample`
    and `df_OOB`, in each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used `df_bootstrap_sample` and `df_OOB` as our bootstrap and OOB samples
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We split both the `df_bootstrap_sample` and the `df_OOB` samples into feature
    sets and response variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We fit the `SGDRegressor()` to our bootstrap sample to build our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We passed the OOB sample to the model to predict our values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We compared the predicted values against the response variable in the OOB sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculated the MSE for each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Step 3*, we created a plot to show the MSE for each iteration up to the
    fiftieth iteration. This result may vary because of randomness.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Bagging Predictors* by Leo Breiman, September 1994'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble meta-estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bagging classifier and the bagging regressor are ensemble meta-estimators
    that fit the base classifier and regressor models respectively on random subsets
    of the original dataset. The predictions from each model are combined to create
    the final prediction. These kinds of meta-estimators induce randomization into
    the model-building process and aggregate the outcome. The aggregation averages
    over the iterations for a numerical target variable and performs a plurality vote
    in order to reach a categorical outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging classifiers train each classifier model on a random subset of the original
    training set and aggregate the predictions, then perform a plurality voting for
    a categorical outcome. In the following recipe, we are going to look at an implementation
    of a bagging classifier with bootstrap samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We import `BaggingClassifier` and `DecisionTreeClassifier` from the `scikit-learn`
    library. We also import the other required libraries as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read out the data and take a look at the dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We separate our features and the response set. We also split our data into training
    and testing subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We create an instance of the `DecisionTreeClassifier` class and pass it to
    the `BaggingClassifier()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding code block, we have declared `bootstrap=True`. This
    is the default value and indicates that samples are drawn with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit our model to the training data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the score after passing the test data to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `predict` function to predict the response variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use a code to plot the confusion matrix. Note that this code has
    been taken from [scikit-learn.org](https://scikit-learn.org/stable/). We execute
    the following code to create the `plot_confusion_matrix()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the preceding `plot_confusion_matrix()` function to plot our confusion
    matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6309b5b-fc95-44fc-94d1-6d5607f77a59.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we imported the required libraries to build our decision tree classifier
    model using the bagging classifier. In *Step 2*, we read our dataset, which was `winedata.csv`. In
    *Step 3*, we separated our feature set and the target variable. We also split
    our data into training and testing subsets. In *Step 4*, we created a decision
    tree classifier model and passed it to the `BaggingClassifier()`. In the `DecisionTreeClassifier()`,
    the default value for the `criterion` parameter was `gini`, but we changed it
    to `entropy`. We then passed our decision tree model to the `BaggingClassfier()`. In
    the `BaggingClassfier()`, we have parameters including `n_estimators` and `bootstrap`. `n_estimators` is
    the number of base estimators in the ensemble and has a default value of `10`.
    The `bootstrap` parameter indicates whether samples are drawn with replacement
    or not and is set to `True` by default.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5* and *Step **6*, we fitted our model to the training data and looked
    at the score of the test set. In *Step 7*, we called the `predict()` method and
    passed the test feature set. In *Step 8*, we added the code for the `plot_confusion_matrix()`
    from [http://scikit-learn.org](http://scikit-learn.org), which takes the confusion
    matrix as one of its input parameters and plots the confusion matrix. In *Step
    9*, we called the `plot_confusion_matrix()` function by passing the confusion
    matrix to generate the confusion matrix plot.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also use `GridSearchCV()` from `sklearn.model_selection` to grid search
    the best parameters and use them in the `BaggingClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We then set our parameter values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate our `DecisionTreeClassifier` class and pass it to the `BaggingClassifier()` function.
    Note that we set the `oob_score` to `True` to evaluate the models built on the
    OOB samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `GridSearchCV()` to determine the best parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the optimum parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11224b03-0e99-4ce4-beb7-7d3cd26242fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now take the values returned by `bc_grid.bestparams` and rebuild our decision
    tree models using the `BaggingClassfier()` function. We pass `10` for the `max_leaf_nodes`,
    `3` for the `max_depth`, and `20` for the `n_estimators`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We set our `n_estimators` to `150` in the preceding code block. The `n_estimators`
    parameter indicates the number of trees we want to build. We fit our final model
    to our training data and make a prediction using our test feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then look at the accuracy of our OOB samples in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot our confusion matrix, we can see that we have made an improvement
    with regard to the number of misclassifications that are made. In the earlier
    example, two instances of class 2 were wrongly predicted as class 3, but we can
    now see that the number of misclassifications has reduced to one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ab6bb14-572f-47c8-a678-f7331a597e13.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn guide to bagging classifiers: [https://bit.ly/2zaq8lS](https://bit.ly/2zaq8lS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging regressors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging regressors are similar to bagging classifiers. They train each regressor
    model on a random subset of the original training set and aggregate the predictions.
    Then, the aggregation averages over the iterations because the target variable
    is numeric. In the following recipe, we are going to showcase the implementation
    of a bagging regressor with bootstrap samples.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will import the required libraries, `BaggingRegressor` and `DecisionTreeRegressor`,
    from `sklearn.ensemble` and `sklearn.tree` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We read our dataset, which is `bostonhousing.csv`, and look at the dimensions
    of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We now move on to creating our feature set and our target variable set.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first separate our feature and response set. We will also split our data
    into training and testing subsets in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create an instance of the `DecisionTreeClassifier` class and pass
    it to the `BaggingClassifier()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will fit our model to the training dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the model score in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `predict()` function and pass the test dataset to predict our target
    variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We plot the scatter plot of our actual values and the predicted values of our
    target variable with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding code gives us the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efe7575b-b3c6-4059-8454-653ce7268e60.png)'
  prefs: []
  type: TYPE_IMG
- en: The `matplotlib.pyplot.tight_layout()` automatically adjusts the subplot parameters
    to create specified padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now change the `n_estimators` parameter to 30 in the following code and
    re-execute the steps from *Step 3* to *Step 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbf118f9-2223-46e4-9f14-575e7f0f64c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot of the actual values against the predicted values looks as follows.
    This shows us that the values are predicted more accurately than in our previous
    case when we changed the value of the `n_estimator` parameter from `5` to `30`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/62df9a66-8da7-4b7d-b9e9-8389e017f331.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we separated the features and the target variable set. We also
    split our data into training and testing subsets. In *Step 2*, we created a decision
    tree regressor model and passed it to the `BaggingRegressor()` function. Note
    that we also passed the `n_estimator=5` parameter to the `BaggingRegressor()` function.
    As mentioned earlier, `n_estimator` is the number of trees in the forest we would
    like the algorithm to build. In *Step 3*, we trained our model.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we looked at the model score, which was 0.71\. In *Step 5*, we
    used the `predict()` function to predict our target variable for the test subset.
    After that, in *Step 6*, we plotted a scatterplot to explore the relationship
    between the actual target values and the predicted target values.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we changed the `n_estimator` parameter's value from `5` to `30`
    and re-built our model. This time, we noticed that the model score improved to
    0.82. In *Step 8*, we plotted the actual and predicted values and saw that the
    correlation between the actual and predicted values was much better than our previous
    model, where we used `n_estimators=5`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn guide to bagging regressors: [https://bit.ly/2pZFmUh](https://bit.ly/2pZFmUh)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single estimator versus bagging: [https://bit.ly/2q08db6](https://bit.ly/2q08db6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
