<html><head></head><body>
		<div id="_idContainer072">
			<h1 id="_idParaDest-55"><em class="italic"><a id="_idTextAnchor058"/>Chapter 3</em>: Automated Machine Learning with Open Source Tools and Libraries</h1>
			<p class="author-quote">"Empowerment of individuals is a key part of what makes open source work since, in the end, innovations tend to come from small groups, not from large, structured efforts."</p>
			<p class="author-quote">– Tim O'Reilly</p>
			<p class="author-quote">"In open source, we feel strongly that to really do something well, you have to get a lot of people involved."</p>
			<p class="author-quote">– Linus Torvalds</p>
			<p><a id="_idTextAnchor059"/>In the previous chapter, you looked under the hood of <a id="_idTextAnchor060"/>automated <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) technologies, techniques, and tools. <a id="_idTextAnchor061"/>You learned how AutoML actually works – that is, the algorithms and techniques of automated feature engineering, automated model and hyperparameter turning, and automated deep learning. You also explored Bayesian optimization, reinforcement learning, the evolutionary algorithm, and various gradient-based approaches by looking at their use in automated ML. </p>
			<p>However, as a hands-on engineer, you probably don't get the satisfaction of understanding something fully until you get your hands dirty by trying it out. This chapter will give you the very opportunity to do this. AutoML <strong class="bold">open source software</strong> (<strong class="bold">OSS</strong>) tools and libraries automate the entire life cycle of ideating, conceptualizing, developing, and deploying predictive models. From data preparation through model training to validation as well as deployment, these tools do everything with almost zero human intervention. </p>
			<p>In this chapter, we will review the major OSS tools, including <strong class="bold">TPOT</strong>, <strong class="bold">AutoKeras</strong>, <strong class="bold">auto-sklearn</strong>, <strong class="bold">Featuretools</strong>, and <strong class="bold">Microsoft NNI</strong>, to help you understand the differential value propositions and approaches that are used in each of these libraries. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The open source ecosystem for AutoML</li>
				<li>Introducing TPOT</li>
				<li>Introducing Featuretools</li>
				<li>Introducing Microsoft NNI</li>
				<li>Introducing auto-sklearn</li>
				<li>Introducing AutoKeras</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor062"/>Technical requirements </h1>
			<p>The technical requirements for this chapter are as follows:</p>
			<ul>
				<li>TPOT installation: <a href="http://github.com/EpistasisLab/tpot&#13;">github.com/EpistasisLab/tpot</a></li>
				<li>Featuretools installation: <a href="https://pypi.org/project/featuretools/">https://pypi.org/project/featuretools/</a></li>
				<li>Microsoft NNI installation: <a href="https://github.com/microsoft/nni">https://github.com/microsoft/nni</a></li>
				<li>auto-sklearn installation: <a href="https://automl.github.io/auto-sklearn/master/installation.html">https://automl.github.io/auto-sklearn/master/installation.html</a></li>
				<li>AutoKeras installation: <a href="https://autokeras.com/install/">https://autokeras.com/install/</a></li>
				<li>MNIST download: <a href="https://www.kaggle.com/c/digit-recognizer">https://www.kaggle.com/c/digit-recognizer</a></li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor063"/>The open source ecosystem for AutoML</h1>
			<p>By reviewing the history of automated ML, it is evident that, in the early days, the <a id="_idIndexMarker110"/>focus had <a id="_idIndexMarker111"/>always been on <strong class="bold">hyperparameter</strong> optimization. The <a id="_idIndexMarker112"/>earlier <a id="_idIndexMarker113"/>tools, such <a id="_idIndexMarker114"/>as <strong class="bold">AutoWeka</strong> and <strong class="bold">HyperoptSkLearn</strong>, and later <strong class="bold">TPOT</strong>, had an <a id="_idIndexMarker115"/>original focus on using Bayesian optimization techniques to find the most suitable <strong class="bold">hyperparameters</strong> for the model. However, this trend shifted left to include model selection, which eventually engulfed the entire pipeline by including feature selection, preprocessing, construction, and <a id="_idIndexMarker116"/>data cleaning. The <a id="_idIndexMarker117"/>following table shows some of the prominent automated ML tools that are available, including <strong class="bold">TPOT</strong>, <strong class="bold">AutoKeras</strong>, <strong class="bold">auto-sklearn</strong>, and <strong class="bold">Featuretools</strong>, along with their optimization techniques, ML tasks, and training frameworks:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16890_03_01.jpg" alt="Figure 3.1 – Features of automated ML frameworks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Features of automated ML frameworks</p>
			<p>For several of the examples in this chapter, we will be using the <strong class="bold">MNIST</strong> database of handwritten digits. We will be using the <strong class="bold">scikit-learn</strong> <strong class="source-inline">datasets</strong> package since it has already taken care of data loading and preprocessing <strong class="bold">MNIST</strong> 60,000 training examples and 10,000 test examples. Most data scientists are ML enthusiasts and are very familiar with the <strong class="bold">MNIST</strong> database, which makes it a great candidate for teaching you how to use this library:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B16890_03_02.jpg" alt="Figure 3.2 – MNIST database of handwritten digits – visualization &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – MNIST database of handwritten digits – visualization </p>
			<p>The preceding <a id="_idIndexMarker118"/>image shows what the MNIST dataset <a id="_idIndexMarker119"/>looks like. The dataset is available as part of all major ML and deep learning libraries, and can be downloaded from <a href="https://www.kaggle.com/c/digit-recognizer">https://www.kaggle.com/c/digit-recognizer</a>. </p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor064"/>Introducing TPOT</h1>
			<p>The <strong class="bold">Tree-based Pipeline Optimization Tool</strong>, or <strong class="bold">TPOT</strong> for short, is a product of the University of Pennsylvania's, Computational Genetics Lab. TPOT is an automated ML <a id="_idIndexMarker120"/>tool written in Python. It helps build and optimize ML pipelines with genetic programming. Built on top of scikit-learn, TPOT helps automate the feature selection, preprocessing, construction, model selection, and parameter optimization processes by "<em class="italic">exploring thousands of possible pipelines to find the best one</em>". It is one of the only toolkits with a short learning curve. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="http://github.com/EpistasisLab/tpot">github.com/EpistasisLab/tpot</a>. </p>
			<p>To explain the framework, let's start with a minimal working example. For this example, we will be using the <strong class="bold">MNIST</strong> database of handwritten digits: </p>
			<ol>
				<li>Create a new <strong class="bold">Colab</strong> notebook and run <strong class="source-inline">pip install TPOT</strong>. TPOT can be directly used from the command line or via Python code:<div id="_idContainer022" class="IMG---Figure"><img src="image/B16890_03_03.jpg" alt="Figure 3.3 – Installing TPOT on a Colab notebook&#13;&#10;"/></div><p class="figure-caption">Figure 3.3 – Installing TPOT on a Colab notebook</p></li>
				<li>Import <strong class="source-inline">TPOTClassifier</strong>, the scikit-learn <strong class="source-inline">datasets</strong> package, and the model selection libraries. We <a id="_idIndexMarker121"/>will use these libraries to load the data that we will be using for classification within TPOT: <div id="_idContainer023" class="IMG---Figure"><img src="image/B16890_03_04.jpg" alt="Figure 3.4 – AutoML TPOT example – import statement&#13;&#10;"/></div><p class="figure-caption">Figure 3.4 – AutoML TPOT example – import statement</p></li>
				<li>Now, proceed by loading the <strong class="bold">MNIST</strong> digits dataset. The following <strong class="source-inline">train_test_split</strong> method returns a list containing a train-test split of given inputs. In this case, the inputs are digits data and digits target arrays. Here, you can see that the training size is <strong class="bold">0.75</strong> and that the test size is <strong class="bold">0.25</strong>, which signifies a standard 75-25 split in training and testing data:<div id="_idContainer024" class="IMG---Figure"><img src="image/B16890_03_05.jpg" alt="Figure 3.5 – AutoML TPOT example – loading the digits dataset&#13;&#10;"/></div><p class="figure-caption">Figure 3.5 – AutoML TPOT example – loading the digits dataset</p></li>
				<li>In a <a id="_idIndexMarker122"/>typical scenario, this is where we will choose a model, assign <strong class="bold">hyperparameters</strong>, and then try to fit it on the given data. However, since we are using automated ML as our virtual assistant, let's ask <strong class="bold">TPOT</strong> to do this for us. It's actually pretty easy. <p>To find the right classifier for the job, you must instantiate a <strong class="source-inline">TPOTClassifier</strong>. This class is parametrically quite extensive, as shown in the following screenshot, but we will only be using three key parameters; that is, <strong class="source-inline">verbosity</strong>, <strong class="source-inline">max_time_mins</strong>, and <strong class="source-inline">population_size</strong>: </p><div id="_idContainer025" class="IMG---Figure"><img src="image/B16890_03_06.jpg" alt="Figure 3.6 – AutoML TPOT example – instantiating the TPOTClassifier object&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – AutoML TPOT example – instantiating the TPOTClassifier object</p><p>A quick <a id="_idIndexMarker123"/>note about the arguments being passed to <strong class="source-inline">Classifier</strong> – setting <strong class="source-inline">Verbosity</strong> to <strong class="source-inline">2</strong> will make TPOT print information alongside a progress bar. The <strong class="source-inline">max_time_mins</strong> parameter sets the time allocation in minutes for TPOT to optimize the pipeline, while the <strong class="source-inline">population_size</strong> parameter is the number of individuals in the genetic programming population for every generation. </p><p>Upon starting the experiment, we will set the maximum time to only 1 minute:</p><div id="_idContainer026" class="IMG---Figure"><img src="image/B16890_03_07.jpg" alt="Figure 3.7 – AutoML TPOT example – optimization run of TPOTClassifier&#13;&#10;"/></div><p class="figure-caption">Figure 3.7 – AutoML TPOT example – optimization run of TPOTClassifier</p><p>You will <a id="_idIndexMarker124"/>see that the optimization progress isn't quite as good; it's at 22% since only 9 out of the 40 individuals have been processed in this generation. In this case, the best recommended pipeline is based on <strong class="source-inline">RandomForestClassifier</strong>. </p></li>
				<li>Now, let's increase this to 5 minutes and check the resulting pipeline. At this point, it seems like the recommended classifier is the Gradient Boosting classifier. This is quite interesting:<div id="_idContainer027" class="IMG---Figure"><img src="image/B16890_03_08.jpg" alt="Figure 3.8 – AutoML TPOT example – executing TPOTClassifier&#13;&#10;"/></div><p class="figure-caption">. </p><p class="figure-caption">Figure 3.8 – AutoML TPOT example – executing TPOTClassifier</p></li>
				<li>This time, we will gradually increase the time to <strong class="source-inline">15</strong> minutes, in which case the best <a id="_idIndexMarker125"/>pipeline will turn out to be from the <strong class="bold">k-nearest neighbours</strong> (<strong class="bold">KNN</strong>) classifier:<div id="_idContainer028" class="IMG---Figure"><img src="image/B16890_03_09.jpg" alt="Figure 3.9 – AutoML TPOT classifier – TPOTClassifier fit to get the predictions &#13;&#10;"/></div><p class="figure-caption">Figure 3.9 – AutoML TPOT classifier – TPOTClassifier fit to get the predictions </p></li>
				<li>Increasing the time to <strong class="source-inline">25</strong> minutes does not change the algorithm, but other <strong class="bold">hyperparameters</strong> (number of neighbors) and their accuracy are increased:<div id="_idContainer029" class="IMG---Figure"><img src="image/B16890_03_10.jpg" alt="Figure 3.10 – AutoML TPOT example – running multiple generations and scores &#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – AutoML TPOT example – running multiple generations and scores </p></li>
				<li>Finally, let's <a id="_idIndexMarker126"/>run the experiment for an entire hour:<p class="figure-caption"> </p><div id="_idContainer030" class="IMG---Figure"><img src="image/B16890_03_11.jpg" alt="Figure 3.11 – AutoML TPOT example – TPOT generations and cross-validation scores&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – AutoML TPOT example – TPOT generations and cross-validation scores</p><p>The resulting best pipeline is <strong class="source-inline">KNeighborsClassifier</strong> using feature ranking with recursive feature elimination. Other hyperparameters include <strong class="source-inline">max_features</strong> and <strong class="source-inline">n_estimators</strong>, and the pipeline has an accuracy of <strong class="source-inline">0.98666</strong>:</p><div id="_idContainer031" class="IMG---Figure"><img src="image/B16890_03_12.jpg" alt="Figure 3.12 – AutoML TPOT example – the best pipeline&#13;&#10;"/></div><p class="figure-caption">Figure 3.12 – AutoML TPOT example – the best pipeline</p><p>This reminds me – if 666 is considered an evil number, then 25.8069758011 is the root of all evil.</p></li>
				<li>Also, as <a id="_idIndexMarker127"/>you have probably observed, the <a id="_idIndexMarker128"/>amount of time TPOT had to run its <strong class="bold">cross-validation</strong> (<strong class="bold">CV</strong>) for multiple generations, the pipeline changes, and not only the algorithm but the <strong class="bold">hyperparameters </strong>have evolved. There are also diminishing returns. The improvements in CV scores become smaller and smaller to where, at a certain point, these refinements doesn't make much difference. </li>
			</ol>
			<p>Now, you can export the actual model from TPOT by calling the <strong class="source-inline">export</strong> method:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B16890_03_13.jpg" alt="Figure 3.13 – AutoML TPOT example – exploring the digits pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – AutoML TPOT example – exploring the digits pipeline</p>
			<p>Once the model has been exported, you will be able to see the file in the left-hand pane of <strong class="bold">Google Colab</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B16890_03_14.jpg" alt="Figure 3.14 – AutoML TPOT example – visualizing the TPOT digits pipeline &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – AutoML TPOT example – visualizing the TPOT digits pipeline </p>
			<p>Now that <a id="_idIndexMarker129"/>we know that this pipeline works the best, let's try this out. Notice how we don't have any need for TPOT anymore since we already have the tea (or pipeline, in this case):</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B16890_03_15.jpg" alt="Figure 3.15 – AutoML TPOT example – exporting the pipeline with ExtraTreesClassifier &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – AutoML TPOT example – exporting the pipeline with ExtraTreesClassifier </p>
			<p>Now that <a id="_idIndexMarker130"/>we've created the exported pipeline, let's load up the dataset. Instead of reading it from the CSV file, I can just use the <strong class="source-inline">sklearn</strong> datasets to expedite things. Also, I chose digit <strong class="bold">1</strong> here (<strong class="source-inline">target</strong> <strong class="source-inline">[10]</strong>) in the array, and voila – the prediction was right: </p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B16890_03_16.jpg" alt="Figure 3.16 – AutoML TPOT example – results from the exported pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – AutoML TPOT example – results from the exported pipeline</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor065"/>How does TPOT do this?</h2>
			<p>This looks great, but you didn't buy this book just to learn how to use an API – you want to understand <a id="_idIndexMarker131"/>a bit more about what is going on under the hood. Well, here is the scoop: TPOT has automated the key components of the pipeline using genetic programming; it tried different approaches, as you saw, and then eventually settled on using KNN as the best classifier:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B16890_03_17.jpg" alt="Figure 3.17 – Overview of the TPOT pipeline search&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Overview of the TPOT pipeline search</p>
			<p>Behind <a id="_idIndexMarker132"/>the scenes, TPOT uses genetic programming constructs (selection, crossover, and mutation) to optimize transformation, which helps maximize classification accuracy. The following is a list of operators provided by TPOT: </p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B16890_03_18.jpg" alt="Figure 3.18 – TPOT – a tree-based pipeline optimization tool for automating ML &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – TPOT – a tree-based pipeline optimization tool for automating ML </p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor066"/>Introducing Featuretools</h1>
			<p>Featuretools is an excellent Python framework that helps with automated feature engineering <a id="_idIndexMarker133"/>by using DFS. Feature engineering is a tough problem due to its very nuanced nature. However, this open source toolkit, with its robust timestamp handling and reusable feature primitives, provides a proper framework for us to build and extract combinations of features and their impact. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="https://github.com/FeatureLabs/featuretools/">https://github.com/FeatureLabs/featuretools/</a>. The following steps will guide you through how to install Featuretools, as well as how to run an automated ML experiment using the library. Let's get started:</p>
			<ol>
				<li value="1">To start Featuretools in Colab, you will need to use pip to install the package. In this example, we will try to create features for the Boston Housing Prices dataset:<div id="_idContainer038" class="IMG---Figure"><img src="image/B16890_03_19.jpg" alt="Figure 3.19 – AutoML with Featuretools – installing Featuretools&#13;&#10;"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 3.19 – AutoML with Featuretools – installing Featuretools</p><p>In this experiment, we will be using the Boston Housing Prices dataset, which is a well-known and widely used dataset in ML. The following is a brief description and the metadata of the dataset:</p><div id="_idContainer039" class="IMG---Figure"><img src="image/B16890_03_20.jpg" alt="Figure 3.20 – AutoML with Featuretools – Boston Housing Prices dataset &#13;&#10;"/></div><p class="figure-caption">Figure 3.20 – AutoML with Featuretools – Boston Housing Prices dataset </p></li>
				<li>The Boston <a id="_idIndexMarker134"/>Housing Prices dataset is part of the <strong class="source-inline">scikit-learn</strong> dataset, which makes it very easy to import, as shown here:<div id="_idContainer040" class="IMG---Figure"><img src="image/B16890_03_21.jpg" alt="Figure 3.21 – AutoML with Featuretools – installing Featuretools&#13;&#10;"/></div><p class="figure-caption">Figure 3.21 – AutoML with Featuretools – installing Featuretools</p></li>
				<li>Now, we will use <strong class="bold">Featuretools</strong> to build features. <strong class="bold">Featuretools</strong> helps us build new features by using the existing features and applying different operations to them. You can also link multiple tables and build relationships, but first, we would like to see it working on a single table. The following code shows how easily you can create an entity set (<strong class="source-inline">boston</strong>) using the <strong class="source-inline">featuretools</strong> <strong class="bold">Deep Feature Synthesis</strong> (<strong class="bold">DFS</strong>) API: <div id="_idContainer041" class="IMG---Figure"><img src="image/B16890_03_22.jpg" alt="Figure 3.22 – AutoML with Featuretools – loading the dataset as a pandas DataFrame&#13;&#10;"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 3.22 – AutoML with Featuretools – loading the dataset as a pandas DataFrame</p></li>
				<li>Let's create <a id="_idIndexMarker135"/>a feature tools entity set for the Boston table, and then define the target entries. In this case, we will just create some new features; that is, the <a id="_idIndexMarker136"/>products and the sum of existing features. Once <strong class="bold">Featuretools</strong> has run the DFS, you will have all the summation and product features:<div id="_idContainer042" class="IMG---Figure"><img src="image/B16890_03_23.jpg" alt="Figure 3.23 – AutoML with Featuretools – results of DFS&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 3.23 – AutoML with Featuretools – results of DFS</p>
			<p>The list of features continues:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B16890_03_24.jpg" alt="Figure 3.24 – AutoML with Featuretools – results of DFS – continued &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 3.24 – AutoML with Featuretools – results of DFS – continued </p>
			<p>At this point, you might be wondering, what is the point of doing DFS if it just contains the sums and products of existing features? I'm glad you asked. Think of these derived features as highlighting the latent relationships between multiple data points – and it's not related to sum and product. For example, you can link multiple tables with average order summation, and the algorithms will have additional pre-defined features <a id="_idIndexMarker137"/>to work with to find correlations. This is a very strong and significant quantitative value proposition that's provided by DFS, and it is typically used in machine learning algorithmic competitions:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B16890_03_25.jpg" alt="Figure 3.25 – DFS – analyzing features from Entity &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – DFS – analyzing features from Entity </p>
			<p>The <strong class="bold">Featuretools</strong> website contains an excellent set of demos for predicting next purchases, remaining useful life, appointment no-shows, loan repayment likelihood, customer churn, household <a id="_idIndexMarker138"/>poverty, and malicious internet traffic, among many other use cases: <a href="https://www.featuretools.com/demos/">https://www.featuretools.com/demos/</a>.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor067"/>Introducing Microsoft NNI</h1>
			<p><strong class="bold">Microsoft Neural Network Intelligence</strong> (<strong class="bold">NNI</strong>) is an open source platform that addresses the <a id="_idIndexMarker139"/>three key areas of any automated ML life cycle – automated <a id="_idIndexMarker140"/>feature engineering, architectural <a id="_idIndexMarker141"/>search (also referred to as <strong class="bold">neural architectural search</strong> or <strong class="bold">NAS</strong>), and <strong class="bold">hyperparameter tuning</strong> (<strong class="bold">HPI</strong>). The toolkit also offers <a id="_idIndexMarker142"/>model compression features and operationalization. NNI comes with many hyperparameter tuning algorithms already built in.</p>
			<p>A high-level architecture diagram of NNI is as follows:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B16890_03_26.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26 – Microsoft NNI high-level architecture</p>
			<p>NNI has several state-of-the-art <strong class="bold">hyperparameter</strong> optimization algorithms built in, and they are called <strong class="bold">tuners</strong>. The list includes <strong class="bold">TPE</strong>, <strong class="bold">Random Search</strong>, <strong class="bold">Anneal</strong>, <strong class="bold">Naive Evolution</strong>, <strong class="bold">SMAC</strong>, <strong class="bold">Metis Tuner</strong>, <strong class="bold">Batch Tuner</strong>, <strong class="bold">Grid Search</strong>, <strong class="bold">GP Tuner</strong>, <strong class="bold">Network Morphism</strong>, <strong class="bold">Hyperband</strong>, <strong class="bold">BOHB</strong>, <strong class="bold">PPO Tuner</strong>, and <strong class="bold">PBT Tuner</strong>. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="https://github.com/microsoft/nni">https://github.com/microsoft/nni</a>. More information <a id="_idIndexMarker143"/>about its built-in tuners can be found here: <a href="https://nni.readthedocs.io/en/latest/builtin_tuner.html">https://nni.readthedocs.io/en/latest/builtin_tuner.html</a>.</p>
			<p>Now, let's learn how to install Microsoft NNI and how to run an automated ML experiment using this library.</p>
			<p>Let's go <a id="_idIndexMarker144"/>ahead and install the NNI on our machine using <strong class="source-inline">pip</strong> :</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16890_03_27.jpg" alt="Figure 3.27 – AutoML with Microsoft NNI – installation via Anaconda &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.27 – AutoML with Microsoft NNI – installation via Anaconda </p>
			<p>One of the best features offered by NNI is that it has both a <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) and a <strong class="bold">web UI</strong> so that we <a id="_idIndexMarker145"/>can view the trials and experiments. NNICtl is the command <a id="_idIndexMarker146"/>line that's used to manage the NNI application. You can see the options for experiments in the following screenshot:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16890_03_28.jpg" alt="Figure 3.28 – AutoML with Microsoft NNI – the nnictl command&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28 – AutoML with Microsoft NNI – the nnictl command</p>
			<p>NNI can have <a id="_idIndexMarker147"/>a learning curve if you do not understand how it works. You need to become familiar with the three primary NNI elements for it to work. First, you must define the search space, which you can find in the <strong class="source-inline">search_space.json</strong> file. You also need to update the model code (<strong class="source-inline">main.py</strong>) so that it incorporates <strong class="bold">hyperparameter</strong> tuning. Finally, you must define the experiment (<strong class="source-inline">config.yml</strong>) so that you can define the tuner and trial (execution model code) information:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16890_03_29.jpg" alt="Figure 3.29 – AutoML with Microsoft NNI – the configuration and execution files&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29 – AutoML with Microsoft NNI – the configuration and execution files</p>
			<p>As a reminder, the search space describes the value range of each hyperparameter and for each trial, various hyperparameter values from this space are picked. While creating a configuration <a id="_idIndexMarker148"/>for a hyperparamter tuning experiment, we can limit the maximum number of trials. Also, while creating a hyperparameter search space, we can list the values that we want to try out in the tuning experiment when using the <strong class="bold">choice</strong> type hyperparameter. </p>
			<p>In this case, we have <a id="_idIndexMarker149"/>taken a simple <strong class="bold">Keras MNIST</strong> model and retrofitted it to use NNI for tuning the parameters. Now that the code files are ready, we can run the experiment using the <strong class="source-inline">nnictl</strong> <strong class="source-inline">create</strong> command:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16890_03_30.jpg" alt="Figure 3.30 – AutoML with Microsoft NNI – running the experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 3.30 – AutoML with Microsoft NNI – running the experiment</p>
			<p>You can use <a id="_idIndexMarker150"/>the following commands to find out more about the experiment:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16890_03_31.jpg" alt="Figure 3.31 – AutoML with Microsoft NNI – the nnictrl parameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.31 – AutoML with Microsoft NNI – the nnictrl parameters</p>
			<p>Now, let's look at NNI's secret weapon – its UI. The NNI UI can be accessed via the web UI URL shown in the output console shown in <em class="italic">Figure 3.29</em>. Here, you can see the experiment running, its parameters, and its details. For instance, in this case, we only ran 19 trials, so it ran through these quickly. However, there were no meaningful results, such as us finding out what the best metric is (<strong class="screen-inline">N/A</strong>), as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16890_03_32.jpg" alt="Figure 3.32 – AutoML with the Microsoft NNI UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.32 – AutoML with the Microsoft NNI UI</p>
			<p>Increasing the number of trials to 30 takes longer, but it also gives you better accuracy in the results. Microsoft NNI helps you report intermediate results (results during a trial or training process <a id="_idIndexMarker151"/>before the training is complete). For instance, if the value of the metric being reported is stored in a variable, "x", you can do intermediate reporting using NNI like this:</p>
			<p class="source-code">nni.report_intermediate_result(x)</p>
			<p>The following will be displayed on your screen:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16890_03_33.jpg" alt="Figure 3.33 – AutoML with Microsoft NNI – the UI after completing an experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.33 – AutoML with Microsoft NNI – the UI after completing an experiment</p>
			<p>The NNI U<a id="_idIndexMarker152"/>I also provides you with views of the <strong class="bold">default metrics</strong>, <strong class="bold">hyperparameters</strong>, <strong class="bold">duration</strong>, and <strong class="bold">intermediate results</strong> of each trial. The <strong class="bold">hyperparameter</strong> view is especially amazing because you can visualize how each <strong class="bold">hyperparameter</strong> was selected. For example, in this case, it seems like RELU with a batch size of 1,024 provided significantly good results. This gives you an idea of what underlying algorithm can be used for model selection, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B16890_03_34.jpg" alt="Figure 3.34 – AutoML with Microsoft NNI – the hyperparameters in the experiment &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.34 – AutoML with Microsoft NNI – the hyperparameters in the experiment </p>
			<p>As we learned earlier regarding diminishing returns, increasing the number of trials doesn't increase <a id="_idIndexMarker153"/>the accuracy of the model significantly. In this case, the experiment spent 40 minutes completing 100 trials and provided a best metric of <strong class="bold">0.981</strong> compared to <strong class="bold">0.980</strong> from earlier, as seen in the following screenshot: </p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16890_03_35.jpg" alt="Figure 3.35 – AutoML with Microsoft NNI – the configuration parameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.35 – AutoML with Microsoft NNI – the configuration parameters</p>
			<p>You can also select a different top percentage of results for the <strong class="bold">hyperparameters</strong> to see what <strong class="bold">hyperparameters</strong> we used to get the best performing results:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16890_03_36.jpg" alt="Figure 3.36 – AutoML with Microsoft NNI – the hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.36 – AutoML with Microsoft NNI – the hyperparameters</p>
			<p>Alternatively, you can <a id="_idIndexMarker154"/>just look at the top 5% of the results by selecting <strong class="screen-inline">Top 5%</strong> from the dropdown on the top right of the graph:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B16890_03_37.jpg" alt="Figure 3.37 – AutoML with Microsoft NNI – the hyperparameters for the top 5%&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.37 – AutoML with Microsoft NNI – the hyperparameters for the top 5%</p>
			<p>NNI also allows <a id="_idIndexMarker155"/>you to drill down into each trial visually. You can see all the trial jobs in the following screenshot:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B16890_03_38.jpg" alt="Figure3.38 – AutoML with Microsoft NNI – the experiments list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure3.38 – AutoML with Microsoft NNI – the experiments list</p>
			<p>Alternatively, you can drill down into individual jobs and view various hyperparameters, including <strong class="source-inline">dropout_rate</strong>, <strong class="source-inline">num_units</strong>, learning rate, <strong class="source-inline">batch_size</strong>, and <strong class="source-inline">activation</strong> function:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B16890_03_39.jpg" alt="Figure 3.39 – AutoML with Microsoft NNI – the path for the top 20% of hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.39 – AutoML with Microsoft NNI – the path for the top 20% of hyperparameters</p>
			<p>Being able to see <a id="_idIndexMarker156"/>this level of detail about experiments and hyperparameters is phenomenal, and makes NNI one of our top open source tools for automated ML.</p>
			<p>Before we move on, it is important to note that, like AutoGluon is part of AWS's automated ML offering, NNI is part of Microsoft Azure's automated ML toolset, which makes it much more powerful and verstaile when it comes to reusing it. </p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor068"/>Introducing auto-sklearn </h1>
			<p><strong class="bold">scikit-learn</strong> (also known as <strong class="bold">sklearn</strong>) is a very <a id="_idIndexMarker157"/>popular ML library <a id="_idIndexMarker158"/>for Python development – so popular that it has its own memes:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B16890_03_40.jpg" alt="Figure 3.40 – An ML meme&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.40 – An ML meme</p>
			<p>As part of this ecosystem and based on <em class="italic">Efficient and Robust Automated Machine Learning </em>by <em class="italic">Feurer et al.</em>, <strong class="bold">auto-sklearn</strong> is an automated ML toolkit that performs algorithm selection and <strong class="bold">hyperparameter tuning</strong> using <strong class="bold">Bayesian optimization</strong>, <strong class="bold">meta-learning</strong>, and <strong class="bold">ensemble construction</strong>. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="http://github.com/automl/auto-sklearn">github.com/automl/auto-sklearn</a>.</p>
			<p><strong class="source-inline">auto-sklearn</strong> touts its ease of use for performing automated ML since it's a four-line automated ML solution:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B16890_03_41.jpg" alt="Figure 3.41 – AutoML with auto-sklearn – getting started&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.41 – AutoML with auto-sklearn – getting started</p>
			<p>If the preceding <a id="_idIndexMarker159"/>syntax looks familiar, then it's because this is how <strong class="source-inline">scikit-learn</strong> does predictions and therefore makes <strong class="source-inline">auto-sklearn</strong> one of the easiest libraries to use. <strong class="source-inline">auto-sklearn</strong> uses <strong class="source-inline">scikit-learn</strong> as its backend framework and supports <strong class="bold">Bayesian optimization</strong> with automated <strong class="bold">ensembled construction</strong>. </p>
			<p>Based on <strong class="bold">Combined Algorithm Selection and Hyperparameter optimization</strong> (<strong class="bold">CASH</strong>), as discussed earlier in this book, <strong class="source-inline">auto-sklearn</strong> addresses the problem of finding the best model and its hyperparameters at the same time. The following diagram shows how <strong class="source-inline">auto-sklearn</strong> describes its internal pipeline: </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B16890_03_42.jpg" alt="Figure 3.42 – An auto-sklearn automated ML pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.42 – An auto-sklearn automated ML pipeline</p>
			<p>The underlying automated ML "engine" uses <strong class="bold">Information Retrieval</strong> (<strong class="bold">IR</strong>) and statistical meta feature approaches <a id="_idIndexMarker160"/>to select a variety of configurations, all of which are used as part of Bayesian optimization input. This process is iterative, and auto-sklearn keeps the models to create an ensemble, thus iteratively building a model to maximize performance. Setting up auto-sklearn on Colab can be tricky as you will need to install the following packages to get started: </p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B16890_03_43.jpg" alt="Figure 3.43 – AutoML with auto-sklearn – installing the necessary libraries &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.43 – AutoML with auto-sklearn – installing the necessary libraries </p>
			<p>You may <a id="_idIndexMarker161"/>have to restart the runtime in Colab upon installation. You can also set up auto-sklearn on your local machine by following the instructions specified here: <a href="https://automl.github.io/auto-sklearn/master/installation.html">https://automl.github.io/auto-sklearn/master/installation.html</a>.</p>
			<p>Once you have <a id="_idIndexMarker162"/>completed the installation, you can run the <strong class="bold">auto-sklearn classifier</strong> and get great accuracy and <strong class="bold">hyperparameters</strong> via the magic of automated ML:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B16890_03_44.jpg" alt="Figure 3.44 – AutoML with AutoSkLearn – running a simple experiment for the auto-sklearn classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.44 – AutoML with AutoSkLearn – running a simple experiment for the auto-sklearn classifier</p>
			<p>We should point out that <strong class="bold">auto-sklearn 2</strong>, an experimental version of auto-sklearn, is also out and includes the latest work that's been done on automatic configuration and performance improvements. You can import <strong class="bold">auto-sklearn 2</strong> like so:</p>
			<p class="source-code">from auto-sklearn.experimental.askl2 import Auto-sklearn2Classifier</p>
			<p>Examples of <a id="_idIndexMarker163"/>basic classification, regression, and multi-label classification <a id="_idIndexMarker164"/>datasets, as well as advanced examples of customizing auto-sklearn, are available here: <a href="https://automl.github.io/auto-sklearn/master/examples/">https://automl.github.io/auto-sklearn/master/examples/</a>.</p>
			<p>If you wish, you can try out the advanced use cases of changing the optimization metrics, the train-validation split, providing different feature types, using pandas DataFrames, and inspecting search procedures. These advanced examples also demonstrate how auto-sklearn can be used to <strong class="bold">extend regression</strong>, <strong class="bold">classification</strong>, and <strong class="bold">preprocessor components</strong>, as well as how a number of <strong class="bold">hyperparameters</strong> can be restricted.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor069"/>AutoKeras </h1>
			<p><strong class="bold">Keras</strong> is one of the <a id="_idIndexMarker165"/>most widely used deep learning frameworks <a id="_idIndexMarker166"/>and is an integral part of the TensorFlow 2.0 ecosystem. <strong class="bold">Auto-Keras</strong> is based on the paper by Jin et al., (<a href="https://arxiv.org/abs/1806.10282">https://arxiv.org/abs/1806.10282</a>) which proposed "<em class="italic">a novel method for efficient neural architecture search with network morphism, enabling Bayesian optimization</em>". <strong class="bold">AutoKeras</strong> is built on the concept that since existing neural architecture search algorithms such as <strong class="bold">NASNet</strong> and <strong class="bold">PNAS</strong> are computationally quite expensive, using <strong class="bold">Bayesian optimization</strong> to guide the network's morphism is an efficient approach to explore the search space. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="http://github.com/jhfjhfj1/autokeras">github.com/jhfjhfj1/autokeras</a>.</p>
			<p>The following steps will guide you through how to install AutoKeras and how to run an automated ML experiment using the library. Let's get started:</p>
			<ol>
				<li value="1">To get started with Auto-Keras, run the following <strong class="source-inline">install</strong> commands in Colab or in a Jupyter Notebook. Doing this will install <strong class="bold">AutoKeras</strong> and the <strong class="bold">Keras</strong> tuner. <strong class="bold">AutoKeras</strong> needs its tuner to be greater than version 1.0.1, and the release candidate version can be found in <strong class="source-inline">git uri</strong>:<div id="_idContainer064" class="IMG---Figure"><img src="image/B16890_03_45.jpg" alt="Figure 3.45 – AutoML with AutoKeras – installation&#13;&#10;"/></div><p class="figure-caption">Figure 3.45 – AutoML with AutoKeras – installation</p></li>
				<li>Once you have met the dependencies, you can load the <strong class="bold">MNIST dataset</strong>:<div id="_idContainer065" class="IMG---Figure"><img src="image/B16890_03_46.jpg" alt="Figure 3.46 – AutoML with AutoKeras – loading the training data&#13;&#10;"/></div><p class="figure-caption">Figure 3.46 – AutoML with AutoKeras – loading the training data</p></li>
				<li>Now, you <a id="_idIndexMarker167"/>can get <strong class="bold">AutoKeras</strong> and go through the code for a classifier – in this case, an image classifier. <strong class="bold">AutoKeras</strong> shows the accuracy of data as it calculates the classification metrics:<div id="_idContainer066" class="IMG---Figure"><img src="image/B16890_03_47.jpg" alt="Figure 3.47 – AutoML with AutoKeras – running the epochs&#13;&#10;"/></div><p class="figure-caption">Figure 3.47 – AutoML with AutoKeras – running the epochs</p></li>
				<li>Fast forwarding through the fit procedure, now that you have discovered the <strong class="bold">hyperparameters</strong> and model, you can predict the results for the test features: <div id="_idContainer067" class="IMG---Figure"><img src="image/B16890_03_48.jpg" alt="Figure 3.48 – AutoML with AutoKeras – predicting the best model using the predict command&#13;&#10;"/></div><p class="figure-caption">Figure 3.48 – AutoML with AutoKeras – predicting the best model using the predict command</p><p>You will <a id="_idIndexMarker168"/>get the following results: </p><div id="_idContainer068" class="IMG---Figure"><img src="image/B16890_03_49.jpg" alt="Figure 3.49 – AutoML with AutoKeras – results from the best model&#13;&#10;"/></div><p class="figure-caption">Figure 3.49 – AutoML with AutoKeras – results from the best model</p><p>With these results, you can evaluate the accuracy metrics on the training and test datasets, respectively:</p><div id="_idContainer069" class="IMG---Figure"><img src="image/B16890_03_50.jpg" alt="Figure 3.50 – AutoML with AutoKeras – evaluating for the best model with the test data&#13;&#10;"/></div><p class="figure-caption">Figure 3.50 – AutoML with AutoKeras – evaluating for the best model with the test data</p><p>Like TPOT, you can <a id="_idIndexMarker169"/>easily export the model using the <strong class="source-inline">model.save</strong> method and use it for <strong class="source-inline">eval</strong> at a later date. You can see the model stored in the <strong class="source-inline">model_autokeras</strong> folder on the left-hand pane of your Colab notebook, as shown in the following screenshot:</p><div id="_idContainer070" class="IMG---Figure"><img src="image/B16890_03_51.jpg" alt="Figure 3.51 – AutoML with AutoKeras – exporting as a Keras model&#13;&#10;"/></div><p class="figure-caption">Figure 3.51 – AutoML with AutoKeras – exporting as a Keras model</p></li>
				<li>Once the model has been saved, it can be used to retrieve data using <strong class="source-inline">load_model</strong> and make predictions against it, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B16890_03_52.jpg" alt="Figure 3.52 – AutoML with AutoKeras – predicting the values &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.52 – AutoML with AutoKeras – predicting the values </p>
			<p><strong class="bold">AutoKeras</strong> uses <strong class="bold">Efficient Neural Architecture Search</strong> (<strong class="bold">ENAS</strong>), an approach similar to <a id="_idIndexMarker170"/>transfer learning. Like ensembles, the <strong class="bold">hyperparameters</strong> that are learned during the search are reused for other models, which <a id="_idIndexMarker171"/>helps us avoid having to retrain and provide improved performance. </p>
			<p>As we conclude our overview of open source libraries, some honorable mentions go to two excellent and easy-to-use AutoML frameworks: <strong class="bold">Ludwig</strong> and <strong class="bold">AutoGluon</strong>. </p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor070"/>Ludwig – a code-free AutoML toolbox</h2>
			<p>Uber's automated ML tool, Ludwig, is an open source deep learning toolbox used <a id="_idIndexMarker172"/>for experimenting with, testing, and training ML models. Built on top of TensorFlow, Ludwig enables users to create model baselines and perform automated ML-style experimentation with different network architectures and models. In its latest release, Ludwig now integrates with CometML and supports BERT text encoders. </p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="https://github.com/uber/ludwig">https://github.com/uber/ludwig</a>.</p>
			<p>There are tons of great examples with regard to this topic over here: <a href="https://ludwig-ai.github.io/ludwig-docs/examples/#image-classification-mnist">https://ludwig-ai.github.io/ludwig-docs/examples/#image-classification-mnist</a>.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor071"/>AutoGluon – the AutoML toolkit for deep learning</h1>
			<p>From AWS Labs, with the goal of democratization of ML in mind, AutoGluon is <a id="_idIndexMarker173"/>described as being developed to enable "<em class="italic">easy-to-use and easy-to-extend AutoML with a focus on deep learning and real-world applications spanning image, text, or tabular data</em>". AutoGluon, an integral part of AWS's automated ML strategy, enables both junior and seasoned data scientists to build deep learning models and end-to-end solutions with ease. Like other automated ML toolkits, AutoGluon offers network architecture search, model selection, and the ability for you to improve cus<a id="_idTextAnchor072"/>tom models.</p>
			<p>The toolkit is available on GitHub to be downloaded: <a href="https://github.com/awslabs/autogluon">https://github.com/awslabs/autogluon</a>.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor073"/>Summary</h1>
			<p>In this chapter, you reviewed some major open source tools that are used for AutoML, including <strong class="bold">TPOT</strong>, <strong class="bold">AutoKeras</strong>, <strong class="bold">auto-sklearn</strong>, <strong class="bold">Featuretools</strong>, and <strong class="bold">Microsoft NNI</strong>. These tools have been provided to help you understand the concepts we discussed in <a href="B16890_02_Final_VK_ePub.xhtml#_idTextAnchor049"><em class="italic">Chapter 2</em></a>, <em class="italic">Automated Machine Learning, Algorithms, and Techniques</em>, and the underlying approaches that are used in each of these libraries. </p>
			<p>In the next chapter, we will do an in-depth review of commercial automated ML offerings, starting with the Microsoft Azure platform.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor074"/>Further reading</h1>
			<p>For more information on the topics that were covered in this chapter, please refer to the resources and links:</p>
			<ul>
				<li>TPOT for Automated ML in Python:<a href="https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/">https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/</a></li>
				<li>Featuretools Demos:<a href="https://www.featuretools.com/demos/">https://www.featuretools.com/demos/</a></li>
				<li>Boston Dataset:<a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html</a></li>
				<li>How to Automate ML: <a href="https://www.knime.com/blog/how-to-automate-machine-learning">https://www.knime.com/blog/how-to-automate-machine-learning</a></li>
				<li><em class="italic">Data-driven advice for applying ML to bioinformatics problems, </em>by Randal S. Olson:<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/</a></li>
				<li>TPOT Automated ML in Python: <a href="https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9">https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9</a></li>
				<li>Microsoft NNI: <a href="https://github.com/microsoft/nni">https://github.com/microsoft/nni</a></li>
				<li>auto-sklearn:https://automl.github.io/auto-sklearn/master/examples/20_basic/example_regression.html#sphx-glr-examples-20-basic-example-regression-py</li>
				<li>TPOT Demos:https://github.com/EpistasisLab/tpot/blob/master/tutorials/Digits.ipynb</li>
			</ul>
		</div>
	</body></html>