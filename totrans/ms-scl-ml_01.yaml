- en: Chapter 1. Exploratory Data Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 探索性数据分析
- en: 'Before I dive into more complex methods to analyze your data later in the book,
    I would like to stop at basic data exploratory tasks on which almost all data
    scientists spend at least 80-90% of their productive time. The data preparation,
    cleansing, transforming, and joining the data alone is estimated to be a $44 billion/year
    industry alone (*Data Preparation in the Big Data Era* by *Federico Castanedo*
    and *Best Practices for Data Integration*, *O''Reilly Media*, *2015*). Given this
    fact, it is surprising that people only recently started spending more time on
    the science of developing best practices and establishing good habits, documentation,
    and teaching materials for the whole process of data preparation (*Beautiful Data:
    The Stories Behind Elegant Data Solutions*, edited by *Toby Segaran* and *Jeff
    Hammerbacher*, *O''Reilly Media*, *2009* and *Advanced Analytics with Spark: Patterns
    for Learning from Data at Scale* by *Sandy Ryza et al.*, *O''Reilly Media*, *2015*).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我深入探讨本书后面更复杂的数据分析方法之前，我想停下来谈谈基本的数据探索性任务，几乎所有数据科学家至少花费80-90%的生产时间在这些任务上。仅数据准备、清洗、转换和合并数据本身就是一个价值440亿美元/年的行业（*《大数据时代的数据准备》*，作者：*Federico
    Castanedo* 和 *《数据集成最佳实践》*，*O'Reilly Media*，*2015年*）。鉴于这一事实，人们最近才开始在开发最佳实践的科学、建立良好的习惯、文档和教学材料上投入更多时间，这对于整个数据准备过程来说是个令人惊讶的现象（*《美丽的数据：优雅数据解决方案背后的故事》*，由*Toby
    Segaran* 和 *Jeff Hammerbacher* 编著，*O'Reilly Media*，*2009年* 和 *《Spark高级分析：大规模数据学习的模式》*，作者：*Sandy
    Ryza* 等人，*O'Reilly Media*，*2015年*）。
- en: Few data scientists would agree on specific tools and techniques—and there are
    multiple ways to perform the exploratory data analysis, ranging from Unix command
    line to using very popular open source and commercial ETL and visualization tools.
    The focus of this chapter is how to use Scala and a laptop-based environment to
    benefit from techniques that are commonly referred as a functional paradigm of
    programming. As I will discuss, these techniques can be transferred to exploratory
    analysis over distributed system of machines using Hadoop/Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有数据科学家会在特定的工具和技术上达成一致意见——进行探索性数据分析的方法有很多，从Unix命令行到使用非常流行的开源和商业ETL和可视化工具。本章的重点是如何使用Scala和基于笔记本电脑的环境来利用通常被称为编程功能范式的技术。正如我将讨论的，这些技术可以转移到使用Hadoop/Spark的分布式机器的探索性分析中。
- en: What has functional programming to do with it? Spark was developed in Scala
    for a good reason. Many basic principles that lie at the foundation of functional
    programming, such as lazy evaluation, immutability, absence of side effects, list
    comprehensions, and monads go really well with processing data in distributed
    environments, specifically, when performing the data preparation and transformation
    tasks on big data. Thanks to abstractions, these techniques work well on a local
    workstation or a laptop. As mentioned earlier, this does not preclude us from
    processing very large datasets up to dozens of TBs on modern laptops connected
    to distributed clusters of storage/processing nodes. We can do it one topic or
    focus area at the time, but often we even do not have to sample or filter the
    dataset with proper partitioning. We will use Scala as our primary tool, but will
    resort to other tools if required.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式编程与它有什么关系？Spark是用Scala开发的，这并非没有原因。许多构成函数式编程基础的基本原则，如惰性评估、不可变性、无副作用、列表推导和单子，非常适合在分布式环境中处理数据，特别是在对大数据进行数据准备和转换任务时。多亏了抽象，这些技术在本地工作站或笔记本电脑上也能很好地工作。如前所述，这并不妨碍我们在连接到分布式存储/处理节点集群的现代笔记本电脑上处理数十TB的大型数据集。我们可以一次处理一个主题或关注领域，但通常我们甚至不需要对数据集进行采样或过滤，只需进行适当的分区。我们将使用Scala作为我们的主要工具，但在需要时也会求助于其他工具。
- en: While Scala is complete in the sense that everything that can be implemented
    in other languages can be implemented in Scala, Scala is fundamentally a high-level,
    or even a scripting, language. One does not have to deal with low-level details
    of data structures and algorithm implementations that in their majority have already
    been tested by a plethora of applications and time, in, say, Java or C++—even
    though Scala has its own collections and even some basic algorithm implementations
    today. Specifically, in this chapter, I'll be focusing on using Scala/Spark only
    for high-level tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Scala在某种意义上是完整的，即其他语言可以实现的任何内容都可以在Scala中实现，但Scala本质上是一种高级语言，甚至是一种脚本语言。你不必处理数据结构和算法实现中的低级细节，这些细节在Java或C++等语言中已经由大量的应用程序和时间测试过——尽管Scala今天有自己的集合和一些基本的算法实现。具体来说，在本章中，我将专注于仅使用
    Scala/Spark 进行高级任务。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing Scala
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Scala
- en: Learning simple techniques for initial data exploration
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习简单的数据探索技术
- en: Learning how to downsample the original dataset for faster turnover
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何对原始数据集进行下采样以加快周转速度
- en: Discussing the implementation of basic data transformation and aggregations
    in Scala
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论在 Scala 中实现基本数据转换和聚合的实现
- en: Getting familiar with big data processing tools such as Spark and Spark Notebook
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉大数据处理工具，如 Spark 和 Spark Notebook
- en: Getting code for some basic visualization of datasets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据集的基本可视化代码
- en: Getting started with Scala
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Scala
- en: If you have already installed Scala, you can skip this paragraph. One can get
    the latest Scala download from [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/).
    I used Scala version 2.11.7 on Mac OS X El Capitan 10.11.5\. You can use any other
    version you like, but you might face some compatibility problems with other packages
    such as Spark, a common problem in open source software as the technology adoption
    usually lags by a few released versions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了 Scala，你可以跳过这一段。你可以从[http://www.scala-lang.org/download/](http://www.scala-lang.org/download/)获取最新的
    Scala 下载。我在 Mac OS X El Capitan 10.11.5 上使用了 Scala 版本 2.11.7。你可以使用你喜欢的任何版本，但可能会遇到与其他包（如
    Spark）的兼容性问题，这是开源软件中常见的问题，因为技术的采用通常落后于几个发布版本。
- en: Tip
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In most cases, you should try to maintain precise match between the recommended
    versions as difference in versions can lead to obscure errors and a lengthy debugging
    process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你应该尽量保持推荐版本之间的精确匹配，因为版本之间的差异可能导致模糊的错误和漫长的调试过程。
- en: 'If you installed Scala correctly, after typing `scala`, you should see something
    similar to the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正确安装了 Scala，在输入 `scala` 后，你应该会看到以下类似的内容：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a Scala **read-evaluate-print-loop** (**REPL**) prompt. Although Scala
    programs can be compiled, the content of this chapter will be in REPL, as we are
    focusing on interactivity with, maybe, a few exceptions. The `:help` command provides
    a some utility commands available in REPL (note the colon at the start):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Scala **读取-评估-打印循环**（**REPL**）提示。尽管 Scala 程序可以编译，但本章的内容将在 REPL 中，因为我们专注于与交互，可能只有少数例外。`:help`
    命令提供了在 REPL 中可用的某些实用命令（注意开头的冒号）：
- en: '![Getting started with Scala](img/B04935_01_09.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![开始使用 Scala](img/B04935_01_09.jpg)'
- en: Distinct values of a categorical field
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的字段的不同值
- en: Now, you have a dataset and a computer. For convenience, I have provided you
    a small anonymized and obfuscated sample of clickstream data with the book repository
    that you can get at [https://github.com/alexvk/ml-in-scala.git](https://github.com/alexvk/ml-in-scala.git).
    The file in the `chapter01/data/clickstream` directory contains lines with timestamp,
    session ID, and some additional event information such as URL, category information,
    and so on at the time of the call. The first thing one would do is apply transformations
    to find out the distribution of values for different columns in the dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有一个数据集和一台电脑。为了方便，我为你提供了一个小型的匿名和混淆的点击流数据样本，你可以从[https://github.com/alexvk/ml-in-scala.git](https://github.com/alexvk/ml-in-scala.git)获取。`chapter01/data/clickstream`目录中的文件包含时间戳、会话ID以及一些额外的调用事件信息，如URL、分类信息等。首先要做的事情是对数据进行转换，以找出数据集中不同列的值分布。
- en: '*Figure 01-1 shows* screenshot shows the output of the dataset in the terminal
    window of the `gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz | less
    –U` command. The columns are tab (`^I`) separated. One can notice that, as in
    many real-world big data datasets, many values are missing. The first column of
    the dataset is recognizable as the timestamp. The file contains complex data such
    as arrays, structs, and maps, another feature of big data datasets.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 01-1 展示的截图显示了 `gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz
    | less –U` 命令的输出。列是用制表符（`^I`）分隔的。人们可以注意到，就像在许多现实世界的大数据数据集中一样，许多值是缺失的。数据集的第一列可以识别为时间戳。文件包含复杂的数据，如数组、结构体和映射，这是大数据数据集的另一个特征。'
- en: 'Unix provides a few tools to dissect the datasets. Probably, **less**, **cut**,
    **sort**, and **uniq** are the most frequently used tools for text file manipulations.
    **Awk**, **sed**, **perl**, and **tr** can do more complex transformations and
    substitutions. Fortunately, Scala allows you to transparently use command-line
    tools from within Scala REPL, as shown in the following screenshot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Unix 提供了一些工具来剖析数据集。可能，**less**、**cut**、**sort** 和 **uniq** 是最常用于文本文件操作的工具。**Awk**、**sed**、**perl**
    和 **tr** 可以执行更复杂的转换和替换。幸运的是，Scala 允许您在 Scala REPL 中透明地使用命令行工具，如下面的截图所示：
- en: '![Distinct values of a categorical field](img/B04935_01_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![分类字段的唯一值](img/B04935_01_01.jpg)'
- en: Figure 01-1\. The clickstream file as an output of the less -U Unix command
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-1\. less -U Unix 命令的输出文件点击流
- en: 'Fortunately, Scala allows you to transparently use command-line tools from
    within Scala REPL:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Scala 允许您在 Scala REPL 中透明地使用命令行工具：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I used the `scala.sys.process` package to call familiar Unix commands from Scala
    REPL. From the output, we can immediately see the customers of our Webshop are
    mostly interested in men's shoes and running, and that most visitors are using
    the referral code, **KW_0611081618**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 `scala.sys.process` 包从 Scala REPL 调用熟悉的 Unix 命令。从输出中，我们可以立即看到我们网店的主要客户对男鞋和跑步感兴趣，并且大多数访客都在使用推荐代码，**KW_0611081618**。
- en: Tip
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: One may wonder when we start using complex Scala types and algorithms. Just
    wait, a lot of highly optimized tools were created before Scala and are much more
    efficient for explorative data analysis. In the initial stage, the biggest bottleneck
    is usually just the disk I/O and slow interactivity. Later, we will discuss more
    iterative algorithms, which are usually more memory intensive. Also note that
    the UNIX pipeline operations can be implicitly parallelized on modern multi-core
    computer architectures, as they are in Spark (we will show it in the later chapters).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用复杂的 Scala 类型和方法时，可能会有人感到好奇。请稍等，在 Scala 之前已经创建了大量的高度优化的工具，它们对于探索性数据分析来说效率更高。在初始阶段，最大的瓶颈通常是磁盘
    I/O 和缓慢的交互性。稍后，我们将讨论更多迭代算法，这些算法通常需要更多的内存。此外，请注意，UNIX 管道操作可以在现代的多核计算机架构上隐式并行化，就像在
    Spark 中一样（我们将在后面的章节中展示）。
- en: It has been shown that using compression, implicit or explicit, on input data
    files can actually save you the I/O time. This is particularly true for (most)
    modern semi-structured datasets with repetitive values and sparse content. Decompression
    can also be implicitly parallelized on modern fast multi-core computer architectures,
    removing the computational bottleneck, except, maybe in cases where compression
    is implemented implicitly in hardware (SSD, where we don't need to compress the
    files explicitly). We also recommend using directories rather than files as a
    paradigm for the dataset, where the insert operation is reduced to dropping the
    data file into a directory. This is how the datasets are presented in big data
    Hadoop tools such as Hive and Impala.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，在输入数据文件上使用压缩（隐式或显式）实际上可以节省 I/O 时间。这对于（大多数）现代半结构化数据集尤其如此，这些数据集具有重复的值和稀疏的内容。解压缩也可以在现代快速的多核计算机架构上隐式并行化，从而消除计算瓶颈，除非，可能是在压缩隐式实现硬件（SSD，在这种情况下我们不需要显式压缩文件）的情况下。我们还建议使用目录而不是文件作为数据集的模式，其中插入操作简化为将数据文件放入目录中。这就是在大数据
    Hadoop 工具（如 Hive 和 Impala）中展示数据集的方式。
- en: Summarization of a numeric field
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字字段的摘要
- en: 'Let''s look at the numeric data, even though most of the columns in the dataset
    are either categorical or complex. The traditional way to summarize the numeric
    data is a five-number-summary, which is a representation of the median or mean,
    interquartile range, and minimum and maximum. I''ll leave the computations of
    the median and interquartile ranges till the Spark DataFrame is introduced, as
    it makes these computations extremely easy; but we can compute mean, min, and
    max in Scala by just applying the corresponding operators:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数值数据，尽管数据集的大部分列都是分类的或复杂的。总结数值数据的传统方法是五数概括，它表示中位数或平均值、四分位数范围以及最小值和最大值。我将把中位数和四分位数范围的计算留到介绍Spark
    DataFrame时再进行，因为这使得这些计算变得极其简单；但我们可以通过应用相应的运算符在Scala中计算平均值、最小值和最大值：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Grepping across multiple fields
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个字段中进行grep搜索
- en: Sometimes one needs to get an idea of how a certain value looks across multiple
    fields—most common are IP/MAC addresses, dates, and formatted messages. For examples,
    if I want to see all IP addresses mentioned throughout a file or a document, I
    need to replace the `cut` command in the previous example by `grep -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}`,
    where the `–o` option instructs `grep` to print only the matching parts—a more
    precise regex for the IP address should be `grep –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)`,
    but is about 50% slower on my laptop and the original one works in most practical
    cases. I'll leave it as an excursive to run this command on the sample file provided
    with the book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有时需要了解某个值在多个字段中的外观——最常见的是IP/MAC地址、日期和格式化消息。例如，如果我想查看文件或文档中提到的所有IP地址，我需要将上一个例子中的`cut`命令替换为`grep
    -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}`，其中`-o`选项指示`grep`只打印匹配的部分——一个更精确的IP地址正则表达式应该是`grep
    –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)`，但在我的笔记本电脑上大约慢50%，而原始的正则表达式在大多数实际情况下都适用。我将把它作为一个练习，在书中提供的样本文件上运行这个命令。
- en: Basic, stratified, and consistent sampling
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本抽样、分层抽样和一致抽样
- en: I've met quite a few data practitioners who scorn sampling. Ideally, if one
    can process the whole dataset, the model can only improve. In practice, the tradeoff
    is much more complex. First, one can build more complex models on a sampled set,
    particularly if the time complexity of the model building is non-linear—and in
    most situations, if it is at least *N* log(N)*. A faster model building cycle
    allows you to iterate over models and converge on the best approach faster. In
    many situations, *time to action* is beating the potential improvements in the
    prediction accuracy due to a model built on complete dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到过很多对抽样不屑一顾的数据从业者。理想情况下，如果可以处理整个数据集，模型只会变得更好。在实践中，权衡要复杂得多。首先，可以在抽样集上构建更复杂的模型，特别是如果模型构建的时间复杂度是非线性的——在大多数情况下，至少是*N*
    log(N)*。更快的模型构建周期允许你更快地迭代模型并收敛到最佳方法。在许多情况下，“行动时间”胜过基于完整数据集构建的模型在预测精度上的潜在改进。
- en: Sampling may be combined with appropriate filtering—in many practical situation,
    focusing on a subproblem at a time leads to better understanding of the whole
    problem domain. In many cases, this partitioning is at the foundation of the algorithm,
    like in decision trees, which are considered later. Often the nature of the problem
    requires you to focus on the subset of original data. For example, a cyber security
    analysis is often focused around a specific set of IPs rather than the whole network,
    as it allows to iterate over hypothesis faster. Including the set of all IPs in
    the network may complicate things initially if not throw the modeling off the
    right track.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样可以与适当的过滤相结合——在许多实际情况下，一次关注一个子问题可以更好地理解整个问题域。在许多情况下，这种分区是算法的基础，比如在稍后讨论的决策树中。通常，问题的性质要求你关注原始数据的一个子集。例如，网络安全分析通常关注一组特定的IP地址，而不是整个网络，因为它允许更快地迭代假设。如果不在一开始就包含网络中的所有IP地址，可能会使事情复杂化，甚至可能导致模型偏离正确的方向。
- en: When dealing with rare events, such as clickthroughs in ADTECH, sampling the
    positive and negative cases with different probabilities, which is also sometimes
    called oversampling, often leads to better predictions in short amount of time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理罕见事件时，例如ADTECH中的点击通过，以不同的概率对正负案例进行抽样，这有时也被称为过采样，通常在短时间内就能得到更好的预测。
- en: 'Fundamentally, sampling is equivalent to just throwing a coin—or calling a
    random number generator—for each data row. Thus it is very much like a stream
    filter operation, where the filtering is on an augmented column of random numbers.
    Let''s consider the following example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，采样相当于对每一行数据抛硬币——或者调用随机数生成器。因此，它非常类似于流过滤操作，这里的过滤是在随机数增强列上进行的。让我们考虑以下示例：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is all good, but it has the following disadvantages:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但它有以下缺点：
- en: The number of lines in the resulting file is not known beforehand—even though
    on average it should be 5% of the original file
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果文件的行数事先是未知的——尽管平均来说应该是原始文件的5%
- en: The results of the sampling is non-deterministic—it is hard to rerun this process
    for either testing or verification
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样的结果是随机的——很难重新运行这个过程进行测试或验证
- en: 'To fix the first point, we''ll need to pass a more complex object to the function,
    as we need to maintain the state during the original list traversal, which makes
    the original algorithm less functional and parallelizable (this will be discussed
    later):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，我们需要将一个更复杂的对象传递给函数，因为我们需要在原始列表遍历期间保持状态，这使得原始算法的功能性和并行性降低（这将在后面讨论）：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will output `numLines` lines. Similarly to reservoir sampling, stratified
    sampling is guaranteed to provide the same ratios of input/output rows for all
    strata defined by levels of another attribute. We can achieve this by splitting
    the original dataset into *N* subsets corresponding to the levels, performing
    the reservoir sampling, and merging the results afterwards. However, MLlib library,
    which will be covered in [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib*, already has stratified sampling implementation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出`numLines`行。类似于蓄水池采样，分层采样保证为所有由另一个属性的级别定义的层提供的输入/输出行的比例相同。我们可以通过将原始数据集分割成与级别相对应的*N*个子集，然后进行蓄水池采样，最后合并结果来实现这一点。然而，将在[第3章](ch03.xhtml
    "第3章。使用Spark和MLlib")中介绍的MLlib库，*使用Spark和MLlib*，已经实现了分层采样的实现：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The other bullet point is more subtle; sometimes we want a consistent subset
    of values across multiple datasets, either for reproducibility or to join with
    another sampled dataset. In general, if we sample two datasets, the results will
    contain random subsets of IDs which might have very little or no intersection.
    The cryptographic hashing functions come to the help here. The result of applying
    a hash function such as MD5 or SHA1 is a sequence of bits that is statistically
    uncorrelated, at least in theory. We will use the `MurmurHash` function, which
    is part of the `scala.util.hashing` package:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要点更为微妙；有时我们希望在多个数据集中有一个一致的值子集，无论是为了可重复性还是为了与其他采样数据集连接。一般来说，如果我们采样两个数据集，结果将包含随机子集的ID，这些ID可能几乎没有交集或没有交集。加密哈希函数在这里提供了帮助。应用MD5或SHA1等哈希函数的结果是一串在理论上至少是统计上不相关的位序列。我们将使用`MurmurHash`函数，它是`scala.util.hashing`包的一部分：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function is guaranteed to return exactly the same subset of records based
    on the value of the first field—it is either all records where the first field
    equals a certain value or none—and will come up with approximately one-sixteenth
    of the original sample; the range of `hash` is `0` to `65,535`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数保证根据第一个字段的值返回完全相同的记录子集——要么是第一个字段等于某个特定值的所有记录，要么一个都没有——并且将产生大约原始样本的六分之一；`hash`的范围是`0`到`65,535`。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: MurmurHash? It is not a cryptographic hash!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: MurmurHash？它不是一个加密哈希！
- en: Unlike cryptographic hash functions, such as MD5 and SHA1, MurmurHash is not
    specifically designed to be hard to find an inverse of a hash. It is, however,
    really fast and efficient. This is what really matters in our use case.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与MD5和SHA1等加密哈希函数不同，MurmurHash并不是专门设计为难以找到哈希的逆。然而，它确实非常快且高效。这正是我们用例中真正重要的。
- en: Working with Scala and Spark Notebooks
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala和Spark笔记本
- en: Often the most frequent values or five-number summary are not sufficient to
    get the first understanding of the data. The term **descriptive statistics** is
    very generic and may refer to very complex ways to describe the data. Quantiles,
    a **Paretto** chart or, when more than one attribute is analyzed, correlations
    are also examples of descriptive statistics. When sharing all these ways to look
    at the data aggregates, in many cases, it is also important to share the specific
    computations to get to them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最频繁的值或五数摘要不足以对数据进行初步理解。术语 **描述性统计** 非常通用，可能指代描述数据非常复杂的方法。分位数、**帕累托**图，或者当分析多个属性时，相关性也是描述性统计的例子。当共享所有这些查看数据聚合的方法时，在许多情况下，共享得到它们的特定计算也非常重要。
- en: Scala or Spark Notebook [https://github.com/Bridgewater/scala-notebook](https://github.com/Bridgewater/scala-notebook),
    [https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)
    record the whole transformation path and the results can be shared as a JSON-based
    `*.snb` file. The Spark Notebook project can be downloaded from [http://spark-notebook.io](http://spark-notebook.io),
    and I will provide a sample `Chapter01.snb` file with the book. I will use Spark,
    which I will cover in more detail in [Chapter 3](ch03.xhtml "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 或 Spark 笔记本 [https://github.com/Bridgewater/scala-notebook](https://github.com/Bridgewater/scala-notebook),
    [https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)
    记录整个转换路径，结果可以共享为基于 JSON 的 `*.snb` 文件。Spark Notebook 项目可以从 [http://spark-notebook.io](http://spark-notebook.io)
    下载，并且我会提供与本书配套的示例 `Chapter01.snb` 文件。我将使用 Spark，这将在第 3 章（[Chapter 3. 使用 Spark
    和 MLlib](ch03.xhtml "Chapter 3. 使用 Spark 和 MLlib")）中详细介绍，*使用 Spark 和 MLlib*。
- en: For this particular example, Spark will run in the local mode. Even in the local
    mode Spark can utilize parallelism on your workstation, but it is limited to the
    number of cores and hyperthreads that can run on your laptop or workstation. With
    a simple configuration change, however, Spark can be pointed to a distributed
    set of machines and use resources across a distributed set of nodes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的例子，Spark 将在本地模式下运行。即使在本地模式下，Spark 也可以在您的台式机上利用并行性，但受限于可以在您的笔记本电脑或工作站上运行的内核和超线程数量。然而，通过简单的配置更改，Spark
    可以指向一组分布式机器，并使用分布式节点集的资源。
- en: 'Here is the set of commands to download the Spark Notebook and copy the necessary
    files from the code repository:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是下载 Spark Notebook 并从代码仓库复制必要文件的命令集：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now you can open the notebook at `http://localhost:9000` in your browser, as
    shown in the following screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在浏览器中打开 `http://localhost:9000` 上的笔记本，如下面的截图所示：
- en: '![Working with Scala and Spark Notebooks](img/B04935_01_02.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/B04935_01_02.jpg)'
- en: Figure 01-2\. The first page of the Spark Notebook with the list of notebooks
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-2\. Spark 笔记本的第一页，列出了笔记本列表
- en: 'Open the `Chapter01` notebook by clicking on it. The statements are organized
    into cells and can be executed by clicking on the small right arrow at the top,
    as shown in the following screenshot, or run all cells at once by navigating to
    **Cell** | **Run All**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过单击它打开 `Chapter01` 笔记本。语句被组织成单元格，可以通过单击顶部的较小右箭头执行，如下面的截图所示，或者通过导航到 **Cell**
    | **Run All** 一次性运行所有单元格：
- en: '![Working with Scala and Spark Notebooks](img/B04935_01_03.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/B04935_01_03.jpg)'
- en: Figure 01-3\. Executing the first few cells in the notebook
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-3\. 在笔记本中执行前几个单元格
- en: 'First, we will look at the discrete variables. For example, to get the other
    observable attributes. This task would be totally impossible if distribution of
    the labels, issue the following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看离散变量。例如，为了获取其他可观察属性，如果标签的分布不允许执行以下代码，这项任务将完全不可能：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first time I read the dataset, it took about a minute on MacBook Pro, but
    Spark caches the data in memory and the subsequent aggregation runs take only
    about a second. Spark Notebook provides you the distribution of the values, as
    shown in the following screenshot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次读取数据集时，在 MacBook Pro 上大约需要一分钟，但 Spark 会将数据缓存到内存中，后续的聚合运行只需大约一秒钟。Spark Notebook
    提供了值的分布，如下面的截图所示：
- en: '![Working with Scala and Spark Notebooks](img/B04935_01_04.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/B04935_01_04.jpg)'
- en: Figure 01-4\. Computing the distribution of values for a categorical field
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-4\. 计算分类字段的值分布
- en: 'I can also look at crosstab counts for pairs of discrete variables, which gives
    me an idea of interdependencies between the variables using [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)—the
    object does not support computing correlation measures such as chi-square yet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我还可以查看离散变量的成对列联计数，这让我对变量之间的相互依赖有了概念，使用[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)——该对象尚不支持计算卡方等关联度量：
- en: '![Working with Scala and Spark Notebooks](img/B04935_01_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/B04935_01_05.jpg)'
- en: Figure 01-5\. Contingency table or crosstab
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-5\. 列联表或交叉表
- en: However, we can see that the most popular service is private and it correlates
    well with the `SF` flag. Another way to analyze dependencies is to look at `0`
    entries. For example, the `S2` and `S3` flags are clearly related to the SMTP
    and FTP traffic since all other entries are `0`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以看到最受欢迎的服务是私有的，并且它与 `SF` 标志相关。另一种分析依赖关系的方法是查看 `0` 条目。例如，`S2` 和 `S3` 标志显然与
    SMTP 和 FTP 流量相关，因为所有其他条目都是 `0`。
- en: Of course, the most interesting correlations are with the target variable, but
    these are better discovered by supervised learning algorithms that I will cover
    in [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark and MLlib"), *Working
    with Spark and MLlib,* and [Chapter 5](ch05.xhtml "Chapter 5. Regression and Classification"),
    *Regression and Classification*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最有趣的关联是与目标变量相关，但这些最好通过我在[第 3 章](ch03.xhtml "第 3 章。使用 Spark 和 MLlib")*使用 Spark
    和 MLlib*和[第 5 章](ch05.xhtml "第 5 章。回归和分类")*回归和分类*中将要介绍的监督学习算法来发现。
- en: '![Working with Scala and Spark Notebooks](img/B04935_01_06.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/B04935_01_06.jpg)'
- en: Figure 01-6\. Computing simple aggregations using org.apache.spark.sql.DataFrameStatFunctions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-6\. 使用 org.apache.spark.sql.DataFrameStatFunctions 计算简单的聚合。
- en: 'Analogously, we can compute correlations for numerical variables with the `dataFrame.stat.corr()`
    and `dataFrame.stat.cov()` functions (refer to *Figure 01-6)*. In this case, the
    class supports the **Pearson correlation coefficient**. Alternatively, we can
    use the standard SQL syntax on the parquet file directly:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用 `dataFrame.stat.corr()` 和 `dataFrame.stat.cov()` 函数（参见图 01-6）计算数值变量的关联。在这种情况下，该类支持**皮尔逊相关系数**。或者，我们可以直接在
    parquet 文件上使用标准的 SQL 语法：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, I promised you to compute percentiles. Computing percentiles usually
    involves sorting the whole dataset, which is expensive; however, if the tile is
    one of the first or the last ones, usually it is possible to optimize the computation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我承诺计算百分位数。计算百分位数通常涉及对整个数据集进行排序，这是昂贵的；然而，如果分块是前几个或最后几个之一，通常可以优化计算：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Computing the exact percentiles for a more generic case is more computationally
    expensive and is provided as a part of the Spark Notebook example code.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更通用的案例，计算精确的百分位数更耗费计算资源，并且作为 Spark 笔记本示例代码的一部分提供。
- en: Basic correlations
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本关联
- en: You probably noticed that detecting correlations from contingency tables is
    hard. Detecting patterns takes practice, but many people are much better at recognizing
    the patterns visually. Detecting actionable patterns is one of the primary goals
    of machine learning. While advanced supervised machine learning techniques that
    will be covered in [Chapter 4](ch04.xhtml "Chapter 4. Supervised and Unsupervised
    Learning"), *Supervised and Unsupervised Learning* and [Chapter 5](ch05.xhtml
    "Chapter 5. Regression and Classification"), *Regression and Classification* exist,
    initial analysis of interdependencies between variables can help with the right
    transformation of variables or selection of the best inference technique.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，从列联表中检测关联是困难的。检测模式需要练习，但许多人更擅长直观地识别模式。检测可操作的模式是机器学习的主要目标之一。虽然将在[第 4
    章](ch04.xhtml "第 4 章。监督学习和无监督学习")*监督学习和无监督学习*和[第 5 章](ch05.xhtml "第 5 章。回归和分类")*回归和分类*中介绍的先进的监督机器学习技术存在，但变量之间相互依赖的初步分析可以帮助正确转换变量或选择最佳推理技术。
- en: Multiple well-established visualization tools exist and there are multiple sites,
    such as [http://www.kdnuggets.com](http://www.kdnuggets.com), which specialize
    on ranking and providing recommendations on data analysis, data explorations,
    and visualization software. I am not going to question the validity and accuracy
    of such rankings in this book, and very few sites actually mention Scala as a
    specific way to visualize the data, even if this is possible with, say, a `D3.js`
    package. A good visualization is a great way to deliver your findings to a larger
    audience. One look is worth a thousand words.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着多个成熟的可视化工具和多个网站，如[http://www.kdnuggets.com](http://www.kdnuggets.com)，它们专注于对数据分析和可视化软件进行排名和提供推荐。我不会在本书中质疑这些排名的有效性和准确性，实际上很少有网站提到Scala作为可视化数据的具体方式，即使使用`D3.js`包也是可能的。一个好的可视化是向更广泛的受众传达发现的好方法。一看胜千言。
- en: 'For the purposes of this chapter, I will use **Grapher** that is present on
    every Mac OS notebook. To open **Grapher**, go to Utilities (*shift* + *command*
    + *U* in Finder) and click on the **Grapher** icon (or search by name by pressing
    *command* + *space*). Grapher presents many options, including the following **Log-Log**
    and **Polar** coordinates:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，我将使用每个Mac OS笔记本电脑上都有的**Grapher**。要打开**Grapher**，转到实用工具（在Finder中按*shift*
    + *command* + *U*），然后点击**Grapher**图标（或按*command* + *space*按名称搜索）。Grapher提供了许多选项，包括以下**对数-对数**和**极坐标**：
- en: '![Basic correlations](img/B04935_01_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![基本相关性](img/B04935_01_07.jpg)'
- en: Figure 01-7\. The Grapher window
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图01-7\. Grapher窗口
- en: Fundamentally, the amount of information that can be delivered through visualization
    is limited by the number of pixels on the screen, which, for most modern computers,
    is in millions and color variations, which arguably can also be in millions (*Judd*,
    *Deane B.*; *Wyszecki*, *Günter* (*1975*). *Color in Business, Science and Industry*.
    *Wiley Series in Pure and Applied Optics (3rd ed.)*. New York). If I am working
    on a multidimensional TB dataset, the dataset first needs to be summarized, processed,
    and reduced to a size that can be viewed on a computer screen.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，通过可视化传递的信息量受屏幕上像素数量的限制，对于大多数现代计算机来说，这个数字是数百万，颜色变化也可以说是数百万（*Judd*，*Deane
    B.*；*Wyszecki*，*Günter*（1975）。*商业、科学和工业中的颜色*。*纯与应用光学系列（第3版）*。纽约）。如果我正在处理一个多维TB数据集，数据集首先需要被总结、处理，并减少到可以在计算机屏幕上查看的大小。
- en: 'For the purpose of illustration, I will use the Iris UCI dataset that can be
    found at [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris).
    To bring the dataset into the tool, type the following code (on Mac OS):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我将使用可以在[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)找到的Iris
    UCI数据集。要将数据集导入工具中，请输入以下代码（在Mac OS上）：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Open the new **Point Set** in the **Grapher** (*command* + *alt* + *P*), press
    **Edit Points…** and paste the data by pressing *command* + *V*. The tools has
    line-fitting capabilities with basic linear, polynomial, and exponential families
    and provides the popular chi-squared metric to estimate the goodness of the fit
    with respect to the number of free parameters:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Grapher**中打开新的**点集**（*command* + *alt* + *P*），点击**编辑点…**，然后按*command* + *V*粘贴数据。该工具具有基本的线性、多项式和指数族拟合能力，并提供流行的卡方指标来估计拟合的优劣，相对于自由参数的数量：
- en: '![Basic correlations](img/B04935_01_08.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![基本相关性](img/B04935_01_08.jpg)'
- en: Figure 01-8\. Fitting the Iris dataset using Grapher on Mac OS X
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图01-8\. 在Mac OS X上使用Grapher拟合Iris数据集
- en: We will cover how to estimate the goodness of model fit in the following chapters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍如何估计模型拟合的优劣。
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: I've tried to establish a common ground to perform a more complex data science
    later in the book. Don't expect these to be a complete set of exploratory techniques,
    as the exploratory techniques can extend to running very complex modes. However,
    we covered simple aggregations, sampling, file operations such as read and write,
    working with tools such as notebooks and Spark DataFrames, which brings familiar
    SQL constructs into the arsenal of an analyst working with Spark/Scala.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我试图建立一个共同的基础，以便在本书的后期进行更复杂的数据科学。不要期望这些是完整的探索性技术集，因为探索性技术可以扩展到运行非常复杂的模式。然而，我们涵盖了简单的聚合、抽样、读写等文件操作，以及使用笔记本和Spark
    DataFrames等工具，这将为使用Spark/Scala的分析师带来熟悉的SQL结构。
- en: 'The next chapter will take a completely different turn by looking at the data
    pipelines as a part of a data-driven enterprise and cover the data discovery process
    from the business perspective: what are the ultimate goals we are trying to accomplish
    by doing the data analysis. I will cover a few traditional topics of ML, such
    as supervised and unsupervised learning, after this before delving into more complex
    representations of the data, where Scala really shows it''s advantage over SQL.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将完全转变方向，将数据管道视为数据驱动企业的组成部分，并从业务角度涵盖数据发现过程：通过数据分析，我们试图实现哪些最终目标。在此之后，我将介绍一些机器学习的传统主题，例如监督学习和无监督学习，然后再深入探讨数据的更复杂表示，这正是Scala相对于SQL的优势所在。
