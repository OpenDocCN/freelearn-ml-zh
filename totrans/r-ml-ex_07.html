<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Credit Risk Detection and Prediction &#x2013; Predictive Analytics" id="1BRPS1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics</h1></div></div></div><p class="calibre8">In the previous chapter, we covered a lot of ground in the financial domain where we took up the challenge of detecting and predicting bank customers who could be potential credit risks. We now have a good idea about our main objective regarding credit risk analysis. Besides this, the substantial knowledge gained from descriptive analytics of the dataset and its features will be useful for predictive analytics, as we had mentioned earlier.</p><p class="calibre8">In this chapter, we will be journeying through the world of predictive analytics, which sits at the core of machine learning and data science. Predictive analytics encompasses several things which include classification algorithms, regression algorithms, domain knowledge, and business logic which are combined to build predictive models and derive useful insights from data. We had discussed various machine learning algorithms at the end of the previous chapter which would be applicable for solving our objective, and we will be exploring several of them in this chapter when we build predictive models using the given dataset and these algorithms.</p><p class="calibre8">An interesting take on predictive analytics is that it holds a lot of promise for organizations who want to strengthen their business and profits in the future. With the advent of big data, most organizations now have more data than they can analyze! While this is a big challenge, a tougher challenge is to select the right data points from this data and build predictive models which would be capable of predicting outcomes correctly in the future. However, there are several caveats in this approach because each model is basically mathematical functions based on formulae, assumptions, and probability. Also, in the real world, conditions and scenarios keep changing and evolving and thus one must remember that a predictive model built today may be completely redundant tomorrow.</p><p class="calibre8">A lot of skeptics say that it is extremely difficult for computers to mimic humans to predict outcomes which even humans can't predict due of the ever changing nature of the environment with time, and hence all statistical methods are only valuable under ideal assumptions and conditions. While this is true to some extent, with the right data, a proper mindset, and by applying the right algorithms and techniques, we can build robust predictive models which can definitely try and tackle problems which would be otherwise impossible to tackle by conventional or brute-force methods.</p><p class="calibre8">Predictive modeling is a difficult task and while there might be a lot of challenges and results might be difficult to obtain always, one must take these challenges with a pinch of salt and remember the quotation from the famous statistician George E.P. Box, who claimed that <span class="strong"><em class="calibre10">Essentially all models are wrong but some are useful!</em></span>, which is quite true based on what we discussed earlier. Always remember that a predictive model will never be 100% perfect but, if it is built with the right principles, it will be very useful!</p><p class="calibre8">In this chapter, we will focus on the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Predictive analytics</li><li class="listitem">How to predict credit risk</li><li class="listitem">Important concepts in predictive modeling</li><li class="listitem">Getting the data</li><li class="listitem">Data preprocessing</li><li class="listitem">Feature selection</li><li class="listitem">Modeling using logistic regression</li><li class="listitem">Modeling using support vector machines</li><li class="listitem">Modeling using decision trees</li><li class="listitem">Modeling using random forests</li><li class="listitem">Modeling using neural networks</li><li class="listitem">Model comparison and selection</li></ul></div></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Credit Risk Detection and Prediction &#x2013; Predictive Analytics" id="1BRPS1-973e731d75c2419489ee73e3a0cf4be8">
<div class="book" title="Predictive analytics"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec40" class="calibre1"/>Predictive analytics</h1></div></div></div><p class="calibre8">We had already <a id="id354" class="calibre1"/>discussed a fair bit about predictive analytics in the previous chapter to give you a general overview of what it means. We will be discussing it in more detail in this section. Predictive analytics can be defined as a subset of the machine learning universe, which encompasses a wide variety of supervised learning algorithms based on data science, statistics, and mathematical formulae which enable us to build predictive models using these algorithms and data which has already been collected. These models enable us to make predictions of what might happen in the future based on past observations. Combining this with domain knowledge, expertise, and business logic enables analysts to make data driven decisions using these predictions, which is the ultimate outcome of predictive analytics.</p><p class="calibre8">The data we are talking about here is data which has already been observed in the past and has been collected over a period of time for analysis. This data is often known as historical data or training data which is fed to the model. However, most of the time in the predictive modeling <a id="id355" class="calibre1"/>methodology, we do not feed the raw data directly but use features extracted from the data after suitable transformations. The data features along with a supervised learning algorithm form a predictive model. The data which is obtained in the present can then be fed to this model to predict outcomes which are under observation and also to test the performance of the model with regards to various accuracy metrics. This data is known as testing data in the machine learning world.</p><p class="calibre8">The analytics pipeline that we will be following for carrying out predictive analytics in this chapter is a standard process, which is explained briefly in the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre9">Getting the data</strong></span>: Here we get the dataset on which we will be building the predictive model. We will perform some basic descriptive analysis of the dataset, which we have already <a id="id356" class="calibre1"/>covered in the previous chapter. Once we have the data we will move on to the next step.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre9">Data preprocessing</strong></span>: In this step, we carry out data transformations, such as changing data types, feature scaling, and normalization, if necessary, to prepare the data for being <a id="id357" class="calibre1"/>trained by models. Usually this step is carried out after the dataset preparation step. However, in this case, the end results are the same, so we can perform these steps in any order.</li><li class="listitem" value="3"><span class="strong"><strong class="calibre9">Dataset preparation</strong></span>: In this step, we use some ratio like 70:30 or 60:40 to separate the instances from <a id="id358" class="calibre1"/>the data into training and testing datasets. We usually use the training dataset to train a model and then check its performance and predicting capability with the testing dataset. Often data is divided in proportions of 60:20:20 where we also have a validation dataset besides the other two datasets. However, we will just keep it to two datasets in this chapter.</li><li class="listitem" value="4"><span class="strong"><strong class="calibre9">Feature selection</strong></span>: This <a id="id359" class="calibre1"/>process is an iterative one which even occurs in a later stage if needed. The main objective in this step is to choose a set of attributes or features from the training dataset that enables the predictive model to give the best predictions possible, minimizing error rates and maximizing accuracy.</li><li class="listitem" value="5"><span class="strong"><strong class="calibre9">Predictive modeling</strong></span>: This <a id="id360" class="calibre1"/>is the main step where we select a machine learning algorithm best suited for solving the problem and build the predictive model using the algorithm by feeding it the features extracted from the data in the training dataset. The output of this stage is a predictive <a id="id361" class="calibre1"/>model which can be used for predictions on future data instances.</li><li class="listitem" value="6"><span class="strong"><strong class="calibre9">Model evaluation</strong></span>: In this phase, we use the testing dataset to get predictions from the predictive <a id="id362" class="calibre1"/>model and use a variety of techniques and metrics to measure the performance of the model.</li><li class="listitem" value="7"><span class="strong"><strong class="calibre9">Model tuning</strong></span>: We fine <a id="id363" class="calibre1"/>tune the various parameters of the model and perform feature selection again if necessary. We then rebuild the model and re-evaluate it until we are satisfied with the results.</li><li class="listitem" value="8"><span class="strong"><strong class="calibre9">Model deployment</strong></span>: Once <a id="id364" class="calibre1"/>the predictive model gives a satisfactory performance, we can deploy this model by using a web service in any application to provide predictions in real time or near real time. This step focuses more on software and application development around deploying the model, so we won't be covering this step since it is out of scope. However, there are a lot of tutorials out there regarding building web services around predictive models to enable <span class="strong"><em class="calibre10">Prediction as a service</em></span>.</li></ol><div class="calibre14"/></div><p class="calibre8"><span class="strong"><strong class="calibre9">The last three steps are iterative and may be performed several times if needed</strong></span>.</p><p class="calibre8">Even though the preceding process might look pretty intensive at first glance, it is really a very simple and straight-forward process, which once understood would be useful in building any type of predictive modeling. An important thing to remember is that predictive modeling is an iterative process where we might need to analyze the data and build the model several times by getting feedback from the model predictions and evaluating them. It is therefore extremely important that you do not get discouraged even if your model doesn't perform well on the first go because a model can never be perfect, as we mentioned before, and building a good predictive model is an art as well as science!</p><p class="calibre8">In the next section, we will be focusing on how we would apply predictive analytics to solve our prediction problem and the kind of machine learning algorithms we will be exploring in this chapter.</p></div></div>
<div class="book" title="How to predict credit risk" id="1CQAE1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec41" class="calibre1"/>How to predict credit risk</h1></div></div></div><p class="calibre8">If you remember our <a id="id365" class="calibre1"/>main objective from the previous chapter, we were dealing with customer data from a German bank. We will quickly recap our main problem scenario to refresh your memory. These bank customers are potential candidates who ask for credit loans from the bank with the stipulation that they make monthly payments with some interest on the amount to repay the credit amount. In a perfect world there would be credit loans dished out freely and people would pay them back without issues. Unfortunately, we are not living in a utopian world, and so there will be customers who will default on their credit loans and be unable to repay the amount, causing huge losses to the bank. Therefore, credit risk analysis is one of the crucial areas which banks focus on where they analyze detailed information pertaining to customers and their credit history.</p><p class="calibre8">Now coming back to the main question, for predicting credit risk, we need to analyze the dataset pertaining to customers, build a predictive model around it using machine learning algorithms, and predict whether a customer is likely to default on paying the credit loan and could be labeled as a potential credit risk. The process which we will follow for achieving this is what we discussed in the previous section. You already have an idea about the data and features associated with it from the previous chapter. We will explore several predictive <a id="id366" class="calibre1"/>models, understand the concepts behind how the models work, and then build these models for predicting credit risk. Once we start predicting the outcomes, we will compare the performance of these different models and then talk about the business impact and how to derive insights from the model prediction outcomes. Do note that the predictions are not the output in the predictive analytics life cycle but the valuable insights that we derive from these predictions is the end goal. Businesses such as financial institutions get value only from using domain knowledge to translate prediction outcomes and raw numbers from machine learning algorithms to data driven decisions, which, when executed at the right time, help grow the business.</p><p class="calibre8">For this scenario, if you remember the dataset well, the feature <code class="email">credit.rating</code> is the response or class variable, which indicates the credit rating of the customers. We will be predicting this value for the other customers based on other features which are independent variables. For modeling, we will be using machine learning algorithms which belong to the supervised learning family of algorithms. These algorithms are used for predictions and can be divided into two broad categories: classification and regression. However, they have some differences which we will talk about now. In the case of regression, the values for the variables to be predicted are continuous values, like predicting prices of houses based on different features such as the number of rooms, the area of the house, and so on. Regression mostly deals with estimating and predicting a response value based on input features. In the case of classification, the values for the variables to be predicted have discrete and distinct labels, such as predicting the credit rating for customers for our bank, where the credit rating can either be good, which is denoted by <code class="email">1</code> or bad, which is denoted by <code class="email">0</code>. Classification mostly deals with categorizing and identifying group memberships for each data tuple in the dataset. Algorithms such as logistic regression are special cases of regression models which are used for classification, where the algorithm estimates the odds that a variable is in one of the class labels as a function of the other features. We will be building predictive models using the following machine learning algorithms in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Logistic regression</li><li class="listitem">Support vector machines</li><li class="listitem">Decision trees</li><li class="listitem">Random forests</li><li class="listitem">Neural networks</li></ul></div><p class="calibre8">We have chosen these algorithms to give a good flavor of the diverse set of supervised machine learning algorithms which are present, so that you gain knowledge not only about the concepts behind these models but also learn to implement building models using them, and compare model performances using various techniques. Before we begin our analysis, we will glance over some basic concepts in predictive modeling that are mentioned in this <a id="id367" class="calibre1"/>book and talk about some of them in detail so you get a good idea of what goes on behind the scenes.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Important concepts in predictive modeling"><div class="book" id="1DOR02-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec42" class="calibre1"/>Important concepts in predictive modeling</h1></div></div></div><p class="calibre8">We already looked at <a id="id368" class="calibre1"/>several concepts when we talked about the machine learning pipeline. In this section, we will look at typical terms which are used in predictive modeling, and also discuss about model building and evaluation concepts in detail.</p></div>

<div class="book" title="Important concepts in predictive modeling">
<div class="book" title="Preparing the data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec64" class="calibre1"/>Preparing the data</h2></div></div></div><p class="calibre8">The data preparation step, as discussed earlier, involves preparing the datasets necessary for feature selection and <a id="id369" class="calibre1"/>building the predictive models using the data. We frequently use the following terms in this context:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Datasets</strong></span>: They are typically a collection of data points or observations. Most datasets usually <a id="id370" class="calibre1"/>correspond to some form of structured data which involves a two dimensional data structure, such as a data matrix or data table (in R this is usually represented using a data frame) containing various values. An example is our <code class="email">german_credit_dataset.csv</code> file from <a class="calibre1" title="Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics" href="part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8">Chapter 5</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Descriptive Analytics</em></span>.</li><li class="listitem"><span class="strong"><strong class="calibre9">Data observations</strong></span>: They are the rows in a dataset where each row consists of a set of <a id="id371" class="calibre1"/>observations against a set of attributes. These rows are also often called tuples. For our dataset, each row containing information about a customer is a good example.</li><li class="listitem"><span class="strong"><strong class="calibre9">Data features</strong></span>: They <a id="id372" class="calibre1"/>are the columns in a dataset which describe each row in the dataset. These features are often called attributes or variables. Features such as <code class="email">credit.rating</code>, <code class="email">account.balance</code>, and so on form the features of our credit risk dataset.</li><li class="listitem"><span class="strong"><strong class="calibre9">Data transformation</strong></span>: It refers to the act of transforming various data features as needed <a id="id373" class="calibre1"/>based on observations from descriptive analytics. Data type conversions, missing values imputation, and scaling and normalization are some of the most used techniques. Also, for categorical variables, if your algorithms are not able to detect the different levels in the variable, you need to convert it to several dummy variables; this process is known as one-hot encoding.</li><li class="listitem"><span class="strong"><strong class="calibre9">Training data</strong></span>: It refers to the data which is solely used to train the predictive models. The <a id="id374" class="calibre1"/>machine learning algorithm picks up the tuples from this dataset and tries to find out patterns and learn <a id="id375" class="calibre1"/>from the various observation instances.</li><li class="listitem"><span class="strong"><strong class="calibre9">Testing data</strong></span>: It refers to the data which is fed to the predictive model to get predictions and <a id="id376" class="calibre1"/>then we check the accuracy of the model using the class labels which are already present in the tuples for this dataset. We never train the model with the testing data because it would bias the model and give incorrect evaluations.</li></ul></div></div></div>

<div class="book" title="Important concepts in predictive modeling">
<div class="book" title="Building predictive models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec65" class="calibre1"/>Building predictive models</h2></div></div></div><p class="calibre8">We build the actual <a id="id377" class="calibre1"/>predictive models using machine learning algorithms and data features which finally start giving out predictions as we feed it <a id="id378" class="calibre1"/>new data tuples. Some concepts associated with building predictive models are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Model training</strong></span>: It is analogous to building the predictive model where we use a supervised machine learning algorithm and feed the training data features to it and <a id="id379" class="calibre1"/>build the predictive model.</li><li class="listitem"><span class="strong"><strong class="calibre9">Predictive model</strong></span>: It is <a id="id380" class="calibre1"/>based on some machine learning algorithm, which is essentially a mathematical model at heart, with some assumptions, formulae, and parameter values.</li><li class="listitem"><span class="strong"><strong class="calibre9">Model selection</strong></span>: It is a process where the main objective is to select a predictive <a id="id381" class="calibre1"/>model from several iterations of predictive models. The criteria for selecting the best model can vary, depending on the metrics we want to choose, such as maximizing the accuracy, minimizing the error rate, or getting the maximum AUC, which is something we will discuss later. Cross-validation is a good way to run this iterative process.</li><li class="listitem"><span class="strong"><strong class="calibre9">Hyperparameter optimization</strong></span>: It is basically trying to choose a set of the hyperparameters <a id="id382" class="calibre1"/>used by the algorithm in the model such that the performance of the model is optimal with regards to its prediction accuracy. This is usually done by a grid search algorithm.</li><li class="listitem"><span class="strong"><strong class="calibre9">Cross validation</strong></span>: It is a model validation technique which is used to estimate how a model would perform in a generic fashion. It is mainly used in iterative processes where the end goal is to optimize the model and make sure it is <a id="id383" class="calibre1"/>not over fit to the data so that the model can generalize well with new data and make good predictions. Usually, several rounds of cross validation are run iteratively. Each <a id="id384" class="calibre1"/>round of cross validation involves splitting the data into train and test sets; using the training data to train the model and then evaluating its performance with the test set. At the end of this, we get a model which is the best of the lot.</li></ul></div></div></div>

<div class="book" title="Important concepts in predictive modeling">
<div class="book" title="Evaluating predictive models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec66" class="calibre1"/>Evaluating predictive models</h2></div></div></div><p class="calibre8">The most important <a id="id385" class="calibre1"/>part in predictive modeling is testing whether the models created are actually useful. This is done by evaluating the models on the testing data and using various metrics to measure the performance of <a id="id386" class="calibre1"/>the model. We will discuss some popular model evaluation techniques here. To explain the concepts clearly, we will consider an example with our data. Let us assume we have 100 customers and 40 of them have a bad credit rating with class label 0 and the remaining 60 have a good credit rating with class label 1 in the test data. Let us now assume that our model predicts 22 instances out of the 40 bad instances as bad and the remaining 18 as good. The model also predicts 40 instances out of the 60 good customers as good and the remaining 20 as bad. We will now see how we evaluate the model performance with different techniques:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Prediction values</strong></span>: They <a id="id387" class="calibre1"/>are usually discrete values which belong to a specific class or category and are often known as class labels. In our case, it is a binary classification problem where we deal with two classes where label 1 indicates customers with good credit rating and 0 indicates bad credit rating.</li><li class="listitem"><span class="strong"><strong class="calibre9">Confusion matrix</strong></span>: It is a nice way to see how the model is predicting the different classes. It is a contingency table with usually two rows and two columns for a binary classification problem like ours. It reports the number of predicted <a id="id388" class="calibre1"/>instances in each class against the actual class values. For our preceding example, the confusion matrix would be a 2x2 matrix where two rows would indicate the predicted class labels and two columns would indicate the actual class labels. The total number of predictions with the bad (0) class label which are actually <a id="id389" class="calibre1"/>having the bad label is called <span class="strong"><strong class="calibre9">True Negative</strong></span> (<span class="strong"><strong class="calibre9">TN</strong></span>) and the remaining bad instances wrongly predicted as good are called <a id="id390" class="calibre1"/><span class="strong"><strong class="calibre9">False Positive</strong></span> (<span class="strong"><strong class="calibre9">FP</strong></span>). Correspondingly, the total number of predictions with the good (1) class label that are <a id="id391" class="calibre1"/>actually labeled as good are called <span class="strong"><strong class="calibre9">True Positive</strong></span> (<span class="strong"><strong class="calibre9">TP</strong></span>) and the remaining good instances wrongly predicted as bad are <a id="id392" class="calibre1"/>called <span class="strong"><strong class="calibre9">False Negative</strong></span> (<span class="strong"><strong class="calibre9">FN</strong></span>).<p class="calibre20">We will depict this in the following figure and discuss some important <a id="id393" class="calibre1"/>metrics derived from the confusion matrix, also depicted in the same figure:</p><div class="mediaobject"><img src="../images/00163.jpeg" alt="Evaluating predictive models" class="calibre11"/></div><p class="calibre21"> </p></li></ul></div><p class="calibre8">In the preceding figure, the values which are highlighted in the 2x2 matrix are the ones which were correctly <a id="id394" class="calibre1"/>predicted by our model. The ones in white were incorrectly predicted by the model. We can therefore infer the following measures quite easily: TN is 22, <span class="strong"><strong class="calibre9">FP</strong></span> is <span class="strong"><strong class="calibre9">18</strong></span>, <span class="strong"><strong class="calibre9">TP</strong></span> is <span class="strong"><strong class="calibre9">40</strong></span>, and <span class="strong"><strong class="calibre9">FN</strong></span> is <span class="strong"><strong class="calibre9">20</strong></span>. Total <span class="strong"><strong class="calibre9">N</strong></span> is <span class="strong"><strong class="calibre9">40</strong></span> and total P is <span class="strong"><strong class="calibre9">60</strong></span>, which add up to 100 customers in our example dataset.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Specificity</strong></span> is <a id="id395" class="calibre1"/>also <a id="id396" class="calibre1"/>known as <span class="strong"><strong class="calibre9">true negative rate</strong></span>, and can be represented by the formula <span class="strong"><img src="../images/00164.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which gives us the proportion of total true negatives correctly predicted by the total number of instances which are actually negative. In our case, we have a specificity of <span class="strong"><strong class="calibre9">55%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Sensitivity</strong></span>, also <a id="id397" class="calibre1"/>known as <span class="strong"><strong class="calibre9">true positive rate</strong></span> and <span class="strong"><strong class="calibre9">recall</strong></span>, has the formula <span class="strong"><img src="../images/00165.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which <a id="id398" class="calibre1"/>indicates the proportion of total true positives correctly predicted by the total number of instances which are actually positive. Our example has a sensitivity of <span class="strong"><strong class="calibre9">67%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Precision</strong></span>, also <a id="id399" class="calibre1"/>known as <span class="strong"><strong class="calibre9">positive predictive value</strong></span>, has the formula <span class="strong"><img src="../images/00166.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which indicates <a id="id400" class="calibre1"/>the number of actual positive instances out of all the positive predictions. Our example has a precision of <span class="strong"><strong class="calibre9">69%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Negative </strong></span><a id="id401" class="calibre1"/>
<span class="strong"><strong class="calibre9">predictive value</strong></span> has the formula <span class="strong"><img src="../images/00167.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which indicates the number of actual negative instances out of all the negative predictions. Our example has an NPV of <span class="strong"><strong class="calibre9">52%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">False </strong></span><a id="id402" class="calibre1"/>
<span class="strong"><strong class="calibre9">positive rate</strong></span>, also <a id="id403" class="calibre1"/>known as <span class="strong"><strong class="calibre9">fall-out</strong></span>, is basically the inverse of specificity; where the formula is <span class="strong"><img src="../images/00168.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which indicates the number of false positive predictions out of all the negative instances. Our example has an FPR of <span class="strong"><strong class="calibre9">45%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">False </strong></span><a id="id404" class="calibre1"/>
<span class="strong"><strong class="calibre9">Negative Rate</strong></span>, also known as <a id="id405" class="calibre1"/>
<span class="strong"><strong class="calibre9">miss rate</strong></span>, is basically the inverse of sensitivity; where the formula is <span class="strong"><img src="../images/00169.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>, which indicates the number of false negative predictions out of all the positive instances. Our example has an FNR of <span class="strong"><strong class="calibre9">33%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Accuracy</strong></span> is basically <a id="id406" class="calibre1"/>the metric which denotes <a id="id407" class="calibre1"/>how accurate the model is in making predictions, where the formula is <span class="strong"><img src="../images/00170.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>. Our prediction accuracy is <span class="strong"><strong class="calibre9">62%</strong></span>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">F1</strong></span> score is another metric of measuring a model's accuracy. It takes into account both the precision and recall values by computing the harmonic mean of the values, depicted by the formula <span class="strong"><img src="../images/00171.jpeg" alt="Evaluating predictive models" class="calibre16"/></span>. Our model has an <span class="strong"><strong class="calibre9">f1</strong></span> score of <span class="strong"><strong class="calibre9">68%</strong></span>.</p><p class="calibre8">A <span class="strong"><strong class="calibre9">Receiver </strong></span><a id="id408" class="calibre1"/>
<span class="strong"><strong class="calibre9">Operator Characteristic</strong></span> (<span class="strong"><strong class="calibre9">ROC</strong></span>) curve is basically a plot which is used to visualize the model performance as we vary its threshold. The ROC plot is defined by the FPR and TPR as the <span class="strong"><em class="calibre10">x</em></span> and <span class="strong"><em class="calibre10">y</em></span> axes respectively, and each prediction sample can be fit as a point in the ROC space. Perfect plot would involve a TPR of 1 and an FPR of 0 for all the data points. An average model or a baseline model would be a diagonal straight line from <span class="strong"><em class="calibre10">(0, 0)</em></span> to <span class="strong"><em class="calibre10">(1, 1)</em></span> indicating both values to be <code class="email">0.5</code>. If our model has an ROC curve above the base diagonal line, it indicates that it is performing better than the baseline. The following figure explains how a <a id="id409" class="calibre1"/>typical ROC curve looks <a id="id410" class="calibre1"/>like in general:</p><div class="mediaobject"><img src="../images/00172.jpeg" alt="Evaluating predictive models" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8"><span class="strong"><strong class="calibre9">Area under curve</strong></span> (<span class="strong"><strong class="calibre9">AUC</strong></span>) is basically the area under the ROC curve obtained from the model evaluation. The <a id="id411" class="calibre1"/>AUC is a value which indicates the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. Therefore, the higher the AUC, the better it is. Do check out the file <code class="email">performance_plot_utils.R</code> (shared with the code bundle of the chapter), which has some utility functions to plot and depict these values that we will be using later when we evaluate our model.</p><p class="calibre8">This should give you enough background on important terms and concepts related to predictive modeling, and now we will start with our predictive analysis on the data!</p></div></div>
<div class="book" title="Getting the data" id="1ENBI1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec43" class="calibre1"/>Getting the data</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics" href="part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8">Chapter 5</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Descriptive Analytics</em></span>, we had analyzed the credit <a id="id412" class="calibre1"/>dataset from the German bank and performed several transformations already. We will be working on that transformed dataset in this chapter. We had saved the transformed dataset which you can check out by opening the <code class="email">credit_dataset_final.csv</code> file. We will be doing all our analysis in R as usual. To load the data in memory, run the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; # load the dataset into data frame</strong></span>
<span class="strong"><strong class="calibre9">&gt; credit.df &lt;- read.csv("credit_dataset_final.csv", header = TRUE, sep = ",")</strong></span>
</pre></div><p class="calibre8">This loads the dataset into a data frame which can now be readily accessed using the <code class="email">credit.df</code> variable. Next, we will focus on data transformation and normalization.</p></div>
<div class="book" title="Data preprocessing" id="1FLS41-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec44" class="calibre1"/>Data preprocessing</h1></div></div></div><p class="calibre8">In the data <a id="id413" class="calibre1"/>preprocessing step, we will be focusing on two things mainly: data type transformations and data normalization. Finally we will split the data into training and testing datasets for predictive modeling. You can access the code for this section in the <code class="email">data_preparation.R</code> file. We will be using some utility functions, which are mentioned in the following code snippet. Remember to load them up in memory by running them in the R console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">## data type transformations - factoring</strong></span>
<span class="strong"><strong class="calibre9">to.factors &lt;- function(df, variables){</strong></span>
<span class="strong"><strong class="calibre9">  for (variable in variables){</strong></span>
<span class="strong"><strong class="calibre9">    df[[variable]] &lt;- as.factor(df[[variable]])</strong></span>
<span class="strong"><strong class="calibre9">  }</strong></span>
<span class="strong"><strong class="calibre9">  return(df)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>

<span class="strong"><strong class="calibre9">## normalizing - scaling</strong></span>
<span class="strong"><strong class="calibre9">scale.features &lt;- function(df, variables){</strong></span>
<span class="strong"><strong class="calibre9">  for (variable in variables){</strong></span>
<span class="strong"><strong class="calibre9">    df[[variable]] &lt;- scale(df[[variable]], center=T, scale=T)</strong></span>
<span class="strong"><strong class="calibre9">  }</strong></span>
<span class="strong"><strong class="calibre9">  return(df)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">The preceding functions operate on the data frame to transform the data. For data type transformations, we mainly perform factoring of the categorical variables, where we transform the data type of the categorical features from numeric to factor. There are several numeric variables, which include <code class="email">credit.amount</code>, <code class="email">age</code>, and <code class="email">credit.duration.months</code>, which all have various values and if you remember the distributions in the previous chapter, they were all skewed distributions. This has multiple adverse effects, such as induced collinearity, gradients being affected, and models taking longer times to converge. Hence, we will be using z-score normalization, where each value represented by, let's say, <span class="strong"><img src="../images/00173.jpeg" alt="Data preprocessing" class="calibre16"/></span>, for a feature named E, can be calculated using the formula <span class="strong"><img src="../images/00174.jpeg" alt="Data preprocessing" class="calibre16"/></span> where <span class="strong"><img src="../images/00175.jpeg" alt="Data preprocessing" class="calibre16"/></span> represents the overall mean and <span class="strong"><img src="../images/00176.jpeg" alt="Data preprocessing" class="calibre16"/></span> represents the standard <a id="id414" class="calibre1"/>deviation of the feature <code class="email">E</code>. We use the following code snippet to perform these transformations on our data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; # normalize variables</strong></span>
<span class="strong"><strong class="calibre9">&gt; numeric.vars &lt;- c("credit.duration.months", "age", </strong></span>
<span class="strong"><strong class="calibre9">                    "credit.amount")</strong></span>
<span class="strong"><strong class="calibre9">&gt; credit.df &lt;- scale.features(credit.df, numeric.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; # factor variables</strong></span>
<span class="strong"><strong class="calibre9">&gt; categorical.vars &lt;- c('credit.rating', 'account.balance', </strong></span>
<span class="strong"><strong class="calibre9">+                       'previous.credit.payment.status',</strong></span>
<span class="strong"><strong class="calibre9">+                       'credit.purpose', 'savings', </strong></span>
<span class="strong"><strong class="calibre9">+                       'employment.duration', 'installment.rate',</strong></span>
<span class="strong"><strong class="calibre9">+                       'marital.status', 'guarantor', </strong></span>
<span class="strong"><strong class="calibre9">+                       'residence.duration', 'current.assets',</strong></span>
<span class="strong"><strong class="calibre9">+                       'other.credits', 'apartment.type', </strong></span>
<span class="strong"><strong class="calibre9">+                       'bank.credits', 'occupation', </strong></span>
<span class="strong"><strong class="calibre9">+                       'dependents', 'telephone', </strong></span>
<span class="strong"><strong class="calibre9">+                       'foreign.worker')</strong></span>
<span class="strong"><strong class="calibre9">&gt; credit.df &lt;- to.factors(df=credit.df, </strong></span>
<span class="strong"><strong class="calibre9">                          variables=categorical.vars)</strong></span>
</pre></div><p class="calibre8">Once the preprocessing is complete, we will split our data into training and test datasets in the ratio of 60:40, where 600 tuples will be in the training dataset and 400 tuples will be in the testing dataset. They will be selected in a random fashion as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; # split data into training and test datasets in 60:40 ratio</strong></span>
<span class="strong"><strong class="calibre9">&gt; indexes &lt;- sample(1:nrow(credit.df), size=0.6*nrow(credit.df))</strong></span>
<span class="strong"><strong class="calibre9">&gt; train.data &lt;- credit.df[indexes,]</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.data &lt;- credit.df[-indexes,]</strong></span>
</pre></div><p class="calibre8">Now that we have our <a id="id415" class="calibre1"/>datasets ready, we will explore feature importance and selection in the following section.</p></div>
<div class="book" title="Feature selection" id="1GKCM1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec45" class="calibre1"/>Feature selection</h1></div></div></div><p class="calibre8">The process of feature <a id="id416" class="calibre1"/>selection involves ranking variables or features according to their importance by training a predictive model using them and then trying to find out which variables were the most relevant features for that model. While each model often has its own set of important features, for classification we will use a random forest model here to try and figure out which variables might be of importance in general for classification-based predictions.</p><p class="calibre8">We perform feature selection for several reasons, which include:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Removing redundant or irrelevant features without too much information loss</li><li class="listitem">Preventing overfitting of models by using too many features</li><li class="listitem">Reducing variance of the model which is contributed from excess features</li><li class="listitem">Reducing training time and converging time of models</li><li class="listitem">Building simple and easy to interpret models</li></ul></div><p class="calibre8">We will be using a recursive feature elimination algorithm for feature selection and an evaluation algorithm using a predictive model where we repeatedly construct several machine learning models with different features in different iterations. At each iteration, we keep eliminating irrelevant or redundant features and check the feature subset for which we get maximum accuracy and minimum error. Since this is an iterative process and follows the principle of the popular greedy hill climbing algorithm, an exhaustive search with a global optima outcome is generally not possible and depending on the starting point, we may end up at local optima with a subset of features which may be different from the subset of features we obtain in a different run. However, most of the features in the obtained subset will usually be constant if we run it several times using cross-validation. We will use the random forest algorithm, which we will explain in more detail later on. For now, just remember it is an ensemble learning algorithm that uses several decision trees at each stage in its training process. This tends to reduce variance and overfitting with a small increase towards bias of the model since we introduce some randomness into this process at each stage in the algorithm.</p><p class="calibre8">The code for this section is present in the <code class="email">feature_selection.R</code> file. We will first load the necessary libraries. Install them in case you do not have them installed, as we did in the previous chapters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; library(caret)  # feature selection algorithm</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(randomForest) # random forest algorithm</strong></span>
</pre></div><p class="calibre8">Now we define the utility function for feature selection using recursive feature elimination and random forests for the model evaluation in the following code snippet. Remember to run it in <a id="id417" class="calibre1"/>the R console to load into memory for using it later:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">run.feature.selection &lt;- function(num.iters=20, feature.vars, class.var){</strong></span>
<span class="strong"><strong class="calibre9">  set.seed(10)</strong></span>
<span class="strong"><strong class="calibre9">  variable.sizes &lt;- 1:10</strong></span>
<span class="strong"><strong class="calibre9">  control &lt;- rfeControl(functions = rfFuncs, method = "cv", </strong></span>
<span class="strong"><strong class="calibre9">                        verbose = FALSE, returnResamp = "all", </strong></span>
<span class="strong"><strong class="calibre9">                        number = num.iters)</strong></span>
<span class="strong"><strong class="calibre9">  results.rfe &lt;- rfe(x = feature.vars, y = class.var, </strong></span>
<span class="strong"><strong class="calibre9">             sizes = variable.sizes, </strong></span>
<span class="strong"><strong class="calibre9">             rfeControl = control)</strong></span>
<span class="strong"><strong class="calibre9">  return(results.rfe)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">By default, the preceding code uses cross-validation where the data is split into training and test sets. For each iteration, recursive feature elimination takes place and the model is trained and tested for accuracy and errors on the test set. The data partitions keep changing randomly for every iteration to prevent overfitting of the model and ultimately give a generalized estimate of how the model would perform in a generic fashion. If you observe, our function runs it for 20 iterations by default. Remember, in our case, we always train on the training data which is internally partitioned for cross-validation by the function. The variable <code class="email">feature.vars</code> indicate all the independent feature variables that can be accessed in the training dataset using the <code class="email">train.data[,-1]</code> subsetting, and to access the <code class="email">class.var</code>,which indicates the class variable to be predicted, we subset using <code class="email">train.data[,1]</code>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note17" class="calibre1"/>Note</h3><p class="calibre8">We do not touch the test data at all because we will be using it only for predictions and model evaluations. Therefore, we would not want to influence the model by using that data since it would lead to incorrect evaluations.</p></div><p class="calibre8">We now run the algorithm using our defined function on the training data using the following code. It may take some time to run, so be patient if you see that R is taking some time to return the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">rfe.results &lt;- run.feature.selection(feature.vars=train.data[,-1], </strong></span>
<span class="strong"><strong class="calibre9">                                     class.var=train.data[,1])</strong></span>
<span class="strong"><strong class="calibre9"># view results</strong></span>
<span class="strong"><strong class="calibre9">rfe.results</strong></span>
</pre></div><p class="calibre8">On viewing the results, we get the following output:</p><p class="calibre8"><span class="strong"><img src="../images/00177.jpeg" alt="Feature selection" class="calibre16"/></span></p><p class="calibre8">From the output, you can see that it has found a total of 10 features that were the most important out of the 20 and it has returned the top five features by default. You can play around with this result <a id="id418" class="calibre1"/>variable even further and see all the variables with their importance by using the <code class="email">varImp(rfe.results)</code> command in the R console. The values and importance values may differ for you because the training and test data partitions are done randomly, if you remember, so do not panic if you see different values from the screenshot. However, the top five features will usually remain consistent based on our observations. We will now start building predictive models using the different machine learning algorithms for the next stage of our analytics pipeline. However, do remember that since the training and test sets are randomly chosen, your sets might give slightly different results than what we depict here when we performed these experiments.</p></div>
<div class="book" title="Modeling using logistic regression"><div class="book" id="1HIT82-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec46" class="calibre1"/>Modeling using logistic regression</h1></div></div></div><p class="calibre8">Logistic <a id="id419" class="calibre1"/>regression is a type of regression model where the dependent or class variable is not continuous but categorical, just as in our case, credit rating <a id="id420" class="calibre1"/>is the dependent variable with two classes. In principle, logistic regression is usually perceived as a special case of the family of generalized linear models. This model functions by trying to find out the relationship between the class variable and the other independent feature variables by estimating probabilities. It uses the logistic or sigmoid function for estimating these probabilities. Logistic regression does not predict classes directly but the probability of the outcome. For our model, since we are dealing with a binary classification problem, we will be dealing with binomial logistic regression.</p><p class="calibre8">First we will load the library dependencies as follows and separate the testing feature and class variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">library(caret) # model training and evaluation</strong></span>
<span class="strong"><strong class="calibre9">library(ROCR) # model evaluation</strong></span>
<span class="strong"><strong class="calibre9">source("performance_plot_utils.R") # plotting metric results</strong></span>
<span class="strong"><strong class="calibre9">## separate feature and class variables</strong></span>
<span class="strong"><strong class="calibre9">test.feature.vars &lt;- test.data[,-1]</strong></span>
<span class="strong"><strong class="calibre9">test.class.var &lt;- test.data[,1]</strong></span>
</pre></div><p class="calibre8">Now we will train the initial model with all the independent variables as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.model &lt;- glm(formula=formula.init, data=train.data, family="binomial")</strong></span>
</pre></div><p class="calibre8">We can view the <a id="id421" class="calibre1"/>model details using the <code class="email">summary(lr.model)</code> command, which shows you the various variables and their importance based on <a id="id422" class="calibre1"/>their significance values. We show a part of these details in the following snapshot:</p><p class="calibre8"><span class="strong"><img src="../images/00178.jpeg" alt="Modeling using logistic regression" class="calibre16"/></span></p><p class="calibre8">You can see that the model automatically performs one-hot encoding of categorical variables, which is basically having a variable for each category in that variable. The variables with stars beside them have p-values <code class="email">&lt; 0.05</code> (which we discussed in the previous chapter) and are therefore significant.</p><p class="calibre8">Next, we perform predictions on the test data and evaluate the results as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; lr.predictions &lt;- predict(lr.model, test.data, type="response")</strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.predictions &lt;- round(lr.predictions)</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=lr.predictions, reference=test.class.var, positive='1')</strong></span>
</pre></div><p class="calibre8">On running this, we get a confusion matrix with associated metrics, which we discussed earlier, which are shown in the following figure. It is quite interesting to see that we achieved an <a id="id423" class="calibre1"/>overall accuracy of <span class="strong"><strong class="calibre9">71.75%</strong></span>, which is quite decent, <a id="id424" class="calibre1"/>considering this dataset has a majority of good credit rating customers. It is predicting bad credit ratings quite well, which is evident from the <span class="strong"><strong class="calibre9">specificity</strong></span> of <span class="strong"><strong class="calibre9">48%</strong></span>. <span class="strong"><strong class="calibre9">Sensitivity</strong></span> is <span class="strong"><strong class="calibre9">83%</strong></span>, which is quite good, <span class="strong"><strong class="calibre9">NPV</strong></span> is <span class="strong"><strong class="calibre9">58%</strong></span>, and <span class="strong"><strong class="calibre9">PPV</strong></span> is <span class="strong"><strong class="calibre9">76%</strong></span>.</p><p class="calibre8"><span class="strong"><img src="../images/00179.jpeg" alt="Modeling using logistic regression" class="calibre16"/></span></p><p class="calibre8">We will now try to build another model with some selected features and see how it performs. If you remember, we had some generic features that are important for classification, which we obtained in the earlier section on feature selection. We will still run feature selection specifically for logistic regression to see feature importance using the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">formula &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">formula &lt;- as.formula(formula)</strong></span>
<span class="strong"><strong class="calibre9">control &lt;- trainControl(method="repeatedcv", number=10, repeats=2)</strong></span>
<span class="strong"><strong class="calibre9">model &lt;- train(formula, data=train.data, method="glm", </strong></span>
<span class="strong"><strong class="calibre9">               trControl=control)</strong></span>
<span class="strong"><strong class="calibre9">importance &lt;- varImp(model, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre9">plot(importance)</strong></span>
</pre></div><p class="calibre8">We get the <a id="id425" class="calibre1"/>following plot from which we select the top five <a id="id426" class="calibre1"/>variables to build the next model. As you can see, reading the plot is pretty simple. The greater the importance, the more important the variable is. Feel free to add more variables and build different models using them!</p><div class="mediaobject"><img src="../images/00180.jpeg" alt="Modeling using logistic regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Next, we build the model using a similar approach to before and test the model performance on the test data using the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- "credit.rating ~ account.balance + credit.purpose </strong></span>
<span class="strong"><strong class="calibre9">                      + previous.credit.payment.status + savings </strong></span>
<span class="strong"><strong class="calibre9">                      + credit.duration.months"</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- as.formula(formula.new)</strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.model.new &lt;- glm(formula=formula.new, data=train.data, family="binomial")</strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.predictions.new &lt;- predict(lr.model.new, test.data, type="response") </strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.predictions.new &lt;- round(lr.predictions.new)</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=lr.predictions.new, reference=test.class.var, positive='1')</strong></span>
</pre></div><p class="calibre8">We get the following confusion matrix. However, if you look at the model evaluation results, as shown in following output, you will see that now accuracy has slightly increased and is <span class="strong"><strong class="calibre9">72.25%</strong></span>. <span class="strong"><strong class="calibre9">Sensitivity</strong></span> has shot up to <span class="strong"><strong class="calibre9">94%</strong></span>, which is excellent, but sadly this has happened at the cost of specificity, which has gone down to <span class="strong"><strong class="calibre9">27%</strong></span>, and you can clearly see that more bad credit ratings are being predicted as good, which is 95 out of the total 130 bad credit rating customers in the <a id="id427" class="calibre1"/>test data! <span class="strong"><strong class="calibre9">NPV</strong></span> has gone up to <span class="strong"><strong class="calibre9">69%</strong></span> because <a id="id428" class="calibre1"/>fewer positive credit ratings are <a id="id429" class="calibre1"/>being misclassified as false negatives because of higher sensitivity.</p><p class="calibre8"><span class="strong"><img src="../images/00181.jpeg" alt="Modeling using logistic regression" class="calibre16"/></span></p><p class="calibre8">Now comes the question of which model we want to select for predictions. This does not solely depend on <a id="id430" class="calibre1"/>the accuracy but on the domain and business requirements of the problem. If we predict a customer with a <span class="strong"><strong class="calibre9">bad credit rating</strong></span> (<span class="strong"><strong class="calibre9">0</strong></span>) as <span class="strong"><strong class="calibre9">good</strong></span> (<span class="strong"><strong class="calibre9">1</strong></span>), it means we are going to approve the credit loan for the customer who will end up not paying it, which will cause losses to the bank. However, if we predict a customer with <span class="strong"><strong class="calibre9">good credit rating</strong></span> (<span class="strong"><strong class="calibre9">1</strong></span>) as <span class="strong"><strong class="calibre9">bad</strong></span> (<span class="strong"><strong class="calibre9">0</strong></span>), it means we will deny him the loan in which case the bank will neither profit nor will incur any losses. This is much better than incurring <a id="id431" class="calibre1"/>huge losses by wrongly predicting bad credit ratings as good.</p><p class="calibre8">Therefore, we choose our first model as the best one and now we will view some metric evaluation plots using the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; lr.model.best &lt;- lr.model</strong></span>
<span class="strong"><strong class="calibre9">&gt; lr.prediction.values &lt;- predict(lr.model.best, test.feature.vars, type="response")</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(lr.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="LR ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="LR Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">We get the <a id="id432" class="calibre1"/>following plots from the preceding <a id="id433" class="calibre1"/>code:</p><div class="mediaobject"><img src="../images/00182.jpeg" alt="Modeling using logistic regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">You can see from the preceding plot that the <span class="strong"><strong class="calibre9">AUC</strong></span> is <span class="strong"><strong class="calibre9">0.74</strong></span>, which is pretty good for a start. We will now build the next predictive model using support vector machines using a similar process and see how it fares.</p></div>
<div class="book" title="Modeling using support vector machines"><div class="book" id="1IHDQ2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec47" class="calibre1"/>Modeling using support vector machines</h1></div></div></div><p class="calibre8">Support <a id="id434" class="calibre1"/>vector machines belong to the family of supervised machine learning algorithms used for both classification and regression. Considering <a id="id435" class="calibre1"/>our binary classification problem, unlike logistic regression, the SVM algorithm will build a model around the training data in such a way that the training data points belonging to different classes are separated by a clear gap, which is optimized such that the distance of separation is the maximum. The samples on the margins are typically called the support vectors. The middle of the margin which separates the two classes is called the optimal separating hyperplane.</p><p class="calibre8">Data points on the wrong side of the margin are weighed down to reduce their influence and this is called the soft margin compared to the hard margins of separation we discussed earlier. SVM classifiers can be simple linear classifiers where the data points can be linearly separated. However, if we are dealing with data consisting of several features such that a linear separation is not possible directly, then we make use of several kernels to <a id="id436" class="calibre1"/>achieve the same and these form the non-linear SVM classifiers. You will be able to visualize how an SVM classifier actually <a id="id437" class="calibre1"/>looks much better with the following figure from the official documentation for the <code class="email">svm</code> library in R:</p><div class="mediaobject"><img src="../images/00183.jpeg" alt="Modeling using support vector machines" class="calibre11"/><div class="caption"><p class="calibre18">Image source: <a class="calibre1" href="https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf">https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf</a></p></div></div><p class="calibre12"> </p><p class="calibre8">From the figure, you can clearly see that we can place multiple hyperplanes separating the data points. However, the criterion for choosing the separating hyperplane is such that the distance of separation from the two classes is the maximum and the support vectors are the representative samples of the two classes as depicted on the margins. Revisiting the issue of non-linear classifiers, SVM has several kernels which can be used to achieve this besides the regular linear kernel used for linear classification. These include polynomial, <span class="strong"><strong class="calibre9">radial basis </strong></span><a id="id438" class="calibre1"/>
<span class="strong"><strong class="calibre9">function</strong></span> (<span class="strong"><strong class="calibre9">RBF</strong></span>), and several others. The main principle behind these non-linear kernel functions is that, even if linear separation is not possible in the original feature space, they enable the separation to happen in a higher dimensional transformed feature space where we can use a hyperplane to separate the classes. An important thing to remember is the curse of dimensionality that applies here; since we may end up working with higher dimensional feature spaces, the model generalization error increases and the predictive power of the model decreases. It we have enough data, it still performs <a id="id439" class="calibre1"/>reasonably. We will be using the RBF <a id="id440" class="calibre1"/>kernel, also known as the radial basis function, in our model and for that two important parameters are cost and gamma.</p><p class="calibre8">We will start by loading the necessary dependencies and preparing the testing data features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">library(e1071) # svm model</strong></span>
<span class="strong"><strong class="calibre9">library(caret) # model training\optimizations</strong></span>
<span class="strong"><strong class="calibre9">library(kernlab) # svm model for hyperparameters</strong></span>
<span class="strong"><strong class="calibre9">library(ROCR) # model evaluation</strong></span>
<span class="strong"><strong class="calibre9">source("performance_plot_utils.R") # plot model metrics</strong></span>
<span class="strong"><strong class="calibre9">## separate feature and class variables</strong></span>
<span class="strong"><strong class="calibre9">test.feature.vars &lt;- test.data[,-1]</strong></span>
<span class="strong"><strong class="calibre9">test.class.var &lt;- test.data[,1]</strong></span>
</pre></div><p class="calibre8">Once this is done, we build the SVM model using the training data and the RBF kernel on all the training set features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.model &lt;- svm(formula=formula.init, data=train.data, </strong></span>
<span class="strong"><strong class="calibre9">+                  kernel="radial", cost=100, gamma=1)</strong></span>
<span class="strong"><strong class="calibre9">&gt; summary(svm.model)</strong></span>
</pre></div><p class="calibre8">The properties of the model are generated as follows from the <code class="email">summary</code> function:</p><p class="calibre8"><span class="strong"><img src="../images/00184.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">Now we use our testing data on this model to make predictions and evaluate the results as follows:</p><div class="informalexample"><pre class="programlisting">&gt; svm.predictions &lt;- predict(svm.model, test.feature.vars)
&gt; confusionMatrix(data=svm.predictions, reference=test.class.var, positive="1")</pre></div><p class="calibre8">This gives us the following confusion matrix like we saw in logistic regression and the details are depicted for the model performance. We observe that the <span class="strong"><strong class="calibre9">accuracy</strong></span> is <span class="strong"><strong class="calibre9">67.5%</strong></span>, <span class="strong"><strong class="calibre9">sensitivity</strong></span> is <span class="strong"><strong class="calibre9">100%</strong></span>, and <span class="strong"><strong class="calibre9">specificity</strong></span> is <span class="strong"><strong class="calibre9">0%</strong></span>, which means that it is a very aggressive model which <a id="id441" class="calibre1"/>just predicts every customer rating as <a id="id442" class="calibre1"/>good. This model clearly suffers from the major class classification problem and we need to improve this.</p><p class="calibre8"><span class="strong"><img src="../images/00185.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">To build a better model, we need some feature selection. We already have the top five best features which we had obtained in the <span class="strong"><em class="calibre10">Feature selection</em></span> section. Nevertheless, we will still run a feature selection algorithm specifically for SVM to see feature importance, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; control &lt;- trainControl(method="repeatedcv", number=10, repeats=2)</strong></span>
<span class="strong"><strong class="calibre9">&gt; model &lt;- train(formula.init, data=train.data, method="svmRadial", </strong></span>
<span class="strong"><strong class="calibre9">+                trControl=control)</strong></span>
<span class="strong"><strong class="calibre9">&gt; importance &lt;- varImp(model, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot(importance, cex.lab=0.5)</strong></span>
</pre></div><p class="calibre8">This gives us a plot and <a id="id443" class="calibre1"/>we see that the top five important variables are similar to our top five best features, except this algorithm ranks age as more important than <code class="email">credit.amount</code>, so you can test this by building several models with different features and see which <a id="id444" class="calibre1"/>one gives the best results. For us, the features selected from random forests gave a better result. The variable importance plot is depicted as follows:</p><div class="mediaobject"><img src="../images/00186.jpeg" alt="Modeling using support vector machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">We now build a new SVM model based on the top five features that gave us the best results and evaluate its performance on the test data using the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- "credit.rating ~ account.balance + </strong></span>
<span class="strong"><strong class="calibre9">                   credit.duration.months + savings + </strong></span>
<span class="strong"><strong class="calibre9">                   previous.credit.payment.status + credit.amount"</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- as.formula(formula.new)</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.model.new &lt;- svm(formula=formula.new, data=train.data, </strong></span>
<span class="strong"><strong class="calibre9">+                  kernel="radial", cost=100, gamma=1)</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.predictions.new &lt;- predict(svm.model.new, test.feature.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=svm.predictions.new, </strong></span>
<span class="strong"><strong class="calibre9">                  reference=test.class.var, positive="1")</strong></span>
</pre></div><p class="calibre8">The preceding snippet gives us a confusion matrix finally on the test data and we observe that the overall accuracy has in fact dropped by <span class="strong"><strong class="calibre9">1%</strong></span> to <span class="strong"><strong class="calibre9">66.5%</strong></span>. However, the most interesting part is that now our model is able to predict more bad ratings from bad, which can be seen from the confusion matrix. The <span class="strong"><strong class="calibre9">specificity</strong></span> is now <span class="strong"><strong class="calibre9">38%</strong></span> compared to <span class="strong"><strong class="calibre9">0%</strong></span> earlier and, correspondingly, the <span class="strong"><strong class="calibre9">sensitivity</strong></span> has gone down to <span class="strong"><strong class="calibre9">80%</strong></span> from <span class="strong"><strong class="calibre9">100%</strong></span>, which is still good <a id="id445" class="calibre1"/>because now this model is actually useful and <a id="id446" class="calibre1"/>profitable! You can see from this that feature selection can indeed be extremely powerful. The confusion matrix for the preceding observations is depicted in the following snapshot:</p><p class="calibre8"><span class="strong"><img src="../images/00187.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">We will definitely select this model and move on to model optimization by hyperparameter tuning <a id="id447" class="calibre1"/>using a grid search algorithm as follows to optimize the cost and gamma parameters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">cost.weights &lt;- c(0.1, 10, 100)</strong></span>
<span class="strong"><strong class="calibre9">gamma.weights &lt;- c(0.01, 0.25, 0.5, 1)</strong></span>
<span class="strong"><strong class="calibre9">tuning.results &lt;- tune(svm, formula.new,</strong></span>
<span class="strong"><strong class="calibre9">                       data = train.data, kernel="Radial", </strong></span>
<span class="strong"><strong class="calibre9">                       ranges=list(cost=cost.weights, gamma=gamma.weights))</strong></span>
<span class="strong"><strong class="calibre9">print(tuning.results)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00188.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">The grid search <a id="id448" class="calibre1"/>plot can be viewed as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; plot(tuning.results, cex.main=0.6, cex.lab=0.8,xaxs="i", yaxs="i")</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><div class="mediaobject"><img src="../images/00189.jpeg" alt="Modeling using support vector machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The darkest region shows the parameter values which gave the best performance. We now select the <a id="id449" class="calibre1"/>best model and evaluate it once again as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; svm.model.best = tuning.results$best.model</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.predictions.best &lt;- predict(svm.model.best,</strong></span>
<span class="strong"><strong class="calibre9">                                  test.feature.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=svm.predictions.best, </strong></span>
<span class="strong"><strong class="calibre9">                  reference=test.class.var, positive="1")</strong></span>
</pre></div><p class="calibre8">On observing <a id="id450" class="calibre1"/>the confusion matrix results we obtained from the following output (we are henceforth depicting only the metrics which we <a id="id451" class="calibre1"/>are tracking), we see that the overall <span class="strong"><strong class="calibre9">accuracy</strong></span> has increased to <span class="strong"><strong class="calibre9">71%</strong></span>, <span class="strong"><strong class="calibre9">sensitivity</strong></span> to <span class="strong"><strong class="calibre9">86%</strong></span>, and <span class="strong"><strong class="calibre9">specificity</strong></span> to <span class="strong"><strong class="calibre9">41%</strong></span>, which is excellent compared to the previous model results:</p><p class="calibre8"><span class="strong"><img src="../images/00190.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">You see how powerful hyperparameter optimizations can be in predictive modeling! We also plot some evaluation curves as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; svm.predictions.best &lt;- predict(svm.model.best, test.feature.vars, decision.values = T)</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.prediction.values &lt;- attributes(svm.predictions.best)$decision.values</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(svm.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="SVM ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="SVM Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">We can see how the predictions are plotted in the evaluation space, and we see that the AUC in this case is 0.69 from the following ROC plot:</p><div class="mediaobject"><img src="../images/00191.jpeg" alt="Modeling using support vector machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Now, let's say we want to optimize the model based on this ROC plot with the objective of maximizing the <a id="id452" class="calibre1"/>AUC. We will try that now, but first we need to encode the values of the categorical variables to include some letters because R causes some problems when representing column names of factor variables that have <a id="id453" class="calibre1"/>only numbers. So basically, if <code class="email">credit.rating</code> has values <code class="email">0</code>, <code class="email">1</code> then it gets transformed to <span class="strong"><strong class="calibre9">X0</strong></span> and <span class="strong"><strong class="calibre9">X1</strong></span>; ultimately our categories are still distinct and nothing changes. We transform our data first with the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; transformed.train &lt;- train.data</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test &lt;- test.data</strong></span>
<span class="strong"><strong class="calibre9">&gt; for (variable in categorical.vars){</strong></span>
<span class="strong"><strong class="calibre9">+   new.train.var &lt;- make.names(train.data[[variable]])</strong></span>
<span class="strong"><strong class="calibre9">+   transformed.train[[variable]] &lt;- new.train.var</strong></span>
<span class="strong"><strong class="calibre9">+   new.test.var &lt;- make.names(test.data[[variable]])</strong></span>
<span class="strong"><strong class="calibre9">+   transformed.test[[variable]] &lt;- new.test.var</strong></span>
<span class="strong"><strong class="calibre9">+ }</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.train &lt;- to.factors(df=transformed.train, variables=categorical.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test &lt;- to.factors(df=transformed.test, variables=categorical.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test.feature.vars &lt;- transformed.test[,-1]</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test.class.var &lt;- transformed.test[,1]</strong></span>
</pre></div><p class="calibre8">Now we build an AUC optimized model using grid search again, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; grid &lt;- expand.grid(C=c(1,10,100), sigma=c(0.01, 0.05, 0.1, 0.5, </strong></span>
<span class="strong"><strong class="calibre9">                                             1))</strong></span>
<span class="strong"><strong class="calibre9">&gt; ctr &lt;- trainControl(method='cv', number=10, classProbs=TRUE,</strong></span>
<span class="strong"><strong class="calibre9">                      summaryFunction=twoClassSummary)</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.roc.model &lt;- train(formula.init, transformed.train,</strong></span>
<span class="strong"><strong class="calibre9">+                        method='svmRadial', trControl=ctr, </strong></span>
<span class="strong"><strong class="calibre9">+                        tuneGrid=grid, metric="ROC")</strong></span>
</pre></div><p class="calibre8">Our next step is to perform predictions on the test data and evaluate the confusion matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- predict(svm.roc.model, </strong></span>
<span class="strong"><strong class="calibre9">                         transformed.test.feature.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(predictions, transformed.test.class.var, </strong></span>
<span class="strong"><strong class="calibre9">                  positive = "X1")</strong></span>
</pre></div><p class="calibre8">This gives us the following results:</p><p class="calibre8"><span class="strong"><img src="../images/00192.jpeg" alt="Modeling using support vector machines" class="calibre16"/></span></p><p class="calibre8">We see now that <a id="id454" class="calibre1"/>
<span class="strong"><strong class="calibre9">accuracy</strong></span> has increased further <a id="id455" class="calibre1"/>to <span class="strong"><strong class="calibre9">72%</strong></span> and <span class="strong"><strong class="calibre9">specificity</strong></span> has decreased slightly to <span class="strong"><strong class="calibre9">40%</strong></span>, but <span class="strong"><strong class="calibre9">sensitivity</strong></span> has increased to <span class="strong"><strong class="calibre9">87%</strong></span>, which is good. We plot the curves once again, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; svm.predictions &lt;- predict(svm.roc.model, transformed.test.feature.vars, type="prob")</strong></span>
<span class="strong"><strong class="calibre9">&gt; svm.prediction.values &lt;- svm.predictions[,2]</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(svm.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="SVM ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="SVM Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">This gives us the following plots, the same as we did in our earlier iterations:</p><div class="mediaobject"><img src="../images/00193.jpeg" alt="Modeling using support vector machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">It is quite pleasing to see that the AUC has indeed increased from 0.69 earlier to 0.74 now, which means <a id="id456" class="calibre1"/>the AUC based optimization algorithm indeed worked, since it has given better performance than the previous model in <a id="id457" class="calibre1"/>all the aspects we have been tracking. Up next, we will look at how to build predictive models using decision trees.</p></div>
<div class="book" title="Modeling using decision trees"><div class="book" id="1JFUC2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec48" class="calibre1"/>Modeling using decision trees</h1></div></div></div><p class="calibre8">Decision trees <a id="id458" class="calibre1"/>are algorithms which again belong to the supervised machine learning algorithms family. They are also used for both classification and regression, often <a id="id459" class="calibre1"/>called <span class="strong"><strong class="calibre9">CART</strong></span>, which stands for <span class="strong"><strong class="calibre9">classification and regression trees</strong></span>. These are used a lot in decision support systems, business intelligence, and operations research.</p><p class="calibre8">Decision trees are <a id="id460" class="calibre1"/>mainly used for making decisions that would be most useful in reaching some objective and designing a strategy based on these decisions. At the core, a decision tree is just a flowchart with several nodes and conditional edges. Each non-leaf node represents a conditional test on one of the features and each edge represents an outcome of the test. Each leaf node represents a class label where predictions are made for the final outcome. Paths from the root to all the leaf nodes give us all the classification rules. Decision trees are easy to represent, construct, and understand. However, the drawback is that they are very prone to overfitting and often these models do not generalize well. We will follow a similar analytics pipeline as before, to build some models based on decision trees.</p><p class="calibre8">We start with loading the necessary dependencies and test data features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; library(rpart)# tree models </strong></span>
<span class="strong"><strong class="calibre9">&gt; library(caret) # feature selection</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(rpart.plot) # plot dtree</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(ROCR) # model evaluation</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(e1071) # tuning model</strong></span>
<span class="strong"><strong class="calibre9">&gt; source("performance_plot_utils.R") # plotting curves</strong></span>
<span class="strong"><strong class="calibre9">&gt; ## separate feature and class variables</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.feature.vars &lt;- test.data[,-1]</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.class.var &lt;- test.data[,1]</strong></span>
</pre></div><p class="calibre8">Now we will build an initial model with all the features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; dt.model &lt;- rpart(formula=formula.init, </strong></span>
<span class="strong"><strong class="calibre9">                    method="class",data=train.data,control = </strong></span>
<span class="strong"><strong class="calibre9">                           rpart.control(minsplit=20, cp=0.05))</strong></span>
</pre></div><p class="calibre8">We predict and evaluate the model on the test data with the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; dt.predictions &lt;- predict(dt.model, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                           type="class")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=dt.predictions, reference=test.class.var, </strong></span>
<span class="strong"><strong class="calibre9">                   positive="1")</strong></span>
</pre></div><p class="calibre8">From the <a id="id461" class="calibre1"/>following output, we see that the model <span class="strong"><strong class="calibre9">accuracy</strong></span> is around <span class="strong"><strong class="calibre9">68%</strong></span>, <span class="strong"><strong class="calibre9">sensitivity</strong></span> is <span class="strong"><strong class="calibre9">92%</strong></span>, which is excellent, but <span class="strong"><strong class="calibre9">specificity</strong></span> is only <span class="strong"><strong class="calibre9">18%</strong></span>, which <a id="id462" class="calibre1"/>we should try and improve:</p><p class="calibre8"><span class="strong"><img src="../images/00194.jpeg" alt="Modeling using decision trees" class="calibre16"/></span></p><p class="calibre8">We will now try feature selection to improve the model. We use the following code to train the model and rank the features by their importance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; control &lt;- trainControl(method="repeatedcv", number=10, repeats=2)</strong></span>
<span class="strong"><strong class="calibre9">&gt; model &lt;- train(formula.init, data=train.data, method="rpart", </strong></span>
<span class="strong"><strong class="calibre9">+                trControl=control)</strong></span>
<span class="strong"><strong class="calibre9">&gt; importance &lt;- varImp(model, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot(importance)</strong></span>
</pre></div><p class="calibre8">This gives <a id="id463" class="calibre1"/>us the following plot showing the importance of <a id="id464" class="calibre1"/>different features:</p><div class="mediaobject"><img src="../images/00195.jpeg" alt="Modeling using decision trees" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">If you observe closely, the decision tree does not use all the features in the model construction and the top five features are the same as those we obtained earlier when we talked about feature selection. We will now build a model using these features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- "credit.rating ~ account.balance + savings +</strong></span>
<span class="strong"><strong class="calibre9">                                  credit.amount +  </strong></span>
<span class="strong"><strong class="calibre9">                                  credit.duration.months + </strong></span>
<span class="strong"><strong class="calibre9">                                  previous.credit.payment.status"</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- as.formula(formula.new)</strong></span>
<span class="strong"><strong class="calibre9">&gt; dt.model.new &lt;- rpart(formula=formula.new, method="class",data=train.data, </strong></span>
<span class="strong"><strong class="calibre9">+                   control = rpart.control(minsplit=20, cp=0.05),</strong></span>
<span class="strong"><strong class="calibre9">+                   parms = list(prior = c(0.7, 0.3)))</strong></span>
</pre></div><p class="calibre8">We now make predictions on the test data and evaluate it, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; dt.predictions.new &lt;- predict(dt.model.new, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                                type="class")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=dt.predictions.new, </strong></span>
<span class="strong"><strong class="calibre9">                  reference=test.class.var, positive="1")</strong></span>
</pre></div><p class="calibre8">This gives us <a id="id465" class="calibre1"/>the following confusion matrix with other <a id="id466" class="calibre1"/>metrics:</p><p class="calibre8"><span class="strong"><img src="../images/00196.jpeg" alt="Modeling using decision trees" class="calibre16"/></span></p><p class="calibre8">You can see now that the overall model <span class="strong"><strong class="calibre9">accuracy</strong></span> has decreased a bit to <span class="strong"><strong class="calibre9">62%</strong></span>. However, we have increased our bad credit rating prediction where we predict a 100 bad credit rating customers out of 130, which is excellent! Consequently, <span class="strong"><strong class="calibre9">specificity</strong></span> has jumped up to <span class="strong"><strong class="calibre9">77%</strong></span> and <span class="strong"><strong class="calibre9">sensitivity</strong></span> is down to <span class="strong"><strong class="calibre9">55%</strong></span>, but we still classify a substantial number of good credit rating customers as good. Though this model is a bit aggressive, it is a reasonable model because though we deny credit loans to more customers who could default on the payment, we also make sure a reasonable number of good customers get their credit loans approved.</p><p class="calibre8">The reason we obtained these results is because we have built the model with a parameter called prior, if you check the modeling section earlier. This prior basically empowers us to apply weightages to the different classes in the class variable. If you remember, we had <span class="strong"><strong class="calibre9">700</strong></span> people with a <span class="strong"><strong class="calibre9">good credit rating</strong></span> and <span class="strong"><strong class="calibre9">300</strong></span> people with a <span class="strong"><strong class="calibre9">bad credit rating</strong></span> in our dataset, which was highly skewed, so while training the model, we can use prior to specify the importance of each of the classes in this variable and thus adjust the importance of misclassification of each class. In our model, we give more importance to the bad credit rating customers.</p><p class="calibre8">You can reverse the priors and give more importance to the good rating customers by using the parameter as <code class="email">prior = c(0.7, 0.3)</code>, which would give the following confusion matrix:</p><p class="calibre8"><span class="strong"><img src="../images/00197.jpeg" alt="Modeling using decision trees" class="calibre16"/></span></p><p class="calibre8">You can clearly <a id="id467" class="calibre1"/>see now that, since we gave more importance to good credit ratings, the <span class="strong"><strong class="calibre9">sensitivity</strong></span> has jumped up to <span class="strong"><strong class="calibre9">92%</strong></span> and <span class="strong"><strong class="calibre9">specificity</strong></span> has gone down to <span class="strong"><strong class="calibre9">18%</strong></span>. You can see that this gives you a lot of flexibility over your modeling depending on what you want to achieve.</p><p class="calibre8">To view the model, we <a id="id468" class="calibre1"/>can use the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; dt.model.best &lt;- dt.model.new</strong></span>
<span class="strong"><strong class="calibre9">&gt; print(dt.model.best)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00198.jpeg" alt="Modeling using decision trees" class="calibre16"/></span></p><p class="calibre8">To visualize the preceding tree, you can use the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,1))</strong></span>
<span class="strong"><strong class="calibre9">&gt; prp(dt.model.best, type=1, extra=3, varlen=0, faclen=0)</strong></span>
</pre></div><p class="calibre8">This gives us the following tree, and we can see that, using the priors, the only feature that is being used now out of the five features is <code class="email">account.balance</code> and it has ignored all the other features. You can try and optimize the model further by using hyperparameter tuning by exploring the <code class="email">tune.rpart</code> function from the <code class="email">e1071</code> package:</p><div class="mediaobject"><img src="../images/00199.jpeg" alt="Modeling using decision trees" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">We finish our <a id="id469" class="calibre1"/>analysis by plotting some metric evaluation curves <a id="id470" class="calibre1"/>as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; dt.predictions.best &lt;- predict(dt.model.best, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                                  type="prob")</strong></span>
<span class="strong"><strong class="calibre9">&gt; dt.prediction.values &lt;- dt.predictions.best[,2]</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(dt.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="DT ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="DT Precision/Recall </strong></span>
<span class="strong"><strong class="calibre9">                Curve")</strong></span>
</pre></div><p class="calibre8">The <span class="strong"><strong class="calibre9">AUC</strong></span> is around <span class="strong"><strong class="calibre9">0.66</strong></span>, which is not the best but definitely better than the baseline denoted by the red line in the following plot:</p><div class="mediaobject"><img src="../images/00200.jpeg" alt="Modeling using decision trees" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Based on our business requirements, this model is quite fair. We will discuss model comparison later <a id="id471" class="calibre1"/>on in this chapter. We will now use random forests <a id="id472" class="calibre1"/>to build our next set of predictive models.</p></div>
<div class="book" title="Modeling using random forests"><div class="book" id="1KEEU2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec49" class="calibre1"/>Modeling using random forests</h1></div></div></div><p class="calibre8">Random forests, also <a id="id473" class="calibre1"/>known as random decision forests, are a machine learning algorithm that comes from the family of ensemble learning <a id="id474" class="calibre1"/>algorithms. It is used for both regression and classification tasks. Random forests are nothing but a collection or ensemble of decision trees, hence the name.</p><p class="calibre8">The working of the algorithm can be described briefly as follows. At any point in time, each tree in the ensemble of decision trees is built from a bootstrap sample, which is basically sampling with replacement. This sampling is done on the training dataset. During the construction of the decision tree, the split which was earlier being chosen as the best split among all the features is not done anymore. Now the best split is always chosen from a random subset of the features each time. This introduction of randomness into the model increases the bias of the model slightly but decreases the variance of the model greatly which prevents the overfitting of models, which is a serious concern in the case of decision trees. Overall, this yields much better performing generalized models. We will now start our analytics pipeline by loading the necessary dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; library(randomForest) #rf model </strong></span>
<span class="strong"><strong class="calibre9">&gt; library(caret) # feature selection</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(e1071) # model tuning</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(ROCR) # model evaluation</strong></span>
<span class="strong"><strong class="calibre9">&gt; source("performance_plot_utils.R") # plot curves</strong></span>
<span class="strong"><strong class="calibre9">&gt; ## separate feature and class variables</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.feature.vars &lt;- test.data[,-1]</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.class.var &lt;- test.data[,1]</strong></span>
</pre></div><p class="calibre8">Next, we will build the initial training model with all the features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; rf.model &lt;- randomForest(formula.init, data = train.data, </strong></span>
<span class="strong"><strong class="calibre9">                           importance=T, proximity=T)</strong></span>
</pre></div><p class="calibre8">You can view the model details by using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; print(rf.model)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00201.jpeg" alt="Modeling using random forests" class="calibre16"/></span></p><p class="calibre8">This gives us <a id="id475" class="calibre1"/>information about the <span class="strong"><strong class="calibre9">out of bag error</strong></span> (<span class="strong"><strong class="calibre9">OOBE</strong></span>), which <a id="id476" class="calibre1"/>is around <span class="strong"><strong class="calibre9">23%</strong></span>, and the confusion matrix <a id="id477" class="calibre1"/>which is calculated on the training data, and also how many variables it is using at each split.</p><p class="calibre8">Next, we will perform predictions using this model on the test data and evaluate them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; rf.predictions &lt;- predict(rf.model, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                            type="class")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=rf.predictions, reference=test.class.var, </strong></span>
<span class="strong"><strong class="calibre9">                  positive="1")</strong></span>
</pre></div><p class="calibre8">The following output depicts that we get an overall <span class="strong"><strong class="calibre9">accuracy</strong></span> of <span class="strong"><strong class="calibre9">73%</strong></span>, <span class="strong"><strong class="calibre9">sensitivity</strong></span> of <span class="strong"><strong class="calibre9">91%</strong></span>, and <span class="strong"><strong class="calibre9">specificity</strong></span> of <span class="strong"><strong class="calibre9">36%</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00202.jpeg" alt="Modeling using random forests" class="calibre16"/></span></p><p class="calibre8">The initial model yields quite decent results. We see that a fair amount of bad credit rating customers are classified as bad and most of the good rating based customers are rated as good.</p><p class="calibre8">We will now build a new model with the top five features from the feature selection section, where we had used the random forest algorithm itself for getting the best features. The following code snippet builds the <a id="id478" class="calibre1"/>new model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">formula.new &lt;- "credit.rating ~ account.balance + savings +</strong></span>
<span class="strong"><strong class="calibre9">                                credit.amount +  </strong></span>
<span class="strong"><strong class="calibre9">                                credit.duration.months + </strong></span>
<span class="strong"><strong class="calibre9">                                previous.credit.payment.status"</strong></span>
<span class="strong"><strong class="calibre9">formula.new &lt;- as.formula(formula.new)</strong></span>
<span class="strong"><strong class="calibre9">rf.model.new &lt;- randomForest(formula.new, data = train.data, </strong></span>
<span class="strong"><strong class="calibre9">                         importance=T, proximity=T)</strong></span>
</pre></div><p class="calibre8">We now make predictions with this model on the test data and evaluate its performance as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; rf.predictions.new &lt;- predict(rf.model.new, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                                 type="class")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=rf.predictions.new,   reference=test.class.var, positive="1")</strong></span>
</pre></div><p class="calibre8">This gives us the <a id="id479" class="calibre1"/>following confusion matrix as the output with the other essential performance metrics:</p><p class="calibre8"><span class="strong"><img src="../images/00203.jpeg" alt="Modeling using random forests" class="calibre16"/></span></p><p class="calibre8">We get a slightly decreased <span class="strong"><strong class="calibre9">accuracy</strong></span> of <span class="strong"><strong class="calibre9">71%</strong></span>, which is obvious because we have eliminated many features, but now the <span class="strong"><strong class="calibre9">specificity</strong></span> has increased to <span class="strong"><strong class="calibre9">42%</strong></span>, which indicates it is able to classify more bad instances correctly as bad. <span class="strong"><strong class="calibre9">Sensitivity</strong></span> has decreased slightly to <span class="strong"><strong class="calibre9">84%</strong></span>. We will now use grid search to perform hyperparameter tuning on this model as follows, to see if <a id="id480" class="calibre1"/>we can improve the performance further. The <a id="id481" class="calibre1"/>parameters of interest here include <code class="email">ntree</code>, indicating the number of trees, <code class="email">nodesize</code>, indicating the minimum size of terminal nodes, and <code class="email">mtry</code>, indicating the number of variables sampled randomly at each split.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">nodesize.vals &lt;- c(2, 3, 4, 5)</strong></span>
<span class="strong"><strong class="calibre9">ntree.vals &lt;- c(200, 500, 1000, 2000)</strong></span>
<span class="strong"><strong class="calibre9">tuning.results &lt;- tune.randomForest(formula.new, </strong></span>
<span class="strong"><strong class="calibre9">                             data = train.data,</strong></span>
<span class="strong"><strong class="calibre9">                             mtry=3, </strong></span>
<span class="strong"><strong class="calibre9">                             nodesize=nodesize.vals,</strong></span>
<span class="strong"><strong class="calibre9">                             ntree=ntree.vals)</strong></span>
<span class="strong"><strong class="calibre9">print(tuning.results)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00204.jpeg" alt="Modeling using random forests" class="calibre16"/></span></p><p class="calibre8">We now get the <a id="id482" class="calibre1"/>best model from the preceding grid search, perform predictions on the test data, and evaluate its performance with the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; rf.model.best &lt;- tuning.results$best.model</strong></span>
<span class="strong"><strong class="calibre9">&gt; rf.predictions.best &lt;- predict(rf.model.best, test.feature.vars, </strong></span>
<span class="strong"><strong class="calibre9">                                 type="class")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=rf.predictions.best,</strong></span>
<span class="strong"><strong class="calibre9">                  reference=test.class.var, positive="1")</strong></span>
</pre></div><p class="calibre8">We can make several observations from the following output. Performance has improved very negligibly as the <a id="id483" class="calibre1"/>overall <span class="strong"><strong class="calibre9">accuracy</strong></span> remains the same at <span class="strong"><strong class="calibre9">71%</strong></span> and <span class="strong"><strong class="calibre9">specificity</strong></span> at <span class="strong"><strong class="calibre9">42%</strong></span>. <span class="strong"><strong class="calibre9">Sensitivity</strong></span> has increased slightly to <span class="strong"><strong class="calibre9">85%</strong></span> from <span class="strong"><strong class="calibre9">84%</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00205.jpeg" alt="Modeling using random forests" class="calibre16"/></span></p><p class="calibre8">We now plot some performance curves for this model, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; rf.predictions.best &lt;- predict(rf.model.best, test.feature.vars, type="prob")</strong></span>
<span class="strong"><strong class="calibre9">&gt; rf.prediction.values &lt;- rf.predictions.best[,2]</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(rf.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="RF ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="RF Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">We observe that the <a id="id484" class="calibre1"/>total <span class="strong"><strong class="calibre9">AUC</strong></span> is about <span class="strong"><strong class="calibre9">0.7</strong></span> and is much better than the red baseline <span class="strong"><strong class="calibre9">AUC</strong></span> of <span class="strong"><strong class="calibre9">0.5</strong></span> in the following plot:</p><div class="mediaobject"><img src="../images/00206.jpeg" alt="Modeling using random forests" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The last algorithm we will explore is neural networks and we will build our models using them in <a id="id485" class="calibre1"/>the following section.</p></div>
<div class="book" title="Modeling using neural networks"><div class="book" id="1LCVG2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec50" class="calibre1"/>Modeling using neural networks</h1></div></div></div><p class="calibre8">Neural networks, or <a id="id486" class="calibre1"/>to be more specific in this case, artificial neural <a id="id487" class="calibre1"/>networks, is a family of machine learning models whose concepts are based on the working of biological neural networks, just like our nervous system. Neural networks have been there for a long time, but recently there has been an upsurge of interest in building highly intelligent systems using deep learning and artificial intelligence. Deep learning makes use of deep neural networks, which are essentially neural networks with a huge number of hidden layers between the input and output layers. A typical neural network can be visualized with the following figure:</p><div class="mediaobject"><img src="../images/00207.jpeg" alt="Modeling using neural networks" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">From the figure, you can deduce that this neural network is an interconnected network of various nodes, also <a id="id488" class="calibre1"/>called neurons. Each node represents a <a id="id489" class="calibre1"/>neuron which is nothing but a mathematical function. It is impossible to go into every detail of how to represent a node mathematically but we will give the gist here. These mathematical functions receive one or more inputs with weights, which are represented in the preceding figure as edges, and then it performs some computation on these inputs to give an output. Various popular functions used in these nodes include step function and the sigmoid function, which you have already seen in use in the logistic regression algorithm. Once the inputs are weighed and transformed by the function, the activation of these functions is sent to further nodes until it reaches the output layer. A collection of nodes form a layer, just like in the earlier figure, we have three layers.</p><p class="calibre8">So, a neural network depends on several neurons or nodes and the pattern of interconnection between them, the learning process that is used to update the weights of the connections at each iteration (popularly called as epoch), and the activation functions of the nodes that convert the node's inputs with weights to its output activation, which is passed layer by layer till we get the output prediction. We will start with loading the necessary dependencies as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; library(caret) # nn models</strong></span>
<span class="strong"><strong class="calibre9">&gt; library(ROCR) # evaluate models</strong></span>
<span class="strong"><strong class="calibre9">&gt; source("performance_plot_utils.R") # plot curves</strong></span>
<span class="strong"><strong class="calibre9">&gt; # data transformation</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.feature.vars &lt;- test.data[,-1]</strong></span>
<span class="strong"><strong class="calibre9">&gt; test.class.var &lt;- test.data[,1]</strong></span>
</pre></div><p class="calibre8">We will now have <a id="id490" class="calibre1"/>to do some feature value encoding, similar to <a id="id491" class="calibre1"/>what we did when we did AUC optimization for SVM. To refresh your memory, you can run the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; transformed.train &lt;- train.data</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test &lt;- test.data</strong></span>
<span class="strong"><strong class="calibre9">&gt; for (variable in categorical.vars){</strong></span>
<span class="strong"><strong class="calibre9">+   new.train.var &lt;- make.names(train.data[[variable]])</strong></span>
<span class="strong"><strong class="calibre9">+   transformed.train[[variable]] &lt;- new.train.var</strong></span>
<span class="strong"><strong class="calibre9">+   new.test.var &lt;- make.names(test.data[[variable]])</strong></span>
<span class="strong"><strong class="calibre9">+   transformed.test[[variable]] &lt;- new.test.var</strong></span>
<span class="strong"><strong class="calibre9">+ }</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.train &lt;- to.factors(df=transformed.train, variables=categorical.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test &lt;- to.factors(df=transformed.test, variables=categorical.vars)</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test.feature.vars &lt;- transformed.test[,-1]</strong></span>
<span class="strong"><strong class="calibre9">&gt; transformed.test.class.var &lt;- transformed.test[,1]</strong></span>
</pre></div><p class="calibre8">Once we have our data ready, we will build our initial neural network model using all the features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; nn.model &lt;- train(formula.init, data = transformed.train, method="nnet")</strong></span>
</pre></div><p class="calibre8">The preceding code snippet might ask you to install the <code class="email">nnet</code> package if you do not have it installed, so just select the option when it asks you and it will install it automatically and build the model. If it fails, you can install it separately and run the code again. Remember, it is an iterative process so the model building might take some time. Once the model converges, you can view the model details using the <code class="email">print(nn.model)</code> command which will show several iteration results with different size and decay options, and you will see that it does hyperparameter tuning internally itself to try and get the best model!</p><p class="calibre8">We now perform predictions on the test data and evaluate the model performance as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; nn.predictions &lt;- predict(nn.model, </strong></span>
<span class="strong"><strong class="calibre9">                       transformed.test.feature.vars, type="raw")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=nn.predictions, </strong></span>
<span class="strong"><strong class="calibre9">                  reference=transformed.test.class.var, </strong></span>
<span class="strong"><strong class="calibre9">                  positive="X1")</strong></span>
</pre></div><p class="calibre8">You can observe <a id="id492" class="calibre1"/>from the following output that our model has an <a id="id493" class="calibre1"/>
<span class="strong"><strong class="calibre9">accuracy</strong></span> of <span class="strong"><strong class="calibre9">72%</strong></span>, which is quite good. It is predicting bad ratings as bad quite well, which is evident from the <span class="strong"><strong class="calibre9">specificity</strong></span> which is <span class="strong"><strong class="calibre9">48%</strong></span>, and as usual <span class="strong"><strong class="calibre9">sensitivity</strong></span> is good at <span class="strong"><strong class="calibre9">84%</strong></span>.</p><p class="calibre8"><span class="strong"><img src="../images/00208.jpeg" alt="Modeling using neural networks" class="calibre16"/></span></p><p class="calibre8">We will now use the following code snippet to plot the features of importance for neural network based models:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- "credit.rating ~ ."</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.init &lt;- as.formula(formula.init)</strong></span>
<span class="strong"><strong class="calibre9">&gt; control &lt;- trainControl(method="repeatedcv", number=10, repeats=2)</strong></span>
<span class="strong"><strong class="calibre9">&gt; model &lt;- train(formula.init, data=transformed.train, method="nnet", </strong></span>
<span class="strong"><strong class="calibre9">               trControl=control)</strong></span>
<span class="strong"><strong class="calibre9">&gt; importance &lt;- varImp(model, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot(importance)</strong></span>
</pre></div><p class="calibre8">This gives us the following plot ranking variables according to their importance:</p><div class="mediaobject"><img src="../images/00209.jpeg" alt="Modeling using neural networks" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">We select some of <a id="id494" class="calibre1"/>the most important features from the <a id="id495" class="calibre1"/>preceding plot and build our next model as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- "credit.rating ~ account.balance + credit.purpose + savings + current.assets +</strong></span>
<span class="strong"><strong class="calibre9">foreign.worker + previous.credit.payment.status"</strong></span>
<span class="strong"><strong class="calibre9">&gt; formula.new &lt;- as.formula(formula.new)</strong></span>
<span class="strong"><strong class="calibre9">&gt; nn.model.new &lt;- train(formula.new, data=transformed.train, method="nnet")</strong></span>
</pre></div><p class="calibre8">We now perform predictions on the test data and evaluate the model performance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; nn.predictions.new &lt;- predict(nn.model.new, </strong></span>
<span class="strong"><strong class="calibre9">                          transformed.test.feature.vars,  </strong></span>
<span class="strong"><strong class="calibre9">                          type="raw")</strong></span>
<span class="strong"><strong class="calibre9">&gt; confusionMatrix(data=nn.predictions.new, </strong></span>
<span class="strong"><strong class="calibre9">                  reference=transformed.test.class.var, </strong></span>
<span class="strong"><strong class="calibre9">                  positive="X1")</strong></span>
</pre></div><p class="calibre8">This gives us the following confusion matrix with various metrics of our interest. We observe from the following output that the <span class="strong"><strong class="calibre9">accuracy</strong></span> has increased slightly to <span class="strong"><strong class="calibre9">73%</strong></span> and <span class="strong"><strong class="calibre9">sensitivity</strong></span> has now increased to <span class="strong"><strong class="calibre9">87%</strong></span> at the cost of <span class="strong"><strong class="calibre9">specificity</strong></span>, which has dropped to <span class="strong"><strong class="calibre9">43%</strong></span>:</p><p class="calibre8"><span class="strong"><img src="../images/00210.jpeg" alt="Modeling using neural networks" class="calibre16"/></span></p><p class="calibre8">You can check the <a id="id496" class="calibre1"/>hyperparameter tuning which it has done <a id="id497" class="calibre1"/>internally, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; plot(nn.model.new, cex.lab=0.5)</strong></span>
</pre></div><p class="calibre8">The following plot shows the accuracy of the various models with different numbers of nodes in the hidden layer and the weight decay:</p><div class="mediaobject"><img src="../images/00211.jpeg" alt="Modeling using neural networks" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Based on our requirement that the bank makes minimum losses, we select the best model as the initial neural network model that was built, since it has an accuracy similar to the new model and its specificity is much higher which is extremely important. We now plot some performance curves for the best model as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt; nn.model.best &lt;- nn.model</strong></span>
<span class="strong"><strong class="calibre9">&gt; nn.predictions.best &lt;- predict(nn.model.best, transformed.test.feature.vars, type="prob")</strong></span>
<span class="strong"><strong class="calibre9">&gt; nn.prediction.values &lt;- nn.predictions.best[,2]</strong></span>
<span class="strong"><strong class="calibre9">&gt; predictions &lt;- prediction(nn.prediction.values, test.class.var)</strong></span>
<span class="strong"><strong class="calibre9">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.roc.curve(predictions, title.text="NN ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">&gt; plot.pr.curve(predictions, title.text="NN Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">We observe from the <a id="id498" class="calibre1"/>following plot that the <span class="strong"><strong class="calibre9">AUC</strong></span> is <span class="strong"><strong class="calibre9">0.74</strong></span>, which <a id="id499" class="calibre1"/>is quite good and performs a lot better than the baseline denoted in red:</p><div class="mediaobject"><img src="../images/00212.jpeg" alt="Modeling using neural networks" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">This concludes our predictive modeling session and we will wrap it up with model selection and comparisons.</p></div>
<div class="book" title="Model comparison and selection" id="1MBG21-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec51" class="calibre1"/>Model comparison and selection</h1></div></div></div><p class="calibre8">We have explored <a id="id500" class="calibre1"/>various machine learning techniques and built several models to predict the credit ratings of customers, so now comes the question of which model we should <a id="id501" class="calibre1"/>select and how the models compare against each other. Our test data has 130 instances of customers with a <span class="strong"><strong class="calibre9">bad credit rating</strong></span> (<span class="strong"><strong class="calibre9">0</strong></span>) and 270 customers with a <span class="strong"><strong class="calibre9">good credit rating</strong></span> (<span class="strong"><strong class="calibre9">1</strong></span>).</p><p class="calibre8">If you remember, earlier we had talked about using domain knowledge and business requirements after doing modeling to interpret results and make decisions. Right now, our decision is to choose the best model to maximize profits and minimize losses for the German bank. Let us consider the following conditions:</p><div class="book"><ul class="itemizedlist"><li class="listitem">If we incorrectly predict a customer with bad credit rating as good, the bank will end up losing the whole credit amount lent to him since he will default on the payment and so loss is 100%, which can be denoted as -1 for our ease of calculation.</li><li class="listitem">If we correctly predict a customer with bad credit rating as bad, we correctly deny him a credit loan and so there is neither any loss nor any profit.</li><li class="listitem">If we correctly predict a customer with good credit rating as good, we correctly give him the credit loan. Assuming the bank has an interest rate on the sum of money lent, let us assume the profit is 30% from the interest money that is paid back monthly by the customer. Therefore, profit is denoted as 30% or +0.3 for our ease of calculation.</li><li class="listitem">If we incorrectly <a id="id502" class="calibre1"/>predict a customer with good credit rating as bad, we incorrectly deny him the credit loan but there is neither any profit nor any loss involved in this case.</li></ul></div><p class="calibre8">Keeping these conditions in <a id="id503" class="calibre1"/>mind, we will make a comparison table for the various models, including some of the metrics we had calculated earlier for the best model for each machine learning algorithm. Remember that considering all the model performance metrics and business requirements, there is no one model that is the best among them all. Each model has its own set of good performance points, which is evident in the following analysis:</p><div class="mediaobject"><img src="../images/00213.jpeg" alt="Model comparison and selection" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The cells highlighted in the preceding table show the best performance for that particular metric. As we mentioned earlier, there is no best model and we have listed down the models that have performed best against each metric. Considering the total overall gain, decision tree seems to be the model of choice. However, this is assuming that the credit loan amount requested is constant per customer. Remember that if each customer requests loans of different amounts then this notion of total gain cannot be compared because then the profit <a id="id504" class="calibre1"/>from one loan might be different to another and the loss <a id="id505" class="calibre1"/>incurred might be different on different loans. This analysis is a bit complex and out of the scope of this chapter, but we will mention briefly how this can be computed. If you remember, there is a <code class="email">credit.amount</code> feature, which specifies the credit amount requested by the customer. Since we already have the customer numbers in the training data, we can aggregate the rated customers with their requested amount and sum up the ones for which losses and profits are incurred, and then we will get the total gain of the bank for each method!</p></div>
<div class="book" title="Summary" id="1NA0K1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec52" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">We explored several important areas in the world of supervised learning in this chapter. If you have followed this chapter from the beginning of our journey and braved your way till the end, give yourself a pat on the back! You now know what constitutes predictive analytics and some of the important concepts associated with it. Also, we have seen how predictive modeling works and the full predictive analytics pipeline in actual practice. This will enable you to build your own predictive models in the future and start deriving valuable insights from model predictions. We also saw how to actually use models to make predictions and evaluate these predictions to test model performance so that we can optimize the models further and then select the best model based on metrics as well and business requirements. Before we conclude and you start your own journey into predictive analytics, I will like to mention that you should always remember Occam's razor, which states that <span class="strong"><em class="calibre10">Among competing hypotheses, the one with the fewest assumptions should be selected,</em></span> which can be also interpreted as <span class="strong"><em class="calibre10">Sometimes, the simplest solution is the best one</em></span>. Do not blindly jump into building predictive models with the latest packages and techniques, because first you need to understand the problem you are solving and then start from the simplest implementation, which will often lead to better results than most complex solutions.</p></div></body></html>