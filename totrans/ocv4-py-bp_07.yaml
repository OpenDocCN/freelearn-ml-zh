- en: Learning to Recognize Traffic Signs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have previously studied how to describe objects by means of key points and
    features, and how to find the correspondence points in two different images of
    the same physical object. However, our previous approaches were rather limited
    when it came to recognizing objects in real-world settings and assigning them
    to conceptual categories. For example, in [Chapter 2](0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml), *Hand
    Gesture Recognition Using a Kinect Depth Sensor*, the required object in the image
    was a hand, and it had to be nicely placed in the center of the screen. Wouldn't
    it be nice if we could remove these restrictions?
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this chapter is to train a **multiclass** **classifier** to recognize
    traffic signs. In this chapter, we will cover the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Briefing on supervised learning concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the **German Traffic Sign Recognition** **Benchmark** (**GTSRB**)
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about dataset feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about **support vector machines** (**SVMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving results with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to apply machine learning models to real-world
    problems. You will learn how to use already available datasets for training models.
    You will also learn how to use SVMs for multiclass classification and how to train,
    test, and improve machine learning algorithms provided with OpenCV to achieve
    real-world tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will train an SVM to recognize all sorts of traffic signs. Although SVMs
    are binary classifiers (that is, they can be used to learn, at most, two categories—positives
    and negatives, animals and non-animals, and so on), they can be extended to be
    used in multiclass classification. In order to achieve good classification performance,
    we will explore a number of color spaces, as well as the **Histogram of Oriented
    Gradients** (**HOG**) feature. The end result will be a classifier that can distinguish more
    than 40 different signs from the dataset, with very high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the basics of machine learning will be very useful for the future when
    you would like to make your vision-related applications even smarter. This chapter
    will teach you the basics of machine learning, on which the following chapters
    will build.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GTSRB dataset can be freely obtained from [http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) (see
    the *Dataset attribution* section for attribution details).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code that we present in this chapter at our GitHub repository: [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7).
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To arrive at such a multiclass classifier (that can differentiate between more
    than 40 different signs from the dataset), we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocess the dataset**: We need a way to load our dataset, extract the
    regions of interest, and split the data into appropriate training and test sets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Extract features**: Chances are that raw pixel values are not the most informative
    representation of the data. We need a way to extract meaningful features from
    the data, such as features based on different color spaces and HOG.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the classifier**: We will train the multiclass classifier on the training
    data using a* one-versus-all* strategy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Score the classifier**: We will evaluate the quality of the trained ensemble
    classifier by calculating different performance metrics, such as **accuracy**, **precision**,
    and **recall**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss all these steps in detail in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final app will parse a dataset, train the ensemble classifier, assess its
    classification performance, and visualize the result. This will require the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main`: The main function routine (in `chapter7.py`) is required for starting
    the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasets.gtsrb`: This is a script for parsing the GTSRB dataset. This script
    contains the following functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_data`: This function is used to load the GTSRB dataset, extract a feature
    of choice, and split the data into training and test sets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*_featurize`, `hog_featurize`: These functions are passed to `load_data` for
    extracting a feature of choice from the dataset. Example functions are as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gray_featurize`: This is a function that creates features based on grayscale
    pixel values.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`surf_featurize`: This is a function that creates features based on **Speeded-Up-Robust
    Features** (**SURF**).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification performance will be judged based on accuracy, precision,
    and recall. The following sections will explain all of these terms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Briefing on supervised learning concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important subfield of machine learning is **supervised learning**. In supervised
    learning, we try to learn from a set of labeled data—that is, every data sample
    has a desired target value or true output value. These target values could correspond
    to the continuous output of a function (such as `y` in `y = sin(x)`), or to more
    abstract and discrete categories (such as *cat* or *dog*).
  prefs: []
  type: TYPE_NORMAL
- en: A supervised learning algorithm uses the already labeled training data, analyzes
    it, and produces a mapping inferred function from features to a label, which can
    be used for mapping new examples. Ideally, the inferred algorithm will generalize
    well and give correct target values for new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We divide supervised learning tasks into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: If we are dealing with continuous output (for example, the probability of rain),
    the process is called **regression**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are dealing with discrete output (for example, species of an animal),
    the process is called **classification**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we focus on the classification problem of labeling images of
    the GTSRB dataset, and we will use an algorithm called SVM to infer a mapping
    function between images and their labels.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first understand how machine learning gives *machines* the ability to *learn** like
    humans*. Here is a hint—we train them.
  prefs: []
  type: TYPE_NORMAL
- en: The training procedure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, we may want to learn what cats and dogs look like. To make this
    a supervised learning task, first, we have to put it as a question that has either
    a categorical answer or a real-valued answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some example questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Which animal is shown in the given picture?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a cat in the picture?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a dog in the picture?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we have to gather an example picture with its corresponding correct
    answer—**training data**.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have to pick a learning algorithm (**learner**) and start tweaking
    its parameters in some way (**learning algorithm**) so that the learner can tell
    the correct answers when presented with a datum from training data.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process until we are satisfied with the learner's performance
    or **score** (which could be **accuracy**, **precision**, or some other **cost
    function**) on the training data. If we are not satisfied, we change the parameters
    of the learner in order to improve the score over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure is outlined in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58a3b6a3-86fc-4fb9-a486-bed5734c4562.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous screenshot, Training data is represented by a set of Features.
    For real-life classification tasks, these features are rarely the raw pixel values
    of an image, since these tend not to represent the data well. Often, the process
    of finding the features that best describe the data is an essential part of the
    entire learning task (also referred to as **feature selection** or **feature engineering**).
  prefs: []
  type: TYPE_NORMAL
- en: That is why it is always a good idea to deeply study the statistics and appearances
    of the training set that you are working with before even thinking about setting
    up a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: As you are probably aware, there is an entire zoo of learners, cost functions,
    and learning algorithms out there. These make up the core of the learning procedure.
    The learner (for example, a linear classifier or SVM) defines how input features
    are converted into a score function (for example, mean-squared error), whereas
    the Learning algorithm (for example, gradient descent) defines how the parameters
    of the learner are changed over time.
  prefs: []
  type: TYPE_NORMAL
- en: The training procedure in a classification task can also be thought of as finding
    an appropriate **decision boundary**, which is a line that best partitions the
    training set into two subsets, one for each class. For example, consider training
    samples with only two features (x and y values) and a corresponding class label
    (positive (**+**), or negative (**–**)).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of the training procedure, the classifier tries to draw a
    line to separate all positives from all negatives. As the training progresses,
    the classifier sees more and more data samples. These are used to update the decision
    boundary, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88288338-4a20-42f3-a19b-8dfd07d3edfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to this simple illustration, an SVM tries to find the optimal decision
    boundary in a high-dimensional space, so the decision boundary can be more complex
    than a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: We now move on to understand the testing procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The testing procedure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for a trained classifier to be of any practical value, we need to know
    how it performs when applied to a data sample (also called **generalization**)
    that has never been seen before. To stick to our example shown earlier, we want
    to know which class the classifier predicts when we present it with a previously
    unseen picture of a cat or a dog.
  prefs: []
  type: TYPE_NORMAL
- en: 'More generally speaking, we want to know which class the ? sign, in the following
    screenshot, corresponds to, based on the decision boundary we learned during the
    training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d225b9d-2003-4164-9df6-cc599c2d1bc6.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, you can see why this is a tricky problem. If
    the location of the question mark (?) were more to the left, we would be certain
    that the corresponding class label is +.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in this case, there are several ways to draw the decision boundary
    such that all the + signs are to the left of it and all the – signs are to the
    right of it, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6274c099-983b-4f31-b9b0-bbdbb196f24d.png)'
  prefs: []
  type: TYPE_IMG
- en: The label of **?** thus depends on the exact decision boundary that was derived
    during training. If the **?** sign in the preceding screenshot is actually a **–**
    sign, then only one decision boundary (the leftmost) would get the correct answer.
    A common problem is that training can result in a decision boundary that works
    *too well* on the training set (also known as **overfitting**) but also makes
    a lot of mistakes when applied to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, it is likely that the learner imprinted details that are specific
    to the training set on the decision boundary, instead of revealing general properties
    about the data that might also be true for unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique for reducing the effect of overfitting is called **regularization**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Long story short: the problem always comes back to finding the boundary that
    best splits not only the training set but also the test set. That is why the most
    important metric for a classifier is its generalization performance (that is,
    how well it classifies data not seen in the training phase).'
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply our classifier to traffic-sign recognition, we need a suitable
    dataset. A good choice might be the GTSRB dataset. Let us learn about it next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the GTSRB dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GTSRB dataset contains more than 50,000 images of traffic signs belonging
    to 43 classes.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was used by professionals in a classification challenge during
    the **International Joint Conference on Neural Networks** (**IJCNN**) in 2011\.
    The GTSRB dataset is perfect for our purposes because it is large, organized,
    open source, and annotated.
  prefs: []
  type: TYPE_NORMAL
- en: Although the actual traffic sign is not necessarily a square or is in the center
    of each image, the dataset comes with an annotation file that specifies the bounding
    boxes for each sign.
  prefs: []
  type: TYPE_NORMAL
- en: A good idea before doing any sort of machine learning is usually to get a feel
    of the dataset, its qualities, and its challenges. Some good ideas include manually
    going through the data and understanding what are some characteristics of it,
    reading a data description—if it's available on the page—to understand which models
    might work best, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we present a snippet from `data/gtsrb.py` that loads and then plots a
    random-15 sample of the training dataset, and does that `100` times, so you can
    paginate through the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Another good strategy would be to plot 15 samples from each of the 43 classes
    and see how images change for the given class. The following screenshot shows
    some examples of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f166f8b5-f216-4a84-ac1d-dcff00181016.png)'
  prefs: []
  type: TYPE_IMG
- en: Even from this small data sample, it is immediately clear that this is a challenging
    dataset for any sort of classifier. The appearance of the signs changes drastically
    based on viewing angle (orientation), viewing distance (blurriness), and lighting
    conditions (shadows and bright spots).
  prefs: []
  type: TYPE_NORMAL
- en: For some of these signs—such as the second sign of the third row—it is difficult,
    even for humans (at least for me), to tell the correct class label right away.
    It's a good thing we are aspiring experts in machine learning!
  prefs: []
  type: TYPE_NORMAL
- en: Let's now learn to parse the dataset in order to convert to a format suitable
    for the SVM to use for training.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GTSRB dataset has 21 files that we can download. We choose to work with
    the raw data to make it more educational and download the official training data—**Images
    and annotations** (`GTSRB_Final_Training_Images.zip`) for training, and the official
    training dataset that was used at the **IJCNN** **2011 competition**—**Images
    and annotations** (`GTSRB-Training_fixed.zip`) for scoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the files from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fa8121b-2ea6-41e7-8b5a-14ffa222433b.png)'
  prefs: []
  type: TYPE_IMG
- en: We chose to download the train and test data separately instead of constructing
    our own train/test data from one of the datasets because, after exploring the
    data, there are usually 30 images of the same sign from different distances that
    look very much alike. Putting these 30 images in different datasets will skew
    the problem and lead to great results, even though our model might not generalize
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is a function that downloads the data from the **University
    of Copenhagen Data Archive**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The previous code takes a filename (you can see the files and their names from
    the previous screenshot) and checks if the file already exists or not (and checks
    whether the `md5sum` matches or not, if provided). This saves a lot of bandwidth
    and time by not having to download the files again and again. Then, it downloads
    the file and stores it in the same directory as the file that contains the code.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation format can be viewed at [http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat).
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have downloaded the file, we write a function that unzips and extracts
    the data using the annotation format provided with the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we open the downloaded `.zip` file (this could be either the training
    or test data), and we iterate over all the files and only open `.csv` files, which
    contain the target information of each image in the corresponding class. This
    is shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check if the label of the image is in the `labels` array that we are
    interested in. Then, we create a `csv.reader` that we will use to iterate over
    the `.csv` file contents, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Every line of the file contains the annotation for one data sample. So, we
    extract the image path, read the data, and convert it to a NumPy array. Usually,
    the object in these samples is not perfectly cut out but is embedded in its surroundings.
    We cut the image using the boundary-box information provided in the archive, using
    a `.csv` file for each of the labels. In the following code, we add the sign to
    `data` and add the label to `targets`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Often, it is desirable to perform some form of feature extraction, because raw
    image data is rarely the best description of the data. We will defer this job
    to another function, which we will discuss in detail later.
  prefs: []
  type: TYPE_NORMAL
- en: 'As pointed out in the previous subsection, it is imperative to separate the
    samples that we use to train our classifier from the samples that we use to test
    it. For this, the following code snippet shows us that we have two different functions
    that download training and testing data and load them into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to convert images into NumPy matrices, we can go on to
    more interesting parts, namely, we can feed the data to the SVM and train it to
    make predictions. So, let's move on to the next section, which covers feature
    extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about dataset feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chances are that raw pixel values are not the most informative way to represent
    the data, as we have already realized in [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*. Instead, we
    need to derive a measurable property of the data that is more informative for
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: However, often, it is not clear which features would perform best. Instead,
    it is often necessary to experiment with different features that the practitioner
    finds appropriate. After all, the choice of features might strongly depend on
    the specific dataset to be analyzed or the specific classification task to be
    performed.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have to distinguish between a stop sign and a warning sign,
    then the most distinctive feature might be the shape of the sign or the color
    scheme. However, if you have to distinguish between two warning signs, then color
    and shape will not help you at all, and you will be required to come up with more
    sophisticated features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to demonstrate how the choice of features affects classification performance,
    we will focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A few simple color transformations** (such as grayscale; **red, green, blue**
    (**RGB**); and **hue, saturation, value** (**HSV**): Classification based on grayscale
    images will give us some baseline performance for the classifier. RGB might give
    us slightly better performance because of the distinct color schemes of some traffic
    signs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even better performance is expected from HSV. This is because it represents
    colors even more robustly than RGB. Traffic signs tend to have very bright, saturated
    colors that (ideally) are quite distinct from their surroundings.
  prefs: []
  type: TYPE_NORMAL
- en: '**SURF**: This should appear very familiar to you by now. We have previously
    recognized SURF as an efficient and robust method of extracting meaningful features
    from an image. So, can''t we use this technique to our advantage in a classification
    task?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HOG**: This is by far the most advanced feature descriptor to be considered
    in this chapter. The technique counts occurrences of gradient orientations along
    a dense grid laid out on the image and is well suited for use with SVMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction is performed by functions in the `data/process.py` file,
    from which we will call different functions to construct and compare different
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a nice blueprint, which—if you follow it—will enable you to easily
    write your own featurization functions and use with our code, and compare if your
    `your_featurize` function will yield better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `*_featurize` functions take a list of images and return a matrix (as a
    2D `np.ndarray`), where each row is a new sample and each column represents a
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: For most of the following features, we will be using the (already suitable)
    default arguments in OpenCV. However, these values are not set in stone, and,
    even in real-world classification tasks, it is often necessary to search across
    the range of possible values for both features extracting and feature learning
    parameters in a process called **hyperparameter exploration**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what we are doing, let's take a look at some featurization
    functions that we have come up with that build on top of concepts from previous
    sections and also add some new concepts as well.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding common preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we look at what we have come up with, let's take our time to look at
    the two most common forms of preprocessing that are almost always applied to any
    data before machine learning tasks—namely, **mean subtraction** and **normalization**.
  prefs: []
  type: TYPE_NORMAL
- en: Mean subtraction is the most common form of preprocessing (sometimes also referred
    to as **zero centering** or de-meaning), where the mean value of every feature
    dimension is calculated across all samples in a dataset. This feature-wise average
    is then subtracted from every sample in the dataset. You can think of this process
    as centering the *cloud* of data on the origin.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization refers to the scaling of data dimensions so that they are of roughly
    the same scale. This can be achieved by either dividing each dimension by its
    standard deviation (once it has been zero-centered) or scaling each dimension
    to lie in the range of [-1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to apply this step only if you have reason to believe that different
    input features have different scales or units. In the case of images, the relative
    scales of pixels are already approximately equal (and in the range of [0, 255]),
    so it is not strictly necessary to perform this additional preprocessing step.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with these two concepts, let's take a look at our feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about grayscale features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest feature to extract is probably the grayscale value of each pixel.
    Usually, grayscale values are not very indicative of the data they describe, but
    we will include them here for illustrative purposes (that is, to achieve baseline
    performance).
  prefs: []
  type: TYPE_NORMAL
- en: 'For each image in the input set, we are going to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resize all images to have the same (usually smaller) size. We use `scale_size=(32,
    32)` to make sure we don''t make the images too small. At the same time, we want
    our data to be small enough to work on our personal computer. We can do this with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the image to grayscale (values are still in 0-255 range), like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert each image to have the pixel value in (0, 1) and flatten, so instead
    of a matrix of `(32, 32)` size for each image, we have a vector of size `1024`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Subtract the average pixel value of the flattened vector, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We use the returned matrix as our training data for the machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at another example—*what would happen if we used information
    in the colors as well?*
  prefs: []
  type: TYPE_NORMAL
- en: Understanding color spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alternatively, you might find that colors contain some information that raw
    grayscale values cannot capture. Traffic signs often have a distinct color scheme,
    and it might be indicative of the information it is trying to convey (that is,
    red for stop signs and forbidden actions; green for informational signs; and so
    on). We could opt to use the RGB images as input, but, in our case, we do not
    have to do anything since the dataset is already RGB.
  prefs: []
  type: TYPE_NORMAL
- en: However, even RGB might not be informative enough. For example, a stop sign
    in broad daylight might appear very bright and clear, but its colors might appear
    much less vibrant on a rainy or foggy day. A better choice might be the HSV color
    space, which represents colors using hue, saturation, and value (or brightness).
  prefs: []
  type: TYPE_NORMAL
- en: The most telling feature of traffic signs in this color space might be the hue
    (a more perceptually relevant description of color or chromaticity), provides
    an improved ability to distinguish between the color scheme of different sign
    types. Saturation and value could be equally important, however, as traffic signs
    tend to use relatively bright and saturated colors that do not typically appear
    in natural scenes (that is, their surroundings).
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenCV, the HSV color space is only a single call to `cv2.cvtColor` away,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, to summarize, featurization is almost the same as for grayscale features.
    For each image, we carry out the following four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Resize all images to have the same (usually smaller) size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the image to HSV (values in the 0-255 range).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert each image to have the pixel value in (0, 1), and flatten it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the average pixel value of the flattened vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's try to look at a more complex example of a feature extractor that
    uses SURF.
  prefs: []
  type: TYPE_NORMAL
- en: Using SURF descriptor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But wait a minute! In [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*, you learned
    that the SURF descriptor is one of the best and most robust ways to describe images
    independent of scale or rotations. Can we use this technique to our advantage
    in a classification task?
  prefs: []
  type: TYPE_NORMAL
- en: Glad you asked! To make this work, we need to adjust SURF so that it returns
    a fixed number of features per image. By default, the SURF descriptor is only
    applied to a small list of *interesting* key points in the image, the number of
    which might differ on an image-by-image basis. This is unsuitable for our current
    purposes because we want to find a fixed number of feature values per data sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we need to apply SURF to a fixed dense grid laid out over the image,
    for which we create a key points array containing all pixels, as illustrated in
    the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it is possible to obtain SURF descriptors for each point on the grid
    and append that data sample to our feature matrix. We initialize SURF with a `hessianThreshold`
    value of `400`, as we did before, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The key points and descriptors can then be obtained via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Because `surf.compute` has two output arguments, `kp_des` will actually be a
    concatenation of both key points and descriptors. The second element in the `kp_des`
    array is the descriptor that we care about.
  prefs: []
  type: TYPE_NORMAL
- en: 'We select the first `num_surf_features` from each data sample and return it
    as a feature for the image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's take a look at a new concept that is very popular in the community—HOG.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping HOG descriptor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last feature descriptor to consider is the HOG. HOG features have previously
    been shown to work exceptionally well in combination with SVMs, especially when
    applied to tasks such as pedestrian recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The essential idea behind HOG features is that the local shapes and appearance
    of objects within an image can be described by the distribution of edge directions.
    The image is divided into small connected regions, within which a histogram of
    gradient directions (or edge directions) is compiled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows such a histogram from a region in a picture.
    Angles are not directional; that''s why the range is (-180, 180):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/169da355-a9f3-4b2c-a7cf-551e233b1e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it has a lot of edge directions in the horizontal direction
    (angles around +180 and -180 degrees), so this seems like a good feature, especially
    when we are working with arrows and lines.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the descriptor is assembled by concatenating the different histograms.
    For improved performance, the local histograms can be contrast normalized, which
    results in better invariance to changes in illumination and shadowing. You can
    see why this sort of preprocessing might be just the perfect fit for recognizing
    traffic signs under different viewing angles and lighting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HOG descriptor is fairly accessible in OpenCV by means of `cv2.HOGDescriptor`,
    which takes the detection window size (32 x 32), the block size (16 x 16), the
    cell size (8 x 8), and the cell stride (8 x 8) as input arguments. For each of
    these cells, the HOG descriptor then calculates a HOG using nine bins, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Applying the HOG descriptor to every data sample is then as easy as calling
    `hog.compute`.
  prefs: []
  type: TYPE_NORMAL
- en: After we have extracted all the features we want, we return a flattened list
    for each of the images.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are finally ready to train the classifier on the preprocessed dataset.
    So, let's move on to the SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An SVM is a learner for binary classification (and regression) that tries to
    separate examples from the two different class labels with a decision boundary
    that maximizes the margin between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to our example of positive and negative data samples, each of
    which has exactly two features (x and y) and two possible decision boundaries,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/451b4435-7bdd-4dd9-b9b5-105450095b86.png)'
  prefs: []
  type: TYPE_IMG
- en: Both of these decision boundaries get the job done. They partition all the samples
    of positives and negatives with zero misclassifications. However, one of them
    seems intuitively better. How can we quantify *better* and thus learn the *best* parameter
    settings?
  prefs: []
  type: TYPE_NORMAL
- en: This is where SVMs come into the picture. SVMs are also called **maximal margin
    classifiers** because they can be used to do exactly that—define the decision
    boundary so as to make those two clouds of + and - as far apart as possible; that
    is, as far apart from the decision boundary as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the preceding example, an SVM would find two parallel lines that pass through
    the data points on the class margins (the *dashed lines* in the following screenshot),
    and then make the line (that passes through the center of the margins) the decision
    boundary (the *bold black line* in the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e826ff4-c743-4082-8bb1-73cebd9ae244.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that to find the maximal margin, it is only important to consider
    the data points that lie on the class margins. These points are sometimes also
    called **support vectors**.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to performing linear classification (that is, when the decision
    boundary is a straight line), SVMs can also perform a non-linear classification
    using what is called the **kernel trick**, implicitly mapping their input to high-dimensional
    feature spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how we can turn this binary classifier into a multiclass
    classifier that is more appropriate for the 43-class classification problem we
    are trying to tackle.
  prefs: []
  type: TYPE_NORMAL
- en: Using SVMs for multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas some classification algorithms, such as neural networks, naturally lend
    themselves to using more than two classes, SVMs are binary classifiers by nature.
    They can, however, be turned into multiclass classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will consider two different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-versus-all**: The *one-versus-all* strategy involves training a single
    classifier per class, with the samples of that class as positive samples and all
    other samples as negatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `k` classes, this strategy thus requires the training of `k` number
    of different SVMs. During testing, all classifiers can express a *+1* vote by
    predicting that an unseen sample belongs to their class.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, an unseen sample is classified by the ensemble as the class with
    the most votes. Usually, this strategy is used in combination with confidence
    scores instead of predicted labels so that, in the end, the class with the highest
    confidence score can be picked.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-versus-one**: The *one-versus-one* strategy involves training a single
    classifier per class pair, with the samples of the first class as positive samples
    and the samples of the second class as negative samples. For the `k` classes,
    this strategy requires the training of `k*(k-1)/2` classifiers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the classifiers have to solve a significantly easier task, so there
    is a trade-off when considering which strategy to use. During testing, all classifiers
    can express a *+1* vote for either the first or the second class. In the end,
    an unseen sample is classified by the ensemble as the class with the most votes.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you would not have to write your own classification algorithms unless
    you really wanted to dive deep into the algorithms and squeeze the last bit of
    performance out of your model. And luckily, OpenCV already comes with a good machine
    learning toolkit that we will use in this chapter. OpenCV uses a one-versus-all
    approach, and we will focus on that approach.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get our hands dirty, and see how we can code this up with OpenCV
    and get some real results.
  prefs: []
  type: TYPE_NORMAL
- en: Training the SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to write the training method in a separate function; it''s a good
    practice if we later wanted to change our training method. First, we define the
    signature of our function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we want a function that takes two arguments—`training_features` and `training_labels`—and
    the correct answers corresponding to each feature. Thus, the first argument will
    be a matrix in the form of a two-dimensional NumPy array, and the second argument
    will be a one-dimensional NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the function will return an object that should have a `predict` method,
    which takes new unseen data and labels it. So, let's get started and see how we
    could train an SVM with OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'We name our function `train_one_vs_all_SVM`, and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate an SVM class instance using `cv2.ml.SVM_create`, which creates
    a multiclass SVM using the one-versus-all strategy, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the hyperparameters of the learner. These are called **hyperparameters**
    because these parameters are out of the control of the learner (versus parameters
    that the learner changes during the learning process). This can be done with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `train` method on the SVM instance, and OpenCV takes care of training
    (this takes a couple of minutes on a regular laptop computer with the GTSRB dataset),
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV will take care of the rest. What happens under the hood is that the SVM
    training uses **Lagrange multipliers** to optimize some constraints that lead
    to the maximum margin decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization process is usually performed until some termination criteria
    are met, which can be specified via the SVM's optional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at training the SVM, let's look at testing it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to evaluate a classifier, but most often, we are simply
    interested in the accuracy metric—that is, how many data samples from the test
    set were classified correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to arrive at this metric, we need to get the prediction results out
    of the SVM—and again, OpenCV has us covered, by providing the `predict` method
    that takes a matrix of features and returns an array of predicted labels. We thus
    need to proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have to first featurize our testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we feed the featurized data to the classifier and get the predicted labels,
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can try to see how many of the labels the classifier got correctly,
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to calculate the desired performance metrics, as described
    in detail in later sections. For the purpose of this chapter, we choose to calculate accuracy,
    precision, and recall.
  prefs: []
  type: TYPE_NORMAL
- en: The `scikit-learn` machine learning package (which can be found at [http://scikit-learn.org](http://scikit-learn.org))
    supports the three metrics—which are accuracy, precision, and recall (as well
    as others)—straight out of the box, and also comes with a variety of other useful
    tools. For educational purposes (and to minimize software dependencies), we will
    derive the three metrics ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most straightforward metric to calculate is probably accuracy. This metric
    simply counts the number of test samples that have been predicted correctly, and
    returns the number as a fraction of the total number of test samples, as shown
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The previous code shows that we have extracted `y_predicted` by calling `model.predict(x_test)`.
    This was quite simple, but, again, to make things reusable, we put this inside
    a function that takes `predicted` and `true` labels. And now, we will go on to
    implement slightly more complicated metrics that are useful to measure classifier
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix is a 2D matrix of size equal to `(num_classes, num_classes)`,
    where the rows correspond to the predicted class labels, and the columns correspond
    to the actual class labels. Then, the `[r,c]` matrix element contains the number
    of samples that were predicted to have label `r`, but in reality, have label `c`.
    Having access to a confusion matrix will allow us to calculate precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement a very simple way to calculate the confusion matrix.
    Similar to accuracy, we create a function with the same arguments, so it''s easy
    to reuse, by following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming our labels are non-negative integers, we can figure out `num_classes`
    by taking the highest integer and adding `1` to account for zero, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate an empty matrix, where we will fill the counts, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we iterate over all data, and, for each datum, we take predicted value
    `r` and actual value `c`, and we increment the appropriate value in the matrix.
    There are much faster ways to achieve this, but nothing is simpler than counting
    everything one by one. We do this with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have accounted for all the data in the training set, we can return
    our confusion matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our confusion matrix for the GTSRB dataset test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d1b2e14e-a3dc-4457-91b2-dad492d830fc.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, most of the values are in the diagonal. This means that at first
    glance, our classifier is doing pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also easy to calculate the accuracy from the confusion matrix as well.
    We just take the number of elements in the diagonal, and divide by the number
    of elements overall, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have a different number of elements in each of the classes. Each
    class contributes to accuracy differently, and our next metric will focus on per-class
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision in binary classification is a useful metric for measuring the fraction
    of retrieved instances that are relevant (also called the **positive predictive
    value**). In a classification task, the number of **true positives** is defined
    as the number of items correctly labeled as belonging to the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Precision is defined as the number of true positives divided by the total number
    of positives. In other words, out of all the pictures in the test set that a classifier
    thinks to contain a cat, precision is the fraction of pictures that actually do
    contain a cat.
  prefs: []
  type: TYPE_NORMAL
- en: Note that here, we have a positive label; thus, precision is a per-class value.
    We usually talk about the precision of one class or the precision of cats, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of positives can also be calculated as the sum of **true positives**
    and **false positives**, the latter being the number of samples incorrectly labeled
    as belonging to a particular class. This is where the confusion matrix comes in
    handy because it will allow us to quickly calculate the number of false positives
    and true positives by following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case, we have to change our function arguments, and add the positive
    class label, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use our confusion matrix, and calculate the number of true positives,
    which will be the element at `[positive_label, positive_label]`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s calculate the number of true and false positives, which will be
    the sum of all elements on the `positive_label` row since the row indicates the
    predicted class label, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, return the ratio of true positives and all positives, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on different classes, we get very different values of precision. Here
    is a histogram of the precision scores for all 43 classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9988c2b9-ae74-42c6-8586-54db08e2b2c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The class with lower precision is 30, which means that a lot of other signs
    are mistaken to be the sign shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3c3138f-b7f6-4796-97a5-82469fb203be.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, it's alright if we are extra cautious while driving on the icy
    road, but it's possible that we missed something important. So, let's look at
    recall values for different classes.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall is similar to precision in the sense that it measures the fraction of
    relevant instances that are retrieved (as opposed to the fraction of retrieved
    instances that are relevant). Thus, it will tell us the probability that we will
    not notice it for a given positive class (given sign).
  prefs: []
  type: TYPE_NORMAL
- en: In a classification task, the number of false negatives is the number of items
    that are not labeled as belonging to the positive class but should have been labeled.
  prefs: []
  type: TYPE_NORMAL
- en: Recall is the number of true positives divided by the sum of true positives
    and false negatives. In other words, out of all the pictures of cats in the world,
    recall is the fraction of pictures that have been correctly identified as pictures
    of cats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to calculate recall of a given positive label using true and predicted
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we have the same signature as for the precision, and we retrieve true
    positives the same way, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, notice that the sum of true positives and false negatives is the total
    number of points in the given data class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we just have to count the number of elements in that class, which means
    we sum the `positive_label` column of the confusion matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we return the ratio as for the precision function, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the distribution of recall values for all 43 classes of
    traffic signs, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15d10f43-3ea6-4eda-86c9-15cd242e0a64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The recall values are a lot more spread out, with class 21 having a value of
    0.66\. Let''s check out which class has a value of 21:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f779bebe-02c8-40d0-9a43-9dcb761b09d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, this is not as harmful as driving on a road covered with snowflakes/ice,
    but it's very important not to miss dangerous curves ahead on the road. Missing
    this sign could have bad consequences.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will demonstrate the `main()` function routine needed to run
    our app.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run our app, we will need to execute the main function routine (in `chapter6.py`).
    This loads the data, trains the classifier, evaluates its performance, and visualizes
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import all the relevant modules and set up the `main` function,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the goal is to compare classification performance across feature extraction
    methods. This includes running the task using a list of different feature extraction
    approaches. So, we first load the data, and repeat the process for each of the
    featurizing functions, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of the `featurize` functions, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Featurize` the data, so we have a matrix of features, like this:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model using our `train_one_vs_all_SVM` method, as follows:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict test labels for the training data, by featurizing the test data and
    passing to the `predict` method (we have to featurize test data separately to
    make sure we don''t have information leakage), as follows:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We score the predicted labels against the true labels, using the `accuracy`
    function, and store the score in a dictionary, to plot after we have results for
    all the `featurize` functions, like this:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to plot the results, and for this, we choose the `bar` plot
    functionality of `matplotlib`. We also make sure to scale the bar plot accordingly
    to visually understand the scale of difference. Since the accuracy is a number
    between `0` and `1`, we limit the *y* axis to `[0, 1]`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We add some nice formatting to the plot by rotating labels on the horizontal
    axis, adding a `grid` and a `title` to the plot, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And after the last line of `plt.show()` has executed, the plot shown in the
    following screenshot pops up in a separate window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fdf5a433-8f27-4d8b-9100-683d862e9fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we see that `hog_featurize` is a winner on this dataset, but we are far
    from having perfect results—slightly above 95%. To understand how good a result
    it's possible to get, you could do a quick Google search, and you will come across
    a lot of papers achieving 99%+ accuracy. So, even though we are not getting cutting-edge
    results, we did pretty well with an off-the-shelf classifier and an easy `featurize`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting fact is that even though we thought that traffic signs having
    bright colors should lead to hsv_featurize (it being more important than grayscale
    features), it turns out that's not the case.
  prefs: []
  type: TYPE_NORMAL
- en: So, a good takeaway is that you should experiment with your data to develop
    better intuition about which features work for your data and which don't.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of experimentation, let's use a neural network to increase the efficiency
    of our obtained results.
  prefs: []
  type: TYPE_NORMAL
- en: Improving results with neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's do a quick teaser of how good we could get if we were to use some fancy
    **deep neural networks** (**DNNs**), and give you a sneak peek of what is to come
    in the future chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: If we use the following "*not quite so deep"* neural network, which takes about
    2 minutes to train on my laptop (where it takes 1* minute* to train the SVM),
    we get an accuracy of around 0.964!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a snippet of the training method (you should be able to plug it into
    the preceding code, and play with some parameters to see if you could do it later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The code uses the high-level Keras API of TensorFlow (we will see more of this
    in the upcoming chapters) and creates a neural network with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layer** with max pooling that is followed by a dropout—which
    is there only during the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Dense layer** that is followed by a dropout—which is there only during
    the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Dense layer** that spits out the final result; it should identify which
    class (among the 43 classes) the input data belongs to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we only have one convolutional layer, which is very similar to HOG
    featurize. If we were to add more convolutional layers, the performance would
    improve quite a lot, but let's leave that for the next chapters to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we trained a multiclass classifier to recognize traffic signs
    from the GTSRB database. We discussed the basics of supervised learning, explored
    the intricacies of feature extraction, and sneaked a peek into DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Using the approach we took in this chapter, you should be able to formulate
    real-life problems as machine learning models, use your Python skills to download
    a sample labeled dataset from the internet, write your featurizing functions that
    convert images to feature vectors, and use OpenCV for training off-the-shelf machine
    learning models that help you solve your real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, we left out some details along the way, such as attempting to fine-tune
    the hyperparameters of the learning algorithm (as they were out of the scope of
    this book). We only looked at accuracy scores and didn't do much feature engineering
    by trying to combine all sets of different features.
  prefs: []
  type: TYPE_NORMAL
- en: With this functional setup and a good understanding of the underlying methodology,
    you can now classify the entire GTSRB dataset to get accuracies higher than 0.97!
    How about 0.99? It is definitely worth taking a look at their website, where you
    will find classification results for a variety of classifiers. Maybe your own
    approach will soon be added to the list.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move even deeper into the field of machine learning.
    Specifically, we will focus on recognizing emotional expressions in human faces
    using **convolutional neural networks** (**CNNs**). This time, we will combine
    the classifier with a framework for object detection, which will allow us to find
    a human face in an image, and then focus on identifying the emotional expression
    contained in that face.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, The German Traffic Sign
    Recognition Benchmark—A multiclass classification competition, in *Proceedings
    of the IEEE International Joint Conference on Neural Networks*, 2011, pages 1453–1460.
  prefs: []
  type: TYPE_NORMAL
