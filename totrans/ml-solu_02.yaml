- en: Chapter 2. Stock Market Price Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover an amazing application that belongs to predictive
    analysis. I hope the name of the chapter has already given you a rough idea of
    what this chapter is going to be all about. We will try to predict the price of
    the stock index. We will apply some modern machine learning techniques as well
    as deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing and data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the Machine Learning (ML) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring problems with the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding concepts and approaches
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stock market is a place where you can buy and sell units of ownership in
    the company, which we call **stocks**. If the company performs well and increases
    its profit, then you will earn some profit as well because you have the stocks
    of the company, but if the company's profit goes down, then you will lose the
    money you have with the company. So if you invest your money in the right company
    at the right time, it could lead to you earning quite a lot of money. The question
    is which company's stock should you buy? Is there any way we can predict the future
    prices of the stock of any company given the historical prices of the company's
    stock so that we can have higher chances of getting good returns? The answer is
    yes. This is what we will explore in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you invest in the stock market, then you may have heard that stock prices
    are completely random and unpredictable. This is called the *efficient market
    hypothesis,* but a majority of the big financial firms, such as JP Morgan, Citigroup,
    and Goldman Sachs, have mathematical and quantitative analysts who are trying
    to develop predictive models to help these big firms decide when to invest and
    in which stock.
  prefs: []
  type: TYPE_NORMAL
- en: Before investing in any stock, we do some basic research regarding the company's
    profile. We try to understand its business model. We also check the balance sheets
    of the company to get to know what the profit and loss of the company is. What
    are the products that the company will launch in the next couple of months? What
    kind of news is coming in about the company? What are the current industry trends?
    After researching all these parameters, we will invest our money in a particular
    company's stock if we feel we will gain some profit; otherwise, we won't invest
    in that company.
  prefs: []
  type: TYPE_NORMAL
- en: 'We depend on various sources of information to get an idea about whether we
    need to buy stocks or sell stocks. Don''t you think all this analysis takes a
    lot of our time? I want to put two questions in front of you. First, can we use
    some of the data points discussed here and build a system that will help us find
    out future stock prices? And can we use historical stock prices to predict future
    stock prices? The answer to both of these questions is yes, definitely: we can
    build a system that will use historical stock prices and some of the other data
    points so that we can predict the future prices of stock. As per the efficient
    market hypothesis, by using historical prices of stock and various other data
    points, we can obtain the future prices of the stock, which will be better than
    a random guess. In this chapter, we will build a predictive model, which will
    predict the close price of the stock. In the next section, we will look at how
    to collect the dataset in order to build the model. So, let''s get started!'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to build the model, first we need to collect the data. We will use
    the following two data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dow Jones Industrial Average** (**DJIA**) index prices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: News articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DJIA index prices give us an overall idea about the stock market's movements
    on a particular day, whereas news articles help us find out how news affects the
    stock prices. We will build our model using these two data points. Now let's collect
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting DJIA index prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to collect the DJIA index prices, we will use Yahoo Finance. You can
    visit this link: [https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d](https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d).
    Once you click on this link, you can see that the price data shows up. You can
    change the time period and click on the **Download Data** link and that''s it;
    you can have all the data in `.csv` file format. Refer to the following screenshot
    of the Yahoo finance DJIA index price page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting DJIA index prices](img/B08394_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Yahoo Finance page for DJIA index price'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have downloaded the dataset for the years 2007-2016, which means we
    have 10 years of data for DJIA index prices. You can see this in *Figure 2.1*,
    as well. You can find this dataset using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: Just bear with me for a while; we will understand the meaning of each of the
    data attributes in the *Understand the dataset* section in this chapter. Now,
    let's look at how we can collect the news articles.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting news articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to collect news articles so that we can establish the correlation between
    how news affects the DJIA index value. We are going to perform a sentiment analysis
    on the news articles. You may wonder why we need to perform sentiment analysis.
    If any news has a negative effect on the financial market, then it is likely that
    the prices of stocks will go down, and if news about the financial market is positive,
    then it is likely that prices of the stocks will go up. For this dataset, we will
    use news articles from the New York Times (NYTimes). In order to collect the dataset
    of news articles, we will use the New York Times' developer API. So, let's start
    coding!
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, register yourself on the NYTimes developer website and generate
    your API key. The link is [https://developer.nytimes.com/signup](https://developer.nytimes.com/signup).
    I have generated the API key for the Archive API. Here, we are using *newsapi,
    JSON, requests*, and *sys* dependencies. You can also refer to the NYTimes developer
    documentation using this link: [https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json](https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py](https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py).
    You can see the code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting news articles](img/B08394_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Code snippet for getting the news article data from the New York
    Times'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the code, there are three methods. The first two methods
    are for exceptions and the third method checks for the validation and requests
    the URL that can generate the news article data for us. This NYTimes API URL takes
    three parameters, which are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After this step, we will call the third function and pass the year value from
    2007 to 2016\. We will save the data in the *JSON* format. You can refer to the
    code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting news articles](img/B08394_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Code snippet for getting news article data from the New York Times'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the raw JSON dataset using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the next section, in which we will understand the dataset
    and the attributes that we have collected so far.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will understand the meaning of data attributes, which will
    help us understand what kind of dataset we are going to deal with and what kind
    of preprocessing is needed for the dataset. We understand our dataset in two sections,
    and those sections are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the DJIA dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the NYTimes news article dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the DJIA dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the DJIA dataset, we have seven data attributes. They are quite easy to
    understand, so let''s look at each of them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Date`: The first column indicates the date in the YYYY-MM-DD format when you
    see data in the .csv file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Open`: This indicates the price at which the market opens, so it is the opening
    value for the DJIA index for that particular trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`High`: This is the highest price for the DJIA index for a particular trading
    day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Low`: This is the lowest price for DJIA index for a particular trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Close`: The price of DJIA index at the close of the trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Adj close`: The adjusted closing price (adj close price) uses the closing
    price as a starting point and takes into account components such as dividends,
    stock splits, and new stock offerings. The adj close price represents the true
    reflection of the DJIA index. Let me give you an example so that you can understand
    the adj close price better: if a company offers a dividend of $5 per share, and
    if the closing price of that company share is $100, then the adj close price will
    become $95\. So, the adj close price considers various factors and, based on them,
    generates the true value of the company''s stock. Here, we are looking at the
    DJIA index value so, most of the time, the closing price and the adj close price
    are the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volume`: These values indicate the number of index traded on exchange for
    a particular trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the basic details of the DJIA index dataset. We use historical data
    and try to predict future movement in the DJIA index.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the NYTimes news article dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the NYTimes news article dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have used the NYTimes developer API and collected the news articles in a
    JSON form, so, here, we will look at the JSON response so we can identify the
    data attributes that are the most important and that we can focus on. In the next
    figure, you can see the JSON response that we get from the NYTimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the NYTimes news article dataset](img/B08394_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: JSON response for news articles using the NYTimes developer tool'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this figure, we can see the JSON response for a single news article. As
    you can see, there is a main data attribute response that carries all other data
    attributes. We will focus on the data attributes that are given inside the docs
    array. Don''t worry; we will not use all the data attributes. Here, we will focus
    on the following data attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`type_of_material`: This attribute indicates that a particular news article
    is derived from a particular kind of source, whether it''s a blog, a news article,
    analysis, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`headlines`: The headline data attribute has the two sub-data attributes. The
    main data attribute contains the actual headline of the news and the kicker data
    attribute is convey the highlight of the article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pub_date`: This data attribute indicates the publication of the news article.
    You can find this attribute in the second-last section of the doc array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`section_name`: This data attribute appeared in the preceding image in the
    last section. It provides the category of the news article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`news_desk`: This data attribute also indicates the news category. When `section_name`
    is absent in a response, we will refer to this attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we understand data attributes properly, we should move on to the next section,
    which is the data preprocessing and data analysis part.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing and data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will mainly cover data preprocessing and data analysis.
    As a part of data preprocessing, we are preparing our training dataset. You may
    be wondering what kind of data preparation I'm talking about, considering we already
    have the data. Allow me to tell you that we have two different datasets and both
    datasets are independent. So, we need to merge the DJIA dataset and NYTimes news
    article dataset in order to get meaningful insights from these datasets. Once
    we prepare our training dataset, we can train the data using different machine
    learning (ML) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's start the coding to prepare the training dataset. We will be using
    `numpy`, `csv`, `JSON`, and `pandas` as our dependency libraries. Here, our code
    is divided into two parts. First, we will prepare the dataset for the DJIA index
    dataset and then we will move to the next part, which is preparing the NYTimes
    news article dataset. During the preparation of the training dataset, we will
    code the basic data analysis steps as well.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the DJIA training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the code snippet in the following screenshot. You can find the
    code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the DJIA training dataset](img/B08394_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Code snippet for preparing the DJIA dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, we are reading the csv file that
    we downloaded from the Yahoo Finance page earlier. After that, we convert the
    data into a list format. We also separated the header and actual data from the
    list. Once we have the data in list format, we convert the data into a numpy array.
    We have selected only three columns from the DIJA dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Close price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adj close price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may have one question in mind: why have we considered only close price
    and Adj close price from the DJIA csv file? Let me clarify: as we know that open
    price is mostly a nearby value of the last day''s close price, we haven''t considered
    the open price. We haven''t considered the high price and low price because we
    don''t know in which particular timestamp these high and low prices occurred.
    For the first iteration, it is quite complicated to predict when the stock index
    reach a high or low value, so, in the meantime, we ignore these two columns. We
    are mainly interested in the overall trend for the DJIA index. If we figure out
    the trend precisely, we can predict the high and low price values later on. Here,
    we restrict our goal to predicting the closing prices for the DJIA index for future
    trading days.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now back to the coding part: we built the pandas dataframe in such a way that
    the date column acts as the index column, and close price and adj close price
    are the two other columns of the dataset. You can see the output of the dataframe
    defined in the form of the `df` variable in the code snippet given in *Figure
    2.5*. You can see the output of dataframe df in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the DJIA training dataset](img/B08394_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Output of pandas dataframe, which is defined as the *df* variable
    in the code snippet in Figure 2.5'
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully now you have a clear understanding of the kind of steps we have followed
    so far. We have created the basic dataframe, so now we will move on to the basic
    data analysis part for a DJIA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Basic data analysis for a DJIA dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will perform basic data analysis on a DJIA dataset. This
    dataset has the date value, but if you look at the values of the date carefully,
    then you will see that there are some missing dates. Suppose data is missing for
    30-12-2006, 31-12-2006, 1-1-2007, and many other dates. In such cases, we will
    add the date values that are missing. You can refer to the code snippet given
    in *Figure 2.7*, as well as find the code for this on this GitHub: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Code snippet for adding all the missing date values in the DJIA
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, we come across another challenge after
    adding these missing date values. We have added the date value, but there is no
    close price or adj close price available corresponding to each of them, so we
    need to replace the NaN values logically, not randomly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to replace the NaN values of close price and adj close price, we will
    use the pandas interpolation functionality. We use linear interpolation to generate
    the missing values for NaN. There are many types of interpolation available, but
    here we are using linear interpolation, and the mathematical equation for linear
    interpolation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2.1: Linear interpolation math formula'
  prefs: []
  type: TYPE_NORMAL
- en: If the two known points are given by the coordinates (x1,y_1) and (x_3,y_3),
    the linear interpolant is the straight line between these points.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Code snippet for basic data analysis and interpolation implementation'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this is available on GitHub at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippet, we haven't defined which type of interpolation
    should be performed on our dataset; in this case, linear interpolation has been
    performed by default. So after applying the linear interpolation, we can replace
    the NaN values with the actual logical values. We have also removed three records
    from the year 2006\. So now, we have a total of 3653 records.
  prefs: []
  type: TYPE_NORMAL
- en: This is the kind of basic data preprocessing and data analysis we did for the
    DJIA index dataset. Now let's move on to the NYTimes news article dataset. We
    need to prepare the training dataset first, so let's begin with it.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the NYTimes news dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will see how we can prepare the NYTimes news dataset. We
    have downloaded the whole news article dataset but we have not put in a filtering
    mechanism for choosing news article categories. Perform the following steps when
    preparing the NYTimes dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting publication date into the YYYY-MM-DD format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filtering news articles by their category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing the filter functionality and merge the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving the merged dataset in the pickle file format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's start coding for each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Converting publication date into the YYYY-MM-DD format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will convert the publication date of the news articles into the YYYY-MM-DD
    format so that we can merge DJIA and NYTimes news article datasets later on. In
    order to achieve this, you can refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting publication date into the YYYY-MM-DD format](img/B08394_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Code snippet for converting the date format of the publication
    date of the news article'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have written a function that can parse and convert the publication
    date format into the necessary YYYY-MM-DD format. We will call this function later
    on when we read the JSON files in which we have stored the JSON response.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering news articles by category
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The other thing that we are going to do here is filter our news article dataset
    by news category. We have downloaded all types of news articles, but for the stock
    market price prediction application, we need news articles that belong to specific
    news categories. So, we need to implement filters that will help us extract the
    necessary subset of news articles. You can refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering news articles by category](img/B08394_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Code snippet for filtering news articles by their categories'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code provided at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding figure, we are extracting news articles that belong
    to the following news categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: National
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: World
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U.S.A.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opinion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foreign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the filter functionality and merging the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we need to iterate each of the JSON files and extract the news articles
    that have one of the news categories defined in the previous section. You can
    refer to the code snippet for the implementation of the filter functionality.
    In the upcoming code snippet, you can also find the implementation for merging
    the DJIA dataset and the NYTimes news articles dataset. To merge the two datasets,
    we are adding each of the news article headlines to the pandas dataframe,and from
    this we will generate our final training dataset. This functionality is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Code snippet for the filtering and merging functionalities'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also coded a bit of the exceptional handling functionality. This is
    done so that if any JSON response does not have the value for the data attributes
    section_name, news_desk, or type_of_material, then this code will throw an exception.
    You can refer to the code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Implementation of exception handling'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will consider news articles that have no `section_name` and `news_desk`
    as well. We will add all the news article headlines to our dataset and put them
    into the pandas dataframe. You can see the code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.13: Handling news articles that have no section_name and news_desk'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the final merged dataset in the form of the pandas dataframe, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Final merged training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Here, for each date, we correspond all the news headlines that belong to the
    business, national, world, U.S.A., politics, opinion, technology, science, and
    heath categories. We have downloaded 1,248,084 news articles, and from these articles,
    we have considered 461,738 news articles for our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access the code using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Saving the merged dataset in the pickle file format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we merge the data, we need to save the data objects, so we will use the
    pickle module of Python. Pickle helps us serialize and de-serialize the data.
    The pickle dependency library is fast because the bulk of it is written in C,
    like the Python interpreter itself. Here, we save our training dataset as a `.pkl`
    file format. You can refer to the following code snippet*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the merged dataset in the pickle file format](img/B08394_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: Code snippet for saving data in the pickle format'
  prefs: []
  type: TYPE_NORMAL
- en: We have saved the dataset as the `pickled_ten_year_filtered_lead_para.pkl` file.
    You can find the code on GitHub at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will mainly focus on the feature engineering part. We
    will also perform some minor data cleaning steps. So let's jump to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed earlier, we want to predict the close price for the DJIA index
    for a particular trading day. In this section, we will do feature selection based
    on our intuition for our basic prediction model for stock prices. We have already
    generated the training dataset. So, now we will load the saved .pkl format dataset
    and perform feature selection as well as minor data processing. We will also generate
    the sentiment score for each of the filtered NYTimes news articles and will use
    this sentiment score to train our baseline model. We will use the following Python
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: numpy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nltk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section has the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minor preprocessing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's begin coding!
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have saved the data in the pickle format, and now we need to load data from
    it. You can refer to the following code snippet*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Code snippet for loading the dataset from the pickle file'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code by clicking on this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in the dataframe output, there is a dot (.) before every article
    headline in the entire dataset, so we need to remove these dots. We will execute
    this change in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Minor preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a part of minor preprocessing, we will be performing the following two changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting the adj close prices into the integer format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing the leftmost dot (.) from news headlines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting adj close price into the integer format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that the adj close price is in the form of a float format. So, here
    we will convert float values into the integer format as well as store the converted
    values as *price* attributes in our pandas dataframe. Now, you may wonder why
    we consider only the adj close prices. Bear with me for a while, and I will give
    you the reason for that. You can find the convergence code snippet in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting adj close price into the integer format](img/B08394_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Code snippet for converting the adj close price into the integer
    format'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the second change.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the leftmost dot from news headlines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will see the implementation for removing the leftmost dot.
    We will be using the `lstrip()` function to remove the dot. You can refer to the
    code snippet in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the leftmost dot from news headlines](img/B08394_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Code snippet for removing *dot* from the news article headlines'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to our next section, which is feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature selection is one of the most important aspects of feature engineering
    and any **Machine Learning** (**ML**) application. So, here we will focus on feature
    selection. In the previous section, I raised the question of why we select only
    the *adj close price* and not the *close price.* The answer to this question lies
    in the feature selection. We select the *adj close prices* because these prices
    give us a better idea about what the last price of the DJIA index is, including
    the stock, mutual funds, dividends, and so on. In our dataset, *close prices*
    are mostly the same as the *adj close price* and in future, if we consider the
    *close price* for unseen data records, we can''t derive the *adj close price*
    because it may be equal to the *close price* or higher than the *close price,*
    The *adj close price* for DJIA index may higher than the c*lose price* because
    it will include stocks, mutual funds, dividend and so on. but we don''t know how
    much higher it will be for unseen dataset where we have just considered *close
    price*. So if we consider the *adj close price,* then we will know that the *close
    price* may be less than or equal to the *adj close price*, but not more than the
    *adj close price*. The *adj close price* is kind of maximum possible value for
    closing price. So, we have considered the *adj close price* for the development.
    For the baseline model, we will be considering the *adj close price*. We have
    renamed the column to *price*. You can refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature engineering](img/B08394_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Code snippet for considering the adj close price as a part of
    feature selection'
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, we will now perform sentiment analysis on the news article dataset.
    We can use the sentiment score when we train our model. So, let's move on to the
    sentiment analysis part.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis of NYTimes news articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement sentiment analysis, we are using the nltk inbuilt sentiment
    analysis module. We will obtain negative, positive, and compound sentiment scores.
    We have used a lexicon-based approach. In the lexicon-based approach, words of
    each sentence are analyzed, and based on the `sentiwordnet` score, each word is
    given a specific sentiment score; then, the aggregate sentence level score is
    decided.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sentiwordnet is the dictionary which contain sentiment score for words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover details related to sentiment analysis in [Chapter 5](ch05.xhtml
    "Chapter 5. Sentiment Analysis"), *Sentiment Analysis*. You can refer to the following
    sentiment analysis code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentiment analysis of NYTimes news articles](img/B08394_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: Sentiment analysis code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: All scores generated by the preceding code are stored in the dataframe, so you
    can see the aggregate score of news article headlines in the following screenshot*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentiment analysis of NYTimes news articles](img/B08394_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: Aggregate sentiment analysis score stored in the dateframe'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, we will obtain the sentiment score for the NYTimes
    news articles dataset and combine these sentiment scores as part of the training
    dataset. So far, we have done minor preprocessing, selected the data attribute
    as per our intuition, and generated the sentiment score. Now, we will select the
    machine learning algorithm and try to build the baseline model. So, let's move
    on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Machine Learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will choose the Machine Learning (ML) algorithm based on
    our intuition and then perform training using our training dataset. This is the
    first model for this particular chapter, so the trained model is our baseline
    model, which we will improve later on. So, let's decide which kind of ML algorithm
    suits this stock price prediction application.
  prefs: []
  type: TYPE_NORMAL
- en: The stock price prediction application is a time-series analysis problem, where
    we need to predict the next point in the time series. This prediction activity
    is similar to linear regression, so we can say that this application is a kind
    of regression problem and any algorithm from the regression family should work.
    Let's select the ensemble algorithm, which is *RandomForestRegressor*, in order
    to develop our baseline model. So let's train our baseline model, and, based on
    the result of that model, we will modify our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Training the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you know, we have selected the **RandomForestRegressor** algorithm. We will
    be using the scikit-learn library to train the model. These are the steps we need
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training and testing dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting prediction labels for the training and testing dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting sentiment scores into the numpy array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the ML model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's implement each of these steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training and testing dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have 10 years of data values. So for training purposes, we will be using
    8 years of the data, which means the dataset from 2007 to 2014\. For testing purposes,
    we will be using 2 years of the data, which means data from 2015 and 2016\. You
    can refer to the code snippet in the following screenshot to implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the training and testing dataset](img/B08394_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: Splitting the training and testing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, our training dataset has been
    stored in the train dataframe and our testing dataset has been stored in the test
    dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting prediction labels for the training and testing datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we split the training and testing dataset, we also need to store the adj
    close price separately because we need to predict these *adj close prices* (indicated
    in the code as `prices`); these price values are labels for our training data,
    and this training becomes supervised training as we will provide the actual price
    in the form of labels. You can refer to the following code for the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting prediction labels for the training and testing datasets](img/B08394_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: Splitting the prediction labels for training and testing datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Here, all attributes except the price are given in a feature vector format and
    the price is in the form of labels. The ML algorithm takes this feature vector,
    labels the pair, learns the necessary pattern, and predicts the price for the
    unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Converting sentiment scores into the numpy array
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start the training, there is one last, necessary point that we need
    to keep in mind: we are converting the sentiment analysis scores into the numpy
    array format. This is because once we set the price attribute as a prediction
    label, our features vector will contain only the sentiment scores and date. So
    in order to generate a proper feature vector, we have converted the sentiment
    score into a numpy array. The code snippet to implement this is provided in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting sentiment scores into the numpy array](img/B08394_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.24: Code snippet for converting sentiment analysis score into the
    numpy array'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the code snippet, we have performed the same conversion
    operation for both training the dataset and testing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that if you get a value error, check the dataset because there may be a
    chance that a column in the dataset has a blank or null value.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's train our model!
  prefs: []
  type: TYPE_NORMAL
- en: Training of the ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first iteration, we use the RandomForestRegressor algorithm, which is
    provided as part of the scikit-learn dependency. You can find the code for this
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training of the ML model](img/B08394_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.25: Code snippet for training using RandomForestRegressor'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, we have used all the default values
    for our hyperparameters. For a more detailed description regarding hyperparameters,
    you can refer to [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now that our model has been trained, we need to test it using our testing dataset.
    Before we test, let's discuss the approach we will take to test our model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will understand the testing matrix and visualization approaches
    to evaluate the performance of the trained ML model. So let''s understand both
    approaches, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The default testing matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visualization approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default testing matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are using the default score API of scikit-learn to check how well the ML
    is performing. In this application, the score function is the coefficient of the
    sum of the squared error. It is also called the coefficient of R2, which is defined
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *u* indicates the residual sum of squares. The equation for *u* is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The variable *v* indicates the total sum of squares. The equation for *v* is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The best possible score is 1.0, and it can be a negative score as well. A negative
    score indicates that the trained model can be arbitrarily worse. A constant model
    that always predicts the expected value for label *y*, disregarding the input
    features, will produce an R2 score of 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain the score, we just need to call the score function. The code
    for testing will be the same as that in the *Test baseline model* section. Now
    let's take a look at another testing approach that is quite helpful in understanding
    the output with respect to true testing labels. So, let's check that out!
  prefs: []
  type: TYPE_NORMAL
- en: The visualization approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be exploring an effective and intuitive approach, which
    is the **visualization** of the predicted output versus real output. This approach
    gives you a lot of insight as the graphs are easy to understand and you can decide
    the next steps to improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this application, we will be using the actual prices from the testing dataset
    and the predicted prices for the testing dataset, which will indicate how good
    or bad the predictions are. You will find the code and graph for this process
    in the next section, named *Testing the baseline model*.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing our testing approach so that we can
    evaluate our model''s accuracy. We will first generate the output prediction and
    then we''ll start testing it. We will be implementing the following steps here:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating and interpreting the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating the score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating and interpreting the output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate the prediction, we are using the `treeinterpreter` library. We
    are predicting the price value for each of our testing dataset records using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating and interpreting the output](img/B08394_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.26: Code snippet for generating the prediction'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *prediction* is the array in which we have elements that are the corresponding
    predicted *adj close price* for all records of the testing dataset. Now, we will
    compare this predicted output with the actual *adj close price* of the testing
    dataset. By doing this, we will get to know how accurately our first model is
    predicting the *adj close price*. In order to evaluate further, we will generate
    the accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the accuracy score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will generate the accuracy score as per the equations provided
    in the *default testing matrix* section. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the accuracy score](img/B08394_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.27: Code snippet for generating the score for the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding code snippet, our model is not doing too well.
    At this point, we don't know what mistakes we've made or what went wrong. This
    kind of situation is common when you are trying to solve or build an ML model.
    We can grasp the problem better using visualization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the visualization graph in this section. Using the graph,
    we will identify the kind of error we have committed so that we can fix that error
    in the next iteration. We will plot a graph where the *y-axis* represents the
    *adj close prices* and the *x-axis* represent the *dates*. We plot the *actual
    prices* and *predicted prices* on the graph so that we will get a brief idea about
    how our algorithm is performing. We will use the following code snippet to generate
    the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the output](img/B08394_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.28: Code snippet for generating graph for predicted prices vs actual
    prices.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding graph, the top single line (orange color)
    represents the actual price and the messy spikes (blue color) below the line represent
    the predicted prices. From this plot, we can summarize that our model can't predict
    the proper prices. Here, you can see that the actual prices and predicted prices
    are not aligned with each other. We need to fix this issue. There are some techniques
    that we can try, such as alignment, smoothing, and trying a different algorithm.
    So, let's cover the problems of this approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can access the entire code on this topic from the GitHub link at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring problems with the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing the problems of the existing approach.
    There are mainly three errors we could have possibly committed, which are listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trying a different ML algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each of the points one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in the graph, our actual price and predicted prices are not
    aligned with each other. This becomes a problem. We need to perform alignment
    on the price of the stocks. We need to consider the average value of our dataset,
    and based on that, we will generate the alignment. You can understand more about
    alignment in upcoming section called *Alignment-based approach*.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second problem I feel we have with our first model is that we haven't applied
    any smoothing techniques. So for our model, we need to apply smoothing techniques
    as well. We will be using the **Exponentially Weighted Moving Average** (**EWMA**)
    technique for smoothing*.* This technique is used to adjust the variance of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Trying a different ML algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our model, we have used the `RandomForestRegressor` algorithm. But what
    if we try the same thing with our model using a different algorithm, say *Logistic
    Regression*? In the next section, you will learn how to implement this algorithm—after
    applying the necessary alignment and smoothing, of course.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the possible problems with our first baseline approach. Now, we
    will try to understand the approach for implementing the alignment, smoothing,
    and `Logistic Regression` algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be looking at the key concepts and approaches for alignment
    and smoothing. It is not that difficult to implement the *Logistic Regression*
    algorithm; we will be using the scikit-learn API. So, we will start with understanding
    the concepts and approaches for implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concepts and approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will discuss how alignment and smoothing will work. Once we understand
    the technicality behind alignment and smoothing, we will focus on the Logistic
    Regression-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using this approach, we will be increasing the prices using a constant value
    so that our predicted price and actual price in testing the dataset will be aligned.
    Suppose we take 10 days into consideration. We will generate the average of the
    value of the prices. After that, we generate the average value for the prices
    that have been predicted by the first ML model. Once we generate both average
    values, we need to subtract the values, and the answer is the alignment value
    for those `10` days.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an intuitive working example that will help clear your vision. Consider
    10 days from January 2, 2015, to January 11, 2015\. For each record, you will
    take the average value for the actual price. Suppose the number will come to 17,676
    and the average of predicted price value will be 13,175\. In this case, you will
    get a difference of 4,501, which is the value for the alignment. We will add this
    value to our testing dataset so that testing price values and predicted price
    values will be aligned. You will find the code implementation in the *Implement
    revised approach* section.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, we will be using EWMA. **EWMA** stands for **Exponentially
    Weighted Moving Average**. The smoothing approach is based on the weighted average
    concept. In general, a weighted moving average is calculated by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Smoothing-based approach](img/B08394_02_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x[t]* is the input and *y[t]* is the output. Weights are calculated
    using the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Smoothing-based approach](img/B08394_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.29: Equation for calculating the weight for EWMA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, α is the smoothing constant. If the value of the smoothing constant is
    high, then it will be close to the actual value, and if the smoothing constant
    is low, then it will be smoother but not close to the actual value. Typically,
    in statistics the smoothing constant ranges between 0.1 and 0.3\. Therefore, we
    can generate the smoothed value using the smoothing constant.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a working example. Take a smoothing constant = 0.3; if the actual
    value is 100 and the predicted value is 110, then the smoothed value can be obtain
    using this equation, which is (smoothing constant * actual value ) + (1- smoothing
    constant) * predicted value. The value that we will obtain is *(0.3* 100) + (1-0.3)*110
    = 107*. For more information, you can refer to [http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows).
  prefs: []
  type: TYPE_NORMAL
- en: We will see the actual code-level implementation in the Implement revised approach
    section. pandas already has an API, so we can easily implement EWMA.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing the Logistic Regression algorithm is a simple task because we just
    need to use the scikit-learn API. For the testing dataset, we will apply alignment
    and smoothing. After evaluating accuracy, we will decide whether we need to change
    the ML algorithm or not. We started with our intuition and slowly we improved
    our approaches. I don't really need to explain the Logistic Regression algorithm
    itself, but during the implementation, we will discuss the important points.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to move on to the implementation part of our revised approach.
    So, let's take a look at the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the three parts of implementation, which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the problem with the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we are implementing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have already discussed the approach and key concepts, so now we just focus
    on the code part here. You can find all the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing alignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The alignment is performed on the testing dataset. You can refer to the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing alignment](img/B08394_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.30: Code snippet for alignment on the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, we obtain a difference of 10
    days *adj close price* using the average price of the last 5 days and the average
    price of the predicted upcoming 5 days in order to align the test data. Here,
    we also convert the date from the string into the date format. As you can see,
    5096.99 is the difference in the test prediction price, which we will add to our
    predicted *adj close price* value. We have generated the graph again so we can
    easily understand that the alignment approach is implemented nicely. You can refer
    to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing alignment](img/B08394_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.31: Code snippet of the graph for the alignment approach'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code snippet, the alignment graph shows that
    our testing dataset price and predicted prices are aligned. The benefit of the
    aligned graph is that now we can define in a precise manner that `RandomForestRegressor`
    didn't do its job with high accuracy as its performance was not great for all
    data records. The alignment graph gave us a crystal clear picture of our previous
    iteration. So when we train the logistic regression now, we will evaluate the
    predicted prices using alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing smoothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are using the pandas EWMA API using 60 days'' time span and frequency time
    *D.* This "D" indicates that we are dealing with the datetime format in our dataset.
    You can see the code implementation in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.32: Code snippet for EWMA smoothing'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also generating the graph in which we put the *predicted price, average
    predicted price, actual price*, and *average actual price*. You can refer to the
    following code and graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.33: Code snippet for generating the graph after smoothing'
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, you can see that after smoothing the *average predicted price,*
    the curve follows the *actual price* trend. Although the accuracy is not great,
    we will move toward a positive direction. The smoothing technique will be useful
    for us if we want to tune our algorithm. You can refer to the following graph
    for the *average predicted price versus actual price:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.34: Code snippet for the graph, indicating average_predicted_price
    versus actual_price'
  prefs: []
  type: TYPE_NORMAL
- en: By referring to the preceding graph, we can indicate that we apply alignment
    and smoothing because it helps tune our ML model for the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing logistic regression. Take a look at
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing logistic regression](img/B08394_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.35: Code snippet for logistic regression'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have trained the model again using the logistic regression ML algorithm.
    We have also implemented alignment and smoothing for the test dataset. Now, let's
    evaluate the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have tested the logistic regression model. You can refer to the visualization
    in the form of graphs that show that this revised approach is certainly better
    than *RandomForesRegressor (without alignment and smoothing),* but it is not up
    to the mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.36: Year-wise prediction graph'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, we have generated a year-wise graph
    for *logistic Regression*; we can see a slight improvement using this model. We
    have also used alignment and smoothing, but they are not too effective.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's discuss what the problems with this revised approach are, and then
    we can implement the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss why our revised approach doesn't give us good
    results. ML models don't work because datasets are not normalized. The second
    reason is that even after alignment and smoothing, the *RandomForestRegression*
    ML model faces an overfitting issue. For the best approach, we need to handle
    normalization and overfitting. We can solve this issue using a neural network-based
    ML algorithm. So in our last iteration, we will develop the neural network that
    can give us the best accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we are going to implement the neural network-based algorithm **multilayer
    perceptron** (**MLP**). You can refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The best approach](img/B08394_02_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.37: Code snippet for multilayer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see that we are using the Relu activation function, and the gradient
    descent solver function is ADAM. We are using a learning rate of 0.0001\. You
    can evaluate the result by referring to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The best approach](img/B08394_02_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.38: Code snippet for generating the graph for the actual and predicted
    prices'
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph shows that all the data records'' predicted prices follow the actual
    price pattern. You can say that our MLP model works well to predict the stock
    market prices. You can find the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to predict stock prices. We covered the different
    machine learning algorithms that can help us in this. We tried Random Forest Regressor,
    Logistic Regression, and multilayer perceptron. We found out that the multilayer
    perceptron works really well. I really want to discuss something beyond what we
    have done so far. If you are under the impression that using the sentiment analysis
    of news and predictive methods, we can now correctly predict the stock market
    price with a hundred percent accuracy, then you would be wrong. We can't predict
    stock prices with a hundred percent accuracy. Many communities, financial organizations,
    and academic researchers are working in this direction in order to make a stock
    market price predictive model that is highly accurate. This is an active research
    area.
  prefs: []
  type: TYPE_NORMAL
- en: So if you are interested in research and freelancing, then you can join some
    pretty cool communities. There are two communities that are quite popular. One
    of these is quantopian ([https://www.quantopian.com/](https://www.quantopian.com/)).
    In this community, you can submit your stock price prediction algorithm, and if
    it outperforms other competitors' algorithms, then you will win a cash price,
    and if you get the license for your algorithm, then you get some profit from transactions
    that will be done through your licensed algorithm. The second community is numer.ai
    ([https://numer.ai/](https://numer.ai/)). This community is similar to quantopian.
    So, the possibilities of this application are limitless. Both communities offer
    some great tutorials. So try something different, and hopefully you will come
    up with a great algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will tap the retail or e-commerce domain and try to
    figure out some interesting facts about the user behavior dataset and users' social
    footprint. This will help us understand how the company should change their website
    or some functionality on the website. What are the chances of the email campaign
    going well and which type of users will respond to this campaign? Keep reading
    this book! We will discuss all these things in the next chapter.
  prefs: []
  type: TYPE_NORMAL
