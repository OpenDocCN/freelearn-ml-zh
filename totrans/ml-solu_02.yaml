- en: Chapter 2. Stock Market Price Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章. 股票市场价格预测
- en: In this chapter, we will cover an amazing application that belongs to predictive
    analysis. I hope the name of the chapter has already given you a rough idea of
    what this chapter is going to be all about. We will try to predict the price of
    the stock index. We will apply some modern machine learning techniques as well
    as deep learning techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一个属于预测分析的精彩应用。我希望章节的名称已经给了你一个大致的印象，本章将要讲述什么。我们将尝试预测股票指数的价格。我们将应用一些现代机器学习技术以及深度学习技术。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Collecting the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据集
- en: Understanding the dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Data preprocessing and data analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理和数据分析
- en: Feature engineering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Selecting the Machine Learning (ML) algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习（ML）算法
- en: Training the baseline model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: Understanding the testing matrix
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the baseline model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: Exploring problems with the existing approach
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索现有方法的问题
- en: Understanding the revised approach
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解修订方法
- en: Understanding concepts and approaches
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解概念和方法
- en: Implementing the revised approach
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施修订方法
- en: Testing the revised approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试修订方法
- en: Understanding problems with the revised approach
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用修订方法理解问题
- en: The best approach
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Summary
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: So, let's get started!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Introducing the problem statement
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: The stock market is a place where you can buy and sell units of ownership in
    the company, which we call **stocks**. If the company performs well and increases
    its profit, then you will earn some profit as well because you have the stocks
    of the company, but if the company's profit goes down, then you will lose the
    money you have with the company. So if you invest your money in the right company
    at the right time, it could lead to you earning quite a lot of money. The question
    is which company's stock should you buy? Is there any way we can predict the future
    prices of the stock of any company given the historical prices of the company's
    stock so that we can have higher chances of getting good returns? The answer is
    yes. This is what we will explore in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 股票市场是一个你可以买卖公司所有权单位的地方，我们称之为**股票**。如果公司表现良好并增加其利润，那么你也会获得一些利润，因为你拥有公司的股票，但如果公司的利润下降，那么你将失去你在公司中的钱。所以如果你在正确的时间和正确的公司投资，可能会让你赚很多钱。问题是你应该购买哪只公司的股票？有没有一种方法，我们可以根据公司股票的历史价格预测任何公司股票的未来价格，这样我们就有更高的机会获得良好的回报？答案是肯定的。这就是我们在本章要探讨的内容。
- en: If you invest in the stock market, then you may have heard that stock prices
    are completely random and unpredictable. This is called the *efficient market
    hypothesis,* but a majority of the big financial firms, such as JP Morgan, Citigroup,
    and Goldman Sachs, have mathematical and quantitative analysts who are trying
    to develop predictive models to help these big firms decide when to invest and
    in which stock.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你投资股市，你可能听说过股票价格是完全随机且不可预测的。这被称为**有效市场假说**，但大多数大型金融公司，如摩根大通、花旗集团和高盛，都有数学和定量分析师，他们试图开发预测模型，帮助这些大公司决定何时投资以及投资哪只股票。
- en: Before investing in any stock, we do some basic research regarding the company's
    profile. We try to understand its business model. We also check the balance sheets
    of the company to get to know what the profit and loss of the company is. What
    are the products that the company will launch in the next couple of months? What
    kind of news is coming in about the company? What are the current industry trends?
    After researching all these parameters, we will invest our money in a particular
    company's stock if we feel we will gain some profit; otherwise, we won't invest
    in that company.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在投资任何股票之前，我们会对公司的概况进行一些基本研究。我们试图了解其商业模式。我们还检查公司的资产负债表，以了解公司的盈亏情况。公司将在接下来的几个月内推出哪些产品？关于公司的哪些新闻正在传来？当前的行业趋势是什么？在研究了所有这些参数之后，如果我们觉得我们将会获得一些利润，我们就会投资特定公司的股票；否则，我们不会投资那个公司。
- en: 'We depend on various sources of information to get an idea about whether we
    need to buy stocks or sell stocks. Don''t you think all this analysis takes a
    lot of our time? I want to put two questions in front of you. First, can we use
    some of the data points discussed here and build a system that will help us find
    out future stock prices? And can we use historical stock prices to predict future
    stock prices? The answer to both of these questions is yes, definitely: we can
    build a system that will use historical stock prices and some of the other data
    points so that we can predict the future prices of stock. As per the efficient
    market hypothesis, by using historical prices of stock and various other data
    points, we can obtain the future prices of the stock, which will be better than
    a random guess. In this chapter, we will build a predictive model, which will
    predict the close price of the stock. In the next section, we will look at how
    to collect the dataset in order to build the model. So, let''s get started!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖各种信息来源来了解我们是否需要购买股票或出售股票。您不觉得所有这些分析都花费了我们很多时间吗？我想向您提出两个问题。首先，我们能否使用这里讨论的一些数据点构建一个系统，帮助我们找出未来的股票价格？还有，我们能否使用历史股票价格来预测未来的股票价格？这两个问题的答案都是肯定的：我们可以构建一个系统，使用历史股票价格和一些其他数据点，以便我们能够预测股票的未来价格。根据有效市场假说，通过使用股票的历史价格和多种其他数据点，我们可以获得股票的未来价格，这将比随机猜测更好。在本章中，我们将构建一个预测模型，该模型将预测股票的收盘价。在下一节中，我们将探讨如何收集数据集以构建模型。那么，让我们开始吧！
- en: Collecting the dataset
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据集
- en: 'In order to build the model, first we need to collect the data. We will use
    the following two data points:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建模型，我们首先需要收集数据。我们将使用以下两个数据点：
- en: '**Dow Jones Industrial Average** (**DJIA**) index prices'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道琼斯工业平均指数**（**DJIA**）指数价格'
- en: News articles
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新闻文章
- en: DJIA index prices give us an overall idea about the stock market's movements
    on a particular day, whereas news articles help us find out how news affects the
    stock prices. We will build our model using these two data points. Now let's collect
    the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 道琼斯工业平均指数价格给我们一个特定日子股市走势的整体概念，而新闻文章帮助我们了解新闻如何影响股票价格。我们将使用这两个数据点来构建我们的模型。现在，让我们收集数据。
- en: Collecting DJIA index prices
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集道琼斯工业平均指数价格
- en: 'In order to collect the DJIA index prices, we will use Yahoo Finance. You can
    visit this link: [https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d](https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d).
    Once you click on this link, you can see that the price data shows up. You can
    change the time period and click on the **Download Data** link and that''s it;
    you can have all the data in `.csv` file format. Refer to the following screenshot
    of the Yahoo finance DJIA index price page:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集道琼斯工业平均指数价格，我们将使用雅虎财经。您可以访问此链接：[https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d](https://finance.yahoo.com/quote/%5EDJI/history?period1=1196706600&period2=1512325800&interval=1d&filter=history&frequency=1d)。一旦您点击此链接，您就可以看到价格数据出现。您可以更改时间范围并点击**下载数据**链接，就这样；您就可以拥有所有以`.csv`文件格式存储的数据。请参考以下雅虎财经道琼斯工业平均指数价格页面截图：
- en: '![Collecting DJIA index prices](img/B08394_02_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![收集道琼斯工业平均指数价格](img/B08394_02_01.jpg)'
- en: 'Figure 2.1: Yahoo Finance page for DJIA index price'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：道琼斯工业平均指数价格雅虎财经页面
- en: 'Here, we have downloaded the dataset for the years 2007-2016, which means we
    have 10 years of data for DJIA index prices. You can see this in *Figure 2.1*,
    as well. You can find this dataset using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已下载了2007-2016年的数据集，这意味着我们拥有10年的道琼斯工业平均指数价格数据。您也可以在*图2.1*中看到这一点。您可以使用以下GitHub链接找到这个数据集：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/DJIA_data.csv)。
- en: Just bear with me for a while; we will understand the meaning of each of the
    data attributes in the *Understand the dataset* section in this chapter. Now,
    let's look at how we can collect the news articles.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请稍等片刻；我们将理解本章“理解数据集”部分中每个数据属性的意义。现在，让我们看看我们如何收集新闻文章。
- en: Collecting news articles
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集新闻文章
- en: We want to collect news articles so that we can establish the correlation between
    how news affects the DJIA index value. We are going to perform a sentiment analysis
    on the news articles. You may wonder why we need to perform sentiment analysis.
    If any news has a negative effect on the financial market, then it is likely that
    the prices of stocks will go down, and if news about the financial market is positive,
    then it is likely that prices of the stocks will go up. For this dataset, we will
    use news articles from the New York Times (NYTimes). In order to collect the dataset
    of news articles, we will use the New York Times' developer API. So, let's start
    coding!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要收集新闻文章，以便我们可以建立新闻如何影响道琼斯工业平均指数（DJIA）值之间的相关性。我们将对新闻文章进行情感分析。您可能会想知道为什么我们需要进行情感分析。如果任何新闻对金融市场有负面影响，那么股票价格很可能会下降；如果有关金融市场的新闻是积极的，那么股票价格很可能会上涨。对于这个数据集，我们将使用《纽约时报》（NYTimes）的新闻文章。为了收集新闻文章数据集，我们将使用《纽约时报》的开发者API。那么，让我们开始编码吧！
- en: 'First of all, register yourself on the NYTimes developer website and generate
    your API key. The link is [https://developer.nytimes.com/signup](https://developer.nytimes.com/signup).
    I have generated the API key for the Archive API. Here, we are using *newsapi,
    JSON, requests*, and *sys* dependencies. You can also refer to the NYTimes developer
    documentation using this link: [https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json](https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要在《纽约时报》开发者网站上注册并生成您的API密钥。链接是[https://developer.nytimes.com/signup](https://developer.nytimes.com/signup)。我已经为存档API生成了API密钥。在这里，我们使用*newsapi,
    JSON, requests*和*sys*依赖项。您也可以通过以下链接参考《纽约时报》开发者文档：[https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json](https://developer.nytimes.com/archive_api.json#/Documentation/GET/%7Byear%7D/%7Bmonth%7D.json)。
- en: 'You can find the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py](https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py).
    You can see the code snippet in the following screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下GitHub链接找到代码：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py](https://github.com/jalajthanaki/stock_price_prediction/blob/master/getdata_NYtimes.py)。您可以在下面的屏幕截图中看到代码片段：
- en: '![Collecting news articles](img/B08394_02_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![收集新闻文章](img/B08394_02_02.jpg)'
- en: 'Figure 2.2: Code snippet for getting the news article data from the New York
    Times'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：从《纽约时报》获取新闻文章数据的代码片段
- en: 'As you can see in the code, there are three methods. The first two methods
    are for exceptions and the third method checks for the validation and requests
    the URL that can generate the news article data for us. This NYTimes API URL takes
    three parameters, which are given as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在代码中所见，有三个方法。前两个方法是用于异常处理，第三个方法用于验证并请求为我们生成新闻文章数据的URL。这个《纽约时报》API URL需要三个参数，如下所示：
- en: Year
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份
- en: Month
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份
- en: API key
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API密钥
- en: 'After this step, we will call the third function and pass the year value from
    2007 to 2016\. We will save the data in the *JSON* format. You can refer to the
    code snippet in the following screenshot:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们将调用第三个函数，并传递从2007年到2016年的年份值。我们将以*JSON*格式保存数据。您可以在下面的屏幕截图中的代码片段中参考：
- en: '![Collecting news articles](img/B08394_02_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![收集新闻文章](img/B08394_02_03.jpg)'
- en: 'Figure 2.3: Code snippet for getting news article data from the New York Times'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：从《纽约时报》获取新闻文章数据的代码片段
- en: 'You can find the raw JSON dataset using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下GitHub链接找到原始JSON数据集：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json](https://github.com/jalajthanaki/stock_price_prediction/blob/master/data/2016-01.json)。
- en: Now let's move on to the next section, in which we will understand the dataset
    and the attributes that we have collected so far.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入下一节，我们将了解我们迄今为止收集的数据集和属性。
- en: Understanding the dataset
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'In this section, we will understand the meaning of data attributes, which will
    help us understand what kind of dataset we are going to deal with and what kind
    of preprocessing is needed for the dataset. We understand our dataset in two sections,
    and those sections are given as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解数据属性的含义，这将帮助我们了解我们将要处理的数据集类型以及数据集需要进行的预处理类型。我们将在两个部分中理解我们的数据集，如下所示：
- en: Understanding the DJIA dataset
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解道琼斯工业平均指数（DJIA）数据集
- en: Understanding the NYTimes news article dataset
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解《纽约时报》新闻文章数据集
- en: Understanding the DJIA dataset
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解道琼斯工业平均指数数据集
- en: 'In the DJIA dataset, we have seven data attributes. They are quite easy to
    understand, so let''s look at each of them one by one:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在道琼斯工业平均指数数据集中，我们有七个数据属性。它们很容易理解，所以让我们逐一查看它们：
- en: '`Date`: The first column indicates the date in the YYYY-MM-DD format when you
    see data in the .csv file.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Date`: 第一列表示在.YYMMDD格式的.csv文件中看到的数据的日期。'
- en: '`Open`: This indicates the price at which the market opens, so it is the opening
    value for the DJIA index for that particular trading day.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Open`: 这表示市场开盘时的价格，因此它是特定交易日的道琼斯工业平均指数的开盘值。'
- en: '`High`: This is the highest price for the DJIA index for a particular trading
    day.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`High`: 这是特定交易日的道琼斯工业平均指数的最高价格。'
- en: '`Low`: This is the lowest price for DJIA index for a particular trading day.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Low`: 这是特定交易日的道琼斯工业平均指数的最低价格。'
- en: '`Close`: The price of DJIA index at the close of the trading day.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Close`: 交易日的道琼斯工业平均指数收盘价。'
- en: '`Adj close`: The adjusted closing price (adj close price) uses the closing
    price as a starting point and takes into account components such as dividends,
    stock splits, and new stock offerings. The adj close price represents the true
    reflection of the DJIA index. Let me give you an example so that you can understand
    the adj close price better: if a company offers a dividend of $5 per share, and
    if the closing price of that company share is $100, then the adj close price will
    become $95\. So, the adj close price considers various factors and, based on them,
    generates the true value of the company''s stock. Here, we are looking at the
    DJIA index value so, most of the time, the closing price and the adj close price
    are the same.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Adj close`: 调整后的收盘价（adj close price）以收盘价为基础，并考虑诸如股息、股票分割和新股票发行等因素。调整后的收盘价代表了道琼斯工业平均指数的真实反映。让我给你举一个例子，以便你能更好地理解调整后的收盘价：如果一家公司提供每股5美元的股息，并且该公司的股票收盘价为100美元，那么调整后的收盘价将变为95美元。因此，调整后的收盘价考虑了各种因素，并根据这些因素生成公司股票的真实价值。在这里，我们关注的是道琼斯工业平均指数的价值，所以，大多数情况下，收盘价和调整后的收盘价是相同的。'
- en: '`Volume`: These values indicate the number of index traded on exchange for
    a particular trading day.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Volume`: 这些值表示特定交易日在交易所交易的数量。'
- en: These are the basic details of the DJIA index dataset. We use historical data
    and try to predict future movement in the DJIA index.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是道琼斯工业平均指数数据集的基本细节。我们使用历史数据并尝试预测道琼斯工业平均指数的未来走势。
- en: In the next section, we will look at the NYTimes news article dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看《纽约时报》新闻文章数据集。
- en: Understanding the NYTimes news article dataset
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解《纽约时报》新闻文章数据集
- en: 'We have used the NYTimes developer API and collected the news articles in a
    JSON form, so, here, we will look at the JSON response so we can identify the
    data attributes that are the most important and that we can focus on. In the next
    figure, you can see the JSON response that we get from the NYTimes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了《纽约时报》开发者API，并以JSON形式收集了新闻文章，因此，在这里，我们将查看JSON响应，以便我们可以识别出最重要的数据属性，并集中关注。在下一张图中，你可以看到我们从《纽约时报》得到的JSON响应：
- en: '![Understanding the NYTimes news article dataset](img/B08394_02_04.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![理解《纽约时报》新闻文章数据集](img/B08394_02_04.jpg)'
- en: 'Figure 2.4: JSON response for news articles using the NYTimes developer tool'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：使用《纽约时报》开发者工具的新闻文章的JSON响应
- en: 'In this figure, we can see the JSON response for a single news article. As
    you can see, there is a main data attribute response that carries all other data
    attributes. We will focus on the data attributes that are given inside the docs
    array. Don''t worry; we will not use all the data attributes. Here, we will focus
    on the following data attributes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，我们可以看到单篇新闻文章的JSON响应。正如你所见，有一个主要数据属性响应，它携带了所有其他数据属性。我们将关注docs数组内部给出的数据属性。不用担心；我们不会使用所有数据属性。在这里，我们将关注以下数据属性：
- en: '`type_of_material`: This attribute indicates that a particular news article
    is derived from a particular kind of source, whether it''s a blog, a news article,
    analysis, and so on.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_of_material`: 此属性表示特定的新闻文章是从某种特定的来源中提取的，无论是博客、新闻文章、分析等等。'
- en: '`headlines`: The headline data attribute has the two sub-data attributes. The
    main data attribute contains the actual headline of the news and the kicker data
    attribute is convey the highlight of the article.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`headlines`: 标题数据属性有两个子数据属性。主要数据属性包含新闻的实际标题，而kicker数据属性则传达文章的亮点。'
- en: '`pub_date`: This data attribute indicates the publication of the news article.
    You can find this attribute in the second-last section of the doc array.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pub_date`: 这个数据属性表示新闻文章的发布日期。你可以在文档数组的倒数第二部分找到这个属性。'
- en: '`section_name`: This data attribute appeared in the preceding image in the
    last section. It provides the category of the news article.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`section_name`: 这个数据属性出现在前面图像的最后一部分。它提供了新闻文章的类别。'
- en: '`news_desk`: This data attribute also indicates the news category. When `section_name`
    is absent in a response, we will refer to this attribute.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`news_desk`: 这个数据属性也指示新闻类别。当响应中缺少`section_name`时，我们将参考这个属性。'
- en: As we understand data attributes properly, we should move on to the next section,
    which is the data preprocessing and data analysis part.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正确理解数据属性后，我们应该继续到下一部分，即数据预处理和数据分析部分。
- en: Data preprocessing and data analysis
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和数据分析
- en: In this section, we will mainly cover data preprocessing and data analysis.
    As a part of data preprocessing, we are preparing our training dataset. You may
    be wondering what kind of data preparation I'm talking about, considering we already
    have the data. Allow me to tell you that we have two different datasets and both
    datasets are independent. So, we need to merge the DJIA dataset and NYTimes news
    article dataset in order to get meaningful insights from these datasets. Once
    we prepare our training dataset, we can train the data using different machine
    learning (ML) algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将主要介绍数据预处理和数据分析。作为数据预处理的一部分，我们正在准备我们的训练数据集。你可能想知道我所说的数据准备是什么，考虑到我们已经有数据了。让我告诉你，我们有两个不同的数据集，并且这两个数据集都是独立的。因此，我们需要合并道琼斯工业平均指数数据集和纽约时报新闻文章数据集，以便从这些数据集中获得有意义的见解。一旦我们准备好了训练数据集，我们就可以使用不同的机器学习（ML）算法来训练数据。
- en: Now let's start the coding to prepare the training dataset. We will be using
    `numpy`, `csv`, `JSON`, and `pandas` as our dependency libraries. Here, our code
    is divided into two parts. First, we will prepare the dataset for the DJIA index
    dataset and then we will move to the next part, which is preparing the NYTimes
    news article dataset. During the preparation of the training dataset, we will
    code the basic data analysis steps as well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始编写代码来准备训练数据集。我们将使用`numpy`、`csv`、`JSON`和`pandas`作为我们的依赖库。在这里，我们的代码分为两部分。首先，我们将为道琼斯指数数据集准备数据集，然后我们将转到下一部分，即准备纽约时报新闻文章数据集。在准备训练数据集的过程中，我们将编写基本的数据分析步骤。
- en: Preparing the DJIA training dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备道琼斯工业平均指数训练数据集
- en: 'You can see the code snippet in the following screenshot. You can find the
    code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的屏幕截图中看到代码片段。你可以在以下GitHub链接中找到代码：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb)。
- en: '![Preparing the DJIA training dataset](img/B08394_02_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![准备道琼斯工业平均指数训练数据集](img/B08394_02_05.jpg)'
- en: 'Figure 2.5: Code snippet for preparing the DJIA dataset'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：准备道琼斯工业平均指数数据集的代码片段
- en: 'As you can see in the preceding code snippet, we are reading the csv file that
    we downloaded from the Yahoo Finance page earlier. After that, we convert the
    data into a list format. We also separated the header and actual data from the
    list. Once we have the data in list format, we convert the data into a numpy array.
    We have selected only three columns from the DIJA dataset, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的代码片段中看到的，我们正在读取我们从雅虎财经页面下载的csv文件。之后，我们将数据转换为列表格式。我们还从列表中分离了标题和实际数据。一旦我们有了列表格式的数据，我们将数据转换为numpy数组。我们从DIJA数据集中选择了仅三个列，如下所示：
- en: Date
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期
- en: Close price
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收盘价
- en: Adj close price
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后收盘价
- en: 'You may have one question in mind: why have we considered only close price
    and Adj close price from the DJIA csv file? Let me clarify: as we know that open
    price is mostly a nearby value of the last day''s close price, we haven''t considered
    the open price. We haven''t considered the high price and low price because we
    don''t know in which particular timestamp these high and low prices occurred.
    For the first iteration, it is quite complicated to predict when the stock index
    reach a high or low value, so, in the meantime, we ignore these two columns. We
    are mainly interested in the overall trend for the DJIA index. If we figure out
    the trend precisely, we can predict the high and low price values later on. Here,
    we restrict our goal to predicting the closing prices for the DJIA index for future
    trading days.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能有一个疑问：为什么我们只考虑了DJIA csv文件中的收盘价和调整后的收盘价？让我澄清一下：正如我们所知，开盘价通常是前一天收盘价的一个附近值，所以我们没有考虑开盘价。我们没有考虑最高价和最低价，因为我们不知道这些最高价和最低价发生在哪个特定的时间戳。对于第一次迭代来说，预测股票指数何时达到高或低值相当复杂，所以，在此期间，我们忽略这两个列。我们主要对DJIA指数的整体趋势感兴趣。如果我们能精确地找出趋势，我们就可以在以后预测高和低的价格值。在这里，我们限制我们的目标为预测未来交易日的DJIA指数的收盘价。
- en: 'Now back to the coding part: we built the pandas dataframe in such a way that
    the date column acts as the index column, and close price and adj close price
    are the two other columns of the dataset. You can see the output of the dataframe
    defined in the form of the `df` variable in the code snippet given in *Figure
    2.5*. You can see the output of dataframe df in the following figure:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到编码部分：我们以这种方式构建了pandas数据框，使得日期列作为索引列，而收盘价和调整后的收盘价是数据集的两个其他列。您可以在代码片段中看到以`df`变量形式定义的数据框的输出，该代码片段见*图2.5*。您可以在以下图中看到数据框df的输出：
- en: '![Preparing the DJIA training dataset](img/B08394_02_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![准备DJIA训练数据集](img/B08394_02_06.jpg)'
- en: 'Figure 2.6: Output of pandas dataframe, which is defined as the *df* variable
    in the code snippet in Figure 2.5'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：pandas数据框的输出，该数据框在图2.5的代码片段中定义为*df*变量
- en: Hopefully now you have a clear understanding of the kind of steps we have followed
    so far. We have created the basic dataframe, so now we will move on to the basic
    data analysis part for a DJIA dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在您已经清楚地理解了我们迄今为止所遵循的步骤。我们已经创建了基本的数据框，所以现在我们将继续进行DJIA数据集的基本数据分析部分。
- en: Basic data analysis for a DJIA dataset
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DJIA数据集的基本数据分析
- en: 'In this section, we will perform basic data analysis on a DJIA dataset. This
    dataset has the date value, but if you look at the values of the date carefully,
    then you will see that there are some missing dates. Suppose data is missing for
    30-12-2006, 31-12-2006, 1-1-2007, and many other dates. In such cases, we will
    add the date values that are missing. You can refer to the code snippet given
    in *Figure 2.7*, as well as find the code for this on this GitHub: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对DJIA数据集进行基本数据分析。这个数据集有日期值，但如果您仔细查看日期值，您会发现有一些缺失的日期。假设数据缺失于2006年12月30日、31日、2007年1月1日以及许多其他日期。在这种情况下，我们将添加缺失的日期值。您可以参考图2.7中的代码片段，以及在此GitHub上找到此代码：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb)。
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![DJIA数据集的基本数据分析](img/B08394_02_07.jpg)'
- en: 'Figure 2.7: Code snippet for adding all the missing date values in the DJIA
    dataset'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：在DJIA数据集中添加所有缺失日期值的代码片段
- en: As you can see in the preceding figure, we come across another challenge after
    adding these missing date values. We have added the date value, but there is no
    close price or adj close price available corresponding to each of them, so we
    need to replace the NaN values logically, not randomly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们在添加这些缺失的日期值之后遇到了另一个挑战。我们已经添加了日期值，但是没有对应每个日期的收盘价或调整后的收盘价，因此我们需要逻辑地替换NaN值，而不是随机替换。
- en: 'In order to replace the NaN values of close price and adj close price, we will
    use the pandas interpolation functionality. We use linear interpolation to generate
    the missing values for NaN. There are many types of interpolation available, but
    here we are using linear interpolation, and the mathematical equation for linear
    interpolation is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了替换收盘价和调整后收盘价的NaN值，我们将使用pandas插值功能。我们使用线性插值生成NaN的缺失值。有几种插值类型可用，但在这里我们使用线性插值，线性插值的数学方程如下：
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_39.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![DJIA数据集的基本数据分析](img/B08394_02_39.jpg)'
- en: 'Equation 2.1: Linear interpolation math formula'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 公式2.1：线性插值数学公式
- en: If the two known points are given by the coordinates (x1,y_1) and (x_3,y_3),
    the linear interpolant is the straight line between these points.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个已知点由坐标(x1,y_1)和(x_3,y_3)给出，线性插值是这两个点之间的直线。
- en: 'You can refer to the code snippet in the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下截图中的代码片段：
- en: '![Basic data analysis for a DJIA dataset](img/B08394_02_08.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![DJIA数据集的基本数据分析](img/B08394_02_08.jpg)'
- en: 'Figure 2.8: Code snippet for basic data analysis and interpolation implementation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：基本数据分析与插值实现的代码片段
- en: The code for this is available on GitHub at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码可在GitHub上找到：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb)。
- en: As you can see in the code snippet, we haven't defined which type of interpolation
    should be performed on our dataset; in this case, linear interpolation has been
    performed by default. So after applying the linear interpolation, we can replace
    the NaN values with the actual logical values. We have also removed three records
    from the year 2006\. So now, we have a total of 3653 records.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码片段所示，我们尚未定义应在我们的数据集上执行哪种类型的插值；在这种情况下，默认执行了线性插值。因此，在应用线性插值后，我们可以用实际的逻辑值替换NaN值。我们还删除了2006年的三条记录。因此，现在我们总共有3653条记录。
- en: This is the kind of basic data preprocessing and data analysis we did for the
    DJIA index dataset. Now let's move on to the NYTimes news article dataset. We
    need to prepare the training dataset first, so let's begin with it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们为道琼斯工业平均指数（DJIA）数据集所做的基本数据预处理和数据分析。现在让我们继续到纽约时报新闻文章数据集。首先，我们需要准备训练数据集，所以让我们从这里开始。
- en: Preparing the NYTimes news dataset
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备纽约时报新闻数据集
- en: 'In this section, we will see how we can prepare the NYTimes news dataset. We
    have downloaded the whole news article dataset but we have not put in a filtering
    mechanism for choosing news article categories. Perform the following steps when
    preparing the NYTimes dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何准备纽约时报新闻数据集。我们已经下载了整个新闻文章数据集，但我们还没有添加选择新闻文章类别的过滤机制。在准备纽约时报数据集时，请执行以下步骤：
- en: Converting publication date into the YYYY-MM-DD format.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将发布日期转换为YYYY-MM-DD格式。
- en: Filtering news articles by their category.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过类别过滤新闻文章。
- en: Implementing the filter functionality and merge the dataset.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现过滤功能并合并数据集。
- en: Saving the merged dataset in the pickle file format.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将合并后的数据集保存为pickle文件格式。
- en: So, let's start coding for each of these steps.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们开始为每个步骤编写代码。
- en: Converting publication date into the YYYY-MM-DD format
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将发布日期转换为YYYY-MM-DD格式
- en: 'First, we will convert the publication date of the news articles into the YYYY-MM-DD
    format so that we can merge DJIA and NYTimes news article datasets later on. In
    order to achieve this, you can refer to the following code snippet:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将新闻文章的发布日期转换为YYYY-MM-DD格式，以便我们可以在以后合并道琼斯工业平均指数（DJIA）和纽约时报新闻文章数据集。为了实现这一点，你可以参考以下代码片段：
- en: '![Converting publication date into the YYYY-MM-DD format](img/B08394_02_09.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![将发布日期转换为YYYY-MM-DD格式](img/B08394_02_09.jpg)'
- en: 'Figure 2.9: Code snippet for converting the date format of the publication
    date of the news article'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：转换新闻文章发布日期格式的代码片段
- en: Here, we have written a function that can parse and convert the publication
    date format into the necessary YYYY-MM-DD format. We will call this function later
    on when we read the JSON files in which we have stored the JSON response.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们编写了一个可以将发布日期格式解析并转换为必要的YYYY-MM-DD格式的函数。稍后当我们读取存储JSON响应的JSON文件时，我们将调用此函数。
- en: Filtering news articles by category
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过类别过滤新闻文章
- en: 'The other thing that we are going to do here is filter our news article dataset
    by news category. We have downloaded all types of news articles, but for the stock
    market price prediction application, we need news articles that belong to specific
    news categories. So, we need to implement filters that will help us extract the
    necessary subset of news articles. You can refer to the following code snippet:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要做的另一件事是按新闻类别过滤我们的新闻文章数据集。我们下载了所有类型的新闻文章，但为了股票市场价格预测应用程序，我们需要属于特定新闻类别的新闻文章。因此，我们需要实现过滤器，帮助我们提取必要的新闻文章子集。您可以在以下代码片段中参考：
- en: '![Filtering news articles by category](img/B08394_02_10.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![按类别过滤新闻文章](img/B08394_02_10.jpg)'
- en: 'Figure 2.10: Code snippet for filtering news articles by their categories'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：按类别过滤新闻文章的代码片段
- en: 'You can refer to the code provided at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下GitHub链接提供的代码：[https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).
- en: 'As shown in the preceding figure, we are extracting news articles that belong
    to the following news categories:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们正在提取属于以下新闻类别的新闻文章：
- en: Business
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业
- en: National
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国内
- en: World
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界
- en: U.S.A.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国
- en: Politics
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政治
- en: Opinion
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论
- en: Tech
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科技
- en: Science
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科学
- en: Health
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康
- en: Foreign
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外国
- en: Implementing the filter functionality and merging the dataset
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现过滤器功能并合并数据集
- en: 'Now, we need to iterate each of the JSON files and extract the news articles
    that have one of the news categories defined in the previous section. You can
    refer to the code snippet for the implementation of the filter functionality.
    In the upcoming code snippet, you can also find the implementation for merging
    the DJIA dataset and the NYTimes news articles dataset. To merge the two datasets,
    we are adding each of the news article headlines to the pandas dataframe,and from
    this we will generate our final training dataset. This functionality is shown
    in the following screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要迭代每个JSON文件，并提取上一节中定义的新闻类别之一的新闻文章。您可以参考实现过滤器功能的代码片段。在即将到来的代码片段中，您还可以找到合并道琼斯工业平均指数（DJIA）数据集和纽约时报新闻文章数据集的实现。为了合并这两个数据集，我们将每个新闻文章的标题添加到pandas数据框中，然后我们将从这个数据框生成我们的最终训练数据集。此功能在以下屏幕截图中显示：
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![实现过滤器功能并合并数据集](img/B08394_02_11.jpg)'
- en: 'Figure 2.11: Code snippet for the filtering and merging functionalities'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：过滤和合并功能的代码片段
- en: 'We have also coded a bit of the exceptional handling functionality. This is
    done so that if any JSON response does not have the value for the data attributes
    section_name, news_desk, or type_of_material, then this code will throw an exception.
    You can refer to the code snippet in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还编写了一部分异常处理功能。这样做是为了如果任何JSON响应没有data属性中的section_name、news_desk或type_of_material的值，则此代码将抛出异常。您可以在以下屏幕截图中查看代码片段：
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![实现过滤器功能并合并数据集](img/B08394_02_12.jpg)'
- en: 'Figure 2.12: Implementation of exception handling'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12：异常处理的实现
- en: 'We will consider news articles that have no `section_name` and `news_desk`
    as well. We will add all the news article headlines to our dataset and put them
    into the pandas dataframe. You can see the code snippet in the following screenshot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将考虑没有`section_name`和`news_desk`的新闻文章。我们将把所有新闻文章的标题添加到我们的数据集中，并将它们放入pandas数据框中。您可以在以下屏幕截图中看到代码片段：
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_13.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![实现过滤器功能并合并数据集](img/B08394_02_13.jpg)'
- en: 'Figure 2.13: Handling news articles that have no section_name and news_desk'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：处理没有section_name和news_desk的新闻文章
- en: 'You can see the final merged dataset in the form of the pandas dataframe, as
    shown in the following screenshot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在下面的屏幕截图中以pandas数据框的形式看到最终的合并数据集：
- en: '![Implementing the filter functionality and merging the dataset](img/B08394_02_14.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![实现过滤器功能并合并数据集](img/B08394_02_14.jpg)'
- en: 'Figure 2.14: Final merged training dataset'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14：最终合并的训练数据集
- en: Here, for each date, we correspond all the news headlines that belong to the
    business, national, world, U.S.A., politics, opinion, technology, science, and
    heath categories. We have downloaded 1,248,084 news articles, and from these articles,
    we have considered 461,738 news articles for our model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access the code using this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Saving the merged dataset in the pickle file format
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we merge the data, we need to save the data objects, so we will use the
    pickle module of Python. Pickle helps us serialize and de-serialize the data.
    The pickle dependency library is fast because the bulk of it is written in C,
    like the Python interpreter itself. Here, we save our training dataset as a `.pkl`
    file format. You can refer to the following code snippet*:*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the merged dataset in the pickle file format](img/B08394_02_15.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: Code snippet for saving data in the pickle format'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: We have saved the dataset as the `pickled_ten_year_filtered_lead_para.pkl` file.
    You can find the code on GitHub at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/datapreparation.ipynb).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will mainly focus on the feature engineering part. We
    will also perform some minor data cleaning steps. So let's jump to the next section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed earlier, we want to predict the close price for the DJIA index
    for a particular trading day. In this section, we will do feature selection based
    on our intuition for our basic prediction model for stock prices. We have already
    generated the training dataset. So, now we will load the saved .pkl format dataset
    and perform feature selection as well as minor data processing. We will also generate
    the sentiment score for each of the filtered NYTimes news articles and will use
    this sentiment score to train our baseline model. We will use the following Python
    dependencies:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: numpy
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nltk
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section has the following steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minor preprocessing
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentiment analysis
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's begin coding!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have saved the data in the pickle format, and now we need to load data from
    it. You can refer to the following code snippet*:*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_02_16.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Code snippet for loading the dataset from the pickle file'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code by clicking on this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in the dataframe output, there is a dot (.) before every article
    headline in the entire dataset, so we need to remove these dots. We will execute
    this change in the next section.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Minor preprocessing
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a part of minor preprocessing, we will be performing the following two changes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Converting the adj close prices into the integer format
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing the leftmost dot (.) from news headlines
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting adj close price into the integer format
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that the adj close price is in the form of a float format. So, here
    we will convert float values into the integer format as well as store the converted
    values as *price* attributes in our pandas dataframe. Now, you may wonder why
    we consider only the adj close prices. Bear with me for a while, and I will give
    you the reason for that. You can find the convergence code snippet in the following
    screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting adj close price into the integer format](img/B08394_02_17.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Code snippet for converting the adj close price into the integer
    format'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the second change.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Removing the leftmost dot from news headlines
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will see the implementation for removing the leftmost dot.
    We will be using the `lstrip()` function to remove the dot. You can refer to the
    code snippet in the following screenshot:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the leftmost dot from news headlines](img/B08394_02_18.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Code snippet for removing *dot* from the news article headlines'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to our next section, which is feature engineering.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature selection is one of the most important aspects of feature engineering
    and any **Machine Learning** (**ML**) application. So, here we will focus on feature
    selection. In the previous section, I raised the question of why we select only
    the *adj close price* and not the *close price.* The answer to this question lies
    in the feature selection. We select the *adj close prices* because these prices
    give us a better idea about what the last price of the DJIA index is, including
    the stock, mutual funds, dividends, and so on. In our dataset, *close prices*
    are mostly the same as the *adj close price* and in future, if we consider the
    *close price* for unseen data records, we can''t derive the *adj close price*
    because it may be equal to the *close price* or higher than the *close price,*
    The *adj close price* for DJIA index may higher than the c*lose price* because
    it will include stocks, mutual funds, dividend and so on. but we don''t know how
    much higher it will be for unseen dataset where we have just considered *close
    price*. So if we consider the *adj close price,* then we will know that the *close
    price* may be less than or equal to the *adj close price*, but not more than the
    *adj close price*. The *adj close price* is kind of maximum possible value for
    closing price. So, we have considered the *adj close price* for the development.
    For the baseline model, we will be considering the *adj close price*. We have
    renamed the column to *price*. You can refer to the following code snippet:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature engineering](img/B08394_02_19.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Code snippet for considering the adj close price as a part of
    feature selection'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, we will now perform sentiment analysis on the news article dataset.
    We can use the sentiment score when we train our model. So, let's move on to the
    sentiment analysis part.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis of NYTimes news articles
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement sentiment analysis, we are using the nltk inbuilt sentiment
    analysis module. We will obtain negative, positive, and compound sentiment scores.
    We have used a lexicon-based approach. In the lexicon-based approach, words of
    each sentence are analyzed, and based on the `sentiwordnet` score, each word is
    given a specific sentiment score; then, the aggregate sentence level score is
    decided.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sentiwordnet is the dictionary which contain sentiment score for words.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover details related to sentiment analysis in [Chapter 5](ch05.xhtml
    "Chapter 5. Sentiment Analysis"), *Sentiment Analysis*. You can refer to the following
    sentiment analysis code snippet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentiment analysis of NYTimes news articles](img/B08394_02_20.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: Sentiment analysis code snippet'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: All scores generated by the preceding code are stored in the dataframe, so you
    can see the aggregate score of news article headlines in the following screenshot*:*
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentiment analysis of NYTimes news articles](img/B08394_02_21.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: Aggregate sentiment analysis score stored in the dateframe'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, we will obtain the sentiment score for the NYTimes
    news articles dataset and combine these sentiment scores as part of the training
    dataset. So far, we have done minor preprocessing, selected the data attribute
    as per our intuition, and generated the sentiment score. Now, we will select the
    machine learning algorithm and try to build the baseline model. So, let's move
    on to the next section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Machine Learning algorithm
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will choose the Machine Learning (ML) algorithm based on
    our intuition and then perform training using our training dataset. This is the
    first model for this particular chapter, so the trained model is our baseline
    model, which we will improve later on. So, let's decide which kind of ML algorithm
    suits this stock price prediction application.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The stock price prediction application is a time-series analysis problem, where
    we need to predict the next point in the time series. This prediction activity
    is similar to linear regression, so we can say that this application is a kind
    of regression problem and any algorithm from the regression family should work.
    Let's select the ensemble algorithm, which is *RandomForestRegressor*, in order
    to develop our baseline model. So let's train our baseline model, and, based on
    the result of that model, we will modify our approach.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Training the baseline model
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you know, we have selected the **RandomForestRegressor** algorithm. We will
    be using the scikit-learn library to train the model. These are the steps we need
    to follow:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training and testing dataset
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting prediction labels for the training and testing dataset
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting sentiment scores into the numpy array
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the ML model
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's implement each of these steps one by one.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training and testing dataset
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have 10 years of data values. So for training purposes, we will be using
    8 years of the data, which means the dataset from 2007 to 2014\. For testing purposes,
    we will be using 2 years of the data, which means data from 2015 and 2016\. You
    can refer to the code snippet in the following screenshot to implement this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the training and testing dataset](img/B08394_02_22.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: Splitting the training and testing dataset'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, our training dataset has been
    stored in the train dataframe and our testing dataset has been stored in the test
    dataframe.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Splitting prediction labels for the training and testing datasets
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we split the training and testing dataset, we also need to store the adj
    close price separately because we need to predict these *adj close prices* (indicated
    in the code as `prices`); these price values are labels for our training data,
    and this training becomes supervised training as we will provide the actual price
    in the form of labels. You can refer to the following code for the implementation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting prediction labels for the training and testing datasets](img/B08394_02_23.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: Splitting the prediction labels for training and testing datasets'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Here, all attributes except the price are given in a feature vector format and
    the price is in the form of labels. The ML algorithm takes this feature vector,
    labels the pair, learns the necessary pattern, and predicts the price for the
    unseen data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Converting sentiment scores into the numpy array
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start the training, there is one last, necessary point that we need
    to keep in mind: we are converting the sentiment analysis scores into the numpy
    array format. This is because once we set the price attribute as a prediction
    label, our features vector will contain only the sentiment scores and date. So
    in order to generate a proper feature vector, we have converted the sentiment
    score into a numpy array. The code snippet to implement this is provided in the
    following screenshot:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting sentiment scores into the numpy array](img/B08394_02_24.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.24: Code snippet for converting sentiment analysis score into the
    numpy array'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the code snippet, we have performed the same conversion
    operation for both training the dataset and testing the dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that if you get a value error, check the dataset because there may be a
    chance that a column in the dataset has a blank or null value.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's train our model!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Training of the ML model
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first iteration, we use the RandomForestRegressor algorithm, which is
    provided as part of the scikit-learn dependency. You can find the code for this
    in the following screenshot:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Training of the ML model](img/B08394_02_25.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.25: Code snippet for training using RandomForestRegressor'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, we have used all the default values
    for our hyperparameters. For a more detailed description regarding hyperparameters,
    you can refer to [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Now that our model has been trained, we need to test it using our testing dataset.
    Before we test, let's discuss the approach we will take to test our model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will understand the testing matrix and visualization approaches
    to evaluate the performance of the trained ML model. So let''s understand both
    approaches, which are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The default testing matrix
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visualization approach
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default testing matrix
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are using the default score API of scikit-learn to check how well the ML
    is performing. In this application, the score function is the coefficient of the
    sum of the squared error. It is also called the coefficient of R2, which is defined
    by the following equation:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_40.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Here, *u* indicates the residual sum of squares. The equation for *u* is as
    follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_41.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'The variable *v* indicates the total sum of squares. The equation for *v* is
    as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![The default testing matrix](img/B08394_02_42.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: The best possible score is 1.0, and it can be a negative score as well. A negative
    score indicates that the trained model can be arbitrarily worse. A constant model
    that always predicts the expected value for label *y*, disregarding the input
    features, will produce an R2 score of 0.0.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain the score, we just need to call the score function. The code
    for testing will be the same as that in the *Test baseline model* section. Now
    let's take a look at another testing approach that is quite helpful in understanding
    the output with respect to true testing labels. So, let's check that out!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The visualization approach
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be exploring an effective and intuitive approach, which
    is the **visualization** of the predicted output versus real output. This approach
    gives you a lot of insight as the graphs are easy to understand and you can decide
    the next steps to improve the model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In this application, we will be using the actual prices from the testing dataset
    and the predicted prices for the testing dataset, which will indicate how good
    or bad the predictions are. You will find the code and graph for this process
    in the next section, named *Testing the baseline model*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Testing the baseline model
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing our testing approach so that we can
    evaluate our model''s accuracy. We will first generate the output prediction and
    then we''ll start testing it. We will be implementing the following steps here:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Generating and interpreting the output
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating the score
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing the output
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating and interpreting the output
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate the prediction, we are using the `treeinterpreter` library. We
    are predicting the price value for each of our testing dataset records using the
    following code:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating and interpreting the output](img/B08394_02_26.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.26: Code snippet for generating the prediction'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Here, *prediction* is the array in which we have elements that are the corresponding
    predicted *adj close price* for all records of the testing dataset. Now, we will
    compare this predicted output with the actual *adj close price* of the testing
    dataset. By doing this, we will get to know how accurately our first model is
    predicting the *adj close price*. In order to evaluate further, we will generate
    the accuracy score.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Generating the accuracy score
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will generate the accuracy score as per the equations provided
    in the *default testing matrix* section. The code for this is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the accuracy score](img/B08394_02_27.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.27: Code snippet for generating the score for the test dataset'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding code snippet, our model is not doing too well.
    At this point, we don't know what mistakes we've made or what went wrong. This
    kind of situation is common when you are trying to solve or build an ML model.
    We can grasp the problem better using visualization techniques.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the output
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the visualization graph in this section. Using the graph,
    we will identify the kind of error we have committed so that we can fix that error
    in the next iteration. We will plot a graph where the *y-axis* represents the
    *adj close prices* and the *x-axis* represent the *dates*. We plot the *actual
    prices* and *predicted prices* on the graph so that we will get a brief idea about
    how our algorithm is performing. We will use the following code snippet to generate
    the graph:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the output](img/B08394_02_28.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.28: Code snippet for generating graph for predicted prices vs actual
    prices.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding graph, the top single line (orange color)
    represents the actual price and the messy spikes (blue color) below the line represent
    the predicted prices. From this plot, we can summarize that our model can't predict
    the proper prices. Here, you can see that the actual prices and predicted prices
    are not aligned with each other. We need to fix this issue. There are some techniques
    that we can try, such as alignment, smoothing, and trying a different algorithm.
    So, let's cover the problems of this approach in the next section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can access the entire code on this topic from the GitHub link at [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Exploring problems with the existing approach
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing the problems of the existing approach.
    There are mainly three errors we could have possibly committed, which are listed
    as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoothing
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trying a different ML algorithm
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each of the points one by one.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in the graph, our actual price and predicted prices are not
    aligned with each other. This becomes a problem. We need to perform alignment
    on the price of the stocks. We need to consider the average value of our dataset,
    and based on that, we will generate the alignment. You can understand more about
    alignment in upcoming section called *Alignment-based approach*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second problem I feel we have with our first model is that we haven't applied
    any smoothing techniques. So for our model, we need to apply smoothing techniques
    as well. We will be using the **Exponentially Weighted Moving Average** (**EWMA**)
    technique for smoothing*.* This technique is used to adjust the variance of the
    dataset.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Trying a different ML algorithm
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our model, we have used the `RandomForestRegressor` algorithm. But what
    if we try the same thing with our model using a different algorithm, say *Logistic
    Regression*? In the next section, you will learn how to implement this algorithm—after
    applying the necessary alignment and smoothing, of course.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the possible problems with our first baseline approach. Now, we
    will try to understand the approach for implementing the alignment, smoothing,
    and `Logistic Regression` algorithms.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the revised approach
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be looking at the key concepts and approaches for alignment
    and smoothing. It is not that difficult to implement the *Logistic Regression*
    algorithm; we will be using the scikit-learn API. So, we will start with understanding
    the concepts and approaches for implementation.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concepts and approaches
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will discuss how alignment and smoothing will work. Once we understand
    the technicality behind alignment and smoothing, we will focus on the Logistic
    Regression-based approach.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Alignment-based approach
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using this approach, we will be increasing the prices using a constant value
    so that our predicted price and actual price in testing the dataset will be aligned.
    Suppose we take 10 days into consideration. We will generate the average of the
    value of the prices. After that, we generate the average value for the prices
    that have been predicted by the first ML model. Once we generate both average
    values, we need to subtract the values, and the answer is the alignment value
    for those `10` days.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an intuitive working example that will help clear your vision. Consider
    10 days from January 2, 2015, to January 11, 2015\. For each record, you will
    take the average value for the actual price. Suppose the number will come to 17,676
    and the average of predicted price value will be 13,175\. In this case, you will
    get a difference of 4,501, which is the value for the alignment. We will add this
    value to our testing dataset so that testing price values and predicted price
    values will be aligned. You will find the code implementation in the *Implement
    revised approach* section.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing-based approach
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, we will be using EWMA. **EWMA** stands for **Exponentially
    Weighted Moving Average**. The smoothing approach is based on the weighted average
    concept. In general, a weighted moving average is calculated by the following
    equation:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Smoothing-based approach](img/B08394_02_43.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x[t]* is the input and *y[t]* is the output. Weights are calculated
    using the following equations:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![Smoothing-based approach](img/B08394_02_29.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.29: Equation for calculating the weight for EWMA'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Here, α is the smoothing constant. If the value of the smoothing constant is
    high, then it will be close to the actual value, and if the smoothing constant
    is low, then it will be smoother but not close to the actual value. Typically,
    in statistics the smoothing constant ranges between 0.1 and 0.3\. Therefore, we
    can generate the smoothed value using the smoothing constant.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a working example. Take a smoothing constant = 0.3; if the actual
    value is 100 and the predicted value is 110, then the smoothed value can be obtain
    using this equation, which is (smoothing constant * actual value ) + (1- smoothing
    constant) * predicted value. The value that we will obtain is *(0.3* 100) + (1-0.3)*110
    = 107*. For more information, you can refer to [http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We will see the actual code-level implementation in the Implement revised approach
    section. pandas already has an API, so we can easily implement EWMA.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression-based approach
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing the Logistic Regression algorithm is a simple task because we just
    need to use the scikit-learn API. For the testing dataset, we will apply alignment
    and smoothing. After evaluating accuracy, we will decide whether we need to change
    the ML algorithm or not. We started with our intuition and slowly we improved
    our approaches. I don't really need to explain the Logistic Regression algorithm
    itself, but during the implementation, we will discuss the important points.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to move on to the implementation part of our revised approach.
    So, let's take a look at the next section.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the three parts of implementation, which are
    as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the problem with the revised approach
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we are implementing the following:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoothing
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have already discussed the approach and key concepts, so now we just focus
    on the code part here. You can find all the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Implementing alignment
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The alignment is performed on the testing dataset. You can refer to the following
    code snippet:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing alignment](img/B08394_02_30.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.30: Code snippet for alignment on the test dataset'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, we obtain a difference of 10
    days *adj close price* using the average price of the last 5 days and the average
    price of the predicted upcoming 5 days in order to align the test data. Here,
    we also convert the date from the string into the date format. As you can see,
    5096.99 is the difference in the test prediction price, which we will add to our
    predicted *adj close price* value. We have generated the graph again so we can
    easily understand that the alignment approach is implemented nicely. You can refer
    to the following code snippet:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing alignment](img/B08394_02_31.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.31: Code snippet of the graph for the alignment approach'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code snippet, the alignment graph shows that
    our testing dataset price and predicted prices are aligned. The benefit of the
    aligned graph is that now we can define in a precise manner that `RandomForestRegressor`
    didn't do its job with high accuracy as its performance was not great for all
    data records. The alignment graph gave us a crystal clear picture of our previous
    iteration. So when we train the logistic regression now, we will evaluate the
    predicted prices using alignment.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Implementing smoothing
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are using the pandas EWMA API using 60 days'' time span and frequency time
    *D.* This "D" indicates that we are dealing with the datetime format in our dataset.
    You can see the code implementation in the following code snippet:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_32.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.32: Code snippet for EWMA smoothing'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also generating the graph in which we put the *predicted price, average
    predicted price, actual price*, and *average actual price*. You can refer to the
    following code and graph:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_33.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.33: Code snippet for generating the graph after smoothing'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, you can see that after smoothing the *average predicted price,*
    the curve follows the *actual price* trend. Although the accuracy is not great,
    we will move toward a positive direction. The smoothing technique will be useful
    for us if we want to tune our algorithm. You can refer to the following graph
    for the *average predicted price versus actual price:*
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing smoothing](img/B08394_02_34.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.34: Code snippet for the graph, indicating average_predicted_price
    versus actual_price'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: By referring to the preceding graph, we can indicate that we apply alignment
    and smoothing because it helps tune our ML model for the next iteration.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logistic regression
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing logistic regression. Take a look at
    the following screenshot:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing logistic regression](img/B08394_02_35.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.35: Code snippet for logistic regression'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have trained the model again using the logistic regression ML algorithm.
    We have also implemented alignment and smoothing for the test dataset. Now, let's
    evaluate the logistic regression model.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have tested the logistic regression model. You can refer to the visualization
    in the form of graphs that show that this revised approach is certainly better
    than *RandomForesRegressor (without alignment and smoothing),* but it is not up
    to the mark:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_02_36.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.36: Year-wise prediction graph'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, we have generated a year-wise graph
    for *logistic Regression*; we can see a slight improvement using this model. We
    have also used alignment and smoothing, but they are not too effective.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's discuss what the problems with this revised approach are, and then
    we can implement the best approach.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem with the revised approach
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss why our revised approach doesn't give us good
    results. ML models don't work because datasets are not normalized. The second
    reason is that even after alignment and smoothing, the *RandomForestRegression*
    ML model faces an overfitting issue. For the best approach, we need to handle
    normalization and overfitting. We can solve this issue using a neural network-based
    ML algorithm. So in our last iteration, we will develop the neural network that
    can give us the best accuracy.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we are going to implement the neural network-based algorithm **multilayer
    perceptron** (**MLP**). You can refer to the following code snippet:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '![The best approach](img/B08394_02_37.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.37: Code snippet for multilayer perceptron'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see that we are using the Relu activation function, and the gradient
    descent solver function is ADAM. We are using a learning rate of 0.0001\. You
    can evaluate the result by referring to the following graph:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![The best approach](img/B08394_02_38.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.38: Code snippet for generating the graph for the actual and predicted
    prices'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph shows that all the data records'' predicted prices follow the actual
    price pattern. You can say that our MLP model works well to predict the stock
    market prices. You can find the code at this GitHub link: [https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb](https://github.com/jalajthanaki/stock_price_prediction/blob/master/Stock_Price_Prediction.ipynb).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to predict stock prices. We covered the different
    machine learning algorithms that can help us in this. We tried Random Forest Regressor,
    Logistic Regression, and multilayer perceptron. We found out that the multilayer
    perceptron works really well. I really want to discuss something beyond what we
    have done so far. If you are under the impression that using the sentiment analysis
    of news and predictive methods, we can now correctly predict the stock market
    price with a hundred percent accuracy, then you would be wrong. We can't predict
    stock prices with a hundred percent accuracy. Many communities, financial organizations,
    and academic researchers are working in this direction in order to make a stock
    market price predictive model that is highly accurate. This is an active research
    area.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: So if you are interested in research and freelancing, then you can join some
    pretty cool communities. There are two communities that are quite popular. One
    of these is quantopian ([https://www.quantopian.com/](https://www.quantopian.com/)).
    In this community, you can submit your stock price prediction algorithm, and if
    it outperforms other competitors' algorithms, then you will win a cash price,
    and if you get the license for your algorithm, then you get some profit from transactions
    that will be done through your licensed algorithm. The second community is numer.ai
    ([https://numer.ai/](https://numer.ai/)). This community is similar to quantopian.
    So, the possibilities of this application are limitless. Both communities offer
    some great tutorials. So try something different, and hopefully you will come
    up with a great algorithm.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will tap the retail or e-commerce domain and try to
    figure out some interesting facts about the user behavior dataset and users' social
    footprint. This will help us understand how the company should change their website
    or some functionality on the website. What are the chances of the email campaign
    going well and which type of users will respond to this campaign? Keep reading
    this book! We will discuss all these things in the next chapter.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
