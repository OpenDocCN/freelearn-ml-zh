["```py\n    Select and perform an action a\n    Observe two things: reward r and new state s'\n    Q [s, a] = Q [s, a] + α (r + γmaxa' Q [s', a'] - Q [s, a])\n    s = s'\n    ```", "```py\n$ python Learner.py\n\n```", "```py\n    Choose an action a       \n    with probability ε we need to select a random action       \n    otherwise we need to select a = argmaxa'Q(s,a')   \n    Perform action a   \n    Check reward r and new state s'   \n    store the gameplay experience <s, a, r, s'> in replay memory D   \n    sample random transitions <ss, aa, rr, ss'> from replay memory D   \n    calculate target for each minibatch transition       \n    if ss' is terminal state then tt = rr       \n    otherwise tt = rr + γmaxa'Q(ss', aa')   \n    We need to train the Q network using (tt - Q(ss, aa))^2 as loss   \n    s = s'\n    until terminated\n    ```", "```py\n$ python me_Pong.py\n```", "```py\n$ python deep_q_network.py\n```"]