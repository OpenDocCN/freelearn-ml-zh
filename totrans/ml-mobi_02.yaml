- en: Supervised and Unsupervised Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习算法
- en: In the previous chapter, we got some insight into the various aspects of machine
    learning and were introduced to the various ways in which machine learning algorithms
    could be categorized. In this chapter, we will go a step further into machine
    learning algorithms and try to understand supervised and unsupervised learning
    algorithms. This categorization is based on the learning mechanism of the algorithm,
    and is the most popular.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了机器学习的各个方面，并介绍了机器学习算法的分类方法。在本章中，我们将进一步探讨机器学习算法，并尝试理解监督学习和无监督学习算法。这种分类基于算法的学习机制，是最流行的。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: An introduction to the supervised learning algorithm in the form of a detailed
    practical example to help understand it and its guiding principles
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以详细实际案例的形式介绍监督学习算法，以帮助理解其原理和指导原则
- en: 'The key supervised learning algorithms and their application areas:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的监督学习算法及其应用领域：
- en: Naive Bayes
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Linear regression
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support vector machines
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Random forest
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: An introduction to the unsupervised learning algorithm in the form of a detailed
    practical example to help understand it
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以详细实际案例的形式介绍无监督学习算法，以帮助理解它
- en: 'The key unsupervised learning algorithms and their application areas:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的无监督学习算法及其应用领域：
- en: Clustering algorithms
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Association rule mapping
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则映射
- en: A broad overview of the different mobile SDKs and tools available to implement
    these algorithms in mobile devices
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不同移动 SDK 和工具的广泛概述，这些工具可用于在移动设备上实现这些算法
- en: Introduction to supervised learning algorithms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习算法简介
- en: Let's look at supervised learning for simple day-to-day activities. A parent
    asks their 15-year-old son to go to the store and get some vegetables. They give
    him a list of vegetables, say beets, carrots, beans, and tomatoes, that they want
    him to buy. He goes to the store and is able to identify the list of vegetables
    as per the list provided by his mother from all the other numerous varieties of
    vegetables present in the store and put them in his cart before going to the checkout.
    How was this possible?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看简单的日常生活中的监督学习。一位父亲要求他们的 15 岁儿子去商店买一些蔬菜。他们给他一份蔬菜清单，比如说甜菜、胡萝卜、豆类和番茄，他们希望他去买。他去了商店，能够根据母亲提供的清单识别出蔬菜，并将它们放入购物车中，然后去结账。这是怎么做到的？
- en: 'Simple. The parent had provided enough training to the son by providing instances
    of each and every vegetable, which equipped him with sufficient knowledge of the
    vegetables. The son used the knowledge he has gained to choose the correct vegetables.
    He used the various attributes of the vegetables to arrive at the correct class
    label of the vegetable, which, in this case, is the name of the vegetable. The
    following table gives us a few of the attributes of the vegetables present in
    the list, by means of which the son was able to recognize the class label, that
    is, the vegetable name:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简单。父亲通过提供每种蔬菜的实例来为儿子提供了足够的训练，这使他具备了足够的蔬菜知识。儿子使用他获得的知识来选择正确的蔬菜。他使用蔬菜的各种属性来确定蔬菜的正确类别标签，在这种情况下，是蔬菜的名称。以下表格给出了列表中蔬菜的一些属性，通过这些属性，儿子能够识别出类别标签，即蔬菜名称：
- en: '| **Vegetable name =****class label** | **Carrots** | **Beets** | **Beans**
    | **Tomatoes** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **蔬菜名称 = 类别标签** | **胡萝卜** | **甜菜** | **豆类** | **番茄** |'
- en: '| Attribute 1 = Color | Orange | Pink | Green | Red |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 属性 1 = 颜色 | 橙色 | 粉色 | 绿色 | 红色 |'
- en: '| Attribute 2 = Shape | Cone | Round | Stick | Round |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 属性 2 = 形状 | 圆锥形 | 圆形 | 棒状 | 圆形 |'
- en: '| Attribute 3 = Texture | Hard | Hard | Soft | Soft and juicy |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 属性 3 = 纹理 | 硬 | 硬 | 软 | 软且多汁 |'
- en: '| Attribute 4 = Size | 10 cm in length | 3 cm radius | 10 cm in length | 3
    cm radius |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 属性 4 = 大小 | 10 厘米长 | 3 厘米半径 | 10 厘米长 | 3 厘米半径 |'
- en: '| Attribute 5 = Taste | Sweet | Sweet | Bland | Sweet and sour |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 属性 5 = 味道 | 甜 | 甜 | 平淡 | 甜酸 |'
- en: 'We just got introduced to supervised learning. We will relate this activity
    to the key steps of machine learning:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚介绍了监督学习。我们将此活动与机器学习的关键步骤联系起来：
- en: '**Define the ML problem**: Purchasing the correct classes of vegetables from
    all the classes of vegetables present in the store, based on the training and
    experience already gained on different attributes of the vegetables.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义机器学习问题**：根据在蔬菜的不同属性上已经获得的经验和训练，从商店中所有类别的蔬菜中购买正确的蔬菜类别。'
- en: '**Prepare/gather the data and train the model**: The 15-year-old son has already
    been trained with sufficient knowledge of all the vegetables. This knowledge of
    all the different types of vegetables he has seen and eaten, and of their attributes
    and features, forms the historical training data for the problem, for the model—the
    15-year-old son.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备/收集数据并训练模型**：15岁的儿子已经接受了关于所有蔬菜的充分知识训练。他对所见所吃的各种蔬菜及其属性和特征的知识，构成了该问题的历史训练数据，对于模型——15岁的儿子来说。'
- en: '**Evaluate the model**: The son is asked to purchase a few vegetables from
    the store. This is the test set provided to him to evaluate the model. The task
    of the model now is to identify the correct class label of the vegetables from
    the store based on the list provided.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估模型**：儿子被要求从商店购买一些蔬菜。这是提供给他的测试集，用于评估模型。模型现在的任务是根据提供的列表识别商店中蔬菜的正确类别标签。'
- en: There may be errors in the identification and purchase of correct vegetables
    in some cases. For example, the son might purchase double beans (a variant of
    beans) instead of ordinary beans. This may be due to a lack of sufficient training
    given to him on the distinguishing features between the beans and the double beans.
    If there is such an error, the parent would retrain him with the new type of vegetable,
    so that next time, he won't make that mistake.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，识别和购买正确蔬菜时可能会出现错误。例如，儿子可能会购买双豆（豆类的一种变种）而不是普通豆子。这可能是由于他缺乏足够的关于豆子和双豆之间区别特征的训练。如果出现这样的错误，家长会重新用新的蔬菜类型对他进行训练，以便下次他不会犯同样的错误。
- en: So, we saw the basic concepts and functions of the supervised machine learning
    problem. Let's now get into the details of supervised learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到了监督机器学习问题的基本概念和功能。现在让我们深入了解监督学习的细节。
- en: Deep dive into supervised learning algorithms
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解监督学习算法
- en: Assume there are predictor attributes, *x1*, *x2*, .... *xn*, and also an objective
    attribute, `y`, for a given dataset. Then, the supervised learning is the machine learning task
    of finding the prediction function that takes as input both the predictor attributes
    and the objective attribute from this dataset, and is capable of mapping the predictive
    attributes to the objective attribute for even unseen data currently not in the
    training dataset with minimal error.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有预测属性，*x1*，*x2*，... *xn*，以及一个目标属性，`y`，对于给定的数据集。那么，监督学习就是机器学习任务，寻找一个预测函数，它接受来自数据集的预测属性和目标属性作为输入，并且能够将预测属性映射到目标属性，对于当前不在训练数据集中的未见数据，以最小的误差进行映射。
- en: 'The data in the dataset used for arriving at the prediction function is called
    the **training data** and it consists of a set of training examples where each
    example consists of an input object, `x` (typically a vector), and a desired output
    value, `Y`. A supervised learning algorithm analyzes the training data and produces
    an inferred function that maps the input to output and could also be used for
    mapping new, unseen example data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 用于得出预测函数的数据集中的数据被称为**训练数据**，它由一组训练示例组成，每个示例包含一个输入对象，`x`（通常是向量），以及一个期望的输出值，`Y`。监督学习算法分析训练数据，并产生一个推断函数，该函数将输入映射到输出，也可以用于映射新的、未见的数据示例：
- en: '*Y = f(X) + error*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y = f(X) + error*'
- en: The whole category of algorithms is called **supervised learning**, because
    here we consider both input and output variables for learning. So learning is
    supervised algorithm is by providing the input as well as the expected output
    in the training data for all the instances of training data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 整个算法类别被称为**监督学习**，因为在这里我们考虑了学习和输出变量。所以，通过提供训练数据中所有实例的输入以及预期的输出，学习是监督算法。
- en: The supervised algorithms have both predictor attributes and an objective function.
    The predictor attributes in a set of data items are those items that are considered
    to predict the objective function. The objective function is the goal of machine
    learning. This usually takes in the predictor attributes, perhaps with some other
    compute functionality, and would usually output a single numeric value.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法既有预测属性又有目标函数。数据集中的预测属性是那些被认为可以预测目标函数的项。目标函数是机器学习的目标。这通常需要预测属性，可能还有一些其他计算功能，并且通常会输出一个单一的数值。
- en: Once we have defined a proper machine learning problem that would require supervised
    learning, the next step is to choose the machine learning algorithm that would
    solve the problem. This is the toughest task, because there is a huge list of
    learning algorithms present, and selecting the most suitable from among them is
    a nightmare.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了一个需要监督学习的适当机器学习问题，下一步就是选择解决该问题的机器学习算法。这是一个最困难的任务，因为存在大量的学习算法，从其中选择最合适的一个是一个噩梦。
- en: 'Professor Pedro Domingos has provided a simple reference architecture ([https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)),
    on which basis we could perform the algorithm selection using on three critical
    components that would be required for any machine learning algorithm, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 彼得·多明戈斯教授提供了一个简单的参考架构 ([https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf))，基于此架构，我们可以使用三个对于任何机器学习算法都至关重要的关键组件来进行算法选择，如下所示：
- en: '**Representation**: The way the model is represented so that it can be understood
    by the computer. It can also be considered as the hypothesis space within which
    the model would act.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：模型被表示的方式，以便计算机可以理解。这也可以被视为模型将在其中起作用的假设空间。'
- en: '**Evaluation**: For each algorithm or model, there needs to be an evaluation
    or scoring function to determine which one performs better. The scoring function
    would be different for each type of algorithm.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：对于每个算法或模型，都需要一个评估或评分函数来确定哪个表现更好。评分函数会因算法类型的不同而不同。'
- en: '**Optimization**: A method to search among the models in the language for the
    highest-scoring one. The choice of optimization technique is integral to the efficiency
    of the learner, and also helps determine the model produced if the evaluation
    function has more than one optimum.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：一种在模型中搜索最高评分的方法。优化技术的选择对于学习者的效率至关重要，并且有助于确定如果评估函数有多个最优解时产生的模型。'
- en: 'Supervised learning problems can be further grouped into regression and classification
    problems:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题可以进一步分为回归和分类问题：
- en: '**Classification**: When the output variable is a category, such as green or
    red, or good or bad.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：当输出变量是一个类别，例如绿色或红色，或者好或坏时。'
- en: '**Regression**: When the output variable is a real value, such as dollars or
    weight.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：当输出变量是一个实数值，例如美元或重量时。'
- en: 'In this section, we will go through the following supervised learning algorithms
    with easy-to-understand examples:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过易于理解的示例介绍以下监督学习算法：
- en: Naive Bayes
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naive Bayes
- en: Decision trees
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Linear regression
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support vector machines
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Random forest
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Naive Bayes
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Naive Bayes
- en: Naive Bayes is a powerful classification algorithm, implemented on the principles of
    Bayes theorem. It assumes that there is non-dependence between the feature variables
    considered in the dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 是一种强大的分类算法，基于贝叶斯定理的原理实现。它假设数据集中考虑的特征变量之间不存在依赖关系。
- en: Bayes theorem describes the probability of an event, based on prior knowledge
    of conditions that might be related to the event. For example, if cancer is related
    to age, then, using Bayes theorem, a person's age can be used to more accurately
    assess the probability that they have cancer, compared to the assessment of the
    probability of cancer made without knowledge of the person's age.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理描述了基于先前关于可能与事件相关的事件条件的知识来描述事件的概率。例如，如果癌症与年龄相关，那么，使用贝叶斯定理，一个人的年龄可以用来更准确地评估他们患癌症的概率，与不知道这个人年龄的情况下评估癌症概率相比。
- en: A Naive Bayes classifier assumes that the presence of a particular feature in
    a class is unrelated to the presence of any other feature. For example, a vegetable
    may be considered to be a carrot if it is orange, cone-shaped, and about three
    inches in length. The algorithm is naive as it considers all of these properties
    independently to contribute to the probability that this vegetable is a carrot.
    Generally, features are not independent, but Naive Bayes considers them so for
    prediction.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器假设一个特定特征在类别中的存在与任何其他特征的存在无关。例如，一个蔬菜如果它是橙色的、锥形的，并且大约三英寸长，就可以被认为是胡萝卜。该算法之所以被称为朴素，是因为它独立地考虑所有这些属性来贡献这个蔬菜是胡萝卜的概率。通常，特征不是独立的，但朴素贝叶斯在预测时将它们视为独立。
- en: 'Let''s see a practical usage where the Naive Bayes algorithm is used. Let''s
    assume we have several news feeds and we want to classify these feeds into cultural
    events and non-cultural. Let''s consider the following sentences:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看朴素贝叶斯算法的实际应用。假设我们有一些新闻源，我们想要将这些新闻源分类为文化事件和非文化事件。让我们考虑以下句子：
- en: '*Dramatic event went well—cultural event*'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*戏剧性事件进行得很顺利——文化事件*'
- en: '*This good public rally had a huge crowd—non-cultural event*'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*这个良好的公共集会有大量人群——非文化事件*'
- en: '*Music show was good—cultural event*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*音乐展示很棒——文化事件*'
- en: '*Dramatic event had a huge crowd—cultural event*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*戏剧性事件吸引了大量人群——文化事件*'
- en: '*The political debate was very informative—non-cultural event*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*政治辩论非常有信息量——非文化事件*'
- en: When we are using Bayes theorem, all we want to do is use probabilities to calculate
    whether the sentences fall under cultural or non-cultural events.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用贝叶斯定理时，我们只想使用概率来计算句子是否属于文化事件或非文化事件。
- en: As in the case of the carrot, we had features of color, shape, and size, and
    we treated all of them as independent to determine whether the vegetable considered
    is a carrot.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像胡萝卜的情况一样，我们有颜色、形状和大小的特征，我们将它们都视为独立特征来决定所考虑的蔬菜是否是胡萝卜。
- en: Similarly, to determine whether a feed is related to a cultural event, we take
    a sentence and then, from the sentence, consider each word as an independent feature.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了确定一个新闻源是否与文化事件相关，我们取一个句子，然后从句子中考虑每个词作为一个独立的特征。
- en: Bayes' theorem states that *p(A|B) = p(B|A). P(A)/ P(B)*, where *P(Cultural
    Event|Dramatic show good) = P(Dramatic show good|Cultural Event).P(Cultural event)/P(Dramatic
    show good)*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理指出 *p(A|B) = p(B|A). P(A)/ P(B)*，其中 *P(文化事件|戏剧性展示良好) = P(戏剧性展示良好|文化事件).P(文化事件)/P(戏剧性展示良好)*。
- en: We can discard the denominator here, as we are determining which tag has a higher
    probability in both cultural and non-cultural categories. The denominator for
    both cultural and non-cultural events is going to be the entire dataset and, hence,
    the same.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略分母，因为我们正在确定文化类别和非文化类别中哪个标签有更高的概率。文化事件和非文化事件的分母将是整个数据集，因此是相同的。
- en: '*P(Dramatic show good)* cannot be found, as this sentence doesn''t occur in
    training data. So this is where the naive Bayes theorem really helps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(戏剧性展示良好)* 无法找到，因为这个句子在训练数据中没有出现。所以这就是朴素贝叶斯定理真正发挥作用的地方：'
- en: '*P( Dramatic show good) = P(Dramatic).P(show).P(good)*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(戏剧性展示良好) = P(戏剧).P(展示).P(良好)*'
- en: '*P(Dramatic show good/Cultural event) = P(Dramatic|cultural event).P(Show|cultural
    event)|P(good|cultural event)*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(戏剧性展示良好/文化事件) = P(戏剧|文化事件).P(展示|文化事件)|P(良好|文化事件)*'
- en: 'Now it is easy to calculate these and determine the probability of whether
    the new news feed will be a cultural news feed or a political news feed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易计算这些并确定新的新闻源是否是文化新闻源或政治新闻源：
- en: '*P( Cultural event ) = 3/5 ( 3 out of total 5 sentences)*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(文化事件) = 3/5 (在总共5个句子中有3个)*'
- en: '*P(Non-cultural event) = 2/5*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(非文化事件) = 2/5*'
- en: '*P(Dramatic/cultural event) = Counting how many times Dramatic appears in cultural
    event tags **= 2/13 ( 2 times dramatic appears in the total number of words of
    cultural event tags)*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(戏剧/文化事件) = 计算戏剧在文化事件标签中出现的次数 **= 2/13 (戏剧在文化事件标签的总词数中出现2次)*'
- en: '*P( Show/cultural event) = 1/13*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(展示/文化事件) = 1/13*'
- en: '*P(good/cultural event) =1/13*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(良好/文化事件) =1/13*'
- en: There are various techniques, such as removing stop words, lemmatizing, n-grams,
    and TF-IDF, that can be used to make the feature identification of text classification
    more effective. We will be going through a few of them in the upcoming chapters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种技术，如去除停用词、词干提取、n-gram和TF-IDF，可以使文本分类的特征识别更加有效。我们将在接下来的章节中介绍其中的一些。
- en: 'Here is the final calculated summary:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终的计算总结：
- en: '| **Word** | **P(word&#124;cultural event)** | **P(word&#124;non-cultural event)**
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **词** | **P(word|文化事件)** | **P(word|非文化事件)** |'
- en: '| Dramatic | 2/13 | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 戏剧性 | 2/13 | 0 |'
- en: '| Show | 1/13 | 0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 显示 | 1/13 | 0 |'
- en: '| Good | 1/13 | 1/13 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 好 | 1/13 | 1/13 |'
- en: Now, we just multiply the probabilities and see which is bigger, and then fit
    the sentence into that category of tags.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需将概率相乘，看看哪个更大，然后将句子拟合到那个标签类别中。
- en: So we know from the table that the tag is going to belong to the cultural event
    category, as that is what is going to result in a bigger product when the individual
    probabilities are multiplied.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以从表中知道，该标签将属于文化事件类别，因为当个体概率相乘时，这将导致更大的产品。
- en: 'These examples have given us a good introduction to the Naive Bayes theorem,
    which can be applied to the following areas:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子为我们介绍了朴素贝叶斯定理，它可以应用于以下领域：
- en: Text classification
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Spam filtering
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件过滤
- en: Document categorization
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: Sentiment analysis in social media
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体中的情感分析
- en: Classification of news articles based on genre
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据体裁对新闻文章进行分类
- en: Decision trees
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision tree algorithms are used for making decisions based on certain conditions.
    A decision tree is drawn upside down with its root at the top*.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法用于根据某些条件做出决策。决策树是倒置的，其根在顶部*.*
- en: Let's take an organization's data where the feature set consists of certain
    software products along with their attributes—the time taken to build the product
    *T*, the effort taken to build the product *E*, and the cost taken to build the
    product *C*. It needs to be decided whether those products are to be built in
    the company or should be bought as products directly from outside the company.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个组织的为例，其特征集包括某些软件产品及其属性——构建产品的耗时 *T*，构建产品的努力 *E*，以及构建产品的成本 *C*。需要决定这些产品是在公司内部构建还是直接从公司外部购买。
- en: Now, let's see how the decision tree could be created for this. In the following
    diagram, the bold text in black represents a condition/internal node, based on
    which the tree splits into branches/edges. The end of the branch that doesn't
    split any more is the decision/leaf.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何创建这个决策树。以下图表中，黑色粗体文本代表条件/内部节点，根据这个节点，树分为分支/边。分支的末端不再分裂的是决策/叶子节点。
- en: 'Decision trees are used in program management, project management, and risk
    planning. Let''s see a practical example. The following diagram shows the decision
    tree used by an organization for deciding which of its software needs to be built
    in-house or be purchased as products directly from outside. There are various
    decision points that need to be considered before making a decision and this can be
    represented in the form of a tree. The three features, cost, effort, and the schedule
    parameters, are considered to arrive at the decision as to **Buy** or **Build**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在项目管理、项目管理和风险规划中使用。让我们看看一个实际例子。以下图表显示了组织用于决定其哪些软件需要内部构建或直接从外部购买产品的决策树。在做出决定之前需要考虑各种决策点，这可以表示为树的形式。三个特征，成本、努力和进度参数，被考虑用于决定**购买**或**构建**：
- en: '![](img/bf8171b2-ea97-4fe9-9674-8a03b443ba32.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf8171b2-ea97-4fe9-9674-8a03b443ba32.png)'
- en: The preceding tree is called a **classification tree** as the aim is to classify
    a product nature as to buy or to build. **Regression trees** are represented in
    the same manner, only they predict continuous values, such as the price of a house.
    In general, decision tree algorithms are referred to as **CART** or **Classification
    and Regression Trees**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述树被称为**分类树**，因为目标是根据购买或构建来对产品性质进行分类。**回归树**以相同的方式表示，只是它们预测连续值，例如房屋的价格。通常，决策树算法被称为**CART**或**分类和回归树**。
- en: 'Decision trees can be applied to the following areas:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以应用于以下领域：
- en: Risk identification
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险识别
- en: Loan processing
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贷款处理
- en: Election result prediction
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举结果预测
- en: Process optimization
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流程优化
- en: Optional Pricing
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选定价
- en: Linear regression
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Regression analysis linear regression is a statistical analysis method that
    finds relationships between variables. It helps us to understand the relationship
    between input and output numerical variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析 线性回归是一种统计分析方法，它寻找变量之间的关系。它帮助我们理解输入和输出数值变量之间的关系。
- en: In this method, it is important to determine the dependent variables. For example,
    the value of the house (dependent variable) varies based on the size of the house;
    that is, how many square feet its area is (independent variable). The value of
    the house varies based on its location. Linear regression techniques can be useful
    for prediction.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方法中，确定因变量非常重要。例如，房屋的价值（因变量）根据房屋的大小而变化；也就是说，其面积是多少平方英尺（自变量）。房屋的价值根据其位置而变化。线性回归技术可用于预测。
- en: 'Linear regression is used when the response is a continuous variable. The following
    diagram clearly shows how the linear regression for one variable work. The price
    of the house varies according to its size and is depicted in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当响应是连续变量时，使用线性回归。以下图表清楚地显示了单变量线性回归的工作原理。房屋的价格根据其大小而变化，并在以下图表中表示：
- en: '![](img/4b7c59ff-b2f7-4f3c-aa38-352a0ea6e7c3.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b7c59ff-b2f7-4f3c-aa38-352a0ea6e7c3.png)'
- en: 'Linear regression can be applied to the following areas:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可以应用于以下领域：
- en: Marketing
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 营销
- en: Pricing
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定价
- en: Promotions
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推广
- en: Analyzing consumer behavior
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析消费者行为
- en: Logistic regression
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a classification algorithm that is best suited to when
    the output to be predicted is a binary type—true or false, male or female, win
    or loss, and so on. Binary type means only two outcomes are possible.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种分类算法，最适合预测的输出是二元类型——真或假、男或女、赢或输等。二元类型意味着只有两种可能的结果。
- en: The logistic regression is so called because of the sigmoid function used by
    the algorithm.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归之所以称为逻辑回归，是因为算法中使用了S形函数。
- en: 'A logistic function or logistic curve is a common S shape (sigmoid curve),
    depicted by the following equation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对数函数或对数曲线是一种常见的S形（S形曲线），由以下方程表示：
- en: '![](img/b2784543-67ad-4ec9-b815-c399b9c1de8b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2784543-67ad-4ec9-b815-c399b9c1de8b.png）'
- en: 'In the preceding equation, the symbols have the following meanings:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，符号具有以下含义：
- en: '*e*: The natural logarithm base (also known as **Euler''s number**)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*e*：自然对数的底（也称为**欧拉数**）'
- en: '*x[0]*: The x-value of the sigmoid''s midpoint'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[0]*：S形曲线中点的x值'
- en: '*L*: The curve''s maximum value'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L*：曲线的最大值'
- en: '*k*: The steepness of the curve'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*：曲线的陡峭程度'
- en: 'The standard logistic function is called a **sigmoid function**:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 标准对数函数被称为**S形函数**：
- en: '![](img/db7523fe-adb9-4808-be3a-b5b1ef5f1379.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db7523fe-adb9-4808-be3a-b5b1ef5f1379.png)'
- en: 'The sigmoid curve is depicted here. It''s an S-shaped curve:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: S形曲线在此处展示。它是一条S形曲线：
- en: '![](img/cdbef754-0c3f-4fda-89d5-7f37df1b3aaf.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cdbef754-0c3f-4fda-89d5-7f37df1b3aaf.png)'
- en: 'This curve has a finite limit of the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此曲线的有限极限如下：
- en: '*0* as *x* approaches *−∞*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*x*趋近于*−∞*时的*0*
- en: '*1* as *x* approaches *+∞*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*x*趋近于*+∞*时的*1*
- en: The output of the sigmoid function when *x=0* is *0.5*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x=0*时，S形函数的输出为*0.5*。
- en: 'Thus, if the output is more than *0.5*, we can classify the outcome as 1 (or
    **YES**), and, if it is less than *0.5*, we can classify it as 0 (or **NO**).
    For example: if the output is *0.65*, in probability terms, it can be interpreted
    as—*There is a 65 percent chance that it is going to rain today.*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果输出大于*0.5*，我们可以将结果分类为1（或**是**），如果小于*0.5*，我们可以将其分类为0（或**否**）。例如：如果输出为*0.65*，从概率的角度来看，它可以解释为—*今天下雨的概率是65%。*
- en: 'Thus, the output of the sigmoid function cannot just be used to classify yes/no;
    it can also be used to determine the probability of yes/no. It can be applied
    to the following areas:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，S形函数的输出不仅可以用作是/否的分类，还可以用来确定是/否的概率。它可以应用于以下领域：
- en: Image segmentation and categorization
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割和分类
- en: Geographic image processing
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理图像处理
- en: Handwriting recognition
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Healthcare, for disease prediction and gene analytics
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健，用于疾病预测和基因分析
- en: Prediction in various areas where a binary outcome is expected
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测在预期有二元结果的各种领域
- en: Support vector machines
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: A **support vector machine** (**SVM**) is a supervised machine learning algorithm
    that can be used for both classification and regression. SVMs are more commonly
    used for classification.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种监督机器学习算法，可用于分类和回归。SVMs更常用于分类。'
- en: Given some data points, each belonging to one of the two binary classes, the
    goal is to decide which class a new data point will be in. We need to visualize
    the data point as a p-dimensional vector, and we need to determine whether we
    can separate two such data points with a (p-1) dimensional hyperplane.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: There may be many hyper planes that separate such data points, and this algorithm
    will help us to arrive at the best hyperplane that provides the largest separation.
    This hyperplane is called the **maximum-margin hyperplane**, and the classifier
    is called the **maximum-margin classifier**. We can extend the concept of a separating
    hyperplane to develop a hyperplane that almost separates the classes, using a
    so-called **soft margin**. The generalization of the maximal margin classifier
    to the non-separable case is known as the **support vector** **classifier**.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the first example. In this, there is one hyperplane that separates
    the red dots and the blue dots:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df1c4793-a07d-4227-b3c7-7a8da1056fb3.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'But imagine that the points were distributed as follows—how will we identify
    the hyperplane that separates the red dots and the blue dots:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d12a5c3-2f11-4e4e-bc58-0b28b38566e3.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'The solution is to identify the hyperplane with SVM. It can execute transformations
    to identify the hyperplane that separates the two for classification. It will
    introduce a new feature, *z*, which is *z=x^2+y^2*. Let''s plot the graph with
    the *x* and *z* axes, and identify the hyperplane for classification:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c4da293-facf-4976-bfa8-d8d5187bb495.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'Now that we understand the basics of SVM, let''s look at the areas where it
    can be applied:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bioinformatics
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geological and environmental sciences
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetics
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protein studies
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting recognition
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen what a decision tree is. Having understood decision trees,
    let's take a look at random forests. A random forest combines many decision trees
    into a single model. Individually, predictions made by decision trees (or humans)
    may not be accurate, but combined together, the predictions will be closer to
    the mark, on average.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows us a random forest, where there are multiple trees
    and each is making a prediction:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dddb6689-4e22-41f6-93b7-20556116d586.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Random forest is a combination of many decision trees and, hence, there is a
    greater probability of having many views from all trees in the forest to arrive
    at the final desired outcome/prediction. If only a single decision tree is taken
    into consideration for prediction, there is less information considered for prediction.
    But in random forest, when there are many trees involved, the source of information
    is diverse and extensive. Unlike decision trees, random forests are not biased,
    since they are not dependent on one source.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the concept of random forests:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c788ab0-cf09-4c26-ad32-dc464266e75e.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Random forests can be applied to the following areas:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以应用于以下领域：
- en: Risk identification
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险识别
- en: Loan processing
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贷款处理
- en: Election result prediction
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举结果预测
- en: Process optimization
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流程优化
- en: Optional pricing
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选定价
- en: Introduction to unsupervised learning algorithms
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习算法简介
- en: Consider a scenario where a child is given a bag full of beads of different
    sizes, colors, shapes, and made of various materials. We just leave to the child
    do whatever they want with the whole bag of beads.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，孩子被给了一袋大小、颜色、形状各异且由不同材料制成的珠子。我们让孩子随意处理整个珠子袋。
- en: 'There are various things the child could do, based on their interests:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据孩子的兴趣，他们可以做很多事情：
- en: Separate the beads into categories based on size
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据大小将珠子分类
- en: Separate the beads into categories based on shape
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据形状将珠子分类
- en: Separate the beads into categories based on a combination of color and shape
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据颜色和形状的组合将珠子分类
- en: Separate the beads into categories based on a combination of material, color,
    and shape
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据材料、颜色和形状的组合将珠子分类
- en: The possibilities are endless. However, the child without any prior teaching
    is able to go through the beads and uncover patterns of which it doesn't need
    any any prior knowledge at all. They are discovering the patterns purely on the
    basis of going through the beads at hand, that is, the data at hand. We just got
    introduced to unsupervised machine learning!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性是无限的。然而，没有任何先前教导的孩子能够通过手头的珠子并揭示出他们不需要任何先前知识的模式。他们纯粹是基于手头的珠子，即手头的数据来发现模式的。我们刚刚接触到了无监督机器学习！
- en: 'We will relate the preceding activity to the key steps of machine learning:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把前面的活动与机器学习的关键步骤联系起来：
- en: '**Define the ML problem**: Uncover hidden patterns of beads from the given
    bag of beads.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义机器学习问题**：从给定的珠子袋中揭示珠子的隐藏模式。'
- en: '**Prepare/gather the data and train the model**: The child opens the bagful
    of beads and understands what the bag contains. They discover the attributes of
    the different beads present:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备/收集数据和训练模型**：孩子打开装满珠子的袋子，了解袋子里有什么。他们发现不同珠子的属性：'
- en: Color
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色
- en: Shape
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状
- en: Size
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小
- en: Material
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 材料
- en: '**Evaluate the model**: If a new set of beads is given to the child, how will
    they cluster these beads based on their previous experience of clustering beads?'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：如果给孩子一组新的珠子，他们将如何根据他们之前的聚类珠子经验对这些珠子进行聚类？'
- en: There may be errors in grouping the beads that need to be corrected/fixed so that
    they don't recur in future.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在分组珠子时可能存在需要纠正/修复的错误，以防止它们在未来再次发生。
- en: So, now that we have seen the basic concepts and functions of the unsupervised
    machine learning problem, let's get into the details of unsupervised learning.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经了解了无监督机器学习的基本概念和函数，让我们深入了解无监督学习的细节。
- en: Deep dive into unsupervised learning algorithms
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解无监督学习算法
- en: Unsupervised machine learning deals with learning unlabeled data—that is, data
    that has not been classified or categorized, and arriving at conclusions/patterns
    in relation to them.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习处理的是未标记的数据——也就是说，尚未分类或归类的数据，并据此得出结论/模式。
- en: These categories learn from test data that has not been labeled, classified,
    or categorized. Instead of responding to feedback, unsupervised learning identifies
    commonalities in the data and reacts based on the presence or absence of such
    commonalities in each new piece of data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别从未标记、未分类或未归类的测试数据中学习。无监督学习不是响应反馈，而是识别数据中的共性，并根据每份数据中这种共性的存在与否做出反应。
- en: The input given to the learning algorithm is unlabeled and, hence, there is
    no straightforward way to evaluate the accuracy of the structure that is produced
    as output by the algorithm. This is one feature that distinguishes unsupervised
    learning from supervised learning.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给学习算法的输入是无标签的，因此没有直接的方法来评估算法产生的输出结构的准确性。这是无监督学习与监督学习区别的一个特征。
- en: The unsupervised algorithms have predictor attributes but **NO** objective function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法具有预测属性，但**没有**目标函数。
- en: 'What does it mean to learn without an objective? Consider the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 没有目标函数意味着什么？考虑以下情况：
- en: Explore the data for natural groupings.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据中的自然分组。
- en: Learn association rules, and later examine whether they can be of any use.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关联规则，然后检查它们是否有任何用处。
- en: 'Here are some classic examples:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些经典的例子：
- en: Performing market basket analysis and then optimizing shelf allocation and placement
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cascaded or correlated mechanical faults
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic grouping beyond known classes
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning product bundling offers
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will go through the following unsupervised learning algorithms
    with easy-to-understand examples:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule mapping
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) and **singular value decomposition**
    (**SVD**) may also be of interest if you want to deep dive into those concepts.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering the dataset into useful groups is what clustering algorithms do. The
    goal of clustering is to create groups of data points, such that points in different
    clusters are dissimilar, while points within a cluster are similar.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two essential elements for clustering algorithms to work:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity function**: This determines how we decide that two points are
    similar.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering method**: This is the method observed in order to arrive at clusters.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There needs to be a mechanism to determine similarity between points, on which
    basis they could be categorized as similar or dissimilar. There are various similarity
    measures. Here are a few:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean**:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/447a4ef5-4329-41b4-b2ea-e6c7841b00ac.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: '**Cosine**:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7763f67b-7e38-4083-9787-9bb1fb188549.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: '**KL-divergence**:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/13af89d4-c884-4b40-b37f-cd26372c34c1.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Clustering methods
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we know the similarity measure, we next need to choose the clustering
    method. We will go through two clustering methods:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical agglomerative clustering methods
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical agglomerative clustering methods
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agglomerative hierarchical clustering is a classical clustering algorithm from
    the statistics domain. It involves iterative merging of the two most similar groups,
    which, in the first instance, contain single elements. The name of the algorithm
    refers to its way of working, as it creates hierarchical results in an agglomerative
    or bottom-up way, that is, by merging smaller groups into larger ones.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Here is the high-level algorithm for this method of clustering used in document
    clustering.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Generic agglomerative process (Salton, G: *Automatic Text Processing: The Transformation,
    Analysis, and Retrieval of Information by Computer*, *Addison-Wesley*, 1989) result
    in nested clusters via iterations.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute all pairwise document-document similarity coefficients
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place each of the *n* documents into a class of its own
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Merge the two most similar clusters into one:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the two clusters with the new cluster
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recompute inter-cluster similarity scores with regard to the new cluster
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the cluster radius is greater than maxsize, block further merging
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the preceding step until there are only *k* clusters left (note: *k*
    could equal *1*)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means clustering
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this K-means clustering algorithm is to find K groups in the data,
    with each group having similar data points. The algorithm works iteratively to
    assign each data point to one of *K* groups based on the features that are provided.
    Data points are clustered based on feature similarity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The K value is assigned randomly at the beginning of the algorithm and different
    variations of results could be obtained by altering the K value. Once the algorithm
    sequence of activities is initiated after the selection of K, as depicted in the
    following points, we find that there are two major steps that keep repeating,
    until there is no further scope for changes in the clusters.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The two major steps that get repeated are *Step 2* and *Step 3*, depicted as
    follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: Assigning the data point from the dataset to any of the K clusters.
    This is done by calculating the distance of the data point from the cluster centroid.
    As specified, any one of the distance functions that we discussed already could
    be used for this calculation.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: Here again, recalibration of the centroid occurs. This is done
    by taking the mean of all data points assigned to that centroid cluster.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final output of the algorithm is K clusters that have similar data points:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Select *k-seeds d(k[i],kj) > d[min]*
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assign points to clusters according to minimum distance:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/226eeaa2-a75e-4049-8e39-175067e5597c.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Compute new cluster centroids:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5c1f8a6-f9f6-4bc7-bfc6-466d617ba5d1.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Reassign points to the cluster (as in *Step 2*)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate until no points change the cluster.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are some areas where clustering algorithms are used:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: City planning
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earthquake studies
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insurance
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marketing
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medicine, for the analysis of antimicrobial activity and medical imaging
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crime analysis
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics, for anomaly detection and natural language processing
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule learning algorithm
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rule mining is more useful for categorical non-numeric data. Association
    rule mining is primarily focused on finding frequent co-occurring associations
    among a collection of items. It is sometimes also called **market-basket analysis**.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'In a shopper''s basket, the goal is to determine what items occur together
    frequently. This shows co-relations that are very hard to find from a random sampling
    method. The classic example of this is the famous Beer and Diapers association,
    which is often mentioned in data mining books. The scenario is this: men who go
    to the store to buy diapers will also tend to buy beer. This scenario is very
    hard to intuit or determine through random sampling.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Another example was discovered by Walmart in 2004, when a series of hurricanes
    crossed Florida. Walmart wanted to know what shoppers usually buy before a hurricane
    strikes. They found one particular item that increased in sales by a factor of
    seven over normal shopping days; that item was not bottled water, batteries, beer,
    flashlights, generators, or any of the usual things that we might imagine. The
    item was **strawberry pop tarts**! It is possible to conceive a multitude  of
    reasons as to why this was the most desired product prior to the arrival of a
    hurricane–pop tarts do not require refrigeration, they do not need to be cooked,
    they come in individually wrapped portions, they have a long shelf life, they
    are a snack food, they are a breakfast food, kids love them, we love them, the
    list goes on. Despite these obvious reasons, it was still a huge surprise!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'When mining for associations, the following could be useful:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Search for rare and unusual co-occurring associations of non-numeric items.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is time-based data, consider the effects of introducing a time lag
    in data mining experiments to see whether the strength of the correlation reaches
    its peak at a later time.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Market-basket analysis can be applied to the following areas:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Retail management
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store management
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inventory management
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NASA and environmental studies
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical diagnoses
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about what supervised learning is through a naive
    example and deep dived into concepts of supervised learning. We went through various
    supervised learning algorithms with practical examples and their application areas
    and then we started going through unsupervised learning with naive examples. We
    also covered the concepts of unsupervised learning and then we went through various
    unsupervised learning algorithms with practical examples and their application
    areas.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent chapters, we will be solving mobile machine learning problems
    by using some of the supervised and unsupervised machine learning algorithms that
    we have gone through in this chapter. We will also be exposing you to mobile machine
    learning SDKs, which will be used to implement mobile machine learning solutions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dr. Pedro Domingo's paper—[https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf),[ ](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)summarizes
    twelve key lessons that machine learning researchers and practitioners have learned,
    including pitfalls to avoid, important issues to focus on, and answers to common
    questions in this area.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
