- en: Supervised and Unsupervised Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we got some insight into the various aspects of machine
    learning and were introduced to the various ways in which machine learning algorithms
    could be categorized. In this chapter, we will go a step further into machine
    learning algorithms and try to understand supervised and unsupervised learning
    algorithms. This categorization is based on the learning mechanism of the algorithm,
    and is the most popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the supervised learning algorithm in the form of a detailed
    practical example to help understand it and its guiding principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key supervised learning algorithms and their application areas:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the unsupervised learning algorithm in the form of a detailed
    practical example to help understand it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key unsupervised learning algorithms and their application areas:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule mapping
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A broad overview of the different mobile SDKs and tools available to implement
    these algorithms in mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to supervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at supervised learning for simple day-to-day activities. A parent
    asks their 15-year-old son to go to the store and get some vegetables. They give
    him a list of vegetables, say beets, carrots, beans, and tomatoes, that they want
    him to buy. He goes to the store and is able to identify the list of vegetables
    as per the list provided by his mother from all the other numerous varieties of
    vegetables present in the store and put them in his cart before going to the checkout.
    How was this possible?
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple. The parent had provided enough training to the son by providing instances
    of each and every vegetable, which equipped him with sufficient knowledge of the
    vegetables. The son used the knowledge he has gained to choose the correct vegetables.
    He used the various attributes of the vegetables to arrive at the correct class
    label of the vegetable, which, in this case, is the name of the vegetable. The
    following table gives us a few of the attributes of the vegetables present in
    the list, by means of which the son was able to recognize the class label, that
    is, the vegetable name:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Vegetable name =****class label** | **Carrots** | **Beets** | **Beans**
    | **Tomatoes** |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute 1 = Color | Orange | Pink | Green | Red |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute 2 = Shape | Cone | Round | Stick | Round |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute 3 = Texture | Hard | Hard | Soft | Soft and juicy |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute 4 = Size | 10 cm in length | 3 cm radius | 10 cm in length | 3
    cm radius |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute 5 = Taste | Sweet | Sweet | Bland | Sweet and sour |'
  prefs: []
  type: TYPE_TB
- en: 'We just got introduced to supervised learning. We will relate this activity
    to the key steps of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the ML problem**: Purchasing the correct classes of vegetables from
    all the classes of vegetables present in the store, based on the training and
    experience already gained on different attributes of the vegetables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prepare/gather the data and train the model**: The 15-year-old son has already
    been trained with sufficient knowledge of all the vegetables. This knowledge of
    all the different types of vegetables he has seen and eaten, and of their attributes
    and features, forms the historical training data for the problem, for the model—the
    15-year-old son.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: The son is asked to purchase a few vegetables from
    the store. This is the test set provided to him to evaluate the model. The task
    of the model now is to identify the correct class label of the vegetables from
    the store based on the list provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be errors in the identification and purchase of correct vegetables
    in some cases. For example, the son might purchase double beans (a variant of
    beans) instead of ordinary beans. This may be due to a lack of sufficient training
    given to him on the distinguishing features between the beans and the double beans.
    If there is such an error, the parent would retrain him with the new type of vegetable,
    so that next time, he won't make that mistake.
  prefs: []
  type: TYPE_NORMAL
- en: So, we saw the basic concepts and functions of the supervised machine learning
    problem. Let's now get into the details of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into supervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume there are predictor attributes, *x1*, *x2*, .... *xn*, and also an objective
    attribute, `y`, for a given dataset. Then, the supervised learning is the machine learning task
    of finding the prediction function that takes as input both the predictor attributes
    and the objective attribute from this dataset, and is capable of mapping the predictive
    attributes to the objective attribute for even unseen data currently not in the
    training dataset with minimal error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data in the dataset used for arriving at the prediction function is called
    the **training data** and it consists of a set of training examples where each
    example consists of an input object, `x` (typically a vector), and a desired output
    value, `Y`. A supervised learning algorithm analyzes the training data and produces
    an inferred function that maps the input to output and could also be used for
    mapping new, unseen example data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = f(X) + error*'
  prefs: []
  type: TYPE_NORMAL
- en: The whole category of algorithms is called **supervised learning**, because
    here we consider both input and output variables for learning. So learning is
    supervised algorithm is by providing the input as well as the expected output
    in the training data for all the instances of training data.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised algorithms have both predictor attributes and an objective function.
    The predictor attributes in a set of data items are those items that are considered
    to predict the objective function. The objective function is the goal of machine
    learning. This usually takes in the predictor attributes, perhaps with some other
    compute functionality, and would usually output a single numeric value.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have defined a proper machine learning problem that would require supervised
    learning, the next step is to choose the machine learning algorithm that would
    solve the problem. This is the toughest task, because there is a huge list of
    learning algorithms present, and selecting the most suitable from among them is
    a nightmare.
  prefs: []
  type: TYPE_NORMAL
- en: 'Professor Pedro Domingos has provided a simple reference architecture ([https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)),
    on which basis we could perform the algorithm selection using on three critical
    components that would be required for any machine learning algorithm, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation**: The way the model is represented so that it can be understood
    by the computer. It can also be considered as the hypothesis space within which
    the model would act.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: For each algorithm or model, there needs to be an evaluation
    or scoring function to determine which one performs better. The scoring function
    would be different for each type of algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: A method to search among the models in the language for the
    highest-scoring one. The choice of optimization technique is integral to the efficiency
    of the learner, and also helps determine the model produced if the evaluation
    function has more than one optimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised learning problems can be further grouped into regression and classification
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: When the output variable is a category, such as green or
    red, or good or bad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: When the output variable is a real value, such as dollars or
    weight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will go through the following supervised learning algorithms
    with easy-to-understand examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is a powerful classification algorithm, implemented on the principles of
    Bayes theorem. It assumes that there is non-dependence between the feature variables
    considered in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes theorem describes the probability of an event, based on prior knowledge
    of conditions that might be related to the event. For example, if cancer is related
    to age, then, using Bayes theorem, a person's age can be used to more accurately
    assess the probability that they have cancer, compared to the assessment of the
    probability of cancer made without knowledge of the person's age.
  prefs: []
  type: TYPE_NORMAL
- en: A Naive Bayes classifier assumes that the presence of a particular feature in
    a class is unrelated to the presence of any other feature. For example, a vegetable
    may be considered to be a carrot if it is orange, cone-shaped, and about three
    inches in length. The algorithm is naive as it considers all of these properties
    independently to contribute to the probability that this vegetable is a carrot.
    Generally, features are not independent, but Naive Bayes considers them so for
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a practical usage where the Naive Bayes algorithm is used. Let''s
    assume we have several news feeds and we want to classify these feeds into cultural
    events and non-cultural. Let''s consider the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dramatic event went well—cultural event*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*This good public rally had a huge crowd—non-cultural event*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Music show was good—cultural event*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dramatic event had a huge crowd—cultural event*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The political debate was very informative—non-cultural event*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we are using Bayes theorem, all we want to do is use probabilities to calculate
    whether the sentences fall under cultural or non-cultural events.
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of the carrot, we had features of color, shape, and size, and
    we treated all of them as independent to determine whether the vegetable considered
    is a carrot.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, to determine whether a feed is related to a cultural event, we take
    a sentence and then, from the sentence, consider each word as an independent feature.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem states that *p(A|B) = p(B|A). P(A)/ P(B)*, where *P(Cultural
    Event|Dramatic show good) = P(Dramatic show good|Cultural Event).P(Cultural event)/P(Dramatic
    show good)*.
  prefs: []
  type: TYPE_NORMAL
- en: We can discard the denominator here, as we are determining which tag has a higher
    probability in both cultural and non-cultural categories. The denominator for
    both cultural and non-cultural events is going to be the entire dataset and, hence,
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Dramatic show good)* cannot be found, as this sentence doesn''t occur in
    training data. So this is where the naive Bayes theorem really helps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P( Dramatic show good) = P(Dramatic).P(show).P(good)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Dramatic show good/Cultural event) = P(Dramatic|cultural event).P(Show|cultural
    event)|P(good|cultural event)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is easy to calculate these and determine the probability of whether
    the new news feed will be a cultural news feed or a political news feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P( Cultural event ) = 3/5 ( 3 out of total 5 sentences)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Non-cultural event) = 2/5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Dramatic/cultural event) = Counting how many times Dramatic appears in cultural
    event tags **= 2/13 ( 2 times dramatic appears in the total number of words of
    cultural event tags)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P( Show/cultural event) = 1/13*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(good/cultural event) =1/13*'
  prefs: []
  type: TYPE_NORMAL
- en: There are various techniques, such as removing stop words, lemmatizing, n-grams,
    and TF-IDF, that can be used to make the feature identification of text classification
    more effective. We will be going through a few of them in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the final calculated summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **P(word&#124;cultural event)** | **P(word&#124;non-cultural event)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dramatic | 2/13 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Show | 1/13 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Good | 1/13 | 1/13 |'
  prefs: []
  type: TYPE_TB
- en: Now, we just multiply the probabilities and see which is bigger, and then fit
    the sentence into that category of tags.
  prefs: []
  type: TYPE_NORMAL
- en: So we know from the table that the tag is going to belong to the cultural event
    category, as that is what is going to result in a bigger product when the individual
    probabilities are multiplied.
  prefs: []
  type: TYPE_NORMAL
- en: 'These examples have given us a good introduction to the Naive Bayes theorem,
    which can be applied to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document categorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis in social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of news articles based on genre
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree algorithms are used for making decisions based on certain conditions.
    A decision tree is drawn upside down with its root at the top*.*
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an organization's data where the feature set consists of certain
    software products along with their attributes—the time taken to build the product
    *T*, the effort taken to build the product *E*, and the cost taken to build the
    product *C*. It needs to be decided whether those products are to be built in
    the company or should be bought as products directly from outside the company.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how the decision tree could be created for this. In the following
    diagram, the bold text in black represents a condition/internal node, based on
    which the tree splits into branches/edges. The end of the branch that doesn't
    split any more is the decision/leaf.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees are used in program management, project management, and risk
    planning. Let''s see a practical example. The following diagram shows the decision
    tree used by an organization for deciding which of its software needs to be built
    in-house or be purchased as products directly from outside. There are various
    decision points that need to be considered before making a decision and this can be
    represented in the form of a tree. The three features, cost, effort, and the schedule
    parameters, are considered to arrive at the decision as to **Buy** or **Build**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf8171b2-ea97-4fe9-9674-8a03b443ba32.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding tree is called a **classification tree** as the aim is to classify
    a product nature as to buy or to build. **Regression trees** are represented in
    the same manner, only they predict continuous values, such as the price of a house.
    In general, decision tree algorithms are referred to as **CART** or **Classification
    and Regression Trees**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees can be applied to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loan processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Election result prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional Pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression analysis linear regression is a statistical analysis method that
    finds relationships between variables. It helps us to understand the relationship
    between input and output numerical variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this method, it is important to determine the dependent variables. For example,
    the value of the house (dependent variable) varies based on the size of the house;
    that is, how many square feet its area is (independent variable). The value of
    the house varies based on its location. Linear regression techniques can be useful
    for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression is used when the response is a continuous variable. The following
    diagram clearly shows how the linear regression for one variable work. The price
    of the house varies according to its size and is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b7c59ff-b2f7-4f3c-aa38-352a0ea6e7c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Linear regression can be applied to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Marketing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promotions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing consumer behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a classification algorithm that is best suited to when
    the output to be predicted is a binary type—true or false, male or female, win
    or loss, and so on. Binary type means only two outcomes are possible.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression is so called because of the sigmoid function used by
    the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'A logistic function or logistic curve is a common S shape (sigmoid curve),
    depicted by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2784543-67ad-4ec9-b815-c399b9c1de8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the symbols have the following meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: '*e*: The natural logarithm base (also known as **Euler''s number**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[0]*: The x-value of the sigmoid''s midpoint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L*: The curve''s maximum value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*: The steepness of the curve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The standard logistic function is called a **sigmoid function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db7523fe-adb9-4808-be3a-b5b1ef5f1379.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid curve is depicted here. It''s an S-shaped curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdbef754-0c3f-4fda-89d5-7f37df1b3aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This curve has a finite limit of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0* as *x* approaches *−∞*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1* as *x* approaches *+∞*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the sigmoid function when *x=0* is *0.5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, if the output is more than *0.5*, we can classify the outcome as 1 (or
    **YES**), and, if it is less than *0.5*, we can classify it as 0 (or **NO**).
    For example: if the output is *0.65*, in probability terms, it can be interpreted
    as—*There is a 65 percent chance that it is going to rain today.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the output of the sigmoid function cannot just be used to classify yes/no;
    it can also be used to determine the probability of yes/no. It can be applied
    to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation and categorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographic image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Healthcare, for disease prediction and gene analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction in various areas where a binary outcome is expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **support vector machine** (**SVM**) is a supervised machine learning algorithm
    that can be used for both classification and regression. SVMs are more commonly
    used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Given some data points, each belonging to one of the two binary classes, the
    goal is to decide which class a new data point will be in. We need to visualize
    the data point as a p-dimensional vector, and we need to determine whether we
    can separate two such data points with a (p-1) dimensional hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: There may be many hyper planes that separate such data points, and this algorithm
    will help us to arrive at the best hyperplane that provides the largest separation.
    This hyperplane is called the **maximum-margin hyperplane**, and the classifier
    is called the **maximum-margin classifier**. We can extend the concept of a separating
    hyperplane to develop a hyperplane that almost separates the classes, using a
    so-called **soft margin**. The generalization of the maximal margin classifier
    to the non-separable case is known as the **support vector** **classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the first example. In this, there is one hyperplane that separates
    the red dots and the blue dots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df1c4793-a07d-4227-b3c7-7a8da1056fb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But imagine that the points were distributed as follows—how will we identify
    the hyperplane that separates the red dots and the blue dots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d12a5c3-2f11-4e4e-bc58-0b28b38566e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The solution is to identify the hyperplane with SVM. It can execute transformations
    to identify the hyperplane that separates the two for classification. It will
    introduce a new feature, *z*, which is *z=x^2+y^2*. Let''s plot the graph with
    the *x* and *z* axes, and identify the hyperplane for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c4da293-facf-4976-bfa8-d8d5187bb495.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we understand the basics of SVM, let''s look at the areas where it
    can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bioinformatics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geological and environmental sciences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protein studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen what a decision tree is. Having understood decision trees,
    let's take a look at random forests. A random forest combines many decision trees
    into a single model. Individually, predictions made by decision trees (or humans)
    may not be accurate, but combined together, the predictions will be closer to
    the mark, on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows us a random forest, where there are multiple trees
    and each is making a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dddb6689-4e22-41f6-93b7-20556116d586.png)'
  prefs: []
  type: TYPE_IMG
- en: Random forest is a combination of many decision trees and, hence, there is a
    greater probability of having many views from all trees in the forest to arrive
    at the final desired outcome/prediction. If only a single decision tree is taken
    into consideration for prediction, there is less information considered for prediction.
    But in random forest, when there are many trees involved, the source of information
    is diverse and extensive. Unlike decision trees, random forests are not biased,
    since they are not dependent on one source.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the concept of random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c788ab0-cf09-4c26-ad32-dc464266e75e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Random forests can be applied to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Risk identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loan processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Election result prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a scenario where a child is given a bag full of beads of different
    sizes, colors, shapes, and made of various materials. We just leave to the child
    do whatever they want with the whole bag of beads.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various things the child could do, based on their interests:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate the beads into categories based on size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate the beads into categories based on shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate the beads into categories based on a combination of color and shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate the beads into categories based on a combination of material, color,
    and shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The possibilities are endless. However, the child without any prior teaching
    is able to go through the beads and uncover patterns of which it doesn't need
    any any prior knowledge at all. They are discovering the patterns purely on the
    basis of going through the beads at hand, that is, the data at hand. We just got
    introduced to unsupervised machine learning!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will relate the preceding activity to the key steps of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the ML problem**: Uncover hidden patterns of beads from the given
    bag of beads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prepare/gather the data and train the model**: The child opens the bagful
    of beads and understands what the bag contains. They discover the attributes of
    the different beads present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Color
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shape
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Material
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: If a new set of beads is given to the child, how will
    they cluster these beads based on their previous experience of clustering beads?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There may be errors in grouping the beads that need to be corrected/fixed so that
    they don't recur in future.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we have seen the basic concepts and functions of the unsupervised
    machine learning problem, let's get into the details of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into unsupervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised machine learning deals with learning unlabeled data—that is, data
    that has not been classified or categorized, and arriving at conclusions/patterns
    in relation to them.
  prefs: []
  type: TYPE_NORMAL
- en: These categories learn from test data that has not been labeled, classified,
    or categorized. Instead of responding to feedback, unsupervised learning identifies
    commonalities in the data and reacts based on the presence or absence of such
    commonalities in each new piece of data.
  prefs: []
  type: TYPE_NORMAL
- en: The input given to the learning algorithm is unlabeled and, hence, there is
    no straightforward way to evaluate the accuracy of the structure that is produced
    as output by the algorithm. This is one feature that distinguishes unsupervised
    learning from supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised algorithms have predictor attributes but **NO** objective function.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean to learn without an objective? Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the data for natural groupings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn association rules, and later examine whether they can be of any use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some classic examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing market basket analysis and then optimizing shelf allocation and placement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cascaded or correlated mechanical faults
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic grouping beyond known classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning product bundling offers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will go through the following unsupervised learning algorithms
    with easy-to-understand examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) and **singular value decomposition**
    (**SVD**) may also be of interest if you want to deep dive into those concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering the dataset into useful groups is what clustering algorithms do. The
    goal of clustering is to create groups of data points, such that points in different
    clusters are dissimilar, while points within a cluster are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two essential elements for clustering algorithms to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity function**: This determines how we decide that two points are
    similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering method**: This is the method observed in order to arrive at clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There needs to be a mechanism to determine similarity between points, on which
    basis they could be categorized as similar or dissimilar. There are various similarity
    measures. Here are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/447a4ef5-4329-41b4-b2ea-e6c7841b00ac.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Cosine**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7763f67b-7e38-4083-9787-9bb1fb188549.png)'
  prefs: []
  type: TYPE_IMG
- en: '**KL-divergence**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/13af89d4-c884-4b40-b37f-cd26372c34c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we know the similarity measure, we next need to choose the clustering
    method. We will go through two clustering methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical agglomerative clustering methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical agglomerative clustering methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agglomerative hierarchical clustering is a classical clustering algorithm from
    the statistics domain. It involves iterative merging of the two most similar groups,
    which, in the first instance, contain single elements. The name of the algorithm
    refers to its way of working, as it creates hierarchical results in an agglomerative
    or bottom-up way, that is, by merging smaller groups into larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the high-level algorithm for this method of clustering used in document
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generic agglomerative process (Salton, G: *Automatic Text Processing: The Transformation,
    Analysis, and Retrieval of Information by Computer*, *Addison-Wesley*, 1989) result
    in nested clusters via iterations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute all pairwise document-document similarity coefficients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place each of the *n* documents into a class of its own
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Merge the two most similar clusters into one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the two clusters with the new cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recompute inter-cluster similarity scores with regard to the new cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the cluster radius is greater than maxsize, block further merging
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the preceding step until there are only *k* clusters left (note: *k*
    could equal *1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this K-means clustering algorithm is to find K groups in the data,
    with each group having similar data points. The algorithm works iteratively to
    assign each data point to one of *K* groups based on the features that are provided.
    Data points are clustered based on feature similarity.
  prefs: []
  type: TYPE_NORMAL
- en: The K value is assigned randomly at the beginning of the algorithm and different
    variations of results could be obtained by altering the K value. Once the algorithm
    sequence of activities is initiated after the selection of K, as depicted in the
    following points, we find that there are two major steps that keep repeating,
    until there is no further scope for changes in the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two major steps that get repeated are *Step 2* and *Step 3*, depicted as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: Assigning the data point from the dataset to any of the K clusters.
    This is done by calculating the distance of the data point from the cluster centroid.
    As specified, any one of the distance functions that we discussed already could
    be used for this calculation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: Here again, recalibration of the centroid occurs. This is done
    by taking the mean of all data points assigned to that centroid cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final output of the algorithm is K clusters that have similar data points:'
  prefs: []
  type: TYPE_NORMAL
- en: Select *k-seeds d(k[i],kj) > d[min]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assign points to clusters according to minimum distance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/226eeaa2-a75e-4049-8e39-175067e5597c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute new cluster centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5c1f8a6-f9f6-4bc7-bfc6-466d617ba5d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Reassign points to the cluster (as in *Step 2*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate until no points change the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are some areas where clustering algorithms are used:'
  prefs: []
  type: TYPE_NORMAL
- en: City planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earthquake studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marketing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medicine, for the analysis of antimicrobial activity and medical imaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crime analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics, for anomaly detection and natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rule mining is more useful for categorical non-numeric data. Association
    rule mining is primarily focused on finding frequent co-occurring associations
    among a collection of items. It is sometimes also called **market-basket analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a shopper''s basket, the goal is to determine what items occur together
    frequently. This shows co-relations that are very hard to find from a random sampling
    method. The classic example of this is the famous Beer and Diapers association,
    which is often mentioned in data mining books. The scenario is this: men who go
    to the store to buy diapers will also tend to buy beer. This scenario is very
    hard to intuit or determine through random sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example was discovered by Walmart in 2004, when a series of hurricanes
    crossed Florida. Walmart wanted to know what shoppers usually buy before a hurricane
    strikes. They found one particular item that increased in sales by a factor of
    seven over normal shopping days; that item was not bottled water, batteries, beer,
    flashlights, generators, or any of the usual things that we might imagine. The
    item was **strawberry pop tarts**! It is possible to conceive a multitude  of
    reasons as to why this was the most desired product prior to the arrival of a
    hurricane–pop tarts do not require refrigeration, they do not need to be cooked,
    they come in individually wrapped portions, they have a long shelf life, they
    are a snack food, they are a breakfast food, kids love them, we love them, the
    list goes on. Despite these obvious reasons, it was still a huge surprise!
  prefs: []
  type: TYPE_NORMAL
- en: 'When mining for associations, the following could be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for rare and unusual co-occurring associations of non-numeric items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is time-based data, consider the effects of introducing a time lag
    in data mining experiments to see whether the strength of the correlation reaches
    its peak at a later time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Market-basket analysis can be applied to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Retail management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inventory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NASA and environmental studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical diagnoses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about what supervised learning is through a naive
    example and deep dived into concepts of supervised learning. We went through various
    supervised learning algorithms with practical examples and their application areas
    and then we started going through unsupervised learning with naive examples. We
    also covered the concepts of unsupervised learning and then we went through various
    unsupervised learning algorithms with practical examples and their application
    areas.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent chapters, we will be solving mobile machine learning problems
    by using some of the supervised and unsupervised machine learning algorithms that
    we have gone through in this chapter. We will also be exposing you to mobile machine
    learning SDKs, which will be used to implement mobile machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dr. Pedro Domingo's paper—[https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf),[ ](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)summarizes
    twelve key lessons that machine learning researchers and practitioners have learned,
    including pitfalls to avoid, important issues to focus on, and answers to common
    questions in this area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
