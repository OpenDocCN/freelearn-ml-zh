- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Data for Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the process of labeling data for regression-based
    machine learning tasks, such as predicting housing prices, in situations where
    there is insufficient labeled data available for training. Regression tasks are
    tasks that involve predicting numerical values using a labeled training dataset,
    making them integral to fields such as finance and economics. However, real-world
    scenarios often present a challenge: labeled data is a precious commodity, often
    in short supply.'
  prefs: []
  type: TYPE_NORMAL
- en: If there is a short supply of labeled data to train a machine learning model,
    you can still use summary statistics, semi-supervised learning, and clustering
    to predict the target labels for your unlabeled data. We have demonstrated this
    using house price data as an example and generated the predicted labels for house
    prices programmatically using Python. We will look at different approaches to
    labeling data for regression using Snorkel libraries, semi-supervised learning,
    data augmentation, and K-means clustering methods. In real-world projects, it
    is challenging to get the labeled data required for training machine learning
    regression models. For example, adequate data may not be available to train the
    model to predict house prices. In those cases, we prepare the training data by
    using various Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using rules based on summary statistics to generate house price labels for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using semi-supervised learning to label regression data for house price prediction
    with regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating house price data labels with data augmentation to generate synthetic
    data for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-means clustering to label the house price data for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to generate labels for regression
    data using Python libraries programmatically. Furthermore, you’ll have the expertise
    required to overcome regression challenges adeptly, ensuring your data-driven
    endeavors steer toward success.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the California house price dataset ([https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices))
    for this chapter. You can download the `housing.csv` file from GitHub at the following
    path: [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/datasets](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/datasets).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to install Python 3.7+ and set up any of the following Python
    editors:'
  prefs: []
  type: TYPE_NORMAL
- en: The VS Code IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend following the complete code on GitHub to follow along with the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using summary statistics to generate housing price labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to generate house price labels using summary statistics
    of a small set of available labeled housing price data. This is useful in real-world
    projects when there is insufficient labeled data for regression tasks. In such
    scenarios, we will generate labeled data by creating some rules based on summary
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: We decode the significance of the data’s underlying trends. By computing the
    mean of each feature within the labeled training dataset, we embark on a journey
    to quantify the essence of the data. This approach ingeniously leverages distance
    metrics to unveil the closest match for a label, bestowing unlabeled data points
    with the wisdom of their labeled counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the data from the `housing.csv` file using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Snippet of the DataFrame](img/B18944_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Snippet of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the labeled data using `pd.read_csv`, we then compute the summary
    statistics for each feature by target label using the `groupby()` and `describe()`
    methods. This gives us the mean, standard deviation, minimum, maximum, and quartile
    values for each feature by target label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Summary statistics of the house price dataset](img/B18944_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Summary statistics of the house price dataset
  prefs: []
  type: TYPE_NORMAL
- en: Finding the closest labeled observation to match the label
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We then loop through each row in the unlabeled data and compute the distances
    to each target label’s summary statistics using Euclidean distance. We select
    the target label with the minimum distance as the predicted target label and assign
    it to the current row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Euclidean distance is the distance between two points on a plane. Here,
    the distance between two points `(x1, y1)` and `(x2, y2)` is `d = √[(x2 – x1)2
    + (y2 – y1)2]`. This is used to find similar points, that is, the closest point
    to an unlabeled data point in the labeled data points, so that we can assign the
    corresponding label from the labeled dataset to the unlabeled data point. unlabeled
    dataset) is calculated by combining the distance between all the features in the
    row. We assign the target label of a row with the minimum distance from the predicted
    target label to the current row in the unlabeled dataset. This helps us to assign
    labels to the unlabeled dataset based on the closest match to the label in the
    training dataset using distance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the outermost `for` loop reads one row at a time from unlabeled data
    and then performs the following steps on that row in the inner `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The outermost `for` loop reads one row at a time from unlabeled data and then
    performs the following steps on that row in the inner `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `for target` loop iterates over each target label in the `summary_stats`
    DataFrame. The `index` attribute of a DataFrame returns the row labels, which
    in this case are the target labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line initializes the `dist` variable to `0`, which we will use
    to accumulate the distance between the current unlabeled data point and the current
    target label’s summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `for col` loop iterates over each column in the `df_unlabeled` DataFrame.
    We want to compute the distance between the current unlabeled data point and each
    target label’s summary statistics for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line checks if the current column is not the target column. We
    don’t want to compute the distance between the current unlabeled data point and
    the summary statistics of the target column, as this would not make sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line computes the squared distance between the current unlabeled
    data point’s feature value and the corresponding feature’s mean value in the current
    target label’s summary statistics. We square the distance to make it positive
    and exaggerate the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line saves the computed distance in the `dists` dictionary for
    the current target label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By the end of this inner loop, the `dists` dictionary will contain the squared
    distances between the current unlabeled data point and each target label’s summary
    statistics. We will then select the target label with the minimum distance as
    the predicted target label for the current data point.
  prefs: []
  type: TYPE_NORMAL
- en: The same process continues for each row of the unlabeled data to compute distances
    from each of the target label’s features mean values in the summary statistics
    to the corresponding column in the unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we save the labeled data to a new CSV file using the `to_csv()` method,
    `df_unlabeled.to_csv('housing_result.csv')`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Labeled data with predicted median house value](img/B18944_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Labeled data with predicted median house value
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can see that `median_house_value` is assigned to the row in the unlabeled
    dataset. Note that this approach assumes that the summary statistics of the labeled
    data can be used to predict the target labels of the unlabeled data accurately.
    Therefore, it is essential to validate the accuracy of the predictions before
    using them in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Using semi-supervised learning to label regression data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to use semi-supervised learning to label the regression
    data. Semi-supervised learning is a type of machine learning that combines both
    labeled and unlabeled data to improve the accuracy of a predictive model. In semi-supervised
    learning, a small amount of labeled data is used with a much larger amount of
    unlabeled data to train the model. The idea is that the unlabeled data can provide
    additional information about the underlying patterns in the data that can help
    the model to learn more effectively. By using both labeled and unlabeled data,
    semi-supervised learning can improve the accuracy of machine learning models,
    especially when labeled data is scarce or expensive to obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look in detail at the pseudo-labeling method and how it is used for
    data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo-labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pseudo-labeling is a technique used in semi-supervised learning where a model
    trained on labeled data is used to predict the labels of the unlabeled data. These
    predicted labels are called pseudo-labels. The model then combines the labeled
    and pseudo-labeled data to retrain and improve the accuracy of the model. Pseudo-labeling
    is a way to leverage the unlabeled data to improve the performance of the model,
    especially when labeled data is limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo-labeling process involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train a model on labeled data**: Train a supervised learning model on the
    labeled data using a training algorithm. The model is fitted to the training set
    using the provided labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predict labels for unlabeled data**: Use the trained model to predict the
    labels for the unlabeled data. These predicted labels are called pseudo-labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Combine labeled and pseudo-labeled data**: Combine the labeled data with
    the pseudo-labeled data to form a new, larger training set. The pseudo-labeled
    data is treated as if it were labeled data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrain the model**: Retrain the model using the combined dataset. The model
    is updated using both the labeled and pseudo-labeled data to improve the model’s
    accuracy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Repeat steps 2-4**: Iterate the process by reusing the updated model to predict
    labels for new, previously unlabeled data, and combining the newly labeled data
    with the existing labeled data for the next round of model retraining, and the
    process is repeated until convergence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pseudo-labeling can be an effective way to leverage the large amount of unlabeled
    data that is typically available in many applications. By using this unlabeled
    data to improve the accuracy of the model, pseudo-labeling can help to improve
    the performance of supervised machine learning models, especially when enough
    labeled training data is not easily available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the house price dataset to predict the labels for regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load the house price dataset and then split the labeled data into the
    `labeled_data` DataFrame and unlabeled data into the `unlabeled_data` DataFrame,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet is used to divide the labeled data into two parts: a training
    set and a testing set. The training set contains the features (input data) and
    the corresponding labels (output data) that we will use to train our machine learning
    model. The testing set is a small portion of the data that we will use to evaluate
    the model’s performance. The `train_test_split` function from the `sklearn.model_selection`
    library helps us achieve this division while specifying the size of the testing
    set (in this case, 20% of the data). Let’s train the model using the training
    dataset for regression, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code snippet, we’re building and training a linear regression model
    using the labeled data. First, we import the `LinearRegression` class from the
    `sklearn.linear_model` library. Then, we create an instance of the linear regression
    model named `regressor`. Finally, we train the model using the training data (`train_data`)
    as the input features and the corresponding labels (`train_labels`) as the desired
    outputs. The model learns from this data to make predictions later. Now, let’s
    predict the labels using the regressor for the unlabeled dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code snippet, we’re employing the trained linear regression model to
    predict labels for unlabeled data points. We initialize an empty list, `predicted_labels`,
    to store the predictions. By applying the `predict` method of the trained `regressor`
    model, we generate predictions based on the features (input data) in the `unlabeled_data`.
    The `price` column is excluded since it’s the target variable we want to predict.
    The `predicted_labels` list now holds the predicted outcomes of the regression
    model for the unlabeled data. Now we will combine this predicted labeled data
    with the labeled data and train the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we’re creating a new dataset, `new_data`, by combining
    the labeled and the newly predicted data. First, we use `pd.concat` to concatenate
    the `labeled_data` and `unlabeled_data` dataframes, creating a continuous dataset.
    The `ignore_index=True` argument ensures that the index is reset for the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’re populating the `''price''` column in the `new_data` DataFrame.
    We achieve this by concatenating the `train_labels` (from the labeled data) with
    the predicted labels stored in the `predicted_labels` list. This step ensures
    that our new dataset has complete labels for all data points, combining both known
    and predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we’re training a new linear regression model on the combined
    dataset that includes both the labeled and predicted data. First, we split the
    combined data into new training and testing sets using the `train_test_split`
    function, similar to what we did before. The new training data is stored in `new_train_data`,
    and the corresponding labels are stored in `new_train_labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a new instance of the linear regression model called `new_regressor`.
    Finally, we train the new model using `new_train_data` as input features and `new_train_labels`
    as the desired outputs. This step ensures that our new model is fine-tuned to
    predict the combined data, leveraging both labeled and predicted information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Performance of the model after adding pseudo-labeled data](img/B18944_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Performance of the model after adding pseudo-labeled data
  prefs: []
  type: TYPE_NORMAL
- en: In this code snippet, we’re evaluating the performance of the new linear regression
    model on the test data that it hasn’t seen during training. The R-squared(coefficient
    of determination) score is calculated using the `score` method of the `new_regressor`
    model. The R^2 score is a measure of how well the model’s predictions match the
    actual data values. Higher R^2 scores indicate better predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the R^2 score is higher (`0.6905783112767134`) with the combined
    dataset than with the original labeled trained dataset (`0.624186740765541`).
    Finally, we use this model to predict the labels. Now, let’s see another method,
    data augmentation, to generate synthetic data with labels for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Using data augmentation to label regression data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data augmentation can be used to generate additional labeled data for regression
    tasks where labeled data is limited. Here is a way to use data augmentation to
    label regression data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect labeled data**: Collect the limited labeled data available for the
    regression task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define data augmentation techniques**: Define a set of data augmentation
    techniques that can be used to generate new data points from the available labeled
    data. For regression tasks, common data augmentation techniques include adding
    noise, scaling, and rotating the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generate augmented data**: Use data augmentation techniques to generate new
    data points from the available labeled data. The new data points will have labels
    based on the labels of the original data points.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: Train a regression model using the augmented data and
    the original labeled data. This step involves fitting a model to the combined
    dataset using a supervised learning algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: Evaluate the performance of the trained model on a
    validation set. This step involves testing the accuracy of the model’s predictions
    on new, unseen data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fine-tune the model**: Fine-tune the model based on the performance on the
    validation set. This step involves adjusting the model’s hyperparameters to improve
    its performance on the validation set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test the model**: Finally, test the model’s performance on a test set to
    evaluate its generalization performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using data augmentation to generate additional labeled data, it is possible
    to train a more accurate regression model even when limited labeled data is available.
    However, it is important to be careful when using data augmentation techniques
    to ensure that the generated data is meaningful and representative of the original
    data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of numerical data, we should focus on the following data augmentation
    techniques that are relevant and meaningful for the given dataset. For example,
    we can consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding noise**: Adding random noise to numerical features and labels can
    simulate variations and uncertainties in the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: Scaling numerical features can simulate changes in units or magnitudes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jittering**: Introducing small perturbations to numerical values can account
    for measurement errors or fluctuations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier injection**: Introducing outliers can help the model become more
    robust to extreme values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffling**: Randomly shuffling the order of data points can prevent the
    model from learning any sequence-related bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that the choice of data augmentation techniques should be based on
    the characteristics of your dataset and the problem you’re trying to solve. The
    techniques should add meaningful variations that align with the nature of your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we generate augmented data for the house price dataset to predict
    labels. Let’s import the necessary libraries, load the house price dataset, and
    define the `noise`, `scale`, and `rotate` data augmentation functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the labeled data stored in a CSV file named `labeled_data.csv`
    with columns for the features and a column named `price` for the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code defines two data augmentation techniques that add noise.
    It generates new data points by applying these augmentation techniques to the
    labeled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The range of data augmentation parameters for noise range is defined, and for
    each available data point, it generates multiple augmented data points with different
    parameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By iterating through each value in `noise_range` and adding noise to each data
    point’s `price` feature, the code generates multiple data points with different
    levels of noise. This process results in more labeled data points for the machine
    learning model to learn from and improves the model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '`noise_range` is a list of standard deviation values for generating different
    levels of noise. It could be any list of values to add different levels of noise
    to the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`for noise in noise_range` creates a loop that iterates through each value
    in the `noise_range` list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_row = row.copy()` creates a copy of the original data point (i.e., row).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_row["price"] = add_noise(row["price"], noise)` adds noise to the copied
    data point’s `price` feature using the `add_noise()` function. The `add_noise()`
    function adds random noise to each data point based on the standard deviation
    provided in the `noise` variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`augmented_data.append(new_row)` appends the newly generated data point to
    the `augmented_data` list. The `augmented_data` list contains all the newly generated
    data points for all levels of noise in the `noise_range` list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, let’s define another data augmentation scale function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The range of parameters for `scale_range` is defined, and for each available
    data point, it generates multiple augmented data points with different parameter
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we’re utilizing data augmentation to generate augmented
    data by applying scaling to the `price` feature. For each scale factor within
    the specified `scale_range`, we duplicate the current data row by creating a copy
    of it using `row.copy()`. Then, we apply scaling to the `price` feature using
    `scale_factor`, effectively modifying the price values while preserving the data’s
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the augmented row is added to the list of augmented data stored in
    the `augmented_data` list. This approach empowers us to explore how varying scales
    affect the `price` feature and enrich our dataset with diverse instances for improved
    model training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the augmented data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Original data and augmented data](img/B18944_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Original data and augmented data
  prefs: []
  type: TYPE_NORMAL
- en: 'The code then combines the original labeled data with the augmented data, splits
    it into training and testing sets, trains a linear regression model on the combined
    data, and evaluates the model’s performance on the test set using mean squared
    error as the evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: By iterating through each value in `noise_range` and adding noise to each available
    data point, it generates multiple augmented data points with different levels
    of noise. This process results in more labeled data points for the machine learning
    model to learn from and improves the model’s accuracy. Similarly, scale factor
    and rotation degree are used to generate labeled data using data augmentation
    to predict house prices using regression.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how to generate the augmented data using noise
    and scale techniques for regression. Now, let’s see how we can use the K-means
    clustering unsupervised learning method to label the house price data.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means clustering to label regression data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to use the unsupervised K-means clustering method
    to label the regression data. We use K-means to cluster data points into groups
    or clusters based on their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Once the clustering is done, we can compute the average label value for each
    cluster by taking the mean of the labeled data points that belong to that cluster.
    This is because the labeled data points in a cluster are likely to have similar
    label values since they are similar in terms of their feature values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Basic k-means clustering with no. of clusters =3](img/B18944_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Basic k-means clustering with no. of clusters =3
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we have a dataset of house prices with which we want
    to predict the price of a house based on features such as size, location, number
    of rooms, and so on. We have some labeled data points that consist of the features
    and their corresponding prices, but we also have some unlabeled data points with
    the same features.
  prefs: []
  type: TYPE_NORMAL
- en: We can use K-means clustering to cluster the labeled and unlabeled data points
    into groups based on their features. Then, we can compute the average price for
    each cluster by taking the mean of the labeled data points in that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can use these average prices to predict the prices of the unlabeled
    data points based on their cluster assignment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use these predicted labels to create a new dataset by combining the
    labeled and unlabeled data. We then train a new model on the combined data and
    evaluate its performance on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we import the necessary libraries and define the labeled and unlabeled
    data arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of clusters (`n_clusters`) and use k-means clustering
    to fit the model to the labeled features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We predict cluster labels for the unlabeled data using the trained k-means
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We calculate the average prices for each cluster by iterating through cluster
    indices and calculating the mean of the labeled prices for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The line `cluster_mask = (kmeans_model.labels_ == cluster_idx)` creates a boolean
    mask that identifies the data points in a specific cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of what each part of the line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kmeans_model.labels_`: This is an attribute of the K-means model that contains
    the cluster labels assigned to each data point during the clustering process.
    Each value in `kmeans_model.labels_` corresponds to the cluster label assigned
    to the corresponding data point in the order they appear in the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_idx`: This is the index of the cluster you’re interested in, ranging
    from 0 to the number of clusters minus one. It’s used to specify which cluster
    you want to create the mask for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kmeans_model.labels_ == cluster_idx`: This part creates a boolean array where
    each element is `True` if the corresponding data point’s cluster label is equal
    to `cluster_idx`, and `False` otherwise. Essentially, it’s checking which data
    points belong to the specific cluster of interest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_mask`: This is the resulting boolean mask that identifies the data
    points belonging to the cluster with the index `cluster_idx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, the line `cluster_mask = (kmeans_model.labels_ == cluster_idx)`
    creates a mask that helps you filter and select the data points in a specific
    cluster based on their assigned cluster labels. This mask can then be used to
    perform various operations on the data points belonging to that cluster. Predicted
    prices are assigned to the unlabeled data based on the calculated cluster average
    prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we display the predicted prices for the unlabeled data using the K-means
    clustering technique, providing insights into the potential housing prices for
    the unlabeled samples.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output for the predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Predicted price for unlabeled data based on the mean value of
    the labeled data cluster](img/B18944_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Predicted price for unlabeled data based on the mean value of the
    labeled data cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the output, we can predict the house price for unlabeled data using
    K-means clustering when there is a scarce training dataset. Then, we can combine
    the predicted labeled dataset and the original training dataset to fit the model
    using regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Overall, we have seen how clustering can be used in unsupervised learning to
    generate labels for unlabeled data. By computing the average label value for each
    cluster, we can effectively assign labels to the unlabeled data points based on
    their similarity to the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored a range of techniques to tackle the challenge
    of data labeling in regression tasks. We began by delving into the power of summary
    statistics, harnessing the mean of each feature in the labeled dataset to predict
    labels for unlabeled data. This technique not only simplifies the labeling process
    but also introduces a foundation for accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Further enriching our labeling arsenal, we ventured into semi-supervised learning,
    leveraging a small set of labeled data to generate pseudo-labels. The amalgamation
    of genuine and pseudo-labels in model training not only extends our labeled data
    but also equips our models to make more informed predictions for unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation has emerged as a vital tool in enhancing regression data.
    Techniques such as scaling and noise injection have breathed new life into our
    dataset, providing varied instances that empower models to discern patterns better
    and boost prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The utilization of k-means clustering rounded off our exploration, as we ventured
    into grouping data into clusters and assigning labels based on cluster mean values.
    This approach not only saves time but also bolsters the prediction precision of
    our models.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaways from this chapter are that summary statistics simplify data
    labeling by leveraging means and distances. Semi-supervised learning merges genuine
    and pseudo-labels for comprehensive training. Data augmentation techniques such
    as scaling and noise addition enrich and diversify datasets. K-means clustering
    optimizes labeling by grouping data into clusters and assigning cluster-wide mean
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: These acquired skills bestow resilience and versatility to our regression models,
    instilling them with the ability to handle real-world, unlabeled data effectively.
    In the next chapter, we’ll delve into the exploratory data analysis of image data
    in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Labeling Image Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part of the book, you will learn how to analyze image data, extract
    features from images, and label images using Python libraries such as Snorkel.
    The content also covers various methods of image data augmentation, along with
    the utilization of **support vector machine** (**SVM**), **convolutional** **neural
    network** (**CNN**), and pre-trained models such as YOLO for image classification
    and labeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18944_04.xhtml#_idTextAnchor081), *Exploring Image Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18944_05.xhtml#_idTextAnchor104), *Labeling Image Data Using
    Rules*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18944_06.xhtml#_idTextAnchor124), *Labeling Image Data Using
    Data Augmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
