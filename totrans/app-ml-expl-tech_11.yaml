- en: '*Chapter 9*: Other Popular XAI Frameworks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第九章*：其他流行的XAI框架'
- en: 'In the previous chapter, we covered the **TCAV framework** from **Google AI**,
    which is used for producing *human-friendly concept-based explanations*. We also
    discussed the other widely used explanation frameworks: **LIME** and **SHAP**.
    However, LIME, SHAP, and even TCAV have certain limitations, which we discussed
    in earlier chapters. None of these frameworks covers all the four dimensions of
    explainability for non-technical end-users. Due to these known drawbacks, the
    search for a robust **Explainable AI** (**XAI**) framework is still on.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了来自**谷歌AI**的**TCAV框架**，该框架用于生成基于概念的*人类友好型解释*。我们还讨论了其他广泛使用的解释框架：**LIME**和**SHAP**。然而，LIME、SHAP甚至TCAV都有一定的局限性，我们在前面的章节中讨论了这些局限性。这些框架中没有哪一个涵盖了非技术最终用户的所有四个可解释性维度。由于这些已知的缺点，寻找稳健的**可解释人工智能**（**XAI**）框架的工作仍在继续。
- en: The journey toward finding a robust XAI framework and addressing the known limitations
    of the popular XAI modules has led to the discovery and development of many other
    robust frameworks trying to address different aspects of ML model explainability.
    In this chapter, we will cover these other popular XAI frameworks apart from LIME,
    SHAP and TCAV.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找稳健的XAI框架和解决流行XAI模块的已知限制的过程，导致了许多其他稳健框架的发现和发展，这些框架试图解决ML模型可解释性的不同方面。在本章中，我们将介绍除了LIME、SHAP和TCAV之外的其他流行XAI框架。
- en: 'More specifically, we will discuss about the important features, and key advantages
    of each of these frameworks. We will also explore how you can apply each framework
    in practice. Covering everything about each framework is beyond the scope of this
    chapter. But in this chapter, you will learn the most important features and practical
    application of these frameworks. In this chapter, we will cover the following
    list of widely used XAI frameworks:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将讨论这些框架的重要功能和关键优势。我们还将探讨如何在实践中应用每个框架。涵盖每个框架的所有内容超出了本章的范围。但在本章中，您将了解这些框架的最重要功能和实际应用。在本章中，我们将涵盖以下广泛使用的XAI框架列表：
- en: DALEX
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALEX
- en: Explainerdashboard
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Explainerdashboard
- en: InterpretML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InterpretML
- en: ALIBI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALIBI
- en: DiCE
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DiCE
- en: ELI5
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELI5
- en: H2O AutoML explainer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O AutoML explainer
- en: At the end of the chapter, I will also share a quick comparison guide comparing
    all these XAI frameworks to help you to decide on the framework depending on your
    problem. Now, let's begin!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我还会分享一个快速比较指南，比较所有这些XAI框架，以帮助您根据您的问题选择框架。现在，让我们开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This code tutorial with necessary resources can be downloaded or cloned from
    the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09).
    Like the other chapters, the Python and Jupyter notebooks are used to implement
    the practical application of the theoretical concepts covered in this chapter.
    But I will recommend you run the notebooks only after you go through this chapter
    for a better understanding. Most of the datasets used in the tutorials are also
    provided in the code repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码教程及其必要资源可以从本章的GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09)。像其他章节一样，Python和Jupyter笔记本用于实现本章中涵盖的理论概念的实际应用。但我建议您在阅读完本章后再运行笔记本，以便更好地理解。教程中使用的多数数据集也提供在代码仓库中：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets)。
- en: DALEX
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DALEX
- en: In the *Dimensions of explainability* section of [*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014),
    *Foundational Concepts of Explainability Techniques*, we discussed the four different
    dimensions of explainability – *data*, *model*, *outcome*, and *end user*. Most
    explainability frameworks such as LIME, SHAP, and TCAV provide model-centric explainability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18216_01_ePub.xhtml#_idTextAnchor014)的“*可解释性的维度*”部分，我们讨论了可解释性的四个不同维度——*数据*、*模型*、*结果*和*最终用户*。大多数可解释性框架，如LIME、SHAP和TCAV，都提供以模型为中心的可解释性。
- en: '**DALEX** (**moDel Agnostic Language for Exploration and eXplanation**) is
    one of the very few widely used XAI frameworks that tries to address most of the
    dimensions of explainability. DALEX is model-agnostic and can provide some metadata
    about the underlying dataset to give some context to the explanation. This framework
    gives you insights into the model performance and model fairness, and it also
    provides global and local model explainability.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**DALEX**（**模型无关的探索和解释语言**）是少数广泛使用的 XAI 框架之一，它试图解决可解释性的大多数维度。DALEX 是模型无关的，并且可以提供一些关于底层数据集的元数据，以提供解释的上下文。此框架提供了对模型性能和模型公平性的洞察，并且它还提供了全局和局部模型可解释性。'
- en: 'The developers of the DALEX framework wanted to comply with the following list
    of requirements, which they have defined in order to explain complex black-box
    algorithms:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX 框架的开发者希望遵守以下要求列表，他们已经定义了这些要求，以便解释复杂的黑盒算法：
- en: '**Prediction''s justifications**: According to the developers of DALEX, ML
    model users should be able to understand the variable or feature attributions
    of the final prediction.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测的论证**：根据 DALEX 开发者的说法，机器学习模型用户应该能够理解最终预测的变量或特征归因。'
- en: '**Prediction''s speculations**: Hypothesizing the what-if scenarios or understanding
    the sensitivity of particular features of a dataset to the model outcome are other
    factors considered by the developers of DALEX.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测的推测**：假设情景或理解数据集特定特征对模型结果敏感性的假设是 DALEX 开发者考虑的其他因素。'
- en: '**Prediction''s validations**: For each predicted outcome of a model, the users
    should be able to verify the strength of the evidence that confirms a particular
    prediction of the model.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测的验证**：对于模型的每个预测结果，用户应该能够验证确认模型特定预测的证据强度。'
- en: 'DALEX is designed to comply with the preceding requirements using the various
    explanation methods provided by the framework. *Figure 9.1* illustrates the model
    exploration stack of DALEX:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX 设计为使用框架提供的各种解释方法来满足上述要求。*图 9.1* 展示了 DALEX 的模型探索堆栈：
- en: '![Figure 9.1 – The model exploration stack of DALEX'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – DALEX 的模型探索堆栈'
- en: '](img/B18216_09_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_001.jpg)'
- en: Figure 9.1 – The model exploration stack of DALEX
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – DALEX 的模型探索堆栈
- en: Next, I will walk you through an example of how to explore DALEX for explaining
    a black-box model in practice.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向您演示如何在实际中探索 DALEX 来解释黑盒模型。
- en: Setting up DALEX for model explainability
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 DALEX 以实现模型可解释性
- en: In this section, you will learn to setup DALEX in Python. Before starting the
    code walk-through, I would ask you to check the notebook at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb).
    It contains the steps needed to understand the concept that we are going to now
    discuss in depth. I also recommend that you take a look at the GitHub project
    repository of DALEX at [https://github.com/ModelOriented/DALEX/tree/master/python/dalex](https://github.com/ModelOriented/DALEX/tree/master/python/dalex)
    in case you need additional details while executing the notebook.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在 Python 中设置 DALEX。在开始代码演示之前，我建议您检查笔记本 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb)，其中包含理解我们现在将深入讨论的概念所需的步骤。我还建议您查看
    DALEX 的 GitHub 项目存储库 [https://github.com/ModelOriented/DALEX/tree/master/python/dalex](https://github.com/ModelOriented/DALEX/tree/master/python/dalex)，以防在执行笔记本时需要更多信息。
- en: 'The DALEX Python framework can be installed using the `pip` installer:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX Python 框架可以使用 `pip` 安装程序进行安装：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to use any additional features of DALEX that require an optional
    dependency, you can try the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用 DALEX 的任何需要可选依赖项的附加功能，您可以尝试以下命令：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can validate the successful installation of the package by importing it
    into the Jupyter notebooks using the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用以下命令将包导入 Jupyter 笔记本来验证包的安装是否成功：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hopefully, your import should be successful; otherwise, if you get any errors,
    you will need to reinstall the framework or separately install its dependencies.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您的导入操作能够成功；否则，如果您遇到任何错误，您将需要重新安装框架或单独安装其依赖项。
- en: Discussions about the dataset
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据集的讨论
- en: Next, let's briefly about the dataset that is being used for this example. For
    this example, I have used the *FIFA Club Position Prediction dataset* ([https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset))
    to predict the valuation of a football player in Euros, based on their skill and
    abilities. So, this is a regression problem that can be solved by regression ML
    models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们简要介绍用于此示例的数据集。对于这个示例，我使用了*FIFA俱乐部位置预测数据集*（[https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset)）来预测足球运动员在欧元联赛中的估值，基于他们的技能和能力。因此，这是一个可以通过回归机器学习模型解决的回归问题。
- en: FIFA Club Position dataset citation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: FIFA俱乐部位置数据集引用
- en: 'Bhattacharya A. (2022). Kaggle - FIFA Club Position Prediction dataset: [https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Bhattacharya A. (2022). Kaggle - FIFA俱乐部位置预测数据集：[https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset)
- en: Similar to all other standard ML solution flows, we start with the data inspection
    process. The dataset can be loaded as a pandas DataFrame, and we can inspect the
    dimension of the dataset, the features that are present, and the data type of
    each feature. Additionally, we can perform any necessary data transformation steps
    such as dropping irrelevant features, checking for missing values, and data imputation
    to fill in missing values for relevant features. I recommend that you follow the
    necessary steps provided in the notebook, but feel free to include other additional
    steps and explore the dataset in more depth.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有其他标准机器学习解决方案流程类似，我们首先进行数据检查过程。数据集可以作为pandas DataFrame加载，我们可以检查数据集的维度、存在的特征以及每个特征的数据类型。此外，我们可以执行任何必要的数据转换步骤，例如删除无关特征、检查缺失值以及为相关特征填充缺失值。我建议您遵循笔记本中提供的必要步骤，但请随意包括其他附加步骤并更深入地探索数据集。
- en: Training the model
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'For this example, I have used a random forest regressor algorithm to fit a
    model after dividing the data into the training set and the validation set. This
    can be done using the following lines of code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我使用随机森林回归算法在将数据分为训练集和验证集后拟合模型。这可以通过以下代码行完成：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We do minimum hyperparameter tuning to train the model as our objective is not
    to build a highly efficient model. Instead, our goal is to use this model as a
    black-box model and use DALEX to explain the model. So, let's proceed to the model
    explainability part using DALEX.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型的超参数调整尽可能少，因为我们的目标不是构建一个高度高效的模型。相反，我们的目标是使用这个模型作为黑盒模型，并使用DALEX来解释模型。因此，让我们继续使用DALEX进行模型可解释性部分的讨论。
- en: Model explainability using DALEX
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DALEX进行模型可解释性
- en: 'DALEX is model-agnostic as it does not assume anything about the model and
    can work with any algorithm. So, it considers the model as a black box. Before
    exploring how to use DALEX in Python, let''s discuss the following key advantages
    of this framework:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX对模型无偏见，因为它不对模型做任何假设，并且可以与任何算法一起工作。因此，它将模型视为黑盒。在探讨如何在Python中使用DALEX之前，让我们讨论以下该框架的关键优势：
- en: '**DALEX provides a uniform abstraction over different prediction models**:
    As an explainer, DALEX is quite robust and works well with different types of
    model frameworks such as scikit-learn, H2O, TensorFlow, and more. It can work
    with data provided in different formats such as a NumPy array or pandas DataFrame.
    It provides additional metadata about the data or the model, which makes it easier
    to develop an end-to-end model explainability pipeline in production.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DALEX为不同的预测模型提供了一个统一的抽象**：作为解释器，DALEX非常稳健，并且与不同的模型框架（如scikit-learn、H2O、TensorFlow等）配合良好。它可以处理不同格式的数据，如NumPy数组或pandas
    DataFrame。它提供了有关数据或模型的额外元数据，这使得在生产环境中开发端到端模型可解释性管道变得更加容易。'
- en: '**DALEX has a robust API structure**: The concise API structure of DALEX ensures
    that a consistent grammar and coding structure is used for model analysis. Using
    just a few lines of code, we can apply the various explainability methods and
    explain any black-box model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DALEX具有稳健的API结构**：DALEX简洁的API结构确保了在模型分析中使用了统一的语法和编码结构。只需几行代码，我们就可以应用各种可解释性方法，并解释任何黑盒模型。'
- en: '**It can provide local explanations for an inference data instance**: Prediction-level
    explainability for a single inference data instance can be easily obtained during
    DALEX. There are different methods available in DALEX such as interactive breakdown
    plots, SHAP feature importance plots, and what-if analysis plots, which can be
    used for local explanations. We will cover these methods, in more detail, in the
    next section.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它可以提供推理数据实例的局部解释**：在 DALEX 中，可以轻松获得单个推理数据实例的预测级可解释性。DALEX 中有不同方法可供选择，如交互式分解图、SHAP
    特征重要性图和假设分析图，这些都可以用于局部解释。我们将在下一节中更详细地介绍这些方法。'
- en: '**It can also provide global explanations while considering the entire dataset
    and the model**: Model-level global explanations can also be provided using DALEX
    partial dependence plots, accumulated dependence plots, global variable importance
    plots, and more.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它还可以在考虑整个数据集和模型的情况下提供全局解释**：使用 DALEX 的部分依赖图、累积依赖图、全局变量重要性图等方法，也可以提供模型级全局解释。'
- en: '**Bias and fairness checks can be easily done using DALEX**: DALEX provides
    quantitative ways in which to measure model fairness and bias. Unlike DALEX, most
    of the XAI frameworks do not provide explicit methods to evaluate model fairness.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 DALEX 可以轻松地进行偏差和公平性检查**：DALEX 提供了衡量模型公平性和偏差的定量方法。与 DALEX 不同，大多数 XAI 框架都没有提供评估模型公平性的明确方法。'
- en: '**The DALEX ARENA platform can be used to build an interactive dashboard for
    better user engagement**: DALEX can be used to create an interactive web app platform
    that can be used to design a custom dashboard to show interactive visualizations
    for the different model explainability methods that are available in DALEX. I
    think this unique feature of DALEX gives you the opportunity to create better
    user engagement by providing a tailor-made dashboard to meet the specific end
    user''s needs.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DALEX ARENA 平台可以用来构建一个交互式仪表板以增强用户体验**：DALEX 可以用来创建一个交互式网络应用平台，可以用来设计一个自定义仪表板，以展示
    DALEX 中可用的不同模型可解释性方法的交互式可视化。我认为 DALEX 的这一独特功能为您提供了通过提供定制仪表板来满足特定最终用户需求的机会。'
- en: Considering all of these key benefits, let's now proceed with learning how to
    apply these features available in DALEX.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些关键优势，我们现在继续学习如何应用 DALEX 中可用的这些功能。
- en: 'First, we need to create a DALEX model explainer object, which takes the trained
    model, data, and model type as input. This can be done using the following lines
    of code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个 DALEX 模型解释器对象，它接受训练好的模型、数据和模型类型作为输入。这可以通过以下代码行完成：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the explainer object has been created, it also provides additional metadata
    about the model, which is shown as follows.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了解释器对象，它还提供了有关模型的附加元数据，如下所示。
- en: '![Figure 9.2 – The DALEX explainer metadata'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.2 – The DALEX explainer metadata]'
- en: '](img/B18216_09_002.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_09_002.jpg]'
- en: Figure 9.2 – The DALEX explainer metadata
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – DALEX 解释器元数据
- en: This initial metadata is very useful for building automated pipelines for certain
    production-level systems. Next, let's explore some model-level explanations provided
    by DALEX.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这初始元数据对于构建某些生产级系统的自动化管道非常有用。接下来，让我们探索一些 DALEX 提供的模型级解释。
- en: Model-level explanations
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型级解释
- en: 'Model-level explanations are global explanations produced by DALEX. The consider
    model performance and the overall impact of all the features considered during
    prediction. The performance of a model can be checked using a single line of code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型级解释是由 DALEX 生成的全局解释。它考虑了模型性能以及预测过程中考虑的所有特征的总体影响。可以使用一行代码来检查模型的性能：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Depending upon the type of ML model, different model evaluation metrics can
    be applied. In this example, we are dealing with a regression problem, and hence,
    DALEX uses the metrics MSE, RMSE, R2, MAE, and so on. For a classification problem,
    metrics such as accuracy, precision, recall, and more will be used. As covered
    in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053), *Data-Centric Approaches*,
    by evaluating the model performance, we get to estimate the *data forecastability*
    of the model, which gives us an indication of the degree of correctness of the
    predicted outcome.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器学习模型的类型，可以应用不同的模型评估指标。在本例中，我们处理的是一个回归问题，因此 DALEX 使用 MSE、RMSE、R2、MAE 等指标。对于分类问题，将使用准确率、精确度、召回率等指标。如在第
    [*3章*](B18216_03_ePub.xhtml#_idTextAnchor053) 中所述的 *以数据为中心的方法*，通过评估模型性能，我们可以估计模型的
    *数据可预测性*，这为我们提供了预测结果正确程度的指示。
- en: 'DALEX provides methods such as global feature importance, **partial dependence
    plots** (**PDPs**), and accumulated dependency plots to analyze the feature-based
    explanations for model-level predictions. First, let''s try out the variable of
    feature importance plots:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX提供全局特征重要性、**部分依赖图**（**PDPs**）和累积依赖图等方法来分析模型级别预测的特征解释。首先，让我们尝试特征重要性图变量：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will produce the following plot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 9.3 – A feature importance plot from DALEX for global feature-based
    explanations'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3 – DALEX的全球特征解释特征重要性图'
- en: '](img/B18216_09_003.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_003.jpg)'
- en: Figure 9.3 – A feature importance plot from DALEX for global feature-based explanations
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – DALEX的全球特征解释特征重要性图
- en: In *Figure 9.3*, we can see that the trained model considers the abilities of
    the players, which comprise the overall rating of the player, the potential rating
    of the player, and other abilities such as pace, dribbling skill, strength, and
    stamina to be the most important factors for deciding the player's valuation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.3*中，我们可以看到训练模型认为球员的能力，包括球员的整体评分、球员的潜力评分以及其他能力，如速度、盘带技巧、力量和耐力，是决定球员估值的最重要因素。
- en: 'Similar to feature importance, we can generate PDPs. Accumulated dependency
    plots can also be generated using the following few lines of code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与特征重要性类似，我们可以生成PDPs。使用以下几行代码也可以生成累积依赖图：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will create the following plots for the aggregated profiles of the players:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为球员的聚合配置文件创建以下图表：
- en: '![Figure 9.4 – A PDP aggregate profile plot for the age and potential features
    with'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – 年龄和潜力特征的PDP聚合配置文件图，用于模型级别解释'
- en: predictions for model-level explanations
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型级别解释的预测
- en: '](img/B18216_09_004.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_004.jpg)'
- en: Figure 9.4 – A PDP aggregate profile plot for the age and potential features
    with predictions for model-level explanations
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 年龄和潜力特征的PDP聚合配置文件图，用于模型级别解释
- en: '*Figure 9.4* shows how the overall features of `age` and `potential` vary with
    the predicted valuation of football players. From the plot, we can understand
    that with an increase in the player''s age, the predicted valuation decreases.
    Similarly, with an increase in a player''s potential rating, the player''s valuation
    increases. All of these observations are also quite consistent with the real-world
    observation for deciding a player''s valuation. Next, let''s see how to obtain
    a prediction-level explanation using DALEX.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.4*展示了`年龄`和`潜力`这两个整体特征如何随着预测的足球运动员估值变化。从图中我们可以看出，随着球员年龄的增加，预测估值会下降。同样，随着球员潜力评级的提高，球员的估值会增加。所有这些观察结果也与决定球员估值的真实世界观察结果非常一致。接下来，让我们看看如何使用DALEX获取预测级别解释。'
- en: Prediction-level explanations
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测级别解释
- en: DALEX can provide model-agnostic local or prediction-level explanations along
    with a global explanation. It uses techniques such as interactive breakdown profiles,
    SHAP feature importance values, and Ceteris Paribus profiles (what-if profiles)
    to explain model predictions at the individual data instance level. To understand
    the practical importance of these techniques, let's use these techniques for our
    use case to explain an ML model trained to predict the overall valuation of a
    football player. For our example, we will compare the prediction-level explanations
    of three players – Cristiano Ronaldo, Lionel Messi, and Jadon Sancho.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX可以提供模型无关的局部或预测级别的解释，以及全局解释。它使用交互式分解配置文件、SHAP特征重要性值和Ceteris Paribus配置文件（假设配置文件）等技术，在单个数据实例级别上解释模型预测。为了理解这些技术的实际重要性，让我们使用这些技术来解释一个用于预测足球运动员整体估值的人工智能模型。在我们的例子中，我们将比较三位球员——克里斯蒂亚诺·罗纳尔多、莱昂内尔·梅西和贾登·桑乔的预测级别解释。
- en: 'First, let''s try out interactive breakdown plots. This can be done using the
    following lines of code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试交互式分解图。这可以通过以下代码行完成：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will generate the following interactive breakdown profile plot for each
    player:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为每位球员生成以下交互式分解配置文件图：
- en: '![Figure 9.5 – An interactive breakdown plot from DALEX'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5 – DALEX的交互式分解图'
- en: '](img/B18216_09_005.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_005.jpg)'
- en: Figure 9.5 – An interactive breakdown plot from DALEX
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – DALEX的交互式分解图
- en: '*Figure 9.5* shows the interactive breakdown plot comparing the model predictions
    for the three selected players. This plot illustrates the contribution of each
    feature to the final predicted value. The feature values that increase the model
    prediction value are shown in a different color than the features that decrease
    the prediction value. This plot shows the breakdown of the total predicted value
    with respect to each feature value of the data instance.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.5* 展示了比较三个选定球员模型预测的交互式分解图。此图说明了每个特征对最终预测值的贡献。增加模型预测值的特征值与降低预测值的特征值以不同的颜色显示。此图显示了总预测值相对于数据实例中每个特征值的分解。'
- en: 'Now, all three players are world-class professional football players; however,
    Ronaldo and Messi are veteran players and living legends of the game, as compared
    to Sancho, who is a young talent. So, if you observe the plot, it shows that for
    Ronaldo and Messi, the `age` feature reduces the predicted value, while for Sancho,
    it slightly increases. It is quite interesting to observe that the model has been
    able to learn how increasing the age of football players can reduce their valuation.
    This observation is also consistent with the observation of domain experts who
    value younger players with higher potential to have a higher market value. Similar
    to breakdown plots, DALEX also provides SHAP feature importance plots to analyze
    the contribution of the features. This method gives similar information such as
    breakdown plots, but the feature importance is calculated based on SHAP values.
    It can be obtained using the following lines of code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有三名球员都是世界级的职业足球运动员；然而，与年轻的桑乔相比，C罗和梅西是经验丰富的球员，也是足球界的传奇人物。因此，如果你观察这个图，它显示对于C罗和梅西，`年龄`特征降低了预测值，而对于桑乔，它略有增加。观察到一个模型能够学习到随着年龄的增长足球球员的估值如何降低是非常有趣的。这一观察结果也与领域专家的观察结果一致，他们认为具有更高潜力的年轻球员具有更高的市场价值。与分解图类似，DALEX
    还提供了 SHAP 特征重要性图来分析特征的贡献。这种方法提供了与分解图类似的信息，但特征重要性是基于 SHAP 值计算的。可以使用以下代码行获得：
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we will use *What-If* plots based on the **Ceteris Paribus profile**
    in DALEX. The Ceteris Paribus profile is similar to **Sensitivity Analysis**,
    which was covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model
    Explainability Methods*. It is based on the *Ceteris Paribus principle*, which
    means that when everything else remains unchanged, we can determine how a change
    in a particular feature will affect the model prediction. This process is often
    referred to as **What-If model analysis** or **Individual Conditional Expectations**.
    In terms of application, in our example, we can use this method to find out how
    the predicted valuation of Jadon Sancho might vary as he grows older or if his
    overall potential increases. We can find this out by using the following lines
    of code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用基于 DALEX 中的 **Ceteris Paribus 配置文件**的“如果...会怎样”图。Ceteris Paribus 配置文件类似于在
    [*第 2 章*](B18216_02_ePub.xhtml#_idTextAnchor033) “模型可解释性方法”中介绍的 **敏感性分析**。它基于
    *Ceteris Paribus 原则*，这意味着当其他一切保持不变时，我们可以确定特定特征的改变将如何影响模型预测。这个过程通常被称为 **“如果...会怎样”模型分析**或**个体条件期望**。在应用方面，在我们的例子中，我们可以使用这种方法来找出贾登·桑乔的预测估值如何随着他年龄的增长或整体潜力的增加而变化。我们可以通过以下代码行来找出这一点：
- en: '[PRE44]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This will produce the following interactive what-if plot in DALEX:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 DALEX 中生成以下交互式“如果...会怎样”图：
- en: '![Figure 9.6 – An interactive what-if plot in DALEX'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6 – DALEX 中的交互式“如果...会怎样”图'
- en: '](img/B18216_09_006.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_006.jpg)'
- en: Figure 9.6 – An interactive what-if plot in DALEX
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – DALEX 中的交互式“如果...会怎样”图
- en: '*Figure 9.6* shows that for Jadon Sancho, the market valuation will start decreasing
    as he grows older; however, it can also increase with an increase in overall potential
    ratings. I would strongly recommend that you explore all of these prediction-level
    explanation options for the features and from the tutorial notebook provided in
    the project repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb).
    Next, we will use DALEX to evaluate model fairness.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.6*显示，对于贾登·桑乔来说，随着他年龄的增长，市场估值将开始下降；然而，随着整体潜力评分的增加，它也可能上升。我强烈建议您探索所有这些预测级别的解释选项，并从项目存储库中提供的教程笔记本中进行：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb)。接下来，我们将使用DALEX评估模型公平性。'
- en: Evaluating model fairness
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型公平性
- en: A **Model Fairness** check is another important feature of DALEX. Although,
    model fairness and bias detection are more important to consider for classification
    problems relying on features related to gender, race, ethnicity, nationality,
    and other similar demographic features. However, we will apply this to regression
    models, too. For more details regarding model fairness checks using DALEX, please
    refer to [https://dalex.drwhy.ai/python-dalex-fairness.html](https://dalex.drwhy.ai/python-dalex-fairness.html).
    Now, let's see whether our model is free from any bias and is fair!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型公平性**检查是DALEX的另一个重要功能。尽管模型公平性和偏差检测对于依赖于与性别、种族、民族、国籍和其他类似人口统计特征的分类问题更为重要，但我们也将将其应用于回归模型。有关使用DALEX进行模型公平性检查的更多详细信息，请参阅[https://dalex.drwhy.ai/python-dalex-fairness.html](https://dalex.drwhy.ai/python-dalex-fairness.html)。现在，让我们看看我们的模型是否没有偏差且是公平的！'
- en: We will create a **protected variable** and **privileged variable** for the
    fairness check. In fairness checks in ML, we try to ensure that a protected variable
    is free from any bias. If we anticipate any feature value or group to have any
    bias due to factors such as an imbalanced dataset, we can declare them as privileged
    variables. For our use case, we will perform a fairness check for three different
    sets of players based on their age.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为公平性检查创建一个**受保护变量**和一个**特权变量**。在机器学习的公平性检查中，我们试图确保受保护变量不受任何偏差的影响。如果我们预计任何特征值或群体由于数据集不平衡等因素而存在偏差，我们可以将它们声明为特权变量。对于我们的用例，我们将根据球员的年龄进行三组不同球员的公平性检查。
- en: 'All players less than 20 years are considered to be *youth* players, players
    between the ages of 20 and 30 are considered to be *developing* players, and players
    above 30 years are considered to be *developed* players. Now, let''s do our fairness
    check using DALEX:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所有20岁以下的球员被视为*青年*球员，20至30岁的球员被视为*发展*球员，30岁以上的球员被视为*成熟*球员。现在，让我们使用DALEX进行公平性检查：
- en: '[PRE50]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is the outcome of the fairness checks:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是公平性检查的结果：
- en: '[PRE55]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can also check the quantitative evidence of the fairness checks and plot
    them to analyze further:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查公平性检查的定量证据，并将它们绘制出来以进行进一步分析：
- en: '[PRE56]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This will generate the following plot for analyzing model fairness checks using
    DALEX:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下用于分析使用DALEX进行模型公平性检查的图表：
- en: '![Figure 9.7 – A model fairness plot using DALEX'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7 – 使用DALEX的模型公平性图]'
- en: '](img/B18216_09_007.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18216_09_007.jpg]'
- en: Figure 9.7 – A model fairness plot using DALEX
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 使用DALEX的模型公平性图
- en: As shown in *Figure 9.7*, the model fairness using DALEX for regression models
    is done with respect to the metrics of independence, separation, and sufficiency.
    For classification models, these metrics could vary, but the API function usage
    is the same. Next, we will discuss the ARENA web-based tool for building interactive
    dashboards using DALEX.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.7*所示，使用DALEX对回归模型进行公平性分析是根据独立性、分离性和充分性等指标进行的。对于分类模型，这些指标可能有所不同，但API函数的使用方式相同。接下来，我们将讨论用于使用DALEX构建交互式仪表板的基于Web的ARENA工具。
- en: Interactive dashboards using ARENA
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ARENA的交互式仪表板
- en: Another interesting feature of DALEX is the ARENA dashboard platform to create
    an interactive web app that can be used to design a custom dashboard for keeping
    all the DALEX interactive visualizations that were obtained using different model
    explainability methods. This particular feature of DALEX gives us an opportunity
    to create better user engagement by creating a custom dashboard for a specific
    problem.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: DALEX的另一个有趣特性是ARENA仪表板平台，可以创建一个交互式Web应用程序，用于设计用于保存使用不同模型可解释性方法获得的DALEX交互式可视化元素的自定义仪表板。这一特定特性为我们提供了一个机会，通过为特定问题创建自定义仪表板来提高用户参与度。
- en: 'Before starting, we need to create a DALEX Arena dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要创建一个DALEX Arena数据集：
- en: '[PRE58]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we need to create an `Arena` object and push the DALEX explainer object
    created from the black-box model that is being explained:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个`Arena`对象，并将从被解释的黑盒模型创建的DALEX解释器对象推送到它：
- en: '[PRE59]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Following this, we just need to push the Arena dataset and start the server
    to make our Arena platform live:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们只需推送Arena数据集并启动服务器，使我们的Arena平台上线：
- en: '[PRE62]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Based on the port provided, the DALEX server will be running on [https://arena.drwhy.ai/?data=http://127.0.0.1:9294/](https://arena.drwhy.ai/?data=http://127.0.0.1:9294/).
    Initially, you will get a blank dashboard, but you can easily drag and drop the
    visuals from the right-hand side panel to make your own custom dashboard, as shown
    in the following screenshot:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的端口，DALEX服务器将在[https://arena.drwhy.ai/?data=http://127.0.0.1:9294/](https://arena.drwhy.ai/?data=http://127.0.0.1:9294/)上运行。最初，您将看到一个空白的仪表板，但您可以从右侧面板轻松拖放视觉元素来创建自己的自定义仪表板，如下面的截图所示：
- en: '![Figure 9.8 – An interactive Arena dashboard created using DALEX'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.8 – An interactive Arena dashboard created using DALEX](Figure 9.8
    – An interactive Arena dashboard created using DALEX)'
- en: '](img/B18216_09_008.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_09_008.jpg](img/B18216_09_008.jpg)'
- en: Figure 9.8 – An interactive Arena dashboard created using DALEX
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 使用DALEX创建的交互式Arena仪表板
- en: Also, you can load an existing dashboard from a configuration JSON or export
    a build dashboard as a configuration JSON file. Try recreating the dashboard,
    as shown in *Figure 9.8*, using the configuration JSON file provided in the code
    repository at [https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json](https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以从配置JSON文件中加载现有的仪表板或将构建的仪表板导出为配置JSON文件。尝试使用代码库中提供的配置JSON文件重新创建仪表板，如图*9.8*所示，该代码库位于[https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json](https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json)。
- en: Overall, I have found DALEX to be a very interesting and powerful XAI framework.
    There are many more examples available at [https://github.com/ModelOriented/DALEX](https://github.com/ModelOriented/DALEX)
    and [https://github.com/ModelOriented/DrWhy/blob/master/README.md](https://github.com/ModelOriented/DrWhy/blob/master/README.md).
    Please do explore all of them. However, DALEX seems to be restricted to structured
    data. I think as a future scope, making DALEX easily applicable with image and
    text data would increase its adoption across the AI research community. In the
    next section, we will explore Explainerdashboard, which is another interesting
    XAI framework.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我发现DALEX是一个非常有趣且强大的XAI框架。在[https://github.com/ModelOriented/DALEX](https://github.com/ModelOriented/DALEX)和[https://github.com/ModelOriented/DrWhy/blob/master/README.md](https://github.com/ModelOriented/DrWhy/blob/master/README.md)上还有更多示例可供参考。请务必探索它们。然而，DALEX似乎仅限于结构化数据。我认为作为一个未来的发展方向，使DALEX能够轻松应用于图像和文本数据将增加其在人工智能研究社区中的采用率。在下一节中，我们将探讨Explainerdashboard，这是另一个有趣的XAI框架。
- en: Explainerdashboard
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Explainerdashboard
- en: The AI research community has always considered interactive visualization to
    be an important approach for interpreting ML model predictions. In this section,
    we will cover **Explainerdashboard**, which is an interesting Python framework
    that can spin up a comprehensive interactive dashboard covering various aspects
    of model explainability with just minimal lines of code. Although this framework
    supports only scikit-learn-compatible models (including XGBoost, CatBoost, and
    LightGBM), it can provide model-agnostic global and local explainability. Currently,
    it supports SHAP-based feature importance and interactions, PDPs, model performance
    analysis, what-if model analysis, and even decision-tree-based breakdown analysis
    plots.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能研究社区一直认为交互式可视化是解释机器学习模型预测的重要方法。在本节中，我们将介绍 **Explainerdashboard**，这是一个有趣的
    Python 框架，只需极少的代码即可启动一个涵盖模型可解释性各个方面的全面交互式仪表板。尽管这个框架只支持与 scikit-learn 兼容的模型（包括
    XGBoost、CatBoost 和 LightGBM），但它可以提供模型无关的全局和局部可解释性。目前，它支持基于 SHAP 的特征重要性和交互，PDPs，模型性能分析，假设模型分析，甚至基于决策树的分解分析图。
- en: The framework allows customization of the dashboard, but I think the default
    version includes all supported aspects of model explainability. The generated
    web-app-based dashboards can be exported as static web pages directly from a live
    dashboard. Otherwise, the dashboards can be programmatically deployed as a web
    app through an automated **Continuous Integration** (**CI**)/**Continuous Deployment**
    (**CD**) deployment process. I recommend that you go through the official documentation
    of the framework ([https://explainerdashboard.readthedocs.io/en/latest/](https://explainerdashboard.readthedocs.io/en/latest/))
    and the GitHub project repository ([https://github.com/oegedijk/explainerdashboard](https://github.com/oegedijk/explainerdashboard))
    before we get started with the walk-through tutorial example next.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架允许自定义仪表板，但我觉得默认版本已经包括了模型可解释性的所有支持方面。生成的基于网页的仪表板可以直接从实时仪表板导出为静态网页。否则，仪表板可以通过自动化的
    **持续集成** (**CI**)/**持续部署** (**CD**) 部署流程以网页应用的形式进行程序化部署。我建议在开始教程演示之前，您先阅读框架的官方文档([https://explainerdashboard.readthedocs.io/en/latest/](https://explainerdashboard.readthedocs.io/en/latest/))和
    GitHub 项目仓库([https://github.com/oegedijk/explainerdashboard](https://github.com/oegedijk/explainerdashboard))。
- en: Setting up Explainerdashboard
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Explainerdashboard
- en: The complete tutorial notebook is provided in the code repository for this chapter
    at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb).
    However, in this section, I will provide a complete walk-through of the tutorial.
    The same *FIFA Club Position Prediction dataset* ([https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset))
    will be used for this tutorial, too. But instead of using the dataset to predict
    the valuation of football players, here, I will use this dataset to predict the
    league position of the football club for the next season based on the skills and
    ability of the football players playing for the club.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的完整教程笔记本已提供在代码仓库中，链接为 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb)。然而，在本节中，我将提供一个完整的教程演示。同样，我们将使用
    *FIFA Club Position Prediction 数据集* ([https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset](https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset))
    进行本教程，但在这里，我将使用这个数据集来预测基于俱乐部球员技能和能力，下赛季足球俱乐部的联赛位置。
- en: The real-world task of predicting a club league position for a future season
    is more complex, and there are several other variables that need to be included
    to get an accurate prediction. However, this prediction problem is solely based
    on the quality of the players playing for the club.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来赛季俱乐部联赛位置的现实任务更为复杂，需要包含几个其他变量才能得到准确的预测。然而，这个预测问题仅基于为俱乐部效力的球员质量。
- en: 'To get started with the tutorial, you will need to install all of the required
    dependencies to run the notebook. If you have executed all the previous tutorial
    examples, then most of the Python modules should be installed, except for `explainerdashboard`
    using the `pip` installer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The Explainerdashboard framework does have a dependency on the `graphviz` module,
    which makes it slightly tedious to install depending on your system. At the time
    of writing, I have discovered that version 0.18 works best with Explainerdashboard.
    This can be installed using the pip installer:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`graphviz` binaries depending on the operating system that you are using. Please
    visit [https://graphviz.org/](https://graphviz.org/) to find out more. Additionally,
    if you are facing any friction during the setup of this module, take a look at
    the installation instructions provided at [https://pypi.org/project/graphviz/](https://pypi.org/project/graphviz/).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We will consider this ML problem to be a regression problem. Therefore, similar
    to the DALEX example, we will need to perform the same data preprocessing, feature
    engineering, model training, and evaluation steps. I recommend that you follow
    the steps provided in the notebook at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb).
    This contains the necessary details to get the trained model. We will use this
    trained model as a black box and use Explainerdashboard to explain it in the next
    section.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Model explainability with Explainerdashboard
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the installation of the Explainerdashboard Python module is successful,
    you can import it to verify the installation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'For this example, we will use the `RegressionExplainer` and `ExplainerDashboard`
    submodules. So, we will load the specific submodules:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, using just two lines of code, we can spin up the `ExplainerDashboard`
    submodule for this example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Once this step is running successfully, the dashboard should be running in `localhost`
    with port `8050` as the default port. So, you can visit `http://localhost:8050/`
    in the browser to view your explainer dashboard.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lists the different explainability methods provided by Explainerdashboards:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature importance**: Similar to other XAI frameworks, feature importance
    is an important method for gaining an understanding of the overall contribution
    of each attribute used for prediction. This framework uses SHAP values, permutation
    importance, and PDPs to analyze the contribution of each feature for the model
    prediction:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Contribution plots and PDPs from Explainerdashboard'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_009.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Contribution plots and PDPs from Explainerdashboard
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Model performance**: Similar to DALEX, Explainerdashboard also allows you
    to analyze the model performance. For classification models, it uses metrics such
    as precision plots, confusion matrices, ROC-AUC plots, PR AUC plots, and more.
    For regression models, we will see plots such as residual plots, goodness-of-fit
    plots, and more:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能**：与DALEX类似，Explainerdashboard也允许您分析模型性能。对于分类模型，它使用精确度图、混淆矩阵、ROC-AUC图、PR
    AUC图等指标。对于回归模型，我们将看到残差图、拟合优度图等更多图表：'
- en: '![Figure 9.10 – Model performance analysis plots for regression models in Explainerdashboard'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.10 – Explainerdashboard中回归模型的模型性能分析图'
- en: '](img/B18216_09_010.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_010.jpg)'
- en: Figure 9.10 – Model performance analysis plots for regression models in Explainerdashboard
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – Explainerdashboard中回归模型的模型性能分析图
- en: '**Prediction-level analysis**: Explainerdashboard provides interesting and
    interactive plots for getting local explanations. This is quite similar to other
    Python frameworks. It is very important to have for analyzing prediction-level
    outcomes.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测级分析**：Explainerdashboard提供了有趣的交互式图表，用于获取局部解释。这与其他Python框架非常相似。对于分析预测级结果来说，这一点非常重要。'
- en: '**What-if analysis**: Another interesting option that Explainerdashboard provides
    is the what-if analysis feature. We can use this feature to vary the feature values
    and observe how the overall prediction gets changed. I find what-if analysis to
    be very useful for providing prescriptive insights:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设分析**：Explainerdashboard提供的另一个有趣选项是假设分析功能。我们可以使用此功能来改变特征值，并观察整体预测如何改变。我发现假设分析对于提供规范性见解非常有用：'
- en: '![Figure 9.11 – What-if model analysis using Explainerdashboard'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.11 – 使用Explainerdashboard进行假设模型分析'
- en: '](img/B18216_09_011.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_011.jpg)'
- en: Figure 9.11 – What-if model analysis using Explainerdashboard
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 使用Explainerdashboard进行假设模型分析
- en: '**Feature dependence and interactions**: Analyzing the dependency and interactions
    between different features is another interesting explainability method provided
    in Explainerdashboard. Mostly, it uses SHAP methods for analyzing feature dependence
    and interactions.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征依赖和交互**：分析不同特征之间的依赖和交互是Explainerdashboard提供的另一种有趣的解释方法。大多数情况下，它使用SHAP方法来分析特征依赖和交互。'
- en: '**Decision tree surrogate explainers**: Explainerdashboard uses decision trees
    as surrogate explainers. Additionally, it uses the decision tree breakdown plot
    for model explainability:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树代理解释器**：Explainerdashboard使用决策树作为代理解释器。此外，它还使用决策树分解图来提高模型的可解释性：'
- en: '![Figure 9.12 – Decision tree surrogate explainers in Explainerdashboard'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.12 – Explainerdashboard中的决策树代理解释器'
- en: '](img/B18216_09_012.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_09_012.jpg)'
- en: Figure 9.12 – Decision tree surrogate explainers in Explainerdashboard
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – Explainerdashboard中的决策树代理解释器
- en: To stop running the dashboards on your local system, you can simply interrupt
    the notebook cell.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止在本地系统上运行仪表板，您只需中断笔记本单元格即可。
- en: 'Explainerdashboard offers you many customization options as well. To customize
    your own dashboard from the given template, it is recommended that you refer to
    [https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard](https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard).
    You can also build multiple dashboards and compile all the dashboards as an explainer
    hub: [https://github.com/oegedijk/explainerdashboard#explainerhub](https://github.com/oegedijk/explainerdashboard#explainerhub).
    To deploy dashboards into a live web app that is accessible from anywhere, I would
    recommend you look at [https://github.com/oegedijk/explainerdashboard#deployment](https://github.com/oegedijk/explainerdashboard#deployment).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Explainerdashboard为您提供了许多自定义选项。要从给定的模板中自定义自己的仪表板，建议您参考[https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard](https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard)。您还可以构建多个仪表板，并将所有仪表板编译为一个解释中心：[https://github.com/oegedijk/explainerdashboard#explainerhub](https://github.com/oegedijk/explainerdashboard#explainerhub)。要将仪表板部署到任何地方都可以访问的实时Web应用中，我建议您查看[https://github.com/oegedijk/explainerdashboard#deployment](https://github.com/oegedijk/explainerdashboard#deployment)。
- en: In comparison to DALEX, I would say that Explainerdashboard is slightly behind
    as it is only restricted to scikit-learn-compatible models. This means that with
    complex deep learning models built on unstructured data such as images and text,
    you can't use this framework. However, I found it easy to use and very useful
    for ML models built on tabular datasets. In the next section, we will cover the
    InterpretML XAI framework from Microsoft.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与DALEX相比，我认为Explainerdashboard稍显落后，因为它仅限于与scikit-learn兼容的模型。这意味着对于建立在非结构化数据（如图像和文本）上的复杂深度学习模型，您无法使用此框架。然而，我发现它易于使用，并且对于建立在表格数据集上的机器学习模型非常有用。在下一节中，我们将介绍微软的InterpretML
    XAI框架。
- en: InterpretML
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InterpretML
- en: InterpretML (https://interpret.ml/) is an XAI toolkit from Microsoft. It aims
    to provide a comprehensive understanding of ML models for the purpose of model
    debugging, outcome explainability, and regulatory audits of ML models. With this
    Python module, we can either train *interpretable glassbox models* or *explain
    black-box models*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: InterpretML（https://interpret.ml/）是微软的一个XAI工具包。它的目的是为了模型调试、结果可解释性和机器学习模型的监管审计，提供对机器学习模型的综合理解。使用这个Python模块，我们可以训练*可解释的玻璃盒模型*或*解释黑盒模型*。
- en: In [*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014), *Foundational Concepts
    of Explainability Techniques*, we discovered that some models such as decision
    trees, linear models, or rule-fit algorithms are inherently explainable. However,
    these models are not efficient for complex datasets. Usually, these models are
    termed glass-box models as opposed to black-box models, as they are extremely
    transparent.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18216_01_ePub.xhtml#_idTextAnchor014)，“可解释性技术的基础概念”中，我们发现一些模型（如决策树、线性模型或规则拟合算法）本质上是可解释的。然而，这些模型对于复杂数据集来说并不高效。通常，这些模型被称为玻璃盒模型，与黑盒模型相对，因为它们极其透明。
- en: Microsoft Research developed another algorithm called **Explainable Boosting
    Machine** (**EBM**), which introduces modern ML techniques such as boosting, bagging,
    and automatic interaction detection into classical algorithms such as **Generalized
    Additive Models** (**GAMs**). Researchers have also found that EBMs are accurate
    as random forests and gradient-boosted trees, but unlike such black-box models,
    EBMs are explainable and transparent. Therefore, EBMs are glass-box models that
    are built into the InterpretML framework.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究开发了一种名为**可解释提升机**（**EBM**）的另一种算法，它将现代机器学习技术如提升、袋装和自动交互检测引入到经典算法如**广义加性模型**（**GAMs**）中。研究人员还发现，EBMs的准确度与随机森林和梯度提升树相当，但与这些黑盒模型不同，EBMs是可解释且透明的。因此，EBMs是内置到InterpretML框架中的玻璃盒模型。
- en: In comparison to DALEX and Explainerdashboard, InterpretML is slightly behind
    in terms of both usability and adoption. However, since this framework as a great
    potential to evolve further, it is important to discuss this framework. Before
    discussing the code tutorial, let us discuss about the explainability techniques
    that are supported by this framework.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与DALEX和Explainerdashboard相比，InterpretML在可用性和采用率方面稍显落后。然而，由于这个框架具有很大的发展潜力，讨论这个框架是很重要的。在讨论代码教程之前，让我们讨论一下这个框架支持的解释性技术。
- en: Supported explanation methods
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的解释方法
- en: 'At the time of writing, the following table illustrates the supported explanation
    methods in InterpretML, as mentioned in the GitHub project source at [https://github.com/interpretml/interpret#supported-techniques](https://github.com/interpretml/interpret#supported-techniques):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，以下表格展示了InterpretML支持的解释方法，如GitHub项目源代码中提到的[https://github.com/interpretml/interpret#supported-techniques](https://github.com/interpretml/interpret#supported-techniques)：
- en: '![Figure 9.13 – Explanation methods supported in InterpretML'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.13 – InterpretML支持的解释方法'
- en: '](img/B18216_09_013.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_013.jpg)'
- en: Figure 9.13 – Explanation methods supported in InterpretML
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – InterpretML支持的解释方法
- en: I recommend that you keep an eye on the project documentation, as I am quite
    certain the supported explanation methods for this framework will increase for
    InterpretML. Next, let's explore how to use this framework in practice.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您关注项目文档，因为我非常确信这个框架对InterpretML支持的解释方法将会增加。接下来，让我们探讨如何在实践中使用这个框架。
- en: Setting up InterpretML
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置InterpretML
- en: In this section, I will walk you through the tutorial example of InterpretML
    that is provided in the code repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb).
    In the tutorial, we used InterpretML to explain an ML model trained for hepatitis
    detection, which is a classification problem.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将带您参观代码仓库中提供的 InterpretML 教程示例，网址为 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb)。在教程中，我们使用
    InterpretML 解释了一个用于肝炎检测的机器学习模型，这是一个分类问题。
- en: 'To begin the problem, you need to have the InterpretML Python module installed.
    You can use the pip installer for this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始解决这个问题，你需要安装 InterpretML Python 模块。你可以使用 pip 安装程序来完成这个任务：
- en: '[PRE72]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Although the framework is supported by Windows, Mac, and Linux, it does require
    you to have a Python version that is higher than 3.6\. You can validate whether
    the installation is successful by importing the module:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然该框架支持 Windows、Mac 和 Linux，但它确实要求你拥有一个高于 3.6 的 Python 版本。你可以通过导入模块来验证安装是否成功：
- en: '[PRE73]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Next, let's discuss the dataset that is used in this tutorial.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论本教程中使用的这个数据集。
- en: Discussions about the dataset
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据集的讨论
- en: The hepatitis detection dataset is taken from the UCI Machine Learning repository
    at https://archive.ics.uci.edu/ml/datasets/hepatitis. It has 155 records and 20
    features of different types for the detection of the hepatitis disease. Therefore,
    this dataset is used for solving binary classification problems. For your convenience,
    I have added this dataset to the code repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 肝炎检测数据集来自 UCI 机器学习仓库，网址为 https://archive.ics.uci.edu/ml/datasets/hepatitis。它包含
    155 条记录和 20 个不同类型的特征，用于检测肝炎疾病。因此，这个数据集被用于解决二元分类问题。为了方便起见，我已经将这个数据集添加到代码仓库中，网址为
    [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data)。
- en: Hepatitis Dataset Citation
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 肝炎数据集引用
- en: '*G.Gong (Carnegie-Mellon University) via Bojan Cestnik, Jozef Stefan Institute
    (*[https://archive.ics.uci.edu/ml/datasets/hepatitis](https://archive.ics.uci.edu/ml/datasets/hepatitis)*)*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*G.Gong（卡内基梅隆大学）通过 Bojan Cestnik，Jozef Stefan Institute ([https://archive.ics.uci.edu/ml/datasets/hepatitis](https://archive.ics.uci.edu/ml/datasets/hepatitis)*)*'
- en: More details about the dataset and initial exploration results are included
    in the tutorial notebook. However, on a very high level, *the dataset is imbalanced*,
    it has *missing values*, and it has both *categorical* and *continuous* variables.
    Therefore, it needs necessary transformation before the model can be built. All
    these necessary steps are included in the tutorial notebook, but please feel free
    to explore additional methods for building a better ML model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集和初步探索结果的更多细节包含在教程笔记本中。然而，从非常高的层面来看，*数据集是不平衡的*，它有*缺失值*，并且包含*分类*和*连续*变量。因此，在构建模型之前需要必要的转换。所有这些必要的步骤都包含在教程笔记本中，但请随时探索构建更好的机器学习模型的其他方法。
- en: Training the model
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'For this example, after dividing the entire data into a training set and a
    test set, I have trained a random forest classifier with minimum hyperparameter
    tuning:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，在将整个数据集划分为训练集和测试集之后，我已经使用最小超参数调整训练了一个随机森林分类器：
- en: '[PRE74]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Note that sufficient hyperparameter tuning is not done for this model, as we
    are more interested in the model explainability part with InterpretML rather than
    learning how to build an efficient ML model. However, I encourage you to explore
    hyperparameters tuning further to get a better model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于这个模型，我们没有进行充分的超参数调整，因为我们更感兴趣的是 InterpretML 的模型可解释性部分，而不是学习如何构建一个高效的机器学习模型。然而，我鼓励您进一步探索超参数调整以获得更好的模型。
- en: On evaluating the model on the test data, we have received an accuracy of 85%
    and an **Area Under the ROC Curve** (**AUC**) score of 70%. The AUC score is much
    lower than the accuracy as the dataset used is imbalanced. This indicates that
    a metric such as accuracy can be misleading. Therefore, it is better to consider
    metrics such as the AUC score, F1 score, and confusion matrix instead of accuracy
    for model evaluation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估模型后，我们得到了 85% 的准确率和 70% 的 **ROC 曲线下面积**（**AUC**）分数。由于数据集不平衡，AUC 分数远低于准确率。这表明准确率等指标可能会误导。因此，在模型评估时，最好考虑
    AUC 分数、F1 分数和混淆矩阵等指标，而不是准确率。
- en: Next, we will use InterpretML for model explainability.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 InterpretML 进行模型可解释性。
- en: Explainability with InterpretML
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 InterpretML 进行可解释性
- en: As mentioned earlier, with InterpretML. you can either use interpretable glass-box
    models as surrogate explainers or explore certain model-agnostic methods to explain
    black-box models. With both approaches, you can get an interactive dashboard for
    analyzing the various aspects of explainability. First, I will cover the model
    explainability using glass-box models in InterpretML.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用 InterpretML，您可以使用可解释的玻璃盒模型作为代理解释器，或者探索某些模型无关的方法来解释黑盒模型。使用这两种方法，您都可以获得一个交互式仪表板来分析可解释性的各个方面。首先，我将介绍使用
    InterpretML 中的玻璃盒模型进行模型可解释性。
- en: Explaining with glass-box models using InterpretML
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 InterpretML 的玻璃盒模型进行解释
- en: InterpretML supports interpretable glass-box models such as the **Explainable
    Boosting Machine** (**EBM**), **Decision Tree**, and **Rule-Fit** algorithms.
    These algorithms are applied as surrogate explainers for providing post hoc model
    explainability. First, let's try out the EBM algorithm.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: InterpretML 支持可解释的玻璃盒模型，例如 **可解释提升机**（**EBM**）、**决策树**和**规则拟合**算法。这些算法作为代理解释器应用，以提供事后模型可解释性。首先，让我们尝试一下
    EBM 算法。
- en: EBM
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EBM
- en: 'To explain a model with EBM, we need to load the required submodule in Python:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 EBM 解释模型，我们需要在 Python 中加载所需的子模块：
- en: '[PRE79]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Once the EBM submodule has been successfully imported, we just need to create
    a trained surrogate explainer object:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功导入 EBM 子模块，我们只需创建一个训练好的代理解释器对象：
- en: '[PRE80]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The `ebm` variable is the EBM explainer object. We can use this variable to
    get global or local explanations. The framework only supports feature importance-based
    global and local explainability but creates an interactive plot for further analysis:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`ebm` 变量是 EBM 解释器对象。我们可以使用这个变量来获取全局或局部解释。框架仅支持基于特征重要性的全局和局部可解释性，但创建交互式图表以进行进一步分析：'
- en: '[PRE82]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '*Figure 9.14* illustrates the global feature importance plot and variation
    of the *Age* feature with the overall data distribution obtained using InterpretML:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.14* 展示了使用 InterpretML 获得的全局特征重要性图和 *年龄* 特征与整体数据分布的变化：'
- en: '![Figure 9.14 – Global explanation plots using InterpretML'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14 – 使用 InterpretML 的全局解释图'
- en: '](img/B18216_09_014.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_014.jpg)'
- en: Figure 9.14 – Global explanation plots using InterpretML
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 使用 InterpretML 的全局解释图
- en: 'Feature importance for the local explanation, which has been done at the prediction
    level of the individual data instance, is shown in *Figure 9.15*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 局部解释的特征重要性，已在单个数据实例的预测级别完成，如图 *9.15* 所示：
- en: '![Figure 9.15 – Local explanation using InterpretML'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.15 – 使用 InterpretML 的本地解释'
- en: '](img/B18216_09_015.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_015.jpg)'
- en: Figure 9.15 – Local explanation using InterpretML
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 使用 InterpretML 的本地解释
- en: Next, we will explore rule-based algorithms in InterpretML as *surrogate explainers*,
    as discussed in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索 InterpretML 中的基于规则的算法作为 *代理解释器*，如在第 [*2章*](B18216_02_ePub.xhtml#_idTextAnchor033)
    中讨论的 *模型可解释性方法*。
- en: Decision rule list
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 决策规则列表
- en: 'Similar to EBM, another popular glass-box surrogate explainer that is available
    in InterpretML is the decision rule list. This is similar to the rule-fit algorithm,
    which can learn specific rules from the dataset to explain the logical working
    of the model. We can apply this method using InterpretML in the following way:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 与 EBM 类似，InterpretML 中可用的另一个流行的玻璃盒代理解释器是决策规则列表。这与规则拟合算法类似，可以从数据集中学习特定规则来解释模型的逻辑工作方式。我们可以使用以下方式通过
    InterpretML 应用此方法：
- en: '[PRE89]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'With this method, the framework displays the learned rules, as shown in the
    following screenshot:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，框架显示学习到的规则，如下面的屏幕截图所示：
- en: '![Figure 9.16 – Decision rule list using InterpretML'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16 – 使用 InterpretML 的决策规则列表'
- en: '](img/B18216_09_016.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_016.jpg)'
- en: Figure 9.16 – Decision rule list using InterpretML
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 使用 InterpretML 的决策规则列表
- en: As we can see in *Figure 9.16*, it generates a list of learned rules. Next,
    we will explore the decision tree-based surrogate explainer in InterpretML.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 9.16* 所示，它生成了一组学习规则列表。接下来，我们将探索 InterpretML 中的基于决策树的代理解释器。
- en: Decision tree
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Similar to a decision rule list, we can also fit the decision tree algorithm
    as a surrogate explainer using InterpretML for model explainability. The API syntax
    is also quite similar for applying a decision tree classifier:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策规则列表类似，我们也可以将决策树算法拟合为代理解释器，使用 InterpretML 进行模型可解释性。API 语法也非常相似，用于应用决策树分类器：
- en: '[PRE95]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'This produces a decision tree breakdown plot, as shown in the following screenshot,
    for the model explanation:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为模型解释生成一个决策树分解图，如下面的屏幕截图所示：
- en: '![Figure 9.17 – A decision tree-based surrogate explainer in InterpretML'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.17 – InterpretML 中的基于决策树的代理解释器'
- en: '](img/B18216_09_017.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_017.jpg)'
- en: Figure 9.17 – A decision tree-based surrogate explainer in InterpretML
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – InterpretML 中的基于决策树的代理解释器
- en: 'Now all of these individual components can also be clubbed together into one
    single dashboard using the following line of code:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有这些单个组件也可以通过以下行代码合并到一个单独的仪表板中：
- en: '[PRE101]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '*Figure 9.18* illustrates the consolidated interactive dashboard using InterpretML:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.18* 展示了使用 InterpretML 的整合交互式仪表板：'
- en: '![Figure 9.18 – The InterpretML dashboard consolidating all individual plots'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.18 – InterpretML 控制台整合所有单个图表'
- en: '](img/B18216_09_018.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_018.jpg)'
- en: Figure 9.18 – The InterpretML dashboard consolidating all individual plots
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – InterpretML 控制台整合所有单个图表
- en: In the next section, we will cover the various methods that are available in
    InterpretML for providing a model-agnostic explanation of the black-box model.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍 InterpretML 提供的用于提供黑盒模型模型无关解释的各种方法。
- en: Explaining black-box models using InterpretML
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 InterpretML 解释黑盒模型
- en: 'In this section, we will cover the four different methods supported in InterpretML
    for explaining black-box models. We will only cover the code part as the visualizations
    of feature importance and feature variation are very similar to the glass-box
    models. I do recommend looking at the tutorial notebook for interacting with the
    generated plots to gain more insights. The methods supported are LIME, Kernel
    SHAP, Morris Sensitivity, and Partial Dependence:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 InterpretML 支持的四种解释黑盒模型的方法。我们将只涵盖代码部分，因为特征重要性和特征变化的可视化与玻璃盒模型非常相似。我确实建议查看教程笔记本，以与生成的图表交互并获得更多见解。支持的方法包括
    LIME、Kernel SHAP、Morris 敏感性分析和部分依赖性：
- en: '[PRE102]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'First, we will explore the LIME tabular method:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探索 LIME 表格方法：
- en: '[PRE103]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Next, InterpretML provides the SHAP Kernel method for a model-agnostic SHAP-based
    explanation:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，InterpretML 提供了 SHAP Kernel 方法进行模型无关的 SHAP 解释：
- en: '[PRE109]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Another model-agnostic global explanation method that is supported is **Morris
    Sensitivity**, which is used to obtain the overall sensitivity of the features:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种受支持的模型无关全局解释方法是 **Morris 敏感性**，用于获取特征的整体敏感性：
- en: '[PRE118]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'InterpretML also supports PDPs for analyzing feature dependence:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: InterpretML 也支持使用 PDPs 分析特征依赖性：
- en: '[PRE125]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Finally, everything can be consolidated into a single dashboard using a single
    line of code:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有内容都可以通过一行代码合并到一个单独的仪表板中：
- en: '[PRE132]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: This will create a similar interactive dashboard, as shown in *Figure 9.18*.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个类似的交互式仪表板，如 *图 9.18* 所示。
- en: With the various surrogate explainers and interactive dashboards, this framework
    does have a lot of potential, even though there are quite a few limitations. It
    is restricted to tabular datasets, it is not compatible with model frameworks
    such as PyTorch, TensorFlow, and H20, and I think the model explanation methods
    are also limited. Improving these limitations can definitely increase the adoption
    of this framework.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 通过各种代理解释器和交互式仪表板，这个框架确实有很大的潜力，尽管存在一些限制。它仅限于表格数据集，与 PyTorch、TensorFlow 和 H20
    等模型框架不兼容，我认为模型解释方法也有局限性。改善这些限制无疑可以增加该框架的采用率。
- en: Next, we will cover another popular XAI framework – ALIBI for model explanation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍另一个流行的 XAI 框架——ALIBI 用于模型解释。
- en: ALIBI
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ALIBI
- en: 'ALIBI is another popular XAI framework that supports both local and global
    explanations for classification and regression models. In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, we did explore this framework for getting counterfactual
    examples, but ALIBI does include other model explainability methods too, which
    we will explore in this section. Primarily, ALIBI is popular for the following
    list of model explanation methods:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ALIBI 是另一个流行的 XAI 框架，它支持分类和回归模型的局部和全局解释。在 *第2章* 中，我们探讨了此框架以获取反事实示例，但 ALIBI 还包括其他模型可解释性方法，我们将在本节中探讨。ALIBI
    主要因其以下列表中的模型解释方法而受到欢迎：
- en: '**Anchor explanations**: An anchor explanation is defined as a rule that sufficiently
    revolves or anchors around the local prediction. This means that if the anchor
    value is present in the data instance, the model prediction is almost always the
    same, irrespective of changes to other feature values.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锚点解释**：锚点解释被定义为围绕局部预测充分旋转或锚定的规则。这意味着如果锚点值存在于数据实例中，那么模型预测几乎总是相同的，无论其他特征值如何变化。'
- en: '**Counterfactual Explanations** (**CFEs**): We have seen counterfactuals in
    [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability Methods*.
    CFEs indicate which feature values should change, and by how much, to produce
    a different outcome.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反事实解释**（**CFEs**）：我们在 *第2章* 中看到了反事实。CFEs 指示哪些特征值应该改变，以及改变多少，以产生不同的结果。'
- en: '**Contrastive Explanation Methods** (**CEMs**): CEMs are used with classification
    models for local explanations in terms of **Pertinent Positives** (**PPs**), meaning
    features that should be minimally and sufficiently present to justify a given
    classification, and **Pertinent Negatives** (**PNs**), meaning features that minimally
    and necessarily absent to justify the classification.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比解释方法**（**CEMs**）：CEMs 与分类模型一起用于局部解释，涉及 **相关正面**（**PPs**），即应最小且充分存在的特征，以证明给定的分类，以及
    **相关负面**（**PNs**），即应最小且必要不存在的特征，以证明分类。'
- en: "**Accumulated Local Effects** (**ALE**): ALE plots illustrate how attributes\
    \ influence \Lthe overall prediction of an ML model. ALE plots are often considered\
    \ to be unbiased and a faster alternative to PDPs, as covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),\
    \ *Model Explainability Methods*."
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**累积局部效应**（**ALE**）：ALE 图展示了属性如何影响机器学习模型的总体预测。ALE 图通常被认为是无偏的，并且是 PDPs 的更快替代方案，如
    *第2章* 中所述，*模型可解释性方法*。'
- en: 'To get a detailed summary of the supported methods for model explanation, please
    take a look at [https://github.com/SeldonIO/alibi#supported-methods](https://github.com/SeldonIO/alibi#supported-methods).
    Please explore the official documentation of this framework to learn more about
    it: [https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html](https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取模型解释支持的详细方法总结，请查看 [https://github.com/SeldonIO/alibi#supported-methods](https://github.com/SeldonIO/alibi#supported-methods)。请探索此框架的官方文档以了解更多信息：[https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html](https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html)。
- en: Now, let me walk you through the code tutorial provided for ALIBI.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我带您了解为 ALIBI 提供的代码教程。
- en: Setting up ALIBI
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 ALIBI
- en: The complete code tutorial is provided in the project repository for this chapter
    at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb).
    If you have followed the tutorials for *Counterfactual explanations* from [*Chapter
    2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability Methods*, you
    should have ALIBI installed already.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的项目仓库中提供了完整的代码教程，链接为 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb)。如果您已经跟随了
    *第2章* 中关于 *反事实解释* 的教程，以及 *模型可解释性方法*，那么您应该已经安装了 ALIBI。
- en: 'You can import the submodules that we are going to use for this example from
    ALIBI:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 ALIBI 导入我们将用于此示例的子模块：
- en: '[PRE134]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Next, let's discuss the dataset for this tutorial.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论这个教程的数据集。
- en: Discussion about the dataset
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据集的讨论
- en: For this example, we will use the *Occupancy Detection* dataset from the *UCI
    Machine Learning* repository at [https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#).
    This dataset is used for detecting whether a room is occupied or not from the
    different sensor values that are provided. Hence, this is a classification problem
    that can be solved by fitting ML classifiers on the given dataset. The detailed
    data inspection, preprocessing, and transformation steps are included in the tutorial
    notebook. On a very high level, the dataset is slightly imbalanced and, mostly,
    contains numerical features with no missing values.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用来自*UCI机器学习*仓库的*占用检测*数据集，网址为[https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#)。这个数据集用于检测房间是否被占用，通过提供不同的传感器值来实现。因此，这是一个可以通过在给定数据集上拟合机器学习分类器来解决的问题。详细的
    数据检查、预处理和转换步骤包含在教程笔记本中。从非常高的层面来看，这个数据集略微不平衡，并且主要包含没有缺失值的数值特征。
- en: Occupancy Detection dataset citation
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 占用检测数据集引用
- en: L.M. Candanedo, V. Feldheim (2016) - Accurate occupancy detection of an office
    room from light, temperature, humidity and CO2 measurements using statistical
    learning models. ([https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: L.M. Candanedo, V. Feldheim (2016) - 使用统计学习模型从光、温度、湿度和CO2测量值中准确检测办公室房间的占用情况。([https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#))
- en: In this tutorial, I have demonstrated how to use a pipeline approach with scikit-learn
    for training ML models. This is a very neat way of building ML models, and it
    is especially useful when working on industrial problems that need to be deployed
    to the production system. To learn more about this approach, take a look at the
    official scikit-learn pipeline documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我演示了如何使用scikit-learn的管道方法来训练机器学习模型。这是一种构建机器学习模型非常整洁的方法，尤其是在处理需要部署到生产系统的工业问题时特别有用。要了解更多关于这种方法的信息，请查看官方scikit-learn管道文档：[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)。
- en: Next, let's discuss the model that will be used for extracting explanations.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论将要用于提取解释的模型。
- en: Training the model
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: For this example, I have used a random forest classifier to train a model with
    minimal hyperparameter tuning. You can explore other ML classifiers too, as the
    choice of the algorithm doesn't matter. Our goal is to explore ALIBI for model
    explainability, which I will cover in the next section.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我使用随机森林分类器进行训练，并进行了最小化超参数调整。你也可以探索其他机器学习分类器，因为算法的选择并不重要。我们的目标是探索ALIBI模型的可解释性，我将在下一节中介绍。
- en: Model explainability with ALIBI
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ALIBI进行模型可解释性
- en: Now, let's use the various explanation methods discussed earlier for the trained
    model, which we can consider a black box.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用之前讨论过的各种解释方法来解释训练好的模型，我们可以将其视为一个黑盒。
- en: Using anchor explanations
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用锚解释
- en: 'In order to get the anchor points, first, we need to create an anchor explanation
    object:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取锚点，首先，我们需要创建一个锚解释对象：
- en: '[PRE136]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Next, we need to fit the explainer object on the training data:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将解释器对象拟合到训练数据上：
- en: '[PRE140]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'We need to learn an anchor value for both the occupied class and the unoccupied
    class. This process involves providing a data instance belonging to each of these
    classes as input for estimating the anchor points. This can be done by using the
    following lines of code:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为占用类和非占用类学习一个锚值。这个过程涉及到为每个类别提供一个数据实例作为输入，以估计锚点。这可以通过以下代码行来完成：
- en: '[PRE141]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: In this example, the anchor point for the occupied class is obtained when the
    light intensity value is greater than `256.7` and the `CO2` value is greater than
    `638.8`. In comparison, for the unoccupied class, it is obtained when the `CO2`
    value is greater than `439`. Essentially, this is telling us that if the sensor
    values measuring light intensity are greater than `256.7` and the `CO2` levels
    are greater than `638.8`, the model predicts that the room is occupied.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，当光强度值大于`256.7`且`CO2`值大于`638.8`时，获得被占用类的锚点。相比之下，对于未被占用类，当`CO2`值大于`439`时，获得锚点。本质上，这告诉我们，如果测量光强度的传感器值大于`256.7`且CO2水平大于`638.8`，模型预测房间是被占用的。
- en: The pattern learned by the model is actually appropriate, as whenever a room
    is occupied, it is more likely that the lights are turned on, and with more occupants,
    CO2 levels are also likely to increase. The anchor points for the unoccupied class
    are not very appropriate, intuitive, or interpretable, but this indicates that,
    usually, CO2 levels are lower when the room is not occupied. Typically, we get
    to learn about some threshold values of certain impact features that the model
    relies on for predicting the outcome.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 模型学习到的模式实际上是合适的，因为每当房间被占用时，灯光更有可能被打开，并且随着占用者的增多，CO2水平也可能会上升。未被占用的类的锚点不太合适，也不直观或可解释，但这表明，通常情况下，当房间未被占用时，CO2水平较低。通常，我们可以了解到模型预测结果所依赖的一些影响特征的阈值。
- en: Using CEM
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用CEM
- en: With CEM, the main idea is to learn PPs or conditions that should be present
    to justify the occurrence of one class and PNs, which should be absent to indicate
    the occurrence of one class. This is used for model-agnostic local explainability.
    You can find out more about this method from this research literature at [https://arxiv.org/pdf/1802.07623.pdf](https://arxiv.org/pdf/1802.07623.pdf).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CEM，主要思想是学习应该存在的PP（Positive Predictive Power）或条件，以证明某一类事件的发生，以及PN（Positive
    Negative Power）或条件，应该不存在以指示某一类事件的发生。这用于模型无关的局部可解释性。您可以从这篇研究文献中了解更多关于这种方法的信息：[https://arxiv.org/pdf/1802.07623.pdf](https://arxiv.org/pdf/1802.07623.pdf)。
- en: 'To apply this in Python, we need to create a CEM object with the required hyper-parameters
    and fit the train values to learn the PP and PN values:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中应用此方法，我们需要创建一个具有所需超参数的CEM对象，并将训练值拟合以学习PP和PN值：
- en: '[PRE154]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: In our example, the PP and PN values that have been learned show by how much
    the value should be increased or decreased to meet the minimum criteria for the
    correct outcome. Surprisingly, no PN value was obtained for our example. This
    indicates that all the features are important for the model. The absence of any
    feature or any particular value range does not help the model predict the outcome.
    Usually, for higher-dimensional data, the PN values would be important to analyze.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，学习到的PP和PN值显示了值应该增加或减少多少才能满足正确结果的最低标准。令人惊讶的是，我们没有获得任何PN值。这表明所有特征对于模型来说都很重要。任何特征或任何特定值范围的缺失都不会帮助模型预测结果。通常，对于高维数据，PN值对于分析来说很重要。
- en: Using CFEs
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用CFEs
- en: 'In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, we looked at tutorial examples of how CFEs can be applied with ALIBI.
    We will follow a similar approach for this example, too. However, ALIBI does allow
    different algorithms to generate CFEs, which I highly recommend you explore: https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html.
    In this chapter, we will stick to the prototype-based method covered in the CFE
    tutorial of [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中，我们探讨了如何使用ALIBI应用CFEs的教程示例。对于本例，我们也将采取类似的方法。然而，ALIBI确实允许不同的算法生成CFEs，我强烈建议您探索：[https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)。在本章中，我们将坚持在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”的CFE教程中涵盖的原型方法：
- en: '[PRE161]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Once the explanation object is ready, we can actually compare the difference
    between CFEs and the original data instance to understand the change in the feature
    values required to flip the outcome. However, using this method to get the correct
    CFE can be slightly challenging as there are many hyperparameters that require
    the right tuning; therefore, the method can be challenging and tedious. Next,
    let's discuss how to use ALE plots for model explainability.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解释对象准备就绪，我们实际上可以比较CFE与原始数据实例之间的差异，以了解翻转结果所需的特征值变化。然而，使用这种方法获取正确的CFE可能有些挑战，因为有许多超参数需要正确的调整；因此，这种方法可能具有挑战性和繁琐。接下来，让我们讨论如何使用ALE图进行模型可解释性。
- en: Using ALE plots
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用ALE图
- en: 'Similar to PDPs, as covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, ALE plots can be used to find the relationship
    of the individual features with respect to the target class. Let''s see how to
    apply this in Python:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 与PDPs类似，如第[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)中所述，“模型可解释性方法”，ALE图可以用来找到各个特征与目标类之间的关系。让我们看看如何在Python中应用这个方法：
- en: '[PRE172]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'This will create the following ALE plots:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建以下ALE图：
- en: '![Figure 9.19 – ALE plots using ALIBI'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.19 – 使用ALIBI的ALE图'
- en: '](img/B18216_09_019.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_09_019.jpg)'
- en: Figure 9.19 – ALE plots using ALIBI
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – 使用ALIBI的ALE图
- en: In *Figure 9.19*, we can see that the variance in feature values for the `occupied`
    and `not occupied` target classes is at the maximum level for the feature light,
    followed by CO2 and temperature, and at the lowest level for `HumidityRatio`.
    This gives us an indication of how the model prediction changes depending on the
    variation of the feature values.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.19*中，我们可以看到对于`occupied`和`not occupied`目标类，特征值的变化在特征光处达到最大水平，其次是CO2和温度，而`HumidityRatio`处则处于最低水平。这为我们提供了模型预测如何根据特征值的变化而变化的指示。
- en: Overall, I feel that ALIBI is an interesting XAI framework that works with tabular
    and unstructured data such as text and images and does have a wide variety of
    techniques for the explainability of ML models. The only limitation I have found
    is that some of the methods are not very simplified, so they require a good amount
    of hyperparameter tuning to get reliable explanations. Please explore [https://github.com/SeldonIO/alibi/tree/master/doc/source/examples](https://github.com/SeldonIO/alibi/tree/master/doc/source/examples)
    for other examples provided for ALIBI to get more practical expertise. In the
    next section, we will discuss DiCE as an XAI Python framework.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我觉得ALIBI是一个有趣的XAI框架，它适用于表格和无结构数据，如文本和图像，并且确实有各种技术用于ML模型的可解释性。我唯一发现的问题是，其中一些方法并不非常简化，因此需要大量的超参数调整才能得到可靠的解释。请访问[https://github.com/SeldonIO/alibi/tree/master/doc/source/examples](https://github.com/SeldonIO/alibi/tree/master/doc/source/examples)以获取ALIBI提供的其他示例，以获得更多实际经验。在下一节中，我们将讨论DiCE作为XAI
    Python框架。
- en: DiCE
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DiCE
- en: '**Diverse Counterfactual Explanations** (**DiCE**) is another popular XAI framework
    that we briefly covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, for the *CFE tutorial*. Interestingly, DiCE is
    also one of the key XAI frameworks from Microsoft Research, but it is yet to be
    integrated with the InterpretML module (I wonder why!). I find the entire idea
    of CFE to be very close to the ideal human-friendly explanation that gives actionable
    recommendations. This blog from Microsoft discusses the motivation and idea behind
    the DiCE framework: [https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/](https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/).'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '**多样化的反事实解释**（**DiCE**）是另一个流行的XAI框架，我们在[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中简要介绍了它，用于*CFE教程*。有趣的是，DiCE也是微软研究院的关键XAI框架之一，但它尚未与InterpretML模块集成（我想知道为什么！）。我发现CFE的整个想法非常接近理想的、对人类友好的解释，它提供了可操作的建议。微软的这篇博客讨论了DiCE框架背后的动机和想法：[https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/](https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/)。'
- en: In comparison to ALIBI CFE, I found DiCE to produce more appropriate CFEs with
    minimal hyperparameter tuning. That's why I feel it's important to mention DiCE,
    as it is primarily designed for example-based explanations. Next, let's discuss
    the CFE methods that are supported in DiCE.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: CFE methods supported in DiCE
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DiCE can generate CFEs based on the following methods:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-agnostic methods:'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KD-Tree
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetic algorithm
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomized sampling
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient-based methods (model-specific methods):'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss-based method for deep learning models
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational Auto-Encoder** (**VAE**)-based methods'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about all these methods, I request that you explore the official
    documentation of DiCE (https://github.com/interpretml/DiCE), which contains the
    necessary research literature for each method. Now, let's use DiCE for model explainability.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Model explainability with DiCE
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete tutorial example is provided at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DiCE_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DiCE_example.ipynb).
    For this example, I have used the same Occupancy Detection dataset that we used
    for the ALIBI tutorial. Since the same data preprocessing, transformation, model
    training, and evaluation steps have been used, we will directly proceed with the
    model explainability part with DALEX. The notebook tutorial contains all the necessary
    steps, so I recommend that you go through the notebook first.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: We will use the DiCE framework in the same way as we have done for the CFE tutorial
    from [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'So, first, we need to define a DiCE data object:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'Next, we need to create a DiCE model object:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'Following this, we need to pass the data object and the model object for the
    DiCE explanation object:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'Next, we can take a query data instance and generate CFEs using the DiCE explainer
    object:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'This will produce a CFE DataFrame that shows the feature values that need to
    be changed to flip the model predicted outcome. The outcome of this approach is
    illustrated in the following screenshot:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – CFE generated using the DiCE framework, which is displayed
    as a DataFrame'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_020.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – CFE generated using the DiCE framework, which is displayed as
    a DataFrame
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, CFEs don''t only provide actionable insights from the data.
    However, they can also be used to generate local and global feature importance.
    The features that can be easily varied to alter the model prediction are considered
    to be more important by this approach of feature importance. Let''s try applying
    the local feature importance using the DiCE method:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: 'This produces the following local feature importance plot for the test query
    selected:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Local feature importance using DiCE'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_021.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.21 – Local feature importance using DiCE
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.21* shows that for the test query data, the humidity, light, and
    CO2 features are the most important for model prediction. This indicates that
    most CFEs would suggest changing the feature values of one of these features to
    alter the model prediction.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: Overall, DiCE is a very promising framework for robust CFEs. I recommend you
    explore the different algorithms to generate CFEs such as KD-Trees, random sampling,
    and genetic algorithms. DiCE examples can sometimes be very random. My recommendation
    is to always use a random seed to control the randomness, clearly define the actionable
    and non-actionable features, and set the boundary conditions of the actionable
    features to generate CFEs that are meaningful and practically feasible. Otherwise,
    the generated CFEs can be very random and practically not feasible and, therefore,
    less impactful to use.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: For other examples of the DiCE framework for multiclass classification or regression
    problems, please explore [https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks](https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks).
    Next, let's cover ELI5, which is one of the initial XAI frameworks that has been
    developed to produce simplistic explanations of ML models.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: ELI5
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*ELI5*, or *Explain Like I''m Five*, is a Python XAI library for debugging,
    inspecting, and explaining ML classifiers. It was one of the initial XAI frameworks
    developed to explain black-box models in the most simplified format. It supports
    a wide range of ML modeling frameworks such as scikit-learn compatible models,
    Keras, and more. It also has integrated LIME explainers and can work with tabular
    datasets along with unstructured data such as text and images. The library documentation
    is provided at [https://eli5.readthedocs.io/en/latest/](https://eli5.readthedocs.io/en/latest/),
    and the GitHub project is available at [https://github.com/eli5-org/eli5](https://github.com/eli5-org/eli5).'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover the application part of ELI5 for a tabular dataset
    only, but please feel free to explore other examples that have been provided in
    the tutorial examples of ELI5 at [https://eli5.readthedocs.io/en/latest/tutorials/index.html](https://eli5.readthedocs.io/en/latest/tutorials/index.html).
    Next, let's get started with the walk-through of the code tutorial.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Setting up ELI5
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The complete tutorial of the ELI5 example is available in the GitHub repository
    for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ELI5_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ELI5_example.ipynb).
    ELI5 can be installed in Python using the pip installer:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'If the installation process is successful, you can verify it by importing the
    module in the Jupyter notebook:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: For this example, we will use the same hepatitis detection dataset from the
    UCI Machine Learning repository (https://archive.ics.uci.edu/ml/datasets/hepatitis),
    which we used for the InterpretML example. Also, we have used a random forest
    classification model with minimum hyperparameter tuning that will be used as our
    black-box model. So, we will skip the discussion about the dataset and model part
    and proceed to the model explainability part using ELI5.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Model explainability using ELI5
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Applying ELI5 in Python is very easy and can be done with a few lines of code:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: 'This will produce the following feature weight tabular visualization that can
    be used to analyze the global feature importance:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Feature weights obtained using ELI5'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_022.jpg)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.22 – Feature weights obtained using ELI5
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.22* indicates that the feature, `BILIRUBIN`, has the maximum weight
    and, hence, has the maximum contribution for influencing the model outcome. The
    +/- values shown beside the weight values can be considered to be confidence intervals.
    This method can be considered a very simple way to provide insights into the black-box
    model. ELI5 calculates the feature weights using tree models. Every node of the
    tree gives an output score that is used to estimate the total contribution of
    a feature. The total contribution on the decision path is how much the score changes
    from parent to child. The total weights of all the features sum up the total probability
    of the model for predicting a particular class.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this method for providing local explainability and for an inference
    data instance:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: 'This will produce the following tabular visualization for analyzing the feature
    contributions of the inference data:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Feature contributions using ELI5 for local explainability'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_023.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – Feature contributions using ELI5 for local explainability
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.23*, we can see the feature contributions using ELI5 for the local
    data used for prediction. There is a `<BIAS>` term that is added to the table.
    This is considered the expected average score output by the model, which depends
    on the distribution of the training data. To find out more, take a look at this
    Stack Overflow post: [https://stackoverflow.com/questions/49402701/eli5-explaining-prediction-xgboost-model](https://stackoverflow.com/questions/49402701/eli5-explaining-prediction-xgboost-model).'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Even though ELI5 is easy to use and probably the least complex of all the XAI
    frameworks covered so far, I would say that the framework is not comprehensive
    enough. Even the visualization provided to analyze the feature contributions appears
    to be very archaic and can be improved. Since ELI5 is one of the initial XAI frameworks
    that works with tabular data, images, and text data, it is important to know about
    it.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, I will cover the model explainability of H2O AutoML models.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: H2O AutoML explainers
  id: totrans-534
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we have mostly used scikit-learn-based and TensorFlow-based
    models. However, when the idea of AutoML was first introduced, the H2O community
    was one of the earliest adopters of this concept and introduced the AutoML feature
    for the H2O ML framework: [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, H2O AutoML is very widely used in the industry, especially for
    high-volume datasets. Unfortunately, there are very few model explainability frameworks
    such as DALEX that are compatible with H2O models. H2O models have a good usage
    in both R and Python, and with the AutoML feature, this framework promises to
    spin up trained and tuned models to give the best performance in a very short
    time and with less effort. So, that's why I feel it is important to mention the
    H2O AutoML explainer in this chapter. This framework does have a built-in implementation
    of model explainability methods for explaining the predictions of an AutoML model
    ([https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)).
    Next, let's dive deeper into H2O explainers.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Explainability with H2O explainers
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'H2O explainers are only supported for H2O models. They can be used to provide
    both global and local explanations. The following list shows the supported methods
    to provide explanations in H2O:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison (this is particularly useful for AutoML models
    that try different algorithms on the same dataset)
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable or feature importance (this is for both global and local explanations)
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model correlation heatmaps
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TreeSHAP-based explanations (this is only for tree models)
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDPs (this is for both global and local explanations)
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individual conditional expectation plots, which are also referred to as what-if
    analysis plots (for both global and local explanations)
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find out more about H2O explainers at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html).
    The complete tutorial example is provided at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/H2o_AutoML_explain_example.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/H2o_AutoML_explain_example.ipynb).
    In this example, I have demonstrated how to use H2O AutoML for predicting the
    league position of top football clubs based on the quality of their players using
    the FIFA Club Position Prediction dataset ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/FIFA_Club_Position](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/FIFA_Club_Position)).
    It is the same dataset that we used for the DALEX and Explainerdashboard tutorials.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the H2O module, you can use the pip installer:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: Since we have already covered the steps of data preparation and transformation
    in the previous tutorials, I will skip those steps here. But please do refer to
    the tutorial notebook for executing the end-to-end example.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O models are not compatible with a pandas DataFrame. So, you will need to
    convert a pandas DataFrame into an H2O DataFrame. Let''s see the lines of code
    for training the H2O AutoML module:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: 'Once the AutoML training process is complete, we can get the best model and
    store it as a variable for future usage:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: 'For the model explainability part, we just need to use the `explain` method
    from an AutoML model object:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE238]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: This automatically creates a wide range of supported XAI methods and generates
    visualizations to interpret the model. At the time of writing, the H2O explainability
    feature is newly released and is in the experimental phase. If you would like
    to give any feedback or find any bugs, please raise a ticket request on the H2O
    JIRA issue tracker ([https://0xdata.atlassian.net/projects/PUBDEV](https://0xdata.atlassian.net/projects/PUBDEV)).
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: With that, I have covered all the popular XAI frameworks apart from *LIME*,
    *SHAP*, and *TCAV* that are commonly used or have a high potential for explaining
    ML models. In the next section, I will give a quick comparison guide to compare
    all seven frameworks covered in this chapter.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: Quick comparison guide
  id: totrans-572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the different types of XAI frameworks available
    in Python. Of course, no one framework is absolutely perfect and can be used for
    all scenarios. Throughout the sections, I did mention the pros and cons of each
    framework, but I believe it will be really handy if you have a quick comparison
    guide to decide on your choice of XAI framework, considering your given problem.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates a quick comparison guide for the seven XAI
    frameworks covered in this chapter. I have tried to compare these based on the
    different dimensions of explainability, their compatibility with various ML models,
    a qualitative assessment of human-friendly explanations, the robustness of the
    explanations produced, a qualitative assessment of scalability, and how fast the
    particular framework can be adopted in production-level systems:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – A quick comparison guide of the popular XAI frameworks covered
    in this chapter'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_09_024.jpg)'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.24 – A quick comparison guide of the popular XAI frameworks covered
    in this chapter
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of this chapter. Next, let me provide a summary of
    the main topics of discussion for this chapter.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-579
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered the seven popular XAI frameworks that are available
    in Python: the *DALEX*, *Explainerdashboard*, *InterpretML*, *ALIBI*, *DiCE*,
    *ELI5*, and *H2O AutoML explainers*. We have discussed the supported explanation
    methods for each of the frameworks, the practical application of each, and the
    various pros and cons. So, we did cover a lot in this chapter! I also provided
    a quick comparison guide to help you decide which framework you should go for.
    This also brings us to the end of *Part 2* of this book, which gave you practical
    exposure to using XAI Python frameworks for problem-solving.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '*Section 3* of this book is targeted mainly at the researchers and experts
    who share the same passion as I do: *bringing AI closer to end users*. So, in
    the next chapter, we will discuss the best practices of XAI that are recommended
    for designing human-friendly AI systems.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-582
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional information, please refer to the following resources:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: 'The DALEX GitHub project: [https://github.com/ModelOriented/DALEX](https://github.com/ModelOriented/DALEX)'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Explainerdashboard GitHub project: [https://github.com/oegedijk/explainerdashboard](https://github.com/oegedijk/explainerdashboard)'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The InterpretML GitHub project: [https://github.com/interpretml/interpret](https://github.com/interpretml/interpret)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ALIBI GitHub project: [https://github.com/SeldonIO/alibi](https://github.com/SeldonIO/alibi)'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DiCE GitHub project: [https://github.com/interpretml/DiCE](https://github.com/interpretml/DiCE)'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official ELI5 documentation: [https://eli5.readthedocs.io/en/latest/overview.html](https://eli5.readthedocs.io/en/latest/overview.html)'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Explainability using H2O: [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html#](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html#)'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
