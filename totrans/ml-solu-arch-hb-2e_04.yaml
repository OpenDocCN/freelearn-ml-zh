- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Management for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an ML solutions architecture practitioner, I often receive requests for guidance
    on designing data management platforms for ML workloads. Although data management
    platform architecture is typically treated as a separate technical discipline,
    it plays a crucial role in ML workloads. To create a comprehensive ML platform,
    ML solutions architects must understand the essential data architecture considerations
    for ML and be familiar with the technical design of a data management platform
    that caters to the needs of data scientists and automated ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the intersection of data management and ML,
    discussing key considerations for designing a data management platform specifically
    tailored for ML. We will delve into the core architectural components of such
    a platform and examine relevant AWS technologies and services that can be used
    to build it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Data management considerations for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data management architecture for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercise – data management for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will need access to an AWS account and AWS services such
    as **Amazon** **S3**, **Amazon** **Lake Formation**, **AWS** **Glue**, and **AWS**
    **Lambda**. If you do not have an AWS account, follow the official AWS website’s
    instructions to create an account.
  prefs: []
  type: TYPE_NORMAL
- en: Data management considerations for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data management is a broad and complex topic. Many organizations have dedicated
    data management teams and organizations to manage and govern the various aspects
    of a data platform. Historically, data management primarily revolved around fulfilling
    the requirements of transactional systems and analytics systems. However, as ML
    solutions gain prominence, there are now additional business and technological
    factors to consider when it comes to data management platforms. The advent of
    ML introduces new requirements and challenges that necessitate an evolution in
    data management practices to effectively support these advanced solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand where data management intersects with the ML workflow, let’s
    bring back the ML lifecycle, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Intersection of data management and the ML life cycle ](img/B20836_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Intersection of data management and the ML lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, data management intersects with the ML lifecycle in three
    stages: *data understanding and preparation*, *model training and evaluation*,
    and *model deployment*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the *data understanding and preparation* stage, data scientists undertake
    several essential tasks. They begin by identifying relevant data sources that
    contain datasets suitable for their modeling tasks. Exploratory data analysis
    is then performed to gain insights into the dataset, including data statistics,
    correlations between features, and data sample distributions. Additionally, data
    preparation for model training and validation is crucial, involving a series of
    steps that typically include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data validation**: The data is checked for errors and anomalies to ensure
    its quality. This includes verifying the data range, distribution, and data types
    and identifying missing or null values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleaning**: Any identified data errors are fixed or corrected to ensure
    the accuracy and consistency of the dataset. This may involve removing duplicates,
    handling missing values, or resolving inconsistencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data enrichment**: Additional value is derived from the data through techniques
    like joining different datasets or transforming the data. This helps generate
    new signals and insights that can enhance the modeling process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data management capabilities needed during this stage encompass the following
    aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset discovery**: The capability to search and locate curated datasets
    using relevant metadata like dataset name, description, field name, and data owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access**: The ability to access both raw and processed datasets to perform
    exploratory data analysis. This ensures data scientists can explore and analyze
    the data effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Querying and retrieval**: The capability to run queries against selected
    datasets to obtain details such as statistical information, data quality metrics,
    and data samples. Additionally, it includes the ability to retrieve data from
    the data management platform to a data science environment for further processing
    and feature engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalable data processing**: The ability to execute data processing operations
    on large datasets efficiently. This ensures that data scientists can handle and
    process substantial amounts of data during model development and experimentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During the stage of model training and validation, data scientists are responsible
    for generating a training and validation dataset to conduct formal model training.
    To facilitate this process, the following data management capabilities are essential:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing and automated workflows**: A data management platform should
    provide robust data processing capabilities along with automated workflows. This
    enables the conversion of raw or curated datasets into training and validation
    datasets in various formats suitable for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data repository and versioning**: An efficient data management platform should
    offer a dedicated data repository to store and manage the training and validation
    datasets. Additionally, it should support versioning, allowing data scientists
    to keep track of different iterations and modifications made to the datasets,
    along with the versions of the code and trained ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models. This is a highly labor-intensive task, requiring purpose-built software
    tools to do it at scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML features/embeddings generation and storage**: Some ML features/embeddings
    (e.g., averages, sums, and text embeddings) need to be pre-computed for one or
    more downstream model training tasks. These features/embeddings often need to
    be managed using purpose-built tools for efficient access and reuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset provisioning for model training**: The platform should provide mechanisms
    to serve the training and validation datasets to the model training infrastructure.
    This ensures that the datasets are accessible by the training environment, allowing
    data scientists to train models effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During the stage of model deployment, the focus shifts toward utilizing the
    trained models to serve predictions. To support this stage effectively, the following
    data management capabilities are crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serving data for feature processing**: The data management platform should
    be capable of serving the data required for feature processing as part of the
    input data when invoking the deployed models. This ensures that the models receive
    the relevant data inputs required for generating predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving pre-computed features/embeddings**: In some cases, pre-computed features/embeddings
    are utilized as inputs when invoking the deployed models. The data management
    platform should have the capability to serve these pre-computed features seamlessly,
    allowing the models to incorporate them into the prediction process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to traditional data access patterns for transactional or business
    intelligence solutions, where developers can utilize non-production data in lower
    environments for development purposes, data scientists typically require access
    to production data for model development.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the considerations for ML data management, we will now delve
    deeper into the data management architecture specifically designed for ML. It
    is important to understand that effective data management is crucial for success
    in applied ML. Organizations fail with ML not just due to poor algorithms or inaccurate
    models, but also due to problems with real-world data and production systems.
    Data management shortcomings can sink ML projects despite brilliant modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Data management architecture for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the scale of your ML initiatives, it is important to consider different
    data management architecture patterns to effectively support them. The right architecture
    depends on the scale and scope of the ML initiatives within an organization in
    order to balance the business needs with engineering efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For *small-scale ML projects* characterized by limited data scope, a small
    team size, and minimal cross-functional dependencies, a purpose-built data pipeline
    tailored to meet specific project requirements can be a suitable approach. For
    instance, if your project involves working with structured data sourced from an
    existing data warehouse and a publicly available dataset, you can consider developing
    a straightforward data pipeline. This pipeline would extract the necessary data
    from the data warehouse and public domain and store it in a dedicated storage
    location owned by the project team. This data extraction process can be scheduled
    as needed to facilitate further analysis and processing. The following diagram
    illustrates a simplified data management flow designed to support a small-scale
    ML project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Data architecture for an ML project with limited scope ](img/B20836_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Data architecture for an ML project with limited scope'
  prefs: []
  type: TYPE_NORMAL
- en: For *large-scale ML initiatives* at the enterprise level, the data management
    architecture closely resembles that of enterprise analytics. Both require robust
    support for data ingestion from diverse sources and centralized management of
    data for various processing and access requirements. While analytics data management
    primarily deals with structured data and often relies on an enterprise data warehouse
    as its core backend, ML data management needs to handle structured, semi-structured,
    and unstructured data for different ML tasks. Consequently, a data lake architecture
    is commonly adopted. ML data management is typically an integral part of the broader
    enterprise data management strategy, encompassing both analytics and ML initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a logical enterprise data management architecture
    comprising key components such as data ingestion, data storage, data processing,
    data catalog, data security, and data access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Enterprise data management ](img/B20836_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Enterprise data management'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will delve into a detailed analysis of each key
    component of enterprise data management, providing an in-depth understanding of
    their functionalities and implications within a data management architecture built
    using AWS native services in the cloud. By exploring the specific characteristics
    and capabilities of these components, we will gain valuable insights into the
    overall structure and mechanics of an AWS-based data management architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage and management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data storage and management is a fundamental component of the overall ML data
    management architecture. ML workloads often require data from diverse sources
    and in various formats, and the sheer volume of data can be substantial, particularly
    when dealing with unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: To address these requirements, cloud object data storage solutions like Amazon
    S3 are commonly employed as the underlying storage medium. Conceptually, cloud
    object storage can be likened to a file storage system that accommodates files
    of different formats. Moreover, the storage system allows for the organization
    of files using prefixes, which serve as virtual folders for enhanced object management.
    It is important to note that these prefixes do not correspond to physical folder
    structures. The term “object storage” stems from the fact that each file is treated
    as an independent object, bundled with metadata, and assigned a unique identifier.
    Object storage boasts features such as virtually unlimited storage capacity, robust
    object analytics based on metadata, API-based access, and cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: To efficiently handle the vast quantities of data stored in cloud object storage,
    it is advisable to implement a data lake architecture that leverages this storage
    medium. A data lake, tailored to encompass the entire enterprise or a specific
    line of business, acts as a centralized hub for data management and access. Designed
    to accommodate limitless data volumes, the data lake facilitates the organization
    of data across various lifecycle stages, including raw, transformed, curated,
    and ML feature data. Its primary purpose is to consolidate disparate data silos
    into a singular repository that enables centralized management and access for
    both analytics and ML requirements. Notably, a data lake can house diverse data
    formats, such as structured data from databases, unstructured data like documents,
    semi-structured data in JSON and XML formats, as well as binary formats encompassing
    images, videos, and audio files. This capability proves particularly invaluable
    for ML workloads, as ML often involves working with data in multiple formats.
  prefs: []
  type: TYPE_NORMAL
- en: The data lake should be organized into different zones. For example, a *landing
    zone* should be established as the target for the initial data ingestion from
    different sources. After data preprocessing and data quality management processing,
    the data can be moved to the raw data zone. Data in the *raw data zone* can be
    further transformed and processed to meet different business and downstream consumption
    needs. To further ensure the reliability of the dataset for usage, the data can
    be curated and stored in the *curated data zone*. For ML tasks, ML features often
    need to be pre-computed and stored in an ML feature zone for reuse purposes.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lake Formation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS Lake Formation is a comprehensive data management service offered by AWS,
    which streamlines the process of building and maintaining a data lake on the AWS
    platform. The following figure illustrates the core components of AWS Lake Formation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – AWS Lake Formation ](img/B20836_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: AWS Lake Formation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, AWS Lake Formation offers four fundamental capabilities to enhance
    data lake management:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source crawler**: This functionality automatically examines data files
    within the data lake to infer their underlying structure, enabling efficient organization
    and categorization of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data catalog creation and maintenance**: AWS Lake Formation facilitates the
    creation and ongoing management of a data catalog, providing a centralized repository
    for metadata, enabling easy data discovery and exploration within the data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation processing**: With built-in data transformation capabilities,
    the service allows for the processing and transformation of data stored in the
    data lake, enabling data scientists and analysts to work with refined and optimized
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data security and access control**: AWS Lake Formation ensures robust data
    security by providing comprehensive access control mechanisms and enabling fine-grained
    permissions management, ensuring that data is accessed only by authorized individuals
    and teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lake Formation integrates with AWS Glue, a serverless **Extract, Transform,
    Load** (**ETL**) and data catalog service, to provide data catalog management
    and data ETL processing functionality. We will cover ETL and data catalog components
    separately in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Lake Formation provides a centralized data access management capability for
    managing data access permissions for databases, tables, or different registered
    S3 locations. For databases and tables, the permission can be granularly assigned
    to individual tables and columns and database functions, such as creating tables
    and inserting records.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data ingestion is the bridge between data sources and data storage. It plays
    a crucial role in acquiring data from diverse sources, including structured, semi-structured,
    and unstructured formats, such as databases, knowledge graphs, social media, file
    storage, and IoT devices. Its primary responsibility is to store this data persistently
    in various storage solutions like object data storage (e.g., Amazon S3), data
    warehouses, or other data stores. Effective data ingestion patterns should incorporate
    both real-time streaming and batch ingestion mechanisms to cater to different
    types of data sources and ensure timely and efficient data acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: Various data ingestion technologies and tools cater to different ingestion patterns.
    For streaming data ingestion, popular choices include Apache Kafka, Apache Spark
    Streaming, and Amazon Kinesis/Kinesis Firehose. These tools enable real-time data
    ingestion and processing. On the other hand, for batch-oriented data ingestion,
    tools like **Secure File Transfer Protocol** (**SFTP**) and AWS Glue are commonly
    used. AWS Glue, in particular, offers support for a wide range of data sources
    and targets, including Amazon RDS, MongoDB, Kafka, Amazon DocumentDB, S3, and
    any databases that support JDBC connections. This flexibility allows for seamless
    ingestion of data from various sources into the desired data storage or processing
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'When making decisions on which tools to use for data ingestion, it is important
    to assess the tools and technologies based on practical needs. The following are
    some of the considerations when deciding on data ingestion tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data format, size, and scalability**: Take into account the various data
    formats, data size, and scalability needs. ML projects could be using data from
    different sources and different formats (e.g., **CSV**, **Parquet**, JSON/XML,
    documents, or image/audio/video files). Determine whether the infrastructure can
    handle large data volumes efficiently when necessary and scale down to reduce
    costs during periods of low volume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingestion patterns**: Consider the different data ingestion patterns that
    need to be supported. The tool or combination of several tools should support
    both batch ingestion patterns (transferring bulk data at specific time intervals)
    and real-time streaming (processing data such as sensor data or website clickstreams
    in real time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preprocessing capability**: Evaluate whether the ingested data needs
    to be preprocessed before it is stored in the target data repository. Look for
    tools that offer built-in processing capability or seamless integration with external
    processing tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Ensure that the selected tools provide robust security mechanisms
    for authentication and authorization to protect sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: Verify that the tools offer failure recovery mechanisms to
    prevent critical data loss during the ingestion process. If recovery capability
    is lacking, ensure there is an option to rerun ingestion jobs from the source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for different data sources and targets**: The chosen ingestion tools
    should be compatible with a wide range of data sources, including databases, files,
    and streaming sources. Additionally, they should provide an API for easy data
    ingestion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manageability**: Another important factor to consider is the level of manageability.
    Does the tool require self-management, or is it a fully managed solution? Consider
    the trade-offs between cost and operational complexity before making a decision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS provides several services for data ingestion into a data lake on their platform.
    These services include Kinesis Data Streams, Kinesis Firehose, AWS Managed Streaming
    for Kafka, and AWS Glue Streaming, which cater to streaming data requirements.
    For batch ingestion, options such as AWS Glue, SFTP, and AWS **Data Migration
    Service** (**DMS**) are available. In the upcoming section, we will delve into
    the usage of Kinesis Firehose and AWS Glue to manage data ingestion processes
    for data lakes. We will also discuss AWS Lambda, a serverless compute service,
    for a simple and lightweight data ingestion alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis Firehose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kinesis Firehose is a service that streamlines the process of loading streaming
    data into a data lake. It is a fully managed solution, meaning you don’t have
    to worry about managing the underlying infrastructure. Instead, you can interact
    with the service’s API to handle the ingestion, processing, and delivery of your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kinesis Firehose provides comprehensive support for various scalable data ingestion
    requirements, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Seamless integration with diverse data sources such as websites, IoT devices,
    and video cameras. This is achieved using an ingestion agent or ingestion API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versatility in delivering data to multiple destinations, including Amazon S3,
    Amazon Redshift (an AWS data warehouse service), Amazon OpenSearch (a managed
    search engine), and Splunk (a log aggregation and analysis product).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamless integration with AWS Lambda and Kinesis Data Analytics, offering advanced
    data processing capabilities. With AWS Lambda, you can leverage serverless computing
    to execute custom functions written in languages like Python, Java, Node.js, Go,
    C#, and Ruby. For more comprehensive information on the functionality of Lambda,
    please refer to the official AWS documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the data flow with Kinesis Firehose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Kinesis Firehose data flow ](img/B20836_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Kinesis Firehose data flow'
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis operates by establishing delivery streams, which are the foundational
    components in the Firehose architecture responsible for receiving streaming data
    from data producers. These delivery streams can be configured with various delivery
    destinations, such as S3 and Redshift. To accommodate the data volume generated
    by the producers, you can adjust the throughput of the data stream by specifying
    the number of shards. Each shard has the capacity to ingest 1 MB/sec of data and
    can support data reading at a rate of 2 MB/sec. Additionally, Kinesis Firehose
    offers APIs for increasing the number of shards and merging them when needed.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS Glue is a comprehensive serverless ETL service that helps manage the data
    integration and ingestion process for data lakes. It seamlessly connects with
    various data sources, including transactional databases, data warehouses, and
    NoSQL databases, facilitating the movement of data to different destinations,
    such as Amazon S3\. This movement can be scheduled or triggered by events. Additionally,
    AWS Glue offers the capability to process and transform data before delivering
    it to the target. It provides a range of processing options, such as the Python
    shell for executing Python scripts and Apache Spark for Spark-based data processing
    tasks. With AWS Glue, you can efficiently integrate and ingest data into your
    data lake, benefiting from its fully managed and serverless nature.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS Lambda is AWS’s serverless computing platform. It seamlessly integrates
    with various AWS services, including Amazon S3\. By leveraging Lambda, you can
    trigger the execution of functions in response to events, such as the creation
    of a new file in S3\. These Lambda functions can be developed to move data from
    different sources, such as copying data from a source S3 bucket to a target landing
    bucket in a data lake.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that AWS Lambda is not specifically designed for large-scale
    data movement or processing tasks, due to limitations such as memory size and
    maximum execution time allowed. However, for simpler data ingestion and processing
    jobs, it proves to be a highly efficient tool.
  prefs: []
  type: TYPE_NORMAL
- en: Data cataloging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A data catalog plays a crucial role in enabling data analysts and scientists
    to discover and access data stored in a central data storage. It becomes particularly
    important during the data understanding and exploration phase of the ML lifecycle
    when scientists need to search and comprehend available data for their ML projects.
    When evaluating a data catalog tool, consider the following key factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata catalog**: The technology should support a central data catalog
    for effective management of data lake metadata. This involves handling metadata
    such as database names, table schemas, and table tags. The Hive metastore catalog
    is a popular standard for managing metadata catalogs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated data cataloging**: The technology should have the capability to
    automatically discover and catalog datasets, as well as to infer data schemas
    from various data sources like Amazon S3, relational databases, NoSQL databases,
    and logs. Typically, this functionality is implemented through a crawler that
    scans data sources, identifies metadata elements (e.g., column names, data types),
    and adds them to the catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tagging flexibility**: The technology should have the ability to assign custom
    attributes or tags to metadata entities like databases, tables, and fields. This
    flexibility supports enhanced data search and discovery capabilities within the
    catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other tools**: The technology should allow seamless integration
    of the data catalog with a wide range of data processing tools, enabling easy
    access to the underlying data. Additionally, native integration with data lake
    management platforms is advantageous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search functionality**: The technology should have a robust search capability
    across diverse metadata attributes within the catalog. This includes searching
    by database, table, and field names, custom tags or descriptions, and data types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to building data catalogs, there are various technical options
    available. In this section, we first explore how AWS Glue can be utilized for
    data cataloging purposes. We will also discuss a **Do-It-Yourself** (**DIY**)
    option for a data catalog using standard AWS services such as Lambda and OpenSearch.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue Data Catalog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS Glue offers a comprehensive solution for data cataloging, integrating seamlessly
    with AWS Lake Formation and other AWS services. The AWS Glue Data Catalog can
    be a drop-in replacement for the Hive metastore catalog, so any Hive metastore-compatible
    applications can work with the AWS Glue Data Catalog. With AWS Glue, you can automatically
    discover, catalog, and organize your data assets, making them easily searchable
    and accessible to data analysts and scientists. Here are some key features and
    benefits of using AWS Glue for data cataloging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated data discovery**: AWS Glue provides automated data discovery capabilities.
    By using data crawlers, Glue can scan and analyze data from diverse structured
    and semi-structured sources such as Amazon S3, relational databases, NoSQL databases,
    and more. It identifies metadata information, including table schemas, column
    names, and data types, that is stored in the AWS Glue Data Catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralized metadata repository**: The AWS Glue Data Catalog serves as a
    centralized metadata repository for your data assets. It provides a unified view
    of your data, making it easier to search, query, and understand the available
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata management**: AWS Glue allows you to manage and maintain metadata
    associated with your data assets. You can define custom tags, add descriptions,
    and organize your data using databases, tables, and partitions within the Data
    Catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata hierarchy of the AWS Glue Data Catalog is organized using databases
    and tables. Databases serve as containers for tables, which hold the actual data.
    Like traditional databases, a single database can house multiple tables, which
    can be sourced from various data stores. However, each table is exclusively associated
    with a single database. To query these databases and tables, one can utilize Hive
    metastore-compatible tools such as Amazon Athena to execute SQL queries. When
    collaborating with AWS Lake Formation, access permissions to the catalog’s databases
    and tables can be controlled through the Lake Formation entitlement layer.
  prefs: []
  type: TYPE_NORMAL
- en: Custom data catalog solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another option for building a data catalog is to create your own with a set
    of AWS services. Consider this option when you have specific requirements that
    are not met by the purpose-built products. The architecture for this DIY approach
    involves leveraging services like DynamoDB and Lambda, as depicted in the accompanying
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![         Comprehensive data catalog using AWS Lambda, DynamoDB,            and
    Amazon OpenSearch Service                ](img/B20836_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Custom data catalog solution'
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, AWS Lambda triggers are used to populate DynamoDB tables with
    object names and metadata when those objects are put into S3; Amazon OpenSearch
    Service is used to search for specific assets, related metadata, and data classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data processing functionality of a data lake encompasses the frameworks
    and compute resources necessary for various data processing tasks, such as data
    correction, transformation, merging, splitting, and ML feature engineering. This
    component is a key step in the ML lifecycle as it helps prepare the data for downstream
    model training and inference steps. Common data processing frameworks include
    Python shell scripts using libraries such as pandas, NumPy, and Apache Spark.
    The essential requirements for data processing technology are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration and compatibility with the underlying storage technology**: The
    ability to seamlessly work with the native storage system simplifies data access
    and movement between the storage and processing layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with the data catalog**: The capability to interact with the
    data catalog’s metastore to query databases and tables within the catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The capacity to scale compute resources up or down to accommodate
    changing data volumes and processing velocity requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language and framework support**: Support for popular data processing libraries
    and frameworks, such as Python and Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch and real-time processing capabilities**: The capability to handle both
    real-time data streams and bulk data processing in batch mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s examine a selection of AWS services that offer data processing capabilities
    within a data lake architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Glue ETL**: In addition to supporting data movement and data catalogs,
    the ETL features of AWS Glue can be used for ETL and general-purpose data processing.
    AWS Glue ETL provides several built-in functions for data transformation, such
    as dropping the `NULL` field (the `NULL` field represents new data) and data filtering.
    It also provides general processing frameworks for Python and Spark to run Python
    scripts and Spark jobs. Glue ETL works natively with the AWS Glue Data Catalog
    to access the databases and tables in the catalog. Glue ETL can also access the
    Amazon S3 storage directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elastic MapReduce (EMR)**: **Amazon** **EMR** is a fully managed big
    data processing platform on AWS. It is designed for large-scale data processing
    using the Spark framework and other Apache tools, such as **Apache** **Hive**,
    **Apache** **Hudi**, and **Presto**. It integrates with the Glue Data Catalog
    and Lake Formation natively to access databases and tables in Lake Formation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Lambda**: AWS Lambda can be used for lightweight data processing tasks
    or as part of a larger data processing pipeline within the data lake architecture.
    Lambda can be triggered by real-time events, so it is a good option for real-time
    data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While efficient data processing prepares raw data for model training and consumption,
    robust data management must also ensure ML teams can track data provenance and
    access historical versions as needed through capabilities like data versioning.
  prefs: []
  type: TYPE_NORMAL
- en: ML data versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To establish a lineage for model training across training data and ML models,
    it is crucial to implement version control for the training, validation, and testing
    datasets. Data versioning control presents challenges as it necessitates the use
    of appropriate tools and adherence to best practices by individuals. During the
    model building process, it is common for data scientists to obtain a copy of a
    dataset, perform cleansing and transformations specific to their needs, and save
    the modified data as a new version. This poses significant challenges in terms
    of data management, including duplication and establishing links between the data
    and its various upstream and downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Data versioning for the entire data lake is out of the scope of this book. Instead,
    we will focus on discussing a few architectural options specifically related to
    versioning control for training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: S3 partitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, each newly created or updated dataset is stored in a separate
    S3 partition with a unique prefix, typically derived from the name of the S3 folder.
    While this method can lead to data duplication, it offers a clear and simple approach
    to differentiate between different datasets intended for model training. To maintain
    data integrity, it is advisable to generate datasets through a controlled processing
    pipeline that enforces naming standards. The processing pipeline should also track
    data provenance and record the processing scripts used for data manipulation and
    feature engineering. Furthermore, the datasets should be configured as read-only
    for downstream applications, ensuring their immutability. The following example
    showcases an S3 partition structure, illustrating multiple versions of a training
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this instance, the two versions of the dataset are segregated using distinct
    S3 prefixes. To effectively track these training files, it is recommended to employ
    a database for storing metadata pertaining to these training files. When utilizing
    these files, it is crucial to establish links between the training datasets, ML
    training jobs, ML training scripts, and the resulting ML models to establish a
    comprehensive lineage.
  prefs: []
  type: TYPE_NORMAL
- en: Versioned S3 buckets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon S3 offers versioning support for S3 buckets, which can be leveraged to
    manage different versions of training datasets when enabled. With this approach,
    each newly created or updated dataset is assigned a unique version ID at the S3
    object level. Additionally, it is recommended to utilize a database to store all
    relevant metadata associated with each version of the training dataset. This enables
    the establishment of lineage, tracking the journey from data processing to ML
    model training. The metadata should capture essential information to facilitate
    comprehensive tracking and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose-built data version tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of developing custom solutions for data version control, there are
    purpose-built tools available for efficient data version management. For example,
    these tools can be used to track and store different versions of ML training and
    validation datasets, which are important for repeatable experimentations and model
    training tasks. Here are a few notable options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git LFS (Large File Storage)**: Git LFS extends Git’s capabilities to handle
    large files, including datasets. It stores these files outside the Git repository
    while retaining versioning information. Git LFS seamlessly integrates with Git
    and is commonly used to version large files in data-centric projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataVersionControl (DVC)**: DVC is an open-source tool designed specifically
    for data versioning and management. It integrates with Git and provides features
    for tracking and managing large datasets. DVC enables lightweight links to actual
    data files stored in remote storage, such as Amazon S3 or a shared file system.
    This approach maintains a history of changes and allows easy switching between
    different dataset versions, eliminating the need for data duplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pachyderm**: Pachyderm is an open-source data versioning and data lineage
    tool. It offers version control for data pipelines, enabling tracking of changes
    to data, code, and configuration files. Pachyderm supports distributed data processing
    frameworks like Apache Spark and provides features like reproducibility, data
    lineage, and data lineage-based branching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These purpose-built tools streamline the process of data versioning, ensuring
    efficient tracking and management of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ML feature stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In large enterprises, it is beneficial to centrally manage common reusable ML
    features like curated customer profile data and standardized product sales data.
    This practice helps reduce the ML project lifecycle, particularly during the data
    understanding and data preparation stages. To achieve this, many organizations
    have built central ML feature stores, an architectural component for storing common
    reusable ML features, as part of the ML development architecture to meet the downstream
    model development, training, and model inference needs. Depending on the specific
    requirements, there are two main options for managing these reusable ML features.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, you can build custom feature stores that fulfill the fundamental requirements
    of inserting and looking up organized features for ML model training. These custom
    feature stores can be tailored to meet the specific needs of the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can opt for commercial-grade feature store products, such
    as Amazon SageMaker Feature Store, a ML service offered by AWS, which we will
    delve into in later chapters. It provides advanced capabilities such as online
    and offline functionality for training and inference, metadata tagging, feature
    versioning, and advanced search. These features enable efficient management and
    utilization of ML features in production-grade scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Data serving for client consumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central data management platform should offer various methods, such as APIs
    or Hive metastore-based approaches, to facilitate online access to the data for
    downstream tasks such as data discovery and model training. Additionally, it is
    important to consider data transfer tools that support the movement of data from
    the central data management platform to other data-consuming environments, catering
    to different data consumption patterns such as local access to the data in the
    consuming environment. It is advantageous to explore tools that either have built-in
    data serving capabilities or can be seamlessly integrated with external data serving
    tools, as building custom data serving features could be a challenging engineering
    undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: When supplying data to data science environments, there are multiple data serving
    patterns to consider. In the following discussion, we will explore two prominent
    data access patterns and their characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Consumption via API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this data serving pattern, consumption environments and applications have
    the capability to directly access data from the data lake. This can be achieved
    using Hive metastore-compliant tools or through direct access to S3, the underlying
    storage of the data lake. Amazon provides various services that facilitate this
    pattern, such as Amazon Athena, a powerful big data query tool, Amazon EMR, a
    robust big data processing tool, and Amazon Redshift Spectrum, a feature of Amazon
    Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging these services, data lake data indexed in Glue catalogs can be
    queried without the need to make a separate copy of the data. This pattern is
    particularly suitable when only a subset of the data is required for downstream
    data processing tasks. It offers the advantage of avoiding data duplication while
    enabling efficient selection and processing of specific data subsets as part of
    the overall data workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Consumption via data copy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this data serving pattern, a specific portion of the data stored in the data
    lake is replicated or copied to the storage of the consumption environment. This
    replication allows for tailored processing and consumption based on specific needs.
    For instance, the latest or most relevant data can be loaded into a data analytics
    environment such as Amazon Redshift. Similarly, it can be delivered to S3 buckets
    owned by a data science environment, enabling efficient access and utilization
    for data science tasks. By replicating the required data subsets, this pattern
    provides flexibility and optimized performance for different processing and consumption
    requirements in various environments.
  prefs: []
  type: TYPE_NORMAL
- en: Special databases for ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering emerging ML paradigms like graph neural networks and generative
    AI, specialized databases have been developed to cater to ML-specific tasks such
    as link prediction, cluster classification, and retrieval-augmented generation.
    In the following section, we will delve into two types of databases—vector databases
    and graph databases—and examine how they are utilized in ML tasks. We will explore
    their unique characteristics and applications in the context of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector databases, also known as vector similarity search engines or vector stores,
    are specialized databases designed to efficiently store, index, and query high-dimensional
    vectors. Examples of high-dimensional vectors include numerical vectors’ representation
    of images or text. These databases are particularly well suited for ML applications
    that rely on vector-based computations.
  prefs: []
  type: TYPE_NORMAL
- en: In ML, vectors are commonly used to represent data points, embeddings, or feature
    representations. These vectors capture essential information about the underlying
    data, enabling similarity search, clustering, classification, and other ML tasks.
    Vector databases provide powerful tools for handling these vector-based operations
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of vector databases is their ability to perform fast
    similarity searches, allowing efficient retrieval of vectors that are most similar
    to a given query vector. This capability is essential in various ML use cases,
    such as recommender systems, content-based search, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several vector database providers on the market, each offering its
    own unique features and capabilities. Some of the prominent ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Facebook AI Similarity Search (FAISS)**: Developed by **Facebook AI Research**
    (**FAIR**), FAISS is an open-source library for efficient similarity search and
    clustering of dense vectors. It provides highly optimized algorithms and data
    structures for fast and scalable vector search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Milvus**: Milvus is an open-source vector database designed for managing
    and serving large-scale vector datasets. It offers efficient similarity search,
    supports multiple similarity metrics, and provides scalability through distributed
    computing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pinecone**: Pinecone is a cloud-native vector database service that specializes
    in high-performance similarity search and recommendation systems. It offers real-time
    indexing and retrieval of vectors with low latency and high throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch**: Although primarily known as a full-text search and analytics
    engine, Elasticsearch also provides vector similarity search capabilities using
    plugins for efficient vector indexing and querying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaviate**: Weaviate is an open-source vector database. It allows you to
    store data objects and vector embeddings from your favorite ML models, and scale
    seamlessly into billions of data objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of vector database providers, and the landscape
    is continuously evolving with new solutions and advancements in the field. When
    choosing a vector database provider, it’s important to consider factors such as
    performance, scalability, ease of integration, and the specific requirements of
    your ML use case.
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph databases are specialized databases designed to store, manage, and query
    graph-structured data. In a graph database, data is represented as nodes (entities)
    and edges (relationships) connecting these nodes, forming a graph-like structure.
    Graph databases excel at capturing and processing complex relationships and dependencies
    between entities, making them highly relevant for ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases offer a powerful way to model and analyze data in domains where
    relationships play a crucial role, such as social networks, recommendation systems,
    fraud detection, knowledge graphs, and network analysis. They enable efficient
    traversal of the graph, allowing for queries that explore connections and patterns
    within the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of ML, graph databases have multiple applications. One key use
    case is graph-based feature engineering, where graphs are used to represent relationships
    between entities, and the graph structure is leveraged to derive features that
    can enhance the performance of ML models. For example, in a recommendation system,
    a graph database can represent user-item interactions and graph-based features
    can be derived to capture user similarities, item similarities, or collaborative
    filtering patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases also enable graph-based algorithms, such as **graph convolutional
    networks** (**GCNs**), for tasks like node classification, link prediction, and
    graph clustering. These algorithms leverage the graph structure to propagate information
    across nodes and capture complex patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, graph databases can be used to store and query graph embeddings,
    which are low-dimensional vector representations of nodes or edges. These embeddings
    capture the structural and semantic information of the graph and can be input
    to ML models for downstream tasks, such as node classification or recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the notable graph databases include **Neo4j**, a popular and widely
    used graph database that allows for efficient storage, retrieval, and querying
    of graph-structured data, and Amazon Neptune, a fully managed graph database service
    provided by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data pipelines streamline the flow of data by automating tasks such as data
    ingestion, validation, transformation, and feature engineering. These pipelines
    ensure data quality and facilitate the creation of training and validation datasets
    for ML models. Numerous workflow tools are available for constructing data pipelines,
    and many data management tools offer built-in capabilities for building and managing
    these pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Glue workflows**: AWS Glue workflows provide a native workflow management
    feature within AWS Glue, enabling the orchestration of various Glue jobs like
    data ingestion, processing, and feature engineering. Comprised of trigger and
    node components, a Glue workflow incorporates schedule triggers, event triggers,
    and on-demand triggers. Nodes within the workflow can be either crawler jobs or
    ETL jobs. Triggers initiate workflow runs, while event triggers are emitted after
    the completion of crawler or ETL jobs. By structuring a series of triggers and
    jobs, workflows facilitate the seamless execution of data pipelines within AWS
    Glue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Step Functions**: AWS Step Functions is a powerful workflow orchestration
    tool that seamlessly integrates with various AWS data processing services like
    AWS Glue and Amazon EMR. It enables the creation of robust workflows to execute
    diverse steps within a data pipeline, such as data ingestion, data processing,
    and feature engineering, ensuring smooth coordination and execution of these tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Managed Workflows for Apache Airflow**: AWS **Managed Workflows for Apache
    Airflow** (**MWAA**) is a fully managed service that simplifies the deployment,
    configuration, and management of Apache Airflow, an open-source platform for orchestrating
    and scheduling data workflows. This service offers scalability, reliability, and
    easy integration with other AWS services, making it an efficient solution for
    managing complex data workflows in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having explored the fundamental elements of ML data management architecture,
    the subsequent sections will delve into subjects related to security and governance.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authentication and authorization are crucial for ensuring secure access to a
    data lake. Federated authentication, such as AWS **Identity and Access Management**
    (**IAM**), verifies user identities for administration and data consumption purposes.
    AWS Lake Formation combines the built-in Lake Formation access control with AWS
    IAM to govern access to data catalog resources and underlying data storage.
  prefs: []
  type: TYPE_NORMAL
- en: The built-in Lake Formation permission model utilizes commands like grant and
    revoke to control access to resources such as databases and tables, as well as
    actions like table creation. When a user requests access to a resource, both IAM
    policies and Lake Formation permissions are evaluated to verify and enforce access
    before granting it. This multi-layered approach enhances data lake security and
    governance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several personas involved in the administration of the data lake
    and consumption of the data lake resources, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lake Formation administrator**: A Lake Formation administrator has permission
    to manage all aspects of a Lake Formation data lake in an AWS account. Examples
    include granting/revoking permissions to access data lake resources for other
    users, registering data stores in S3, and creating/deleting databases. When setting
    up Lake Formation, you will need to register as an administrator. An administrator
    can be an AWS IAM user or IAM role. You can add more than one administrator to
    a Lake Formation data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation database creator**: A Lake Formation database creator is granted
    permission to create databases in Lake Formation. A database creator can be an
    IAM user or IAM role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation database user**: A Lake Formation database user can be granted
    permission to perform different actions against a database. Example permissions
    include create table, drop table, describe table, and alter table. A database
    user can be an IAM user or IAM role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation data user**: A Lake Formation data user can be granted permission
    to perform different actions against database tables and columns. Example permissions
    include insert, select, describe, delete, alter, and drop. A data user can be
    an IAM user or an IAM role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing and querying the database and tables in Lake Formation is facilitated
    through compatible AWS services like Amazon Athena and Amazon EMR. When performing
    queries using these services, Lake Formation verifies the principals (IAM users,
    groups, and roles) associated with them to ensure they have the necessary access
    permissions for the database, tables, and corresponding S3 data location. If access
    is granted, Lake Formation issues a temporary credential to the service, enabling
    it to execute the query securely and efficiently. This process ensures that only
    authorized services can interact with Lake Formation and perform queries on the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having secure access to trustworthy data is essential to the success of an ML
    initiative. Data governance encompasses essential practices to ensure the reliability,
    security, and accountability of data assets. Trustworthy data is achieved through
    the identification and documentation of data flows, as well as the measurement
    and reporting of data quality. Data protection and security involve classifying
    data and applying appropriate access permissions to safeguard its confidentiality
    and integrity. To maintain visibility of data activities, monitoring and auditing
    mechanisms should be implemented, allowing organizations to track and analyze
    actions performed on data, ensuring transparency and accountability in data management.
  prefs: []
  type: TYPE_NORMAL
- en: A data catalog is one of the most important components of data governance. On
    AWS, the Glue Data Catalog is a fully managed service for data catalog management.
    You also have the option to build custom data catalogs using different foundational
    building blocks. For example, you can follow the reference architecture at [https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html](https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html)
    for building a custom data catalog on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To establish and document data lineage during the ingestion and processing
    of data across different zones, it is important to capture specific data points.
    When utilizing data ingestion and processing tools like AWS Glue, AWS EMR, or
    AWS Lambda in a data pipeline, the following information can be captured to establish
    comprehensive data lineage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source details**: Include the name of the data source, its location,
    and ownership information to identify the origin of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing job history**: Capture the history and details of the data
    processing jobs involved in the pipeline. This includes information such as the
    job name, unique **identifier** (**ID**), associated processing script, and owner
    of the job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated artifacts**: Document the artifacts generated because of the data
    processing jobs. For example, record the S3 URI or other storage location for
    the target data produced by the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data metrics**: Track relevant metrics at different stages of data processing.
    This can include the number of records, data size, data schema, and feature statistics
    to provide insights into the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To store and manage data lineage information and processing metrics, it is recommended
    to establish a central data operational data store. AWS DynamoDB, a fully managed
    NoSQL database, is an excellent technology choice for this purpose. With its capabilities
    optimized for low latency and high transaction access, DynamoDB provides efficient
    storage and retrieval of data lineage records and processing metrics. By capturing
    and documenting these data points, organizations can establish a comprehensive
    data lineage that provides a clear understanding of the data’s journey from its
    source through various processing stages. This documentation enables traceability,
    auditability, and better management of the data as it moves through the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Other data governance measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to managing data lineage, there are several other important measures
    for effective data governance, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality**: Automated data quality checks should be implemented at different
    stages, and quality metrics should be reported. For example, after the source
    data is ingested into the landing zone, an AWS Glue quality check job can run
    to check the data quality using tools such as the open-source `Deequ` library.
    Data quality metrics (such as counts, schema validation, missing data, the wrong
    data type, or statistical deviations from the baseline) and reports can be generated
    for reviews. Optionally, manual or automated operational data cleansing processes
    should be established to correct data quality issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cataloging**: Create a central data catalog and run Glue crawlers on
    datasets in the data lake to automatically create an inventory of data and populate
    the central data catalog. Enrich the catalogs with additional metadata to track
    other information to support discovery and data audits, such as the business owner,
    data classification, and data refresh date. For ML workloads, data science teams
    also generate new datasets (for example, new ML features) from the existing datasets
    in the data lake for model training purposes. These datasets should also be registered
    and tracked in a data catalog, and different versions of the data should be retained
    and archived for audit purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access provisioning**: A formal process should be established for requesting
    and granting access to datasets and Lake Formation databases and tables. An external
    ticketing system can be used to manage the workflow for requesting access and
    granting access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and auditing**: Data access should be monitored, and access history
    should be maintained. Amazon S3 server access logging can be enabled to track
    access to all S3 objects directly. AWS Lake Formation also records all accesses
    to Lake Formation datasets in **AWS** **CloudTrail** (AWS CloudTrail provides
    event history in an AWS account to enable governance, compliance, and operational
    auditing). With Lake Formation auditing, you can get details such as event source,
    event name, SQL queries, and data output location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these key data governance measures, organizations can establish
    a strong foundation for data management, security, and compliance, enabling them
    to maximize the value of their data assets while mitigating risks.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – data management for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this hands-on exercise, you will go through the process of constructing a
    simple data management platform for a fictional retail bank. This platform will
    serve as the foundation for an ML workflow, and we will leverage different AWS
    technologies to build it. If you don’t have an AWS account, you can easily create
    one by following the instructions at [https://aws.amazon.com/console/](https://aws.amazon.com/console/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The data management platform we create will have the following key components:'
  prefs: []
  type: TYPE_NORMAL
- en: A data lake environment for data management using Lake Formation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data ingestion component for ingesting files to the data lake using Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data catalog component using the Glue Data Catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data discovery and query component using the Glue Data Catalog and Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data processing component using Glue ETL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pipeline component using a Glue pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the data management architecture we will build
    in this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Data management architecture for the hands-on exercise ](img/B20836_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Data management architecture for the hands-on exercise'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with building out this architecture on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data lake using Lake Formation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will build the data lake architecture using AWS Lake Formation; it is the
    primary service for building data lakes on AWS. After you log on to the **AWS**
    **Management Console**, create an S3 bucket called `MLSA-DataLake-<your initials>`.
    We will use this bucket as the storage for the data lake. If you get a message
    that the bucket name is already in use, try adding some random characters to the
    name to make it unique. If you are not familiar with how to create S3 buckets,
    follow the instructions at the following link: [https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the bucket is created, follow these steps to get started with creating
    a data lake:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Register Lake Formation administrators**: We need to add Lake Formation administrators
    to the data lake. The administrators will have full permission to manage all aspects
    of the data lake. To do this, navigate to the Lake Formation management console,
    click on the **Administrative roles and tasks** link, and you should be prompted
    to add an administrator. Select **Add myself** and click on the **Get started**
    button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register S3 storage**: Next, we need to register the S3 bucket (`MLSA-DataLake-<your
    initials>`) you created earlier in Lake Formation, so it will be managed and accessible
    through Lake Formation. To do this, click on the **Dashboard** link, expand **Data
    lake setup**, and then click on **Register Location**. Browse and select the bucket
    you created and click on **Register Location**. This S3 bucket will be used by
    Lake Formation to store data for the databases and manage its access permissions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create database**: Now, we are ready to set up a database called `bank_customer_db`
    for managing retail customers. Before we register the database, let’s first create
    a folder called the `bank_customer_db` folder under the `MLSA-DataLake-<your initials>`
    bucket. This folder will be used to store data files associated with the database.
    To do this, click on the **Create database** button on the Lake Formation dashboard
    and follow the instructions on the screen to create the database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully created a data lake powered by Lake Formation and
    created a database for data management. With this data lake created, we are now
    ready to build additional data management components. Next, we will create a data
    ingestion pipeline to move files into the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data ingestion pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the database is prepared, we can proceed to ingest data into this
    newly created database. As mentioned earlier, there are various data sources available,
    including databases like Amazon RDS, streaming platforms like social media feeds,
    and logs such as CloudTrail. Additionally, AWS offers a range of services for
    building data ingestion pipelines, such as AWS Glue, Amazon Kinesis, and AWS Lambda.
    In this phase of the exercise, we will focus on creating an AWS Lambda function
    job that will facilitate the ingestion of data from other S3 buckets into our
    target database. As mentioned earlier, Lambda functions can be used for lightweight
    data ingestion and processing tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a source S3 bucket and download data files**: Let’s create another
    S3 bucket, called `customer-data-source`, to represent the data source where we
    will ingest the data from.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a Lambda function**: Now, we will create the Lambda function that
    ingests data from the `customer-data-source` bucket to the `MLSA-DataLake-<your
    initials>` bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get started, navigate to the AWS Lambda management console, click on the
    **Functions** link in the left pane, and click on the **Create Function** button
    in the right pane. Choose **Author from scratch**, then enter `datalake-s3-ingest`
    for the function name, and select the latest Python version (e.g., 3.10) as the
    runtime. Keep the default for the execution role, which will create a new IAM
    role for this Lambda function. Click on **Create function** to continue.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, click on **Add trigger**, select **S3** as the trigger,
    and select the `customer-data-source` bucket as the source. For **Event Type**,
    choose the **Put** event and click on the **Add** button to complete the step.
    This trigger will allow the Lambda function to be invoked when there is an S3
    bucket event, such as saving a file into the bucket.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you add the trigger, you will be brought back to the `Lambda->function->
    datalake-s3-ingest` screen. Next, let’s create the function by replacing the default
    function template with the following code block. Replace the `desBucket` variable
    with the name of the actual bucket:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: The new function will also need S3 permission to copy files (*objects*) from
    one bucket to another. For simplicity, just add the `AmazonS3FullAccess` policy
    to the **execution IAM role** associated with the function. You can find the IAM
    role by clicking on the **Permission** tab for the Lambda function.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Trigger data ingestion**: Now, download the sample data files from the following
    link: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, save the file to your local machine. Extract the archived files. There
    should be two files (`customer_data.csv` and `churn_list.csv`).
  prefs: []
  type: TYPE_NORMAL
- en: You can now trigger the data ingestion process by uploading the `customer_detail.csv`
    and `churn_list.csv` files to the `customer-data-source` bucket and verify the
    process completion by checking the `MLSA-DataLake-<your initials>/bank_customer_db`
    folder for the two files.
  prefs: []
  type: TYPE_NORMAL
- en: You have now successfully created an AWS Lambda-based data ingestion pipeline
    to automatically move data from a source S3 bucket to a target S3 bucket. With
    this simple ingestion pipeline created and data moved, we are now ready to implement
    components to support the discovery of these data files. Next, let’s create an
    AWS Glue Data Catalog using the Glue crawler.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Glue Data Catalog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To allow discovery and querying of the data in the `bank_customer_db` database,
    we need to create a data catalog. As discussed earlier, Glue Data Catalog is a
    managed data catalog on AWS. It comes with a utility called an AWS Glue crawler
    that can help discover data and populate the catalog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use an AWS Glue crawler to crawl the files in the `bank_customer_db`
    S3 folder and generate the catalog:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grant permission for Glue**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let’s grant permission for AWS Glue to access the `bank_customer_db`
    database. We will create a new IAM role for the Glue service to assume on your
    behalf. To do this, create a new IAM service role called `AWSGlueServiceRole_data_lake`,
    and attach the `AWSGlueServiceRole` and `AmazonS3FullAccess` IAM-managed policies
    to it. Make sure you select **Glue** as the service when you create the role.
    If you are not familiar with how to create a role and attach a policy, follow
    the instructions at the following link: [https://docs.aws.amazon.com/IAM/latest/UserGuide](https://docs.aws.amazon.com/IAM/latest/UserGuide)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After the role is created, click on **Data lake permission** in the left pane
    of the Lake Formation management console and then click the **Grant** button in
    the right pane.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, select `AWSGlueServiceRole_data_lake` for **IAM users and
    role**and `bank_customer_db` under **Named data catalog resources**, choose **Super**
    for both **Database permissions** and **Grantable permissions**, and finally click
    on **Grant**. The **Super** permission allows the service role to have access
    to create databases and grant permission as part of the automation. `AWSGlueServiceRole_data_lake`
    will be used later to configure the Glue crawler job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Configure the Glue crawler job**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the Glue crawler by clicking on the **Crawler** link in the Lake Formation
    management console. A new browser tab for Glue will open. Click on the **Create
    Crawler** button to get started. Enter `bank_customer_db_crawler` as the name
    of the crawler. Click on the **Add a data source** button, select **S3**, and
    enter `s3://MLSA-DataLake-<your initials>/bank_customer_db/churn_list/` for the
    **include path** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Add another data source** button again. This time, enter `s3://MLSA-DataLake-<your
    initials>/bank_customer_db/customer_data/`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen, **Configure security settings**, select `AWSGlueServiceRole_data_lake`
    for the existing IAM role, which you used earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next **Set output and scheduling** screen, select `bank_customer_db`
    as the target database, and choose **on demand** as the frequency for the crawler
    schedule.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next **Review and create** screen, select **Finish** on the final screen
    to complete the setup.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Crawler** screen, select the `bank_customer_db_crawler` job you just
    created, click on **Run crawler**, and wait for the status to say **Ready**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate back to the Lake Formation management console and click on the **Tables**
    link. You will now see two new tables created (`churn_list` and `customer_data`).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully configured an AWS Glue crawler that automatically
    discovers table schemas from data files and creates data catalogs for the new
    data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You have successfully created the Glue Data Catalog for the newly ingested data.
    With that, we now have the proper component to support data discovery and query.
    Next, we will use Lake Formation and Athena to discover and query the data in
    the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering and querying data in the data lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To facilitate the data discovery and data understanding phase of the ML workflow,
    it is essential to incorporate data discovery and data query capabilities within
    the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Lake Formation already provides a list of tags, such as data type
    classification (for example, CSV), for searching tables in the database. Let’s
    add a few more tags for each table to make it more discoverable:'
  prefs: []
  type: TYPE_NORMAL
- en: Grant permission to edit the database tables by granting your current user ID
    **Super** permission for both the `customer_data` and `churn_list` tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s add some metadata to the table fields. Select the `customer_data` table,
    click on **Edit Schema**, select the `creditscore` field, click on **Edit** and
    **Add** to add a column property, and enter the following, where `description`
    is the key and the actual text is the value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Follow the same previous steps and add the following column property for the
    `exited` field in the `churn_list` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to do some searches using metadata inside the Lake Formation
    management console. Try typing the following words separately in the text box
    for **Find table by properties** to search for tables and see what’s returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FICO`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`churn flag`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`creditscore`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`customerid`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that you have found the table you are looking for, let’s query the table
    and see the actual data to learn how to query the data interactively, which is
    an important task performed by data scientists for data exploration and understanding.
    Select the table you want to query and click on the **View data** button in the
    **Actions** drop-down menu. This should bring you to the **Amazon Athena** screen.
    You should see a **Query** tab already created, and the query already executed.
    The results are displayed at the bottom of the screen. If you get a warning message
    stating that you need to provide an output location, select the **Settings** tab,
    and then click on the **Manage** button to provide an S3 location as the output
    location. You can run any other SQL query to explore the data further, such as
    joining the `customer_data` and `churn_list` tables with the `customerid` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You have now learned how to discover the data in Lake Formation and run queries
    against the data in a Lake Formation database and tables. Next, let’s run a data
    processing job using the Amazon Glue ETL service to make the data ready for ML
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Amazon Glue ETL job to process data for ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `customer_data` and `churn_list` tables contain features that are useful
    for ML. However, they need to be joined and processed so they can be used to train
    ML models. One option is for the data scientists to download these datasets and
    process them in a Jupyter notebook for model training. Another option is to process
    the data using a separate processing engine so that the data scientists can work
    with the processed data directly. Here, we will set up an AWS Glue job to process
    the data in the `customer_data` and `churn_list` tables and transform them into
    new ML features that are ready for model training directly:'
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new S3 bucket called `MLSA-DataLake-Serving-<your initials>`.
    We will use this bucket to store the output training datasets from the Glue job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the Lake Formation console, grant `AWSGlueService_Role` **Super** access
    to the `customer_data` and `churn_list` tables. We will use this role to run the
    Glue job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To start creating the Glue job, go to the Glue console and click on the **ETL
    Jobs** link on the Glue console. Click on **Script editor** and then click on
    the **Create script** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the script editor screen, change the job name from **Untitled job** to `customer_churn_process`
    for easy tracking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Job details** tab, select `AWSGlueService_Role` as the IAM role. Add
    a new `Job` parameter called `target_bucket` under `Advanced Properties` and enter
    the value of your target bucket for the output files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Script tab** screen, copy the following code blocks to the code section.
    Make sure to replace `default_bucket` with your own bucket in the code. The following
    code block first joins the `churn_list` and `customer_data` tables using the `customerid`
    column as the key, then transforms the `gender` and `geo` columns with an index,
    creates a new DataFrame with only the relevant columns, and finally saves the
    output file to an S3 location using the date and generated version ID as partitions.
    The code uses default values for the target bucket and prefix variables and generates
    a date partition and version partition for the S3 location. The job can also accept
    input arguments for these parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block sets up default configurations, such as `SparkContext`
    and a default bucket:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code joins the `customer_data` and `churn_list` tables into a
    single table using the `customerid` column as the key:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code block transforms several data columns from string labels
    to label indices and writes the final file to an output location in S3:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on **Save** and then the **Run job** button to run the job. Check the
    job running status by clicking on the **ETL jobs** link in the Glue console, and
    then click on **Job run monitoring**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the job completes, check the `s3://MLSA-DataLake-Serving-<your initials>/ml-customer-churn/<date>/<guid>/`
    location in S3 and see whether a new CSV file was generated. Open the file and
    see whether you see the new processed dataset in the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully built an AWS Glue job for data processing and feature
    engineering for ML. With this, you can automate data processing and feature engineering,
    which is critical to achieve reproducibility and governance. Try creating a crawler
    to crawl the newly processed data in the `MLSA-DataLake-Serving-<your initials>`
    bucket to make it available in the Glue catalog and run some queries against it.
    You should see a new table created with multiple partitions (for example, `ml-customer-churn`,
    `date`, and `GUID`) for the different training datasets. You can query the data
    by using the `GUID` partition as a query condition.
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline using Glue workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will construct a pipeline that executes a data ingestion job, followed
    by the creation of a database catalog for the data. Finally, a data processing
    job will be initiated to generate the training dataset. This pipeline will automate
    the flow of data from the source to the desired format, ensuring seamless and
    efficient data processing for ML model training:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, click on the **Workflows (orchestration)** link in the left pane of
    the Gluemanagement console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Add workflow** and enter a name for your workflow on the next screen.
    Then, click on the **Create workflow** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the workflow you just created and click on **Add trigger**. Select the
    **Add New** tab, and then enter a name for the trigger and select the `on-demand`
    trigger type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, you will see a new **Add Node** icon show up. Click
    on the **Add Node** icon, select the **Crawler** tab, and select `bank_customer_db_crawler`,
    then click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, click on the **Crawler** icon, and you will see
    a new **Add Trigger** icon show up. Click on the **Add Trigger** icon, select
    the **Add new** tab, and select **Start after ANY event** as the trigger logic,
    and then click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, click on the **Add Node** icon, select the **Jobs**
    tab, and select the `customer_churn_process` job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, the final workflow should look like the following
    diagram:![Figure 4.7 – Glue data flow design ](img/B20836_04_08.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.8: Glue data flow design'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, you are ready to run the workflow. Select the workflow and select **Run**
    from the **Actions** dropdown. You can monitor the running status by selecting
    the **Run ID** and clicking on **View run details**. You should see something
    similar to the following screenshot:![Figure 4.8 – Glue workflow execution ](img/Image2513.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.9: Glue workflow execution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Try deleting the `customer_data` and `churn_list` tables and re-run the workflow.
    See whether the new tables are created again. Check the `s3://MLSA-DataLake-Serving-<your
    initials>/ml-customer-churn/<date>/` S3 location to verify a new folder is created
    with a new dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have completed the hands-on lab and learned how to build
    a simple data lake and its supporting components to allow data cataloging, data
    querying, and data processing. You should now be able to apply some of the skills
    learned to real-world design and the implementation of a data management platform
    on AWS to support the ML development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the considerations for managing data in the
    context of ML and explored the architecture of an enterprise data management platform
    for ML. We examined the intersection of data management with the ML lifecycle
    and learned how to design a data lake architecture on AWS. To apply these concepts,
    we went through the process of building a data lake using AWS Lake Formation.
  prefs: []
  type: TYPE_NORMAL
- en: Through hands-on experience, we practiced data ingestion, processing, and cataloging
    for data discovery, querying, and ML tasks. Additionally, we gained proficiency
    in using AWS data management tools such as AWS Glue, AWS Lambda, and Amazon Athena.
    In the next chapter, our focus will shift to the architecture and technologies
    involved in constructing data science environments using open-source tools.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  prefs: []
  type: TYPE_NORMAL
