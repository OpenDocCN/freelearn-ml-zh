- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Data Management for ML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理
- en: As an ML solutions architecture practitioner, I often receive requests for guidance
    on designing data management platforms for ML workloads. Although data management
    platform architecture is typically treated as a separate technical discipline,
    it plays a crucial role in ML workloads. To create a comprehensive ML platform,
    ML solutions architects must understand the essential data architecture considerations
    for ML and be familiar with the technical design of a data management platform
    that caters to the needs of data scientists and automated ML pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习解决方案架构师，我经常收到关于设计机器学习工作负载的数据管理平台的指导请求。尽管数据管理平台架构通常被视为一个独立的技术学科，但它对机器学习工作负载起着至关重要的作用。为了创建一个全面的机器学习平台，机器学习解决方案架构师必须了解机器学习的基本数据架构考虑因素，并熟悉满足数据科学家和自动化机器学习管道需求的数据管理平台的技术设计。
- en: In this chapter, we will explore the intersection of data management and ML,
    discussing key considerations for designing a data management platform specifically
    tailored for ML. We will delve into the core architectural components of such
    a platform and examine relevant AWS technologies and services that can be used
    to build it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨数据管理和机器学习（ML）的交汇点，讨论为机器学习量身定制的数据管理平台的关键考虑因素。我们将深入研究此类平台的核心架构组件，并检查可用于构建该平台的相关AWS技术和服务。
- en: 'The following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容将涵盖：
- en: Data management considerations for ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的数据管理考虑因素
- en: Data management architecture for ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的数据管理架构
- en: Hands-on exercise – data management for ML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践练习 – 机器学习的数据管理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, you will need access to an AWS account and AWS services such
    as **Amazon** **S3**, **Amazon** **Lake Formation**, **AWS** **Glue**, and **AWS**
    **Lambda**. If you do not have an AWS account, follow the official AWS website’s
    instructions to create an account.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要访问AWS账户以及AWS服务，如**Amazon S3**、**Amazon Lake Formation**、**AWS Glue**和**AWS
    Lambda**。如果您没有AWS账户，请按照AWS官方网站的说明创建账户。
- en: Data management considerations for ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理考虑因素
- en: Data management is a broad and complex topic. Many organizations have dedicated
    data management teams and organizations to manage and govern the various aspects
    of a data platform. Historically, data management primarily revolved around fulfilling
    the requirements of transactional systems and analytics systems. However, as ML
    solutions gain prominence, there are now additional business and technological
    factors to consider when it comes to data management platforms. The advent of
    ML introduces new requirements and challenges that necessitate an evolution in
    data management practices to effectively support these advanced solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管理是一个广泛且复杂的话题。许多组织都设有专门的数据管理团队和组织来管理和治理数据平台的各种方面。从历史上看，数据管理主要围绕满足事务系统和分析系统的需求展开。然而，随着机器学习解决方案的兴起，在考虑数据管理平台时，现在还需要考虑额外的商业和技术因素。机器学习的出现引入了新的需求和挑战，这需要数据管理实践的发展，以有效地支持这些高级解决方案。
- en: 'To understand where data management intersects with the ML workflow, let’s
    bring back the ML lifecycle, as illustrated in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解数据管理如何与机器学习工作流程相交，让我们回顾一下以下图中所示的机器学习生命周期：
- en: '![Figure 4.1 – Intersection of data management and the ML life cycle ](img/B20836_04_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 数据管理和机器学习生命周期的交汇](img/B20836_04_01.png)'
- en: 'Figure 4.1: Intersection of data management and the ML lifecycle'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：数据管理和机器学习生命周期的交汇
- en: 'At a high level, data management intersects with the ML lifecycle in three
    stages: *data understanding and preparation*, *model training and evaluation*,
    and *model deployment*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，数据管理在三个阶段与机器学习生命周期相交：*数据理解和准备*、*模型训练和评估*以及*模型部署*。
- en: 'During the *data understanding and preparation* stage, data scientists undertake
    several essential tasks. They begin by identifying relevant data sources that
    contain datasets suitable for their modeling tasks. Exploratory data analysis
    is then performed to gain insights into the dataset, including data statistics,
    correlations between features, and data sample distributions. Additionally, data
    preparation for model training and validation is crucial, involving a series of
    steps that typically include the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在**数据理解和准备**阶段，数据科学家需要执行多项基本任务。他们首先识别与建模任务相关的合适数据集的数据来源。然后进行探索性数据分析，以了解数据集，包括数据统计、特征之间的相关性以及数据样本分布。此外，为模型训练和验证准备数据至关重要，通常包括以下步骤：
- en: '**Data validation**: The data is checked for errors and anomalies to ensure
    its quality. This includes verifying the data range, distribution, and data types
    and identifying missing or null values.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：检查数据中的错误和异常，以确保其质量。这包括验证数据范围、分布和数据类型，并识别缺失或空值。'
- en: '**Data cleaning**: Any identified data errors are fixed or corrected to ensure
    the accuracy and consistency of the dataset. This may involve removing duplicates,
    handling missing values, or resolving inconsistencies.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：任何识别出的数据错误都需被修正或更正，以确保数据集的准确性和一致性。这可能包括删除重复项、处理缺失值或解决不一致性。'
- en: '**Data enrichment**: Additional value is derived from the data through techniques
    like joining different datasets or transforming the data. This helps generate
    new signals and insights that can enhance the modeling process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据丰富**：通过合并不同数据集或转换数据等技术从数据中提取额外价值。这有助于生成新的信号和洞察，从而增强建模过程。'
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据标注**：对于监督式机器学习模型训练，训练和测试数据集需要由人工标注员或机器学习模型准确标注。这一关键步骤对于确保高质量模型的发展和验证是必要的。'
- en: 'The data management capabilities needed during this stage encompass the following
    aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段所需的数据管理能力包括以下方面：
- en: '**Dataset discovery**: The capability to search and locate curated datasets
    using relevant metadata like dataset name, description, field name, and data owner.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集发现**：能够使用相关元数据（如数据集名称、描述、字段名称和数据所有者）搜索和定位经过整理的数据集。'
- en: '**Data access**: The ability to access both raw and processed datasets to perform
    exploratory data analysis. This ensures data scientists can explore and analyze
    the data effectively.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问**：能够访问原始和经过处理的数据集以执行探索性数据分析。这确保数据科学家可以有效地探索和分析数据。'
- en: '**Querying and retrieval**: The capability to run queries against selected
    datasets to obtain details such as statistical information, data quality metrics,
    and data samples. Additionally, it includes the ability to retrieve data from
    the data management platform to a data science environment for further processing
    and feature engineering.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询和检索**：运行查询以针对选定数据集获取详细信息的能力，例如统计信息、数据质量指标和数据样本。此外，还包括将数据从数据管理平台检索到数据科学环境进行进一步处理和特征工程的能力。'
- en: '**Scalable data processing**: The ability to execute data processing operations
    on large datasets efficiently. This ensures that data scientists can handle and
    process substantial amounts of data during model development and experimentation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展的数据处理**：在大型数据集上高效执行数据处理操作的能力。这确保数据科学家在模型开发和实验过程中可以处理和加工大量数据。'
- en: 'During the stage of model training and validation, data scientists are responsible
    for generating a training and validation dataset to conduct formal model training.
    To facilitate this process, the following data management capabilities are essential:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在**模型训练和验证**阶段，数据科学家负责生成训练和验证数据集以进行正式的模型训练。为了促进这一过程，以下数据管理能力是必不可少的：
- en: '**Data processing and automated workflows**: A data management platform should
    provide robust data processing capabilities along with automated workflows. This
    enables the conversion of raw or curated datasets into training and validation
    datasets in various formats suitable for model training.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理和自动化工作流**：数据管理平台应提供强大的数据处理能力以及自动化工作流。这使将原始或整理好的数据集转换为适合模型训练的多种格式的训练和验证数据集成为可能。'
- en: '**Data repository and versioning**: An efficient data management platform should
    offer a dedicated data repository to store and manage the training and validation
    datasets. Additionally, it should support versioning, allowing data scientists
    to keep track of different iterations and modifications made to the datasets,
    along with the versions of the code and trained ML models.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储库和版本控制**：一个高效的数据管理平台应提供专门的数据存储库来存储和管理训练和验证数据集。此外，它应支持版本控制，使数据科学家能够跟踪对数据集进行的不同迭代和修改，以及代码和训练好的机器学习模型的版本。'
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models. This is a highly labor-intensive task, requiring purpose-built software
    tools to do it at scale.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据标注**：对于监督式机器学习模型训练，训练和测试数据集需要由人工标注员或机器学习模型准确标注。这一关键步骤对于确保高质量模型的发展和验证至关重要。这是一项劳动密集型任务，需要专门构建的软件工具来大规模完成。'
- en: '**ML features/embeddings generation and storage**: Some ML features/embeddings
    (e.g., averages, sums, and text embeddings) need to be pre-computed for one or
    more downstream model training tasks. These features/embeddings often need to
    be managed using purpose-built tools for efficient access and reuse.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习特征/嵌入生成和存储**：一些机器学习特征/嵌入（例如，平均值、总和和文本嵌入）需要为一个或多个下游模型训练任务预先计算。这些特征/嵌入通常需要使用专门构建的工具进行管理，以实现高效的访问和重用。'
- en: '**Dataset provisioning for model training**: The platform should provide mechanisms
    to serve the training and validation datasets to the model training infrastructure.
    This ensures that the datasets are accessible by the training environment, allowing
    data scientists to train models effectively.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为模型训练提供数据集**：平台应提供机制，将训练和验证数据集提供给模型训练基础设施。这确保了数据集可由训练环境访问，使数据科学家能够有效地训练模型。'
- en: 'During the stage of model deployment, the focus shifts toward utilizing the
    trained models to serve predictions. To support this stage effectively, the following
    data management capabilities are crucial:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署阶段，重点转向利用训练好的模型进行预测。为了有效地支持这一阶段，以下数据管理能力至关重要：
- en: '**Serving data for feature processing**: The data management platform should
    be capable of serving the data required for feature processing as part of the
    input data when invoking the deployed models. This ensures that the models receive
    the relevant data inputs required for generating predictions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为特征处理提供数据**：数据管理平台应能够在调用部署的模型时，作为输入数据的一部分提供用于特征处理的数据。这确保了模型接收到生成预测所需的相关数据输入。'
- en: '**Serving pre-computed features/embeddings**: In some cases, pre-computed features/embeddings
    are utilized as inputs when invoking the deployed models. The data management
    platform should have the capability to serve these pre-computed features seamlessly,
    allowing the models to incorporate them into the prediction process.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供预计算的特性和嵌入**：在某些情况下，在调用部署的模型时，会使用预计算的特性和嵌入作为输入。数据管理平台应具备无缝提供这些预计算特性的能力，使模型能够将它们纳入预测过程。'
- en: In contrast to traditional data access patterns for transactional or business
    intelligence solutions, where developers can utilize non-production data in lower
    environments for development purposes, data scientists typically require access
    to production data for model development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的事务或商业智能解决方案的数据访问模式不同，在这些模式中，开发者可以在较低的环境中利用非生产数据进行开发，数据科学家通常需要访问生产数据以进行模型开发。
- en: Having explored the considerations for ML data management, we will now delve
    deeper into the data management architecture specifically designed for ML. It
    is important to understand that effective data management is crucial for success
    in applied ML. Organizations fail with ML not just due to poor algorithms or inaccurate
    models, but also due to problems with real-world data and production systems.
    Data management shortcomings can sink ML projects despite brilliant modeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了机器学习数据管理的考虑因素之后，我们将更深入地探讨专门为机器学习设计的机器学习数据管理架构。重要的是要理解，有效的数据管理对于应用机器学习的成功至关重要。组织在机器学习上失败的原因不仅仅是算法差或模型不准确，还可能是由于现实世界数据和生产系统的问题。数据管理的不足可能会使即使建模出色的人工智能项目失败。
- en: Data management architecture for ML
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理架构
- en: Depending on the scale of your ML initiatives, it is important to consider different
    data management architecture patterns to effectively support them. The right architecture
    depends on the scale and scope of the ML initiatives within an organization in
    order to balance the business needs with engineering efforts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的ML倡议的规模，考虑不同的数据管理架构模式以有效支持它们是很重要的。正确的架构取决于组织内ML倡议的规模和范围，以便在业务需求和工程努力之间取得平衡。
- en: 'For *small-scale ML projects* characterized by limited data scope, a small
    team size, and minimal cross-functional dependencies, a purpose-built data pipeline
    tailored to meet specific project requirements can be a suitable approach. For
    instance, if your project involves working with structured data sourced from an
    existing data warehouse and a publicly available dataset, you can consider developing
    a straightforward data pipeline. This pipeline would extract the necessary data
    from the data warehouse and public domain and store it in a dedicated storage
    location owned by the project team. This data extraction process can be scheduled
    as needed to facilitate further analysis and processing. The following diagram
    illustrates a simplified data management flow designed to support a small-scale
    ML project:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有有限数据范围、小型团队规模和最小跨职能依赖性的*小型ML项目*，一个专门构建的、满足特定项目需求的数据管道可能是一个合适的方法。例如，如果你的项目涉及从现有的数据仓库和公开可用的数据集中获取结构化数据，你可以考虑开发一个简单的数据管道。这个管道将从数据仓库和公共领域提取必要的数据，并将其存储在项目团队拥有的专用存储位置。此数据提取过程可以根据需要安排，以方便进一步的分析和处理。以下图表展示了一个简化的数据管理流程，旨在支持小型ML项目：
- en: '![Figure 4.2 – Data architecture for an ML project with limited scope ](img/B20836_04_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 有限范围ML项目的数据架构](img/B20836_04_02.png)'
- en: 'Figure 4.2: Data architecture for an ML project with limited scope'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：有限范围ML项目的数据架构
- en: For *large-scale ML initiatives* at the enterprise level, the data management
    architecture closely resembles that of enterprise analytics. Both require robust
    support for data ingestion from diverse sources and centralized management of
    data for various processing and access requirements. While analytics data management
    primarily deals with structured data and often relies on an enterprise data warehouse
    as its core backend, ML data management needs to handle structured, semi-structured,
    and unstructured data for different ML tasks. Consequently, a data lake architecture
    is commonly adopted. ML data management is typically an integral part of the broader
    enterprise data management strategy, encompassing both analytics and ML initiatives.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业层面的*大规模ML倡议*，数据管理架构与企业分析非常相似。两者都需要对来自不同来源的数据摄取提供强大的支持，以及针对各种处理和访问需求的数据集中管理。虽然分析数据管理主要处理结构化数据，并且通常依赖于企业数据仓库作为其核心后端，但ML数据管理需要处理结构化、半结构化和非结构化数据以执行不同的ML任务。因此，通常采用数据湖架构。ML数据管理通常是更广泛的企业数据管理策略的一部分，包括分析和ML倡议。
- en: 'The following diagram illustrates a logical enterprise data management architecture
    comprising key components such as data ingestion, data storage, data processing,
    data catalog, data security, and data access:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了一个包含关键组件如数据摄取、数据存储、数据处理、数据目录、数据安全和数据访问的逻辑企业数据管理架构：
- en: '![Figure 4.3 – Enterprise data management ](img/B20836_04_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 企业数据管理](img/B20836_04_03.png)'
- en: 'Figure 4.3: Enterprise data management'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：企业数据管理
- en: In the following sections, we will delve into a detailed analysis of each key
    component of enterprise data management, providing an in-depth understanding of
    their functionalities and implications within a data management architecture built
    using AWS native services in the cloud. By exploring the specific characteristics
    and capabilities of these components, we will gain valuable insights into the
    overall structure and mechanics of an AWS-based data management architecture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将深入分析企业数据管理的每个关键组件，提供对它们在云中使用AWS原生服务构建的数据管理架构中的功能和影响的深入了解。通过探索这些组件的具体特性和能力，我们将获得关于基于AWS的数据管理架构的整体结构和机制的有价值见解。
- en: Data storage and management
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储和管理
- en: Data storage and management is a fundamental component of the overall ML data
    management architecture. ML workloads often require data from diverse sources
    and in various formats, and the sheer volume of data can be substantial, particularly
    when dealing with unstructured data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储和管理是整体机器学习（ML）数据管理架构的基本组成部分。ML工作负载通常需要来自不同来源和多种格式的数据，尤其是处理非结构化数据时，数据量可能非常庞大。
- en: To address these requirements, cloud object data storage solutions like Amazon
    S3 are commonly employed as the underlying storage medium. Conceptually, cloud
    object storage can be likened to a file storage system that accommodates files
    of different formats. Moreover, the storage system allows for the organization
    of files using prefixes, which serve as virtual folders for enhanced object management.
    It is important to note that these prefixes do not correspond to physical folder
    structures. The term “object storage” stems from the fact that each file is treated
    as an independent object, bundled with metadata, and assigned a unique identifier.
    Object storage boasts features such as virtually unlimited storage capacity, robust
    object analytics based on metadata, API-based access, and cost-effectiveness.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，云对象数据存储解决方案，如Amazon S3，通常被用作底层存储介质。从概念上讲，云对象存储可以类比为一种能够容纳不同格式文件的文件存储系统。此外，存储系统允许使用前缀来组织文件，这些前缀作为虚拟文件夹，以增强对象管理。需要注意的是，这些前缀并不对应于物理文件夹结构。术语“对象存储”源于每个文件都被视为一个独立的对象，附带元数据，并分配一个唯一的标识符。对象存储具有诸如几乎无限的存储容量、基于元数据的强大对象分析、基于API的访问和成本效益等特性。
- en: To efficiently handle the vast quantities of data stored in cloud object storage,
    it is advisable to implement a data lake architecture that leverages this storage
    medium. A data lake, tailored to encompass the entire enterprise or a specific
    line of business, acts as a centralized hub for data management and access. Designed
    to accommodate limitless data volumes, the data lake facilitates the organization
    of data across various lifecycle stages, including raw, transformed, curated,
    and ML feature data. Its primary purpose is to consolidate disparate data silos
    into a singular repository that enables centralized management and access for
    both analytics and ML requirements. Notably, a data lake can house diverse data
    formats, such as structured data from databases, unstructured data like documents,
    semi-structured data in JSON and XML formats, as well as binary formats encompassing
    images, videos, and audio files. This capability proves particularly invaluable
    for ML workloads, as ML often involves working with data in multiple formats.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地处理存储在云对象存储中的大量数据，建议实施一个利用这种存储介质的数据湖架构。一个针对整个企业或特定业务线量身定制的数据湖，充当数据管理和访问的中心枢纽。设计用于容纳无限数据量，数据湖促进了数据在各个生命周期阶段（包括原始、转换、精选和ML特征数据）的组织。其主要目的是将分散的数据孤岛合并到一个单一的存储库中，以便为分析和ML需求提供集中式管理和访问。值得注意的是，数据湖可以容纳多种数据格式，如来自数据库的结构化数据、文档等非结构化数据、JSON和XML格式的半结构化数据，以及包含图像、视频和音频文件的二进制格式。这种能力对于ML工作负载尤其有价值，因为ML通常涉及处理多种格式的数据。
- en: The data lake should be organized into different zones. For example, a *landing
    zone* should be established as the target for the initial data ingestion from
    different sources. After data preprocessing and data quality management processing,
    the data can be moved to the raw data zone. Data in the *raw data zone* can be
    further transformed and processed to meet different business and downstream consumption
    needs. To further ensure the reliability of the dataset for usage, the data can
    be curated and stored in the *curated data zone*. For ML tasks, ML features often
    need to be pre-computed and stored in an ML feature zone for reuse purposes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖应组织成不同的区域。例如，应建立一个*着陆区*作为来自不同来源的初始数据摄入的目标。在数据预处理和数据质量管理处理之后，数据可以被移动到原始数据区。*原始数据区*中的数据可以进一步转换和处理，以满足不同的业务和下游消费需求。为了进一步确保数据集的使用可靠性，数据可以被精选并存储在*精选数据区*中。对于ML任务，ML特征通常需要预先计算并存储在ML特征区中，以便于重复使用。
- en: AWS Lake Formation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Lake Formation
- en: 'AWS Lake Formation is a comprehensive data management service offered by AWS,
    which streamlines the process of building and maintaining a data lake on the AWS
    platform. The following figure illustrates the core components of AWS Lake Formation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lake Formation 是 AWS 提供的全面数据管理服务，它简化了在 AWS 平台上构建和维护数据湖的过程。以下图示说明了 AWS Lake
    Formation 的核心组件：
- en: '![Figure 4.4 – AWS Lake Formation ](img/B20836_04_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – AWS Lake Formation](img/B20836_04_04.png)'
- en: 'Figure 4.4: AWS Lake Formation'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：AWS Lake Formation
- en: 'Overall, AWS Lake Formation offers four fundamental capabilities to enhance
    data lake management:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，AWS Lake Formation 提供了四个基本功能来增强数据湖管理：
- en: '**Data source crawler**: This functionality automatically examines data files
    within the data lake to infer their underlying structure, enabling efficient organization
    and categorization of the data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据源爬虫**：此功能自动检查数据湖中的数据文件，以推断其底层结构，从而实现数据的有效组织和分类。'
- en: '**Data catalog creation and maintenance**: AWS Lake Formation facilitates the
    creation and ongoing management of a data catalog, providing a centralized repository
    for metadata, enabling easy data discovery and exploration within the data lake.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据目录创建和维护**：AWS Lake Formation 促进了数据目录的创建和持续管理，提供了一个集中式元数据存储库，便于在数据湖中进行数据发现和探索。'
- en: '**Data transformation processing**: With built-in data transformation capabilities,
    the service allows for the processing and transformation of data stored in the
    data lake, enabling data scientists and analysts to work with refined and optimized
    datasets.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换处理**：凭借内置的数据转换功能，该服务允许处理和转换存储在数据湖中的数据，使数据科学家和分析人员能够使用精炼和优化的数据集进行工作。'
- en: '**Data security and access control**: AWS Lake Formation ensures robust data
    security by providing comprehensive access control mechanisms and enabling fine-grained
    permissions management, ensuring that data is accessed only by authorized individuals
    and teams.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据安全和访问控制**：AWS Lake Formation 通过提供全面的访问控制机制和启用细粒度权限管理，确保数据仅由授权的个人和团队访问，从而确保数据安全。'
- en: Lake Formation integrates with AWS Glue, a serverless **Extract, Transform,
    Load** (**ETL**) and data catalog service, to provide data catalog management
    and data ETL processing functionality. We will cover ETL and data catalog components
    separately in later sections.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Lake Formation 与 AWS Glue 集成，AWS Glue 是一个无服务器 **提取、转换、加载**（**ETL**）和数据目录服务，以提供数据目录管理和数据
    ETL 处理功能。我们将在后面的章节中分别介绍 ETL 和数据目录组件。
- en: Lake Formation provides a centralized data access management capability for
    managing data access permissions for databases, tables, or different registered
    S3 locations. For databases and tables, the permission can be granularly assigned
    to individual tables and columns and database functions, such as creating tables
    and inserting records.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Lake Formation 提供了集中式数据访问管理功能，用于管理数据库、表或不同注册的 S3 位置的数据访问权限。对于数据库和表，权限可以细粒度地分配给单个表、列和数据库功能，例如创建表和插入记录。
- en: Data ingestion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data ingestion is the bridge between data sources and data storage. It plays
    a crucial role in acquiring data from diverse sources, including structured, semi-structured,
    and unstructured formats, such as databases, knowledge graphs, social media, file
    storage, and IoT devices. Its primary responsibility is to store this data persistently
    in various storage solutions like object data storage (e.g., Amazon S3), data
    warehouses, or other data stores. Effective data ingestion patterns should incorporate
    both real-time streaming and batch ingestion mechanisms to cater to different
    types of data sources and ensure timely and efficient data acquisition.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取是数据源和数据存储之间的桥梁。它在从各种来源获取数据方面发挥着关键作用，包括结构化、半结构化和非结构化格式，如数据库、知识图谱、社交媒体、文件存储和物联网设备。其主要职责是将这些数据持久地存储在各种存储解决方案中，如对象数据存储（例如，Amazon
    S3）、数据仓库或其他数据存储。有效的数据摄取模式应结合实时流式传输和批量摄取机制，以满足不同类型的数据源，并确保及时高效的数据获取。
- en: Various data ingestion technologies and tools cater to different ingestion patterns.
    For streaming data ingestion, popular choices include Apache Kafka, Apache Spark
    Streaming, and Amazon Kinesis/Kinesis Firehose. These tools enable real-time data
    ingestion and processing. On the other hand, for batch-oriented data ingestion,
    tools like **Secure File Transfer Protocol** (**SFTP**) and AWS Glue are commonly
    used. AWS Glue, in particular, offers support for a wide range of data sources
    and targets, including Amazon RDS, MongoDB, Kafka, Amazon DocumentDB, S3, and
    any databases that support JDBC connections. This flexibility allows for seamless
    ingestion of data from various sources into the desired data storage or processing
    systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的数据摄入技术和工具满足不同的摄入模式。对于流数据摄入，流行的选择包括Apache Kafka、Apache Spark Streaming和Amazon
    Kinesis/Kinesis Firehose。这些工具能够实现实时数据摄入和处理。另一方面，对于面向批次的摄入，常用的工具有**安全文件传输协议**（**SFTP**）和AWS
    Glue。特别是AWS Glue，它为广泛的源和目标提供支持，包括Amazon RDS、MongoDB、Kafka、Amazon DocumentDB、S3以及任何支持JDBC连接的数据库。这种灵活性使得从各种来源无缝摄入数据到所需的数据存储或处理系统成为可能。
- en: 'When making decisions on which tools to use for data ingestion, it is important
    to assess the tools and technologies based on practical needs. The following are
    some of the considerations when deciding on data ingestion tools:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪些工具进行数据摄入时，根据实际需求评估工具和技术非常重要。以下是在决定数据摄入工具时的一些考虑因素：
- en: '**Data format, size, and scalability**: Take into account the various data
    formats, data size, and scalability needs. ML projects could be using data from
    different sources and different formats (e.g., **CSV**, **Parquet**, JSON/XML,
    documents, or image/audio/video files). Determine whether the infrastructure can
    handle large data volumes efficiently when necessary and scale down to reduce
    costs during periods of low volume.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据格式、大小和可扩展性**：考虑各种数据格式、数据大小和可扩展性需求。机器学习项目可能使用来自不同来源和不同格式的数据（例如，**CSV**、**Parquet**、JSON/XML、文档或图像/音频/视频文件）。确定在必要时基础设施是否能够高效地处理大量数据，并在低量期间缩减规模以降低成本。'
- en: '**Ingestion patterns**: Consider the different data ingestion patterns that
    need to be supported. The tool or combination of several tools should support
    both batch ingestion patterns (transferring bulk data at specific time intervals)
    and real-time streaming (processing data such as sensor data or website clickstreams
    in real time).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄入模式**：考虑需要支持的不同数据摄入模式。工具或几个工具的组合应支持批量摄入模式（在特定时间间隔传输大量数据）和实时流（实时处理如传感器数据或网站点击流）。'
- en: '**Data preprocessing capability**: Evaluate whether the ingested data needs
    to be preprocessed before it is stored in the target data repository. Look for
    tools that offer built-in processing capability or seamless integration with external
    processing tools.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理能力**：评估在数据存储到目标数据存储库之前，是否需要对摄入的数据进行预处理。寻找提供内置处理能力或与外部处理工具无缝集成的工具。'
- en: '**Security**: Ensure that the selected tools provide robust security mechanisms
    for authentication and authorization to protect sensitive data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：确保所选工具提供强大的安全机制，用于身份验证和授权，以保护敏感数据。'
- en: '**Reliability**: Verify that the tools offer failure recovery mechanisms to
    prevent critical data loss during the ingestion process. If recovery capability
    is lacking, ensure there is an option to rerun ingestion jobs from the source.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：验证工具是否提供故障恢复机制，以防止在摄入过程中发生关键数据丢失。如果缺乏恢复能力，确保有选项可以从源重新运行摄入作业。'
- en: '**Support for different data sources and targets**: The chosen ingestion tools
    should be compatible with a wide range of data sources, including databases, files,
    and streaming sources. Additionally, they should provide an API for easy data
    ingestion.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持不同的数据源和目标**：所选的摄入工具应与广泛的源兼容，包括数据库、文件和流源。此外，它们应提供API以实现轻松的数据摄入。'
- en: '**Manageability**: Another important factor to consider is the level of manageability.
    Does the tool require self-management, or is it a fully managed solution? Consider
    the trade-offs between cost and operational complexity before making a decision.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可管理性**：另一个需要考虑的重要因素是可管理性水平。该工具是否需要自我管理，或者它是否是一个完全管理的解决方案？在做出决定之前，考虑成本和操作复杂性的权衡。'
- en: AWS provides several services for data ingestion into a data lake on their platform.
    These services include Kinesis Data Streams, Kinesis Firehose, AWS Managed Streaming
    for Kafka, and AWS Glue Streaming, which cater to streaming data requirements.
    For batch ingestion, options such as AWS Glue, SFTP, and AWS **Data Migration
    Service** (**DMS**) are available. In the upcoming section, we will delve into
    the usage of Kinesis Firehose and AWS Glue to manage data ingestion processes
    for data lakes. We will also discuss AWS Lambda, a serverless compute service,
    for a simple and lightweight data ingestion alternative.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AWS在其平台上提供了多种服务，用于将数据导入其数据湖。这些服务包括Kinesis数据流、Kinesis Firehose、AWS管理的Kafka流和AWS
    Glue流，它们满足流数据的需要。对于批量导入，可用的选项包括AWS Glue、SFTP和AWS **数据迁移服务**（**DMS**）。在下一节中，我们将深入探讨如何使用Kinesis
    Firehose和AWS Glue来管理数据湖的数据导入过程。我们还将讨论AWS Lambda，这是一种无服务器计算服务，提供简单轻量级的数据导入替代方案。
- en: Kinesis Firehose
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kinesis Firehose
- en: Kinesis Firehose is a service that streamlines the process of loading streaming
    data into a data lake. It is a fully managed solution, meaning you don’t have
    to worry about managing the underlying infrastructure. Instead, you can interact
    with the service’s API to handle the ingestion, processing, and delivery of your
    data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose是一种简化将流数据加载到数据湖中的过程的工具。它是一个完全托管解决方案，这意味着您无需担心管理底层基础设施。相反，您可以通过与服务的API交互来处理数据的导入、处理和交付。
- en: 'Kinesis Firehose provides comprehensive support for various scalable data ingestion
    requirements, including:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose为各种可扩展的数据导入需求提供全面支持，包括：
- en: Seamless integration with diverse data sources such as websites, IoT devices,
    and video cameras. This is achieved using an ingestion agent or ingestion API.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与各种数据源的无缝集成，如网站、物联网设备和视频摄像头。这是通过使用导入代理或导入API实现的。
- en: Versatility in delivering data to multiple destinations, including Amazon S3,
    Amazon Redshift (an AWS data warehouse service), Amazon OpenSearch (a managed
    search engine), and Splunk (a log aggregation and analysis product).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将数据发送到多个目的地方面具有多功能性，包括Amazon S3、Amazon Redshift（一个AWS数据仓库服务）、Amazon OpenSearch（一个托管搜索引擎）和Splunk（一个日志聚合和分析产品）。
- en: Seamless integration with AWS Lambda and Kinesis Data Analytics, offering advanced
    data processing capabilities. With AWS Lambda, you can leverage serverless computing
    to execute custom functions written in languages like Python, Java, Node.js, Go,
    C#, and Ruby. For more comprehensive information on the functionality of Lambda,
    please refer to the official AWS documentation.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AWS Lambda和Kinesis Data Analytics的无缝集成，提供高级数据处理能力。使用AWS Lambda，您可以利用无服务器计算来执行用Python、Java、Node.js、Go、C#和Ruby等语言编写的自定义函数。有关Lambda功能的更详细信息，请参阅AWS官方文档。
- en: 'The following figure illustrates the data flow with Kinesis Firehose:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用Kinesis Firehose的数据流：
- en: '![Figure 4.5 – Kinesis Firehose data flow ](img/B20836_04_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – Kinesis Firehose数据流](img/B20836_04_05.png)'
- en: 'Figure 4.5: Kinesis Firehose data flow'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：Kinesis Firehose数据流
- en: Kinesis operates by establishing delivery streams, which are the foundational
    components in the Firehose architecture responsible for receiving streaming data
    from data producers. These delivery streams can be configured with various delivery
    destinations, such as S3 and Redshift. To accommodate the data volume generated
    by the producers, you can adjust the throughput of the data stream by specifying
    the number of shards. Each shard has the capacity to ingest 1 MB/sec of data and
    can support data reading at a rate of 2 MB/sec. Additionally, Kinesis Firehose
    offers APIs for increasing the number of shards and merging them when needed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis通过建立交付流来运行，这些流是Firehose架构中的基础组件，负责从数据生产者接收流数据。这些交付流可以配置各种交付目的地，如S3和Redshift。为了适应生产者生成的大量数据，您可以通过指定分片数量来调整数据流的吞吐量。每个分片具有每秒1
    MB的数据导入能力，并可以以每秒2 MB的速度支持数据读取。此外，Kinesis Firehose还提供API来增加分片数量并在需要时合并它们。
- en: AWS Glue
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Glue
- en: AWS Glue is a comprehensive serverless ETL service that helps manage the data
    integration and ingestion process for data lakes. It seamlessly connects with
    various data sources, including transactional databases, data warehouses, and
    NoSQL databases, facilitating the movement of data to different destinations,
    such as Amazon S3\. This movement can be scheduled or triggered by events. Additionally,
    AWS Glue offers the capability to process and transform data before delivering
    it to the target. It provides a range of processing options, such as the Python
    shell for executing Python scripts and Apache Spark for Spark-based data processing
    tasks. With AWS Glue, you can efficiently integrate and ingest data into your
    data lake, benefiting from its fully managed and serverless nature.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS Lambda is AWS’s serverless computing platform. It seamlessly integrates
    with various AWS services, including Amazon S3\. By leveraging Lambda, you can
    trigger the execution of functions in response to events, such as the creation
    of a new file in S3\. These Lambda functions can be developed to move data from
    different sources, such as copying data from a source S3 bucket to a target landing
    bucket in a data lake.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that AWS Lambda is not specifically designed for large-scale
    data movement or processing tasks, due to limitations such as memory size and
    maximum execution time allowed. However, for simpler data ingestion and processing
    jobs, it proves to be a highly efficient tool.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Data cataloging
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A data catalog plays a crucial role in enabling data analysts and scientists
    to discover and access data stored in a central data storage. It becomes particularly
    important during the data understanding and exploration phase of the ML lifecycle
    when scientists need to search and comprehend available data for their ML projects.
    When evaluating a data catalog tool, consider the following key factors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata catalog**: The technology should support a central data catalog
    for effective management of data lake metadata. This involves handling metadata
    such as database names, table schemas, and table tags. The Hive metastore catalog
    is a popular standard for managing metadata catalogs.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated data cataloging**: The technology should have the capability to
    automatically discover and catalog datasets, as well as to infer data schemas
    from various data sources like Amazon S3, relational databases, NoSQL databases,
    and logs. Typically, this functionality is implemented through a crawler that
    scans data sources, identifies metadata elements (e.g., column names, data types),
    and adds them to the catalog.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tagging flexibility**: The technology should have the ability to assign custom
    attributes or tags to metadata entities like databases, tables, and fields. This
    flexibility supports enhanced data search and discovery capabilities within the
    catalog.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other tools**: The technology should allow seamless integration
    of the data catalog with a wide range of data processing tools, enabling easy
    access to the underlying data. Additionally, native integration with data lake
    management platforms is advantageous.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search functionality**: The technology should have a robust search capability
    across diverse metadata attributes within the catalog. This includes searching
    by database, table, and field names, custom tags or descriptions, and data types.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to building data catalogs, there are various technical options
    available. In this section, we first explore how AWS Glue can be utilized for
    data cataloging purposes. We will also discuss a **Do-It-Yourself** (**DIY**)
    option for a data catalog using standard AWS services such as Lambda and OpenSearch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue Data Catalog
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS Glue offers a comprehensive solution for data cataloging, integrating seamlessly
    with AWS Lake Formation and other AWS services. The AWS Glue Data Catalog can
    be a drop-in replacement for the Hive metastore catalog, so any Hive metastore-compatible
    applications can work with the AWS Glue Data Catalog. With AWS Glue, you can automatically
    discover, catalog, and organize your data assets, making them easily searchable
    and accessible to data analysts and scientists. Here are some key features and
    benefits of using AWS Glue for data cataloging:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated data discovery**: AWS Glue provides automated data discovery capabilities.
    By using data crawlers, Glue can scan and analyze data from diverse structured
    and semi-structured sources such as Amazon S3, relational databases, NoSQL databases,
    and more. It identifies metadata information, including table schemas, column
    names, and data types, that is stored in the AWS Glue Data Catalog.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralized metadata repository**: The AWS Glue Data Catalog serves as a
    centralized metadata repository for your data assets. It provides a unified view
    of your data, making it easier to search, query, and understand the available
    datasets.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata management**: AWS Glue allows you to manage and maintain metadata
    associated with your data assets. You can define custom tags, add descriptions,
    and organize your data using databases, tables, and partitions within the Data
    Catalog.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata hierarchy of the AWS Glue Data Catalog is organized using databases
    and tables. Databases serve as containers for tables, which hold the actual data.
    Like traditional databases, a single database can house multiple tables, which
    can be sourced from various data stores. However, each table is exclusively associated
    with a single database. To query these databases and tables, one can utilize Hive
    metastore-compatible tools such as Amazon Athena to execute SQL queries. When
    collaborating with AWS Lake Formation, access permissions to the catalog’s databases
    and tables can be controlled through the Lake Formation entitlement layer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Custom data catalog solution
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another option for building a data catalog is to create your own with a set
    of AWS services. Consider this option when you have specific requirements that
    are not met by the purpose-built products. The architecture for this DIY approach
    involves leveraging services like DynamoDB and Lambda, as depicted in the accompanying
    diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![         Comprehensive data catalog using AWS Lambda, DynamoDB,            and
    Amazon OpenSearch Service                ](img/B20836_04_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Custom data catalog solution'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, AWS Lambda triggers are used to populate DynamoDB tables with
    object names and metadata when those objects are put into S3; Amazon OpenSearch
    Service is used to search for specific assets, related metadata, and data classifications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data processing functionality of a data lake encompasses the frameworks
    and compute resources necessary for various data processing tasks, such as data
    correction, transformation, merging, splitting, and ML feature engineering. This
    component is a key step in the ML lifecycle as it helps prepare the data for downstream
    model training and inference steps. Common data processing frameworks include
    Python shell scripts using libraries such as pandas, NumPy, and Apache Spark.
    The essential requirements for data processing technology are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration and compatibility with the underlying storage technology**: The
    ability to seamlessly work with the native storage system simplifies data access
    and movement between the storage and processing layers.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with the data catalog**: The capability to interact with the
    data catalog’s metastore to query databases and tables within the catalog.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The capacity to scale compute resources up or down to accommodate
    changing data volumes and processing velocity requirements.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language and framework support**: Support for popular data processing libraries
    and frameworks, such as Python and Spark.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch and real-time processing capabilities**: The capability to handle both
    real-time data streams and bulk data processing in batch mode.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s examine a selection of AWS services that offer data processing capabilities
    within a data lake architecture:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Glue ETL**: In addition to supporting data movement and data catalogs,
    the ETL features of AWS Glue can be used for ETL and general-purpose data processing.
    AWS Glue ETL provides several built-in functions for data transformation, such
    as dropping the `NULL` field (the `NULL` field represents new data) and data filtering.
    It also provides general processing frameworks for Python and Spark to run Python
    scripts and Spark jobs. Glue ETL works natively with the AWS Glue Data Catalog
    to access the databases and tables in the catalog. Glue ETL can also access the
    Amazon S3 storage directly.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elastic MapReduce (EMR)**: **Amazon** **EMR** is a fully managed big
    data processing platform on AWS. It is designed for large-scale data processing
    using the Spark framework and other Apache tools, such as **Apache** **Hive**,
    **Apache** **Hudi**, and **Presto**. It integrates with the Glue Data Catalog
    and Lake Formation natively to access databases and tables in Lake Formation.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Lambda**: AWS Lambda can be used for lightweight data processing tasks
    or as part of a larger data processing pipeline within the data lake architecture.
    Lambda can be triggered by real-time events, so it is a good option for real-time
    data processing.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While efficient data processing prepares raw data for model training and consumption,
    robust data management must also ensure ML teams can track data provenance and
    access historical versions as needed through capabilities like data versioning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: ML data versioning
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To establish a lineage for model training across training data and ML models,
    it is crucial to implement version control for the training, validation, and testing
    datasets. Data versioning control presents challenges as it necessitates the use
    of appropriate tools and adherence to best practices by individuals. During the
    model building process, it is common for data scientists to obtain a copy of a
    dataset, perform cleansing and transformations specific to their needs, and save
    the modified data as a new version. This poses significant challenges in terms
    of data management, including duplication and establishing links between the data
    and its various upstream and downstream tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Data versioning for the entire data lake is out of the scope of this book. Instead,
    we will focus on discussing a few architectural options specifically related to
    versioning control for training datasets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: S3 partitions
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, each newly created or updated dataset is stored in a separate
    S3 partition with a unique prefix, typically derived from the name of the S3 folder.
    While this method can lead to data duplication, it offers a clear and simple approach
    to differentiate between different datasets intended for model training. To maintain
    data integrity, it is advisable to generate datasets through a controlled processing
    pipeline that enforces naming standards. The processing pipeline should also track
    data provenance and record the processing scripts used for data manipulation and
    feature engineering. Furthermore, the datasets should be configured as read-only
    for downstream applications, ensuring their immutability. The following example
    showcases an S3 partition structure, illustrating multiple versions of a training
    dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this instance, the two versions of the dataset are segregated using distinct
    S3 prefixes. To effectively track these training files, it is recommended to employ
    a database for storing metadata pertaining to these training files. When utilizing
    these files, it is crucial to establish links between the training datasets, ML
    training jobs, ML training scripts, and the resulting ML models to establish a
    comprehensive lineage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Versioned S3 buckets
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon S3 offers versioning support for S3 buckets, which can be leveraged to
    manage different versions of training datasets when enabled. With this approach,
    each newly created or updated dataset is assigned a unique version ID at the S3
    object level. Additionally, it is recommended to utilize a database to store all
    relevant metadata associated with each version of the training dataset. This enables
    the establishment of lineage, tracking the journey from data processing to ML
    model training. The metadata should capture essential information to facilitate
    comprehensive tracking and analysis.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Purpose-built data version tools
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of developing custom solutions for data version control, there are
    purpose-built tools available for efficient data version management. For example,
    these tools can be used to track and store different versions of ML training and
    validation datasets, which are important for repeatable experimentations and model
    training tasks. Here are a few notable options:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**Git LFS (Large File Storage)**: Git LFS extends Git’s capabilities to handle
    large files, including datasets. It stores these files outside the Git repository
    while retaining versioning information. Git LFS seamlessly integrates with Git
    and is commonly used to version large files in data-centric projects.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataVersionControl (DVC)**: DVC is an open-source tool designed specifically
    for data versioning and management. It integrates with Git and provides features
    for tracking and managing large datasets. DVC enables lightweight links to actual
    data files stored in remote storage, such as Amazon S3 or a shared file system.
    This approach maintains a history of changes and allows easy switching between
    different dataset versions, eliminating the need for data duplication.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pachyderm**: Pachyderm is an open-source data versioning and data lineage
    tool. It offers version control for data pipelines, enabling tracking of changes
    to data, code, and configuration files. Pachyderm supports distributed data processing
    frameworks like Apache Spark and provides features like reproducibility, data
    lineage, and data lineage-based branching.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These purpose-built tools streamline the process of data versioning, ensuring
    efficient tracking and management of datasets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: ML feature stores
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In large enterprises, it is beneficial to centrally manage common reusable ML
    features like curated customer profile data and standardized product sales data.
    This practice helps reduce the ML project lifecycle, particularly during the data
    understanding and data preparation stages. To achieve this, many organizations
    have built central ML feature stores, an architectural component for storing common
    reusable ML features, as part of the ML development architecture to meet the downstream
    model development, training, and model inference needs. Depending on the specific
    requirements, there are two main options for managing these reusable ML features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, you can build custom feature stores that fulfill the fundamental requirements
    of inserting and looking up organized features for ML model training. These custom
    feature stores can be tailored to meet the specific needs of the organization.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can opt for commercial-grade feature store products, such
    as Amazon SageMaker Feature Store, a ML service offered by AWS, which we will
    delve into in later chapters. It provides advanced capabilities such as online
    and offline functionality for training and inference, metadata tagging, feature
    versioning, and advanced search. These features enable efficient management and
    utilization of ML features in production-grade scenarios.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Data serving for client consumption
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central data management platform should offer various methods, such as APIs
    or Hive metastore-based approaches, to facilitate online access to the data for
    downstream tasks such as data discovery and model training. Additionally, it is
    important to consider data transfer tools that support the movement of data from
    the central data management platform to other data-consuming environments, catering
    to different data consumption patterns such as local access to the data in the
    consuming environment. It is advantageous to explore tools that either have built-in
    data serving capabilities or can be seamlessly integrated with external data serving
    tools, as building custom data serving features could be a challenging engineering
    undertaking.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: When supplying data to data science environments, there are multiple data serving
    patterns to consider. In the following discussion, we will explore two prominent
    data access patterns and their characteristics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Consumption via API
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this data serving pattern, consumption environments and applications have
    the capability to directly access data from the data lake. This can be achieved
    using Hive metastore-compliant tools or through direct access to S3, the underlying
    storage of the data lake. Amazon provides various services that facilitate this
    pattern, such as Amazon Athena, a powerful big data query tool, Amazon EMR, a
    robust big data processing tool, and Amazon Redshift Spectrum, a feature of Amazon
    Redshift.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging these services, data lake data indexed in Glue catalogs can be
    queried without the need to make a separate copy of the data. This pattern is
    particularly suitable when only a subset of the data is required for downstream
    data processing tasks. It offers the advantage of avoiding data duplication while
    enabling efficient selection and processing of specific data subsets as part of
    the overall data workflow.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Consumption via data copy
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this data serving pattern, a specific portion of the data stored in the data
    lake is replicated or copied to the storage of the consumption environment. This
    replication allows for tailored processing and consumption based on specific needs.
    For instance, the latest or most relevant data can be loaded into a data analytics
    environment such as Amazon Redshift. Similarly, it can be delivered to S3 buckets
    owned by a data science environment, enabling efficient access and utilization
    for data science tasks. By replicating the required data subsets, this pattern
    provides flexibility and optimized performance for different processing and consumption
    requirements in various environments.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Special databases for ML
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering emerging ML paradigms like graph neural networks and generative
    AI, specialized databases have been developed to cater to ML-specific tasks such
    as link prediction, cluster classification, and retrieval-augmented generation.
    In the following section, we will delve into two types of databases—vector databases
    and graph databases—and examine how they are utilized in ML tasks. We will explore
    their unique characteristics and applications in the context of ML.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector databases, also known as vector similarity search engines or vector stores,
    are specialized databases designed to efficiently store, index, and query high-dimensional
    vectors. Examples of high-dimensional vectors include numerical vectors’ representation
    of images or text. These databases are particularly well suited for ML applications
    that rely on vector-based computations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In ML, vectors are commonly used to represent data points, embeddings, or feature
    representations. These vectors capture essential information about the underlying
    data, enabling similarity search, clustering, classification, and other ML tasks.
    Vector databases provide powerful tools for handling these vector-based operations
    at scale.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of vector databases is their ability to perform fast
    similarity searches, allowing efficient retrieval of vectors that are most similar
    to a given query vector. This capability is essential in various ML use cases,
    such as recommender systems, content-based search, and anomaly detection.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several vector database providers on the market, each offering its
    own unique features and capabilities. Some of the prominent ones include:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**Facebook AI Similarity Search (FAISS)**: Developed by **Facebook AI Research**
    (**FAIR**), FAISS is an open-source library for efficient similarity search and
    clustering of dense vectors. It provides highly optimized algorithms and data
    structures for fast and scalable vector search.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Milvus**: Milvus is an open-source vector database designed for managing
    and serving large-scale vector datasets. It offers efficient similarity search,
    supports multiple similarity metrics, and provides scalability through distributed
    computing.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pinecone**: Pinecone is a cloud-native vector database service that specializes
    in high-performance similarity search and recommendation systems. It offers real-time
    indexing and retrieval of vectors with low latency and high throughput.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch**: Although primarily known as a full-text search and analytics
    engine, Elasticsearch also provides vector similarity search capabilities using
    plugins for efficient vector indexing and querying.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaviate**: Weaviate is an open-source vector database. It allows you to
    store data objects and vector embeddings from your favorite ML models, and scale
    seamlessly into billions of data objects.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of vector database providers, and the landscape
    is continuously evolving with new solutions and advancements in the field. When
    choosing a vector database provider, it’s important to consider factors such as
    performance, scalability, ease of integration, and the specific requirements of
    your ML use case.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph databases are specialized databases designed to store, manage, and query
    graph-structured data. In a graph database, data is represented as nodes (entities)
    and edges (relationships) connecting these nodes, forming a graph-like structure.
    Graph databases excel at capturing and processing complex relationships and dependencies
    between entities, making them highly relevant for ML tasks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases offer a powerful way to model and analyze data in domains where
    relationships play a crucial role, such as social networks, recommendation systems,
    fraud detection, knowledge graphs, and network analysis. They enable efficient
    traversal of the graph, allowing for queries that explore connections and patterns
    within the data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: In the context of ML, graph databases have multiple applications. One key use
    case is graph-based feature engineering, where graphs are used to represent relationships
    between entities, and the graph structure is leveraged to derive features that
    can enhance the performance of ML models. For example, in a recommendation system,
    a graph database can represent user-item interactions and graph-based features
    can be derived to capture user similarities, item similarities, or collaborative
    filtering patterns.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases also enable graph-based algorithms, such as **graph convolutional
    networks** (**GCNs**), for tasks like node classification, link prediction, and
    graph clustering. These algorithms leverage the graph structure to propagate information
    across nodes and capture complex patterns in the data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, graph databases can be used to store and query graph embeddings,
    which are low-dimensional vector representations of nodes or edges. These embeddings
    capture the structural and semantic information of the graph and can be input
    to ML models for downstream tasks, such as node classification or recommendation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Some of the notable graph databases include **Neo4j**, a popular and widely
    used graph database that allows for efficient storage, retrieval, and querying
    of graph-structured data, and Amazon Neptune, a fully managed graph database service
    provided by AWS.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data pipelines streamline the flow of data by automating tasks such as data
    ingestion, validation, transformation, and feature engineering. These pipelines
    ensure data quality and facilitate the creation of training and validation datasets
    for ML models. Numerous workflow tools are available for constructing data pipelines,
    and many data management tools offer built-in capabilities for building and managing
    these pipelines:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Glue workflows**: AWS Glue workflows provide a native workflow management
    feature within AWS Glue, enabling the orchestration of various Glue jobs like
    data ingestion, processing, and feature engineering. Comprised of trigger and
    node components, a Glue workflow incorporates schedule triggers, event triggers,
    and on-demand triggers. Nodes within the workflow can be either crawler jobs or
    ETL jobs. Triggers initiate workflow runs, while event triggers are emitted after
    the completion of crawler or ETL jobs. By structuring a series of triggers and
    jobs, workflows facilitate the seamless execution of data pipelines within AWS
    Glue.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Step Functions**: AWS Step Functions is a powerful workflow orchestration
    tool that seamlessly integrates with various AWS data processing services like
    AWS Glue and Amazon EMR. It enables the creation of robust workflows to execute
    diverse steps within a data pipeline, such as data ingestion, data processing,
    and feature engineering, ensuring smooth coordination and execution of these tasks.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Managed Workflows for Apache Airflow**: AWS **Managed Workflows for Apache
    Airflow** (**MWAA**) is a fully managed service that simplifies the deployment,
    configuration, and management of Apache Airflow, an open-source platform for orchestrating
    and scheduling data workflows. This service offers scalability, reliability, and
    easy integration with other AWS services, making it an efficient solution for
    managing complex data workflows in the cloud.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having explored the fundamental elements of ML data management architecture,
    the subsequent sections will delve into subjects related to security and governance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authentication and authorization are crucial for ensuring secure access to a
    data lake. Federated authentication, such as AWS **Identity and Access Management**
    (**IAM**), verifies user identities for administration and data consumption purposes.
    AWS Lake Formation combines the built-in Lake Formation access control with AWS
    IAM to govern access to data catalog resources and underlying data storage.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The built-in Lake Formation permission model utilizes commands like grant and
    revoke to control access to resources such as databases and tables, as well as
    actions like table creation. When a user requests access to a resource, both IAM
    policies and Lake Formation permissions are evaluated to verify and enforce access
    before granting it. This multi-layered approach enhances data lake security and
    governance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several personas involved in the administration of the data lake
    and consumption of the data lake resources, including:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**Lake Formation administrator**: A Lake Formation administrator has permission
    to manage all aspects of a Lake Formation data lake in an AWS account. Examples
    include granting/revoking permissions to access data lake resources for other
    users, registering data stores in S3, and creating/deleting databases. When setting
    up Lake Formation, you will need to register as an administrator. An administrator
    can be an AWS IAM user or IAM role. You can add more than one administrator to
    a Lake Formation data lake.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation database creator**: A Lake Formation database creator is granted
    permission to create databases in Lake Formation. A database creator can be an
    IAM user or IAM role.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation database user**: A Lake Formation database user can be granted
    permission to perform different actions against a database. Example permissions
    include create table, drop table, describe table, and alter table. A database
    user can be an IAM user or IAM role.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lake Formation data user**: A Lake Formation data user can be granted permission
    to perform different actions against database tables and columns. Example permissions
    include insert, select, describe, delete, alter, and drop. A data user can be
    an IAM user or an IAM role.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing and querying the database and tables in Lake Formation is facilitated
    through compatible AWS services like Amazon Athena and Amazon EMR. When performing
    queries using these services, Lake Formation verifies the principals (IAM users,
    groups, and roles) associated with them to ensure they have the necessary access
    permissions for the database, tables, and corresponding S3 data location. If access
    is granted, Lake Formation issues a temporary credential to the service, enabling
    it to execute the query securely and efficiently. This process ensures that only
    authorized services can interact with Lake Formation and perform queries on the
    data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having secure access to trustworthy data is essential to the success of an ML
    initiative. Data governance encompasses essential practices to ensure the reliability,
    security, and accountability of data assets. Trustworthy data is achieved through
    the identification and documentation of data flows, as well as the measurement
    and reporting of data quality. Data protection and security involve classifying
    data and applying appropriate access permissions to safeguard its confidentiality
    and integrity. To maintain visibility of data activities, monitoring and auditing
    mechanisms should be implemented, allowing organizations to track and analyze
    actions performed on data, ensuring transparency and accountability in data management.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: A data catalog is one of the most important components of data governance. On
    AWS, the Glue Data Catalog is a fully managed service for data catalog management.
    You also have the option to build custom data catalogs using different foundational
    building blocks. For example, you can follow the reference architecture at [https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html](https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html)
    for building a custom data catalog on AWS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To establish and document data lineage during the ingestion and processing
    of data across different zones, it is important to capture specific data points.
    When utilizing data ingestion and processing tools like AWS Glue, AWS EMR, or
    AWS Lambda in a data pipeline, the following information can be captured to establish
    comprehensive data lineage:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source details**: Include the name of the data source, its location,
    and ownership information to identify the origin of the data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing job history**: Capture the history and details of the data
    processing jobs involved in the pipeline. This includes information such as the
    job name, unique **identifier** (**ID**), associated processing script, and owner
    of the job.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated artifacts**: Document the artifacts generated because of the data
    processing jobs. For example, record the S3 URI or other storage location for
    the target data produced by the pipeline.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data metrics**: Track relevant metrics at different stages of data processing.
    This can include the number of records, data size, data schema, and feature statistics
    to provide insights into the processed data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To store and manage data lineage information and processing metrics, it is recommended
    to establish a central data operational data store. AWS DynamoDB, a fully managed
    NoSQL database, is an excellent technology choice for this purpose. With its capabilities
    optimized for low latency and high transaction access, DynamoDB provides efficient
    storage and retrieval of data lineage records and processing metrics. By capturing
    and documenting these data points, organizations can establish a comprehensive
    data lineage that provides a clear understanding of the data’s journey from its
    source through various processing stages. This documentation enables traceability,
    auditability, and better management of the data as it moves through the pipeline.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Other data governance measures
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to managing data lineage, there are several other important measures
    for effective data governance, including:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality**: Automated data quality checks should be implemented at different
    stages, and quality metrics should be reported. For example, after the source
    data is ingested into the landing zone, an AWS Glue quality check job can run
    to check the data quality using tools such as the open-source `Deequ` library.
    Data quality metrics (such as counts, schema validation, missing data, the wrong
    data type, or statistical deviations from the baseline) and reports can be generated
    for reviews. Optionally, manual or automated operational data cleansing processes
    should be established to correct data quality issues.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cataloging**: Create a central data catalog and run Glue crawlers on
    datasets in the data lake to automatically create an inventory of data and populate
    the central data catalog. Enrich the catalogs with additional metadata to track
    other information to support discovery and data audits, such as the business owner,
    data classification, and data refresh date. For ML workloads, data science teams
    also generate new datasets (for example, new ML features) from the existing datasets
    in the data lake for model training purposes. These datasets should also be registered
    and tracked in a data catalog, and different versions of the data should be retained
    and archived for audit purposes.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access provisioning**: A formal process should be established for requesting
    and granting access to datasets and Lake Formation databases and tables. An external
    ticketing system can be used to manage the workflow for requesting access and
    granting access.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and auditing**: Data access should be monitored, and access history
    should be maintained. Amazon S3 server access logging can be enabled to track
    access to all S3 objects directly. AWS Lake Formation also records all accesses
    to Lake Formation datasets in **AWS** **CloudTrail** (AWS CloudTrail provides
    event history in an AWS account to enable governance, compliance, and operational
    auditing). With Lake Formation auditing, you can get details such as event source,
    event name, SQL queries, and data output location.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these key data governance measures, organizations can establish
    a strong foundation for data management, security, and compliance, enabling them
    to maximize the value of their data assets while mitigating risks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – data management for ML
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this hands-on exercise, you will go through the process of constructing a
    simple data management platform for a fictional retail bank. This platform will
    serve as the foundation for an ML workflow, and we will leverage different AWS
    technologies to build it. If you don’t have an AWS account, you can easily create
    one by following the instructions at [https://aws.amazon.com/console/](https://aws.amazon.com/console/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'The data management platform we create will have the following key components:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: A data lake environment for data management using Lake Formation
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data ingestion component for ingesting files to the data lake using Lambda
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data catalog component using the Glue Data Catalog
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data discovery and query component using the Glue Data Catalog and Athena
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data processing component using Glue ETL
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pipeline component using a Glue pipeline
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the data management architecture we will build
    in this exercise:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Data management architecture for the hands-on exercise ](img/B20836_04_07.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Data management architecture for the hands-on exercise'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with building out this architecture on AWS.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data lake using Lake Formation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will build the data lake architecture using AWS Lake Formation; it is the
    primary service for building data lakes on AWS. After you log on to the **AWS**
    **Management Console**, create an S3 bucket called `MLSA-DataLake-<your initials>`.
    We will use this bucket as the storage for the data lake. If you get a message
    that the bucket name is already in use, try adding some random characters to the
    name to make it unique. If you are not familiar with how to create S3 buckets,
    follow the instructions at the following link: [https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'After the bucket is created, follow these steps to get started with creating
    a data lake:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**Register Lake Formation administrators**: We need to add Lake Formation administrators
    to the data lake. The administrators will have full permission to manage all aspects
    of the data lake. To do this, navigate to the Lake Formation management console,
    click on the **Administrative roles and tasks** link, and you should be prompted
    to add an administrator. Select **Add myself** and click on the **Get started**
    button.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register S3 storage**: Next, we need to register the S3 bucket (`MLSA-DataLake-<your
    initials>`) you created earlier in Lake Formation, so it will be managed and accessible
    through Lake Formation. To do this, click on the **Dashboard** link, expand **Data
    lake setup**, and then click on **Register Location**. Browse and select the bucket
    you created and click on **Register Location**. This S3 bucket will be used by
    Lake Formation to store data for the databases and manage its access permissions.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create database**: Now, we are ready to set up a database called `bank_customer_db`
    for managing retail customers. Before we register the database, let’s first create
    a folder called the `bank_customer_db` folder under the `MLSA-DataLake-<your initials>`
    bucket. This folder will be used to store data files associated with the database.
    To do this, click on the **Create database** button on the Lake Formation dashboard
    and follow the instructions on the screen to create the database.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully created a data lake powered by Lake Formation and
    created a database for data management. With this data lake created, we are now
    ready to build additional data management components. Next, we will create a data
    ingestion pipeline to move files into the data lake.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data ingestion pipeline
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the database is prepared, we can proceed to ingest data into this
    newly created database. As mentioned earlier, there are various data sources available,
    including databases like Amazon RDS, streaming platforms like social media feeds,
    and logs such as CloudTrail. Additionally, AWS offers a range of services for
    building data ingestion pipelines, such as AWS Glue, Amazon Kinesis, and AWS Lambda.
    In this phase of the exercise, we will focus on creating an AWS Lambda function
    job that will facilitate the ingestion of data from other S3 buckets into our
    target database. As mentioned earlier, Lambda functions can be used for lightweight
    data ingestion and processing tasks:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a source S3 bucket and download data files**: Let’s create another
    S3 bucket, called `customer-data-source`, to represent the data source where we
    will ingest the data from.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a Lambda function**: Now, we will create the Lambda function that
    ingests data from the `customer-data-source` bucket to the `MLSA-DataLake-<your
    initials>` bucket:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get started, navigate to the AWS Lambda management console, click on the
    **Functions** link in the left pane, and click on the **Create Function** button
    in the right pane. Choose **Author from scratch**, then enter `datalake-s3-ingest`
    for the function name, and select the latest Python version (e.g., 3.10) as the
    runtime. Keep the default for the execution role, which will create a new IAM
    role for this Lambda function. Click on **Create function** to continue.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, click on **Add trigger**, select **S3** as the trigger,
    and select the `customer-data-source` bucket as the source. For **Event Type**,
    choose the **Put** event and click on the **Add** button to complete the step.
    This trigger will allow the Lambda function to be invoked when there is an S3
    bucket event, such as saving a file into the bucket.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you add the trigger, you will be brought back to the `Lambda->function->
    datalake-s3-ingest` screen. Next, let’s create the function by replacing the default
    function template with the following code block. Replace the `desBucket` variable
    with the name of the actual bucket:'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The new function will also need S3 permission to copy files (*objects*) from
    one bucket to another. For simplicity, just add the `AmazonS3FullAccess` policy
    to the **execution IAM role** associated with the function. You can find the IAM
    role by clicking on the **Permission** tab for the Lambda function.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Trigger data ingestion**: Now, download the sample data files from the following
    link: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, save the file to your local machine. Extract the archived files. There
    should be two files (`customer_data.csv` and `churn_list.csv`).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: You can now trigger the data ingestion process by uploading the `customer_detail.csv`
    and `churn_list.csv` files to the `customer-data-source` bucket and verify the
    process completion by checking the `MLSA-DataLake-<your initials>/bank_customer_db`
    folder for the two files.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: You have now successfully created an AWS Lambda-based data ingestion pipeline
    to automatically move data from a source S3 bucket to a target S3 bucket. With
    this simple ingestion pipeline created and data moved, we are now ready to implement
    components to support the discovery of these data files. Next, let’s create an
    AWS Glue Data Catalog using the Glue crawler.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Glue Data Catalog
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To allow discovery and querying of the data in the `bank_customer_db` database,
    we need to create a data catalog. As discussed earlier, Glue Data Catalog is a
    managed data catalog on AWS. It comes with a utility called an AWS Glue crawler
    that can help discover data and populate the catalog.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use an AWS Glue crawler to crawl the files in the `bank_customer_db`
    S3 folder and generate the catalog:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**Grant permission for Glue**:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let’s grant permission for AWS Glue to access the `bank_customer_db`
    database. We will create a new IAM role for the Glue service to assume on your
    behalf. To do this, create a new IAM service role called `AWSGlueServiceRole_data_lake`,
    and attach the `AWSGlueServiceRole` and `AmazonS3FullAccess` IAM-managed policies
    to it. Make sure you select **Glue** as the service when you create the role.
    If you are not familiar with how to create a role and attach a policy, follow
    the instructions at the following link: [https://docs.aws.amazon.com/IAM/latest/UserGuide](https://docs.aws.amazon.com/IAM/latest/UserGuide)'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After the role is created, click on **Data lake permission** in the left pane
    of the Lake Formation management console and then click the **Grant** button in
    the right pane.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, select `AWSGlueServiceRole_data_lake` for **IAM users and
    role**and `bank_customer_db` under **Named data catalog resources**, choose **Super**
    for both **Database permissions** and **Grantable permissions**, and finally click
    on **Grant**. The **Super** permission allows the service role to have access
    to create databases and grant permission as part of the automation. `AWSGlueServiceRole_data_lake`
    will be used later to configure the Glue crawler job.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Configure the Glue crawler job**:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the Glue crawler by clicking on the **Crawler** link in the Lake Formation
    management console. A new browser tab for Glue will open. Click on the **Create
    Crawler** button to get started. Enter `bank_customer_db_crawler` as the name
    of the crawler. Click on the **Add a data source** button, select **S3**, and
    enter `s3://MLSA-DataLake-<your initials>/bank_customer_db/churn_list/` for the
    **include path** field.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Add another data source** button again. This time, enter `s3://MLSA-DataLake-<your
    initials>/bank_customer_db/customer_data/`.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen, **Configure security settings**, select `AWSGlueServiceRole_data_lake`
    for the existing IAM role, which you used earlier:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next **Set output and scheduling** screen, select `bank_customer_db`
    as the target database, and choose **on demand** as the frequency for the crawler
    schedule.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next **Review and create** screen, select **Finish** on the final screen
    to complete the setup.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Crawler** screen, select the `bank_customer_db_crawler` job you just
    created, click on **Run crawler**, and wait for the status to say **Ready**.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate back to the Lake Formation management console and click on the **Tables**
    link. You will now see two new tables created (`churn_list` and `customer_data`).
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully configured an AWS Glue crawler that automatically
    discovers table schemas from data files and creates data catalogs for the new
    data.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You have successfully created the Glue Data Catalog for the newly ingested data.
    With that, we now have the proper component to support data discovery and query.
    Next, we will use Lake Formation and Athena to discover and query the data in
    the data lake.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Discovering and querying data in the data lake
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To facilitate the data discovery and data understanding phase of the ML workflow,
    it is essential to incorporate data discovery and data query capabilities within
    the data lake.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Lake Formation already provides a list of tags, such as data type
    classification (for example, CSV), for searching tables in the database. Let’s
    add a few more tags for each table to make it more discoverable:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Grant permission to edit the database tables by granting your current user ID
    **Super** permission for both the `customer_data` and `churn_list` tables.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s add some metadata to the table fields. Select the `customer_data` table,
    click on **Edit Schema**, select the `creditscore` field, click on **Edit** and
    **Add** to add a column property, and enter the following, where `description`
    is the key and the actual text is the value:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the same previous steps and add the following column property for the
    `exited` field in the `churn_list` table:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to do some searches using metadata inside the Lake Formation
    management console. Try typing the following words separately in the text box
    for **Find table by properties** to search for tables and see what’s returned:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FICO`'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv`'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`churn flag`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`creditscore`'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`customerid`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that you have found the table you are looking for, let’s query the table
    and see the actual data to learn how to query the data interactively, which is
    an important task performed by data scientists for data exploration and understanding.
    Select the table you want to query and click on the **View data** button in the
    **Actions** drop-down menu. This should bring you to the **Amazon Athena** screen.
    You should see a **Query** tab already created, and the query already executed.
    The results are displayed at the bottom of the screen. If you get a warning message
    stating that you need to provide an output location, select the **Settings** tab,
    and then click on the **Manage** button to provide an S3 location as the output
    location. You can run any other SQL query to explore the data further, such as
    joining the `customer_data` and `churn_list` tables with the `customerid` field:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You have now learned how to discover the data in Lake Formation and run queries
    against the data in a Lake Formation database and tables. Next, let’s run a data
    processing job using the Amazon Glue ETL service to make the data ready for ML
    tasks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Amazon Glue ETL job to process data for ML
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `customer_data` and `churn_list` tables contain features that are useful
    for ML. However, they need to be joined and processed so they can be used to train
    ML models. One option is for the data scientists to download these datasets and
    process them in a Jupyter notebook for model training. Another option is to process
    the data using a separate processing engine so that the data scientists can work
    with the processed data directly. Here, we will set up an AWS Glue job to process
    the data in the `customer_data` and `churn_list` tables and transform them into
    new ML features that are ready for model training directly:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new S3 bucket called `MLSA-DataLake-Serving-<your initials>`.
    We will use this bucket to store the output training datasets from the Glue job.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the Lake Formation console, grant `AWSGlueService_Role` **Super** access
    to the `customer_data` and `churn_list` tables. We will use this role to run the
    Glue job.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To start creating the Glue job, go to the Glue console and click on the **ETL
    Jobs** link on the Glue console. Click on **Script editor** and then click on
    the **Create script** button.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the script editor screen, change the job name from **Untitled job** to `customer_churn_process`
    for easy tracking.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Job details** tab, select `AWSGlueService_Role` as the IAM role. Add
    a new `Job` parameter called `target_bucket` under `Advanced Properties` and enter
    the value of your target bucket for the output files.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Script tab** screen, copy the following code blocks to the code section.
    Make sure to replace `default_bucket` with your own bucket in the code. The following
    code block first joins the `churn_list` and `customer_data` tables using the `customerid`
    column as the key, then transforms the `gender` and `geo` columns with an index,
    creates a new DataFrame with only the relevant columns, and finally saves the
    output file to an S3 location using the date and generated version ID as partitions.
    The code uses default values for the target bucket and prefix variables and generates
    a date partition and version partition for the S3 location. The job can also accept
    input arguments for these parameters.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block sets up default configurations, such as `SparkContext`
    and a default bucket:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code joins the `customer_data` and `churn_list` tables into a
    single table using the `customerid` column as the key:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code block transforms several data columns from string labels
    to label indices and writes the final file to an output location in S3:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Click on **Save** and then the **Run job** button to run the job. Check the
    job running status by clicking on the **ETL jobs** link in the Glue console, and
    then click on **Job run monitoring**.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the job completes, check the `s3://MLSA-DataLake-Serving-<your initials>/ml-customer-churn/<date>/<guid>/`
    location in S3 and see whether a new CSV file was generated. Open the file and
    see whether you see the new processed dataset in the file.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now successfully built an AWS Glue job for data processing and feature
    engineering for ML. With this, you can automate data processing and feature engineering,
    which is critical to achieve reproducibility and governance. Try creating a crawler
    to crawl the newly processed data in the `MLSA-DataLake-Serving-<your initials>`
    bucket to make it available in the Glue catalog and run some queries against it.
    You should see a new table created with multiple partitions (for example, `ml-customer-churn`,
    `date`, and `GUID`) for the different training datasets. You can query the data
    by using the `GUID` partition as a query condition.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline using Glue workflows
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will construct a pipeline that executes a data ingestion job, followed
    by the creation of a database catalog for the data. Finally, a data processing
    job will be initiated to generate the training dataset. This pipeline will automate
    the flow of data from the source to the desired format, ensuring seamless and
    efficient data processing for ML model training:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: To start, click on the **Workflows (orchestration)** link in the left pane of
    the Gluemanagement console.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Add workflow** and enter a name for your workflow on the next screen.
    Then, click on the **Create workflow** button.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the workflow you just created and click on **Add trigger**. Select the
    **Add New** tab, and then enter a name for the trigger and select the `on-demand`
    trigger type.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, you will see a new **Add Node** icon show up. Click
    on the **Add Node** icon, select the **Crawler** tab, and select `bank_customer_db_crawler`,
    then click on **Add**.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, click on the **Crawler** icon, and you will see
    a new **Add Trigger** icon show up. Click on the **Add Trigger** icon, select
    the **Add new** tab, and select **Start after ANY event** as the trigger logic,
    and then click on **Add**.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, click on the **Add Node** icon, select the **Jobs**
    tab, and select the `customer_churn_process` job.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workflow UI designer, the final workflow should look like the following
    diagram:![Figure 4.7 – Glue data flow design ](img/B20836_04_08.png)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.8: Glue data flow design'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, you are ready to run the workflow. Select the workflow and select **Run**
    from the **Actions** dropdown. You can monitor the running status by selecting
    the **Run ID** and clicking on **View run details**. You should see something
    similar to the following screenshot:![Figure 4.8 – Glue workflow execution ](img/Image2513.jpg)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.9: Glue workflow execution'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Try deleting the `customer_data` and `churn_list` tables and re-run the workflow.
    See whether the new tables are created again. Check the `s3://MLSA-DataLake-Serving-<your
    initials>/ml-customer-churn/<date>/` S3 location to verify a new folder is created
    with a new dataset.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have completed the hands-on lab and learned how to build
    a simple data lake and its supporting components to allow data cataloging, data
    querying, and data processing. You should now be able to apply some of the skills
    learned to real-world design and the implementation of a data management platform
    on AWS to support the ML development lifecycle.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the considerations for managing data in the
    context of ML and explored the architecture of an enterprise data management platform
    for ML. We examined the intersection of data management with the ML lifecycle
    and learned how to design a data lake architecture on AWS. To apply these concepts,
    we went through the process of building a data lake using AWS Lake Formation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Through hands-on experience, we practiced data ingestion, processing, and cataloging
    for data discovery, querying, and ML tasks. Additionally, we gained proficiency
    in using AWS data management tools such as AWS Glue, AWS Lambda, and Amazon Athena.
    In the next chapter, our focus will shift to the architecture and technologies
    involved in constructing data science environments using open-source tools.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
